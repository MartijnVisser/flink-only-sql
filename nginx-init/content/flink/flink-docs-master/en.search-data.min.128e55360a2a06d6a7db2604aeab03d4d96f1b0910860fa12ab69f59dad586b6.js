"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/flink/flink-docs-master/docs/connectors/dataset/",title:"DataSet Connectors",section:"Connectors",content:" "}),e.add({id:1,href:"/flink/flink-docs-master/docs/connectors/datastream/",title:"DataStream Connectors",section:"Connectors",content:""}),e.add({id:2,href:"/flink/flink-docs-master/docs/libs/cep/",title:"Event Processing (CEP)",section:"Libraries",content:` FlinkCEP - Complex event processing for Flink # FlinkCEP is the Complex Event Processing (CEP) library implemented on top of Flink. It allows you to detect event patterns in an endless stream of events, giving you the opportunity to get hold of what\u0026rsquo;s important in your data.
This page describes the API calls available in Flink CEP. We start by presenting the Pattern API, which allows you to specify the patterns that you want to detect in your stream, before presenting how you can detect and act upon matching event sequences. We then present the assumptions the CEP library makes when dealing with lateness in event time and how you can migrate your job from an older Flink version to Flink-1.13.
Getting Started # If you want to jump right in, set up a Flink program and add the FlinkCEP dependency to the pom.xml of your project.
Java \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-cep\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Scala \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-cep-scala_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! FlinkCEP is not part of the binary distribution. See how to link with it for cluster execution here.
Now you can start writing your first CEP program using the Pattern API.
The events in the DataStream to which you want to apply pattern matching must implement proper equals() and hashCode() methods because FlinkCEP uses them for comparing and matching events. Java DataStream\u0026lt;Event\u0026gt; input = ...; Pattern\u0026lt;Event, ?\u0026gt; pattern = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where( new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event event) { return event.getId() == 42; } } ).next(\u0026#34;middle\u0026#34;).subtype(SubEvent.class).where( new SimpleCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent subEvent) { return subEvent.getVolume() \u0026gt;= 10.0; } } ).followedBy(\u0026#34;end\u0026#34;).where( new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event event) { return event.getName().equals(\u0026#34;end\u0026#34;); } } ); PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); DataStream\u0026lt;Alert\u0026gt; result = patternStream.process( new PatternProcessFunction\u0026lt;Event, Alert\u0026gt;() { @Override public void processMatch( Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern, Context ctx, Collector\u0026lt;Alert\u0026gt; out) throws Exception { out.collect(createAlertFrom(pattern)); } }); Scala val input: DataStream[Event] = ... val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;).where(_.getId == 42) .next(\u0026#34;middle\u0026#34;).subtype(classOf[SubEvent]).where(_.getVolume \u0026gt;= 10.0) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;end\u0026#34;) val patternStream = CEP.pattern(input, pattern) val result: DataStream[Alert] = patternStream.process( new PatternProcessFunction[Event, Alert]() { override def processMatch( \`match\`: util.Map[String, util.List[Event]], ctx: PatternProcessFunction.Context, out: Collector[Alert]): Unit = { out.collect(createAlertFrom(pattern)) } }) The Pattern API # The pattern API allows you to define complex pattern sequences that you want to extract from your input stream.
Each complex pattern sequence consists of multiple simple patterns, i.e. patterns looking for individual events with the same properties. From now on, we will call these simple patterns patterns, and the final complex pattern sequence we are searching for in the stream, the pattern sequence. You can see a pattern sequence as a graph of such patterns, where transitions from one pattern to the next occur based on user-specified conditions, e.g. event.getName().equals(\u0026quot;end\u0026quot;). A match is a sequence of input events which visits all patterns of the complex pattern graph, through a sequence of valid pattern transitions.
Each pattern must have a unique name, which you use later to identify the matched events. Pattern names CANNOT contain the character \u0026quot;:\u0026quot;. In the rest of this section we will first describe how to define Individual Patterns, and then how you can combine individual patterns into Complex Patterns.
Individual Patterns # A Pattern can be either a singleton or a looping pattern. Singleton patterns accept a single event, while looping patterns can accept more than one. In pattern matching symbols, the pattern \u0026quot;a b+ c? d\u0026quot; (or \u0026quot;a\u0026quot;, followed by one or more \u0026quot;b\u0026quot;\u0026rsquo;s, optionally followed by a \u0026quot;c\u0026quot;, followed by a \u0026quot;d\u0026quot;), a, c?, and d are singleton patterns, while b+ is a looping one. By default, a pattern is a singleton pattern and you can transform it to a looping one by using Quantifiers. Each pattern can have one or more Conditions based on which it accepts events.
Quantifiers # In FlinkCEP, you can specify looping patterns using these methods: pattern.oneOrMore(), for patterns that expect one or more occurrences of a given event (e.g. the b+ mentioned before); and pattern.times(#ofTimes), for patterns that expect a specific number of occurrences of a given type of event, e.g. 4 a\u0026rsquo;s; and pattern.times(#fromTimes, #toTimes), for patterns that expect a specific minimum number of occurrences and a maximum number of occurrences of a given type of event, e.g. 2-4 as.
You can make looping patterns greedy using the pattern.greedy() method, but you cannot yet make group patterns greedy. You can make all patterns, looping or not, optional using the pattern.optional() method.
For a pattern named start, the following are valid quantifiers:
Java // expecting 4 occurrences start.times(4); // expecting 0 or 4 occurrences start.times(4).optional(); // expecting 2, 3 or 4 occurrences start.times(2, 4); // expecting 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).greedy(); // expecting 0, 2, 3 or 4 occurrences start.times(2, 4).optional(); // expecting 0, 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).optional().greedy(); // expecting 1 or more occurrences start.oneOrMore(); // expecting 1 or more occurrences and repeating as many as possible start.oneOrMore().greedy(); // expecting 0 or more occurrences start.oneOrMore().optional(); // expecting 0 or more occurrences and repeating as many as possible start.oneOrMore().optional().greedy(); // expecting 2 or more occurrences start.timesOrMore(2); // expecting 2 or more occurrences and repeating as many as possible start.timesOrMore(2).greedy(); // expecting 0, 2 or more occurrences start.timesOrMore(2).optional() // expecting 0, 2 or more occurrences and repeating as many as possible start.timesOrMore(2).optional().greedy(); Scala // expecting 4 occurrences start.times(4) // expecting 0 or 4 occurrences start.times(4).optional() // expecting 2, 3 or 4 occurrences start.times(2, 4) // expecting 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).greedy() // expecting 0, 2, 3 or 4 occurrences start.times(2, 4).optional() // expecting 0, 2, 3 or 4 occurrences and repeating as many as possible start.times(2, 4).optional().greedy() // expecting 1 or more occurrences start.oneOrMore() // expecting 1 or more occurrences and repeating as many as possible start.oneOrMore().greedy() // expecting 0 or more occurrences start.oneOrMore().optional() // expecting 0 or more occurrences and repeating as many as possible start.oneOrMore().optional().greedy() // expecting 2 or more occurrences start.timesOrMore(2) // expecting 2 or more occurrences and repeating as many as possible start.timesOrMore(2).greedy() // expecting 0, 2 or more occurrences start.timesOrMore(2).optional() // expecting 0, 2 or more occurrences and repeating as many as possible start.timesOrMore(2).optional().greedy() Conditions # For every pattern you can specify a condition that an incoming event has to meet in order to be \u0026ldquo;accepted\u0026rdquo; into the pattern e.g. its value should be larger than 5, or larger than the average value of the previously accepted events. You can specify conditions on the event properties via the pattern.where(), pattern.or() or pattern.until() methods. These can be either IterativeConditions or SimpleConditions.
Iterative Conditions: This is the most general type of condition. This is how you can specify a condition that accepts subsequent events based on properties of the previously accepted events or a statistic over a subset of them.
Below is the code for an iterative condition that accepts the next event for a pattern named \u0026ldquo;middle\u0026rdquo; if its name starts with \u0026ldquo;foo\u0026rdquo;, and if the sum of the prices of the previously accepted events for that pattern plus the price of the current event do not exceed the value of 5.0. Iterative conditions can be powerful, especially in combination with looping patterns, e.g. oneOrMore().
Java middle.oneOrMore() .subtype(SubEvent.class) .where(new IterativeCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent value, Context\u0026lt;SubEvent\u0026gt; ctx) throws Exception { if (!value.getName().startsWith(\u0026#34;foo\u0026#34;)) { return false; } double sum = value.getPrice(); for (Event event : ctx.getEventsForPattern(\u0026#34;middle\u0026#34;)) { sum += event.getPrice(); } return Double.compare(sum, 5.0) \u0026lt; 0; } }); Scala middle.oneOrMore() .subtype(classOf[SubEvent]) .where( (value, ctx) =\u0026gt; { lazy val sum = ctx.getEventsForPattern(\u0026#34;middle\u0026#34;).map(_.getPrice).sum value.getName.startsWith(\u0026#34;foo\u0026#34;) \u0026amp;\u0026amp; sum + value.getPrice \u0026lt; 5.0 } ) The call to ctx.getEventsForPattern(...) finds all the previously accepted events for a given potential match. The cost of this operation can vary, so when implementing your condition, try to minimize its use. Described context gives one access to event time characteristics as well. For more info see Time context.
Simple Conditions: This type of condition extends the aforementioned IterativeCondition class and decides whether to accept an event or not, based only on properties of the event itself.
Java start.where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return value.getName().startsWith(\u0026#34;foo\u0026#34;); } }); Scala start.where(event =\u0026gt; event.getName.startsWith(\u0026#34;foo\u0026#34;)) Finally, you can also restrict the type of the accepted event to a subtype of the initial event type (here Event) via the pattern.subtype(subClass) method.
Java start.subtype(SubEvent.class).where(new SimpleCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent value) { return ...; // some condition } }); Scala start.subtype(classOf[SubEvent]).where(subEvent =\u0026gt; ... /* some condition */) Combining Conditions: As shown above, you can combine the subtype condition with additional conditions. This holds for every condition. You can arbitrarily combine conditions by sequentially calling where(). The final result will be the logical AND of the results of the individual conditions. To combine conditions using OR, you can use the or() method, as shown below.
Java pattern.where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return ...; // some condition } }).or(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return ...; // or condition } }); Scala pattern.where(event =\u0026gt; ... /* some condition */).or(event =\u0026gt; ... /* or condition */) Stop condition: In case of looping patterns (oneOrMore() and oneOrMore().optional()) you can also specify a stop condition, e.g. accept events with value larger than 5 until the sum of values is smaller than 50.
To better understand it, have a look at the following example. Given
pattern like \u0026quot;(a+ until b)\u0026quot; (one or more \u0026quot;a\u0026quot; until \u0026quot;b\u0026quot;)
a sequence of incoming events \u0026quot;a1\u0026quot; \u0026quot;c\u0026quot; \u0026quot;a2\u0026quot; \u0026quot;b\u0026quot; \u0026quot;a3\u0026quot;
the library will output results: {a1 a2} {a1} {a2} {a3}.
As you can see {a1 a2 a3} or {a2 a3} are not returned due to the stop condition.
where(condition) # Defines a condition for the current pattern. To match the pattern, an event must satisfy the condition. Multiple consecutive where() clauses lead to their conditions being ANDed.
Java pattern.where(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // some condition } }); Scala pattern.where(event =\u0026gt; ... /* some condition */) or(condition) # Adds a new condition which is ORed with an existing one. An event can match the pattern only if it passes at least one of the conditions.
Java pattern.where(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // some condition } }).or(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // alternative condition } }); Scala pattern.where(event =\u0026gt; ... /* some condition */) .or(event =\u0026gt; ... /* alternative condition */) until(condition) # Specifies a stop condition for a looping pattern. Meaning if event matching the given condition occurs, no more events will be accepted into the pattern. Applicable only in conjunction with oneOrMore() NOTE: It allows for cleaning state for corresponding pattern on event-based condition.
Java pattern.oneOrMore().until(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // alternative condition } }); Scala pattern.oneOrMore().until(event =\u0026gt; ... /* some condition */) subtype(subClass) # Defines a subtype condition for the current pattern. An event can only match the pattern if it is of this subtype.
Java pattern.subtype(SubEvent.class); Scala pattern.subtype(classOf[SubEvent]) oneOrMore() # Defines a subtype condition for the current pattern. An event can only match the pattern if it is of this subtype.
Specifies that this pattern expects at least one occurrence of a matching event. By default a relaxed internal contiguity (between subsequent events) is used. For more info on internal contiguity see consecutive. It is advised to use either until() or within() to enable state clearing.
Java pattern.oneOrMore(); Scala pattern.oneOrMore() timesOrMore(#times) # Specifies that this pattern expects at least #times occurrences of a matching event. By default a relaxed internal contiguity (between subsequent events) is used. For more info on internal contiguity see consecutive.
Java pattern.timesOrMore(2); times(#ofTimes) # Specifies that this pattern expects an exact number of occurrences of a matching event. By default a relaxed internal contiguity (between subsequent events) is used. For more info on internal contiguity see consecutive.
Java pattern.times(2); Scala pattern.times(2) times(#fromTimes, #toTimes) # Specifies that this pattern expects occurrences between #fromTimes and #toTimes of a matching event. By default a relaxed internal contiguity (between subsequent events) is used. For more info on internal contiguity see consecutive.
Java pattern.times(2, 4); Scala pattern.times(2, 4) optional() # Specifies that this pattern is optional, i.e. it may not occur at all. This is applicable to all aforementioned quantifiers.
Java pattern.oneOrMore().optional(); Scala pattern.oneOrMore().optional() greedy() # Specifies that this pattern is greedy, i.e. it will repeat as many as possible. This is only applicable to quantifiers and it does not support group pattern currently.
Java pattern.oneOrMore().greedy(); Scala pattern.oneOrMore().greedy() Combining Patterns # Now that you\u0026rsquo;ve seen what an individual pattern can look like, it is time to see how to combine them into a full pattern sequence.
A pattern sequence has to start with an initial pattern, as shown below:
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;); Scala val start : Pattern[Event, _] = Pattern.begin(\u0026#34;start\u0026#34;) Next, you can append more patterns to your pattern sequence by specifying the desired contiguity conditions between them. FlinkCEP supports the following forms of contiguity between events:
Strict Contiguity: Expects all matching events to appear strictly one after the other, without any non-matching events in-between.
Relaxed Contiguity: Ignores non-matching events appearing in-between the matching ones.
Non-Deterministic Relaxed Contiguity: Further relaxes contiguity, allowing additional matches that ignore some matching events.
To apply them between consecutive patterns, you can use:
next(), for strict, followedBy(), for relaxed, and followedByAny(), for non-deterministic relaxed contiguity. or
notNext(), if you do not want an event type to directly follow another notFollowedBy(), if you do not want an event type to be anywhere between two other event types. A pattern sequence cannot end with notFollowedBy() if the time interval is not defined via withIn(). A NOT pattern cannot be preceded by an optional one. Java // strict contiguity Pattern\u0026lt;Event, ?\u0026gt; strict = start.next(\u0026#34;middle\u0026#34;).where(...); // relaxed contiguity Pattern\u0026lt;Event, ?\u0026gt; relaxed = start.followedBy(\u0026#34;middle\u0026#34;).where(...); // non-deterministic relaxed contiguity Pattern\u0026lt;Event, ?\u0026gt; nonDetermin = start.followedByAny(\u0026#34;middle\u0026#34;).where(...); // NOT pattern with strict contiguity Pattern\u0026lt;Event, ?\u0026gt; strictNot = start.notNext(\u0026#34;not\u0026#34;).where(...); // NOT pattern with relaxed contiguity Pattern\u0026lt;Event, ?\u0026gt; relaxedNot = start.notFollowedBy(\u0026#34;not\u0026#34;).where(...); Scala // strict contiguity val strict: Pattern[Event, _] = start.next(\u0026#34;middle\u0026#34;).where(...) // relaxed contiguity val relaxed: Pattern[Event, _] = start.followedBy(\u0026#34;middle\u0026#34;).where(...) // non-deterministic relaxed contiguity val nonDetermin: Pattern[Event, _] = start.followedByAny(\u0026#34;middle\u0026#34;).where(...) // NOT pattern with strict contiguity val strictNot: Pattern[Event, _] = start.notNext(\u0026#34;not\u0026#34;).where(...) // NOT pattern with relaxed contiguity val relaxedNot: Pattern[Event, _] = start.notFollowedBy(\u0026#34;not\u0026#34;).where(...) Relaxed contiguity means that only the first succeeding matching event will be matched, while with non-deterministic relaxed contiguity, multiple matches will be emitted for the same beginning. As an example, a pattern \u0026quot;a b\u0026quot;, given the event sequence \u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, will give the following results:
Strict Contiguity between \u0026quot;a\u0026quot; and \u0026quot;b\u0026quot;: {} (no match), the \u0026quot;c\u0026quot; after \u0026quot;a\u0026quot; causes \u0026quot;a\u0026quot; to be discarded.
Relaxed Contiguity between \u0026quot;a\u0026quot; and \u0026quot;b\u0026quot;: {a b1}, as relaxed continuity is viewed as \u0026ldquo;skip non-matching events till the next matching one\u0026rdquo;.
Non-Deterministic Relaxed Contiguity between \u0026quot;a\u0026quot; and \u0026quot;b\u0026quot;: {a b1}, {a b2}, as this is the most general form.
It\u0026rsquo;s also possible to define a temporal constraint for the pattern to be valid. For example, you can define that a pattern should occur within 10 seconds via the pattern.within() method. Temporal patterns are supported for both processing and event time.
A pattern sequence can only have one temporal constraint. If multiple such constraints are defined on different individual patterns, then the smallest is applied. Java next.within(Time.seconds(10)); Scala next.within(Time.seconds(10)) Notice that a pattern sequence can end with notFollowedBy() with temporal constraint E.g. a pattern like:
Java Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).notFollowedBy(\u0026#34;end\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }).within(Time.seconds(10)); Scala Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .notFollowedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;b\u0026#34;) .within(Time.seconds(10)) Contiguity within looping patterns # You can apply the same contiguity condition as discussed in the previous section within a looping pattern. The contiguity will be applied between elements accepted into such a pattern. To illustrate the above with an example, a pattern sequence \u0026quot;a b+ c\u0026quot; (\u0026quot;a\u0026quot; followed by any(non-deterministic relaxed) sequence of one or more \u0026quot;b\u0026quot;\u0026rsquo;s followed by a \u0026quot;c\u0026quot;) with input \u0026quot;a\u0026quot;, \u0026quot;b1\u0026quot;, \u0026quot;d1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;d2\u0026quot;, \u0026quot;b3\u0026quot; \u0026quot;c\u0026quot; will have the following results:
Strict Contiguity: {a b1 c}, {a b2 c}, {a b3 c} - there are no adjacent \u0026quot;b\u0026quot;s.
Relaxed Contiguity: {a b1 c}, {a b1 b2 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - \u0026quot;d\u0026quot;\u0026rsquo;s are ignored.
Non-Deterministic Relaxed Contiguity: {a b1 c}, {a b1 b2 c}, {a b1 b3 c}, {a b1 b2 b3 c}, {a b2 c}, {a b2 b3 c}, {a b3 c} - notice the {a b1 b3 c}, which is the result of relaxing contiguity between \u0026quot;b\u0026quot;\u0026rsquo;s.
For looping patterns (e.g. oneOrMore() and times()) the default is relaxed contiguity. If you want strict contiguity, you have to explicitly specify it by using the consecutive() call, and if you want non-deterministic relaxed contiguity you can use the allowCombinations() call.
consecutive() # Works in conjunction with oneOrMore() and times() and imposes strict contiguity between the matching events, i.e. any non-matching element breaks the match (as in next()). If not applied a relaxed contiguity (as in followedBy()) is used.
E.g. a pattern like:
Java Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;c\u0026#34;); } }) .followedBy(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).oneOrMore().consecutive() .followedBy(\u0026#34;end1\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }); Scala Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().consecutive() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B with consecutive applied: {C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B} without consecutive applied: {C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}.
allowCombinations() # Works in conjunction with oneOrMore() and times() and imposes non-deterministic relaxed contiguity between the matching events (as in followedByAny()). If not applied a relaxed contiguity (as in followedBy()) is used.
E.g. a pattern like:
Java Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;c\u0026#34;); } }) .followedBy(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).oneOrMore().allowCombinations() .followedBy(\u0026#34;end1\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }); Scala Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().allowCombinations() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {C A1 B}, {C A1 A2 B}, {C A1 A3 B}, {C A1 A4 B}, {C A1 A2 A3 B}, {C A1 A2 A4 B}, {C A1 A3 A4 B}, {C A1 A2 A3 A4 B} without combinations enabled: {C A1 B}, {C A1 A2 B}, {C A1 A2 A3 B}, {C A1 A2 A3 A4 B}.
Groups of patterns # It\u0026rsquo;s also possible to define a pattern sequence as the condition for begin, followedBy, followedByAny and next. The pattern sequence will be considered as the matching condition logically and a GroupPattern will be returned and it is possible to apply oneOrMore(), times(#ofTimes), times(#fromTimes, #toTimes), optional(), consecutive(), allowCombinations() to the GroupPattern.
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.begin( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;start_middle\u0026#34;).where(...) ); // strict contiguity Pattern\u0026lt;Event, ?\u0026gt; strict = start.next( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;next_start\u0026#34;).where(...).followedBy(\u0026#34;next_middle\u0026#34;).where(...) ).times(3); // relaxed contiguity Pattern\u0026lt;Event, ?\u0026gt; relaxed = start.followedBy( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;followedby_start\u0026#34;).where(...).followedBy(\u0026#34;followedby_middle\u0026#34;).where(...) ).oneOrMore(); // non-deterministic relaxed contiguity Pattern\u0026lt;Event, ?\u0026gt; nonDetermin = start.followedByAny( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;followedbyany_start\u0026#34;).where(...).followedBy(\u0026#34;followedbyany_middle\u0026#34;).where(...) ).optional(); Scala val start: Pattern[Event, _] = Pattern.begin( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;start_middle\u0026#34;).where(...) ) // strict contiguity val strict: Pattern[Event, _] = start.next( Pattern.begin[Event](\u0026#34;next_start\u0026#34;).where(...).followedBy(\u0026#34;next_middle\u0026#34;).where(...) ).times(3) // relaxed contiguity val relaxed: Pattern[Event, _] = start.followedBy( Pattern.begin[Event](\u0026#34;followedby_start\u0026#34;).where(...).followedBy(\u0026#34;followedby_middle\u0026#34;).where(...) ).oneOrMore() // non-deterministic relaxed contiguity val nonDetermin: Pattern[Event, _] = start.followedByAny( Pattern.begin[Event](\u0026#34;followedbyany_start\u0026#34;).where(...).followedBy(\u0026#34;followedbyany_middle\u0026#34;).where(...) ).optional() begin(#name) # Defines a starting pattern.
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;); Scala val start = Pattern.begin[Event](\u0026#34;start\u0026#34;) begin(#pattern_sequence) # Defines a starting pattern
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.\u0026lt;Event\u0026gt;begin( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ); Scala val start = Pattern.begin( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ) next(#name) # Appends a new pattern. A matching event has to directly succeed the previous matching event (strict contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; next = start.next(\u0026#34;middle\u0026#34;); Scala val next = start.next(\u0026#34;middle\u0026#34;) next(#pattern_sequence) # Appends a new pattern. A sequence of matching events have to directly succeed the previous matching event (strict contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; next = start.next( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ); Scala val next = start.next( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ) followedBy(#name) # Appends a new pattern. Other events can occur between a matching event and the previous matching event (relaxed contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; followedBy = start.followedBy(\u0026#34;middle\u0026#34;); Scala val followedBy = start.followedBy(\u0026#34;middle\u0026#34;) followedBy(#pattern_sequence) # Appends a new pattern. Other events can occur between a matching event and the previous matching event (relaxed contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; followedBy = start.followedBy( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ); Scala val followedBy = start.followedBy( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ) followedByAny(#name) # Appends a new pattern. Other events can occur between a matching event and the previous matching event, and alternative matches will be presented for every alternative matching event (non-deterministic relaxed contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; followedByAny = start.followedByAny(\u0026#34;middle\u0026#34;); Scala val followedByAny = start.followedByAny(\u0026#34;middle\u0026#34;) followedByAny(#pattern_sequence) # Appends a new pattern. Other events can occur between a matching event and the previous matching event, and alternative matches will be presented for every alternative matching event (non-deterministic relaxed contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; next = start.next( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ); Scala val followedByAny = start.followedByAny( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;middle\u0026#34;).where(...) ) notNext() # Appends a new negative pattern. A matching (negative) event has to directly succeed the previous matching event (strict contiguity) for the partial match to be discarded.
Java Pattern\u0026lt;Event, ?\u0026gt; notNext = start.notNext(\u0026#34;not\u0026#34;); Scala val notNext = start.notNext(\u0026#34;not\u0026#34;) notFollowedBy() # Appends a new negative pattern. A partial matching event sequence will be discarded even if other events occur between the matching (negative) event and the previous matching event (relaxed contiguity).
Java Pattern\u0026lt;Event, ?\u0026gt; notFollowedBy = start.notFollowedBy(\u0026#34;not\u0026#34;); Scala val notFollowedBy = start.notFollowedBy(\u0026#34;not\u0026#34;) within(time) # Defines the maximum time interval for an event sequence to match the pattern. If a non-completed event sequence exceeds this time, it is discarded.
Java pattern.within(Time.seconds(10)); Scala pattern.within(Time.seconds(10)) After Match Skip Strategy # For a given pattern, the same event may be assigned to multiple successful matches. To control to how many matches an event will be assigned, you need to specify the skip strategy called AfterMatchSkipStrategy. There are five types of skip strategies, listed as follows:
NO_SKIP: Every possible match will be emitted. SKIP_TO_NEXT: Discards every partial match that started with the same event, emitted match was started. SKIP_PAST_LAST_EVENT: Discards every partial match that started after the match started but before it ended. SKIP_TO_FIRST: Discards every partial match that started after the match started but before the first event of PatternName occurred. SKIP_TO_LAST: Discards every partial match that started after the match started but before the last event of PatternName occurred. Notice that when using SKIP_TO_FIRST and SKIP_TO_LAST skip strategy, a valid PatternName should also be specified.
For example, for a given pattern b+ c and a data stream b1 b2 b3 c, the differences between these four skip strategies are as follows:
Skip Strategy Result Description NO_SKIP b1 b2 b3 c
b2 b3 c
b3 c
After found matching b1 b2 b3 c, the match process will not discard any result. SKIP_TO_NEXT b1 b2 b3 c
b2 b3 c
b3 c
After found matching b1 b2 b3 c, the match process will not discard any result, because no other match could start at b1. SKIP_PAST_LAST_EVENT b1 b2 b3 c
After found matching b1 b2 b3 c, the match process will discard all started partial matches. SKIP_TO_FIRST[b] b1 b2 b3 c
b2 b3 c
b3 c
After found matching b1 b2 b3 c, the match process will try to discard all partial matches started before b1, but there are no such matches. Therefore nothing will be discarded. SKIP_TO_LAST[b] b1 b2 b3 c
b3 c
After found matching b1 b2 b3 c, the match process will try to discard all partial matches started before b3. There is one such match b2 b3 c Have a look also at another example to better see the difference between NO_SKIP and SKIP_TO_FIRST: Pattern: (a | b | c) (b | c) c+.greedy d and sequence: a b c1 c2 c3 d Then the results will be:
Skip Strategy Result Description NO_SKIP a b c1 c2 c3 d
b c1 c2 c3 d
c1 c2 c3 d
After found matching a b c1 c2 c3 d, the match process will not discard any result. SKIP_TO_FIRST[c*] a b c1 c2 c3 d
c1 c2 c3 d
After found matching a b c1 c2 c3 d, the match process will discard all partial matches started before c1. There is one such match b c1 c2 c3 d. To better understand the difference between NO_SKIP and SKIP_TO_NEXT take a look at following example: Pattern: a b+ and sequence: a b1 b2 b3 Then the results will be:
Skip Strategy Result Description NO_SKIP a b1
a b1 b2
a b1 b2 b3
After found matching a b1, the match process will not discard any result. SKIP_TO_NEXT a b1
After found matching a b1, the match process will discard all partial matches started at a. This means neither a b1 b2 nor a b1 b2 b3 could be generated. To specify which skip strategy to use, just create an AfterMatchSkipStrategy by calling:
Function Description AfterMatchSkipStrategy.noSkip() Create a NO_SKIP skip strategy AfterMatchSkipStrategy.skipToNext() Create a SKIP_TO_NEXT skip strategy AfterMatchSkipStrategy.skipPastLastEvent() Create a SKIP_PAST_LAST_EVENT skip strategy AfterMatchSkipStrategy.skipToFirst(patternName) Create a SKIP_TO_FIRST skip strategy with the referenced pattern name patternName AfterMatchSkipStrategy.skipToLast(patternName) Create a SKIP_TO_LAST skip strategy with the referenced pattern name patternName Then apply the skip strategy to a pattern by calling:
Java AfterMatchSkipStrategy skipStrategy = ...; Pattern.begin(\u0026#34;patternName\u0026#34;, skipStrategy); Scala val skipStrategy = ... Pattern.begin(\u0026#34;patternName\u0026#34;, skipStrategy) For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no events mapped to the PatternName. By default a NO_SKIP strategy will be used in this case. The other option is to throw exception in such situation. One can enable this option by: Java AfterMatchSkipStrategy.skipToFirst(patternName).throwExceptionOnMiss(); Scala AfterMatchSkipStrategy.skipToFirst(patternName).throwExceptionOnMiss() Detecting Patterns # After specifying the pattern sequence you are looking for, it is time to apply it to your input stream to detect potential matches. To run a stream of events against your pattern sequence, you have to create a PatternStream. Given an input stream input, a pattern pattern and an optional comparator comparator used to sort events with the same timestamp in case of EventTime or that arrived at the same moment, you create the PatternStream by calling:
Java DataStream\u0026lt;Event\u0026gt; input = ...; Pattern\u0026lt;Event, ?\u0026gt; pattern = ...; EventComparator\u0026lt;Event\u0026gt; comparator = ...; // optional PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern, comparator); Scala val input : DataStream[Event] = ... val pattern : Pattern[Event, _] = ... var comparator : EventComparator[Event] = ... // optional val patternStream: PatternStream[Event] = CEP.pattern(input, pattern, comparator) The input stream can be keyed or non-keyed depending on your use-case.
Applying your pattern on a non-keyed stream will result in a job with parallelism equal to 1. Selecting from Patterns # Once you have obtained a PatternStream you can apply transformation to detected event sequences. The suggested way of doing that is by PatternProcessFunction.
A PatternProcessFunction has a processMatch method which is called for each matching event sequence. It receives a match in the form of Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; where the key is the name of each pattern in your pattern sequence and the value is a list of all accepted events for that pattern (IN is the type of your input elements). The events for a given pattern are ordered by timestamp. The reason for returning a list of accepted events for each pattern is that when using looping patterns (e.g. oneToMany() and times()), more than one event may be accepted for a given pattern.
class MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); IN endEvent = match.get(\u0026#34;end\u0026#34;).get(0); out.collect(OUT(startEvent, endEvent)); } } The PatternProcessFunction gives access to a Context object. Thanks to it, one can access time related characteristics such as currentProcessingTime or timestamp of current match (which is the timestamp of the last element assigned to the match). For more info see Time context. Through this context one can also emit results to a side-output.
Handling Timed Out Partial Patterns # Whenever a pattern has a window length attached via the within keyword, it is possible that partial event sequences are discarded because they exceed the window length. To act upon a timed out partial match one can use TimedOutPartialMatchHandler interface. The interface is supposed to be used in a mixin style. This mean you can additionally implement this interface with your PatternProcessFunction. The TimedOutPartialMatchHandler provides the additional processTimedOutMatch method which will be called for every timed out partial match.
class MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; implements TimedOutPartialMatchHandler\u0026lt;IN\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; ... } @Override public void processTimedOutMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); ctx.output(outputTag, T(startEvent)); } } Note The processTimedOutMatch does not give one access to the main output. You can still emit results through side-outputs though, through the Context object.
Convenience API # The aforementioned PatternProcessFunction was introduced in Flink 1.8 and since then it is the recommended way to interact with matches. One can still use the old style API like select/flatSelect, which internally will be translated into a PatternProcessFunction.
Java PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;ComplexEvent\u0026gt; flatResult = patternStream.flatSelect( outputTag, new PatternFlatTimeoutFunction\u0026lt;Event, TimeoutEvent\u0026gt;() { public void timeout( Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern, long timeoutTimestamp, Collector\u0026lt;TimeoutEvent\u0026gt; out) throws Exception { out.collect(new TimeoutEvent()); } }, new PatternFlatSelectFunction\u0026lt;Event, ComplexEvent\u0026gt;() { public void flatSelect(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; pattern, Collector\u0026lt;OUT\u0026gt; out) throws Exception { out.collect(new ComplexEvent()); } } ); DataStream\u0026lt;TimeoutEvent\u0026gt; timeoutFlatResult = flatResult.getSideOutput(outputTag); Scala val patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream.flatSelect(outputTag){ (pattern: Map[String, Iterable[Event]], timestamp: Long, out: Collector[TimeoutEvent]) =\u0026gt; out.collect(TimeoutEvent()) } { (pattern: mutable.Map[String, Iterable[Event]], out: Collector[ComplexEvent]) =\u0026gt; out.collect(ComplexEvent()) } val timeoutResult: DataStream[TimeoutEvent] = result.getSideOutput(outputTag) Time in CEP library # Handling Lateness in Event Time # In CEP the order in which elements are processed matters. To guarantee that elements are processed in the correct order when working in event time, an incoming element is initially put in a buffer where elements are sorted in ascending order based on their timestamp, and when a watermark arrives, all the elements in this buffer with timestamps smaller than that of the watermark are processed. This implies that elements between watermarks are processed in event-time order.
The library assumes correctness of the watermark when working in event time. To guarantee that elements across watermarks are processed in event-time order, Flink\u0026rsquo;s CEP library assumes correctness of the watermark, and considers as late elements whose timestamp is smaller than that of the last seen watermark. Late elements are not further processed. Also, you can specify a sideOutput tag to collect the late elements come after the last seen watermark, you can use it like this.
Java PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); OutputTag\u0026lt;String\u0026gt; lateDataOutputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;late-data\u0026#34;){}; SingleOutputStreamOperator\u0026lt;ComplexEvent\u0026gt; result = patternStream .sideOutputLateData(lateDataOutputTag) .select( new PatternSelectFunction\u0026lt;Event, ComplexEvent\u0026gt;() {...} ); DataStream\u0026lt;String\u0026gt; lateData = result.getSideOutput(lateDataOutputTag); Scala val patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val lateDataOutputTag = OutputTag[String](\u0026#34;late-data\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream .sideOutputLateData(lateDataOutputTag) .select{ pattern: Map[String, Iterable[ComplexEvent]] =\u0026gt; ComplexEvent() } val lateData: DataStream[String] = result.getSideOutput(lateDataOutputTag) Time context # In PatternProcessFunction as well as in IterativeCondition user has access to a context that implements TimeContext as follows:
/** * Enables access to time related characteristics such as current processing time or timestamp of * currently processed element. Used in {@link PatternProcessFunction} and * {@link org.apache.flink.cep.pattern.conditions.IterativeCondition} */ @PublicEvolving public interface TimeContext { /** * Timestamp of the element currently being processed. * * \u0026lt;p\u0026gt;In case of {@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime} this * will be set to the time when event entered the cep operator. */ long timestamp(); /** Returns the current processing time. */ long currentProcessingTime(); } This context gives user access to time characteristics of processed events (incoming records in case of IterativeCondition and matches in case of PatternProcessFunction). Call to TimeContext#currentProcessingTime always gives you the value of current processing time and this call should be preferred to e.g. calling System.currentTimeMillis().
In case of TimeContext#timestamp() the returned value is equal to assigned timestamp in case of EventTime. In ProcessingTime this will equal to the point of time when said event entered cep operator (or when the match was generated in case of PatternProcessFunction). This means that the value will be consistent across multiple calls to that method.
Optional Configuration # Options to configure the cache capacity of Flink CEP SharedBuffer. It could accelerate the CEP operate process speed and limit the number of elements of cache in pure memory.
Note It\u0026rsquo;s only effective to limit usage of memory when state.backend was set as rocksdb, which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage. The configuration items are helpful for memory limitation when the state.backend is set as rocksdb. By contrast，when the state.backend is set as not rocksdb, the cache would cause performance decreased. Compared with old cache implemented with Map, the state part will contain more elements swapped out from new guava-cache, which would make it heavier to copy on write for state.
Key Default Type Description pipeline.cep.sharedbuffer.cache.entry-slots 1024 Integer The Config option to set the maximum element number the entryCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed with state.And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage. pipeline.cep.sharedbuffer.cache.event-slots 1024 Integer The Config option to set the maximum element number the eventsBufferCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage. pipeline.cep.sharedbuffer.cache.statistics-interval 30 min Duration The interval to log the information of cache state statistics in CEP operator. Examples # The following example detects the pattern start, middle(name = \u0026quot;error\u0026quot;) -\u0026gt; end(name = \u0026quot;critical\u0026quot;) on a keyed data stream of Events. The events are keyed by their ids and a valid pattern has to occur within 10 seconds. The whole processing is done with event time.
Java StreamExecutionEnvironment env = ...; DataStream\u0026lt;Event\u0026gt; input = ...; DataStream\u0026lt;Event\u0026gt; partitionedInput = input.keyBy(new KeySelector\u0026lt;Event, Integer\u0026gt;() { @Override public Integer getKey(Event value) throws Exception { return value.getId(); } }); Pattern\u0026lt;Event, ?\u0026gt; pattern = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;error\u0026#34;); } }).followedBy(\u0026#34;end\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;critical\u0026#34;); } }).within(Time.seconds(10)); PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(partitionedInput, pattern); DataStream\u0026lt;Alert\u0026gt; alerts = patternStream.select(new PatternSelectFunction\u0026lt;Event, Alert\u0026gt;() { @Override public Alert select(Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern) throws Exception { return createAlert(pattern); } }); Scala val env : StreamExecutionEnvironment = ... val input : DataStream[Event] = ... val partitionedInput = input.keyBy(event =\u0026gt; event.getId) val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(_.getName == \u0026#34;error\u0026#34;) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;critical\u0026#34;) .within(Time.seconds(10)) val patternStream = CEP.pattern(partitionedInput, pattern) val alerts = patternStream.select(createAlert(_)) Migrating from an older Flink version(pre 1.5) # Migrating from Flink \u0026lt;= 1.5 # In Flink 1.13 we dropped direct savepoint backward compatibility with Flink \u0026lt;= 1.5. If you want to restore from a savepoint taken from an older version, migrate it first to a newer version (1.6-1.12), take a savepoint and then use that savepoint to restore with Flink \u0026gt;= 1.13.
Back to top
`}),e.add({id:3,href:"/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/",title:"Execution Configuration",section:"Managing Execution",content:` Execution Configuration # The StreamExecutionEnvironment contains the ExecutionConfig which allows to set job specific configuration values for the runtime. To change the defaults that affect all jobs, see Configuration.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ExecutionConfig executionConfig = env.getConfig(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment var executionConfig = env.getConfig Python env = StreamExecutionEnvironment.get_execution_environment() execution_config = env.get_config() The following configuration options are available: (the default is bold)
setClosureCleanerLevel(). The closure cleaner level is set to ClosureCleanerLevel.RECURSIVE by default. The closure cleaner removes unneeded references to the surrounding class of anonymous functions inside Flink programs. With the closure cleaner disabled, it might happen that an anonymous user function is referencing the surrounding class, which is usually not Serializable. This will lead to exceptions by the serializer. The settings are: NONE: disable the closure cleaner completely, TOP_LEVEL: clean only the top-level class without recursing into fields, RECURSIVE: clean all the fields recursively.
getParallelism() / setParallelism(int parallelism) Set the default parallelism for the job.
getMaxParallelism() / setMaxParallelism(int parallelism) Set the default maximum parallelism for the job. This setting determines the maximum degree of parallelism and specifies the upper limit for dynamic scaling.
getNumberOfExecutionRetries() / setNumberOfExecutionRetries(int numberOfExecutionRetries) Sets the number of times that failed tasks are re-executed. A value of zero effectively disables fault tolerance. A value of -1 indicates that the system default value (as defined in the configuration) should be used. This is deprecated, use restart strategies instead.
getExecutionRetryDelay() / setExecutionRetryDelay(long executionRetryDelay) Sets the delay in milliseconds that the system waits after a job has failed, before re-executing it. The delay starts after all tasks have been successfully stopped on the TaskManagers, and once the delay is past, the tasks are re-started. This parameter is useful to delay re-execution in order to let certain time-out related failures surface fully (like broken connections that have not fully timed out), before attempting a re-execution and immediately failing again due to the same problem. This parameter only has an effect if the number of execution re-tries is one or more. This is deprecated, use restart strategies instead.
getExecutionMode() / setExecutionMode(). The default execution mode is PIPELINED. Sets the execution mode to execute the program. The execution mode defines whether data exchanges are performed in a batch or on a pipelined manner.
enableForceKryo() / disableForceKryo. Kryo is not forced by default. Forces the GenericTypeInformation to use the Kryo serializer for POJOs even though we could analyze them as a POJO. In some cases this might be preferable. For example, when Flink\u0026rsquo;s internal serializers fail to handle a POJO properly.
enableForceAvro() / disableForceAvro(). Avro is not forced by default. Forces the Flink AvroTypeInfo to use the Avro serializer instead of Kryo for serializing Avro POJOs.
enableObjectReuse() / disableObjectReuse() By default, objects are not reused in Flink. Enabling the object reuse mode will instruct the runtime to reuse user objects for better performance. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behavior.
getGlobalJobParameters() / setGlobalJobParameters() This method allows users to set custom objects as a global configuration for the job. Since the ExecutionConfig is accessible in all user defined functions, this is an easy method for making configuration globally available in a job.
addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Serializer\u0026lt;?\u0026gt; serializer) Register a Kryo serializer instance for the given type.
addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Class\u0026lt;? extends Serializer\u0026lt;?\u0026gt;\u0026gt; serializerClass) Register a Kryo serializer class for the given type.
registerTypeWithKryoSerializer(Class\u0026lt;?\u0026gt; type, Serializer\u0026lt;?\u0026gt; serializer) Register the given type with Kryo and specify a serializer for it. By registering a type with Kryo, the serialization of the type will be much more efficient.
registerKryoType(Class\u0026lt;?\u0026gt; type) If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags (integer IDs) are written. If a type is not registered with Kryo, its entire class-name will be serialized with every instance, leading to much higher I/O costs.
registerPojoType(Class\u0026lt;?\u0026gt; type) Registers the given type with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written. If a type is not registered with Kryo, its entire class-name will be serialized with every instance, leading to much higher I/O costs.
Note that types registered with registerKryoType() are not available to Flink\u0026rsquo;s POJO serializer instance.
disableAutoTypeRegistration() Automatic type registration is enabled by default. The automatic type registration is registering all types (including sub-types) used by usercode with Kryo and the POJO serializer.
setTaskCancellationInterval(long interval) Sets the interval (in milliseconds) to wait between consecutive attempts to cancel a running task. When a task is canceled a new thread is created which periodically calls interrupt() on the task thread, if the task thread does not terminate within a certain time. This parameter refers to the time between consecutive calls to interrupt() and is set by default to 30000 milliseconds, or 30 seconds.
The RuntimeContext which is accessible in Rich* functions through the getRuntimeContext() method also allows to access the ExecutionConfig in all user defined functions.
Back to top
`}),e.add({id:4,href:"/flink/flink-docs-master/docs/try-flink/local_installation/",title:"First steps",section:"Try Flink",content:` First steps # Welcome to Flink! :)
Flink is designed to process continuous streams of data at a lightning fast pace. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink job and view it in the web UI.
Downloading Flink # Note: Flink is also available as a Docker image. Flink runs on all UNIX-like environments, i.e. Linux, Mac OS X, and Cygwin (for Windows). You need to have Java 11 installed. To check the Java version installed, type in your terminal:
\$ java -version Next, download the latest binary release of Flink, then extract the archive:
\$ tar -xzf flink-*.tgz Browsing the project directory # Navigate to the extracted directory and list the contents by issuing:
\$ cd flink-* \u0026amp;\u0026amp; ls -l You should see something like:
For now, you may want to note that:
bin/ directory contains the flink binary as well as several bash scripts that manage various jobs and tasks conf/ directory contains configuration files, including flink-conf.yaml examples/ directory contains sample applications that can be used as is with Flink Starting and stopping a local cluster # To start a local cluster, run the bash script that comes with Flink:
\$ ./bin/start-cluster.sh You should see an output like this:
Flink is now running as a background process. You can check its status with the following command:
\$ ps aux | grep flink You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.
To quickly stop the cluster and all running components, you can use the provided script:
\$ ./bin/stop-cluster.sh Submitting a Flink job # Flink provides a CLI tool, bin/flink, that can run programs packaged as Java ARchives (JAR) and control their execution. Submitting a job means uploading the job’s JAR ﬁle and related dependencies to the running Flink cluster and executing it.
Flink releases come with example jobs, which you can ﬁnd in the examples/ folder.
To deploy the example word count job to the running cluster, issue the following command:
\$ ./bin/flink run examples/streaming/WordCount.jar You can verify the output by viewing the logs:
\$ tail log/flink-*-taskexecutor-*.out Sample output:
(nymph,1) (in,3) (thy,1) (orisons,1) (be,4) (all,2) (my,1) (sins,1) (remember,1) (d,4) Additionally, you can check Flink\u0026rsquo;s web UI to monitor the status of the cluster and running job.
You can view the data flow plan for the execution:
Here for the job execution, Flink has two operators. The ﬁrst is the source operator which reads data from the collection source. The second operator is the transformation operator which aggregates counts of words. Learn more about DataStream operators.
You can also look at the timeline of the job execution:
You have successfully ran a Flink application! Feel free to select any other JAR archive from the examples/ folder or deploy your own job!
Summary # In this guide, you downloaded Flink, explored the project directory, started and stopped a local cluster, and submitted a sample Flink job!
To learn more about Flink fundamentals, check out the concepts section. If you want to try something more hands-on, try one of the tutorials.
`}),e.add({id:5,href:"/flink/flink-docs-master/docs/connectors/table/formats/overview/",title:"Formats",section:"Formats",content:` Formats # Flink provides a set of table formats that can be used with table connectors. A table format is a storage format defines how to map binary data onto table columns.
Flink supports the following formats:
Formats Supported Connectors CSV Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem JSON Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem, Elasticsearch Apache Avro Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem Confluent Avro Apache Kafka, Upsert Kafka Debezium CDC Apache Kafka, Filesystem Canal CDC Apache Kafka, Filesystem Maxwell CDC Apache Kafka, Filesystem OGG CDC Apache Kafka, Filesystem Apache Parquet Filesystem Apache ORC Filesystem Raw Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem `}),e.add({id:6,href:"/flink/flink-docs-master/docs/dev/python/datastream/intro_to_datastream_api/",title:"Intro to the Python DataStream API",section:"DataStream API",content:` Intro to the Python DataStream API # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal).
Python DataStream API is a Python version of DataStream API which allows Python users could write Python DatStream API jobs.
Common Structure of Python DataStream API Programs # The following code example shows the common structure of Python DataStream API programs.
from pyflink.common import WatermarkStrategy, Row from pyflink.common.serialization import Encoder from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig from pyflink.datastream.connectors.number_seq import NumberSequenceSource from pyflink.datastream.functions import RuntimeContext, MapFunction from pyflink.datastream.state import ValueStateDescriptor class MyMapFunction(MapFunction): def open(self, runtime_context: RuntimeContext): state_desc = ValueStateDescriptor(\u0026#39;cnt\u0026#39;, Types.PICKLED_BYTE_ARRAY()) self.cnt_state = runtime_context.get_state(state_desc) def map(self, value): cnt = self.cnt_state.value() if cnt is None or cnt \u0026lt; 2: self.cnt_state.update(1 if cnt is None else cnt + 1) return value[0], value[1] + 1 else: return value[0], value[1] def state_access_demo(): # 1. create a StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() # 2. create source DataStream seq_num_source = NumberSequenceSource(1, 10000) ds = env.from_source( source=seq_num_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#39;seq_num_source\u0026#39;, type_info=Types.LONG()) # 3. define the execution logic ds = ds.map(lambda a: Row(a % 4, 1), output_type=Types.ROW([Types.LONG(), Types.LONG()])) \\ .key_by(lambda a: a[0]) \\ .map(MyMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.LONG()])) # 4. create sink and emit result to sink output_path = \u0026#39;/opt/output/\u0026#39; file_sink = FileSink \\ .for_row_format(output_path, Encoder.simple_string_encoder()) \\ .with_output_file_config(OutputFileConfig.builder().with_part_prefix(\u0026#39;pre\u0026#39;).with_part_suffix(\u0026#39;suf\u0026#39;).build()) \\ .build() ds.sink_to(file_sink) # 5. execute the job env.execute(\u0026#39;state_access_demo\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: state_access_demo() Back to top
Create a StreamExecutionEnvironment # The StreamExecutionEnvironment is a central concept of the DataStream API program. The following code example shows how to create a StreamExecutionEnvironment:
from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() Back to top
Create a DataStream # The DataStream API gets its name from the special DataStream class that is used to represent a collection of data in a Flink program. You can think of them as immutable collections of data that can contain duplicates. This data can either be finite or unbounded, the API that you use to work on them is the same.
A DataStream is similar to a regular Python Collection in terms of usage but is quite different in some key ways. They are immutable, meaning that once they are created you cannot add or remove elements. You can also not simply inspect the elements inside but only work on them using the DataStream API operations, which are also called transformations.
You can create an initial DataStream by adding a source in a Flink program. Then you can derive new streams from this and combine them by using API methods such as map, filter, and so on.
Create from a list object # You can create a DataStream from a list object:
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() ds = env.from_collection( collection=[(1, \u0026#39;aaa|bb\u0026#39;), (2, \u0026#39;bb|a\u0026#39;), (3, \u0026#39;aaa|a\u0026#39;)], type_info=Types.ROW([Types.INT(), Types.STRING()])) The parameter type_info is optional, if not specified, the output type of the returned DataStream will be Types.PICKLED_BYTE_ARRAY().
Create using DataStream connectors # You can also create a DataStream using DataStream connectors with method add_source as following:
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer from pyflink.datastream.formats.json import JsonRowDeserializationSchema env = StreamExecutionEnvironment.get_execution_environment() # the sql connector for kafka is used here as it\u0026#39;s a fat jar and could avoid dependency issues env.add_jars(\u0026#34;file:///path/to/flink-sql-connector-kafka.jar\u0026#34;) deserialization_schema = JsonRowDeserializationSchema.builder() \\ .type_info(type_info=Types.ROW([Types.INT(), Types.STRING()])).build() kafka_consumer = FlinkKafkaConsumer( topics=\u0026#39;test_source_topic\u0026#39;, deserialization_schema=deserialization_schema, properties={\u0026#39;bootstrap.servers\u0026#39;: \u0026#39;localhost:9092\u0026#39;, \u0026#39;group.id\u0026#39;: \u0026#39;test_group\u0026#39;}) ds = env.add_source(kafka_consumer) Note It currently only supports FlinkKafkaConsumer to be used as DataStream source connectors with method add_source.
Note The DataStream created using add_source could only be executed in streaming executing mode.
You could also call the from_source method to create a DataStream using unified DataStream source connectors:
from pyflink.common.typeinfo import Types from pyflink.common.watermark_strategy import WatermarkStrategy from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.number_seq import NumberSequenceSource env = StreamExecutionEnvironment.get_execution_environment() seq_num_source = NumberSequenceSource(1, 1000) ds = env.from_source( source=seq_num_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#39;seq_num_source\u0026#39;, type_info=Types.LONG()) Note Currently, it only supports NumberSequenceSource and FileSource as unified DataStream source connectors.
Note The DataStream created using from_source could be executed in both batch and streaming executing mode.
Create using Table \u0026amp; SQL connectors # Table \u0026amp; SQL connectors could also be used to create a DataStream. You could firstly create a Table using Table \u0026amp; SQL connectors and then convert it to a DataStream.
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;number-of-rows\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) ds = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.INT(), Types.STRING()])) Note The StreamExecutionEnvironment env should be specified when creating the TableEnvironment t_env.
Note As all the Java Table \u0026amp; SQL connectors could be used in PyFlink Table API, this means that all of them could also be used in PyFlink DataStream API.
Back to top
DataStream Transformations # Operators transform one or more DataStream into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
The following example shows a simple example about how to convert a DataStream into another DataStream using map transformation:
ds = ds.map(lambda a: a + 1) Please see operators for an overview of the available DataStream transformations.
Conversion between DataStream and Table # It also supports to convert a DataStream to a Table and vice verse.
# convert a DataStream to a Table table = t_env.from_data_stream(ds, \u0026#39;a, b, c\u0026#39;) # convert a Table to a DataStream ds = t_env.to_append_stream(table, Types.ROW([Types.INT(), Types.STRING()])) # or ds = t_env.to_retract_stream(table, Types.ROW([Types.INT(), Types.STRING()])) Back to top
Emit Results # Print # You can call the print method to print the data of a DataStream to the standard output:
ds.print() Collect results to client # You can call the execute_and_collect method to collect the data of a DataStream to client:
with ds.execute_and_collect() as results: for result in results: print(result) Note The method execute_and_collect will collect the data of the DataStream to the memory of the client and so it\u0026rsquo;s a good practice to limit the number of rows collected.
Emit results to a DataStream sink connector # You can call the add_sink method to emit the data of a DataStream to a DataStream sink connector:
from pyflink.common.typeinfo import Types from pyflink.datastream.connectors.kafka import FlinkKafkaProducer from pyflink.datastream.formats.json import JsonRowSerializationSchema serialization_schema = JsonRowSerializationSchema.builder().with_type_info( type_info=Types.ROW([Types.INT(), Types.STRING()])).build() kafka_producer = FlinkKafkaProducer( topic=\u0026#39;test_sink_topic\u0026#39;, serialization_schema=serialization_schema, producer_config={\u0026#39;bootstrap.servers\u0026#39;: \u0026#39;localhost:9092\u0026#39;, \u0026#39;group.id\u0026#39;: \u0026#39;test_group\u0026#39;}) ds.add_sink(kafka_producer) Note It currently only supports FlinkKafkaProducer and JdbcSink to be used as DataStream sink connectors with method add_sink.
Note The method add_sink could only be used in streaming executing mode.
You could also call the sink_to method to emit the data of a DataStream to a unified DataStream sink connector:
from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig from pyflink.common.serialization import Encoder output_path = \u0026#39;/opt/output/\u0026#39; file_sink = FileSink \\ .for_row_format(output_path, Encoder.simple_string_encoder()) \\ .with_output_file_config(OutputFileConfig.builder().with_part_prefix(\u0026#39;pre\u0026#39;).with_part_suffix(\u0026#39;suf\u0026#39;).build()) \\ .build() ds.sink_to(file_sink) Note It currently only supports FileSink as unified DataStream sink connectors.
Note The method sink_to could be used in both batch and streaming executing mode.
Emit results to a Table \u0026amp; SQL sink connector # Table \u0026amp; SQL connectors could also be used to write out a DataStream. You need firstly convert a DataStream to a Table and then write it to a Table \u0026amp; SQL sink connector.
from pyflink.common import Row from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) # option 1：the result type of ds is Types.ROW def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield Row(s[0], sp) ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.ROW([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: Row(i[0] + j[0], i[1])) # option 1：the result type of ds is Types.TUPLE def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.TUPLE([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: (i[0] + j[0], i[1])) # emit ds to print sink t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_sink ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = t_env.from_data_stream(ds) table_result = table.execute_insert(\u0026#34;my_sink\u0026#34;) Note The output type of DataStream ds must be composite type.
Submit Job # Finally, you should call the StreamExecutionEnvironment.execute method to submit the DataStream API job for execution:
env.execute() If you convert the DataStream to a Table and then write it to a Table API \u0026amp; SQL sink connector, it may happen that you need to submit the job using TableEnvironment.execute method.
t_env.execute() `}),e.add({id:7,href:"/flink/flink-docs-master/docs/concepts/overview/",title:"Overview",section:"Concepts",content:` Concepts # The Hands-on Training explains the basic concepts of stateful and timely stream processing that underlie Flink\u0026rsquo;s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of Data Pipelines \u0026amp; ETL and is further developed in the section on Fault Tolerance. Timely stream processing is introduced in the section on Streaming Analytics.
This Concepts in Depth section provides a deeper understanding of how Flink\u0026rsquo;s architecture and runtime implement these concepts.
Flink\u0026rsquo;s APIs # Flink offers different levels of abstraction for developing streaming/batch applications.
The lowest level abstraction simply offers stateful and timely stream processing. It is embedded into the DataStream API via the Process Function. It allows users to freely process events from one or more streams, and provides consistent, fault tolerant state. In addition, users can register event time and processing time callbacks, allowing programs to realize sophisticated computations.
In practice, many applications do not need the low-level abstractions described above, and can instead program against the Core APIs: the DataStream API (bounded/unbounded streams) and the DataSet API (bounded data sets). These fluent APIs offer the common building blocks for data processing, like various forms of user-specified transformations, joins, aggregations, windows, state, etc. Data types processed in these APIs are represented as classes in the respective programming languages.
The low level Process Function integrates with the DataStream API, making it possible to use the lower-level abstraction on an as-needed basis. The DataSet API offers additional primitives on bounded data sets, like loops/iterations.
The Table API is a declarative DSL centered around tables, which may be dynamically changing tables (when representing streams). The Table API follows the (extended) relational model: Tables have a schema attached (similar to tables in relational databases) and the API offers comparable operations, such as select, project, join, group-by, aggregate, etc. Table API programs declaratively define what logical operation should be done rather than specifying exactly how the code for the operation looks. Though the Table API is extensible by various types of user-defined functions, it is less expressive than the Core APIs, and more concise to use (less code to write). In addition, Table API programs also go through an optimizer that applies optimization rules before execution.
One can seamlessly convert between tables and DataStream/DataSet, allowing programs to mix the Table API with the DataStream and DataSet APIs.
The highest level abstraction offered by Flink is SQL. This abstraction is similar to the Table API both in semantics and expressiveness, but represents programs as SQL query expressions. The SQL abstraction closely interacts with the Table API, and SQL queries can be executed over tables defined in the Table API.
`}),e.add({id:8,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/overview/",title:"Overview",section:"Formats",content:` DataStream Formats # Available Formats # Formats define how information is encoded for storage. Currently these formats are supported:
Avro Azure Table Hadoop Parquet Text files Back to top
`}),e.add({id:9,href:"/flink/flink-docs-master/docs/connectors/datastream/overview/",title:"Overview",section:"DataStream Connectors",content:` DataStream Connectors # Predefined Sources and Sinks # A few basic data sources and sinks are built into Flink and are always available. The predefined data sources include reading from files, directories, and sockets, and ingesting data from collections and iterators. The predefined data sinks support writing to files, to stdout and stderr, and to sockets.
Bundled Connectors # Connectors provide code for interfacing with various third-party systems. Currently these systems are supported:
Apache Kafka (source/sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) FileSystem (sink) RabbitMQ (source/sink) Google PubSub (source/sink) Hybrid Source (source) Apache Pulsar (source) JDBC (sink) Keep in mind that to use one of these connectors in an application, additional third party components are usually required, e.g. servers for the data stores or message queues. Note also that while the streaming connectors listed in this section are part of the Flink project and are included in source releases, they are not included in the binary distributions. Further instructions can be found in the corresponding subsections.
Filesystem source formats are gradually replaced with new Flink Source API starting with Flink 1.14.0.
Connectors in Apache Bahir # Additional streaming connectors for Flink are being released through Apache Bahir, including:
Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) Other Ways to Connect to Flink # Data Enrichment via Async I/O # Using a connector isn\u0026rsquo;t the only way to get data in and out of Flink. One common pattern is to query an external database or web service in a Map or FlatMap in order to enrich the primary datastream. Flink offers an API for Asynchronous I/O to make it easier to do this kind of enrichment efficiently and robustly.
Queryable State # When a Flink application pushes a lot of data to an external data store, this can become an I/O bottleneck. If the data involved has many fewer reads than writes, a better approach can be for an external application to pull from Flink the data it needs. The Queryable State interface enables this by allowing the state being managed by Flink to be queried on demand.
Back to top
`}),e.add({id:10,href:"/flink/flink-docs-master/docs/connectors/table/hive/overview/",title:"Overview",section:"Hive",content:` Apache Hive # Apache Hive has established itself as a focal point of the data warehousing ecosystem. It serves as not only a SQL engine for big data analytics and ETL, but also a data management platform, where data is discovered, defined, and evolved.
Flink offers a two-fold integration with Hive.
The first is to leverage Hive\u0026rsquo;s Metastore as a persistent catalog with Flink\u0026rsquo;s HiveCatalog for storing Flink specific metadata across sessions. For example, users can store their Kafka or ElasticSearch tables in Hive Metastore by using HiveCatalog, and reuse them later on in SQL queries.
The second is to offer Flink as an alternative engine for reading and writing Hive tables.
The HiveCatalog is designed to be “out of the box” compatible with existing Hive installations. You do not need to modify your existing Hive Metastore or change the data placement or partitioning of your tables.
Supported Hive Versions # Flink supports the following Hive versions.
1.0 1.0.0 1.0.1 1.1 1.1.0 1.1.1 1.2 1.2.0 1.2.1 1.2.2 2.0 2.0.0 2.0.1 2.1 2.1.0 2.1.1 2.2 2.2.0 2.3 2.3.0 2.3.1 2.3.2 2.3.3 2.3.4 2.3.5 2.3.6 2.3.7 2.3.8 2.3.9 3.1 3.1.0 3.1.1 3.1.2 3.1.3 Please note Hive itself have different features available for different versions, and these issues are not caused by Flink:
Hive built-in functions are supported in 1.2.0 and later. Column constraints, i.e. PRIMARY KEY and NOT NULL, are supported in 3.1.0 and later. Altering table statistics is supported in 1.2.0 and later. DATE column statistics are supported in 1.2.0 and later. Writing to ORC tables is not supported in 2.0.x. Dependencies # To integrate with Hive, you need to add some extra dependencies to the /lib/ directory in Flink distribution to make the integration work in Table API program or SQL in SQL Client. Alternatively, you can put these dependencies in a dedicated folder, and add them to classpath with the -C or -l option for Table API program or SQL Client respectively.
Apache Hive is built on Hadoop, so you need to provide Hadoop dependencies, by setting the HADOOP_CLASSPATH environment variable:
export HADOOP_CLASSPATH=\`hadoop classpath\` There are two ways to add Hive dependencies. First is to use Flink\u0026rsquo;s bundled Hive jars. You can choose a bundled Hive jar according to the version of the metastore you use. Second is to add each of the required jars separately. The second way can be useful if the Hive version you\u0026rsquo;re using is not listed here.
NOTE: the recommended way to add dependency is to use a bundled jar. Separate jars should be used only if bundled jars don\u0026rsquo;t meet your needs.
Using bundled hive jar # The following tables list all available bundled hive jars. You can pick one to the /lib/ directory in Flink distribution.
Metastore version Maven dependency SQL Client JAR 2.3.0 - 2.3.9 flink-sql-connector-hive-2.3.9 Only available for stable releases 3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Only available for stable releases User defined dependencies # Please find the required dependencies for different Hive major versions below.
Hive 2.3.4 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector.Contains flink-hadoop-compatibility and flink-orc jars flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.3.4.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.0.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.0.0.jar hive-exec-1.0.0.jar libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.1.0.jar hive-exec-1.1.0.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.2.1 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.2.1.jar hive-exec-1.2.1.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.0.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.0.0.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.1.0.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.2.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.2.0.jar // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 3.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-3.1.0.jar libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Program maven # If you are building your own program, you need the following dependencies in your mvn file. It\u0026rsquo;s recommended not to include these dependencies in the resulting jar file. You\u0026rsquo;re supposed to add dependencies as stated above at runtime.
\u0026lt;!-- Flink Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Hive Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\${hive.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Connecting To Hive # Connect to an existing Hive installation using the catalog interface and HiveCatalog through the table environment or YAML configuration.
Following is an example of how to connect to Hive:
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name = \u0026#34;myhive\u0026#34;; String defaultDatabase = \u0026#34;mydatabase\u0026#34;; String hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34;; HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir); tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive); // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;); Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;myhive\u0026#34; val defaultDatabase = \u0026#34;mydatabase\u0026#34; val hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34; val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir) tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive) // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;) Python from pyflink.table import * from pyflink.table.catalog import HiveCatalog settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) catalog_name = \u0026#34;myhive\u0026#34; default_database = \u0026#34;mydatabase\u0026#34; hive_conf_dir = \u0026#34;/opt/hive-conf\u0026#34; hive_catalog = HiveCatalog(catalog_name, default_database, hive_conf_dir) t_env.register_catalog(\u0026#34;myhive\u0026#34;, hive_catalog) # set the HiveCatalog as the current catalog of the session tableEnv.use_catalog(\u0026#34;myhive\u0026#34;) YAML execution: ... current-catalog: myhive # set the HiveCatalog as the current catalog of the session current-database: mydatabase catalogs: - name: myhive type: hive hive-conf-dir: /opt/hive-conf SQL CREATE CATALOG myhive WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;default-database\u0026#39; = \u0026#39;mydatabase\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39; ); -- set the HiveCatalog as the current catalog of the session USE CATALOG myhive; Below are the options supported when creating a HiveCatalog instance with YAML file or DDL.
Option Required Default Type Description type Yes (none) String Type of the catalog. Must be set to 'hive' when creating a HiveCatalog. name Yes (none) String The unique name of the catalog. Only applicable to YAML file. hive-conf-dir No (none) String URI to your Hive conf dir containing hive-site.xml. The URI needs to be supported by Hadoop FileSystem. If the URI is relative, i.e. without a scheme, local file system is assumed. If the option is not specified, hive-site.xml is searched in class path. default-database No default String The default database to use when the catalog is set as the current catalog. hive-version No (none) String HiveCatalog is capable of automatically detecting the Hive version in use. It's recommended NOT to specify the Hive version, unless the automatic detection fails. hadoop-conf-dir No (none) String Path to Hadoop conf dir. Only local file system paths are supported. The recommended way to set Hadoop conf is via the HADOOP_CONF_DIR environment variable. Use the option only if environment variable doesn't work for you, e.g. if you want to configure each HiveCatalog separately. DDL # It\u0026rsquo;s recommended to use Hive dialect to execute DDLs to create Hive tables, views, partitions, functions within Flink.
DML # Flink supports DML writing to Hive tables. Please refer to details in Reading \u0026amp; Writing Hive Tables
`}),e.add({id:11,href:"/flink/flink-docs-master/docs/connectors/table/overview/",title:"Overview",section:"Table API Connectors",content:` Table \u0026amp; SQL Connectors # Flink\u0026rsquo;s Table API \u0026amp; SQL programs can be connected to other external systems for reading and writing both batch and streaming tables. A table source provides access to data which is stored in external systems (such as a database, key-value store, message queue, or file system). A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, Avro, Parquet, or ORC.
This page describes how to register table sources and table sinks in Flink using the natively supported connectors. After a source or sink has been registered, it can be accessed by Table API \u0026amp; SQL statements.
If you want to implement your own custom table source or sink, have a look at the user-defined sources \u0026amp; sinks page.
Supported Connectors # Flink natively support various connectors. The following tables list all available connectors.
Name Version Source Sink Filesystem Bounded and Unbounded Scan, Lookup Streaming Sink, Batch Sink Elasticsearch 6.x \u0026 7.x Not supported Streaming Sink, Batch Sink Apache Kafka 0.10+ Unbounded Scan Streaming Sink, Batch Sink Amazon Kinesis Data Streams Unbounded Scan Streaming Sink JDBC Bounded Scan, Lookup Streaming Sink, Batch Sink Apache HBase 1.4.x \u0026 2.2.x Bounded Scan, Lookup Streaming Sink, Batch Sink Apache Hive Supported Versions Unbounded Scan, Bounded Scan, Lookup Streaming Sink, Batch Sink Back to top
Please refer to the configuration section on how to add connectors as a dependency.
How to use connectors # Flink supports using SQL CREATE TABLE statements to register tables. One can define the table name, the table schema, and the table options for connecting to an external system.
See the SQL section for more information about creating a table.
The following code shows a full example of how to connect to Kafka for reading and writing JSON records.
SQL CREATE TABLE MyUserTable ( -- declare the schema of the table \`user\` BIGINT, \`message\` STRING, \`rowtime\` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, -- use a metadata column to access Kafka\u0026#39;s record timestamp \`proctime\` AS PROCTIME(), -- use a computed column to define a proctime attribute WATERMARK FOR \`rowtime\` AS \`rowtime\` - INTERVAL \u0026#39;5\u0026#39; SECOND -- use a WATERMARK statement to define a rowtime attribute ) WITH ( -- declare the external system to connect to \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;topic_name\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; -- declare a format for this system ) The desired connection properties are converted into string-based key-value pairs. Factories will create configured table sources, table sinks, and corresponding formats from the key-value pairs based on factory identifiers (kafka and json in this example). All factories that can be found via Java\u0026rsquo;s Service Provider Interfaces (SPI) are taken into account when searching for exactly one matching factory for each component.
If no factory can be found or multiple factories match for the given properties, an exception will be thrown with additional information about considered factories and supported properties.
Transform table connector/format resources # Flink uses Java\u0026rsquo;s Service Provider Interfaces (SPI) to load the table connector/format factories by their identifiers. Since the SPI resource file named org.apache.flink.table.factories.Factory for every table connector/format is under the same directory META-INF/services, these resource files will override each other when build the uber-jar of the project which uses more than one table connector/format, which will cause Flink to fail to load table connector/format factories.
In this situation, the recommended way is transforming these resource files under the directory META-INF/services by ServicesResourceTransformer of maven shade plugin. Given the pom.xml file content of example that contains connector flink-sql-connector-hive-3.1.2 and format flink-parquet in a project.
\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;myProject\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- other project dependencies ...--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-sql-connector-hive-3.1.2_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-parquet_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;shade\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformers combine.children=\u0026#34;append\u0026#34;\u0026gt; \u0026lt;!-- The service transformer is needed to merge META-INF/services files --\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; After configured the ServicesResourceTransformer, the table connector/format resource files under the directory META-INF/services would be merged rather than overwritten each other when build the uber-jar of above project.
Back to top
Schema Mapping # The body clause of a SQL CREATE TABLE statement defines the names and types of physical columns, constraints and watermarks. Flink doesn\u0026rsquo;t hold the data, thus the schema definition only declares how to map physical columns from an external system to Flink’s representation. The mapping may not be mapped by names, it depends on the implementation of formats and connectors. For example, a MySQL database table is mapped by field names (not case sensitive), and a CSV filesystem is mapped by field order (field names can be arbitrary). This will be explained in every connector.
The following example shows a simple schema without time attributes and one-to-one field mapping of input/output to table columns.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN ) WITH ( ... ) Metadata # Some connectors and formats expose additional metadata fields that can be accessed in metadata columns next to the physical payload columns. See the CREATE TABLE section for more information about metadata columns.
Primary Key # Primary key constraints tell that a column or a set of columns of a table are unique and they do not contain nulls. Primary key uniquely identifies a row in a table.
The primary key of a source table is a metadata information for optimization. The primary key of a sink table is usually used by the sink implementation for upserting.
SQL standard specifies that a constraint can either be ENFORCED or NOT ENFORCED. This controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data the only mode we want to support is the NOT ENFORCED mode. Its up to the user to ensure that the query enforces key integrity.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN, PRIMARY KEY (MyField1, MyField2) NOT ENFORCED -- defines a primary key on columns ) WITH ( ... ) Time Attributes # Time attributes are essential when working with unbounded streaming tables. Therefore both proctime and rowtime attributes can be defined as part of the schema.
For more information about time handling in Flink and especially event-time, we recommend the general event-time section.
Proctime Attributes # In order to declare a proctime attribute in the schema, you can use Computed Column syntax to declare a computed column which is generated from PROCTIME() builtin function. The computed column is a virtual column which is not stored in the physical data.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN, MyField4 AS PROCTIME() -- declares a proctime attribute ) WITH ( ... ) Rowtime Attributes # In order to control the event-time behavior for tables, Flink provides predefined timestamp extractors and watermark strategies.
Please refer to CREATE TABLE statements for more information about defining time attributes in DDL.
The following timestamp extractors are supported:
DDL -- use the existing TIMESTAMP(3) field in schema as the rowtime attribute CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ... ) WITH ( ... ) -- use system functions or UDFs or expressions to extract the expected TIMESTAMP(3) rowtime field CREATE TABLE MyTable ( log_ts STRING, ts_field AS TO_TIMESTAMP(log_ts), WATERMARK FOR ts_field AS ... ) WITH ( ... ) The following watermark strategies are supported:
DDL -- Sets a watermark strategy for strictly ascending rowtime attributes. Emits a watermark of the -- maximum observed timestamp so far. Rows that have a timestamp bigger to the max timestamp -- are not late. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field ) WITH ( ... ) -- Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum -- observed timestamp so far minus 1. Rows that have a timestamp bigger or equal to the max timestamp -- are not late. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field - INTERVAL \u0026#39;0.001\u0026#39; SECOND ) WITH ( ... ) -- Sets a watermark strategy for rowtime attributes which are out-of-order by a bounded time interval. -- Emits watermarks which are the maximum observed timestamp minus the specified delay, e.g. 2 seconds. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field - INTERVAL \u0026#39;2\u0026#39; SECOND ) WITH ( ... ) Make sure to always declare both timestamps and watermarks. Watermarks are required for triggering time-based operations.
SQL Types # Please see the Data Types page about how to declare a type in SQL.
Back to top
`}),e.add({id:12,href:"/flink/flink-docs-master/docs/deployment/filesystems/overview/",title:"Overview",section:"File Systems",content:` File Systems # Apache Flink uses file systems to consume and persistently store data, both for the results of applications and for fault tolerance and recovery. These are some of most of the popular file systems, including local, hadoop-compatible, Amazon S3, Aliyun OSS and Azure Blob Storage.
The file system used for a particular file is determined by its URI scheme. For example, file:///home/user/text.txt refers to a file in the local file system, while hdfs://namenode:50010/data/user/text.txt is a file in a specific HDFS cluster.
File system instances are instantiated once per process and then cached/pooled, to avoid configuration overhead per stream creation and to enforce certain constraints, such as connection/stream limits.
Local File System # Flink has built-in support for the file system of the local machine, including any NFS or SAN drives mounted into that local file system. It can be used by default without additional configuration. Local files are referenced with the file:// URI scheme.
Pluggable File Systems # The Apache Flink project supports the following file systems:
Amazon S3 object storage is supported by two alternative implementations: flink-s3-fs-presto and flink-s3-fs-hadoop. Both implementations are self-contained with no dependency footprint.
Aliyun Object Storage Service is supported by flink-oss-fs-hadoop and registered under the oss:// URI scheme. The implementation is based on the Hadoop Project but is self-contained with no dependency footprint.
Azure Data Lake Store Gen2 is supported by flink-azure-fs-hadoop and registered under the abfs(s):// URI schemes. The implementation is based on the Hadoop Project but is self-contained with no dependency footprint.
Azure Blob Storage is supported by flink-azure-fs-hadoop and registered under the wasb(s):// URI schemes. The implementation is based on the Hadoop Project but is self-contained with no dependency footprint.
Google Cloud Storage is supported by gcs-connector and registered under the gs:// URI scheme. The implementation is based on the Hadoop Project but is self-contained with no dependency footprint.
You can and should use any of them as plugins.
To use a pluggable file systems, copy the corresponding JAR file from the opt directory to a directory under plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/s3-fs-hadoop cp ./opt/flink-s3-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/s3-fs-hadoop/ Attention The plugin mechanism for file systems was introduced in Flink version 1.9 to support dedicated Java class loaders per plugin and to move away from the class shading mechanism. You can still use the provided file systems (or your own implementations) via the old mechanism by copying the corresponding JAR file into lib directory. However, since 1.10, s3 plugins must be loaded through the plugin mechanism; the old way no longer works as these plugins are not shaded anymore (or more specifically the classes are not relocated since 1.10).
It\u0026rsquo;s encouraged to use the plugins-based loading mechanism for file systems that support it. Loading file systems components from the lib directory will not supported in future Flink versions.
Adding a new pluggable File System implementation # File systems are represented via the org.apache.flink.core.fs.FileSystem class, which captures the ways to access and modify files and objects in that file system.
To add a new file system:
Add the File System implementation, which is a subclass of org.apache.flink.core.fs.FileSystem. Add a factory that instantiates that file system and declares the scheme under which the FileSystem is registered. This must be a subclass of org.apache.flink.core.fs.FileSystemFactory. Add a service entry. Create a file META-INF/services/org.apache.flink.core.fs.FileSystemFactory which contains the class name of your file system factory class (see the Java Service Loader docs for more details). During plugins discovery, the file system factory class will be loaded by a dedicated Java class loader to avoid class conflicts with other plugins and Flink components. The same class loader should be used during file system instantiation and the file system operation calls.
In practice, it means you should avoid using Thread.currentThread().getContextClassLoader() class loader in your implementation. Hadoop File System (HDFS) and its other implementations # For all schemes where Flink cannot find a directly supported file system, it falls back to Hadoop. All Hadoop file systems are automatically available when flink-runtime and the Hadoop libraries are on the classpath.
This way, Flink seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).
HDFS (tested) Alluxio (tested, see configuration specifics below) XtreemFS (tested) FTP via Hftp (not tested) HAR (not tested) \u0026hellip; The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file. See example for Alluxio.
We recommend using Flink\u0026rsquo;s built-in file systems unless required otherwise. Using a Hadoop File System directly may be required, for example, when using that file system for YARN\u0026rsquo;s resource storage, via the fs.defaultFS configuration property in Hadoop\u0026rsquo;s core-site.xml.
Alluxio # For Alluxio support add the following entry into the core-site.xml file:
\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Back to top
`}),e.add({id:13,href:"/flink/flink-docs-master/docs/deployment/ha/overview/",title:"Overview",section:"High Availability",content:` High Availability # JobManager High Availability (HA) hardens a Flink cluster against JobManager failures. This feature ensures that a Flink cluster will always continue executing your submitted jobs.
JobManager High Availability # The JobManager coordinates every Flink deployment. It is responsible for both scheduling and resource management.
By default, there is a single JobManager instance per Flink cluster. This creates a single point of failure (SPOF): if the JobManager crashes, no new programs can be submitted and running programs fail.
With JobManager High Availability, you can recover from JobManager failures and thereby eliminate the SPOF. You can configure high availability for every cluster deployment. See the list of available high availability services for more information.
How to make a cluster highly available # The general idea of JobManager High Availability is that there is a single leading JobManager at any time and multiple standby JobManagers to take over leadership in case the leader fails. This guarantees that there is no single point of failure and programs can make progress as soon as a standby JobManager has taken leadership.
As an example, consider the following setup with three JobManager instances:
Flink\u0026rsquo;s high availability services encapsulate the required services to make everything work:
Leader election: Selecting a single leader out of a pool of n candidates Service discovery: Retrieving the address of the current leader State persistence: Persisting state which is required for the successor to resume the job execution (JobGraphs, user code jars, completed checkpoints) Back to top
High Availability Services # Flink ships with two high availability service implementations:
ZooKeeper: ZooKeeper HA services can be used with every Flink cluster deployment. They require a running ZooKeeper quorum.
Kubernetes: Kubernetes HA services only work when running on Kubernetes.
Back to top
High Availability data lifecycle # In order to recover submitted jobs, Flink persists metadata and the job artifacts. The HA data will be kept until the respective job either succeeds, is cancelled or fails terminally. Once this happens, all the HA data, including the metadata stored in the HA services, will be deleted.
Back to top
JobResultStore # The JobResultStore is used to archive the final result of a job that reached a globally-terminal state (i.e. finished, cancelled or failed). The data is stored on a file system (see job-result-store.storage-path). Entries in this store are marked as dirty as long as the corresponding job wasn\u0026rsquo;t cleaned up properly (artifacts are found in the job\u0026rsquo;s subfolder in high-availability.storageDir).
Dirty entries are subject to cleanup, i.e. the corresponding job is either cleaned up by Flink at the moment or will be picked up for cleanup as part of a recovery. The entries will be deleted as soon as the cleanup succeeds. Check the JobResultStore configuration parameters under HA configuration options for further details on how to adapt the behavior.
Back to top
`}),e.add({id:14,href:"/flink/flink-docs-master/docs/deployment/overview/",title:"Overview",section:"Deployment",content:` Deployment # Flink is a versatile framework, supporting many different deployment scenarios in a mix and match fashion.
Below, we briefly explain the building blocks of a Flink cluster, their purpose and available implementations. If you just want to start Flink locally, we recommend setting up a Standalone Cluster.
Overview and Reference Architecture # The figure below shows the building blocks of every Flink cluster. There is always somewhere a client running. It takes the code of the Flink applications, transforms it into a JobGraph and submits it to the JobManager.
The JobManager distributes the work onto the TaskManagers, where the actual operators (such as sources, transformations and sinks) are running.
When deploying Flink, there are often multiple options available for each building block. We have listed them in the table below the figure.
Component Purpose Implementations Flink Client Compiles batch or streaming applications into a dataflow graph, which it then submits to the JobManager. Command Line Interface REST Endpoint SQL Client Python REPL JobManager JobManager is the name of the central work coordination component of Flink. It has implementations for different resource providers, which differ on high-availability, resource allocation behavior and supported job submission modes. JobManager modes for job submissions: Application Mode: runs the cluster exclusively for one application. The job's main method (or client) gets executed on the JobManager. Calling \`execute\`/\`executeAsync\` multiple times in an application is supported. Per-Job Mode: runs the cluster exclusively for one job. The job's main method (or client) runs only prior to the cluster creation. Session Mode: one JobManager instance manages multiple jobs sharing the same cluster of TaskManagers Standalone (this is the barebone mode that requires just JVMs to be launched. Deployment with Docker, Docker Swarm / Compose, non-native Kubernetes and other models is possible through manual setup in this mode) Kubernetes YARN TaskManager TaskManagers are the services actually performing the work of a Flink job. External Components (all optional) High Availability Service Provider Flink's JobManager can be run in high availability mode which allows Flink to recover from JobManager faults. In order to failover faster, multiple standby JobManagers can be started to act as backups. Zookeeper Kubernetes HA File Storage and Persistency For checkpointing (recovery mechanism for streaming jobs) Flink relies on external file storage systems See FileSystems page. Resource Provider Flink can be deployed through different Resource Provider Frameworks, such as Kubernetes or YARN. See JobManager implementations above. Metrics Storage Flink components report internal metrics and Flink jobs can report additional, job specific metrics as well. See Metrics Reporter page. Application-level data sources and sinks While application-level data sources and sinks are not technically part of the deployment of Flink cluster components, they should be considered when planning a new Flink production deployment. Colocating frequently used data with Flink can have significant performance benefits For example: Apache Kafka Amazon S3 ElasticSearch Apache Cassandra See Connectors page. Repeatable Resource Cleanup # Once a job has reached a globally terminal state of either finished, failed or cancelled, the external component resources associated with the job are then cleaned up. In the event of a failure when cleaning up a resource, Flink will attempt to retry the cleanup. You can configure the retry strategy used. Reaching the maximum number of retries without succeeding will leave the job in a dirty state. Its artifacts would need to be cleaned up manually (see the High Availability Services / JobResultStore section for further details). Restarting the very same job (i.e. using the same job ID) will result in the cleanup being restarted without running the job again.
There is currently an issue with the cleanup of CompletedCheckpoints that failed to be deleted while subsuming them as part of the usual CompletedCheckpoint management. These artifacts are not covered by the repeatable cleanup, i.e. they have to be deleted manually, still. This is covered by FLINK-26606.
Deployment Modes # Flink can execute applications in one of three ways:
in Application Mode, in Session Mode, in a Per-Job Mode (deprecated). The above modes differ in:
the cluster lifecycle and resource isolation guarantees whether the application\u0026rsquo;s main() method is executed on the client or on the cluster. Application Mode # In all the other modes, the application\u0026rsquo;s main() method is executed on the client side. This process includes downloading the application\u0026rsquo;s dependencies locally, executing the main() to extract a representation of the application that Flink\u0026rsquo;s runtime can understand (i.e. the JobGraph) and ship the dependencies and the JobGraph(s) to the cluster. This makes the Client a heavy resource consumer as it may need substantial network bandwidth to download dependencies and ship binaries to the cluster, and CPU cycles to execute the main(). This problem can be more pronounced when the Client is shared across users.
Building on this observation, the Application Mode creates a cluster per submitted application, but this time, the main() method of the application is executed by the JobManager. Creating a cluster per application can be seen as creating a session cluster shared only among the jobs of a particular application, and turning down when the application finishes. With this architecture, the Application Mode provides the same resource isolation and load balancing guarantees as the Per-Job mode, but at the granularity of a whole application.
The Application Mode builds on an assumption that the user jars are already available on the classpath (usrlib folder) of all Flink components that needs access to it (JobManager, TaskManager). In other words, your application comes bundled with the Flink distribution. This allows the application mode to speed up the deployment / recovery process, by not having to distribute the user jars to the Flink components via RPC as the other deployment modes do.
The application mode assumes that the user jars are bundled with the Flink distribution.
Executing the main() method on the cluster may have other implications for your code, such as any paths you register in your environment using the registerCachedFile() must be accessible by the JobManager of your application.
Compared to the Per-Job (deprecated) mode, the Application Mode allows the submission of applications consisting of multiple jobs. The order of job execution is not affected by the deployment mode but by the call used to launch the job. Using execute(), which is blocking, establishes an order and it will lead to the execution of the \u0026ldquo;next\u0026rdquo; job being postponed until \u0026ldquo;this\u0026rdquo; job finishes. Using executeAsync(), which is non-blocking, will lead to the \u0026ldquo;next\u0026rdquo; job starting before \u0026ldquo;this\u0026rdquo; job finishes.
The Application Mode allows for multi-execute() applications but High-Availability is not supported in these cases. High-Availability in Application Mode is only supported for single-execute() applications.
Additionally, when any of multiple running jobs in Application Mode (submitted for example using executeAsync()) gets cancelled, all jobs will be stopped and the JobManager will shut down. Regular job completions (by the sources shutting down) are supported.
Session Mode # Session mode assumes an already running cluster and uses the resources of that cluster to execute any submitted application. Applications executed in the same (session) cluster use, and consequently compete for, the same resources. This has the advantage that you do not pay the resource overhead of spinning up a full cluster for every submitted job. But, if one of the jobs misbehaves or brings down a TaskManager, then all jobs running on that TaskManager will be affected by the failure. This, apart from a negative impact on the job that caused the failure, implies a potential massive recovery process with all the restarting jobs accessing the filesystem concurrently and making it unavailable to other services. Additionally, having a single cluster running multiple jobs implies more load for the JobManager, who is responsible for the book-keeping of all the jobs in the cluster.
Per-Job Mode (deprecated) # Per-job mode is only supported by YARN and has been deprecated in Flink 1.15. It will be dropped in FLINK-26000. Please consider application mode to launch a dedicated cluster per-job on YARN. Aiming at providing better resource isolation guarantees, the Per-Job mode uses the available resource provider framework (e.g. YARN) to spin up a cluster for each submitted job. This cluster is available to that job only. When the job finishes, the cluster is torn down and any lingering resources (files, etc) are cleared up. This provides better resource isolation, as a misbehaving job can only bring down its own TaskManagers. In addition, it spreads the load of book-keeping across multiple JobManagers, as there is one per job.
Summary # In Session Mode, the cluster lifecycle is independent of that of any job running on the cluster and the resources are shared across all jobs. Application Mode creates a session cluster per application and executes the application\u0026rsquo;s main() method on the cluster. It thus comes with better resource isolation as the resources are only used by the job(s) launched from a single main() method. This comes at the price of spining up a dedicated cluster for each application.
Vendor Solutions # A number of vendors offer managed or fully hosted Flink solutions. None of these vendors are officially supported or endorsed by the Apache Flink PMC. Please refer to vendor maintained documentation on how to use these products.
AliCloud Realtime Compute # Website
Supported Environments: AliCloud
Amazon EMR # Website
Supported Environments: AWS
Amazon Kinesis Data Analytics for Apache Flink # Website
Supported Environments: AWS
Cloudera DataFlow # Website
Supported Environment: AWS Azure Google On-Premise
Eventador # Website
Supported Environment: AWS
Huawei Cloud Stream Service # Website
Supported Environment: Huawei
Ververica Platform # Website
Supported Environments: AliCloud AWS Azure Google On-Premise
Back to top
`}),e.add({id:15,href:"/flink/flink-docs-master/docs/dev/configuration/overview/",title:"Overview",section:"Project Configuration",content:` Project Configuration # The guides in this section will show you how to configure your projects via popular build tools (Maven, Gradle), add the necessary dependencies (i.e. connectors and formats, testing), and cover some advanced configuration topics.
Every Flink application depends on a set of Flink libraries. At a minimum, the application depends on the Flink APIs and, in addition, on certain connector libraries (i.e. Kafka, Cassandra) and 3rd party dependencies required to the user to develop custom functions to process the data.
Getting started # To get started working on your Flink application, use the following commands, scripts, and templates to create a Flink project.
Maven You can create a project based on an Archetype with the Maven command below or use the provided quickstart bash script.
Maven command # \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.16-SNAPSHOT This allows you to name your newly created project and will interactively ask you for the groupId, artifactId, and package name.
Quickstart script # \$ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.16-SNAPSHOT Gradle You can create an empty project, where you are required to create the src/main/java and src/main/resources directories manually and start writing some class(es) in that, with the use of the following Gradle build script or instead use the provided quickstart bash script to get a completely functional startup project.
Gradle build script # To execute these build configuration scripts, run the gradle command in the directory with these scripts.
build.gradle
plugins { id \u0026#39;java\u0026#39; id \u0026#39;application\u0026#39; // shadow plugin to produce fat JARs id \u0026#39;com.github.johnrengelman.shadow\u0026#39; version \u0026#39;7.1.2\u0026#39; } // artifact properties group = \u0026#39;org.quickstart\u0026#39; version = \u0026#39;0.1-SNAPSHOT\u0026#39; mainClassName = \u0026#39;org.quickstart.DataStreamJob\u0026#39; description = \u0026#34;\u0026#34;\u0026#34;Flink Quickstart Job\u0026#34;\u0026#34;\u0026#34; ext { javaVersion = \u0026#39;1.8\u0026#39; flinkVersion = \u0026#39;1.16-SNAPSHOT\u0026#39; scalaBinaryVersion = \u0026#39;_2.12\u0026#39; slf4jVersion = \u0026#39;1.7.32\u0026#39; log4jVersion = \u0026#39;2.17.1\u0026#39; } sourceCompatibility = javaVersion targetCompatibility = javaVersion tasks.withType(JavaCompile) { options.encoding = \u0026#39;UTF-8\u0026#39; } applicationDefaultJvmArgs = [\u0026#34;-Dlog4j.configurationFile=log4j2.properties\u0026#34;] // declare where to find the dependencies of your project repositories { mavenCentral() maven { url \u0026#34;https://repository.apache.org/content/repositories/snapshots\u0026#34; mavenContent { snapshotsOnly() } } } // NOTE: We cannot use \u0026#34;compileOnly\u0026#34; or \u0026#34;shadow\u0026#34; configurations since then we could not run code // in the IDE or with \u0026#34;gradle run\u0026#34;. We also cannot exclude transitive dependencies from the // shadowJar yet (see https://github.com/johnrengelman/shadow/issues/159). // -\u0026gt; Explicitly define the // libraries we want to be included in the \u0026#34;flinkShadowJar\u0026#34; configuration! configurations { flinkShadowJar // dependencies which go into the shadowJar // always exclude these (also from transitive dependencies) since they are provided by Flink flinkShadowJar.exclude group: \u0026#39;org.apache.flink\u0026#39;, module: \u0026#39;force-shading\u0026#39; flinkShadowJar.exclude group: \u0026#39;com.google.code.findbugs\u0026#39;, module: \u0026#39;jsr305\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.slf4j\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.apache.logging.log4j\u0026#39; } // declare the dependencies for your production and test code dependencies { // -------------------------------------------------------------- // Compile-time dependencies that should NOT be part of the // shadow (uber) jar and are provided in the lib folder of Flink // -------------------------------------------------------------- implementation \u0026#34;org.apache.flink:flink-streaming-java:\${flinkVersion}\u0026#34; implementation \u0026#34;org.apache.flink:flink-clients:\${flinkVersion}\u0026#34; // -------------------------------------------------------------- // Dependencies that should be part of the shadow jar, e.g. // connectors. These must be in the flinkShadowJar configuration! // -------------------------------------------------------------- //flinkShadowJar \u0026#34;org.apache.flink:flink-connector-kafka:\${flinkVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-slf4j-impl:\${log4jVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-api:\${log4jVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-core:\${log4jVersion}\u0026#34; // Add test dependencies here. // testCompile \u0026#34;junit:junit:4.12\u0026#34; } // make compileOnly dependencies available for tests: sourceSets { main.compileClasspath += configurations.flinkShadowJar main.runtimeClasspath += configurations.flinkShadowJar test.compileClasspath += configurations.flinkShadowJar test.runtimeClasspath += configurations.flinkShadowJar javadoc.classpath += configurations.flinkShadowJar } run.classpath = sourceSets.main.runtimeClasspath jar { manifest { attributes \u0026#39;Built-By\u0026#39;: System.getProperty(\u0026#39;user.name\u0026#39;), \u0026#39;Build-Jdk\u0026#39;: System.getProperty(\u0026#39;java.version\u0026#39;) } } shadowJar { configurations = [project.configurations.flinkShadowJar] } settings.gradle
rootProject.name = \u0026#39;quickstart\u0026#39; Quickstart script # bash -c \u0026#34;\$(curl https://flink.apache.org/q/gradle-quickstart.sh)\u0026#34; -- 1.16-SNAPSHOT _2.12 Which dependencies do you need? # To start working on a Flink job, you usually need the following dependencies:
Flink APIs, in order to develop your job Connectors and formats, in order to integrate your job with external systems Testing utilities, in order to test your job And in addition to these, you might want to add 3rd party dependencies that you need to develop custom functions.
Flink APIs # Flink offers two major APIs: Datastream API and Table API \u0026amp; SQL. They can be used separately, or they can be mixed, depending on your use cases:
APIs you want to use Dependency you need to add DataStream flink-streaming-java DataStream with Scala flink-streaming-scala_2.12 Table API flink-table-api-java Table API with Scala flink-table-api-scala_2.12 Table API + DataStream flink-table-api-java-bridge Table API + DataStream with Scala flink-table-api-scala-bridge_2.12 Just include them in your build tool script/descriptor, and you can start developing your job!
Running and packaging # If you want to run your job by simply executing the main class, you will need flink-runtime in your classpath. In case of Table API programs, you will also need flink-table-runtime and flink-table-planner-loader.
As a rule of thumb, we suggest packaging the application code and all its required dependencies into one fat/uber JAR. This includes packaging connectors, formats, and third-party dependencies of your job. This rule does not apply to Java APIs, DataStream Scala APIs, and the aforementioned runtime modules, which are already provided by Flink itself and should not be included in a job uber JAR. This job JAR can be submitted to an already running Flink cluster, or added to a Flink application container image easily without modifying the distribution.
What\u0026rsquo;s next? # To start developing your job, check out DataStream API and Table API \u0026amp; SQL. For more details on how to package your job depending on the build tools, check out the following specific guides: Maven Gradle For more advanced topics about project configuration, check out the section on advanced topics. `}),e.add({id:16,href:"/flink/flink-docs-master/docs/dev/dataset/overview/",title:"Overview",section:"DataSet API (Legacy)",content:` DataSet API # DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.
Please refer to the DataStream API overview for an introduction to the basic concepts of the Flink API. That overview is for the DataStream API but the basic concepts of the two APIs are the same.
In order to create your own Flink DataSet program, we encourage you to start with the anatomy of a Flink Program and gradually add your own transformations. The remaining sections act as references for additional operations and advanced features.
Starting with Flink 1.12 the DataSet API has been soft deprecated.
We recommend that you use the Table API and SQL to run efficient batch pipelines in a fully unified API. Table API is well integrated with common batch connectors and catalogs.
Alternatively, you can also use the DataStream API with BATCH execution mode. The linked section also outlines cases where it makes sense to use the DataSet API but those cases will become rarer as development progresses and the DataSet API will eventually be removed. Please also see FLIP-131 for background information on this decision.
Example Program # The following program is a complete, working example of WordCount. You can copy \u0026amp; paste the code to run it locally. You only have to include the correct Flink’s library into your project and specify the imports. Then you are ready to go!
Java public class WordCountExample { public static void main(String[] args) throws Exception { final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;String\u0026gt; text = env.fromElements( \u0026#34;Who\u0026#39;s there?\u0026#34;, \u0026#34;I think I hear them. Stand, ho! Who\u0026#39;s there?\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = text .flatMap(new LineSplitter()) .groupBy(0) .sum(1); wordCounts.print(); } public static class LineSplitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String line, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { for (String word : line.split(\u0026#34; \u0026#34;)) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(word, 1)); } } } } Scala import org.apache.flink.api.scala._ object WordCount { def main(args: Array[String]) { val env = ExecutionEnvironment.getExecutionEnvironment val text = env.fromElements( \u0026#34;Who\u0026#39;s there?\u0026#34;, \u0026#34;I think I hear them. Stand, ho! Who\u0026#39;s there?\u0026#34;) val counts = text .flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.print() } } DataSet Transformations # Data transformations transform one or more DataSets into a new DataSet. Programs can combine multiple transformations into sophisticated assemblies.
Map # Takes one element and produces one element.
Java data.map(new MapFunction\u0026lt;String, Integer\u0026gt;() { public Integer map(String value) { return Integer.parseInt(value); } }); Scala data.map { x =\u0026gt; x.toInt } FlatMap # Takes one element and produces zero, one, or more elements.
Java data.flatMap(new FlatMapFunction\u0026lt;String, String\u0026gt;() { public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) { for (String s : value.split(\u0026#34; \u0026#34;)) { out.collect(s); } } }); Scala data.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) } MapPartition # Transforms a parallel partition in a single function call. The function gets the partition as an Iterable stream and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.
Java data.mapPartition(new MapPartitionFunction\u0026lt;String, Long\u0026gt;() { public void mapPartition(Iterable\u0026lt;String\u0026gt; values, Collector\u0026lt;Long\u0026gt; out) { long c = 0; for (String s : values) { c++; } out.collect(c); } }); Scala data.mapPartition { in =\u0026gt; in map { (_, 1) } } Filter # Evaluates a boolean function for each element and retains those for which the function returns true. IMPORTANT: The system assumes that the function does not modify the element on which the predicate is applied. Violating this assumption can lead to incorrect results.
Java data.filter(new FilterFunction\u0026lt;Integer\u0026gt;() { public boolean filter(Integer value) { return value \u0026gt; 1000; } }); Scala data.filter { _ \u0026gt; 1000 } Reduce # Combines a group of elements into a single element by repeatedly combining two elements into one. Reduce may be applied on a full data set or on a grouped data set.
Java data.reduce(new ReduceFunction\u0026lt;Integer\u0026gt; { public Integer reduce(Integer a, Integer b) { return a + b; } }); Scala data.reduce { _ + _ } If the reduce was applied to a grouped data set then you can specify the way that the runtime executes the combine phase of the reduce by supplying a CombineHint to setCombineHint. The hash-based strategy should be faster in most cases, especially if the number of different keys is small compared to the number of input elements (eg. 1/10).
ReduceGroup # Combines a group of elements into one or more elements. ReduceGroup may be applied on a full data set, or on a grouped data set.
Java data.reduceGroup(new GroupReduceFunction\u0026lt;Integer, Integer\u0026gt; { public void reduce(Iterable\u0026lt;Integer\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) { int prefixSum = 0; for (Integer i : values) { prefixSum += i; out.collect(prefixSum); } } }); Scala data.reduceGroup { elements =\u0026gt; elements.sum } Aggregate # Aggregates a group of values into a single value. Aggregation functions can be thought of as built-in reduce functions. Aggregate may be applied on a full data set, or on a grouped data set.
Java Dataset\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input.aggregate(SUM, 0).and(MIN, 2); Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input.aggregate(SUM, 0).aggregate(MIN, 2) Distinct # Returns the distinct elements of a data set. It removes the duplicate entries from the input DataSet, with respect to all fields of the elements, or a subset of fields.
Java data.distinct(); Scala data.distinct() Join # Joins two data sets by creating all pairs of elements that are equal on their keys. Optionally uses a JoinFunction to turn the pair of elements into a single element, or a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.
Java result = input1.join(input2) .where(0) // key of the first input (tuple field 0) .equalTo(1); // key of the second input (tuple field 1) Scala // In this case tuple fields are used as keys. \u0026#34;0\u0026#34; is the join field on the first tuple // \u0026#34;1\u0026#34; is the join field on the second tuple. val result = input1.join(input2).where(0).equalTo(1) You can specify the way that the runtime executes the join via Join Hints. The hints describe whether the join happens through partitioning or broadcasting, and whether it uses a sort-based or a hash-based algorithm. Please refer to the Transformations Guide for a list of possible hints and an example. If no hint is specified, the system will try to make an estimate of the input sizes and pick the best strategy according to those estimates.
Java // This executes a join by broadcasting the first data set // using a hash table for the broadcast data result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1); Scala // This executes a join by broadcasting the first data set // using a hash table for the broadcast data val result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1) Note that the join transformation works only for equi-joins. Other join types need to be expressed using OuterJoin or CoGroup.
OuterJoin # Performs a left, right, or full outer join on two data sets. Outer joins are similar to regular (inner) joins and create all pairs of elements that are equal on their keys. In addition, records of the \u0026ldquo;outer\u0026rdquo; side (left, right, or both in case of full) are preserved if no matching key is found in the other side. Matching pairs of elements (or one element and a null value for the other input) are given to a JoinFunction to turn the pair of elements into a single element, or to a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.
Java input1.leftOuterJoin(input2) // rightOuterJoin or fullOuterJoin for right or full outer joins .where(0) // key of the first input (tuple field 0) .equalTo(1) // key of the second input (tuple field 1) .with(new JoinFunction\u0026lt;String, String, String\u0026gt;() { public String join(String v1, String v2) { // NOTE: // - v2 might be null for leftOuterJoin // - v1 might be null for rightOuterJoin // - v1 OR v2 might be null for fullOuterJoin } }); Scala val joined = left.leftOuterJoin(right).where(0).equalTo(1) { (left, right) =\u0026gt; val a = if (left == null) \u0026#34;none\u0026#34; else left._1 (a, right) } CoGroup # The two-dimensional variant of the reduce operation. Groups each input on one or more fields and then joins the groups. The transformation function is called per pair of groups. See the keys section to learn how to define coGroup keys.
Java data1.coGroup(data2) .where(0) .equalTo(1) .with(new CoGroupFunction\u0026lt;String, String, String\u0026gt;() { public void coGroup(Iterable\u0026lt;String\u0026gt; in1, Iterable\u0026lt;String\u0026gt; in2, Collector\u0026lt;String\u0026gt; out) { out.collect(...); } }); Scala data1.coGroup(data2).where(0).equalTo(1) Cross # Builds the Cartesian product (cross product) of two inputs, creating all pairs of elements. Optionally uses a CrossFunction to turn the pair of elements into a single element
Java DataSet\u0026lt;Integer\u0026gt; data1 = // [...] DataSet\u0026lt;String\u0026gt; data2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; result = data1.cross(data2); Scala val data1: DataSet[Int] = // [...] val data2: DataSet[String] = // [...] val result: DataSet[(Int, String)] = data1.cross(data2) Cross is potentially a very compute-intensive operation which can challenge even large compute clusters! It is advised to hint the system with the DataSet sizes by using crossWithTiny() and crossWithHuge(). Union # Produces the union of two data sets.
Java data.union(data2); Scala data.union(data2) Rebalance # Evenly rebalances the parallel partitions of a data set to eliminate data skew. Only Map-like transformations may follow a rebalance transformation.
Java DataSet\u0026lt;Int\u0026gt; data1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Int, String\u0026gt;\u0026gt; result = data1.rebalance().map(...); Scala val data1: DataSet[Int] = // [...] val result: DataSet[(Int, String)] = data1.rebalance().map(...) Hash-Partition # Hash-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.partitionByHash(0).mapPartition { ... } Range-Partition # Range-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.partitionByRange(0).mapPartition { ... } Custom Partitioning # Assigns records based on a key to a specific partition using a custom Partitioner function. The key can be specified as position key, expression key, and key selector function. Note: This method only works with a single field key.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionCustom(partitioner, key) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in .partitionCustom(partitioner, key).mapPartition { ... } Sort Partitioning # Locally sorts all partitions of a data set on a specified field in a specified order. Fields can be specified as tuple positions or field expressions. Sorting on multiple fields is done by chaining sortPartition() calls.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.sortPartition(1, Order.ASCENDING) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.sortPartition(1, Order.ASCENDING).mapPartition { ... } First-N # Returns the first n (arbitrary) elements of a data set. First-n can be applied on a regular data set, a grouped data set, or a grouped-sorted data set. Grouping keys can be specified as key-selector functions or field position keys.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] // regular data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result1 = in.first(3); // grouped data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result2 = in.groupBy(0) .first(3); // grouped-sorted data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); Scala val in: DataSet[(Int, String)] = // [...] // regular data set val result1 = in.first(3) // grouped data set val result2 = in.groupBy(0).first(3) // grouped-sorted data set val result3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) Project # Selects a subset of fields from tuples.
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); Scala This feature is not available in the Scala API MinBy / MaxBy # Selects a tuple from a group of tuples whose values of one or more fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) field values, an arbitrary tuple of these tuples is returned. MinBy (MaxBy) may be applied on a full data set or a grouped data set.
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] // a DataSet with a single tuple with minimum values for the Integer and String fields. DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; out = in.minBy(0, 2); // a DataSet with one tuple for each group with the minimum value for the Double field. DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; out2 = in.groupBy(2) .minBy(1); Scala val in: DataSet[(Int, Double, String)] = // [...] // a data set with a single tuple with minimum values for the Int and String fields. val out: DataSet[(Int, Double, String)] = in.minBy(0, 2) // a data set with one tuple for each group with the minimum value for the Double field. val out2: DataSet[(Int, Double, String)] = in.groupBy(2) .minBy(1) Specifying Keys # Some transformations (join, coGroup, groupBy) require that a key be defined on a collection of elements. Other transformations (Reduce, GroupReduce, Aggregate) allow data being grouped on a key before they are applied.
A DataSet is grouped as
DataSet\u0026lt;...\u0026gt; input = // [...] DataSet\u0026lt;...\u0026gt; reduced = input .groupBy(/*define key here*/) .reduceGroup(/*do something*/); The data model of Flink is not based on key-value pairs. Therefore, you do not need to physically pack the data set types into keys and values. Keys are “virtual”: they are defined as functions over the actual data to guide the grouping operator.
Define keys for Tuples # The simplest case is grouping Tuples on one or more fields of the Tuple:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;\u0026gt; input = // [...] UnsortedGrouping\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;,Tuple\u0026gt; keyed = input.groupBy(0); Scala val input: DataSet[(Int, String, Long)] = // [...] val keyed = input.groupBy(0) Tuples are grouped on the first field (the one of Integer type).
Java DataSet\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;\u0026gt; input = // [...] UnsortedGrouping\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;,Tuple\u0026gt; keyed = input.groupBy(0,1); Scala val input: DataSet[(Int, String, Long)] = // [...] val grouped = input.groupBy(0,1) Here, we group the tuples on a composite key consisting of the first and the second field.
A note on nested Tuples: If you have a DataSet with a nested tuple, such as:
DataSet\u0026lt;Tuple3\u0026lt;Tuple2\u0026lt;Integer, Float\u0026gt;,String,Long\u0026gt;\u0026gt; ds; Specifying groupBy(0) will cause the system to use the full Tuple2 as a key (with the Integer and Float being the key). If you want to “navigate” into the nested Tuple2, you have to use field expression keys which are explained below.
Define keys using Field Expressions # You can use String-based field expressions to reference nested fields and define keys for grouping, sorting, joining, or coGrouping. Field expressions make it very easy to select fields in (nested) composite types such as Tuple and POJO types.
In the example below, we have a WC POJO with two fields “word” and “count”. To group by the field word, we just pass its name to the groupBy() function.
Java // some ordinary POJO (Plain old Java Object) public class WC { public String word; public int count; } DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words.groupBy(\u0026#34;word\u0026#34;); Scala // some ordinary POJO (Plain old Java Object) class WC(var word: String, var count: Int) { def this() { this(\u0026#34;\u0026#34;, 0L) } } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) // or, as a case class, which is less typing case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) Field Expression Syntax: # Select POJO fields by their field name. For example \u0026ldquo;user\u0026rdquo; refers to the “user” field of a POJO type.
Select Tuple fields by their 1-offset field name or 0-offset field index. For example \u0026ldquo;_1\u0026rdquo; and \u0026ldquo;5\u0026rdquo; refer to the first and sixth field of a Scala Tuple type, respectively.
You can select nested fields in POJOs and Tuples. For example \u0026ldquo;user.zip\u0026rdquo; refers to the “zip” field of a POJO which is stored in the “user” field of a POJO type. Arbitrary nesting and mixing of POJOs and Tuples is supported such as \u0026ldquo;_2.user.zip\u0026rdquo; or \u0026ldquo;user._4.1.zip\u0026rdquo;.
You can select the full type using the \u0026ldquo;_\u0026rdquo; wildcard expressions. This does also work for types which are not Tuple or POJO types.
Field Expression Example: # Java public static class WC { public ComplexNestedClass complex; //nested POJO private int count; // getter / setter for private field (count) public int getCount() { return count; } public void setCount(int c) { this.count = c; } } public static class ComplexNestedClass { public Integer someNumber; public float someFloat; public Tuple3\u0026lt;Long, Long, String\u0026gt; word; public IntWritable hadoopCitizen; } Scala class WC(var complex: ComplexNestedClass, var count: Int) { def this() { this(null, 0) } } class ComplexNestedClass( var someNumber: Int, someFloat: Float, word: (Long, Long, String), hadoopCitizen: IntWritable) { def this() { this(0, 0, (0, 0, \u0026#34;\u0026#34;), new IntWritable(0)) } } These are valid field expressions for the example code above:
\u0026ldquo;count\u0026rdquo;: The count field in the WC class.
\u0026ldquo;complex\u0026rdquo;: Recursively selects all fields of the field complex of POJO type ComplexNestedClass.
\u0026ldquo;complex.word.f2\u0026rdquo;: Selects the last field of the nested Tuple3.
\u0026ldquo;complex.hadoopCitizen\u0026rdquo;: Selects the Hadoop IntWritable type.
Define keys using Key Selector Functions # An additional way to define keys are “key selector” functions. A key selector function takes a single element as input and returns the key for the element. The key can be of any type and be derived from deterministic computations.
The following example shows a key selector function that simply returns the field of an object:
Java // some ordinary POJO public class WC {public String word; public int count;} DataSet\u0026lt;WC\u0026gt; words = // [...] UnsortedGrouping\u0026lt;WC\u0026gt; keyed = words .groupBy(new KeySelector\u0026lt;WC, String\u0026gt;() { public String getKey(WC wc) { return wc.word; } }); Scala // some ordinary case class case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val keyed = words.groupBy( _.word ) Data Sources # Data sources create the initial data sets, such as from files or from Java collections. The general mechanism of creating data sets is abstracted behind an InputFormat. Flink comes with several built-in formats to create data sets from common file formats. Many of them have shortcut methods on the ExecutionEnvironment.
File-based:
readTextFile(path) / TextInputFormat - Reads files line wise and returns them as Strings.
readTextFileWithValue(path) / TextValueInputFormat - Reads files line wise and returns them as StringValues. StringValues are mutable strings.
readCsvFile(path) / CsvInputFormat - Parses files of comma (or another char) delimited fields. Returns a DataSet of tuples or POJOs. Supports the basic java types and their Value counterparts as field types.
readFileOfPrimitives(path, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer.
readFileOfPrimitives(path, delimiter, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer using the given delimiter.
Collection-based:
fromCollection(Collection) - Creates a data set from a Java.util.Collection. All elements in the collection must be of the same type.
fromCollection(Iterator, Class) - Creates a data set from an iterator. The class specifies the data type of the elements returned by the iterator.
fromElements(T \u0026hellip;) - Creates a data set from the given sequence of objects. All objects must be of the same type.
fromParallelCollection(SplittableIterator, Class) - Creates a data set from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
generateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.
Generic:
readFile(inputFormat, path) / FileInputFormat - Accepts a file input format.
createInput(inputFormat) / InputFormat - Accepts a generic input format.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // read text file from local files system DataSet\u0026lt;String\u0026gt; localLines = env.readTextFile(\u0026#34;file:///path/to/my/textfile\u0026#34;); // read text file from an HDFS running at nnHost:nnPort DataSet\u0026lt;String\u0026gt; hdfsLines = env.readTextFile(\u0026#34;hdfs://nnHost:nnPort/path/to/my/textfile\u0026#34;); // read a CSV file with three fields DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .types(Integer.class, String.class, Double.class); // read a CSV file with five fields, taking only two of them DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .includeFields(\u0026#34;10010\u0026#34;) // take the first and the fourth field .types(String.class, Double.class); // read a CSV file with three fields into a POJO (Person.class) with corresponding fields DataSet\u0026lt;Person\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .pojoType(Person.class, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;zipcode\u0026#34;); // read a file from the specified path of type SequenceFileInputFormat DataSet\u0026lt;Tuple2\u0026lt;IntWritable, Text\u0026gt;\u0026gt; tuples = env.createInput(HadoopInputs.readSequenceFile(IntWritable.class, Text.class, \u0026#34;hdfs://nnHost:nnPort/path/to/file\u0026#34;)); // creates a set from some given elements DataSet\u0026lt;String\u0026gt; value = env.fromElements(\u0026#34;Foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;foobar\u0026#34;, \u0026#34;fubar\u0026#34;); // generate a number sequence DataSet\u0026lt;Long\u0026gt; numbers = env.generateSequence(1, 10000000); // Read data from a relational database using the JDBC input format DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt; dbData = env.createInput( JdbcInputFormat.buildJdbcInputFormat() .setDrivername(\u0026#34;org.apache.derby.jdbc.EmbeddedDriver\u0026#34;) .setDBUrl(\u0026#34;jdbc:derby:memory:persons\u0026#34;) .setQuery(\u0026#34;select name, age from persons\u0026#34;) .setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO)) .finish() ); // Note: Flink\u0026#39;s program compiler needs to infer the data types of the data items which are returned // by an InputFormat. If this information cannot be automatically inferred, it is necessary to // manually provide the type information as shown in the examples above. Scala val env = ExecutionEnvironment.getExecutionEnvironment // read text file from local files system val localLines = env.readTextFile(\u0026#34;file:///path/to/my/textfile\u0026#34;) // read text file from an HDFS running at nnHost:nnPort val hdfsLines = env.readTextFile(\u0026#34;hdfs://nnHost:nnPort/path/to/my/textfile\u0026#34;) // read a CSV file with three fields val csvInput = env.readCsvFile[(Int, String, Double)](\u0026#34;hdfs:///the/CSV/file\u0026#34;) // read a CSV file with five fields, taking only two of them val csvInput = env.readCsvFile[(String, Double)]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field // CSV input can also be used with Case Classes case class MyCaseClass(str: String, dbl: Double) val csvInput = env.readCsvFile[MyCaseClass]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field // read a CSV file with three fields into a POJO (Person) with corresponding fields val csvInput = env.readCsvFile[Person]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, pojoFields = Array(\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;zipcode\u0026#34;)) // create a set from some given elements val values = env.fromElements(\u0026#34;Foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;foobar\u0026#34;, \u0026#34;fubar\u0026#34;) // generate a number sequence val numbers = env.generateSequence(1, 10000000) // read a file from the specified path of type SequenceFileInputFormat val tuples = env.createInput(HadoopInputs.readSequenceFile(classOf[IntWritable], classOf[Text], \u0026#34;hdfs://nnHost:nnPort/path/to/file\u0026#34;)) Configuring CSV Parsing # Flink offers a number of configuration options for CSV parsing:
types(Class \u0026hellip; types) specifies the types of the fields to parse. It is mandatory to configure the types of the parsed fields. In case of the type class Boolean.class, “True” (case-insensitive), “False” (case-insensitive), “1” and “0” are treated as booleans.
lineDelimiter(String del) specifies the delimiter of individual records. The default line delimiter is the new-line character \u0026lsquo;\\n\u0026rsquo;.
fieldDelimiter(String del) specifies the delimiter that separates fields of a record. The default field delimiter is the comma character \u0026lsquo;,\u0026rsquo;.
includeFields(boolean \u0026hellip; flag), includeFields(String mask), or includeFields(long bitMask) defines which fields to read from the input file (and which to ignore). By default the first n fields (as defined by the number of types in the types() call) are parsed.
parseQuotedStrings(char quoteChar) enables quoted string parsing. Strings are parsed as quoted strings if the first character of the string field is the quote character (leading or tailing whitespaces are not trimmed). Field delimiters within quoted strings are ignored. Quoted string parsing fails if the last character of a quoted string field is not the quote character or if the quote character appears at some point which is not the start or the end of the quoted string field (unless the quote character is escaped using ‘\u0026rsquo;). If quoted string parsing is enabled and the first character of the field is not the quoting string, the string is parsed as unquoted string. By default, quoted string parsing is disabled.
ignoreComments(String commentPrefix) specifies a comment prefix. All lines that start with the specified comment prefix are not parsed and ignored. By default, no lines are ignored.
ignoreInvalidLines() enables lenient parsing, i.e., lines that cannot be correctly parsed are ignored. By default, lenient parsing is disabled and invalid lines raise an exception.
ignoreFirstLine() configures the InputFormat to ignore the first line of the input file. By default no line is ignored.
Recursive Traversal of the Input Path Directory # For file-based inputs, when the input path is a directory, nested files are not enumerated by default. Instead, only the files inside the base directory are read, while nested files are ignored. Recursive enumeration of nested files can be enabled through the recursive.file.enumeration configuration parameter, like in the following example.
Java // enable recursive enumeration of nested input files ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a configuration object Configuration parameters = new Configuration(); // set the recursive enumeration parameter parameters.setBoolean(\u0026#34;recursive.file.enumeration\u0026#34;, true); // pass the configuration to the data source DataSet\u0026lt;String\u0026gt; logs = env.readTextFile(\u0026#34;file:///path/with.nested/files\u0026#34;) .withParameters(parameters); Scala // enable recursive enumeration of nested input files val env = ExecutionEnvironment.getExecutionEnvironment // create a configuration object val parameters = new Configuration // set the recursive enumeration parameter parameters.setBoolean(\u0026#34;recursive.file.enumeration\u0026#34;, true) // pass the configuration to the data source env.readTextFile(\u0026#34;file:///path/with.nested/files\u0026#34;).withParameters(parameters) Read Compressed Files # Flink currently supports transparent decompression of input files if these are marked with an appropriate file extension. In particular, this means that no further configuration of the input formats is necessary and any FileInputFormat support the compression, including custom input formats. Please notice that compressed files might not be read in parallel, thus impacting job scalability.
The following table lists the currently supported compression methods.
Compressed Method File Extensions Parallelizable DEFLATE .deflate no GZip .gz, .gzip no Bzip2 .bz2 no XZ .xz no Data Sinks # Data sinks consume DataSets and are used to store or return them. Data sink operations are described using an OutputFormat. Flink comes with a variety of built-in output formats that are encapsulated behind operations on the DataSet:
writeAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element. writeAsFormattedText() / TextOutputFormat - Write elements line-wise as Strings. The Strings are obtained by calling a user-defined format() method for each element. writeAsCsv(\u0026hellip;) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects. print() / printToErr() / print(String msg) / printToErr(String msg) - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output. write() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion. output()/ OutputFormat - Most generic output method, for data sinks that are not file based (such as storing the result in a database). A DataSet can be input to multiple operations. Programs can write or print a data set and at the same time run additional transformations on them.
Java // text data DataSet\u0026lt;String\u0026gt; textData = // [...] // write DataSet to a file on the local file system textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;); // write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort textData.writeAsText(\u0026#34;hdfs://nnHost:nnPort/my/result/on/localFS\u0026#34;); // write DataSet to a file and overwrite the file if it exists textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;, WriteMode.OVERWRITE); // tuples as lines with pipe as the separator \u0026#34;a|b|c\u0026#34; DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; values = // [...] values.writeAsCsv(\u0026#34;file:///path/to/the/result/file\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;|\u0026#34;); // this writes tuples in the text formatting \u0026#34;(a, b, c)\u0026#34;, rather than as CSV lines values.writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;); // this writes values as strings using a user-defined TextFormatter object values.writeAsFormattedText(\u0026#34;file:///path/to/the/result/file\u0026#34;, new TextFormatter\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt;() { public String format (Tuple2\u0026lt;Integer, Integer\u0026gt; value) { return value.f1 + \u0026#34; - \u0026#34; + value.f0; } }); Scala // text data val textData: DataSet[String] = // [...] // write DataSet to a file on the local file system textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;) // write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort textData.writeAsText(\u0026#34;hdfs://nnHost:nnPort/my/result/on/localFS\u0026#34;) // write DataSet to a file and overwrite the file if it exists textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;, WriteMode.OVERWRITE) // tuples as lines with pipe as the separator \u0026#34;a|b|c\u0026#34; val values: DataSet[(String, Int, Double)] = // [...] values.writeAsCsv(\u0026#34;file:///path/to/the/result/file\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;|\u0026#34;) // this writes tuples in the text formatting \u0026#34;(a, b, c)\u0026#34;, rather than as CSV lines values.writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) // this writes values as strings using a user-defined formatting values map { tuple =\u0026gt; tuple._1 + \u0026#34; - \u0026#34; + tuple._2 } .writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) Or with a custom output format:
DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; myResult = [...] // write Tuple DataSet to a relational database myResult.output( // build and configure OutputFormat JdbcOutputFormat.buildJdbcOutputFormat() .setDrivername(\u0026#34;org.apache.derby.jdbc.EmbeddedDriver\u0026#34;) .setDBUrl(\u0026#34;jdbc:derby:memory:persons\u0026#34;) .setQuery(\u0026#34;insert into persons (name, age, height) values (?,?,?)\u0026#34;) .finish() ); Locally Sorted Output # The output of a data sink can be locally sorted on specified fields in specified orders using tuple field positions or field expressions. This works for every output format.
The following examples show how to use this feature:
DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; tData = // [...] DataSet\u0026lt;Tuple2\u0026lt;BookPojo, Double\u0026gt;\u0026gt; pData = // [...] DataSet\u0026lt;String\u0026gt; sData = // [...] // sort output on String field in ascending order tData.sortPartition(1, Order.ASCENDING).print(); // sort output on Double field in descending and Integer field in ascending order tData.sortPartition(2, Order.DESCENDING).sortPartition(0, Order.ASCENDING).print(); // sort output on the \u0026#34;author\u0026#34; field of nested BookPojo in descending order pData.sortPartition(\u0026#34;f0.author\u0026#34;, Order.DESCENDING).writeAsText(...); // sort output on the full tuple in ascending order tData.sortPartition(\u0026#34;*\u0026#34;, Order.ASCENDING).writeAsCsv(...); // sort atomic type (String) output in descending order sData.sortPartition(\u0026#34;*\u0026#34;, Order.DESCENDING).writeAsText(...); Globally sorted output is not supported.
Iteration Operators # Iterations implement loops in Flink programs. The iteration operators encapsulate a part of the program and execute it repeatedly, feeding back the result of one iteration (the partial solution) into the next iteration. There are two types of iterations in Flink: BulkIteration and DeltaIteration.
This section provides quick examples on how to use both operators. Check out the Introduction to Iterations page for a more detailed introduction.
Java Bulk Iterations # To create a BulkIteration call the iterate(int) method of the DataSet the iteration should start at. This will return an IterativeDataSet, which can be transformed with the regular operators. The single argument to the iterate call specifies the maximum number of iterations.
To specify the end of an iteration call the closeWith(DataSet) method on the IterativeDataSet to specify which transformation should be fed back to the next iteration. You can optionally specify a termination criterion with closeWith(DataSet, DataSet), which evaluates the second DataSet and terminates the iteration, if this DataSet is empty. If no termination criterion is specified, the iteration terminates after the given maximum number iterations.
The following example iteratively estimates the number Pi. The goal is to count the number of random points, which fall into the unit circle. In each iteration, a random point is picked. If this point lies inside the unit circle, we increment the count. Pi is then estimated as the resulting count divided by the number of iterations multiplied by 4.
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Create initial IterativeDataSet IterativeDataSet\u0026lt;Integer\u0026gt; initial = env.fromElements(0).iterate(10000); DataSet\u0026lt;Integer\u0026gt; iteration = initial.map(new MapFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public Integer map(Integer i) throws Exception { double x = Math.random(); double y = Math.random(); return i + ((x * x + y * y \u0026lt; 1) ? 1 : 0); } }); // Iteratively transform the IterativeDataSet DataSet\u0026lt;Integer\u0026gt; count = initial.closeWith(iteration); count.map(new MapFunction\u0026lt;Integer, Double\u0026gt;() { @Override public Double map(Integer count) throws Exception { return count / (double) 10000 * 4; } }).print(); env.execute(\u0026#34;Iterative Pi Example\u0026#34;); Delta Iterations # Delta iterations exploit the fact that certain algorithms do not change every data point of the solution in each iteration.
In addition to the partial solution that is fed back (called workset) in every iteration, delta iterations maintain state across iterations (called solution set), which can be updated through deltas. The result of the iterative computation is the state after the last iteration. Please refer to the Introduction to Iterations for an overview of the basic principle of delta iterations.
Defining a DeltaIteration is similar to defining a BulkIteration. For delta iterations, two data sets form the input to each iteration (workset and solution set), and two data sets are produced as the result (new workset, solution set delta) in each iteration.
To create a DeltaIteration call the iterateDelta(DataSet, int, int) (or iterateDelta(DataSet, int, int[]) respectively). This method is called on the initial solution set. The arguments are the initial delta set, the maximum number of iterations and the key positions. The returned DeltaIteration object gives you access to the DataSets representing the workset and solution set via the methods iteration.getWorkset() and iteration.getSolutionSet().
Below is an example for the syntax of a delta iteration
// read the initial data sets DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; initialSolutionSet = // [...] DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; initialDeltaSet = // [...] int maxIterations = 100; int keyPosition = 0; DeltaIteration\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; iteration = initialSolutionSet .iterateDelta(initialDeltaSet, maxIterations, keyPosition); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; candidateUpdates = iteration.getWorkset() .groupBy(1) .reduceGroup(new ComputeCandidateChanges()); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; deltas = candidateUpdates .join(iteration.getSolutionSet()) .where(0) .equalTo(0) .with(new CompareChangesToCurrent()); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; nextWorkset = deltas .filter(new FilterByThreshold()); iteration.closeWith(deltas, nextWorkset) .writeAsCsv(outputPath); Scala Bulk Iterations # To create a BulkIteration call the iterate(int) method of the DataSet the iteration should start at and also specify a step function. The step function gets the input DataSet for the current iteration and must return a new DataSet. The parameter of the iterate call is the maximum number of iterations after which to stop.
There is also the iterateWithTermination(int) function that accepts a step function that returns two DataSets: The result of the iteration step and a termination criterion. The iterations are stopped once the termination criterion DataSet is empty.
The following example iteratively estimates the number Pi. The goal is to count the number of random points, which fall into the unit circle. In each iteration, a random point is picked. If this point lies inside the unit circle, we increment the count. Pi is then estimated as the resulting count divided by the number of iterations multiplied by 4.
val env = ExecutionEnvironment.getExecutionEnvironment() // Create initial DataSet val initial = env.fromElements(0) val count = initial.iterate(10000) { iterationInput: DataSet[Int] =\u0026gt; val result = iterationInput.map { i =\u0026gt; val x = Math.random() val y = Math.random() i + (if (x * x + y * y \u0026lt; 1) 1 else 0) } result } val result = count map { c =\u0026gt; c / 10000.0 * 4 } result.print() env.execute(\u0026#34;Iterative Pi Example\u0026#34;) Delta Iterations # Delta iterations exploit the fact that certain algorithms do not change every data point of the solution in each iteration.
In addition to the partial solution that is fed back (called workset) in every iteration, delta iterations maintain state across iterations (called solution set), which can be updated through deltas. The result of the iterative computation is the state after the last iteration. Please refer to the Introduction to Iterations for an overview of the basic principle of delta iterations.
Defining a DeltaIteration is similar to defining a BulkIteration. For delta iterations, two data sets form the input to each iteration (workset and solution set), and two data sets are produced as the result (new workset, solution set delta) in each iteration.
To create a DeltaIteration call the iterateDelta(initialWorkset, maxIterations, key) on the initial solution set. The step function takes two parameters: (solutionSet, workset), and must return two values: (solutionSetDelta, newWorkset).
Below is an example for the syntax of a delta iteration
// read the initial data sets val initialSolutionSet: DataSet[(Long, Double)] = // [...] val initialWorkset: DataSet[(Long, Double)] = // [...] val maxIterations = 100 val keyPosition = 0 val result = initialSolutionSet.iterateDelta(initialWorkset, maxIterations, Array(keyPosition)) { (solution, workset) =\u0026gt; val candidateUpdates = workset.groupBy(1).reduceGroup(new ComputeCandidateChanges()) val deltas = candidateUpdates.join(solution).where(0).equalTo(0)(new CompareChangesToCurrent()) val nextWorkset = deltas.filter(new FilterByThreshold()) (deltas, nextWorkset) } result.writeAsCsv(outputPath) env.execute() Operating on Data Objects in Functions # Flink’s runtime exchanges data with user functions in form of Java objects. Functions receive input objects from the runtime as method parameters and return output objects as result. Because these objects are accessed by user functions and runtime code, it is very important to understand and follow the rules about how the user code may access, i.e., read and modify, these objects.
User functions receive objects from Flink’s runtime either as regular method parameters (like a MapFunction) or through an Iterable parameter (like a GroupReduceFunction). We refer to objects that the runtime passes to a user function as input objects. User functions can emit objects to the Flink runtime either as a method return value (like a MapFunction) or through a Collector (like a FlatMapFunction). We refer to objects which have been emitted by the user function to the runtime as output objects.
Flink’s DataSet API features two modes that differ in how Flink’s runtime creates or reuses input objects. This behavior affects the guarantees and constraints for how user functions may interact with input and output objects. The following sections define these rules and give coding guidelines to write safe user function code.
Object-Reuse Disabled (DEFAULT) # By default, Flink operates in object-reuse disabled mode. This mode ensures that functions always receive new input objects within a function call. The object-reuse disabled mode gives better guarantees and is safer to use. However, it comes with a certain processing overhead and might cause higher Java garbage collection activity. The following table explains how user functions may access input and output objects in object-reuse disabled mode.
Operation Guarantees and Restrictions Reading Input Objects Within a method call it is guaranteed that the value of an input object does not change. This includes objects served by an Iterable. For example it is safe to collect input objects served by an Iterable in a List or Map. Note that objects may be modified after the method call is left. It is not safe to remember objects across function calls. Modifying Input Objects You may modify input objects. Emitting Input Objects You may emit input objects. The value of an input object may have changed after it was emitted. It is not safe to read an input object after it was emitted. Reading Output Objects An object that was given to a Collector or returned as method result might have changed its value. It is not safe to read an output object. Modifying Output Objects You may modify an object after it was emitted and emit it again. Coding guidelines for the object-reuse disabled (default) mode:
Do not remember the read input objects across method calls. Do not read objects after you emitted them. Object-Reuse Enabled # In object-reuse enabled mode, Flink’s runtime minimizes the number of object instantiations. This can improve the performance and can reduce the Java garbage collection pressure. The object-reuse enabled mode is activated by calling ExecutionConfig.enableObjectReuse(). The following table explains how user functions may access input and output objects in object-reuse enabled mode.
Operation Guarantees and Restrictions Reading input objects received as regular method parameters Input objects received as regular method arguments are not modified within a function call. Objects may be modified after method call is left. It is not safe to remember objects across function calls. Reading input objects received from an Iterable parameter Input objects received from an Iterable are only valid until the next() method is called. An Iterable or Iterator may serve the same object instance multiple times. It is not safe to remember input objects received from an Iterable, e.g., by putting them in a List or Map. Modifying Input Objects You must not modify input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). Emitting Input Objects You must not emit input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). Reading output Objects An object that was given to a Collector or returned as method result might have changed its value. It is not safe to read an output object. Modifying Output Objects You may modify an output object and emit it again. Coding guidelines for object-reuse enabled:
Do not remember input objects received from an Iterable. Do not remember and read input objects across method calls. Do not modify or emit input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). To reduce object instantiations, you can always emit a dedicated output object which is repeatedly modified but never read. Debugging # Before running a data analysis program on a large data set in a distributed cluster, it is a good idea to make sure that the implemented algorithm works as desired. Hence, implementing data analysis programs is usually an incremental process of checking results, debugging, and improving.
Flink provides a few nice features to significantly ease the development process of data analysis programs by supporting local debugging from within an IDE, injection of test data, and collection of result data. This section give some hints how to ease the development of Flink programs.
Local Execution Envronment # A LocalEnvironment starts a Flink system within the same JVM process it was created in. If you start the LocalEnvironment from an IDE, you can set breakpoints in your code and easily debug your program.
A LocalEnvironment is created and used as follows:
Java final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); DataSet\u0026lt;String\u0026gt; lines = env.readTextFile(pathToTextFile); // build your program env.execute(); Scala val env = ExecutionEnvironment.createLocalEnvironment() val lines = env.readTextFile(pathToTextFile) // build your program env.execute() Collection Data Sources and Sinks # Providing input for an analysis program and checking its output is cumbersome when done by creating input files and reading output files. Flink features special data sources and sinks which are backed by Java collections to ease testing. Once a program has been tested, the sources and sinks can be easily replaced by sources and sinks that read from / write to external data stores such as HDFS.
Collection data sources can be used as follows:
Java final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); // Create a DataSet from a list of elements DataSet\u0026lt;Integer\u0026gt; myInts = env.fromElements(1, 2, 3, 4, 5); // Create a DataSet from any Java collection List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; data = ...; DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myTuples = env.fromCollection(data); // Create a DataSet from an Iterator Iterator\u0026lt;Long\u0026gt; longIt = ...; DataSet\u0026lt;Long\u0026gt; myLongs = env.fromCollection(longIt, Long.class); Scala val env = ExecutionEnvironment.createLocalEnvironment() // Create a DataSet from a list of elements val myInts = env.fromElements(1, 2, 3, 4, 5) // Create a DataSet from any Collection val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // Create a DataSet from an Iterator val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) Note: Currently, the collection data source requires that data types and iterators implement Serializable. Furthermore, collection data sources can not be executed in parallel ( parallelism = 1).
Broadcast Variables # Broadcast variables allow you to make a data set available to all parallel instances of an operation, in addition to the regular input of the operation. This is useful for auxiliary data sets, or data-dependent parameterization. The data set will then be accessible at the operator as a Collection.
Broadcast: broadcast sets are registered by name via withBroadcastSet(DataSet, String), and Access: accessible via getRuntimeContext().getBroadcastVariable(String) at the target operator. Java Java Scala // 1. The DataSet to be broadcast DataSet\u0026lt;Integer\u0026gt; toBroadcast = env.fromElements(1, 2, 3); DataSet\u0026lt;String\u0026gt; data = env.fromElements(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); data.map(new RichMapFunction\u0026lt;String, String\u0026gt;() { @Override public void open(Configuration parameters) throws Exception { // 3. Access the broadcast DataSet as a Collection Collection\u0026lt;Integer\u0026gt; broadcastSet = getRuntimeContext().getBroadcastVariable(\u0026#34;broadcastSetName\u0026#34;); } @Override public String map(String value) throws Exception { ... } }).withBroadcastSet(toBroadcast, \u0026#34;broadcastSetName\u0026#34;); // 2. Broadcast the DataSet Scala // 1. The DataSet to be broadcast val toBroadcast = env.fromElements(1, 2, 3) val data = env.fromElements(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) data.map(new RichMapFunction[String, String]() { var broadcastSet: Traversable[String] = null override def open(config: Configuration): Unit = { // 3. Access the broadcast DataSet as a Collection broadcastSet = getRuntimeContext().getBroadcastVariable[String](\u0026#34;broadcastSetName\u0026#34;).asScala } def map(in: String): String = { ... } }).withBroadcastSet(toBroadcast, \u0026#34;broadcastSetName\u0026#34;) // 2. Broadcast the DataSet Make sure that the names (broadcastSetName in the previous example) match when registering and accessing broadcast data sets. For a complete example program, have a look at K-Means Algorithm.
Note: As the content of broadcast variables is kept in-memory on each node, it should not become too large. For simpler things like scalar values you can simply make parameters part of the closure of a function, or use the withParameters(\u0026hellip;) method to pass in a configuration.
Distributed Cache # Flink offers a distributed cache, similar to Apache Hadoop, to make files locally accessible to parallel instances of user functions. This functionality can be used to share files that contain static external data such as dictionaries or machine-learned regression models.
The cache works as follows. A program registers a file or directory of a local or remote filesystem such as HDFS or S3 under a specific name in its ExecutionEnvironment as a cached file. When the program is executed, Flink automatically copies the file or directory to the local filesystem of all workers. A user function can look up the file or directory under the specified name and access it from the worker’s local filesystem.
The distributed cache is used as follows:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // register a file from HDFS env.registerCachedFile(\u0026#34;hdfs:///path/to/your/file\u0026#34;, \u0026#34;hdfsFile\u0026#34;); // register a local executable file (script, executable, ...) env.registerCachedFile(\u0026#34;file:///path/to/exec/file\u0026#34;, \u0026#34;localExecFile\u0026#34;, true); // define your program and execute ... DataSet\u0026lt;String\u0026gt; input = ...; DataSet\u0026lt;Integer\u0026gt; result = input.map(new MyMapper()); ... env.execute(); Scala val env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\u0026#34;hdfs:///path/to/your/file\u0026#34;, \u0026#34;hdfsFile\u0026#34;) // register a local executable file (script, executable, ...) env.registerCachedFile(\u0026#34;file:///path/to/exec/file\u0026#34;, \u0026#34;localExecFile\u0026#34;, true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() Access the cached file in a user function (here a MapFunction). The function must extend a RichFunction class because it needs access to the RuntimeContext.
Java // extend a RichFunction to have access to the RuntimeContext public final class MyMapper extends RichMapFunction\u0026lt;String, Integer\u0026gt; { @Override public void open(Configuration config) { // access cached file via RuntimeContext and DistributedCache File myFile = getRuntimeContext().getDistributedCache().getFile(\u0026#34;hdfsFile\u0026#34;); // read the file (or navigate the directory) ... } @Override public Integer map(String value) throws Exception { // use content of cached file ... } } Scala // extend a RichFunction to have access to the RuntimeContext class MyMapper extends RichMapFunction[String, Int] { override def open(config: Configuration): Unit = { // access cached file via RuntimeContext and DistributedCache val myFile: File = getRuntimeContext.getDistributedCache.getFile(\u0026#34;hdfsFile\u0026#34;) // read the file (or navigate the directory) ... } override def map(value: String): Int = { // use content of cached file ... } } Passing Parameters to Functions # Parameters can be passed to functions using either the constructor or the withParameters(Configuration) method. The parameters are serialized as part of the function object and shipped to all parallel task instances.
Via Constructor # Java DataSet\u0026lt;Integer\u0026gt; toFilter = env.fromElements(1, 2, 3); toFilter.filter(new MyFilter(2)); private static class MyFilter implements FilterFunction\u0026lt;Integer\u0026gt; { private final int limit; public MyFilter(int limit) { this.limit = limit; } @Override public boolean filter(Integer value) throws Exception { return value \u0026gt; limit; } } Scala val toFilter = env.fromElements(1, 2, 3) toFilter.filter(new MyFilter(2)) class MyFilter(limit: Int) extends FilterFunction[Int] { override def filter(value: Int): Boolean = { value \u0026gt; limit } } Via withParameters(Configuration) # Java DataSet\u0026lt;Integer\u0026gt; toFilter = env.fromElements(1, 2, 3); Configuration config = new Configuration(); config.setInteger(\u0026#34;limit\u0026#34;, 2); toFilter.filter(new RichFilterFunction\u0026lt;Integer\u0026gt;() { private int limit; @Override public void open(Configuration parameters) throws Exception { limit = parameters.getInteger(\u0026#34;limit\u0026#34;, 0); } @Override public boolean filter(Integer value) throws Exception { return value \u0026gt; limit; } }).withParameters(config); Scala val toFilter = env.fromElements(1, 2, 3) val c = new Configuration() c.setInteger(\u0026#34;limit\u0026#34;, 2) toFilter.filter(new RichFilterFunction[Int]() { var limit = 0 override def open(config: Configuration): Unit = { limit = config.getInteger(\u0026#34;limit\u0026#34;, 0) } def filter(in: Int): Boolean = { in \u0026gt; limit } }).withParameters(c) Globally via the ExecutionConfig # Flink also allows to pass custom configuration values to the ExecutionConfig interface of the environment. Since the execution config is accessible in all (rich) user functions, the custom configuration will be available globally in all functions.
Setting a custom global configuration
Java Configuration conf = new Configuration(); conf.setString(\u0026#34;mykey\u0026#34;,\u0026#34;myvalue\u0026#34;); final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(conf); Scala val env = ExecutionEnvironment.getExecutionEnvironment val conf = new Configuration() conf.setString(\u0026#34;mykey\u0026#34;, \u0026#34;myvalue\u0026#34;) env.getConfig.setGlobalJobParameters(conf) Please note that you can also pass a custom class extending the ExecutionConfig.GlobalJobParameters class as the global job parameters to the execution config. The interface allows to implement the Map\u0026lt;String, String\u0026gt; toMap() method which will in turn show the values from the configuration in the web frontend.
Accessing values from the global configuration
public static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private String mykey; @Override public void open(Configuration parameters) throws Exception { ExecutionConfig.GlobalJobParameters globalParams = getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); Configuration globConf = (Configuration) globalParams; mykey = globConf.getString(\u0026#34;mykey\u0026#34;, null); } `}),e.add({id:17,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/types_serialization/",title:"Overview",section:"Data Types \u0026 Serialization",content:` Data Types \u0026amp; Serialization # Apache Flink handles data types and serialization in a unique way, containing its own type descriptors, generic type extraction, and type serialization framework. This document describes the concepts and the rationale behind them.
Supported Data Types # Flink places some restrictions on the type of elements that can be in a DataStream. The reason for this is that the system analyzes the types to determine efficient execution strategies.
There are seven different categories of data types:
Java Tuples and Scala Case Classes Java POJOs Primitive Types Regular Classes Values Hadoop Writables Special Types Tuples and Case Classes # Java Tuples are composite types that contain a fixed number of fields with various types. The Java API provides classes from Tuple1 up to Tuple25. Every field of a tuple can be an arbitrary Flink type including further tuples, resulting in nested tuples. Fields of a tuple can be accessed directly using the field\u0026rsquo;s name as tuple.f4, or using the generic getter method tuple.getField(int position). The field indices start at 0. Note that this stands in contrast to the Scala tuples, but it is more consistent with Java\u0026rsquo;s general indexing.
DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = env.fromElements( new Tuple2\u0026lt;String, Integer\u0026gt;(\u0026#34;hello\u0026#34;, 1), new Tuple2\u0026lt;String, Integer\u0026gt;(\u0026#34;world\u0026#34;, 2)); wordCounts.map(new MapFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Integer\u0026gt;() { @Override public Integer map(Tuple2\u0026lt;String, Integer\u0026gt; value) throws Exception { return value.f1; } }); wordCounts.keyBy(value -\u0026gt; value.f0); Scala Scala case classes (and Scala tuples which are a special case of case classes), are composite types that contain a fixed number of fields with various types. Tuple fields are addressed by their 1-offset names such as _1 for the first field. Case class fields are accessed by their name.
case class WordCount(word: String, count: Int) val input = env.fromElements( WordCount(\u0026#34;hello\u0026#34;, 1), WordCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set input.keyBy(_.word) val input2 = env.fromElements((\u0026#34;hello\u0026#34;, 1), (\u0026#34;world\u0026#34;, 2)) // Tuple2 Data Set input2.keyBy(value =\u0026gt; (value._1, value._2)) POJOs # Java and Scala classes are treated by Flink as a special POJO data type if they fulfill the following requirements:
The class must be public.
It must have a public constructor without arguments (default constructor).
All fields are either public or must be accessible through getter and setter functions. For a field called foo the getter and setter methods must be named getFoo() and setFoo().
The type of a field must be supported by a registered serializer.
POJOs are generally represented with a PojoTypeInfo and serialized with the PojoSerializer (using Kryo as configurable fallback). The exception is when the POJOs are actually Avro types (Avro Specific Records) or produced as \u0026ldquo;Avro Reflect Types\u0026rdquo;. In that case the POJO\u0026rsquo;s are represented by an AvroTypeInfo and serialized with the AvroSerializer. You can also register your own custom serializer if required; see Serialization for further information.
Flink analyzes the structure of POJO types, i.e., it learns about the fields of a POJO. As a result POJO types are easier to use than general types. Moreover, Flink can process POJOs more efficiently than general types.
You can test whether your class adheres to the POJO requirements via org.apache.flink.types.PojoTestUtils#assertSerializedAsPojo() from the flink-test-utils. If you additionally want to ensure that no field of the POJO will be serialized with Kryo, use assertSerializedAsPojoWithoutKryo() instead.
The following example shows a simple POJO with two public fields.
Java public class WordWithCount { public String word; public int count; public WordWithCount() {} public WordWithCount(String word, int count) { this.word = word; this.count = count; } } DataStream\u0026lt;WordWithCount\u0026gt; wordCounts = env.fromElements( new WordWithCount(\u0026#34;hello\u0026#34;, 1), new WordWithCount(\u0026#34;world\u0026#34;, 2)); wordCounts.keyBy(value -\u0026gt; value.word); Scala class WordWithCount(var word: String, var count: Int) { def this() { this(null, -1) } } val input = env.fromElements( new WordWithCount(\u0026#34;hello\u0026#34;, 1), new WordWithCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set input.keyBy(_.word) Primitive Types # Flink supports all Java and Scala primitive types such as Integer, String, and Double.
General Class Types # Flink supports most Java and Scala classes (API and custom). Restrictions apply to classes containing fields that cannot be serialized, like file pointers, I/O streams, or other native resources. Classes that follow the Java Beans conventions work well in general.
All classes that are not identified as POJO types (see POJO requirements above) are handled by Flink as general class types. Flink treats these data types as black boxes and is not able to access their content (e.g., for efficient sorting). General types are de/serialized using the serialization framework Kryo.
Values # Value types describe their serialization and deserialization manually. Instead of going through a general purpose serialization framework, they provide custom code for those operations by means of implementing the org.apache.flink.types.Value interface with the methods read and write. Using a Value type is reasonable when general purpose serialization would be highly inefficient. An example would be a data type that implements a sparse vector of elements as an array. Knowing that the array is mostly zero, one can use a special encoding for the non-zero elements, while the general purpose serialization would simply write all array elements.
The org.apache.flink.types.CopyableValue interface supports manual internal cloning logic in a similar way.
Flink comes with pre-defined Value types that correspond to basic data types. (ByteValue, ShortValue, IntValue, LongValue, FloatValue, DoubleValue, StringValue, CharValue, BooleanValue). These Value types act as mutable variants of the basic data types: Their value can be altered, allowing programmers to reuse objects and take pressure off the garbage collector.
Hadoop Writables # You can use types that implement the org.apache.hadoop.Writable interface. The serialization logic defined in the write()and readFields() methods will be used for serialization.
Special Types # You can use special types, including Scala\u0026rsquo;s Either, Option, and Try. The Java API has its own custom implementation of Either. Similarly to Scala\u0026rsquo;s Either, it represents a value of two possible types, Left or Right. Either can be useful for error handling or operators that need to output two different types of records.
Type Erasure \u0026amp; Type Inference # Note: This Section is only relevant for Java.
The Java compiler throws away much of the generic type information after compilation. This is known as type erasure in Java. It means that at runtime, an instance of an object does not know its generic type any more. For example, instances of DataStream\u0026lt;String\u0026gt; and DataStream\u0026lt;Long\u0026gt; look the same to the JVM.
Flink requires type information at the time when it prepares the program for execution (when the main method of the program is called). The Flink Java API tries to reconstruct the type information that was thrown away in various ways and store it explicitly in the data sets and operators. You can retrieve the type via DataStream.getType(). The method returns an instance of TypeInformation, which is Flink\u0026rsquo;s internal way of representing types.
The type inference has its limits and needs the \u0026ldquo;cooperation\u0026rdquo; of the programmer in some cases. Examples for that are methods that create data sets from collections, such as StreamExecutionEnvironment.fromCollection(), where you can pass an argument that describes the type. But also generic functions like MapFunction\u0026lt;I, O\u0026gt; may need extra type information.
The ResultTypeQueryable interface can be implemented by input formats and functions to tell the API explicitly about their return type. The input types that the functions are invoked with can usually be inferred by the result types of the previous operations.
Back to top
Type handling in Flink # Flink tries to infer a lot of information about the data types that are exchanged and stored during the distributed computation. Think about it like a database that infers the schema of tables. In most cases, Flink infers all necessary information seamlessly by itself. Having the type information allows Flink to do some cool things:
The more Flink knows about data types, the better the serialization and data layout schemes are. That is quite important for the memory usage paradigm in Flink (work on serialized data inside/outside the heap where ever possible and make serialization very cheap).
Finally, it also spares users in the majority of cases from worrying about serialization frameworks and having to register types.
In general, the information about data types is needed during the pre-flight phase - that is, when the program\u0026rsquo;s calls on DataStream are made, and before any call to execute(), print(), count(), or collect().
Most Frequent Issues # The most frequent issues where users need to interact with Flink\u0026rsquo;s data type handling are:
Registering subtypes: If the function signatures describe only the supertypes, but they actually use subtypes of those during execution, it may increase performance a lot to make Flink aware of these subtypes. For that, call .registerType(clazz) on the StreamExecutionEnvironment for each subtype.
Registering custom serializers: Flink falls back to Kryo for the types that it does not handle transparently by itself. Not all types are seamlessly handled by Kryo (and thus by Flink). For example, many Google Guava collection types do not work well by default. The solution is to register additional serializers for the types that cause problems. Call .getConfig().addDefaultKryoSerializer(clazz, serializer) on the StreamExecutionEnvironment. Additional Kryo serializers are available in many libraries. See 3rd party serializer for more details on working with external serializers.
Adding Type Hints: Sometimes, when Flink cannot infer the generic types despite all tricks, a user must pass a type hint. That is generally only necessary in the Java API. The Type Hints Section describes that in more detail.
Manually creating a TypeInformation: This may be necessary for some API calls where it is not possible for Flink to infer the data types due to Java\u0026rsquo;s generic type erasure. See Creating a TypeInformation or TypeSerializer for details.
Flink\u0026rsquo;s TypeInformation class # The class TypeInformation is the base class for all type descriptors. It reveals some basic properties of the type and can generate serializers and, in specializations, comparators for the types. (Note that comparators in Flink do much more than defining an order - they are basically the utility to handle keys)
Internally, Flink makes the following distinctions between types:
Basic types: All Java primitives and their boxed form, plus void, String, Date, BigDecimal, and BigInteger.
Primitive arrays and Object arrays
Composite types
Flink Java Tuples (part of the Flink Java API): max 25 fields, null fields not supported
Scala case classes (including Scala tuples): null fields not supported
Row: tuples with arbitrary number of fields and support for null fields
POJOs: classes that follow a certain bean-like pattern
Auxiliary types (Option, Either, Lists, Maps, \u0026hellip;)
Generic types: These will not be serialized by Flink itself, but by Kryo.
POJOs are of particular interest, because they support the creation of complex types. They are also transparent to the runtime and can be handled very efficiently by Flink.
Rules for POJO types # Flink recognizes a data type as a POJO type (and allows \u0026ldquo;by-name\u0026rdquo; field referencing) if the following conditions are fulfilled:
The class is public and standalone (no non-static inner class) The class has a public no-argument constructor All non-static, non-transient fields in the class (and all superclasses) are either public (and non-final) or have a public getter- and a setter- method that follows the Java beans naming conventions for getters and setters. Note that when a user-defined data type can\u0026rsquo;t be recognized as a POJO type, it must be processed as GenericType and serialized with Kryo.
Creating a TypeInformation or TypeSerializer # To create a TypeInformation object for a type, use the language specific way:
Java Because Java generally erases generic type information, you need to pass the type to the TypeInformation construction:
For non-generic types, you can pass the Class:
TypeInformation\u0026lt;String\u0026gt; info = TypeInformation.of(String.class); For generic types, you need to \u0026ldquo;capture\u0026rdquo; the generic type information via the TypeHint:
TypeInformation\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; info = TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt;(){}); Internally, this creates an anonymous subclass of the TypeHint that captures the generic information to preserve it until runtime.
Scala In Scala, Flink uses macros that runs at compile time and captures all generic type information while it is still available.
// important: this import is needed to access the \u0026#39;createTypeInformation\u0026#39; macro function import org.apache.flink.streaming.api.scala._ val stringInfo: TypeInformation[String] = createTypeInformation[String] val tupleInfo: TypeInformation[(String, Double)] = createTypeInformation[(String, Double)] You can still use the same method as in Java as a fallback.
To create a TypeSerializer, simply call typeInfo.createSerializer(config) on the TypeInformation object.
The config parameter is of type ExecutionConfig and holds the information about the program\u0026rsquo;s registered custom serializers. Where ever possibly, try to pass the programs proper ExecutionConfig. You can usually obtain it from DataStream via calling getExecutionConfig(). Inside functions (like MapFunction), you can get it by making the function a Rich Function and calling getRuntimeContext().getExecutionConfig().
Type Information in the Scala API # Scala has very elaborate concepts for runtime type information though type manifests and class tags. In general, types and methods have access to the types of their generic parameters - thus, Scala programs do not suffer from type erasure as Java programs do.
In addition, Scala allows to run custom code in the Scala Compiler through Scala Macros - that means that some Flink code gets executed whenever you compile a Scala program written against Flink\u0026rsquo;s Scala API.
We use the Macros to look at the parameter types and return types of all user functions during compilation - that is the point in time when certainly all type information is perfectly available. Within the macro, we create a TypeInformation for the function\u0026rsquo;s return types (or parameter types) and make it part of the operation.
No Implicit Value for Evidence Parameter Error # In the case where TypeInformation could not be created, programs fail to compile with an error stating \u0026ldquo;could not find implicit value for evidence parameter of type TypeInformation\u0026rdquo;.
A frequent reason if that the code that generates the TypeInformation has not been imported. Make sure to import the entire flink.api.scala package.
import org.apache.flink.api.scala._ Another common cause are generic methods, which can be fixed as described in the following section.
Generic Methods # Consider the following case below:
def selectFirst[T](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v =\u0026gt; v._1 } } val data : DataStream[(String, Long) = ... val result = selectFirst(data) For such generic methods, the data types of the function parameters and return type may not be the same for every call and are not known at the site where the method is defined. The code above will result in an error that not enough implicit evidence is available.
In such cases, the type information has to be generated at the invocation site and passed to the method. Scala offers implicit parameters for that.
The following code tells Scala to bring a type information for T into the function. The type information will then be generated at the sites where the method is invoked, rather than where the method is defined.
def selectFirst[T : TypeInformation](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v =\u0026gt; v._1 } } Type Information in the Java API # In the general case, Java erases generic type information. Flink tries to reconstruct as much type information as possible via reflection, using the few bits that Java preserves (mainly function signatures and subclass information). This logic also contains some simple type inference for cases where the return type of a function depends on its input type:
public class AppendOne\u0026lt;T\u0026gt; implements MapFunction\u0026lt;T, Tuple2\u0026lt;T, Long\u0026gt;\u0026gt; { public Tuple2\u0026lt;T, Long\u0026gt; map(T value) { return new Tuple2\u0026lt;T, Long\u0026gt;(value, 1L); } } There are cases where Flink cannot reconstruct all generic type information. In that case, a user has to help out via type hints.
Type Hints in the Java API # In cases where Flink cannot reconstruct the erased generic type information, the Java API offers so called type hints. The type hints tell the system the type of the data stream or data set produced by a function:
DataStream\u0026lt;SomeType\u0026gt; result = stream .map(new MyGenericNonInferrableFunction\u0026lt;Long, SomeType\u0026gt;()) .returns(SomeType.class); The returns statement specifies the produced type, in this case via a class. The hints support type definition via
Classes, for non-parameterized types (no generics) TypeHints in the form of returns(new TypeHint\u0026lt;Tuple2\u0026lt;Integer, SomeType\u0026gt;\u0026gt;(){}). The TypeHint class can capture generic type information and preserve it for the runtime (via an anonymous subclass). Type extraction for Java 8 lambdas # Type extraction for Java 8 lambdas works differently than for non-lambdas, because lambdas are not associated with an implementing class that extends the function interface.
Currently, Flink tries to figure out which method implements the lambda and uses Java\u0026rsquo;s generic signatures to determine the parameter types and the return type. However, these signatures are not generated for lambdas by all compilers. If you observe unexpected behavior, manually specify the return type using the returns method.
Serialization of POJO types # The PojoTypeInfo is creating serializers for all the fields inside the POJO. Standard types such as int, long, String etc. are handled by serializers we ship with Flink. For all other types, we fall back to Kryo.
If Kryo is not able to handle the type, you can ask the PojoTypeInfo to serialize the POJO using Avro. To do so, you have to call
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceAvro(); Note that Flink is automatically serializing POJOs generated by Avro with the Avro serializer.
If you want your entire POJO Type to be treated by the Kryo serializer, set
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceKryo(); If Kryo is not able to serialize your POJO, you can add a custom serializer to Kryo, using
env.getConfig().addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Class\u0026lt;? extends Serializer\u0026lt;?\u0026gt;\u0026gt; serializerClass); There are different variants of these methods available.
Disabling Kryo Fallback # There are cases when programs may want to explicitly avoid using Kryo as a fallback for generic types. The most common one is wanting to ensure that all types are efficiently serialized either through Flink\u0026rsquo;s own serializers, or via user-defined custom serializers.
The setting below will raise an exception whenever a data type is encountered that would go through Kryo:
env.getConfig().disableGenericTypes(); Defining Type Information using a Factory # A type information factory allows for plugging-in user-defined type information into the Flink type system. You have to implement org.apache.flink.api.common.typeinfo.TypeInfoFactory to return your custom type information. The factory is called during the type extraction phase if either the corresponding type or a POJO\u0026rsquo;s field using this type has been annotated with the @org.apache.flink.api.common.typeinfo.TypeInfo annotation.
Type information factories can be used in both the Java and Scala API.
In a hierarchy of types the closest factory will be chosen while traversing upwards, however, a built-in factory has highest precedence. A factory has also higher precedence than Flink\u0026rsquo;s built-in types, therefore you should know what you are doing.
The following example shows how to annotate a custom type MyTuple and supply custom type information for it using a factory in Java.
The annotated custom type:
@TypeInfo(MyTupleTypeInfoFactory.class) public class MyTuple\u0026lt;T0, T1\u0026gt; { public T0 myfield0; public T1 myfield1; } The factory supplying custom type information:
public class MyTupleTypeInfoFactory extends TypeInfoFactory\u0026lt;MyTuple\u0026gt; { @Override public TypeInformation\u0026lt;MyTuple\u0026gt; createTypeInfo(Type t, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;\u0026gt; genericParameters) { return new MyTupleTypeInfo(genericParameters.get(\u0026#34;T0\u0026#34;), genericParameters.get(\u0026#34;T1\u0026#34;)); } } Instead of annotating the type itself, which may not be possible for third-party code, you can also annotate the usage of this type inside a valid Flink POJO like this:
public class MyPojo { public int id; @TypeInfo(MyTupleTypeInfoFactory.class) public MyTuple\u0026lt;Integer, String\u0026gt; tuple; } The method createTypeInfo(Type, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;\u0026gt;) creates type information for the type the factory is targeted for. The parameters provide additional information about the type itself as well as the type\u0026rsquo;s generic type parameters if available.
If your type contains generic parameters that might need to be derived from the input type of a Flink function, make sure to also implement org.apache.flink.api.common.typeinfo.TypeInformation#getGenericParameters for a bidirectional mapping of generic parameters to type information.
Back to top
`}),e.add({id:18,href:"/flink/flink-docs-master/docs/dev/datastream/operators/overview/",title:"Overview",section:"Operators",content:` Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
This section gives a description of the basic transformations, the effective physical partitioning after applying those as well as insights into Flink\u0026rsquo;s operator chaining.
DataStream Transformations # Map # DataStream → DataStream # Takes one element and produces one element. A map function that doubles the values of the input stream:
Java DataStream\u0026lt;Integer\u0026gt; dataStream = //... dataStream.map(new MapFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public Integer map(Integer value) throws Exception { return 2 * value; } }); Scala dataStream.map { x =\u0026gt; x * 2 } Python data_stream = env.from_collection(collection=[1, 2, 3, 4, 5]) data_stream.map(lambda x: 2 * x, output_type=Types.INT()) FlatMap # DataStream → DataStream # Takes one element and produces zero, one, or more elements. A flatmap function that splits sentences to words:
Java dataStream.flatMap(new FlatMapFunction\u0026lt;String, String\u0026gt;() { @Override public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) throws Exception { for(String word: value.split(\u0026#34; \u0026#34;)){ out.collect(word); } } }); Scala dataStream.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) } Python data_stream = env.from_collection(collection=[\u0026#39;hello apache flink\u0026#39;, \u0026#39;streaming compute\u0026#39;]) data_stream.flat_map(lambda x: x.split(\u0026#39; \u0026#39;), output_type=Types.STRING()) Filter # DataStream → DataStream # Evaluates a boolean function for each element and retains those for which the function returns true. A filter that filters out zero values:
Java dataStream.filter(new FilterFunction\u0026lt;Integer\u0026gt;() { @Override public boolean filter(Integer value) throws Exception { return value != 0; } }); Scala dataStream.filter { _ != 0 } Python data_stream = env.from_collection(collection=[0, 1, 2, 3, 4, 5]) data_stream.filter(lambda x: x != 0) KeyBy # DataStream → KeyedStream # Logically partitions a stream into disjoint partitions. All records with the same key are assigned to the same partition. Internally, keyBy() is implemented with hash partitioning. There are different ways to specify keys.
Java dataStream.keyBy(value -\u0026gt; value.getSomeKey()); dataStream.keyBy(value -\u0026gt; value.f0); Scala dataStream.keyBy(_.someKey) dataStream.keyBy(_._1) Python data_stream = env.from_collection(collection=[(1, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;b\u0026#39;)]) data_stream.key_by(lambda x: x[1], key_type=Types.STRING()) // Key by the result of KeySelector A type cannot be a key if:
it is a POJO type but does not override the hashCode() method and relies on the Object.hashCode() implementation. it is an array of any type. Reduce # KeyedStream → DataStream # A \u0026ldquo;rolling\u0026rdquo; reduce on a keyed data stream. Combines the current element with the last reduced value and emits the new value.
A reduce function that creates a stream of partial sums:
Java keyedStream.reduce(new ReduceFunction\u0026lt;Integer\u0026gt;() { @Override public Integer reduce(Integer value1, Integer value2) throws Exception { return value1 + value2; } }); Scala keyedStream.reduce { _ + _ } Python data_stream = env.from_collection(collection=[(1, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;a\u0026#39;), (4, \u0026#39;b\u0026#39;)], type_info=Types.TUPLE([Types.INT(), Types.STRING()])) data_stream.key_by(lambda x: x[1]).reduce(lambda a, b: (a[0] + b[0], b[1])) Window # KeyedStream → WindowedStream # Windows can be defined on already partitioned KeyedStreams. Windows group the data in each key according to some characteristic (e.g., the data that arrived within the last 5 seconds). See windows for a complete description of windows.
Java dataStream .keyBy(value -\u0026gt; value.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))); Scala dataStream .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) Python This feature is not yet supported in Python WindowAll # DataStream → AllWindowedStream # Windows can be defined on regular DataStreams. Windows group all the stream events according to some characteristic (e.g., the data that arrived within the last 5 seconds). See windows for a complete description of windows.
This is in many cases a non-parallel transformation. All records will be gathered in one task for the windowAll operator. Java dataStream .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))); Scala dataStream .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) Python This feature is not yet supported in Python Window Apply # WindowedStream → DataStream # AllWindowedStream → DataStream # Applies a general function to the window as a whole. Below is a function that manually sums the elements of a window.
If you are using a windowAll transformation, you need to use an AllWindowFunction instead. Java windowedStream.apply(new WindowFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;, Integer, Tuple, Window\u0026gt;() { public void apply (Tuple tuple, Window window, Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) throws Exception { int sum = 0; for (value t: values) { sum += t.f1; } out.collect (new Integer(sum)); } }); // applying an AllWindowFunction on non-keyed window stream allWindowedStream.apply (new AllWindowFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;, Integer, Window\u0026gt;() { public void apply (Window window, Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) throws Exception { int sum = 0; for (value t: values) { sum += t.f1; } out.collect (new Integer(sum)); } }); Scala windowedStream.apply { WindowFunction } // applying an AllWindowFunction on non-keyed window stream allWindowedStream.apply { AllWindowFunction } Python This feature is not yet supported in Python WindowReduce # WindowedStream → DataStream # Applies a functional reduce function to the window and returns the reduced value.
Java windowedStream.reduce (new ReduceFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt;() { public Tuple2\u0026lt;String, Integer\u0026gt; reduce(Tuple2\u0026lt;String, Integer\u0026gt; value1, Tuple2\u0026lt;String, Integer\u0026gt; value2) throws Exception { return new Tuple2\u0026lt;String,Integer\u0026gt;(value1.f0, value1.f1 + value2.f1); } }); Scala windowedStream.reduce { _ + _ } Python This feature is not yet supported in Python Union # DataStream* → DataStream # Union of two or more data streams creating a new stream containing all the elements from all the streams. Note: If you union a data stream with itself you will get each element twice in the resulting stream.
Java dataStream.union(otherStream1, otherStream2, ...); Scala dataStream.union(otherStream1, otherStream2, ...) Python data_stream.union(otherStream1, otherStream2, ...) Window Join # DataStream,DataStream → DataStream # Join two data streams on a given key and a common window.
Java dataStream.join(otherStream) .where(\u0026lt;key selector\u0026gt;).equalTo(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () {...}); Scala dataStream.join(otherStream) .where(\u0026lt;key selector\u0026gt;).equalTo(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply { ... } Python This feature is not yet supported in Python Interval Join # KeyedStream,KeyedStream → DataStream # Join two elements e1 and e2 of two keyed streams with a common key over a given time interval, so that e1.timestamp + lowerBound \u0026lt;= e2.timestamp \u0026lt;= e1.timestamp + upperBound.
Java // this will join the two streams so that // key1 == key2 \u0026amp;\u0026amp; leftTs - 2 \u0026lt; rightTs \u0026lt; leftTs + 2 keyedStream.intervalJoin(otherKeyedStream) .between(Time.milliseconds(-2), Time.milliseconds(2)) // lower and upper bound .upperBoundExclusive(true) // optional .lowerBoundExclusive(true) // optional .process(new IntervalJoinFunction() {...}); Scala // this will join the two streams so that // key1 == key2 \u0026amp;\u0026amp; leftTs - 2 \u0026lt; rightTs \u0026lt; leftTs + 2 keyedStream.intervalJoin(otherKeyedStream) .between(Time.milliseconds(-2), Time.milliseconds(2)) // lower and upper bound .upperBoundExclusive(true) // optional .lowerBoundExclusive(true) // optional .process(new IntervalJoinFunction() {...}) Python This feature is not yet supported in Python Window CoGroup # DataStream,DataStream → DataStream # Cogroups two data streams on a given key and a common window.
Java dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new CoGroupFunction () {...}); Scala dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply {} Python This feature is not yet supported in Python Connect # DataStream,DataStream → ConnectedStream # \u0026ldquo;Connects\u0026rdquo; two data streams retaining their types. Connect allowing for shared state between the two streams.
Java DataStream\u0026lt;Integer\u0026gt; someStream = //... DataStream\u0026lt;String\u0026gt; otherStream = //... ConnectedStreams\u0026lt;Integer, String\u0026gt; connectedStreams = someStream.connect(otherStream); Scala someStream : DataStream[Int] = ... otherStream : DataStream[String] = ... val connectedStreams = someStream.connect(otherStream) Python stream_1 = ... stream_2 = ... connected_streams = stream_1.connect(stream_2) CoMap, CoFlatMap # ConnectedStream → DataStream # Similar to map and flatMap on a connected data stream
Java connectedStreams.map(new CoMapFunction\u0026lt;Integer, String, Boolean\u0026gt;() { @Override public Boolean map1(Integer value) { return true; } @Override public Boolean map2(String value) { return false; } }); connectedStreams.flatMap(new CoFlatMapFunction\u0026lt;Integer, String, String\u0026gt;() { @Override public void flatMap1(Integer value, Collector\u0026lt;String\u0026gt; out) { out.collect(value.toString()); } @Override public void flatMap2(String value, Collector\u0026lt;String\u0026gt; out) { for (String word: value.split(\u0026#34; \u0026#34;)) { out.collect(word); } } }); Scala connectedStreams.map( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false ) connectedStreams.flatMap( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false ) Python class MyCoMapFunction(CoMapFunction): def map1(self, value): return value[0] + 1, value[1] def map2(self, value): return value[0], value[1] + \u0026#39;flink\u0026#39; class MyCoFlatMapFunction(CoFlatMapFunction): def flat_map1(self, value) for i in range(value[0]): yield i def flat_map2(self, value): yield value[0] + 1 connectedStreams.map(MyCoMapFunction()) connectedStreams.flat_map(MyCoFlatMapFunction()) Iterate # DataStream → IterativeStream → ConnectedStream # Creates a \u0026ldquo;feedback\u0026rdquo; loop in the flow, by redirecting the output of one operator to some previous operator. This is especially useful for defining algorithms that continuously update a model. The following code starts with a stream and applies the iteration body continuously. Elements that are greater than 0 are sent back to the feedback channel, and the rest of the elements are forwarded downstream.
Java IterativeStream\u0026lt;Long\u0026gt; iteration = initialStream.iterate(); DataStream\u0026lt;Long\u0026gt; iterationBody = iteration.map (/*do something*/); DataStream\u0026lt;Long\u0026gt; feedback = iterationBody.filter(new FilterFunction\u0026lt;Long\u0026gt;(){ @Override public boolean filter(Long value) throws Exception { return value \u0026gt; 0; } }); iteration.closeWith(feedback); DataStream\u0026lt;Long\u0026gt; output = iterationBody.filter(new FilterFunction\u0026lt;Long\u0026gt;(){ @Override public boolean filter(Long value) throws Exception { return value \u0026lt;= 0; } }); Scala initialStream.iterate { iteration =\u0026gt; { val iterationBody = iteration.map {/*do something*/} (iterationBody.filter(_ \u0026gt; 0), iterationBody.filter(_ \u0026lt;= 0)) } } Python This feature is not yet supported in Python Cache # DataStream → CachedDataStream # Cache the intermediate result of the transformation. Currently, only jobs that run with batch execution mode are supported. The cache intermediate result is generated lazily at the first time the intermediate result is computed so that the result can be reused by later jobs. If the cache is lost, it will be recomputed using the original transformations.
Java DataStream\u0026lt;Integer\u0026gt; dataStream = //... CachedDataStream\u0026lt;Integer\u0026gt; cachedDataStream = dataStream.cache(); cachedDataStream.print(); // Do anything with the cachedDataStream ... env.execute(); // Execute and create cache. cachedDataStream.print(); // Consume cached result. env.execute(); Scala val dataStream : DataStream[Int] = //... val cachedDataStream = dataStream.cache() cachedDataStream.print() // Do anything with the cachedDataStream ... env.execute() // Execute and create cache. cachedDataStream.print() // Consume cached result. env.execute() Python data_stream = ... # DataStream cached_data_stream = data_stream.cache() cached_data_stream.print() # ... env.execute() # Execute and create cache. cached_data_stream.print() # Consume cached result. env.execute() Physical Partitioning # Flink also gives low-level control (if desired) on the exact stream partitioning after a transformation, via the following functions.
Custom Partitioning # DataStream → DataStream # Uses a user-defined Partitioner to select the target task for each element.
Java dataStream.partitionCustom(partitioner, \u0026#34;someKey\u0026#34;); dataStream.partitionCustom(partitioner, 0); Scala dataStream.partitionCustom(partitioner, \u0026#34;someKey\u0026#34;) dataStream.partitionCustom(partitioner, 0) Python data_stream = env.from_collection(collection=[(2, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;b\u0026#39;)]) data_stream.partition_custom(lambda key, num_partition: key % partition, lambda x: x[0]) Random Partitioning # DataStream → DataStream # Partitions elements randomly according to a uniform distribution.
Java dataStream.shuffle(); Scala dataStream.shuffle() Python data_stream.shuffle() Rescaling # DataStream → DataStream # Partitions elements, round-robin, to a subset of downstream operations. This is useful if you want to have pipelines where you, for example, fan out from each parallel instance of a source to a subset of several mappers to distribute load but don\u0026rsquo;t want the full rebalance that rebalance() would incur. This would require only local data transfers instead of transferring data over network, depending on other configuration values such as the number of slots of TaskManagers.
The subset of downstream operations to which the upstream operation sends elements depends on the degree of parallelism of both the upstream and downstream operation. For example, if the upstream operation has parallelism 2 and the downstream operation has parallelism 6, then one upstream operation would distribute elements to three downstream operations while the other upstream operation would distribute to the other three downstream operations. If, on the other hand, the downstream operation has parallelism 2 while the upstream operation has parallelism 6 then three upstream operations would distribute to one downstream operation while the other three upstream operations would distribute to the other downstream operation.
In cases where the different parallelisms are not multiples of each other one or several downstream operations will have a differing number of inputs from upstream operations.
Please see this figure for a visualization of the connection pattern in the above example:
Java dataStream.rescale(); Scala dataStream.rescale() Python data_stream.rescale() Broadcasting # DataStream → DataStream # Broadcasts elements to every partition.
Java dataStream.broadcast(); Scala dataStream.broadcast() Python data_stream.broadcast() Task Chaining and Resource Groups # Chaining two subsequent transformations means co-locating them within the same thread for better performance. Flink by default chains operators if this is possible (e.g., two subsequent map transformations). The API gives fine-grained control over chaining if desired:
Use StreamExecutionEnvironment.disableOperatorChaining() if you want to disable chaining in the whole job. For more fine grained control, the following functions are available. Note that these functions can only be used right after a DataStream transformation as they refer to the previous transformation. For example, you can use someStream.map(...).startNewChain(), but you cannot use someStream.startNewChain().
A resource group is a slot in Flink, see slots. You can manually isolate operators in separate slots if desired.
Start New Chain # Begin a new chain, starting with this operator. The two mappers will be chained, and filter will not be chained to the first mapper.
Java someStream.filter(...).map(...).startNewChain().map(...); Scala someStream.filter(...).map(...).startNewChain().map(...) Python some_stream.filter(...).map(...).start_new_chain().map(...) Disable Chaining # Do not chain the map operator.
Java someStream.map(...).disableChaining(); Scala someStream.map(...).disableChaining() Python some_stream.map(...).disable_chaining() Set Slot Sharing Group # Set the slot sharing group of an operation. Flink will put operations with the same slot sharing group into the same slot while keeping operations that don\u0026rsquo;t have the slot sharing group in other slots. This can be used to isolate slots. The slot sharing group is inherited from input operations if all input operations are in the same slot sharing group. The name of the default slot sharing group is \u0026ldquo;default\u0026rdquo;, operations can explicitly be put into this group by calling slotSharingGroup(\u0026ldquo;default\u0026rdquo;).
Java someStream.filter(...).slotSharingGroup(\u0026#34;name\u0026#34;); Scala someStream.filter(...).slotSharingGroup(\u0026#34;name\u0026#34;) Python some_stream.filter(...).slot_sharing_group(\u0026#34;name\u0026#34;) Name And Description # Operators and job vertices in flink have a name and a description. Both name and description are introduction about what an operator or a job vertex is doing, but they are used differently.
The name of operator and job vertex will be used in web ui, thread name, logging, metrics, etc. The name of a job vertex is constructed based on the name of operators in it. The name needs to be as concise as possible to avoid high pressure on external systems.
The description will be used in the execution plan and displayed as the details of a job vertex in web UI. The description of a job vertex is constructed based on the description of operators in it. The description can contain detail information about operators to facilitate debugging at runtime.
Java someStream.filter(...).setName(\u0026#34;filter\u0026#34;).setDescription(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;); Scala someStream.filter(...).setName(\u0026#34;filter\u0026#34;).setDescription(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;) Python some_stream.filter(...).name(\u0026#34;filter\u0026#34;).set_description(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;) The format of description of a job vertex is a tree format string by default. Users can set pipeline.vertex-description-mode to CASCADING, if they want to set description to be the cascading format as in former versions.
Operators generated by Flink SQL will have a name consisted by type of operator and id, and a detailed description, by default. Users can set table.optimizer.simplify-operator-name-enabled to false, if they want to set name to be the detailed description as in former versions.
When the topology of the pipeline is complex, users can add a topological index in the name of vertex by set pipeline.vertex-name-include-index-prefix to true, so that we can easily find the vertex in the graph according to logs or metrics tags.
`}),e.add({id:19,href:"/flink/flink-docs-master/docs/dev/datastream/overview/",title:"Overview",section:"DataStream API",content:` Flink DataStream API Programming Guide # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.
In order to create your own Flink DataStream program, we encourage you to start with anatomy of a Flink Program and gradually add your own stream transformations. The remaining sections act as references for additional operations and advanced features.
What is a DataStream? # The DataStream API gets its name from the special DataStream class that is used to represent a collection of data in a Flink program. You can think of them as immutable collections of data that can contain duplicates. This data can either be finite or unbounded, the API that you use to work on them is the same.
A DataStream is similar to a regular Java Collection in terms of usage but is quite different in some key ways. They are immutable, meaning that once they are created you cannot add or remove elements. You can also not simply inspect the elements inside but only work on them using the DataStream API operations, which are also called transformations.
You can create an initial DataStream by adding a source in a Flink program. Then you can derive new streams from this and combine them by using API methods such as map, filter, and so on.
Anatomy of a Flink Program # Flink programs look like regular programs that transform DataStreams. Each program consists of the same basic parts:
Obtain an execution environment, Load/create the initial data, Specify transformations on this data, Specify where to put the results of your computations, Trigger the program execution Java We will now give an overview of each of those steps, please refer to the respective sections for more details. Note that all core classes of the Java DataStream API can be found in org.apache.flink.streaming.api .
The StreamExecutionEnvironment is the basis for all Flink programs. You can obtain one using these static methods on StreamExecutionEnvironment:
getExecutionEnvironment(); createLocalEnvironment(); createRemoteEnvironment(String host, int port, String... jarFiles); Typically, you only need to use getExecutionEnvironment(), since this will do the right thing depending on the context: if you are executing your program inside an IDE or as a regular Java program it will create a local environment that will execute your program on your local machine. If you created a JAR file from your program, and invoke it through the command line, the Flink cluster manager will execute your main method and getExecutionEnvironment() will return an execution environment for executing your program on a cluster.
For specifying data sources the execution environment has several methods to read from files using various methods: you can just read them line by line, as CSV files, or using any of the other provided sources. To just read a text file as a sequence of lines, you can use:
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; text = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;); This will give you a DataStream on which you can then apply transformations to create new derived DataStreams.
You apply transformations by calling methods on DataStream with a transformation functions. For example, a map transformation looks like this:
DataStream\u0026lt;String\u0026gt; input = ...; DataStream\u0026lt;Integer\u0026gt; parsed = input.map(new MapFunction\u0026lt;String, Integer\u0026gt;() { @Override public Integer map(String value) { return Integer.parseInt(value); } }); This will create a new DataStream by converting every String in the original collection to an Integer.
Once you have a DataStream containing your final results, you can write it to an outside system by creating a sink. These are just some example methods for creating a sink:
writeAsText(String path); print(); Scala We will now give an overview of each of those steps, please refer to the respective sections for more details. Note that all core classes of the Scala DataStream API can be found in org.apache.flink.streaming.api.scala .
The StreamExecutionEnvironment is the basis for all Flink programs. You can obtain one using these static methods on StreamExecutionEnvironment:
getExecutionEnvironment() createLocalEnvironment() createRemoteEnvironment(host: String, port: Int, jarFiles: String*) Typically, you only need to use getExecutionEnvironment(), since this will do the right thing depending on the context: if you are executing your program inside an IDE or as a regular Java program it will create a local environment that will execute your program on your local machine. If you created a JAR file from your program, and invoke it through the command line, the Flink cluster manager will execute your main method and getExecutionEnvironment() will return an execution environment for executing your program on a cluster.
For specifying data sources the execution environment has several methods to read from files using various methods: you can just read them line by line, as CSV files, or using any of the other provided sources. To just read a text file as a sequence of lines, you can use:
val env = StreamExecutionEnvironment.getExecutionEnvironment() val text: DataStream[String] = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;) This will give you a DataStream on which you can then apply transformations to create new derived DataStreams.
You apply transformations by calling methods on DataStream with a transformation functions. For example, a map transformation looks like this:
val input: DataSet[String] = ... val mapped = input.map { x =\u0026gt; x.toInt } This will create a new DataStream by converting every String in the original collection to an Integer.
Once you have a DataStream containing your final results, you can write it to an outside system by creating a sink. These are just some example methods for creating a sink:
writeAsText(path: String) print() Once you specified the complete program you need to trigger the program execution by calling execute() on the StreamExecutionEnvironment. Depending on the type of the ExecutionEnvironment the execution will be triggered on your local machine or submit your program for execution on a cluster.
The execute() method will wait for the job to finish and then return a JobExecutionResult, this contains execution times and accumulator results.
If you don\u0026rsquo;t want to wait for the job to finish, you can trigger asynchronous job execution by calling executeAsync() on the StreamExecutionEnvironment. It will return a JobClient with which you can communicate with the job you just submitted. For instance, here is how to implement the semantics of execute() by using executeAsync().
final JobClient jobClient = env.executeAsync(); final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult().get(); That last part about program execution is crucial to understanding when and how Flink operations are executed. All Flink programs are executed lazily: When the program\u0026rsquo;s main method is executed, the data loading and transformations do not happen directly. Rather, each operation is created and added to a dataflow graph. The operations are actually executed when the execution is explicitly triggered by an execute() call on the execution environment. Whether the program is executed locally or on a cluster depends on the type of execution environment.
The lazy evaluation lets you construct sophisticated programs that Flink executes as one holistically planned unit.
Back to top
Example Program # The following program is a complete, working example of streaming window word count application, that counts the words coming from a web socket in 5 second windows. You can copy \u0026amp; paste the code to run it locally.
Java import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class WindowWordCount { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dataStream = env .socketTextStream(\u0026#34;localhost\u0026#34;, 9999) .flatMap(new Splitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); dataStream.print(); env.execute(\u0026#34;Window WordCount\u0026#34;); } public static class Splitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String sentence, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) throws Exception { for (String word: sentence.split(\u0026#34; \u0026#34;)) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(word, 1)); } } } } Scala import org.apache.flink.streaming.api.scala._ import org.apache.flink.streaming.api.windowing.time.Time object WindowWordCount { def main(args: Array[String]) { val env = StreamExecutionEnvironment.getExecutionEnvironment val text = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .keyBy(_._1) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1) counts.print() env.execute(\u0026#34;Window Stream WordCount\u0026#34;) } } To run the example program, start the input stream with netcat first from a terminal:
nc -lk 9999 Just type some words hitting return for a new word. These will be the input to the word count program. If you want to see counts greater than 1, type the same word again and again within 5 seconds (increase the window size from 5 seconds if you cannot type that fast ☺).
Back to top
Data Sources # Java Sources are where your program reads its input from. You can attach a source to your program by using StreamExecutionEnvironment.addSource(sourceFunction). Flink comes with a number of pre-implemented source functions, but you can always write your own custom sources by implementing the SourceFunction for non-parallel sources, or by implementing the ParallelSourceFunction interface or extending the RichParallelSourceFunction for parallel sources.
There are several predefined stream sources accessible from the StreamExecutionEnvironment:
File-based:
readTextFile(path) - Reads text files, i.e. files that respect the TextInputFormat specification, line-by-line and returns them as Strings.
readFile(fileInputFormat, path) - Reads (once) files as dictated by the specified file input format.
readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - This is the method called internally by the two previous ones. It reads files in the path based on the given fileInputFormat. Depending on the provided watchType, this source may periodically monitor (every interval ms) the path for new data (FileProcessingMode.PROCESS_CONTINUOUSLY), or process once the data currently in the path and exit (FileProcessingMode.PROCESS_ONCE). Using the pathFilter, the user can further exclude files from being processed.
IMPLEMENTATION:
Under the hood, Flink splits the file reading process into two sub-tasks, namely directory monitoring and data reading. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, non-parallel (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the watchType), find the files to be processed, divide them in splits, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.
IMPORTANT NOTES:
If the watchType is set to FileProcessingMode.PROCESS_CONTINUOUSLY, when a file is modified, its contents are re-processed entirely. This can break the \u0026ldquo;exactly-once\u0026rdquo; semantics, as appending data at the end of a file will lead to all its contents being re-processed.
If the watchType is set to FileProcessingMode.PROCESS_ONCE, the source scans the path once and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.
Socket-based:
socketTextStream - Reads from a socket. Elements can be separated by a delimiter. Collection-based:
fromCollection(Collection) - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.
fromCollection(Iterator, Class) - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.
fromElements(T ...) - Creates a data stream from the given sequence of objects. All objects must be of the same type.
fromParallelCollection(SplittableIterator, Class) - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
generateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.
Custom:
addSource - Attach a new source function. For example, to read from Apache Kafka you can use addSource(new FlinkKafkaConsumer\u0026lt;\u0026gt;(...)). See connectors for more details. Scala Sources are where your program reads its input from. You can attach a source to your program by using StreamExecutionEnvironment.addSource(sourceFunction). Flink comes with a number of pre-implemented source functions, but you can always write your own custom sources by implementing the SourceFunction for non-parallel sources, or by implementing the ParallelSourceFunction interface or extending the RichParallelSourceFunction for parallel sources.
There are several predefined stream sources accessible from the StreamExecutionEnvironment:
File-based:
readTextFile(path) - Reads text files, i.e. files that respect the TextInputFormat specification, line-by-line and returns them as Strings.
readFile(fileInputFormat, path) - Reads (once) files as dictated by the specified file input format.
readFile(fileInputFormat, path, watchType, interval, pathFilter) - This is the method called internally by the two previous ones. It reads files in the path based on the given fileInputFormat. Depending on the provided watchType, this source may periodically monitor (every interval ms) the path for new data (FileProcessingMode.PROCESS_CONTINUOUSLY), or process once the data currently in the path and exit (FileProcessingMode.PROCESS_ONCE). Using the pathFilter, the user can further exclude files from being processed.
IMPLEMENTATION:
Under the hood, Flink splits the file reading process into two sub-tasks, namely directory monitoring and data reading. Each of these sub-tasks is implemented by a separate entity. Monitoring is implemented by a single, non-parallel (parallelism = 1) task, while reading is performed by multiple tasks running in parallel. The parallelism of the latter is equal to the job parallelism. The role of the single monitoring task is to scan the directory (periodically or only once depending on the watchType), find the files to be processed, divide them in splits, and assign these splits to the downstream readers. The readers are the ones who will read the actual data. Each split is read by only one reader, while a reader can read multiple splits, one-by-one.
IMPORTANT NOTES:
If the watchType is set to FileProcessingMode.PROCESS_CONTINUOUSLY, when a file is modified, its contents are re-processed entirely. This can break the \u0026ldquo;exactly-once\u0026rdquo; semantics, as appending data at the end of a file will lead to all its contents being re-processed.
If the watchType is set to FileProcessingMode.PROCESS_ONCE, the source scans the path once and exits, without waiting for the readers to finish reading the file contents. Of course the readers will continue reading until all file contents are read. Closing the source leads to no more checkpoints after that point. This may lead to slower recovery after a node failure, as the job will resume reading from the last checkpoint.
Socket-based:
socketTextStream - Reads from a socket. Elements can be separated by a delimiter. Collection-based:
fromCollection(Seq) - Creates a data stream from the Java Java.util.Collection. All elements in the collection must be of the same type.
fromCollection(Iterator) - Creates a data stream from an iterator. The class specifies the data type of the elements returned by the iterator.
fromElements(elements: _*) - Creates a data stream from the given sequence of objects. All objects must be of the same type.
fromParallelCollection(SplittableIterator) - Creates a data stream from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
generateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.
Custom:
addSource - Attach a new source function. For example, to read from Apache Kafka you can use addSource(new FlinkKafkaConsumer\u0026lt;\u0026gt;(...)). See connectors for more details. Back to top
DataStream Transformations # Please see operators for an overview of the available stream transformations.
Back to top
Data Sinks # Java Data sinks consume DataStreams and forward them to files, sockets, external systems, or print them. Flink comes with a variety of built-in output formats that are encapsulated behind operations on the DataStreams:
writeAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element.
writeAsCsv(...) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects.
print() / printToErr() - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.
writeUsingOutputFormat() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.
writeToSocket - Writes elements to a socket according to a SerializationSchema
addSink - Invokes a custom sink function. Flink comes bundled with connectors to other systems (such as Apache Kafka) that are implemented as sink functions.
Scala Data sinks consume DataStreams and forward them to files, sockets, external systems, or print them. Flink comes with a variety of built-in output formats that are encapsulated behind operations on the DataStreams:
writeAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element.
writeAsCsv(...) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects.
print() / printToErr() - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output.
writeUsingOutputFormat() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion.
writeToSocket - Writes elements to a socket according to a SerializationSchema
addSink - Invokes a custom sink function. Flink comes bundled with connectors to other systems (such as Apache Kafka) that are implemented as sink functions.
Note that the write*() methods on DataStream are mainly intended for debugging purposes. They are not participating in Flink\u0026rsquo;s checkpointing, this means these functions usually have at-least-once semantics. The data flushing to the target system depends on the implementation of the OutputFormat. This means that not all elements send to the OutputFormat are immediately showing up in the target system. Also, in failure cases, those records might be lost.
For reliable, exactly-once delivery of a stream into a file system, use the FileSink. Also, custom implementations through the .addSink(...) method can participate in Flink\u0026rsquo;s checkpointing for exactly-once semantics.
Back to top
Iterations # Java Iterative streaming programs implement a step function and embed it into an IterativeStream. As a DataStream program may never finish, there is no maximum number of iterations. Instead, you need to specify which part of the stream is fed back to the iteration and which part is forwarded downstream using a side output or a filter. Here, we show an example using filters. First, we define an IterativeStream
IterativeStream\u0026lt;Integer\u0026gt; iteration = input.iterate(); Then, we specify the logic that will be executed inside the loop using a series of transformations (here a simple map transformation)
DataStream\u0026lt;Integer\u0026gt; iterationBody = iteration.map(/* this is executed many times */); To close an iteration and define the iteration tail, call the closeWith(feedbackStream) method of the IterativeStream. The DataStream given to the closeWith function will be fed back to the iteration head. A common pattern is to use a filter to separate the part of the stream that is fed back, and the part of the stream which is propagated forward. These filters can, e.g., define the \u0026ldquo;termination\u0026rdquo; logic, where an element is allowed to propagate downstream rather than being fed back.
iteration.closeWith(iterationBody.filter(/* one part of the stream */)); DataStream\u0026lt;Integer\u0026gt; output = iterationBody.filter(/* some other part of the stream */); For example, here is program that continuously subtracts 1 from a series of integers until they reach zero:
DataStream\u0026lt;Long\u0026gt; someIntegers = env.generateSequence(0, 1000); IterativeStream\u0026lt;Long\u0026gt; iteration = someIntegers.iterate(); DataStream\u0026lt;Long\u0026gt; minusOne = iteration.map(new MapFunction\u0026lt;Long, Long\u0026gt;() { @Override public Long map(Long value) throws Exception { return value - 1 ; } }); DataStream\u0026lt;Long\u0026gt; stillGreaterThanZero = minusOne.filter(new FilterFunction\u0026lt;Long\u0026gt;() { @Override public boolean filter(Long value) throws Exception { return (value \u0026gt; 0); } }); iteration.closeWith(stillGreaterThanZero); DataStream\u0026lt;Long\u0026gt; lessThanZero = minusOne.filter(new FilterFunction\u0026lt;Long\u0026gt;() { @Override public boolean filter(Long value) throws Exception { return (value \u0026lt;= 0); } }); Scala Iterative streaming programs implement a step function and embed it into an IterativeStream. As a DataStream program may never finish, there is no maximum number of iterations. Instead, you need to specify which part of the stream is fed back to the iteration and which part is forwarded downstream using a side output or a filter. Here, we show an example iteration where the body (the part of the computation that is repeated) is a simple map transformation, and the elements that are fed back are distinguished by the elements that are forwarded downstream using filters.
val iteratedStream = someDataStream.iterate( iteration =\u0026gt; { val iterationBody = iteration.map(/* this is executed many times */) (iterationBody.filter(/* one part of the stream */), iterationBody.filter(/* some other part of the stream */)) }) For example, here is program that continuously subtracts 1 from a series of integers until they reach zero:
val someIntegers: DataStream[Long] = env.generateSequence(0, 1000) val iteratedStream = someIntegers.iterate( iteration =\u0026gt; { val minusOne = iteration.map( v =\u0026gt; v - 1) val stillGreaterThanZero = minusOne.filter (_ \u0026gt; 0) val lessThanZero = minusOne.filter(_ \u0026lt;= 0) (stillGreaterThanZero, lessThanZero) } ) Back to top
Execution Parameters # The StreamExecutionEnvironment contains the ExecutionConfig which allows to set job specific configuration values for the runtime.
Please refer to execution configuration for an explanation of most parameters. These parameters pertain specifically to the DataStream API:
setAutoWatermarkInterval(long milliseconds): Set the interval for automatic watermark emission. You can get the current value with long getAutoWatermarkInterval() Back to top
Fault Tolerance # State \u0026amp; Checkpointing describes how to enable and configure Flink\u0026rsquo;s checkpointing mechanism.
Controlling Latency # By default, elements are not transferred on the network one-by-one (which would cause unnecessary network traffic) but are buffered. The size of the buffers (which are actually transferred between machines) can be set in the Flink config files. While this method is good for optimizing throughput, it can cause latency issues when the incoming stream is not fast enough. To control throughput and latency, you can use env.setBufferTimeout(timeoutMillis) on the execution environment (or on individual operators) to set a maximum wait time for the buffers to fill up. After this time, the buffers are sent automatically even if they are not full. The default value for this timeout is 100 ms.
Usage:
Java LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); env.setBufferTimeout(timeoutMillis); env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis); Scala val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment env.setBufferTimeout(timeoutMillis) env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis) To maximize throughput, set setBufferTimeout(-1) which will remove the timeout and buffers will only be flushed when they are full. To minimize latency, set the timeout to a value close to 0 (for example 5 or 10 ms). A buffer timeout of 0 should be avoided, because it can cause severe performance degradation.
Back to top
Debugging # Before running a streaming program in a distributed cluster, it is a good idea to make sure that the implemented algorithm works as desired. Hence, implementing data analysis programs is usually an incremental process of checking results, debugging, and improving.
Flink provides features to significantly ease the development process of data analysis programs by supporting local debugging from within an IDE, injection of test data, and collection of result data. This section give some hints how to ease the development of Flink programs.
Local Execution Environment # A LocalStreamEnvironment starts a Flink system within the same JVM process it was created in. If you start the LocalEnvironment from an IDE, you can set breakpoints in your code and easily debug your program.
A LocalEnvironment is created and used as follows:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); DataStream\u0026lt;String\u0026gt; lines = env.addSource(/* some source */); // build your program env.execute(); Scala val env = StreamExecutionEnvironment.createLocalEnvironment() val lines = env.addSource(/* some source */) // build your program env.execute() Collection Data Sources # Flink provides special data sources which are backed by Java collections to ease testing. Once a program has been tested, the sources and sinks can be easily replaced by sources and sinks that read from / write to external systems.
Collection data sources can be used as follows:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); // Create a DataStream from a list of elements DataStream\u0026lt;Integer\u0026gt; myInts = env.fromElements(1, 2, 3, 4, 5); // Create a DataStream from any Java collection List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; data = ... DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myTuples = env.fromCollection(data); // Create a DataStream from an Iterator Iterator\u0026lt;Long\u0026gt; longIt = ...; DataStream\u0026lt;Long\u0026gt; myLongs = env.fromCollection(longIt, Long.class); Scala val env = StreamExecutionEnvironment.createLocalEnvironment() // Create a DataStream from a list of elements val myInts = env.fromElements(1, 2, 3, 4, 5) // Create a DataStream from any Collection val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // Create a DataStream from an Iterator val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) Note: Currently, the collection data source requires that data types and iterators implement Serializable. Furthermore, collection data sources can not be executed in parallel ( parallelism = 1).
Iterator Data Sink # Flink also provides a sink to collect DataStream results for testing and debugging purposes. It can be used as follows:
Java DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myResult = ...; Iterator\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myOutput = myResult.collectAsync(); Scala val myResult: DataStream[(String, Int)] = ... val myOutput: Iterator[(String, Int)] = myResult.collectAsync() Where to go next? # Operators: Specification of available streaming operators. Event Time: Introduction to Flink\u0026rsquo;s notion of time. State \u0026amp; Fault Tolerance: Explanation of how to develop stateful applications. Connectors: Description of available input and output connectors. Back to top
`}),e.add({id:20,href:"/flink/flink-docs-master/docs/dev/python/datastream/operators/overview/",title:"Overview",section:"Operators",content:` Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
DataStream Transformations # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., mapping, filtering, reducing). Please see operators for an overview of the available transformations in Python DataStream API.
Functions # Transformations accept user-defined functions as input to define the functionality of the transformations. The following section describes different ways of defining Python user-defined functions in Python DataStream API.
Implementing Function Interfaces # Different Function interfaces are provided for different transformations in the Python DataStream API. For example, MapFunction is provided for the map transformation, FilterFunction is provided for the filter transformation, etc. Users can implement the corresponding Function interface according to the type of the transformation. Take MapFunction for instance:
# Implementing MapFunction class MyMapFunction(MapFunction): def map(self, value): return value + 1 data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(MyMapFunction(), output_type=Types.INT()) Lambda Function # As shown in the following example, the transformations can also accept a lambda function to define the functionality of the transformation:
data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(lambda x: x + 1, output_type=Types.INT()) Note ConnectedStream.map() and ConnectedStream.flat_map() do not support lambda function and must accept CoMapFunction and CoFlatMapFunction separately.
Python Function # Users could also use Python function to define the functionality of the transformation:
def my_map_func(value): return value + 1 data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(my_map_func, output_type=Types.INT()) Output Type # Users could specify the output type information of the transformation explicitly in Python DataStream API. If not specified, the output type will be Types.PICKLED_BYTE_ARRAY by default, and the result data will be serialized using pickle serializer. For more details about the pickle serializer, please refer to Pickle Serialization.
Generally, the output type needs to be specified in the following scenarios.
Convert DataStream into Table # from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment def data_stream_api_demo(): env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;number-of-rows\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) ds = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.INT(), Types.STRING()])) def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.TUPLE([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: (i[0] + j[0], i[1])) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_sink ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = t_env.from_data_stream(ds) table_result = table.execute_insert(\u0026#34;my_sink\u0026#34;) # 1）wait for job finishes and only used in local execution, otherwise, it may happen that the script exits with the job is still running # 2）should be removed when submitting the job to a remote cluster such as YARN, standalone, K8s etc in detach mode table_result.wait() if __name__ == \u0026#39;__main__\u0026#39;: data_stream_api_demo() The output type must be specified for the flat_map operation in the above example which will be used as the output type of the reduce operation implicitly. The reason is that t_env.from_data_stream(ds) requires the output type of ds must be a composite type.
Write DataStream to Sink # from pyflink.common.typeinfo import Types def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds.map(lambda i: (i[0] + 1, i[1]), Types.TUPLE([Types.INT(), Types.STRING()])) \\ .sink_to(...) Generally, the output type needs to be specified for the map operation in the above example if the sink only accepts special kinds of data, e.g. Row, etc.
Operator Chaining # By default, multiple non-shuffle Python functions will be chained together to avoid the serialization and deserialization and improve the performance. There are also cases where you may want to disable the chaining, e.g., there is a flatmap function which will produce a large number of elements for each input element and disabling the chaining allows to process its output in a different parallelism.
Operator chaining could be disabled in one of the following ways:
Disable chaining with following operators by adding a key_by operation, shuffle operation, rescale operation, rebalance operation or partition_custom operation after the current operator. Disable chaining with preceding operators by applying a start_new_chain operation for the current operator. Disable chaining with preceding and following operators by applying a disable_chaining operation for the current operator. Disable chaining of two operators by setting different parallelisms or different slot sharing group for them. You could also disable all the operator chaining via configuration python.operator-chaining.enabled. Bundling Python Functions # To run Python functions in any non-local mode, it is strongly recommended bundling your Python functions definitions using the config option python-files, if your Python functions live outside the file where the main() function is defined. Otherwise, you may run into ModuleNotFoundError: No module named 'my_function' if you define Python functions in a file called my_function.py.
Loading resources in Python Functions # There are scenarios when you want to load some resources in Python functions first, then running computation over and over again, without having to re-load the resources. For example, you may want to load a large deep learning model only once, then run batch prediction against the model multiple times.
Overriding the open method inherited from the base class Function is exactly what you need.
class Predict(MapFunction): def open(self, runtime_context: RuntimeContext): import pickle with open(\u0026#34;resources.zip/resources/model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: self.model = pickle.load(f) def eval(self, x): return self.model.predict(x) `}),e.add({id:21,href:"/flink/flink-docs-master/docs/dev/python/overview/",title:"Overview",section:"Python API",content:` Python API # PyFlink is a Python API for Apache Flink that allows you to build scalable batch and streaming workloads, such as real-time data processing pipelines, large-scale exploratory data analysis, Machine Learning (ML) pipelines and ETL processes. If you\u0026rsquo;re already familiar with Python and libraries such as Pandas, then PyFlink makes it simpler to leverage the full capabilities of the Flink ecosystem. Depending on the level of abstraction you need, there are two different APIs that can be used in PyFlink:
The PyFlink Table API allows you to write powerful relational queries in a way that is similar to using SQL or working with tabular data in Python. At the same time, the PyFlink DataStream API gives you lower-level control over the core building blocks of Flink, state and time, to build more complex stream processing use cases. Try PyFlink # If you’re interested in playing around with Flink, try one of our tutorials:
Intro to PyFlink DataStream API Intro to PyFlink Table API For more examples, you can also refer to PyFlink Examples Explore PyFlink # The reference documentation covers all the details. Some starting points:
PyFlink DataStream API PyFlink Table API \u0026amp; SQL Get Help with PyFlink # If you get stuck, check out our community support resources. In particular, Apache Flink’s user mailing list is consistently ranked as one of the most active of any Apache project, and is a great way to get help quickly.
`}),e.add({id:22,href:"/flink/flink-docs-master/docs/dev/python/table/udfs/overview/",title:"Overview",section:"User Defined Functions",content:` User-defined Functions # PyFlink Table API empowers users to do data transformations with Python user-defined functions.
Currently, it supports two kinds of Python user-defined functions: the general Python user-defined functions which process data one row at a time and vectorized Python user-defined functions which process data one batch at a time.
Bundling UDFs # To run Python UDFs (as well as Pandas UDFs) in any non-local mode, it is strongly recommended bundling your Python UDF definitions using the config option python-files, if your Python UDFs live outside the file where the main() function is defined. Otherwise, you may run into ModuleNotFoundError: No module named 'my_udf' if you define Python UDFs in a file called my_udf.py.
Loading resources in UDFs # There are scenarios when you want to load some resources in UDFs first, then running computation (i.e., eval) over and over again, without having to re-load the resources. For example, you may want to load a large deep learning model only once, then run batch prediction against the model multiple times.
Overriding the open method of UserDefinedFunction is exactly what you need.
class Predict(ScalarFunction): def open(self, function_context): import pickle with open(\u0026#34;resources.zip/resources/model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: self.model = pickle.load(f) def eval(self, x): return self.model.predict(x) predict = udf(Predict(), result_type=DataTypes.DOUBLE(), func_type=\u0026#34;pandas\u0026#34;) Testing User-Defined Functions # Suppose you have defined a Python user-defined function as following:
add = udf(lambda i, j: i + j, result_type=DataTypes.BIGINT()) To unit test it, you need to extract the original Python function using ._func and then unit test it:
f = add._func assert f(1, 2) == 3 `}),e.add({id:23,href:"/flink/flink-docs-master/docs/dev/table/concepts/overview/",title:"Overview",section:"Streaming Concepts",content:` Streaming Concepts # Flink\u0026rsquo;s Table API and SQL support are unified APIs for batch and stream processing. This means that Table API and SQL queries have the same semantics regardless whether their input is bounded batch input or unbounded stream input.
The following pages explain concepts, practical limitations, and stream-specific configuration parameters of Flink\u0026rsquo;s relational APIs on streaming data.
State Management # Table programs that run in streaming mode leverage all capabilities of Flink as a stateful stream processor.
In particular, a table program can be configured with a state backend and various checkpointing options for handling different requirements regarding state size and fault tolerance. It is possible to take a savepoint of a running Table API \u0026amp; SQL pipeline and to restore the application\u0026rsquo;s state at a later point in time.
State Usage # Due to the declarative nature of Table API \u0026amp; SQL programs, it is not always obvious where and how much state is used within a pipeline. The planner decides whether state is necessary to compute a correct result. A pipeline is optimized to claim as little state as possible given the current set of optimizer rules.
Conceptually, source tables are never kept entirely in state. An implementer deals with logical tables (i.e. dynamic tables). Their state requirements depend on the used operations. Queries such as SELECT ... FROM ... WHERE which only consist of field projections or filters are usually stateless pipelines. However, operations such as joins, aggregations, or deduplications require keeping intermediate results in a fault-tolerant storage for which Flink\u0026rsquo;s state abstractions are used.
Please refer to the individual operator documentation for more details about how much state is required and how to limit a potentially ever-growing state size. For example, a regular SQL join of two tables requires the operator to keep both input tables in state entirely. For correct SQL semantics, the runtime needs to assume that a matching could occur at any point in time from both sides. Flink provides optimized window and interval joins that aim to keep the state size small by exploiting the concept of watermarks.
Another example is the following query that computes the number of clicks per session.
SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId; The sessionId attribute is used as a grouping key and the continuous query maintains a count for each sessionId it observes. The sessionId attribute is evolving over time and sessionId values are only active until the session ends, i.e., for a limited period of time. However, the continuous query cannot know about this property of sessionId and expects that every sessionId value can occur at any point of time. It maintains a count for each observed sessionId value. Consequently, the total state size of the query is continuously growing as more and more sessionId values are observed.
Idle State Retention Time # The Idle State Retention Time parameter table.exec.state.ttl defines for how long the state of a key is retained without being updated before it is removed. For the previous example query, the count of asessionId would be removed as soon as it has not been updated for the configured period of time.
By removing the state of a key, the continuous query completely forgets that it has seen this key before. If a record with a key, whose state has been removed before, is processed, the record will be treated as if it was the first record with the respective key. For the example above this means that the count of a sessionId would start again at 0.
Stateful Upgrades and Evolution # Table programs that are executed in streaming mode are intended as standing queries which means they are defined once and are continuously evaluated as static end-to-end pipelines.
In case of stateful pipelines, any change to both the query or Flink\u0026rsquo;s planner might lead to a completely different execution plan. This makes stateful upgrades and the evolution of table programs challenging at the moment. The community is working on improving those shortcomings.
For example, by adding a filter predicate, the optimizer might decide to reorder joins or change the schema of an intermediate operator. This prevents restoring from a savepoint due to either changed topology or different column layout within the state of an operator.
The query implementer must ensure that the optimized plans before and after the change are compatible. Use the EXPLAIN command in SQL or table.explain() in Table API to get insights.
Since new optimizer rules are continuously added, and operators become more efficient and specialized, also the upgrade to a newer Flink version could lead to incompatible plans.
Currently, the framework cannot guarantee that state can be mapped from a savepoint to a new table operator topology.
In other words: Savepoints are only supported if both the query and the Flink version remain constant.
Since the community rejects contributions that modify the optimized plan and the operator topology in a patch version (e.g. from 1.13.1 to 1.13.2), it should be safe to upgrade a Table API \u0026amp; SQL pipeline to a newer bug fix release. However, major-minor upgrades from (e.g. from 1.12 to 1.13) are not supported.
For both shortcomings (i.e. modified query and modified Flink version), we recommend to investigate whether the state of an updated table program can be \u0026ldquo;warmed up\u0026rdquo; (i.e. initialized) with historical data again before switching to real-time data. The Flink community is working on a hybrid source to make this switching as convenient as possible.
Where to go next? # Dynamic Tables: Describes the concept of dynamic tables. Time attributes: Explains time attributes and how time attributes are handled in Table API \u0026amp; SQL. Versioned Tables: Describes the Temporal Table concept. Joins in Continuous Queries: Different supported types of Joins in Continuous Queries. Query configuration: Lists Table API \u0026amp; SQL specific configuration options. Back to top
`}),e.add({id:24,href:"/flink/flink-docs-master/docs/dev/table/functions/overview/",title:"Overview",section:"Functions",content:` Functions # Flink Table API \u0026amp; SQL empowers users to do data transformations with functions.
Types of Functions # There are two dimensions to classify functions in Flink.
One dimension is system (or built-in) functions v.s. catalog functions. System functions have no namespace and can be referenced with just their names. Catalog functions belong to a catalog and database therefore they have catalog and database namespaces, they can be referenced by either fully/partially qualified name (catalog.db.func or db.func) or just the function name.
The other dimension is temporary functions v.s. persistent functions. Temporary functions are volatile and only live up to lifespan of a session, they are always created by users. Persistent functions live across lifespan of sessions, they are either provided by the system or persisted in catalogs.
The two dimensions give Flink users 4 categories of functions:
Temporary system functions System functions Temporary catalog functions Catalog functions Referencing Functions # There are two ways users can reference a function in Flink - referencing function precisely or ambiguously.
Precise Function Reference # Precise function reference empowers users to use catalog functions specifically, and across catalog and across database, e.g. select mycatalog.mydb.myfunc(x) from mytable and select mydb.myfunc(x) from mytable.
This is only supported starting from Flink 1.10.
Ambiguous Function Reference # In ambiguous function reference, users just specify the function\u0026rsquo;s name in SQL query, e.g. select myfunc(x) from mytable.
Function Resolution Order # The resolution order only matters when there are functions of different types but the same name, e.g. when there’re three functions all named “myfunc” but are of temporary catalog, catalog, and system function respectively. If there’s no function name collision, functions will just be resolved to the sole one.
Precise Function Reference # Because system functions don’t have namespaces, a precise function reference in Flink must be pointing to either a temporary catalog function or a catalog function.
The resolution order is:
Temporary catalog function Catalog function Ambiguous Function Reference # The resolution order is:
Temporary system function System function Temporary catalog function, in the current catalog and current database of the session Catalog function, in the current catalog and current database of the session `}),e.add({id:25,href:"/flink/flink-docs-master/docs/dev/table/overview/",title:"Overview",section:"Table API \u0026 SQL",content:` Table API \u0026amp; SQL # Apache Flink features two relational APIs - the Table API and SQL - for unified stream and batch processing. The Table API is a language-integrated query API for Java, Scala, and Python that allows the composition of queries from relational operators such as selection, filter, and join in a very intuitive way. Flink\u0026rsquo;s SQL support is based on Apache Calcite which implements the SQL standard. Queries specified in either interface have the same semantics and specify the same result regardless of whether the input is continuous (streaming) or bounded (batch).
The Table API and SQL interfaces integrate seamlessly with each other and Flink\u0026rsquo;s DataStream API. You can easily switch between all APIs and libraries which build upon them. For instance, you can detect patterns from a table using MATCH_RECOGNIZE clause and later use the DataStream API to build alerting based on the matched patterns.
Table Program Dependencies # You will need to add the Table API as a dependency to a project in order to use Table API \u0026amp; SQL for defining data pipelines.
For more information on how to configure these dependencies for Java and Scala, please refer to the project configuration section.
If you are using Python, please refer to the documentation on the Python API
Where to go next? # Concepts \u0026amp; Common API: Shared concepts and APIs of the Table API and SQL. Data Types: Lists pre-defined data types and their properties. Streaming Concepts: Streaming-specific documentation for the Table API or SQL such as configuration of time attributes and handling of updating results. Connect to External Systems: Available connectors and formats for reading and writing data to external systems. Table API: Supported operations and API for the Table API. SQL: Supported operations and syntax for SQL. Built-in Functions: Supported functions in Table API and SQL. SQL Client: Play around with Flink SQL and submit a table program to a cluster without programming knowledge. Back to top
`}),e.add({id:26,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/overview/",title:"Overview",section:"Queries",content:" Queries # SELECT statements and VALUES statements are specified with the sqlQuery() method of the TableEnvironment. The method returns the result of the SELECT statement (or the VALUES statements) as a Table. A Table can be used in subsequent SQL and Table API queries, be converted into a DataStream, or written to a TableSink. SQL and Table API queries can be seamlessly mixed and are holistically optimized and translated into a single program.\nIn order to access a table in a SQL query, it must be registered in the TableEnvironment. A table can be registered from a TableSource, Table, CREATE TABLE statement, DataStream. Alternatively, users can also register catalogs in a TableEnvironment to specify the location of the data sources.\nFor convenience, Table.toString() automatically registers the table under a unique name in its TableEnvironment and returns the name. So, Table objects can be directly inlined into SQL queries as shown in the examples below.\nNote: Queries that include unsupported SQL features cause a TableException. The supported features of SQL on batch and streaming tables are listed in the following sections.\nSpecifying a Query # The following examples show how to specify a SQL queries on registered and inlined tables.\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // ingest a DataStream from an external source DataStream\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; ds = env.addSource(...); // SQL query with an inlined (unregistered) table Table table = tableEnv.fromDataStream(ds, $(\u0026#34;user\u0026#34;), $(\u0026#34;product\u0026#34;), $(\u0026#34;amount\u0026#34;)); Table result = tableEnv.sqlQuery( \u0026#34;SELECT SUM(amount) FROM \u0026#34; + table + \u0026#34; WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // SQL query with a registered table // register the DataStream as view \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $(\u0026#34;user\u0026#34;), $(\u0026#34;product\u0026#34;), $(\u0026#34;amount\u0026#34;)); // run a SQL query on the Table and retrieve the result as a new Table Table result2 = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // create and register a TableSink final Schema schema = Schema.newBuilder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build(); final TableDescriptor sinkDescriptor = TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build(); tableEnv.createTemporaryTable(\u0026#34;RubberOrders\u0026#34;, sinkDescriptor); // run an INSERT SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // read a DataStream from an external source val ds: DataStream[(Long, String, Integer)] = env.addSource(...) // SQL query with an inlined (unregistered) table val table = ds.toTable(tableEnv, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) val result = tableEnv.sqlQuery( s\u0026#34;SELECT SUM(amount) FROM $table WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // SQL query with a registered table // register the DataStream under the name \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) // run a SQL query on the Table and retrieve the result as a new Table val result2 = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // create and register a TableSink val schema = Schema.newBuilder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build() val sinkDescriptor = TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build() tableEnv.createTemporaryTable(\u0026#34;RubberOrders\u0026#34;, sinkDescriptor) // run an INSERT SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) # SQL query with an inlined (unregistered) table # elements data type: BIGINT, STRING, BIGINT table = table_env.from_elements(..., [\u0026#39;user\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;amount\u0026#39;]) result = table_env \\ .sql_query(\u0026#34;SELECT SUM(amount) FROM %s WHERE product LIKE \u0026#39;%%Rubber%%\u0026#39;\u0026#34; % table) # create and register a TableSink schema = Schema.new_builder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build() sink_descriptor = TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build() t_env.create_temporary_table(\u0026#34;RubberOrders\u0026#34;, sink_descriptor) # run an INSERT SQL on the Table and emit the result to the TableSink table_env \\ .execute_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Back to top\nExecute a Query # A SELECT statement or a VALUES statement can be executed to collect the content to local through the TableEnvironment.executeSql() method. The method returns the result of the SELECT statement (or the VALUES statement) as a TableResult. Similar to a SELECT statement, a Table object can be executed using the Table.execute() method to collect the content of the query to the local client. TableResult.collect() method returns a closeable row iterator. The select job will not be finished unless all result data has been collected. We should actively close the job to avoid resource leak through the CloseableIterator#close() method. We can also print the select result to client console through the TableResult.print() method. The result data in TableResult can be accessed only once. Thus, collect() and print() must not be called after each other.\nTableResult.collect() and TableResult.print() have slightly different behaviors under different checkpointing settings (to enable checkpointing for a streaming job, see checkpointing config).\nFor batch jobs or streaming jobs without checkpointing, TableResult.collect() and TableResult.print() have neither exactly-once nor at-least-once guarantee. Query results are immediately accessible by the clients once they\u0026rsquo;re produced, but exceptions will be thrown when the job fails and restarts. For streaming jobs with exactly-once checkpointing, TableResult.collect() and TableResult.print() guarantee an end-to-end exactly-once record delivery. A result will be accessible by clients only after its corresponding checkpoint completes. For streaming jobs with at-least-once checkpointing, TableResult.collect() and TableResult.print() guarantee an end-to-end at-least-once record delivery. Query results are immediately accessible by the clients once they\u0026rsquo;re produced, but it is possible for the same result to be delivered multiple times. Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings); tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // execute SELECT statement TableResult tableResult1 = tableEnv.executeSql(\u0026#34;SELECT * FROM Orders\u0026#34;); // use try-with-resources statement to make sure the iterator will be closed automatically try (CloseableIterator\u0026lt;Row\u0026gt; it = tableResult1.collect()) { while(it.hasNext()) { Row row = it.next(); // handle row } } // execute Table TableResult tableResult2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM Orders\u0026#34;).execute(); tableResult2.print(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tableEnv = StreamTableEnvironment.create(env, settings) // enable checkpointing tableEnv.getConfig.set( ExecutionCheckpointingOptions.CHECKPOINTING_MODE, CheckpointingMode.EXACTLY_ONCE) tableEnv.getConfig.set( ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(10)) tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // execute SELECT statement val tableResult1 = tableEnv.executeSql(\u0026#34;SELECT * FROM Orders\u0026#34;) val it = tableResult1.collect() try while (it.hasNext) { val row = it.next // handle row } finally it.close() // close the iterator to avoid resource leak // execute Table val tableResult2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM Orders\u0026#34;).execute() tableResult2.print() Python env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env, settings) # enable checkpointing table_env.get_config().set(\u0026#34;execution.checkpointing.mode\u0026#34;, \u0026#34;EXACTLY_ONCE\u0026#34;) table_env.get_config().set(\u0026#34;execution.checkpointing.interval\u0026#34;, \u0026#34;10s\u0026#34;) table_env.execute_sql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) # execute SELECT statement table_result1 = table_env.execute_sql(\u0026#34;SELECT * FROM Orders\u0026#34;) table_result1.print() # execute Table table_result2 = table_env.sql_query(\u0026#34;SELECT * FROM Orders\u0026#34;).execute() table_result2.print() Back to top\nSyntax # Flink parses SQL using Apache Calcite, which supports standard ANSI SQL.\nThe following BNF-grammar describes the superset of supported SQL features in batch and streaming queries. The Operations section shows examples for the supported features and indicates which features are only supported for batch or streaming queries.\nGrammar ↕ query: values | WITH withItem [ , withItem ]* query | { select | selectWithoutFrom | query UNION [ ALL ] query | query EXCEPT query | query INTERSECT query } [ ORDER BY orderItem [, orderItem ]* ] [ LIMIT { count | ALL } ] [ OFFSET start { ROW | ROWS } ] [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY] withItem: name [ \u0026#39;(\u0026#39; column [, column ]* \u0026#39;)\u0026#39; ] AS \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; orderItem: expression [ ASC | DESC ] select: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } FROM tableExpression [ WHERE booleanExpression ] [ GROUP BY { groupItem [, groupItem ]* } ] [ HAVING booleanExpression ] [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ] selectWithoutFrom: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } projectItem: expression [ [ AS ] columnAlias ] | tableAlias . * tableExpression: tableReference [, tableReference ]* | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ] joinCondition: ON booleanExpression | USING \u0026#39;(\u0026#39; column [, column ]* \u0026#39;)\u0026#39; tableReference: tablePrimary [ matchRecognize ] [ [ AS ] alias [ \u0026#39;(\u0026#39; columnAlias [, columnAlias ]* \u0026#39;)\u0026#39; ] ] tablePrimary: [ TABLE ] tablePath [ dynamicTableOptions ] [systemTimePeriod] [[AS] correlationName] | LATERAL TABLE \u0026#39;(\u0026#39; functionName \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; \u0026#39;)\u0026#39; | [ LATERAL ] \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; | UNNEST \u0026#39;(\u0026#39; expression \u0026#39;)\u0026#39; tablePath: [ [ catalogName . ] databaseName . ] tableName systemTimePeriod: FOR SYSTEM_TIME AS OF dateTimeExpression dynamicTableOptions: /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral values: VALUES expression [, expression ]* groupItem: expression | \u0026#39;(\u0026#39; \u0026#39;)\u0026#39; | \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | CUBE \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | ROLLUP \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | GROUPING SETS \u0026#39;(\u0026#39; groupItem [, groupItem ]* \u0026#39;)\u0026#39; windowRef: windowName | windowSpec windowSpec: [ windowName ] \u0026#39;(\u0026#39; [ ORDER BY orderItem [, orderItem ]* ] [ PARTITION BY expression [, expression ]* ] [ RANGE numericOrIntervalExpression {PRECEDING} | ROWS numericExpression {PRECEDING} ] \u0026#39;)\u0026#39; matchRecognize: MATCH_RECOGNIZE \u0026#39;(\u0026#39; [ PARTITION BY expression [, expression ]* ] [ ORDER BY orderItem [, orderItem ]* ] [ MEASURES measureColumn [, measureColumn ]* ] [ ONE ROW PER MATCH ] [ AFTER MATCH ( SKIP TO NEXT ROW | SKIP PAST LAST ROW | SKIP TO FIRST variable | SKIP TO LAST variable | SKIP TO variable ) ] PATTERN \u0026#39;(\u0026#39; pattern \u0026#39;)\u0026#39; [ WITHIN intervalLiteral ] DEFINE variable AS condition [, variable AS condition ]* \u0026#39;)\u0026#39; measureColumn: expression AS alias pattern: patternTerm [ \u0026#39;|\u0026#39; patternTerm ]* patternTerm: patternFactor [ patternFactor ]* patternFactor: variable [ patternQuantifier ] patternQuantifier: \u0026#39;*\u0026#39; | \u0026#39;*?\u0026#39; | \u0026#39;+\u0026#39; | \u0026#39;+?\u0026#39; | \u0026#39;?\u0026#39; | \u0026#39;??\u0026#39; | \u0026#39;{\u0026#39; { [ minRepeat ], [ maxRepeat ] } \u0026#39;}\u0026#39; [\u0026#39;?\u0026#39;] | \u0026#39;{\u0026#39; repeat \u0026#39;}\u0026#39; Flink SQL uses a lexical policy for identifier (table, attribute, function names) similar to Java:\nThe case of identifiers is preserved whether or not they are quoted. After which, identifiers are matched case-sensitively. Unlike Java, back-ticks allow identifiers to contain non-alphanumeric characters (e.g. SELECT a AS `my field` FROM t). String literals must be enclosed in single quotes (e.g., SELECT 'Hello World'). Duplicate a single quote for escaping (e.g., SELECT 'It''s me').\nFlink SQL\u0026gt; SELECT \u0026#39;Hello World\u0026#39;, \u0026#39;It\u0026#39;\u0026#39;s me\u0026#39;; +-------------+---------+ | EXPR$0 | EXPR$1 | +-------------+---------+ | Hello World | It\u0026#39;s me | +-------------+---------+ 1 row in set Unicode characters are supported in string literals. If explicit unicode code points are required, use the following syntax:\nUse the backslash (\\) as escaping character (default): SELECT U\u0026amp;'\\263A' Use a custom escaping character: SELECT U\u0026amp;'#263A' UESCAPE '#' Back to top\nOperations # WITH clause SELECT \u0026amp; WHERE SELECT DISTINCT Windowing TVF Window Aggregation Group Aggregation Over Aggregation Joins Set Operations ORDER BY clause LIMIT clause Top-N Window Top-N Deduplication Pattern Recognition Back to top\n"}),e.add({id:27,href:"/flink/flink-docs-master/docs/learn-flink/overview/",title:"Overview",section:"Learn Flink",content:` Learn Flink: Hands-On Training # Goals and Scope of this Training # This training presents an introduction to Apache Flink that includes just enough to get you started writing scalable streaming ETL, analytics, and event-driven applications, while leaving out a lot of (ultimately important) details. The focus is on providing straightforward introductions to Flink’s APIs for managing state and time, with the expectation that having mastered these fundamentals, you’ll be much better equipped to pick up the rest of what you need to know from the more detailed reference documentation. The links at the end of each section will lead you to where you can learn more.
Specifically, you will learn:
how to implement streaming data processing pipelines how and why Flink manages state how to use event time to consistently compute accurate analytics how to build event-driven applications on continuous streams how Flink is able to provide fault-tolerant, stateful stream processing with exactly-once semantics This training focuses on four critical concepts: continuous processing of streaming data, event time, stateful stream processing, and state snapshots. This page introduces these concepts.
Note: Accompanying this training is a set of hands-on exercises that will guide you through learning how to work with the concepts being presented. A link to the relevant exercise is provided at the end of each section. Stream Processing # Streams are data’s natural habitat. Whether it is events from web servers, trades from a stock exchange, or sensor readings from a machine on a factory floor, data is created as part of a stream. But when you analyze data, you can either organize your processing around bounded or unbounded streams, and which of these paradigms you choose has profound consequences.
Batch processing is the paradigm at work when you process a bounded data stream. In this mode of operation you can choose to ingest the entire dataset before producing any results, which means that it is possible, for example, to sort the data, compute global statistics, or produce a final report that summarizes all of the input.
Stream processing, on the other hand, involves unbounded data streams. Conceptually, at least, the input may never end, and so you are forced to continuously process the data as it arrives.
In Flink, applications are composed of streaming dataflows that may be transformed by user-defined operators. These dataflows form directed graphs that start with one or more sources, and end in one or more sinks.
Often there is a one-to-one correspondence between the transformations in the program and the operators in the dataflow. Sometimes, however, one transformation may consist of multiple operators.
An application may consume real-time data from streaming sources such as message queues or distributed logs, like Apache Kafka or Kinesis. But flink can also consume bounded, historic data from a variety of data sources. Similarly, the streams of results being produced by a Flink application can be sent to a wide variety of systems that can be connected as sinks.
Parallel Dataflows # Programs in Flink are inherently parallel and distributed. During execution, a stream has one or more stream partitions, and each operator has one or more operator subtasks. The operator subtasks are independent of one another, and execute in different threads and possibly on different machines or containers.
The number of operator subtasks is the parallelism of that particular operator. Different operators of the same program may have different levels of parallelism.
Streams can transport data between two operators in a one-to-one (or forwarding) pattern, or in a redistributing pattern:
One-to-one streams (for example between the Source and the map() operators in the figure above) preserve the partitioning and ordering of the elements. That means that subtask[1] of the map() operator will see the same elements in the same order as they were produced by subtask[1] of the Source operator.
Redistributing streams (as between map() and keyBy/window above, as well as between keyBy/window and Sink) change the partitioning of streams. Each operator subtask sends data to different target subtasks, depending on the selected transformation. Examples are keyBy() (which re-partitions by hashing the key), broadcast(), or rebalance() (which re-partitions randomly). In a redistributing exchange the ordering among the elements is only preserved within each pair of sending and receiving subtasks (for example, subtask[1] of map() and subtask[2] of keyBy/window). So, for example, the redistribution between the keyBy/window and the Sink operators shown above introduces non-determinism regarding the order in which the aggregated results for different keys arrive at the Sink.
Timely Stream Processing # For most streaming applications it is very valuable to be able re-process historic data with the same code that is used to process live data – and to produce deterministic, consistent results, regardless.
It can also be crucial to pay attention to the order in which events occurred, rather than the order in which they are delivered for processing, and to be able to reason about when a set of events is (or should be) complete. For example, consider the set of events involved in an e-commerce transaction, or financial trade.
These requirements for timely stream processing can be met by using event time timestamps that are recorded in the data stream, rather than using the clocks of the machines processing the data.
Stateful Stream Processing # Flink’s operations can be stateful. This means that how one event is handled can depend on the accumulated effect of all the events that came before it. State may be used for something simple, such as counting events per minute to display on a dashboard, or for something more complex, such as computing features for a fraud detection model.
A Flink application is run in parallel on a distributed cluster. The various parallel instances of a given operator will execute independently, in separate threads, and in general will be running on different machines.
The set of parallel instances of a stateful operator is effectively a sharded key-value store. Each parallel instance is responsible for handling events for a specific group of keys, and the state for those keys is kept locally.
The diagram below shows a job running with a parallelism of two across the first three operators in the job graph, terminating in a sink that has a parallelism of one. The third operator is stateful, and you can see that a fully-connected network shuffle is occurring between the second and third operators. This is being done to partition the stream by some key, so that all of the events that need to be processed together, will be.
State is always accessed locally, which helps Flink applications achieve high throughput and low-latency. You can choose to keep state on the JVM heap, or if it is too large, in efficiently organized on-disk data structures.
Fault Tolerance via State Snapshots # Flink is able to provide fault-tolerant, exactly-once semantics through a combination of state snapshots and stream replay. These snapshots capture the entire state of the distributed pipeline, recording offsets into the input queues as well as the state throughout the job graph that has resulted from having ingested the data up to that point. When a failure occurs, the sources are rewound, the state is restored, and processing is resumed. As depicted above, these state snapshots are captured asynchronously, without impeding the ongoing processing.
`}),e.add({id:28,href:"/flink/flink-docs-master/docs/libs/gelly/overview/",title:"Overview",section:"Graphs",content:` Gelly: Flink Graph API # Gelly is a Graph API for Flink. It contains a set of methods and utilities which aim to simplify the development of graph analysis applications in Flink. In Gelly, graphs can be transformed and modified using high-level functions similar to the ones provided by the batch processing API. Gelly provides methods to create, transform and modify graphs, as well as a library of graph algorithms.
Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators Bipartite Graphs Using Gelly # Gelly is currently part of the libraries Maven project. All relevant classes are located in the org.apache.flink.graph package.
Add the following dependency to your pom.xml to use Gelly.
Java \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-gelly\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Scala \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-gelly-scala_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note that Gelly is not part of the binary distribution. See linking for instructions on packaging Gelly libraries into Flink user programs.
The remaining sections provide a description of available methods and present several examples of how to use Gelly and how to mix it with the Flink DataSet API.
Running Gelly Examples # The Gelly library jars are provided in the Flink distribution in the opt directory (for versions older than Flink 1.2 these can be manually downloaded from Maven Central). To run the Gelly examples the flink-gelly (for Java) or flink-gelly-scala (for Scala) jar must be copied to Flink\u0026rsquo;s lib directory.
cp opt/flink-gelly_*.jar lib/ cp opt/flink-gelly-scala_*.jar lib/ Gelly\u0026rsquo;s examples jar includes drivers for each of the library methods and is provided in the examples directory. After configuring and starting the cluster, list the available algorithm classes:
./bin/start-cluster.sh ./bin/flink run examples/gelly/flink-gelly-examples_*.jar The Gelly drivers can generate graph data or read the edge list from a CSV file (each node in a cluster must have access to the input file). The algorithm description, available inputs and outputs, and configuration are displayed when an algorithm is selected. Print usage for JaccardIndex:
./bin/flink run examples/gelly/flink-gelly-examples_*.jar --algorithm JaccardIndex Display graph metrics for a million vertex graph:
./bin/flink run examples/gelly/flink-gelly-examples_*.jar \\ --algorithm GraphMetrics --order directed \\ --input RMatGraph --type integer --scale 20 --simplify directed \\ --output print The size of the graph is adjusted by the --scale and --edge_factor parameters. The library generator provides access to additional configuration to adjust the power-law skew and random noise.
Sample social network data is provided by the Stanford Network Analysis Project. The com-lj data set is a good starter size. Run a few algorithms and monitor the job progress in Flink\u0026rsquo;s Web UI:
wget -O - http://snap.stanford.edu/data/bigdata/communities/com-lj.ungraph.txt.gz | gunzip -c \u0026gt; com-lj.ungraph.txt ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm GraphMetrics --order undirected \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output print ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm ClusteringCoefficient --order undirected \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output hash ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm JaccardIndex \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output hash Please submit feature requests and report issues on the user mailing list or Flink Jira. We welcome suggestions for new algorithms and features as well as code contributions.
Back to top
`}),e.add({id:29,href:"/flink/flink-docs-master/docs/dev/configuration/",title:"Project Configuration",section:"Application Development",content:" "}),e.add({id:30,href:"/flink/flink-docs-master/docs/deployment/memory/mem_setup/",title:"Set up Flink's Process Memory",section:"Memory Configuration",content:` Set up Flink\u0026rsquo;s Process Memory # Apache Flink provides efficient workloads on top of the JVM by tightly controlling the memory usage of its various components. While the community strives to offer sensible defaults to all configurations, the full breadth of applications that users deploy on Flink means this isn\u0026rsquo;t always possible. To provide the most production value to our users, Flink allows both high-level and fine-grained tuning of memory allocation within clusters. To optimize memory requirements, check the network memory tuning guide.
The further described memory configuration is applicable starting with the release version 1.10 for TaskManager and 1.11 for JobManager processes. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.10 and 1.11 releases.
Configure Total Memory # The total process memory of Flink JVM processes consists of memory consumed by the Flink application (total Flink memory) and by the JVM to run the process. The total Flink memory consumption includes usage of JVM Heap and Off-heap (Direct or Native) memory.
The simplest way to setup memory in Flink is to configure either of the two following options:
Component Option for TaskManager Option for JobManager Total Flink memory taskmanager.memory.flink.size jobmanager.memory.flink.size Total process memory taskmanager.memory.process.size jobmanager.memory.process.size For local execution, see detailed information for TaskManager and JobManager processes. The rest of the memory components will be adjusted automatically, based on default values or additionally configured options. See also how to set up other components for TaskManager and JobManager memory.
Configuring total Flink memory is better suited for standalone deployments where you want to declare how much memory is given to Flink itself. The total Flink memory splits up into JVM Heap and Off-heap memory. See also how to configure memory for standalone deployments.
If you configure total process memory you declare how much memory in total should be assigned to the Flink JVM process. For the containerized deployments it corresponds to the size of the requested container, see also how to configure memory for containers (Kubernetes or Yarn).
Another way to set up the memory is to configure the required internal components of the total Flink memory which are specific to the concrete Flink process. Check how to configure them for TaskManager and for JobManager.
One of the three mentioned ways has to be used to configure Flink’s memory (except for local execution), or the Flink startup will fail. This means that one of the following option subsets, which do not have default values, have to be configured explicitly:
for TaskManager: for JobManager: taskmanager.memory.flink.size jobmanager.memory.flink.size taskmanager.memory.process.size jobmanager.memory.process.size taskmanager.memory.task.heap.size and taskmanager.memory.managed.size jobmanager.memory.heap.size Explicitly configuring both total process memory and total Flink memory is not recommended. It may lead to deployment failures due to potential memory configuration conflicts. Configuring other memory components also requires caution as it can produce further configuration conflicts. JVM Parameters # Flink explicitly adds the following memory related JVM arguments while starting its processes, based on the configured or derived memory component sizes:
JVM Arguments Value for TaskManager Value for JobManager -Xmx and -Xms Framework + Task Heap Memory JVM Heap Memory (*) -XX:MaxDirectMemorySize(always added only for TaskManager, see note for JobManager) Framework + Task Off-heap (**) + Network Memory Off-heap Memory (**),(***) -XX:MaxMetaspaceSize JVM Metaspace JVM Metaspace (*) Keep in mind that you might not be able to use the full amount of heap memory depending on the GC algorithm used. Some GC algorithms allocate a certain amount of heap memory for themselves. This will lead to a different maximum being returned by the Heap metrics. (**) Notice, that the native non-direct usage of memory in user code can be also accounted for as a part of the off-heap memory. (***) The JVM Direct memory limit is added for JobManager process only if the corresponding option jobmanager.memory.enable-jvm-direct-memory-limit is set. Check also the detailed memory model for TaskManager and JobManager to understand how to configure the relevant components.
Capped Fractionated Components # This section describes the configuration details of options which can be a fraction of some other memory size while being constrained by a min-max range:
JVM Overhead can be a fraction of the total process memory Network memory can be a fraction of the total Flink memory (only for TaskManager) Check also the detailed memory model for TaskManager and JobManager to understand how to configure the relevant components.
The size of those components always has to be between its maximum and minimum value, otherwise Flink startup will fail. The maximum and minimum values have defaults or can be explicitly set by corresponding configuration options. For example, if you only set the following memory options:
total Process memory = 1000MB, JVM Overhead min = 64MB, JVM Overhead max = 128MB, JVM Overhead fraction = 0.1 then the JVM Overhead will be 1000MB x 0.1 = 100MB which is within the range 64-128MB.
Notice if you configure the same maximum and minimum value it effectively fixes the size to that value.
If you do not explicitly configure the component memory, then Flink will use the fraction to calculate the memory size based on the total memory. The calculated value is capped by its corresponding min/max options. For example, if only the following memory options are set:
total Process memory = 1000MB, JVM Overhead min = 128MB, JVM Overhead max = 256MB, JVM Overhead fraction = 0.1 then the JVM Overhead will be 128MB because the size derived from fraction is 100MB, and it is less than the minimum.
It can also happen that the fraction is ignored if the sizes of the total memory and its other components are defined. In this case, the JVM Overhead is the rest of the total memory. The derived value still has to be within its min/max range otherwise the configuration fails. For example, suppose only the following memory options are set:
total Process memory = 1000MB, task heap = 100MB, (similar example can be for JVM Heap in the JobManager) JVM Overhead min = 64MB, JVM Overhead max = 256MB, JVM Overhead fraction = 0.1 All other components of the total Process memory have default values, including the default Managed Memory fraction (or Off-heap memory in the JobManager). Then the JVM Overhead is not the fraction (1000MB x 0.1 = 100MB), but the rest of the total Process memory which will either be within the range 64-256MB or fail.
`}),e.add({id:31,href:"/flink/flink-docs-master/docs/dev/table/sql/overview/",title:"SQL",section:"SQL",content:` SQL # This page describes the SQL language supported in Flink, including Data Definition Language (DDL), Data Manipulation Language (DML) and Query Language. Flink’s SQL support is based on Apache Calcite which implements the SQL standard.
This page lists all the supported statements supported in Flink SQL for now:
SELECT (Queries) CREATE TABLE, CATALOG, DATABASE, VIEW, FUNCTION DROP TABLE, DATABASE, VIEW, FUNCTION ALTER TABLE, DATABASE, FUNCTION ANALYZE TABLE INSERT DESCRIBE EXPLAIN USE SHOW LOAD UNLOAD Data Types # Please see the dedicated page about data types.
Generic types and (nested) composite types (e.g., POJOs, tuples, rows, Scala case classes) can be fields of a row as well.
Fields of composite types with arbitrary nesting can be accessed with value access functions.
Generic types are treated as a black box and can be passed on or processed by user-defined functions.
For DDLs, we support full data types defined in page Data Types.
Notes: Some of the data types are not supported in SQL queries yet (i.e. in cast expressions or literals). E.g. STRING, BYTES, RAW, TIME(p) WITHOUT TIME ZONE, TIME(p) WITH LOCAL TIME ZONE, TIMESTAMP(p) WITHOUT TIME ZONE, TIMESTAMP(p) WITH LOCAL TIME ZONE, ARRAY, MULTISET, ROW.
Back to top
Reserved Keywords # Although not every SQL feature is implemented yet, some string combinations are already reserved as keywords for future use. If you want to use one of the following strings as a field name, make sure to surround them with backticks (e.g. \`value\`, \`count\`).
A, ABS, ABSOLUTE, ACTION, ADA, ADD, ADMIN, AFTER, ALL, ALLOCATE, ALLOW, ALTER, ALWAYS, AND, ANALYZE, ANY, ARE, ARRAY, AS, ASC, ASENSITIVE, ASSERTION, ASSIGNMENT, ASYMMETRIC, AT, ATOMIC, ATTRIBUTE, ATTRIBUTES, AUTHORIZATION, AVG, BEFORE, BEGIN, BERNOULLI, BETWEEN, BIGINT, BINARY, BIT, BLOB, BOOLEAN, BOTH, BREADTH, BY, BYTES, C, CALL, CALLED, CARDINALITY, CASCADE, CASCADED, CASE, CAST, CATALOG, CATALOG_NAME, CEIL, CEILING, CENTURY, CHAIN, CHAR, CHARACTER, CHARACTERISTICS, CHARACTERS, CHARACTER_LENGTH, CHARACTER_SET_CATALOG, CHARACTER_SET_NAME, CHARACTER_SET_SCHEMA, CHAR_LENGTH, CHECK, CLASS_ORIGIN, CLOB, CLOSE, COALESCE, COBOL, COLLATE, COLLATION, COLLATION_CATALOG, COLLATION_NAME, COLLATION_SCHEMA, COLLECT, COLUMN, COLUMNS, COLUMN_NAME, COMMAND_FUNCTION, COMMAND_FUNCTION_CODE, COMMIT, COMMITTED, CONDITION, CONDITION_NUMBER, CONNECT, CONNECTION, CONNECTION_NAME, CONSTRAINT, CONSTRAINTS, CONSTRAINT_CATALOG, CONSTRAINT_NAME, CONSTRAINT_SCHEMA, CONSTRUCTOR, CONTAINS, CONTINUE, CONVERT, CORR, CORRESPONDING, COUNT, COVAR_POP, COVAR_SAMP, CREATE, CROSS, CUBE, CUME_DIST, CURRENT, CURRENT_CATALOG, CURRENT_DATE, CURRENT_DEFAULT_TRANSFORM_GROUP, CURRENT_PATH, CURRENT_ROLE, CURRENT_SCHEMA, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_TRANSFORM_GROUP_FOR_TYPE, CURRENT_USER, CURSOR, CURSOR_NAME, CYCLE, DATA, DATABASE, DATE, DATETIME_INTERVAL_CODE, DATETIME_INTERVAL_PRECISION, DAY, DEALLOCATE, DEC, DECADE, DECIMAL, DECLARE, DEFAULT, DEFAULTS, DEFERRABLE, DEFERRED, DEFINED, DEFINER, DEGREE, DELETE, DENSE_RANK, DEPTH, DEREF, DERIVED, DESC, DESCRIBE, DESCRIPTION, DESCRIPTOR, DETERMINISTIC, DIAGNOSTICS, DISALLOW, DISCONNECT, DISPATCH, DISTINCT, DOMAIN, DOUBLE, DOW, DOY, DROP, DYNAMIC, DYNAMIC_FUNCTION, DYNAMIC_FUNCTION_CODE, EACH, ELEMENT, ELSE, END, END-EXEC, EPOCH, EQUALS, ESCAPE, EVERY, EXCEPT, EXCEPTION, EXCLUDE, EXCLUDING, EXEC, EXECUTE, EXISTS, EXP, EXPLAIN, EXTEND, EXTERNAL, EXTRACT, FALSE, FETCH, FILTER, FINAL, FIRST, FIRST_VALUE, FLOAT, FLOOR, FOLLOWING, FOR, FOREIGN, FORTRAN, FOUND, FRAC_SECOND, FREE, FROM, FULL, FUNCTION, FUSION, G, GENERAL, GENERATED, GET, GLOBAL, GO, GOTO, GRANT, GRANTED, GROUP, GROUPING, HAVING, HIERARCHY, HOLD, HOUR, IDENTITY, IMMEDIATE, IMPLEMENTATION, IMPORT, IN, INCLUDING, INCREMENT, INDICATOR, INITIALLY, INNER, INOUT, INPUT, INSENSITIVE, INSERT, INSTANCE, INSTANTIABLE, INT, INTEGER, INTERSECT, INTERSECTION, INTERVAL, INTO, INVOKER, IS, ISOLATION, JAVA, JOIN, K, KEY, KEY_MEMBER, KEY_TYPE, LABEL, LANGUAGE, LARGE, LAST, LAST_VALUE, LATERAL, LEADING, LEFT, LENGTH, LEVEL, LIBRARY, LIKE, LIMIT, LN, LOCAL, LOCALTIME, LOCALTIMESTAMP, LOCATOR, LOWER, M, MAP, MATCH, MATCHED, MAX, MAXVALUE, MEMBER, MERGE, MESSAGE_LENGTH, MESSAGE_OCTET_LENGTH, MESSAGE_TEXT, METHOD, MICROSECOND, MILLENNIUM, MIN, MINUTE, MINVALUE, MOD, MODIFIES, MODULE, MODULES, MONTH, MORE, MULTISET, MUMPS, NAME, NAMES, NATIONAL, NATURAL, NCHAR, NCLOB, NESTING, NEW, NEXT, NO, NONE, NORMALIZE, NORMALIZED, NOT, NULL, NULLABLE, NULLIF, NULLS, NUMBER, NUMERIC, OBJECT, OCTETS, OCTET_LENGTH, OF, OFFSET, OLD, ON, ONLY, OPEN, OPTION, OPTIONS, OR, ORDER, ORDERING, ORDINALITY, OTHERS, OUT, OUTER, OUTPUT, OVER, OVERLAPS, OVERLAY, OVERRIDING, PAD, PARAMETER, PARAMETER_MODE, PARAMETER_NAME, PARAMETER_ORDINAL_POSITION, PARAMETER_SPECIFIC_CATALOG, PARAMETER_SPECIFIC_NAME, PARAMETER_SPECIFIC_SCHEMA, PARTIAL, PARTITION, PASCAL, PASSTHROUGH, PATH, PERCENTILE_CONT, PERCENTILE_DISC, PERCENT_RANK, PLACING, PLAN, PLI, POSITION, POWER, PRECEDING, PRECISION, PREPARE, PRESERVE, PRIMARY, PRIOR, PRIVILEGES, PROCEDURE, PUBLIC, QUARTER, RANGE, RANK, RAW, READ, READS, REAL, RECURSIVE, REF, REFERENCES, REFERENCING, REGR_AVGX, REGR_AVGY, REGR_COUNT, REGR_INTERCEPT, REGR_R2, REGR_SLOPE, REGR_SXX, REGR_SXY, REGR_SYY, RELATIVE, RELEASE, REPEATABLE, RESET, RESTART, RESTRICT, RESULT, RETURN, RETURNED_CARDINALITY, RETURNED_LENGTH, RETURNED_OCTET_LENGTH, RETURNED_SQLSTATE, RETURNS, REVOKE, RIGHT, ROLE, ROLLBACK, ROLLUP, ROUTINE, ROUTINE_CATALOG, ROUTINE_NAME, ROUTINE_SCHEMA, ROW, ROWS, ROW_COUNT, ROW_NUMBER, SAVEPOINT, SCALE, SCHEMA, SCHEMA_NAME, SCOPE, SCOPE_CATALOGS, SCOPE_NAME, SCOPE_SCHEMA, SCROLL, SEARCH, SECOND, SECTION, SECURITY, SELECT, SELF, SENSITIVE, SEQUENCE, SERIALIZABLE, SERVER, SERVER_NAME, SESSION, SESSION_USER, SET, SETS, SIMILAR, SIMPLE, SIZE, SMALLINT, SOME, SOURCE, SPACE, SPECIFIC, SPECIFICTYPE, SPECIFIC_NAME, SQL, SQLEXCEPTION, SQLSTATE, SQLWARNING, SQL_TSI_DAY, SQL_TSI_FRAC_SECOND, SQL_TSI_HOUR, SQL_TSI_MICROSECOND, SQL_TSI_MINUTE, SQL_TSI_MONTH, SQL_TSI_QUARTER, SQL_TSI_SECOND, SQL_TSI_WEEK, SQL_TSI_YEAR, SQRT, START, STATE, STATEMENT, STATIC, STATISTICS, STDDEV_POP, STDDEV_SAMP, STREAM, STRING, STRUCTURE, STYLE, SUBCLASS_ORIGIN, SUBMULTISET, SUBSTITUTE, SUBSTRING, SUM, SYMMETRIC, SYSTEM, SYSTEM_USER, TABLE, TABLESAMPLE, TABLE_NAME, TEMPORARY, THEN, TIES, TIME, TIMESTAMP, TIMESTAMPADD, TIMESTAMPDIFF, TIMEZONE_HOUR, TIMEZONE_MINUTE, TINYINT, TO, TOP_LEVEL_COUNT, TRAILING, TRANSACTION, TRANSACTIONS_ACTIVE, TRANSACTIONS_COMMITTED, TRANSACTIONS_ROLLED_BACK, TRANSFORM, TRANSFORMS, TRANSLATE, TRANSLATION, TREAT, TRIGGER, TRIGGER_CATALOG, TRIGGER_NAME, TRIGGER_SCHEMA, TRIM, TRUE, TYPE, UESCAPE, UNBOUNDED, UNCOMMITTED, UNDER, UNION, UNIQUE, UNKNOWN, UNNAMED, UNNEST, UPDATE, UPPER, UPSERT, USAGE, USER, USER_DEFINED_TYPE_CATALOG, USER_DEFINED_TYPE_CODE, USER_DEFINED_TYPE_NAME, USER_DEFINED_TYPE_SCHEMA, USING, VALUE, VALUES, VARBINARY, VARCHAR, VARYING, VAR_POP, VAR_SAMP, VERSION, VIEW, WEEK, WHEN, WHENEVER, WHERE, WIDTH_BUCKET, WINDOW, WITH, WITHIN, WITHOUT, WORK, WRAPPER, WRITE, XML, YEAR, ZONE
Back to top
`}),e.add({id:32,href:"/flink/flink-docs-master/docs/ops/state/",title:"State \u0026 Fault Tolerance",section:"Operations",content:""}),e.add({id:33,href:"/flink/flink-docs-master/docs/try-flink/",title:"Try Flink",section:"Docs",content:" "}),e.add({id:34,href:"/flink/flink-docs-master/docs/deployment/filesystems/common/",title:"Common Configurations",section:"File Systems",content:` Common Configurations # Apache Flink provides several standard configuration settings that work across all file system implementations.
Default File System # A default scheme (and authority) is used if paths to files do not explicitly specify a file system scheme (and authority).
fs.default-scheme: \u0026lt;default-fs\u0026gt; For example, if the default file system configured as fs.default-scheme: hdfs://localhost:9000/, then a file path of /user/hugo/in.txt is interpreted as hdfs://localhost:9000/user/hugo/in.txt.
Connection limiting # You can limit the total number of connections that a file system can concurrently open which is useful when the file system cannot handle a large number of concurrent reads/writes or open connections at the same time.
For example, small HDFS clusters with few RPC handlers can sometimes be overwhelmed by a large Flink job trying to build up many connections during a checkpoint.
To limit a specific file system\u0026rsquo;s connections, add the following entries to the Flink configuration. The file system to be limited is identified by its scheme.
fs.\u0026lt;scheme\u0026gt;.limit.total: (number, 0/-1 mean no limit) fs.\u0026lt;scheme\u0026gt;.limit.input: (number, 0/-1 mean no limit) fs.\u0026lt;scheme\u0026gt;.limit.output: (number, 0/-1 mean no limit) fs.\u0026lt;scheme\u0026gt;.limit.timeout: (milliseconds, 0 means infinite) fs.\u0026lt;scheme\u0026gt;.limit.stream-timeout: (milliseconds, 0 means infinite) You can limit the number of input/output connections (streams) separately (fs.\u0026lt;scheme\u0026gt;.limit.input and fs.\u0026lt;scheme\u0026gt;.limit.output), as well as impose a limit on the total number of concurrent streams (fs.\u0026lt;scheme\u0026gt;.limit.total). If the file system tries to open more streams, the operation blocks until some streams close. If the opening of the stream takes longer than fs.\u0026lt;scheme\u0026gt;.limit.timeout, the stream opening fails.
To prevent inactive streams from taking up the full pool (preventing new connections to be opened), you can add an inactivity timeout which forcibly closes them if they do not read/write any bytes for at least that amount of time: fs.\u0026lt;scheme\u0026gt;.limit.stream-timeout.
Limit enforcement on a per TaskManager/file system basis. Because file systems creation occurs per scheme and authority, different authorities have independent connection pools. For example hdfs://myhdfs:50010/ and hdfs://anotherhdfs:4399/ will have separate pools.
Back to top
`}),e.add({id:35,href:"/flink/flink-docs-master/docs/dev/table/common/",title:"Concepts \u0026 Common API",section:"Table API \u0026 SQL",content:" Concepts \u0026amp; Common API # The Table API and SQL are integrated in a joint API. The central concept of this API is a Table which serves as input and output of queries. This document shows the common structure of programs with Table API and SQL queries, how to register a Table, how to query a Table, and how to emit a Table.\nStructure of Table API and SQL Programs # The following code example shows the common structure of Table API and SQL programs.\nJava import org.apache.flink.table.api.*; import org.apache.flink.connector.datagen.table.DataGenConnectorOptions; // Create a TableEnvironment for batch or streaming execution. // See the \u0026#34;Create a TableEnvironment\u0026#34; section for details. TableEnvironment tableEnv = TableEnvironment.create(/*…*/); // Create a source table tableEnv.createTemporaryTable(\u0026#34;SourceTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build()); // Create a sink table (using SQL DDL) tableEnv.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;); // Create a Table object from a Table API query Table table1 = tableEnv.from(\u0026#34;SourceTable\u0026#34;); // Create a Table object from a SQL query Table table2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM SourceTable\u0026#34;); // Emit a Table API result Table to a TableSink, same for SQL result TableResult tableResult = table1.insertInto(\u0026#34;SinkTable\u0026#34;).execute(); Scala import org.apache.flink.table.api._ import org.apache.flink.connector.datagen.table.DataGenConnectorOptions // Create a TableEnvironment for batch or streaming execution. // See the \u0026#34;Create a TableEnvironment\u0026#34; section for details. val tableEnv = TableEnvironment.create(/*…*/) // Create a source table tableEnv.createTemporaryTable(\u0026#34;SourceTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build()) // Create a sink table (using SQL DDL) tableEnv.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;) // Create a Table object from a Table API query val table1 = tableEnv.from(\u0026#34;SourceTable\u0026#34;) // Create a Table object from a SQL query val table2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM SourceTable\u0026#34;) // Emit a Table API result Table to a TableSink, same for SQL result val tableResult = table1.insertInto(\u0026#34;SinkTable\u0026#34;).execute() Python from pyflink.table import * # Create a TableEnvironment for batch or streaming execution table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # Create a source table table_env.executeSql(\u0026#34;\u0026#34;\u0026#34;CREATE TEMPORARY TABLE SourceTable ( f0 STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;rows-per-second\u0026#39; = \u0026#39;100\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # Create a sink table table_env.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;) # Create a Table from a Table API query table1 = table_env.from_path(\u0026#34;SourceTable\u0026#34;).select(...) # Create a Table from a SQL query table2 = table_env.sql_query(\u0026#34;SELECT ... FROM SourceTable ...\u0026#34;) # Emit a Table API result Table to a TableSink, same for SQL result table_result = table1.execute_insert(\u0026#34;SinkTable\u0026#34;) Table API and SQL queries can be easily integrated with and embedded into DataStream programs. Have a look at the DataStream API Integration page to learn how DataStreams can be converted into Tables and vice versa. Back to top\nCreate a TableEnvironment # The TableEnvironment is the entrypoint for Table API and SQL integration and is responsible for:\nRegistering a Table in the internal catalog Registering catalogs Loading pluggable modules Executing SQL queries Registering a user-defined (scalar, table, or aggregation) function Converting between DataStream and Table (in case of StreamTableEnvironment) A Table is always bound to a specific TableEnvironment. It is not possible to combine tables of different TableEnvironments in the same query, e.g., to join or union them. A TableEnvironment is created by calling the static TableEnvironment.create() method.\nJava import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build(); TableEnvironment tEnv = TableEnvironment.create(settings); Scala import org.apache.flink.table.api.{EnvironmentSettings, TableEnvironment} val settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build() val tEnv = TableEnvironment.create(settings) Python from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # create a batch TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) Alternatively, users can create a StreamTableEnvironment from an existing StreamExecutionEnvironment to interoperate with the DataStream API.\nJava import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); Scala import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.EnvironmentSettings import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment s_env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(s_env) Back to top\nCreate Tables in the Catalog # A TableEnvironment maintains a map of catalogs of tables which are created with an identifier. Each identifier consists of 3 parts: catalog name, database name and object name. If a catalog or database is not specified, the current default value will be used (see examples in the Table identifier expanding section).\nTables can be either virtual (VIEWS) or regular (TABLES). VIEWS can be created from an existing Table object, usually the result of a Table API or SQL query. TABLES describe external data, such as a file, database table, or message queue.\nTemporary vs Permanent tables. # Tables may either be temporary, and tied to the lifecycle of a single Flink session, or permanent, and visible across multiple Flink sessions and clusters.\nPermanent tables require a catalog (such as Hive Metastore) to maintain metadata about the table. Once a permanent table is created, it is visible to any Flink session that is connected to the catalog and will continue to exist until the table is explicitly dropped.\nOn the other hand, temporary tables are always stored in memory and only exist for the duration of the Flink session they are created within. These tables are not visible to other sessions. They are not bound to any catalog or database but can be created in the namespace of one. Temporary tables are not dropped if their corresponding database is removed.\nShadowing # It is possible to register a temporary table with the same identifier as an existing permanent table. The temporary table shadows the permanent one and makes the permanent table inaccessible as long as the temporary one exists. All queries with that identifier will be executed against the temporary table.\nThis might be useful for experimentation. It allows running exactly the same query first against a temporary table that e.g. has just a subset of data, or the data is obfuscated. Once verified that the query is correct it can be run against the real production table.\nCreate a Table # Virtual Tables # A Table API object corresponds to a VIEW (virtual table) in SQL terms. It encapsulates a logical query plan. It can be created in a catalog as follows:\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // table is the result of a simple projection query Table projTable = tableEnv.from(\u0026#34;X\u0026#34;).select(...); // register the Table projTable as table \u0026#34;projectedTable\u0026#34; tableEnv.createTemporaryView(\u0026#34;projectedTable\u0026#34;, projTable); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // table is the result of a simple projection query val projTable: Table = tableEnv.from(\u0026#34;X\u0026#34;).select(...) // register the Table projTable as table \u0026#34;projectedTable\u0026#34; tableEnv.createTemporaryView(\u0026#34;projectedTable\u0026#34;, projTable) Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # table is the result of a simple projection query proj_table = table_env.from_path(\u0026#34;X\u0026#34;).select(...) # register the Table projTable as table \u0026#34;projectedTable\u0026#34; table_env.register_table(\u0026#34;projectedTable\u0026#34;, proj_table) Note: Table objects are similar to VIEW\u0026rsquo;s from relational database systems, i.e., the query that defines the Table is not optimized but will be inlined when another query references the registered Table. If multiple queries reference the same registered Table, it will be inlined for each referencing query and executed multiple times, i.e., the result of the registered Table will not be shared.\nBack to top\nConnector Tables # It is also possible to create a TABLE as known from relational databases from a connector declaration. The connector describes the external system that stores the data of a table. Storage systems such as Apache Kafka or a regular file system can be declared here.\nSuch tables can either be created using the Table API directly, or by switching to SQL DDL.\nJava // Using table descriptors final TableDescriptor sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build(); tableEnv.createTable(\u0026#34;SourceTableA\u0026#34;, sourceDescriptor); tableEnv.createTemporaryTable(\u0026#34;SourceTableB\u0026#34;, sourceDescriptor); // Using SQL DDL tableEnv.executeSql(\u0026#34;CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)\u0026#34;); Python # Using table descriptors source_descriptor = TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) \\ .schema(Schema.new_builder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) \\ .option(\u0026#34;rows-per-second\u0026#34;, \u0026#34;100\u0026#34;) \\ .build() t_env.create_table(\u0026#34;SourceTableA\u0026#34;, source_descriptor) t_env.create_temporary_table(\u0026#34;SourceTableB\u0026#34;, source_descriptor) # Using SQL DDL t_env.execute_sql(\u0026#34;CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)\u0026#34;) Expanding Table identifiers # Tables are always registered with a 3-part identifier consisting of catalog, database, and table name.\nUsers can set one catalog and one database inside it to be the “current catalog” and “current database”. With them, the first two parts in the 3-parts identifier mentioned above can be optional - if they are not provided, the current catalog and current database will be referred. Users can switch the current catalog and current database via table API or SQL.\nIdentifiers follow SQL requirements which means that they can be escaped with a backtick character (`).\nJava TableEnvironment tEnv = ...; tEnv.useCatalog(\u0026#34;custom_catalog\u0026#34;); tEnv.useDatabase(\u0026#34;custom_database\u0026#34;); Table table = ...; // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;exampleView\u0026#34;, table); // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_database.exampleView\u0026#34;, table); // register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;`example.View`\u0026#34;, table); // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table); Scala // get a TableEnvironment val tEnv: TableEnvironment = ... tEnv.useCatalog(\u0026#34;custom_catalog\u0026#34;) tEnv.useDatabase(\u0026#34;custom_database\u0026#34;) val table: Table = ... // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;exampleView\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_database.exampleView\u0026#34;, table) // register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;`example.View`\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table) Python # get a TableEnvironment t_env = TableEnvironment.create(...) t_env.use_catalog(\u0026#34;custom_catalog\u0026#34;) t_env.use_database(\u0026#34;custom_database\u0026#34;) table = ... # register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; # in the database named \u0026#39;custom_database\u0026#39; t_env.create_temporary_view(\u0026#34;other_database.exampleView\u0026#34;, table) # register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; # in the database named \u0026#39;custom_database\u0026#39; t_env.create_temporary_view(\u0026#34;`example.View`\u0026#34;, table) # register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; # in the database named \u0026#39;other_database\u0026#39; t_env.create_temporary_view(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table) Query a Table # Table API # The Table API is a language-integrated query API for Scala and Java. In contrast to SQL, queries are not specified as Strings but are composed step-by-step in the host language.\nThe API is based on the Table class which represents a table (streaming or batch) and offers methods to apply relational operations. These methods return a new Table object, which represents the result of applying the relational operation on the input Table. Some relational operations are composed of multiple method calls such as table.groupBy(...).select(), where groupBy(...) specifies a grouping of table, and select(...) the projection on the grouping of table.\nThe Table API document describes all Table API operations that are supported on streaming and batch tables.\nThe following example shows a simple Table API aggregation query:\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // scan registered Orders table Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); // compute revenue for all customers from France Table revenue = orders .filter($(\u0026#34;cCountry\u0026#34;).isEqual(\u0026#34;FRANCE\u0026#34;)) .groupBy($(\u0026#34;cID\u0026#34;), $(\u0026#34;cName\u0026#34;)) .select($(\u0026#34;cID\u0026#34;), $(\u0026#34;cName\u0026#34;), $(\u0026#34;revenue\u0026#34;).sum().as(\u0026#34;revSum\u0026#34;)); // emit or convert Table // execute query Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // scan registered Orders table val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) // compute revenue for all customers from France val revenue = orders .filter($\u0026#34;cCountry\u0026#34; === \u0026#34;FRANCE\u0026#34;) .groupBy($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;) .select($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;, $\u0026#34;revenue\u0026#34;.sum AS \u0026#34;revSum\u0026#34;) // emit or convert Table // execute query Note: The Scala Table API uses Scala String interpolation that starts with a dollar sign ($) to reference the attributes of a Table. The Table API uses Scala implicits. Make sure to import\norg.apache.flink.table.api._ - for implicit expression conversions org.apache.flink.api.scala._ and org.apache.flink.table.api.bridge.scala._ if you want to convert from/to DataStream. Python # get a TableEnvironment table_env = # see \u0026#34;Create a TableEnvironment\u0026#34; section # register Orders table # scan registered Orders table orders = table_env.from_path(\u0026#34;Orders\u0026#34;) # compute revenue for all customers from France revenue = orders \\ .filter(col(\u0026#39;cCountry\u0026#39;) == \u0026#39;FRANCE\u0026#39;) \\ .group_by(col(\u0026#39;cID\u0026#39;), col(\u0026#39;cName\u0026#39;)) \\ .select(col(\u0026#39;cID\u0026#39;), col(\u0026#39;cName\u0026#39;), col(\u0026#39;revenue\u0026#39;).sum.alias(\u0026#39;revSum\u0026#39;)) # emit or convert Table # execute query Back to top\nSQL # Flink\u0026rsquo;s SQL integration is based on Apache Calcite, which implements the SQL standard. SQL queries are specified as regular Strings.\nThe SQL document describes Flink\u0026rsquo;s SQL support for streaming and batch tables.\nThe following example shows how to specify a query and return the result as a Table.\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // compute revenue for all customers from France Table revenue = tableEnv.sqlQuery( \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; + \u0026#34;FROM Orders \u0026#34; + \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; + \u0026#34;GROUP BY cID, cName\u0026#34; ); // emit or convert Table // execute query Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // compute revenue for all customers from France val revenue = tableEnv.sqlQuery(\u0026#34;\u0026#34;\u0026#34; |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) // emit or convert Table // execute query Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # register Orders table # compute revenue for all customers from France revenue = table_env.sql_query( \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; \u0026#34;FROM Orders \u0026#34; \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; \u0026#34;GROUP BY cID, cName\u0026#34; ) # emit or convert Table # execute query The following example shows how to specify an update query that inserts its result into a registered table.\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register \u0026#34;Orders\u0026#34; table // register \u0026#34;RevenueFrance\u0026#34; output table // compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; tableEnv.executeSql( \u0026#34;INSERT INTO RevenueFrance \u0026#34; + \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; + \u0026#34;FROM Orders \u0026#34; + \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; + \u0026#34;GROUP BY cID, cName\u0026#34; ); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register \u0026#34;Orders\u0026#34; table // register \u0026#34;RevenueFrance\u0026#34; output table // compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; tableEnv.executeSql(\u0026#34;\u0026#34;\u0026#34; |INSERT INTO RevenueFrance |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # register \u0026#34;Orders\u0026#34; table # register \u0026#34;RevenueFrance\u0026#34; output table # compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; table_env.execute_sql( \u0026#34;INSERT INTO RevenueFrance \u0026#34; \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; \u0026#34;FROM Orders \u0026#34; \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; \u0026#34;GROUP BY cID, cName\u0026#34; ) Back to top\nMixing Table API and SQL # Table API and SQL queries can be easily mixed because both return Table objects:\nA Table API query can be defined on the Table object returned by a SQL query. A SQL query can be defined on the result of a Table API query by registering the resulting Table in the TableEnvironment and referencing it in the FROM clause of the SQL query. Back to top\nEmit a Table # A Table is emitted by writing it to a TableSink. A TableSink is a generic interface to support a wide variety of file formats (e.g. CSV, Apache Parquet, Apache Avro), storage systems (e.g., JDBC, Apache HBase, Apache Cassandra, Elasticsearch), or messaging systems (e.g., Apache Kafka, RabbitMQ).\nA batch Table can only be written to a BatchTableSink, while a streaming Table requires either an AppendStreamTableSink, a RetractStreamTableSink, or an UpsertStreamTableSink.\nPlease see the documentation about Table Sources \u0026amp; Sinks for details about available sinks and instructions for how to implement a custom DynamicTableSink.\nThe Table.insertInto(String tableName) method defines a complete end-to-end pipeline emitting the source table to a registered sink table. The method looks up the table sink from the catalog by the name and validates that the schema of the Table is identical to the schema of the sink. A pipeline can be explained with TablePipeline.explain() and executed invoking TablePipeline.execute().\nThe following examples shows how to emit a Table:\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // create an output Table final Schema schema = Schema.newBuilder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build(); tableEnv.createTemporaryTable(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()); // compute a result Table using Table API operators and/or SQL queries Table result = ...; // Prepare the insert into pipeline TablePipeline pipeline = result.insertInto(\u0026#34;CsvSinkTable\u0026#34;); // Print explain details pipeline.printExplain(); // emit the result Table to the registered TableSink pipeline.execute(); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // create an output Table val schema = Schema.newBuilder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build() tableEnv.createTemporaryTable(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()) // compute a result Table using Table API operators and/or SQL queries val result: Table = ... // Prepare the insert into pipeline val pipeline = result.insertInto(\u0026#34;CsvSinkTable\u0026#34;) // Print explain details pipeline.printExplain() // emit the result Table to the registered TableSink pipeline.execute() Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # create a TableSink schema = Schema.new_builder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build() table_env.create_temporary_table(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()) # compute a result Table using Table API operators and/or SQL queries result = ... # emit the result Table to the registered TableSink result.execute_insert(\u0026#34;CsvSinkTable\u0026#34;) Back to top\nTranslate and Execute a Query # Table API and SQL queries are translated into DataStream programs whether their input is streaming or batch. A query is internally represented as a logical query plan and is translated in two phases:\nOptimization of the logical plan, Translation into a DataStream program. A Table API or SQL query is translated when:\nTableEnvironment.executeSql() is called. This method is used for executing a given statement, and the sql query is translated immediately once this method is called. TablePipeline.execute() is called. This method is used for executing a source-to-sink pipeline, and the Table API program is translated immediately once this method is called. Table.execute() is called. This method is used for collecting the table content to the local client, and the Table API is translated immediately once this method is called. StatementSet.execute() is called. A TablePipeline (emitted to a sink through StatementSet.add()) or an INSERT statement (specified through StatementSet.addInsertSql()) will be buffered in StatementSet first. They are transformed once StatementSet.execute() is called. All sinks will be optimized into one DAG. A Table is translated when it is converted into a DataStream (see Integration with DataStream). Once translated, it\u0026rsquo;s a regular DataStream program and is executed when StreamExecutionEnvironment.execute() is called. Back to top\nQuery Optimization # Apache Flink leverages and extends Apache Calcite to perform sophisticated query optimization. This includes a series of rule and cost-based optimizations such as:\nSubquery decorrelation based on Apache Calcite Project pruning Partition pruning Filter push-down Sub-plan deduplication to avoid duplicate computation Special subquery rewriting, including two parts: Converts IN and EXISTS into left semi-joins Converts NOT IN and NOT EXISTS into left anti-join Optional join reordering Enabled via table.optimizer.join-reorder-enabled Note: IN/EXISTS/NOT IN/NOT EXISTS are currently only supported in conjunctive conditions in subquery rewriting.\nThe optimizer makes intelligent decisions, based not only on the plan but also rich statistics available from the data sources and fine-grain costs for each operator such as io, cpu, network, and memory.\nAdvanced users may provide custom optimizations via a CalciteConfig object that can be provided to the table environment by calling TableEnvironment#getConfig#setPlannerConfig.\nExplaining a Table # The Table API provides a mechanism to explain the logical and optimized query plans to compute a Table. This is done through the Table.explain() method or StatementSet.explain() method. Table.explain()returns the plan of a Table. StatementSet.explain() returns the plan of multiple sinks. It returns a String describing three plans:\nthe Abstract Syntax Tree of the relational query, i.e., the unoptimized logical query plan, the optimized logical query plan, and the physical execution plan. TableEnvironment.explainSql() and TableEnvironment.executeSql() support execute a EXPLAIN statement to get the plans, Please refer to EXPLAIN page.\nThe following code shows an example and the corresponding output for given Table using Table.explain() method:\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); DataStream\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; stream1 = env.fromElements(new Tuple2\u0026lt;\u0026gt;(1, \u0026#34;hello\u0026#34;)); DataStream\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; stream2 = env.fromElements(new Tuple2\u0026lt;\u0026gt;(1, \u0026#34;hello\u0026#34;)); // explain Table API Table table1 = tEnv.fromDataStream(stream1, $(\u0026#34;count\u0026#34;), $(\u0026#34;word\u0026#34;)); Table table2 = tEnv.fromDataStream(stream2, $(\u0026#34;count\u0026#34;), $(\u0026#34;word\u0026#34;)); Table table = table1 .where($(\u0026#34;word\u0026#34;).like(\u0026#34;F%\u0026#34;)) .unionAll(table2); System.out.println(table.explain()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) val table1 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table2 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table = table1 .where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) .unionAll(table2) println(table.explain()) Python env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) table1 = t_env.from_elements([(1, \u0026#34;hello\u0026#34;)], [\u0026#34;count\u0026#34;, \u0026#34;word\u0026#34;]) table2 = t_env.from_elements([(1, \u0026#34;hello\u0026#34;)], [\u0026#34;count\u0026#34;, \u0026#34;word\u0026#34;]) table = table1 \\ .where(col(\u0026#39;word\u0026#39;).like(\u0026#39;F%\u0026#39;)) \\ .union_all(table2) print(table.explain()) The result of the above example is\nExplain ↕ == Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[Unregistered_DataStream_1]]) +- LogicalTableScan(table=[[Unregistered_DataStream_2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word]) +- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word]) == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word]) +- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word]) The following code shows an example and the corresponding output for multiple-sinks plan using StatementSet.explain() method:\nJava EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); final Schema schema = Schema.newBuilder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build(); tEnv.createTemporaryTable(\u0026#34;MySource1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySource2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySink1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySink2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); StatementSet stmtSet = tEnv.createStatementSet(); Table table1 = tEnv.from(\u0026#34;MySource1\u0026#34;).where($(\u0026#34;word\u0026#34;).like(\u0026#34;F%\u0026#34;)); stmtSet.add(table1.insertInto(\u0026#34;MySink1\u0026#34;)); Table table2 = table1.unionAll(tEnv.from(\u0026#34;MySource2\u0026#34;)); stmtSet.add(table2.insertInto(\u0026#34;MySink2\u0026#34;)); String explanation = stmtSet.explain(); System.out.println(explanation); Scala val settings = EnvironmentSettings.inStreamingMode() val tEnv = TableEnvironment.create(settings) val schema = Schema.newBuilder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build() tEnv.createTemporaryTable(\u0026#34;MySource1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySource2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySink1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySink2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) val stmtSet = tEnv.createStatementSet() val table1 = tEnv.from(\u0026#34;MySource1\u0026#34;).where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) stmtSet.add(table1.insertInto(\u0026#34;MySink1\u0026#34;)) val table2 = table1.unionAll(tEnv.from(\u0026#34;MySource2\u0026#34;)) stmtSet.add(table2.insertInto(\u0026#34;MySink2\u0026#34;)) val explanation = stmtSet.explain() println(explanation) Python settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(environment_settings=settings) schema = Schema.new_builder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build() t_env.create_temporary_table(\u0026#34;MySource1\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySource2\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySink1\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySink2\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) stmt_set = t_env.create_statement_set() table1 = t_env.from_path(\u0026#34;MySource1\u0026#34;).where(col(\u0026#39;word\u0026#39;).like(\u0026#39;F%\u0026#39;)) stmt_set.add_insert(\u0026#34;MySink1\u0026#34;, table1) table2 = table1.union_all(t_env.from_path(\u0026#34;MySource2\u0026#34;)) stmt_set.add_insert(\u0026#34;MySink2\u0026#34;, table2) explanation = stmt_set.explain() print(explanation) the result of multiple-sinks plan is\nMultiTable Explain ↕ == Abstract Syntax Tree == LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]]) == Optimized Physical Plan == LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) == Optimized Execution Plan == Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])(reuse_id=[1]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- Reused(reference_id=[1]) LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Reused(reference_id=[1]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) Back to top\n"}),e.add({id:36,href:"/flink/flink-docs-master/docs/connectors/table/formats/csv/",title:"CSV",section:"Formats",content:" CSV Format # Format: Serialization Schema Format: Deserialization Schema\nThe CSV format allows to read and write CSV data based on an CSV schema. Currently, the CSV schema is derived from table schema.\nDependencies # In order to use the CSV format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nMaven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-csv\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in How to create a table with CSV format # Here is an example to create a table using Kafka connector and CSV format.\nCREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;csv.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;csv.allow-comments\u0026#39; = \u0026#39;true\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'csv'. csv.field-delimiter optional yes , String Field delimiter character (',' by default), must be single character. You can use backslash to specify special characters, e.g. '\\t' represents the tab character. You can also use unicode to specify them in plain SQL, e.g. 'csv.field-delimiter' = U\u0026'\\0001' represents the 0x01 character. csv.disable-quote-character optional yes false Boolean Disabled quote character for enclosing field values (false by default). If true, option 'csv.quote-character' can not be set. csv.quote-character optional yes \" String Quote character for enclosing field values (\" by default). csv.allow-comments optional yes false Boolean Ignore comment lines that start with '#' (disabled by default). If enabled, make sure to also ignore parse errors to allow empty rows. csv.ignore-parse-errors optional no false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. csv.array-element-delimiter optional yes ; String Array element delimiter string for separating array and row element values (';' by default). csv.escape-character optional yes (none) String Escape character for escaping values (disabled by default). csv.null-literal optional yes (none) String Null literal string that is interpreted as a null value (disabled by default). csv.write-bigdecimal-in-scientific-notation optional yes true Boolean Enables representation of BigDecimal data type in scientific notation (default is true). For example, 100000 is encoded as 1E+5 by default, and will be written as 100000 if set this option to false. Note: Only when the value is not 0 and a multiple of 10 is converted to scientific notation. Data Type Mapping # Currently, the CSV schema is always derived from table schema. Explicitly defining an CSV schema is not supported yet.\nFlink CSV format uses jackson databind API to parse and generate CSV string.\nThe following table lists the type mapping from Flink type to CSV type.\nFlink SQL type CSV type CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY string with encoding: base64 DECIMAL number TINYINT number SMALLINT number INT number BIGINT number FLOAT number DOUBLE number DATE string with format: date TIME string with format: time TIMESTAMP string with format: date-time INTERVAL number ARRAY array ROW object "}),e.add({id:37,href:"/flink/flink-docs-master/docs/dev/datastream/",title:"DataStream API",section:"Application Development",content:" "}),e.add({id:38,href:"/flink/flink-docs-master/docs/ops/debugging/debugging_event_time/",title:"Debugging Windows \u0026 Event Time",section:"Debugging",content:` Debugging Windows \u0026amp; Event Time # Monitoring Current Event Time # Flink\u0026rsquo;s event time and watermark support are powerful features for handling out-of-order events. However, it\u0026rsquo;s harder to understand what exactly is going on because the progress of time is tracked within the system.
Low watermarks of each task can be accessed through Flink web interface or metrics system.
Each Task in Flink exposes a metric called currentInputWatermark that represents the lowest watermark received by this task. This long value represents the \u0026ldquo;current event time\u0026rdquo;. The value is calculated by taking the minimum of all watermarks received by upstream operators. This means that the event time tracked with watermarks is always dominated by the furthest-behind source.
The low watermark metric is accessible using the web interface, by choosing a task in the metric tab, and selecting the \u0026lt;taskNr\u0026gt;.currentInputWatermark metric. In the new box you\u0026rsquo;ll now be able to see the current low watermark of the task.
Another way of getting the metric is using one of the metric reporters, as described in the documentation for the metrics system. For local setups, we recommend using the JMX metric reporter and a tool like VisualVM.
Handling Event Time Stragglers # Approach 1: Watermark stays late (indicated completeness), windows fire early Approach 2: Watermark heuristic with maximum lateness, windows accept late data Back to top
`}),e.add({id:39,href:"/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/",title:"Dynamic Tables",section:"Streaming Concepts",content:` Dynamic Tables # SQL - and the Table API - offer flexible and powerful capabilities for real-time data processing. This page describes how relational concepts elegantly translate to streaming, allowing Flink to achieve the same semantics on unbounded streams.
Relational Queries on Data Streams # The following table compares traditional relational algebra and stream processing for input data, execution, and output results.
Relational Algebra / SQL Stream Processing Relations (or tables) are bounded (multi-)sets of tuples. A stream is an infinite sequences of tuples. A query that is executed on batch data (e.g., a table in a relational database) has access to the complete input data. A streaming query cannot access all data when it is started and has to "wait" for data to be streamed in. A batch query terminates after it produced a fixed sized result. A streaming query continuously updates its result based on the received records and never completes. Despite these differences, relational queries and SQL provide a powerful toolset for processing streams. Advanced relational database systems offer a feature called Materialized Views. A materialized view is defined as a SQL query, just like a regular virtual view. In contrast to a virtual view, a materialized view caches the query result such that the query does not need to be evaluated when it is accessed. A common challenge for caching is to prevent a cache from serving outdated results. A materialized view becomes obsolete when the base tables of its definition query are modified. Eager View Maintenance is a technique to update a materialized view as soon as its base tables are updated.
The connection between eager view maintenance and SQL queries on streams becomes evident if we consider the following:
A database table results from a stream of INSERT, UPDATE, and DELETE DML statements, often called changelog stream. A materialized view is defined as a SQL query. To update the view, queries must continuously process the changelog streams of the view\u0026rsquo;s base relations. The materialized view is the result of the streaming SQL query. We introduce the following concept of Dynamic tables in the next section with these points in mind.
Dynamic Tables \u0026amp; Continuous Queries # Dynamic tables are the core concept of Flink\u0026rsquo;s Table API and SQL support for streaming data. In contrast to the static tables that represent batch data, dynamic tables change over time. But just like static batch tables, systems can execute queries over dynamic tables. Querying dynamic tables yields a Continuous Query. A continuous query never terminates and produces dynamic results - another dynamic table. The query continuously updates its (dynamic) result table to reflect changes on its (dynamic) input tables. Essentially, a continuous query on a dynamic table is very similar to a query that defines a materialized view.
It is important to note that a continuous query output is always semantically equivalent to the result of the same query executed in batch mode on a snapshot of the input tables.
The following figure visualizes the relationship of streams, dynamic tables, and continuous queries:
A stream is converted into a dynamic table. A continuous query is evaluated on the dynamic table yielding a new dynamic table. The resulting dynamic table is converted back into a stream. Dynamic tables are foremost a logical concept. Dynamic tables are not necessarily (fully) materialized during query execution. In the following, we will explain the concepts of dynamic tables and continuous queries with a stream of click events that have the following schema:
CREATE TABLE clicks ( user VARCHAR, -- the name of the user url VARCHAR, -- the URL that was accessed by the user cTime TIMESTAMP(3) -- the time when the URL was accessed ) WITH (...); Defining a Table on a Stream # Processing streams with a relational query require converting it into a Table. Conceptually, each record of the stream is interpreted as an INSERT modification on the resulting table. We are building a table from an INSERT-only changelog stream.
The following figure visualizes how the stream of click event (left-hand side) is converted into a table (right-hand side). The resulting table is continuously growing as more records of the click stream are inserted.
Remember, a table defined on a stream is internally not materialized. Continuous Queries # A continuous query is evaluated on a dynamic table and produces a new dynamic table as a result. In contrast to a batch query, a continuous query never terminates and updates its result table according to its input tables\u0026rsquo; updates. At any point in time, a continuous query is semantically equivalent to the result of the same query executed in batch mode on a snapshot of the input tables.
In the following, we show two example queries on a clicks table defined on the stream of click events.
The first query is a simple GROUP-BY COUNT aggregation query. It groups the clicks table on the user field and counts the number of visited URLs. The following figure shows how the query is evaluated over time as the clicks table is updated with additional rows.
When the query starts, the clicks table (left-hand side) is empty. The query computes the result table when the first row is inserted. After the first row [Mary, ./home] arrives, the result table (right-hand side, top) consists of a single row [Mary, 1]. When the second row [Bob, ./cart] is inserted into the clicks table, the query updates the result table and inserts a new row [Bob, 1]. The third row, [Mary, ./prod?id=1] yields an update of an already computed result row such that [Mary, 1] is updated to [Mary, 2]. Finally, the query inserts a third row [Liz, 1] into the result table, when the fourth row is appended to the clicks table.
The second query is similar to the first one but groups the clicks table in addition to the user attribute also on an hourly tumbling window before it counts the number of URLs (time-based computations such as windows are based on special time attributes are discussed later). Again, the figure shows the input and output at different points in time to visualize the changing nature of dynamic tables.
As before, the input table clicks is shown on the left. The query continuously computes results every hour and updates the result table. The clicks table contains four rows with timestamps (cTime) between 12:00:00 and 12:59:59. The query computes two results rows from this input (one for each user) and appends them to the result table. For the next window between 13:00:00 and 13:59:59, the clicks table contains three rows, which results in another two rows being appended to the result table. The result table is updated as more rows are appended to clicks over time.
Update and Append Queries # Although the two example queries appear to be quite similar (both compute a grouped count aggregate), they differ in one crucial aspect:
The first query updates previously emitted results, i.e., the changelog stream that defines the result table contains INSERT and UPDATE changes. The second query only appends to the result table, i.e., the result table\u0026rsquo;s changelog stream only consists of INSERT changes. Whether a query produces an append-only table or an updated table has some implications:
Queries that make update changes usually have to maintain more state (see the following section). The conversion of an append-only table into a stream is different from the conversion of an updated table (see the Table to Stream Conversion section). Query Restrictions # Many, but not all, semantically valid queries can be evaluated as continuous queries on streams. Some queries are too expensive to compute, either due to the size of state they need to maintain or because computing updates is too expensive.
State Size: Continuous queries are evaluated on unbounded streams and are often supposed to run for weeks or months. Hence, the total amount of data that a continuous query processes can be very large. Queries that have to update previously emitted results need to maintain all emitted rows to update them. For instance, the first example query needs to store the URL count for each user to increase the count and send out a new result when the input table receives a new row. If only registered users are tracked, the number of counts to maintain might not be too high. However, if non-registered users get a unique user name assigned, the number of counts to maintain would grow over time and might eventually cause the query to fail. SELECT user, COUNT(url) FROM clicks GROUP BY user; Computing Updates: Some queries require to recompute and update a large fraction of the emitted result rows even if only a single input record is added or updated. Such queries are not well suited to be executed as continuous queries. An example is the following query that computes a RANK for each user based on the time of the last click. As soon as the clicks table receives a new row, the user\u0026rsquo;s lastAction is updated and a new rank computed. However, since two rows cannot have the same rank, all lower ranked rows also need to be updated. SELECT user, RANK() OVER (ORDER BY lastAction) FROM ( SELECT user, MAX(cTime) AS lastAction FROM clicks GROUP BY user ); The Query Configuration page discusses parameters to control the execution of continuous queries. Some parameters can be used to trade the size of the maintained state for result accuracy.
Table to Stream Conversion # A dynamic table can be continuously modified by INSERT, UPDATE, and DELETE changes just like a regular database table. It might be a table with a single row, which is constantly updated, an insert-only table without UPDATE and DELETE modifications, or anything in between.
When converting a dynamic table into a stream or writing it to an external system, these changes need to be encoded. Flink\u0026rsquo;s Table API and SQL support three ways to encode the changes of a dynamic table:
Append-only stream: A dynamic table that is only modified by INSERT changes can be converted into a stream by emitting the inserted rows.
Retract stream: A retract stream is a stream with two types of messages, add messages and retract messages. A dynamic table is converted into a retract stream by encoding an INSERT change as add message, a DELETE change as a retract message, and an UPDATE change as a retract message for the updated (previous) row, and an additional message for the updating (new) row. The following figure visualizes the conversion of a dynamic table into a retract stream.
Upsert stream: An upsert stream is a stream with two types of messages, upsert messages and delete messages. A dynamic table that is converted into an upsert stream requires a (possibly composite) unique key. A dynamic table with a unique key is transformed into a stream by encoding INSERT and UPDATE changes as upsert messages and DELETE changes as delete messages. The stream consuming operator needs to be aware of the unique key attribute to apply messages correctly. The main difference to a retract stream is that UPDATE changes are encoded with a single message and hence more efficient. The following figure visualizes the conversion of a dynamic table into an upsert stream. The API to convert a dynamic table into a DataStream is discussed on the Common Concepts page. Please note that only append and retract streams are supported when converting a dynamic table into a DataStream. The TableSink interface to emit a dynamic table to an external system are discussed on the TableSources and TableSinks page.
Back to top
`}),e.add({id:40,href:"/flink/flink-docs-master/docs/dev/datastream/execution_mode/",title:"Execution Mode (Batch/Streaming)",section:"DataStream API",content:` Execution Mode (Batch/Streaming) # The DataStream API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job.
There is the \u0026ldquo;classic\u0026rdquo; execution behavior of the DataStream API, which we call STREAMING execution mode. This should be used for unbounded jobs that require continuous incremental processing and are expected to stay online indefinitely.
Additionally, there is a batch-style execution mode that we call BATCH execution mode. This executes jobs in a way that is more reminiscent of batch processing frameworks such as MapReduce. This should be used for bounded jobs for which you have a known fixed input and which do not run continuously.
Apache Flink\u0026rsquo;s unified approach to stream and batch processing means that a DataStream application executed over bounded input will produce the same final results regardless of the configured execution mode. It is important to note what final means here: a job executing in STREAMING mode might produce incremental updates (think upserts in a database) while a BATCH job would only produce one final result at the end. The final result will be the same if interpreted correctly but the way to get there can be different.
By enabling BATCH execution, we allow Flink to apply additional optimizations that we can only do when we know that our input is bounded. For example, different join/aggregation strategies can be used, in addition to a different shuffle implementation that allows more efficient task scheduling and failure recovery behavior. We will go into some of the details of the execution behavior below.
When can/should I use BATCH execution mode? # The BATCH execution mode can only be used for Jobs/Flink Programs that are bounded. Boundedness is a property of a data source that tells us whether all the input coming from that source is known before execution or whether new data will show up, potentially indefinitely. A job, in turn, is bounded if all its sources are bounded, and unbounded otherwise.
STREAMING execution mode, on the other hand, can be used for both bounded and unbounded jobs.
As a rule of thumb, you should be using BATCH execution mode when your program is bounded because this will be more efficient. You have to use STREAMING execution mode when your program is unbounded because only this mode is general enough to be able to deal with continuous data streams.
One obvious outlier is when you want to use a bounded job to bootstrap some job state that you then want to use in an unbounded job. For example, by running a bounded job using STREAMING mode, taking a savepoint, and then restoring that savepoint on an unbounded job. This is a very specific use case and one that might soon become obsolete when we allow producing a savepoint as additional output of a BATCH execution job.
Another case where you might run a bounded job using STREAMING mode is when writing tests for code that will eventually run with unbounded sources. For testing it can be more natural to use a bounded source in those cases.
Configuring BATCH execution mode # The execution mode can be configured via the execution.runtime-mode setting. There are three possible values:
STREAMING: The classic DataStream execution mode (default) BATCH: Batch-style execution on the DataStream API AUTOMATIC: Let the system decide based on the boundedness of the sources This can be configured via command line parameters of bin/flink run ..., or programmatically when creating/configuring the StreamExecutionEnvironment.
Here\u0026rsquo;s how you can configure the execution mode via the command line:
\$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar This example shows how you can configure the execution mode in code:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); We recommend users to NOT set the runtime mode in their program but to instead set it using the command-line when submitting the application. Keeping the application code configuration-free allows for more flexibility as the same application can be executed in any execution mode. Execution Behavior # This section provides an overview of the execution behavior of BATCH execution mode and contrasts it with STREAMING execution mode. For more details, please refer to the FLIPs that introduced this feature: FLIP-134 and FLIP-140.
Task Scheduling And Network Shuffle # Flink jobs consist of different operations that are connected together in a dataflow graph. The system decides how to schedule the execution of these operations on different processes/machines (TaskManagers) and how data is shuffled (sent) between them.
Multiple operations/operators can be chained together using a feature called chaining. A group of one or multiple (chained) operators that Flink considers as a unit of scheduling is called a task. Often the term subtask is used to refer to the individual instances of tasks that are running in parallel on multiple TaskManagers but we will only use the term task here.
Task scheduling and network shuffles work differently for BATCH and STREAMING execution mode. Mostly due to the fact that we know our input data is bounded in BATCH execution mode, which allows Flink to use more efficient data structures and algorithms.
We will use this example to explain the differences in task scheduling and network transfer:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource\u0026lt;String\u0026gt; source = env.fromElements(...); source.name(\u0026#34;source\u0026#34;) .map(...).name(\u0026#34;map1\u0026#34;) .map(...).name(\u0026#34;map2\u0026#34;) .rebalance() .map(...).name(\u0026#34;map3\u0026#34;) .map(...).name(\u0026#34;map4\u0026#34;) .keyBy((value) -\u0026gt; value) .map(...).name(\u0026#34;map5\u0026#34;) .map(...).name(\u0026#34;map6\u0026#34;) .sinkTo(...).name(\u0026#34;sink\u0026#34;); Operations that imply a 1-to-1 connection pattern between operations, such as map(), flatMap(), or filter() can just forward data straight to the next operation, which allows these operations to be chained together. This means that Flink would not normally insert a network shuffle between them.
Operation such as keyBy() or rebalance() on the other hand require data to be shuffled between different parallel instances of tasks. This induces a network shuffle.
For the above example Flink would group operations together as tasks like this:
Task1: source, map1, and map2 Task2: map3, map4 Task3: map5, map6, and sink And we have a network shuffle between Tasks 1 and 2, and also Tasks 2 and 3. This is a visual representation of that job:
STREAMING Execution Mode # In STREAMING execution mode, all tasks need to be online/running all the time. This allows Flink to immediately process new records through the whole pipeline, which we need for continuous and low-latency stream processing. This also means that the TaskManagers that are allotted to a job need to have enough resources to run all the tasks at the same time.
Network shuffles are pipelined, meaning that records are immediately sent to downstream tasks, with some buffering on the network layer. Again, this is required because when processing a continuous stream of data there are no natural points (in time) where data could be materialized between tasks (or pipelines of tasks). This contrasts with BATCH execution mode where intermediate results can be materialized, as explained below.
BATCH Execution Mode # In BATCH execution mode, the tasks of a job can be separated into stages that can be executed one after another. We can do this because the input is bounded and Flink can therefore fully process one stage of the pipeline before moving on to the next. In the above example the job would have three stages that correspond to the three tasks that are separated by the shuffle barriers.
Instead of sending records immediately to downstream tasks, as explained above for STREAMING mode, processing in stages requires Flink to materialize intermediate results of tasks to some non-ephemeral storage which allows downstream tasks to read them after upstream tasks have already gone off line. This will increase the latency of processing but comes with other interesting properties. For one, this allows Flink to backtrack to the latest available results when a failure happens instead of restarting the whole job. Another side effect is that BATCH jobs can execute on fewer resources (in terms of available slots at TaskManagers) because the system can execute tasks sequentially one after the other.
TaskManagers will keep intermediate results at least as long as downstream tasks have not consumed them. (Technically, they will be kept until the consuming pipelined regions have produced their output.) After that, they will be kept for as long as space allows in order to allow the aforementioned backtracking to earlier results in case of a failure.
State Backends / State # In STREAMING mode, Flink uses a StateBackend to control how state is stored and how checkpointing works.
In BATCH mode, the configured state backend is ignored. Instead, the input of a keyed operation is grouped by key (using sorting) and then we process all records of a key in turn. This allows keeping only the state of only one key at the same time. State for a given key will be discarded when moving on to the next key.
See FLIP-140 for background information on this.
Order of Processing # The order in which records are processed in operators or user-defined functions (UDFs) can differ between BATCH and STREAMING execution.
In STREAMING mode, user-defined functions should not make any assumptions about incoming records\u0026rsquo; order. Data is processed as soon as it arrives.
In BATCH execution mode, there are some operations where Flink guarantees order. The ordering can be a side effect of the particular task scheduling, network shuffle, and state backend (see above), or a conscious choice by the system.
There are three general types of input that we can differentiate:
broadcast input: input from a broadcast stream (see also Broadcast State) regular input: input that is neither broadcast nor keyed keyed input: input from a KeyedStream Functions, or Operators, that consume multiple input types will process them in the following order:
broadcast inputs are processed first regular inputs are processed second keyed inputs are processed last For functions that consume from multiple regular or broadcast inputs — such as a CoProcessFunction — Flink has the right to process data from any input of that type in any order.
For functions that consume from multiple keyed inputs — such as a KeyedCoProcessFunction — Flink processes all records for a single key from all keyed inputs before moving on to the next.
Event Time / Watermarks # When it comes to supporting event time, Flink’s streaming runtime builds on the pessimistic assumption that events may come out-of-order, i.e. an event with timestamp t may come after an event with timestamp t+1. Because of this, the system can never be sure that no more elements with timestamp t \u0026lt; T for a given timestamp T can come in the future. To amortise the impact of this out-of-orderness on the final result while making the system practical, in STREAMING mode, Flink uses a heuristic called Watermarks. A watermark with timestamp T signals that no element with timestamp t \u0026lt; T will follow.
In BATCH mode, where the input dataset is known in advance, there is no need for such a heuristic as, at the very least, elements can be sorted by timestamp so that they are processed in temporal order. For readers familiar with streaming, in BATCH we can assume “perfect watermarks”.
Given the above, in BATCH mode, we only need a MAX_WATERMARK at the end of the input associated with each key, or at the end of input if the input stream is not keyed. Based on this scheme, all registered timers will fire at the end of time and user-defined WatermarkAssigners or WatermarkGenerators are ignored. Specifying a WatermarkStrategy is still important, though, because its TimestampAssigner will still be used to assign timestamps to records.
Processing Time # Processing Time is the wall-clock time on the machine that a record is processed, at the specific instance that the record is being processed. Based on this definition, we see that the results of a computation that is based on processing time are not reproducible. This is because the same record processed twice will have two different timestamps.
Despite the above, using processing time in STREAMING mode can be useful. The reason has to do with the fact that streaming pipelines often ingest their unbounded input in real time so there is a correlation between event time and processing time. In addition, because of the above, in STREAMING mode 1h in event time can often be almost 1h in processing time, or wall-clock time. So using processing time can be used for early (incomplete) firings that give hints about the expected results.
This correlation does not exist in the batch world where the input dataset is static and known in advance. Given this, in BATCH mode we allow users to request the current processing time and register processing time timers, but, as in the case of Event Time, all the timers are going to fire at the end of the input.
Conceptually, we can imagine that processing time does not advance during the execution of a job and we fast-forward to the end of time when the whole input is processed.
Failure Recovery # In STREAMING execution mode, Flink uses checkpoints for failure recovery. Take a look at the checkpointing documentation for hands-on documentation about this and how to configure it. There is also a more introductory section about fault tolerance via state snapshots that explains the concepts at a higher level.
One of the characteristics of checkpointing for failure recovery is that Flink will restart all the running tasks from a checkpoint in case of a failure. This can be more costly than what we have to do in BATCH mode (as explained below), which is one of the reasons that you should use BATCH execution mode if your job allows it.
In BATCH execution mode, Flink will try and backtrack to previous processing stages for which intermediate results are still available. Potentially, only the tasks that failed (or their predecessors in the graph) will have to be restarted, which can improve processing efficiency and overall processing time of the job compared to restarting all tasks from a checkpoint.
Important Considerations # Compared to classic STREAMING execution mode, in BATCH mode some things might not work as expected. Some features will work slightly differently while others are not supported.
Behavior Change in BATCH mode:
\u0026ldquo;Rolling\u0026rdquo; operations such as reduce() or sum() emit an incremental update for every new record that arrives in STREAMING mode. In BATCH mode, these operations are not \u0026ldquo;rolling\u0026rdquo;. They emit only the final result. Unsupported in BATCH mode:
Checkpointing and any operations that depend on checkpointing do not work. Iterations Custom operators should be implemented with care, otherwise they might behave improperly. See also additional explanations below for more details.
Checkpointing # As explained above, failure recovery for batch programs does not use checkpointing.
It is important to remember that because there are no checkpoints, certain features such as CheckpointListener and, as a result, Kafka\u0026rsquo;s EXACTLY_ONCE mode or File Sink\u0026rsquo;s OnCheckpointRollingPolicy won\u0026rsquo;t work. If you need a transactional sink that works in BATCH mode make sure it uses the Unified Sink API as proposed in FLIP-143.
You can still use all the state primitives, it\u0026rsquo;s just that the mechanism used for failure recovery will be different.
Writing Custom Operators # Note: Custom operators are an advanced usage pattern of Apache Flink. For most use-cases, consider using a (keyed-)process function instead. It is important to remember the assumptions made for BATCH execution mode when writing a custom operator. Otherwise, an operator that works just fine for STREAMING mode might produce wrong results in BATCH mode. Operators are never scoped to a particular key which means they see some properties of BATCH processing Flink tries to leverage.
First of all you should not cache the last seen watermark within an operator. In BATCH mode we process records key by key. As a result, the Watermark will switch from MAX_VALUE to MIN_VALUE between each key. You should not assume that the Watermark will always be ascending in an operator. For the same reasons timers will fire first in key order and then in timestamp order within each key. Moreover, operations that change a key manually are not supported.
`}),e.add({id:41,href:"/flink/flink-docs-master/docs/deployment/advanced/external_resources/",title:"External Resources",section:"Advanced",content:` External Resource Framework # In addition to CPU and memory, many workloads also need some other resources, e.g. GPUs for deep learning. To support external resources, Flink provides an external resource framework. The framework supports requesting various types of resources from the underlying resource management systems (e.g., Kubernetes), and supplies information needed for using these resources to the operators. Different resource types can be supported. You can either leverage built-in plugins provided by Flink (currently only for GPU support), or implement your own plugins for custom resource types.
What the external resource framework does # In general, the external resource framework does two things:
Set the corresponding fields of the resource requests (for requesting resources from the underlying system) with respect to your configuration.
Provide operators with the information needed for using the resources.
When deployed on resource management systems (Kubernetes / Yarn), the external resource framework will ensure that the allocated pod/container will contain the desired external resources. Currently, many resource management systems support external resources. For example, Kubernetes supports GPU, FPGA, etc. through its Device Plugin mechanism since v1.10, and Yarn supports GPU and FPGA resources since 2.10 and 3.1. In Standalone mode, the user has to ensure that the external resources are available.
The external resource framework will provide the corresponding information to operators. The external resource information, which contains the basic properties needed for using the resources, is generated by the configured external resource drivers.
Enable the external resource framework for your workload # To enable an external resource with the external resource framework, you need to:
Prepare the external resource plugin.
Set configurations for the external resource.
Get the external resource information from RuntimeContext and use it in your operators.
Prepare plugins # You need to prepare the external resource plugin and put it into the plugins/ folder of your Flink distribution, see Flink Plugins. Apache Flink provides a first-party plugin for GPU resources. You can also implement a plugin for your custom resource type.
Configurations # First, you need to add resource names for all the external resource types to the external resource list (with the configuration key ‘external-resources’) with delimiter \u0026ldquo;;\u0026rdquo;, e.g. \u0026ldquo;external-resources: gpu;fpga\u0026rdquo; for two external resources \u0026ldquo;gpu\u0026rdquo; and \u0026ldquo;fpga\u0026rdquo;. Only the \u0026lt;resource_name\u0026gt; defined here will go into effect in the external resource framework.
For each external resource, you could configure the below options. The \u0026lt;resource_name\u0026gt; in all the below configuration options corresponds to the name listed in the external resource list:
Amount (external.\u0026lt;resource_name\u0026gt;.amount): This is the quantity of the external resource that should be requested from the external system.
Config key in Yarn (external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key): optional. If configured, the external resource framework will add this key to the resource profile of container requests for Yarn. The value will be set to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount.
Config key in Kubernetes (external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key): optional. If configured, external resource framework will add resources.limits.\u0026lt;config-key\u0026gt; and resources.requests.\u0026lt;config-key\u0026gt; to the main container spec of TaskManager and set the value to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount.
Driver Factory (external-resource.\u0026lt;resource_name\u0026gt;.driver-factory.class): optional. Defines the factory class name for the external resource identified by \u0026lt;resource_name\u0026gt;. If configured, the factory will be used to instantiate drivers in the external resource framework. If not configured, the requested resource will still exist in the TaskManager as long as the relevant options are configured. However, the operator will not get any information of the resource from RuntimeContext in that case.
Driver Parameters (external-resource.\u0026lt;resource_name\u0026gt;.param.\u0026lt;param\u0026gt;): optional. The naming pattern of custom config options for the external resource specified by \u0026lt;resource_name\u0026gt;. Only the configurations that follow this pattern will be passed into the driver factory of that external resource.
An example configuration that specifies two external resources:
external-resources: gpu;fpga # Define two external resources, \u0026#34;gpu\u0026#34; and \u0026#34;fpga\u0026#34;. external-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory # Define the driver factory class of gpu resource. external-resource.gpu.amount: 2 # Define the amount of gpu resource per TaskManager. external-resource.gpu.param.discovery-script.args: --enable-coordination # Define the custom param discovery-script.args which will be passed into the gpu driver. external-resource.fpga.driver-factory.class: org.apache.flink.externalresource.fpga.FPGADriverFactory # Define the driver factory class of fpga resource. external-resource.fpga.amount: 1 # Define the amount of fpga resource per TaskManager. external-resource.fpga.yarn.config-key: yarn.io/fpga # Define the corresponding config key of fpga in Yarn. Use the resources # To use the resources, operators need to get the ExternalResourceInfo set from the RuntimeContext. ExternalResourceInfo wraps the information needed for using the resource, which can be retrieved with getProperty. What properties are available and how to access the resource with the properties depends on the specific plugin.
Operators can get the ExternalResourceInfo set of a specific external resource from RuntimeContext or FunctionContext by getExternalResourceInfos(String resourceName). The resourceName here should have the same value as the name configured in the external resource list. It can be used as follows:
Java public class ExternalResourceMapFunction extends RichMapFunction\u0026lt;String, String\u0026gt; { private static final String RESOURCE_NAME = \u0026#34;foo\u0026#34;; @Override public String map(String value) { Set\u0026lt;ExternalResourceInfo\u0026gt; externalResourceInfos = getRuntimeContext().getExternalResourceInfos(RESOURCE_NAME); List\u0026lt;String\u0026gt; addresses = new ArrayList\u0026lt;\u0026gt;(); externalResourceInfos.iterator().forEachRemaining(externalResourceInfo -\u0026gt; addresses.add(externalResourceInfo.getProperty(\u0026#34;address\u0026#34;).get())); // map function with addresses. // ... } } Scala class ExternalResourceMapFunction extends RichMapFunction[(String, String)] { var RESOURCE_NAME = \u0026#34;foo\u0026#34; override def map(value: String): String = { val externalResourceInfos = getRuntimeContext().getExternalResourceInfos(RESOURCE_NAME) val addresses = new util.ArrayList[String] externalResourceInfos.asScala.foreach( externalResourceInfo =\u0026gt; addresses.add(externalResourceInfo.getProperty(\u0026#34;address\u0026#34;).get())) // map function with addresses. // ... } } Each ExternalResourceInfo contains one or more properties with keys representing the different dimensions of the resource. You could get all valid keys by ExternalResourceInfo#getKeys.
Note: Currently, the information returned by RuntimeContext#getExternalResourceInfos is available to all the operators. Implement a plugin for your custom resource type # To implement a plugin for your custom resource type, you need to:
Add your own external resource driver by implementing the org.apache.flink.api.common.externalresource.ExternalResourceDriver interface.
Add a driver factory, which instantiates the driver, by implementing the org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory.
Add a service entry. Create a file META-INF/services/org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory which contains the class name of your driver factory class (see the Java Service Loader docs for more details).
For example, to implement a plugin for external resource named \u0026ldquo;FPGA\u0026rdquo;, you need to implement FPGADriver and FPGADriverFactory first:
Java public class FPGADriver implements ExternalResourceDriver { @Override public Set\u0026lt;FPGAInfo\u0026gt; retrieveResourceInfo(long amount) { // return the information set of \u0026#34;FPGA\u0026#34; } } public class FPGADriverFactory implements ExternalResourceDriverFactory { @Override public ExternalResourceDriver createExternalResourceDriver(Configuration config) { return new FPGADriver(); } } // Also implement FPGAInfo which contains basic properties of \u0026#34;FPGA\u0026#34; resource. public class FPGAInfo implements ExternalResourceInfo { @Override public Optional\u0026lt;String\u0026gt; getProperty(String key) { // return the property with the given key. } @Override public Collection\u0026lt;String\u0026gt; getKeys() { // return all property keys. } } Scala class FPGADriver extends ExternalResourceDriver { override def retrieveResourceInfo(amount: Long): Set[FPGAInfo] = { // return the information set of \u0026#34;FPGA\u0026#34; } } class FPGADriverFactory extends ExternalResourceDriverFactory { override def createExternalResourceDriver(config: Configuration): ExternalResourceDriver = { new FPGADriver() } } // Also implement FPGAInfo which contains basic properties of \u0026#34;FPGA\u0026#34; resource. class FPGAInfo extends ExternalResourceInfo { override def getProperty(key: String): Option[String] = { // return the property with the given key. } override def getKeys(): util.Collection[String] = { // return all property keys. } } Create a file with name org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory in META-INF/services/ and write the factory class name (e.g. your.domain.FPGADriverFactory) to it.
Then, create a jar which includes FPGADriver, FPGADriverFactory, META-INF/services/ and all the external dependencies. Make a directory in plugins/ of your Flink distribution with an arbitrary name, e.g. \u0026ldquo;fpga\u0026rdquo;, and put the jar into this directory. See Flink Plugin for more details.
Note: External resources are shared by all operators running on the same machine. The community might add external resource isolation in a future release. Existing supported external resource plugins # Currently, Flink supports GPUs as external resources.
Plugin for GPU resources # We provide a first-party plugin for GPU resources. The plugin leverages a discovery script to discover indexes of GPU devices, which can be accessed from the resource information via the property \u0026ldquo;index\u0026rdquo;. We provide a default discovery script that can be used to discover NVIDIA GPUs. You can also provide your custom script.
We provide an example which shows how to use the GPUs to do matrix-vector multiplication in Flink.
Note: Currently, for all the operators, RuntimeContext#getExternalResourceInfos returns the same set of resource information. That means, the same set of GPU devices are always accessible to all the operators running in the same TaskManager. There is no operator level isolation at the moment. Pre-requisites # To make GPU resources accessible, certain prerequisites are needed depending on your environment:
For standalone mode, administrators should ensure the NVIDIA driver is installed and GPU resources are accessible on all the nodes in the cluster.
For Yarn deployment, administrators should configure the Yarn cluster to enable GPU scheduling. Notice the required Hadoop version is 2.10+ or 3.1+.
For Kubernetes deployment, administrators should make sure the NVIDIA GPU device plugin is installed. Notice the required version is 1.10+. At the moment, Kubernetes only supports NVIDIA GPU and AMD GPU. Flink only provides discovery script for NVIDIA GPUs, but you can provide a custom discovery script for AMD GPUs yourself, see Discovery script.
Enable GPU resources for your workload # As mentioned in Enable external resources for your workload, you also need to do two things to enable GPU resources:
Configure the GPU resource.
Get the information of GPU resources, which contains the GPU index as property with key \u0026ldquo;index\u0026rdquo;, in operators.
Configurations # For the GPU plugin, you need to specify the common external resource configurations:
external-resources: You need to append your resource name (e.g. gpu) for GPU resources to it.
external-resource.\u0026lt;resource_name\u0026gt;.amount: The amount of GPU devices per TaskManager.
external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key: For Yarn, the config key of GPU is yarn.io/gpu. Notice that Yarn only supports NVIDIA GPU at the moment.
external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key: For Kubernetes, the config key of GPU is \u0026lt;vendor\u0026gt;.com/gpu. Currently, \u0026ldquo;nvidia\u0026rdquo; and \u0026ldquo;amd\u0026rdquo; are the two supported vendors. Notice that if you use AMD GPUs, you need to provide a discovery script yourself, see Discovery script.
external-resource.\u0026lt;resource_name\u0026gt;.driver-factory.class: Should be set to org.apache.flink.externalresource.gpu.GPUDriverFactory.
In addition, there are some specific configurations for the GPU plugin:
external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.path: The path of the discovery script. It can either be an absolute path, or a relative path to FLINK_HOME when defined or current directory otherwise. If not explicitly configured, the default script will be used.
external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.args: The arguments passed to the discovery script. For the default discovery script, see Default Script for the available parameters.
An example configuration for GPU resource:
external-resources: gpu external-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory # Define the driver factory class of gpu resource. external-resource.gpu.amount: 2 # Define the amount of gpu resource per TaskManager. external-resource.gpu.param.discovery-script.path: plugins/external-resource-gpu/nvidia-gpu-discovery.sh external-resource.gpu.param.discovery-script.args: --enable-coordination # Define the custom param \u0026#34;discovery-script.args\u0026#34; which will be passed into the gpu driver. external-resource.gpu.yarn.config-key: yarn.io/gpu # for Yarn external-resource.gpu.kubernetes.config-key: nvidia.com/gpu # for Kubernetes Discovery script # The GPUDriver leverages a discovery script to discover GPU resources and generate the GPU resource information.
Default Script # We provide a default discovery script for NVIDIA GPU, located at plugins/external-resource-gpu/nvidia-gpu-discovery.sh of your Flink distribution. The script gets the indexes of visible GPU resources through the nvidia-smi command. It tries to return the required amount (specified by external-resource.\u0026lt;resource_name\u0026gt;.amount) of GPU indexes in a list, and exit with non-zero if the amount cannot be satisfied.
For standalone mode, multiple TaskManagers might be co-located on the same machine, and each GPU device is visible to all the TaskManagers. The default discovery script supports a coordination mode, in which it leverages a coordination file to synchronize the allocation state of GPU devices and ensure each GPU device can only be used by one TaskManager process. The relevant arguments are:
--enable-coordination-mode: Enable the coordination mode. By default the coordination mode is disabled.
--coordination-file filePath: The path of the coordination file used to synchronize the allocation state of GPU resources. The default path is /var/tmp/flink-gpu-coordination.
Note: The coordination mode only ensures that a GPU device is not shared by multiple TaskManagers of the same Flink cluster. Please be aware that another Flink cluster (with a different coordination file) or a non-Flink application can still use the same GPU devices. Custom Script # You can also provide a discovery script to address your custom requirements, e.g. discovering AMD GPU. Please make sure the path of your custom script is accessible to Flink and configured (external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.path) correctly. The contract of the discovery script:
GPUDriver passes the amount (specified by external-resource.\u0026lt;resource_name\u0026gt;.amount) as the first argument into the script. The user-defined arguments in external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.args would be appended after it.
The script should return a list of the available GPU indexes, split by a comma. Whitespace only indexes will be ignored.
The script can also suggest that the discovery is not properly performed, by exiting with non-zero. In that case, no gpu information will be provided to operators.
`}),e.add({id:42,href:"/flink/flink-docs-master/docs/connectors/datastream/guarantees/",title:"Fault Tolerance Guarantees",section:"DataStream Connectors",content:` Fault Tolerance Guarantees of Data Sources and Sinks # Flink\u0026rsquo;s fault tolerance mechanism recovers programs in the presence of failures and continues to execute them. Such failures include machine hardware failures, network failures, transient program failures, etc.
Flink can guarantee exactly-once state updates to user-defined state only when the source participates in the snapshotting mechanism. The following table lists the state update guarantees of Flink coupled with the bundled connectors.
Please read the documentation of each connector to understand the details of the fault tolerance guarantees.
Source Guarantees Notes Apache Kafka exactly once Use the appropriate Kafka connector for your version AWS Kinesis Streams exactly once RabbitMQ at most once (v 0.10) / exactly once (v 1.0) Google PubSub at least once Collections exactly once Files exactly once Sockets at most once To guarantee end-to-end exactly-once record delivery (in addition to exactly-once state semantics), the data sink needs to take part in the checkpointing mechanism. The following table lists the delivery guarantees (assuming exactly-once state updates) of Flink coupled with bundled sinks:
Sink Guarantees Notes Elasticsearch at least once Kafka producer at least once / exactly once exactly once with transactional producers (v 0.11+) Cassandra sink at least once / exactly once exactly once only for idempotent updates AWS Kinesis Streams at least once File sinks exactly once Socket sinks at least once Standard output at least once Redis sink at least once Back to top
`}),e.add({id:43,href:"/flink/flink-docs-master/docs/connectors/dataset/formats/",title:"Formats",section:"DataSet Connectors",content:" "}),e.add({id:44,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/",title:"Formats",section:"DataStream Connectors",content:" "}),e.add({id:45,href:"/flink/flink-docs-master/docs/connectors/table/formats/",title:"Formats",section:"Table API Connectors",content:""}),e.add({id:46,href:"/flink/flink-docs-master/docs/try-flink/datastream/",title:"Fraud Detection with the DataStream API",section:"Try Flink",content:` Fraud Detection with the DataStream API # Apache Flink offers a DataStream API for building robust, stateful streaming applications. It provides fine-grained control over state and time, which allows for the implementation of advanced event-driven systems. In this step-by-step guide you\u0026rsquo;ll learn how to build a stateful streaming application with Flink\u0026rsquo;s DataStream API.
What Are You Building? # Credit card fraud is a growing concern in the digital age. Criminals steal credit card numbers by running scams or hacking into insecure systems. Stolen numbers are tested by making one or more small purchases, often for a dollar or less. If that works, they then make more significant purchases to get items they can sell or keep for themselves.
In this tutorial, you will build a fraud detection system for alerting on suspicious credit card transactions. Using a simple set of rules, you will see how Flink allows us to implement advanced business logic and act in real-time.
Prerequisites # This walkthrough assumes that you have some familiarity with Java or Scala, but you should be able to follow along even if you are coming from a different programming language.
Running in an IDE # Running the project in an IDE may result in a java.lang.NoClassDefFoundError exception. This is probably because you do not have all required Flink dependencies implicitly loaded into the classpath.
IntelliJ IDEA: Go to Run \u0026gt; Edit Configurations \u0026gt; Modify options \u0026gt; Select include dependencies with \u0026quot;Provided\u0026quot; scope. This run configuration will now include all required classes to run the application from within the IDE. Help, I’m Stuck! # If you get stuck, check out the community support resources. In particular, Apache Flink\u0026rsquo;s user mailing list is consistently ranked as one of the most active of any Apache project and a great way to get help quickly.
How to Follow Along # If you want to follow along, you will require a computer with:
Java 11 Maven A provided Flink Maven Archetype will create a skeleton project with all the necessary dependencies quickly, so you only need to focus on filling out the business logic. These dependencies include flink-streaming-java which is the core dependency for all Flink streaming applications and flink-walkthrough-common that has data generators and other classes specific to this walkthrough.
Java \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-walkthrough-datastream-java \\ -DarchetypeVersion=1.16-SNAPSHOT \\ -DgroupId=frauddetection \\ -DartifactId=frauddetection \\ -Dversion=0.1 \\ -Dpackage=spendreport \\ -DinteractiveMode=false Scala \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-walkthrough-datastream-scala \\ -DarchetypeVersion=1.16-SNAPSHOT \\ -DgroupId=frauddetection \\ -DartifactId=frauddetection \\ -Dversion=0.1 \\ -Dpackage=spendreport \\ -DinteractiveMode=false For Maven 3.0 or higher, it is no longer possible to specify the repository (-DarchetypeCatalog) via the command line. For details about this change, please refer to Maven official documentation. If you wish to use the snapshot repository, you need to add a repository entry to your settings.xml. For example:
\u0026lt;settings\u0026gt; \u0026lt;activeProfiles\u0026gt; \u0026lt;activeProfile\u0026gt;apache\u0026lt;/activeProfile\u0026gt; \u0026lt;/activeProfiles\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;apache\u0026lt;/id\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;apache-snapshots\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;/settings\u0026gt; You can edit the groupId, artifactId and package if you like. With the above parameters, Maven will create a folder named frauddetection that contains a project with all the dependencies to complete this tutorial. After importing the project into your editor, you can find a file FraudDetectionJob.java (or FraudDetectionJob.scala) with the following code which you can run directly inside your IDE. Try setting break points through out the data stream and run the code in DEBUG mode to get a feeling for how everything works.
Java FraudDetectionJob.java # package spendreport; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.walkthrough.common.sink.AlertSink; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; import org.apache.flink.walkthrough.common.source.TransactionSource; public class FraudDetectionJob { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Transaction\u0026gt; transactions = env .addSource(new TransactionSource()) .name(\u0026#34;transactions\u0026#34;); DataStream\u0026lt;Alert\u0026gt; alerts = transactions .keyBy(Transaction::getAccountId) .process(new FraudDetector()) .name(\u0026#34;fraud-detector\u0026#34;); alerts .addSink(new AlertSink()) .name(\u0026#34;send-alerts\u0026#34;); env.execute(\u0026#34;Fraud Detection\u0026#34;); } } FraudDetector.java # package spendreport; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } } Scala FraudDetectionJob.scala # package spendreport import org.apache.flink.streaming.api.scala._ import org.apache.flink.walkthrough.common.sink.AlertSink import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction import org.apache.flink.walkthrough.common.source.TransactionSource object FraudDetectionJob { @throws[Exception] def main(args: Array[String]): Unit = { val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val transactions: DataStream[Transaction] = env .addSource(new TransactionSource) .name(\u0026#34;transactions\u0026#34;) val alerts: DataStream[Alert] = transactions .keyBy(transaction =\u0026gt; transaction.getAccountId) .process(new FraudDetector) .name(\u0026#34;fraud-detector\u0026#34;) alerts .addSink(new AlertSink) .name(\u0026#34;send-alerts\u0026#34;) env.execute(\u0026#34;Fraud Detection\u0026#34;) } } FraudDetector.scala # package spendreport import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @throws[Exception] def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } } Breaking Down the Code # Let\u0026rsquo;s walk step-by-step through the code of these two files. The FraudDetectionJob class defines the data flow of the application and the FraudDetector class defines the business logic of the function that detects fraudulent transactions.
We start describing how the Job is assembled in the main method of the FraudDetectionJob class.
The Execution Environment # The first line sets up your StreamExecutionEnvironment. The execution environment is how you set properties for your Job, create your sources, and finally trigger the execution of the Job.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment Creating a Source # Sources ingest data from external systems, such as Apache Kafka, Rabbit MQ, or Apache Pulsar, into Flink Jobs. This walkthrough uses a source that generates an infinite stream of credit card transactions for you to process. Each transaction contains an account ID (accountId), timestamp (timestamp) of when the transaction occurred, and US\$ amount (amount). The name attached to the source is just for debugging purposes, so if something goes wrong, we will know where the error originated.
Java DataStream\u0026lt;Transaction\u0026gt; transactions = env .addSource(new TransactionSource()) .name(\u0026#34;transactions\u0026#34;); Scala val transactions: DataStream[Transaction] = env .addSource(new TransactionSource) .name(\u0026#34;transactions\u0026#34;) Partitioning Events \u0026amp; Detecting Fraud # The transactions stream contains a lot of transactions from a large number of users, such that it needs to be processed in parallel by multiple fraud detection tasks. Since fraud occurs on a per-account basis, you must ensure that all transactions for the same account are processed by the same parallel task of the fraud detector operator.
To ensure that the same physical task processes all records for a particular key, you can partition a stream using DataStream#keyBy. The process() call adds an operator that applies a function to each partitioned element in the stream. It is common to say the operator immediately after a keyBy, in this case FraudDetector, is executed within a keyed context.
Java DataStream\u0026lt;Alert\u0026gt; alerts = transactions .keyBy(Transaction::getAccountId) .process(new FraudDetector()) .name(\u0026#34;fraud-detector\u0026#34;); Scala val alerts: DataStream[Alert] = transactions .keyBy(transaction =\u0026gt; transaction.getAccountId) .process(new FraudDetector) .name(\u0026#34;fraud-detector\u0026#34;) Outputting Results # A sink writes a DataStream to an external system; such as Apache Kafka, Cassandra, and AWS Kinesis. The AlertSink logs each Alert record with log level INFO, instead of writing it to persistent storage, so you can easily see your results.
Java alerts.addSink(new AlertSink()); Scala alerts.addSink(new AlertSink) The Fraud Detector # The fraud detector is implemented as a KeyedProcessFunction. Its method KeyedProcessFunction#processElement is called for every transaction event. This first version produces an alert on every transaction, which some may say is overly conservative.
The next steps of this tutorial will guide you to expand the fraud detector with more meaningful business logic.
Java public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } } Scala object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @throws[Exception] def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } } Writing a Real Application (v1) # For the first version, the fraud detector should output an alert for any account that makes a small transaction immediately followed by a large one. Where small is anything less than \$1.00 and large is more than \$500. Imagine your fraud detector processes the following stream of transactions for a particular account.
Transactions 3 and 4 should be marked as fraudulent because it is a small transaction, \$0.09, followed by a large one, \$510. Alternatively, transactions 7, 8, and 9 are not fraud because the small amount of \$0.02 is not immediately followed by the large one; instead, there is an intermediate transaction that breaks the pattern.
To do this, the fraud detector must remember information across events; a large transaction is only fraudulent if the previous one was small. Remembering information across events requires state, and that is why we decided to use a KeyedProcessFunction. It provides fine-grained control over both state and time, which will allow us to evolve our algorithm with more complex requirements throughout this walkthrough.
The most straightforward implementation is a boolean flag that is set whenever a small transaction is processed. When a large transaction comes through, you can simply check if the flag is set for that account.
However, merely implementing the flag as a member variable in the FraudDetector class will not work. Flink processes the transactions of multiple accounts with the same object instance of FraudDetector, which means if accounts A and B are routed through the same instance of FraudDetector, a transaction for account A could set the flag to true, and then a transaction for account B could set off a false alert. We could of course use a data structure like a Map to keep track of the flags for individual keys, however, a simple member variable would not be fault-tolerant and all its information be lost in case of a failure. Hence, the fraud detector would possibly miss alerts if the application ever had to restart to recover from a failure.
To address these challenges, Flink provides primitives for a fault-tolerant state that are almost as easy to use as regular member variables.
The most basic type of state in Flink is ValueState, a data type that adds fault tolerance to any variable it wraps. ValueState is a form of keyed state, meaning it is only available in operators that are applied in a keyed context; any operator immediately following DataStream#keyBy. A keyed state of an operator is automatically scoped to the key of the record that is currently processed. In this example, the key is the account id for the current transaction (as declared by keyBy()), and FraudDetector maintains an independent state for each account. ValueState is created using a ValueStateDescriptor which contains metadata about how Flink should manage the variable. The state should be registered before the function starts processing data. The right hook for this is the open() method.
Java public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private transient ValueState\u0026lt;Boolean\u0026gt; flagState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); } Scala @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) } ValueState is a wrapper class, similar to AtomicReference or AtomicLong in the Java standard library. It provides three methods for interacting with its contents; update sets the state, value gets the current value, and clear deletes its contents. If the state for a particular key is empty, such as at the beginning of an application or after calling ValueState#clear, then ValueState#value will return null. Modifications to the object returned by ValueState#value are not guaranteed to be recognized by the system, and so all changes must be performed with ValueState#update. Otherwise, fault tolerance is managed automatically by Flink under the hood, and so you can interact with it like with any standard variable.
Below, you can see an example of how you can use a flag state to track potential fraudulent transactions.
Java @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { // Get the current state for the current key Boolean lastTransactionWasSmall = flagState.value(); // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount() \u0026gt; LARGE_AMOUNT) { // Output an alert downstream Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } // Clean up our state flagState.clear(); } if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // Set the flag to true flagState.update(true); } } Scala override def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { // Get the current state for the current key val lastTransactionWasSmall = flagState.value // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount \u0026gt; FraudDetector.LARGE_AMOUNT) { // Output an alert downstream val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } // Clean up our state flagState.clear() } if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) } } For every transaction, the fraud detector checks the state of the flag for that account. Remember, ValueState is always scoped to the current key, i.e., account. If the flag is non-null, then the last transaction seen for that account was small, and so if the amount for this transaction is large, then the detector outputs a fraud alert.
After that check, the flag state is unconditionally cleared. Either the current transaction caused a fraud alert, and the pattern is over, or the current transaction did not cause an alert, and the pattern is broken and needs to be restarted.
Finally, the transaction amount is checked to see if it is small. If so, then the flag is set so that it can be checked by the next event. Notice that ValueState\u0026lt;Boolean\u0026gt; has three states, unset (null), true, and false, because all ValueState\u0026rsquo;s are nullable. This job only makes use of unset (null) and true to check whether the flag is set or not.
Fraud Detector v2: State + Time = ❤️ # Scammers don\u0026rsquo;t wait long to make their large purchases to reduce the chances their test transaction is noticed. For example, suppose you wanted to set a 1-minute timeout to your fraud detector; i.e., in the previous example transactions 3 and 4 would only be considered fraud if they occurred within 1 minute of each other. Flink\u0026rsquo;s KeyedProcessFunction allows you to set timers that invoke a callback method at some point in time in the future.
Let\u0026rsquo;s see how we can modify our Job to comply with our new requirements:
Whenever the flag is set to true, also set a timer for 1 minute in the future. When the timer fires, reset the flag by clearing its state. If the flag is ever cleared the timer should be canceled. To cancel a timer, you have to remember what time it is set for, and remembering implies state, so you will begin by creating a timer state along with your flag state.
Java private transient ValueState\u0026lt;Boolean\u0026gt; flagState; private transient ValueState\u0026lt;Long\u0026gt; timerState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); ValueStateDescriptor\u0026lt;Long\u0026gt; timerDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;timer-state\u0026#34;, Types.LONG); timerState = getRuntimeContext().getState(timerDescriptor); } Scala @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @transient private var timerState: ValueState[java.lang.Long] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) val timerDescriptor = new ValueStateDescriptor(\u0026#34;timer-state\u0026#34;, Types.LONG) timerState = getRuntimeContext.getState(timerDescriptor) } KeyedProcessFunction#processElement is called with a Context that contains a timer service. The timer service can be used to query the current time, register timers, and delete timers. With this, you can set a timer for 1 minute in the future every time the flag is set and store the timestamp in timerState.
Java if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // set the flag to true flagState.update(true); // set the timer and timer state long timer = context.timerService().currentProcessingTime() + ONE_MINUTE; context.timerService().registerProcessingTimeTimer(timer); timerState.update(timer); } Scala if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) // set the timer and timer state val timer = context.timerService.currentProcessingTime + FraudDetector.ONE_MINUTE context.timerService.registerProcessingTimeTimer(timer) timerState.update(timer) } Processing time is wall clock time, and is determined by the system clock of the machine running the operator.
When a timer fires, it calls KeyedProcessFunction#onTimer. Overriding this method is how you can implement your callback to reset the flag.
Java public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;Alert\u0026gt; out) { // remove flag after 1 minute timerState.clear(); flagState.clear(); } Scala override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Long, Transaction, Alert]#OnTimerContext, out: Collector[Alert]): Unit = { // remove flag after 1 minute timerState.clear() flagState.clear() } Finally, to cancel the timer, you need to delete the registered timer and delete the timer state. You can wrap this in a helper method and call this method instead of flagState.clear().
Java private void cleanUp(Context ctx) throws Exception { // delete timer Long timer = timerState.value(); ctx.timerService().deleteProcessingTimeTimer(timer); // clean up all state timerState.clear(); flagState.clear(); } Scala @throws[Exception] private def cleanUp(ctx: KeyedProcessFunction[Long, Transaction, Alert]#Context): Unit = { // delete timer val timer = timerState.value ctx.timerService.deleteProcessingTimeTimer(timer) // clean up all states timerState.clear() flagState.clear() } And that\u0026rsquo;s it, a fully functional, stateful, distributed streaming application!
Final Application # Java import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; private transient ValueState\u0026lt;Boolean\u0026gt; flagState; private transient ValueState\u0026lt;Long\u0026gt; timerState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); ValueStateDescriptor\u0026lt;Long\u0026gt; timerDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;timer-state\u0026#34;, Types.LONG); timerState = getRuntimeContext().getState(timerDescriptor); } @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { // Get the current state for the current key Boolean lastTransactionWasSmall = flagState.value(); // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount() \u0026gt; LARGE_AMOUNT) { //Output an alert downstream Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } // Clean up our state cleanUp(context); } if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // set the flag to true flagState.update(true); long timer = context.timerService().currentProcessingTime() + ONE_MINUTE; context.timerService().registerProcessingTimeTimer(timer); timerState.update(timer); } } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;Alert\u0026gt; out) { // remove flag after 1 minute timerState.clear(); flagState.clear(); } private void cleanUp(Context ctx) throws Exception { // delete timer Long timer = timerState.value(); ctx.timerService().deleteProcessingTimeTimer(timer); // clean up all state timerState.clear(); flagState.clear(); } } Scala import org.apache.flink.api.common.state.{ValueState, ValueStateDescriptor} import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @transient private var timerState: ValueState[java.lang.Long] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) val timerDescriptor = new ValueStateDescriptor(\u0026#34;timer-state\u0026#34;, Types.LONG) timerState = getRuntimeContext.getState(timerDescriptor) } override def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { // Get the current state for the current key val lastTransactionWasSmall = flagState.value // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount \u0026gt; FraudDetector.LARGE_AMOUNT) { // Output an alert downstream val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } // Clean up our state cleanUp(context) } if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) val timer = context.timerService.currentProcessingTime + FraudDetector.ONE_MINUTE context.timerService.registerProcessingTimeTimer(timer) timerState.update(timer) } } override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Long, Transaction, Alert]#OnTimerContext, out: Collector[Alert]): Unit = { // remove flag after 1 minute timerState.clear() flagState.clear() } @throws[Exception] private def cleanUp(ctx: KeyedProcessFunction[Long, Transaction, Alert]#Context): Unit = { // delete timer val timer = timerState.value ctx.timerService.deleteProcessingTimeTimer(timer) // clean up all states timerState.clear() flagState.clear() } } Expected Output # Running this code with the provided TransactionSource will emit fraud alerts for account 3. You should see the following output in your task manager logs:
2019-08-19 14:22:06,220 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:11,383 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:16,551 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:21,723 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:26,896 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} `}),e.add({id:47,href:"/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/",title:"Generating Watermarks",section:"Event Time",content:` Generating Watermarks # In this section you will learn about the APIs that Flink provides for working with event time timestamps and watermarks. For an introduction to event time, processing time, and ingestion time, please refer to the introduction to event time.
Introduction to Watermark Strategies # In order to work with event time, Flink needs to know the events timestamps, meaning each element in the stream needs to have its event timestamp assigned. This is usually done by accessing/extracting the timestamp from some field in the element by using a TimestampAssigner.
Timestamp assignment goes hand-in-hand with generating watermarks, which tell the system about progress in event time. You can configure this by specifying a WatermarkGenerator.
The Flink API expects a WatermarkStrategy that contains both a TimestampAssigner and WatermarkGenerator. A number of common strategies are available out of the box as static methods on WatermarkStrategy, but users can also build their own strategies when required.
Here is the interface for completeness\u0026rsquo; sake:
public interface WatermarkStrategy\u0026lt;T\u0026gt; extends TimestampAssignerSupplier\u0026lt;T\u0026gt;, WatermarkGeneratorSupplier\u0026lt;T\u0026gt;{ /** * Instantiates a {@link TimestampAssigner} for assigning timestamps according to this * strategy. */ @Override TimestampAssigner\u0026lt;T\u0026gt; createTimestampAssigner(TimestampAssignerSupplier.Context context); /** * Instantiates a WatermarkGenerator that generates watermarks according to this strategy. */ @Override WatermarkGenerator\u0026lt;T\u0026gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context); } As mentioned, you usually don\u0026rsquo;t implement this interface yourself but use the static helper methods on WatermarkStrategy for common watermark strategies or to bundle together a custom TimestampAssigner with a WatermarkGenerator. For example, to use bounded-out-of-orderness watermarks and a lambda function as a timestamp assigner you use this:
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -\u0026gt; event.f0); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withTimestampAssigner(new SerializableTimestampAssigner[(Long, String)] { override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long = element._1 }) Python class FirstElementTimestampAssigner(TimestampAssigner): def extract_timestamp(self, value, record_timestamp): return value[0] WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_timestamp_assigner(FirstElementTimestampAssigner()) Specifying a TimestampAssigner is optional and in most cases you don\u0026rsquo;t actually want to specify one. For example, when using Kafka or Kinesis you would get timestamps directly from the Kafka/Kinesis records.
We will look at the WatermarkGenerator interface later in Writing WatermarkGenerators.
Attention: Both timestamps and watermarks are specified as milliseconds since the Java epoch of 1970-01-01T00:00:00Z. Using Watermark Strategies # There are two places in Flink applications where a WatermarkStrategy can be used: 1) directly on sources and 2) after non-source operation.
The first option is preferable, because it allows sources to exploit knowledge about shards/partitions/splits in the watermarking logic. Sources can usually then track watermarks at a finer level and the overall watermark produced by a source will be more accurate. Specifying a WatermarkStrategy directly on the source usually means you have to use a source specific interface/ Refer to Watermark Strategies and the Kafka Connector for how this works on a Kafka Connector and for more details about how per-partition watermarking works there.
The second option (setting a WatermarkStrategy after arbitrary operations) should only be used if you cannot set a strategy directly on the source:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;MyEvent\u0026gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo); DataStream\u0026lt;MyEvent\u0026gt; withTimestampsAndWatermarks = stream .filter( event -\u0026gt; event.severity() == WARNING ) .assignTimestampsAndWatermarks(\u0026lt;watermark strategy\u0026gt;); withTimestampsAndWatermarks .keyBy( (event) -\u0026gt; event.getGroup() ) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce( (a, b) -\u0026gt; a.add(b) ) .addSink(...); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter()) val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(\u0026lt;watermark strategy\u0026gt;) withTimestampsAndWatermarks .keyBy( _.getGroup ) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce( (a, b) =\u0026gt; a.add(b) ) .addSink(...) Python env = StreamExecutionEnvironment.get_execution_environment() # currently read_file is not supported in PyFlink stream = env \\ .read_text_file(my_file_path, charset) \\ .map(lambda s: MyEvent.from_string(s)) with_timestamp_and_watermarks = stream \\ .filter(lambda e: e.severity() == WARNING) \\ .assign_timestamp_and_watermarks(\u0026lt;watermark strategy\u0026gt;) with_timestamp_and_watermarks \\ .key_by(lambda e: e.get_group()) \\ .window(TumblingEventTimeWindows.of(Time.seconds(10))) \\ .reduce(lambda a, b: a.add(b)) \\ .add_sink(...) Using a WatermarkStrategy this way takes a stream and produce a new stream with timestamped elements and watermarks. If the original stream had timestamps and/or watermarks already, the timestamp assigner overwrites them.
Dealing With Idle Sources # If one of the input splits/partitions/shards does not carry events for a while this means that the WatermarkGenerator also does not get any new information on which to base a watermark. We call this an idle input or an idle source. This is a problem because it can happen that some of your partitions do still carry events. In that case, the watermark will be held back, because it is computed as the minimum over all the different parallel watermarks.
To deal with this, you can use a WatermarkStrategy that will detect idleness and mark an input as idle. WatermarkStrategy provides a convenience helper for this:
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)) Python WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_idleness(Duration.of_minutes(1)) Watermark alignment Beta # In the previous paragraph we discussed a situation when splits/partitions/shards or sources are idle and can stall increasing watermarks. On the other side of the spectrum, a split/partition/shard or source may process records very fast and in turn increase its watermark relatively faster than the others. This on its own is not a problem per se. However, for downstream operators that are using watermarks to emit some data it can actually become a problem.
In this case, contrary to idle sources, the watermark of such downstream operator (like windowed joins on aggregations) can progress. However, such operator might need to buffer excessive amount of data coming from the fast inputs, as the minimal watermark from all of its inputs is held back by the lagging input. All records emitted by the fast input will hence have to be buffered in the said downstream operator state, which can lead into uncontrollable growth of the operator\u0026rsquo;s state.
In order to address the issue, you can enable watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest. You can enable alignment for every source separately:
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withWatermarkAlignment(\u0026#34;alignment-group-1\u0026#34;, Duration.ofSeconds(20), Duration.ofSeconds(1)); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withWatermarkAlignment(\u0026#34;alignment-group-1\u0026#34;, Duration.ofSeconds(20), Duration.ofSeconds(1)) Python WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_watermark_alignment(\u0026#34;alignment-group-1\u0026#34;, Duration.of_seconds(20), Duration.of_seconds(1)) Note: You can enable watermark alignment only for FLIP-27 sources. It does not work for legacy or if applied after the source via DataStream#assignTimestampsAndWatermarks. When enabling the alignment, you need to tell Flink, which group should the source belong. You do that by providing a label (e.g. alignment-group-1) which bind together all sources that share it. Moreover, you have to tell the maximal drift from the current minimal watermarks across all sources belonging to that group. The third parameter describes how often the current maximal watermark should be updated. The downside of frequent updates is that there will be more RPC messages travelling between TMs and the JM.
In order to achieve the alignment Flink will pause consuming from the source/task, which generated watermark that is too far into the future. In the meantime it will continue reading records from other sources/tasks which can move the combined watermark forward and that way unblock the faster one.
Note: As of 1.15, Flink supports aligning across tasks of the same source and/or different sources. It does not support aligning splits/partitions/shards in the same task.
In a case where there are e.g. two Kafka partitions that produce watermarks at different pace, that get assigned to the same task watermark might not behave as expected. Fortunately, worst case it should not perform worse than without alignment.
Given the limitation above, we suggest applying watermark alignment in two situations:
You have two different sources (e.g. Kafka and File) that produce watermarks at different speeds You run your source with parallelism equal to the number of splits/shards/partitions, which results in every subtask being assigned a single unit of work. Writing WatermarkGenerators # A TimestampAssigner is a simple function that extracts a field from an event, we therefore don\u0026rsquo;t need to look at them in detail. A WatermarkGenerator, on the other hand, is a bit more complicated to write and we will look at how you can do that in the next two sections. This is the WatermarkGenerator interface:
/** * The {@code WatermarkGenerator} generates watermarks either based on events or * periodically (in a fixed interval). * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Note:\u0026lt;/b\u0026gt; This WatermarkGenerator subsumes the previous distinction between the * {@code AssignerWithPunctuatedWatermarks} and the {@code AssignerWithPeriodicWatermarks}. */ @Public public interface WatermarkGenerator\u0026lt;T\u0026gt; { /** * Called for every event, allows the watermark generator to examine * and remember the event timestamps, or to emit a watermark based on * the event itself. */ void onEvent(T event, long eventTimestamp, WatermarkOutput output); /** * Called periodically, and might emit a new watermark, or not. * * \u0026lt;p\u0026gt;The interval in which this method is called and Watermarks * are generated depends on {@link ExecutionConfig#getAutoWatermarkInterval()}. */ void onPeriodicEmit(WatermarkOutput output); } There are two different styles of watermark generation: periodic and punctuated.
A periodic generator usually observes the incoming events via onEvent() and then emits a watermark when the framework calls onPeriodicEmit().
A puncutated generator will look at events in onEvent() and wait for special marker events or punctuations that carry watermark information in the stream. When it sees one of these events it emits a watermark immediately. Usually, punctuated generators don\u0026rsquo;t emit a watermark from onPeriodicEmit().
We will look at how to implement generators for each style next.
Writing a Periodic WatermarkGenerator # A periodic generator observes stream events and generates watermarks periodically (possibly depending on the stream elements, or purely based on processing time).
The interval (every n milliseconds) in which the watermark will be generated is defined via ExecutionConfig.setAutoWatermarkInterval(...). The generators\u0026rsquo;s onPeriodicEmit() method will be called each time, and a new watermark will be emitted if the returned watermark is non-null and larger than the previous watermark.
Here we show two simple examples of watermark generators that use periodic watermark generation. Note that Flink ships with BoundedOutOfOrdernessWatermarks, which is a WatermarkGenerator that works similarly to the BoundedOutOfOrdernessGenerator shown below. You can read about using that here.
Java /** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */ public class BoundedOutOfOrdernessGenerator implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { private final long maxOutOfOrderness = 3500; // 3.5 seconds private long currentMaxTimestamp; @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp); } @Override public void onPeriodicEmit(WatermarkOutput output) { // emit the watermark as current highest timestamp minus the out-of-orderness bound output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)); } } /** * This generator generates watermarks that are lagging behind processing time * by a fixed amount. It assumes that elements arrive in Flink after a bounded delay. */ public class TimeLagWatermarkGenerator implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { private final long maxTimeLag = 5000; // 5 seconds @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { // don\u0026#39;t need to do anything because we work on processing time } @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)); } } Scala /** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */ class BoundedOutOfOrdernessGenerator extends WatermarkGenerator[MyEvent] { val maxOutOfOrderness = 3500L // 3.5 seconds var currentMaxTimestamp: Long = _ override def onEvent(element: MyEvent, eventTimestamp: Long, output: WatermarkOutput): Unit = { currentMaxTimestamp = max(eventTimestamp, currentMaxTimestamp) } override def onPeriodicEmit(output: WatermarkOutput): Unit = { // emit the watermark as current highest timestamp minus the out-of-orderness bound output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)) } } /** * This generator generates watermarks that are lagging behind processing * time by a fixed amount. It assumes that elements arrive in Flink after * a bounded delay. */ class TimeLagWatermarkGenerator extends WatermarkGenerator[MyEvent] { val maxTimeLag = 5000L // 5 seconds override def onEvent(element: MyEvent, eventTimestamp: Long, output: WatermarkOutput): Unit = { // don\u0026#39;t need to do anything because we work on processing time } override def onPeriodicEmit(output: WatermarkOutput): Unit = { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)) } } Python Still not supported in Python API. Writing a Punctuated WatermarkGenerator # A punctuated watermark generator will observe the stream of events and emit a watermark whenever it sees a special element that carries watermark information.
This is how you can implement a punctuated generator that emits a watermark whenever an event indicates that it carries a certain marker:
Java public class PunctuatedAssigner implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())); } } @Override public void onPeriodicEmit(WatermarkOutput output) { // don\u0026#39;t need to do anything because we emit in reaction to events above } } Scala class PunctuatedAssigner extends WatermarkGenerator[MyEvent] { override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())) } } override def onPeriodicEmit(): Unit = { // don\u0026#39;t need to do anything because we emit in reaction to events above } } Python Still not supported in Python API. Note: It is possible to generate a watermark on every single event. However, because each watermark causes some computation downstream, an excessive number of watermarks degrades performance. Watermark Strategies and the Kafka Connector # When using Apache Kafka as a data source, each Kafka partition may have a simple event time pattern (ascending timestamps or bounded out-of-orderness). However, when consuming streams from Kafka, multiple partitions often get consumed in parallel, interleaving the events from the partitions and destroying the per-partition patterns (this is inherent in how Kafka\u0026rsquo;s consumer clients work).
In that case, you can use Flink\u0026rsquo;s Kafka-partition-aware watermark generation. Using that feature, watermarks are generated inside the Kafka consumer, per Kafka partition, and the per-partition watermarks are merged in the same way as watermarks are merged on stream shuffles.
For example, if event timestamps are strictly ascending per Kafka partition, generating per-partition watermarks with the ascending timestamps watermark generator will result in perfect overall watermarks. Note, that we don\u0026rsquo;t provide a TimestampAssigner in the example, the timestamps of the Kafka records themselves will be used instead.
The illustrations below show how to use the per-Kafka-partition watermark generation, and how watermarks propagate through the streaming dataflow in that case.
Java FlinkKafkaConsumer\u0026lt;MyType\u0026gt; kafkaSource = new FlinkKafkaConsumer\u0026lt;\u0026gt;(\u0026#34;myTopic\u0026#34;, schema, props); kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20))); DataStream\u0026lt;MyType\u0026gt; stream = env.addSource(kafkaSource); Scala val kafkaSource = new FlinkKafkaConsumer[MyType](\u0026#34;myTopic\u0026#34;, schema, props) kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20))) val stream: DataStream[MyType] = env.addSource(kafkaSource) Python kafka_source = FlinkKafkaConsumer(\u0026#34;timer-stream-source\u0026#34;, schema, props) stream = env .add_source(kafka_source) .assign_timestamps_and_watermarks( WatermarkStrategy .for_bounded_out_of_orderness(Duration.of_seconds(20))) How Operators Process Watermarks # As a general rule, operators are required to completely process a given watermark before forwarding it downstream. For example, WindowOperator will first evaluate all windows that should be fired, and only after producing all of the output triggered by the watermark will the watermark itself be sent downstream. In other words, all elements produced due to occurrence of a watermark will be emitted before the watermark.
The same rule applies to TwoInputStreamOperator. However, in this case the current watermark of the operator is defined as the minimum of both of its inputs.
The details of this behavior are defined by the implementations of the OneInputStreamOperator#processWatermark, TwoInputStreamOperator#processWatermark1 and TwoInputStreamOperator#processWatermark2 methods.
The Deprecated AssignerWithPeriodicWatermarks and AssignerWithPunctuatedWatermarks # Prior to introducing the current abstraction of WatermarkStrategy, TimestampAssigner, and WatermarkGenerator, Flink used AssignerWithPeriodicWatermarks and AssignerWithPunctuatedWatermarks. You will still see them in the API but it is recommended to use the new interfaces because they offer a clearer separation of concerns and also unify periodic and punctuated styles of watermark generation.
Back to top
`}),e.add({id:48,href:"/flink/flink-docs-master/docs/dev/table/sql/gettingstarted/",title:"Getting Started",section:"SQL",content:` Getting Started # Flink SQL makes it simple to develop streaming applications using standard SQL. It is easy to learn Flink if you have ever worked with a database or SQL like system by remaining ANSI-SQL 2011 compliant. This tutorial will help you get started quickly with a Flink SQL development environment.
Prerequisites # You only need to have basic knowledge of SQL to follow along. No other programming experience is assumed.
Installation # There are multiple ways to install Flink. For experimentation, the most common option is to download the binaries and run them locally. You can follow the steps in local installation to set up an environment for the rest of the tutorial.
Once you\u0026rsquo;re all set, use the following command to start a local cluster from the installation folder:
./bin/start-cluster.sh Once started, the Flink WebUI on localhost:8081 is available locally, from which you can monitor the different jobs.
SQL Client # The SQL Client is an interactive client to submit SQL queries to Flink and visualize the results. To start the SQL client, run the sql-client script from the installation folder.
./bin/sql-client.sh Hello World # Once the SQL client, our query editor, is up and running, it\u0026rsquo;s time to start writing queries. Let\u0026rsquo;s start with printing \u0026lsquo;Hello World\u0026rsquo;, using the following simple query:
SELECT \u0026#39;Hello World\u0026#39;; Running the HELP command lists the full set of supported SQL statements. Let\u0026rsquo;s run one such command, SHOW, to see a full list of Flink built-in functions.
SHOW FUNCTIONS; These functions provide users with a powerful toolbox of functionality when developing SQL queries. For example, CURRENT_TIMESTAMP will print the machine\u0026rsquo;s current system time where it is executed.
SELECT CURRENT_TIMESTAMP; Back to top
Source Tables # As with all SQL engines, Flink queries operate on top of tables. It differs from a traditional database because Flink does not manage data at rest locally; instead, its queries operate continuously over external tables.
Flink data processing pipelines begin with source tables. Source tables produce rows operated over during the query\u0026rsquo;s execution; they are the tables referenced in the FROM clause of a query. These could be Kafka topics, databases, filesystems, or any other system that Flink knows how to consume.
Tables can be defined through the SQL client or using environment config file. The SQL client support SQL DDL commands similar to traditional SQL. Standard SQL DDL is used to create, alter, drop tables.
Flink has a support for different connectors and formats that can be used with tables. Following is an example to define a source table backed by a CSV file with emp_id, name, dept_id as columns in a CREATE table statement.
CREATE TABLE employee_information ( emp_id INT, name VARCHAR, dept_id INT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/something.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); A continuous query can be defined from this table that reads new rows as they are made available and immediately outputs their results. For example, we can filter for just those employees who work in department 1.
SELECT * from employee_information WHERE dept_id = 1; Back to top
Continuous Queries # While not designed initially with streaming semantics in mind, SQL is a powerful tool for building continuous data pipelines. Where Flink SQL differs from traditional database queries is that is continuously consuming rows as the arrives and produces updates to its results.
A continuous query never terminates and produces a dynamic table as a result. Dynamic tables are the core concept of Flink\u0026rsquo;s Table API and SQL support for streaming data.
Aggregations on continuous streams need to store aggregated results continuously during the execution of the query. For example, suppose you need to count the number of employees for each department from an incoming data stream. The query needs to maintain the most up to date count for each department to output timely results as new rows are processed.
SELECT dept_id, COUNT(*) as emp_count FROM employee_information GROUP BY dept_id; Such queries are considered stateful. Flink\u0026rsquo;s advanced fault-tolerance mechanism will maintain internal state and consistency, so queries always return the correct result, even in the face of hardware failure.
Sink Tables # When running this query, the SQL client provides output in real-time but in a read-only fashion. Storing results - to power a report or dashboard - requires writing out to another table. This can be achieved using an INSERT INTO statement. The table referenced in this clause is known as a sink table. An INSERT INTO statement will be submitted as a detached query to the Flink cluster.
INSERT INTO department_counts SELECT dept_id, COUNT(*) as emp_count FROM employee_information; Once submitted, this will run and store the results into the sink table directly, instead of loading the results into the system memory.
Back to top
Looking for Help! # If you get stuck, check out the community support resources. In particular, Apache Flink\u0026rsquo;s user mailing list consistently ranks as one of the most active of any Apache project and a great way to get help quickly.
Resources to Learn more # SQL: Supported operations and syntax for SQL. SQL Client: Play around with Flink SQL and submit a table program to a cluster without programming knowledge Concepts \u0026amp; Common API: Shared concepts and APIs of the Table API and SQL. Streaming Concepts: Streaming-specific documentation for the Table API or SQL such as configuration of time attributes and handling of updating results. Built-in Functions: Supported functions in Table API and SQL. Connect to External Systems: Available connectors and formats for reading and writing data to external systems. Back to top
`}),e.add({id:49,href:"/flink/flink-docs-master/docs/libs/gelly/graph_api/",title:"Graph API",section:"Graphs",content:` Graph API # Graph Representation # In Gelly, a Graph is represented by a DataSet of vertices and a DataSet of edges.
The Graph nodes are represented by the Vertex type. A Vertex is defined by a unique ID and a value. Vertex IDs should implement the Comparable interface. Vertices without value can be represented by setting the value type to NullValue.
Java // create a new vertex with a Long ID and a String value Vertex\u0026lt;Long, String\u0026gt; v = new Vertex\u0026lt;Long, String\u0026gt;(1L, \u0026#34;foo\u0026#34;); // create a new vertex with a Long ID and no value Vertex\u0026lt;Long, NullValue\u0026gt; v = new Vertex\u0026lt;Long, NullValue\u0026gt;(1L, NullValue.getInstance()); Scala // create a new vertex with a Long ID and a String value val v = new Vertex(1L, \u0026#34;foo\u0026#34;) // create a new vertex with a Long ID and no value val v = new Vertex(1L, NullValue.getInstance()) The graph edges are represented by the Edge type. An Edge is defined by a source ID (the ID of the source Vertex), a target ID (the ID of the target Vertex) and an optional value. The source and target IDs should be of the same type as the Vertex IDs. Edges with no value have a NullValue value type.
Java Edge\u0026lt;Long, Double\u0026gt; e = new Edge\u0026lt;Long, Double\u0026gt;(1L, 2L, 0.5); // reverse the source and target of this edge Edge\u0026lt;Long, Double\u0026gt; reversed = e.reverse(); Double weight = e.getValue(); // weight = 0.5 Scala val e = new Edge(1L, 2L, 0.5) // reverse the source and target of this edge val reversed = e.reverse val weight = e.getValue // weight = 0.5 In Gelly an Edge is always directed from the source vertex to the target vertex. A Graph may be undirected if for every Edge it contains a matching Edge from the target vertex to the source vertex.
Back to top
Graph Creation # You can create a Graph in the following ways:
from a DataSet of edges and an optional DataSet of vertices: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; vertices = ...; DataSet\u0026lt;Edge\u0026lt;String, Double\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertices: DataSet[Vertex[String, Long]] = ... val edges: DataSet[Edge[String, Double]] = ... val graph = Graph.fromDataSet(vertices, edges, env) from a DataSet of Tuple2 representing the edges. Gelly will convert each Tuple2 to an Edge, where the first field will be the source ID and the second field will be the target ID. Both vertex and edge values will be set to NullValue. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;String, NullValue, NullValue\u0026gt; graph = Graph.fromTuple2DataSet(edges, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val edges: DataSet[(String, String)] = ... val graph = Graph.fromTuple2DataSet(edges, env) from a DataSet of Tuple3 and an optional DataSet of Tuple2. In this case, Gelly will convert each Tuple3 to an Edge, where the first field will be the source ID, the second field will be the target ID and the third field will be the edge value. Equivalently, each Tuple2 will be converted to a Vertex, where the first field will be the vertex ID and the second field will be the vertex value: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; vertexTuples = env.readCsvFile(\u0026#34;path/to/vertex/input\u0026#34;).types(String.class, Long.class); DataSet\u0026lt;Tuple3\u0026lt;String, String, Double\u0026gt;\u0026gt; edgeTuples = env.readCsvFile(\u0026#34;path/to/edge/input\u0026#34;).types(String.class, String.class, Double.class); Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env); from a CSV file of Edge data and an optional CSV file of Vertex data. In this case, Gelly will convert each row from the Edge CSV file to an Edge, where the first field will be the source ID, the second field will be the target ID and the third field (if present) will be the edge value. Equivalently, each row from the optional Vertex CSV file will be converted to a Vertex, where the first field will be the vertex ID and the second field (if present) will be the vertex value. In order to get a Graph from a GraphCsvReader one has to specify the types, using one of the following methods: types(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;VV\u0026gt; vertexValue,Class\u0026lt;EV\u0026gt; edgeValue): both vertex and edge values are present. edgeTypes(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;EV\u0026gt; edgeValue): the Graph has edge values, but no vertex values. vertexTypes(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;VV\u0026gt; vertexValue): the Graph has vertex values, but no edge values. keyType(Class\u0026lt;K\u0026gt; vertexKey): the Graph has no vertex values and no edge values. ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a Graph with String Vertex IDs, Long Vertex values and Double Edge values Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromCsvReader(\u0026#34;path/to/vertex/input\u0026#34;, \u0026#34;path/to/edge/input\u0026#34;, env) .types(String.class, Long.class, Double.class); // create a Graph with neither Vertex nor Edge values Graph\u0026lt;Long, NullValue, NullValue\u0026gt; simpleGraph = Graph.fromCsvReader(\u0026#34;path/to/edge/input\u0026#34;, env).keyType(Long.class); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertexTuples = env.readCsvFile[String, Long](\u0026#34;path/to/vertex/input\u0026#34;) val edgeTuples = env.readCsvFile[String, String, Double](\u0026#34;path/to/edge/input\u0026#34;) val graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env) from a CSV file of Edge data and an optional CSV file of Vertex data. In this case, Gelly will convert each row from the Edge CSV file to an Edge. The first field of the each row will be the source ID, the second field will be the target ID and the third field (if present) will be the edge value. If the edges have no associated value, set the edge value type parameter (3rd type argument) to NullValue. You can also specify that the vertices are initialized with a vertex value. If you provide a path to a CSV file via pathVertices, each row of this file will be converted to a Vertex. The first field of each row will be the vertex ID and the second field will be the vertex value. If you provide a vertex value initializer MapFunction via the vertexValueInitializer parameter, then this function is used to generate the vertex values. The set of vertices will be created automatically from the edges input. If the vertices have no associated value, set the vertex value type parameter (2nd type argument) to NullValue. The vertices will then be automatically created from the edges input with vertex value of type NullValue. val env = ExecutionEnvironment.getExecutionEnvironment // create a Graph with String Vertex IDs, Long Vertex values and Double Edge values val graph = Graph.fromCsvReader[String, Long, Double]( pathVertices = \u0026#34;path/to/vertex/input\u0026#34;, pathEdges = \u0026#34;path/to/edge/input\u0026#34;, env = env) // create a Graph with neither Vertex nor Edge values val simpleGraph = Graph.fromCsvReader[Long, NullValue, NullValue]( pathEdges = \u0026#34;path/to/edge/input\u0026#34;, env = env) // create a Graph with Double Vertex values generated by a vertex value initializer and no Edge values val simpleGraph = Graph.fromCsvReader[Long, Double, NullValue]( pathEdges = \u0026#34;path/to/edge/input\u0026#34;, vertexValueInitializer = new MapFunction[Long, Double]() { def map(id: Long): Double = { id.toDouble } }, env = env) from a Collection of edges and an optional Collection of vertices: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); List\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; vertexList = new ArrayList...; List\u0026lt;Edge\u0026lt;Long, String\u0026gt;\u0026gt; edgeList = new ArrayList...; Graph\u0026lt;Long, Long, String\u0026gt; graph = Graph.fromCollection(vertexList, edgeList, env); If no vertex input is provided during Graph creation, Gelly will automatically produce the Vertex DataSet from the edge input. In this case, the created vertices will have no values. Alternatively, you can provide a MapFunction as an argument to the creation method, in order to initialize the Vertex values:
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // initialize the vertex value to be equal to the vertex ID Graph\u0026lt;Long, Long, String\u0026gt; graph = Graph.fromCollection(edgeList, new MapFunction\u0026lt;Long, Long\u0026gt;() { public Long map(Long value) { return value; } }, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertexList = List(...) val edgeList = List(...) val graph = Graph.fromCollection(vertexList, edgeList, env) If no vertex input is provided during Graph creation, Gelly will automatically produce the Vertex DataSet from the edge input. In this case, the created vertices will have no values. Alternatively, you can provide a MapFunction as an argument to the creation method, in order to initialize the Vertex values:
val env = ExecutionEnvironment.getExecutionEnvironment // initialize the vertex value to be equal to the vertex ID val graph = Graph.fromCollection(edgeList, new MapFunction[Long, Long] { def map(id: Long): Long = id }, env) Back to top
Graph Properties # Gelly includes the following methods for retrieving various Graph properties and metrics:
Java // get the Vertex DataSet DataSet\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; getVertices() // get the Edge DataSet DataSet\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; getEdges() // get the IDs of the vertices as a DataSet DataSet\u0026lt;K\u0026gt; getVertexIds() // get the source-target pairs of the edge IDs as a DataSet DataSet\u0026lt;Tuple2\u0026lt;K, K\u0026gt;\u0026gt; getEdgeIds() // get a DataSet of \u0026lt;vertex ID, in-degree\u0026gt; pairs for all vertices DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; inDegrees() // get a DataSet of \u0026lt;vertex ID, out-degree\u0026gt; pairs for all vertices DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; outDegrees() // get a DataSet of \u0026lt;vertex ID, degree\u0026gt; pairs for all vertices, where degree is the sum of in- and out- degrees DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; getDegrees() // get the number of vertices long numberOfVertices() // get the number of edges long numberOfEdges() // get a DataSet of Triplets\u0026lt;srcVertex, trgVertex, edge\u0026gt; DataSet\u0026lt;Triplet\u0026lt;K, VV, EV\u0026gt;\u0026gt; getTriplets() Scala // get the Vertex DataSet getVertices: DataSet[Vertex[K, VV]] // get the Edge DataSet getEdges: DataSet[Edge[K, EV]] // get the IDs of the vertices as a DataSet getVertexIds: DataSet[K] // get the source-target pairs of the edge IDs as a DataSet getEdgeIds: DataSet[(K, K)] // get a DataSet of \u0026lt;vertex ID, in-degree\u0026gt; pairs for all vertices inDegrees: DataSet[(K, LongValue)] // get a DataSet of \u0026lt;vertex ID, out-degree\u0026gt; pairs for all vertices outDegrees: DataSet[(K, LongValue)] // get a DataSet of \u0026lt;vertex ID, degree\u0026gt; pairs for all vertices, where degree is the sum of in- and out- degrees getDegrees: DataSet[(K, LongValue)] // get the number of vertices numberOfVertices: Long // get the number of edges numberOfEdges: Long // get a DataSet of Triplets\u0026lt;srcVertex, trgVertex, edge\u0026gt; getTriplets: DataSet[Triplet[K, VV, EV]] Back to top
Graph Transformations # Map: Gelly provides specialized methods for applying a map transformation on the vertex values or edge values. mapVertices and mapEdges return a new Graph, where the IDs of the vertices (or edges) remain unchanged, while the values are transformed according to the provided user-defined map function. The map functions also allow changing the type of the vertex or edge values. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); // increment each vertex value by one Graph\u0026lt;Long, Long, Long\u0026gt; updatedGraph = graph.mapVertices( new MapFunction\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Long\u0026gt;() { public Long map(Vertex\u0026lt;Long, Long\u0026gt; value) { return value.getValue() + 1; } }); Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph = Graph.fromDataSet(vertices, edges, env) // increment each vertex value by one val updatedGraph = graph.mapVertices(v =\u0026gt; v.getValue + 1) Translate: Gelly provides specialized methods for translating the value and/or type of vertex and edge IDs (translateGraphIDs), vertex values (translateVertexValues), or edge values (translateEdgeValues). Translation is performed by the user-defined map function, several of which are provided in the org.apache.flink.graph.asm.translate package. The same MapFunction can be used for all the three translate methods. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); // translate each vertex and edge ID to a String Graph\u0026lt;String, Long, Long\u0026gt; updatedGraph = graph.translateGraphIds( new MapFunction\u0026lt;Long, String\u0026gt;() { public String map(Long id) { return id.toString(); } }); // translate vertex IDs, edge IDs, vertex values, and edge values to LongValue Graph\u0026lt;LongValue, LongValue, LongValue\u0026gt; updatedGraph = graph .translateGraphIds(new LongToLongValue()) .translateVertexValues(new LongToLongValue()) .translateEdgeValues(new LongToLongValue()); Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph = Graph.fromDataSet(vertices, edges, env) // translate each vertex and edge ID to a String val updatedGraph = graph.translateGraphIds(id =\u0026gt; id.toString) Filter: A filter transformation applies a user-defined filter function on the vertices or edges of the Graph. filterOnEdges will create a sub-graph of the original graph, keeping only the edges that satisfy the provided predicate. Note that the vertex dataset will not be modified. Respectively, filterOnVertices applies a filter on the vertices of the graph. Edges whose source and/or target do not satisfy the vertex predicate are removed from the resulting edge dataset. The subgraph method can be used to apply a filter function to the vertices and the edges at the same time. Java Graph\u0026lt;Long, Long, Long\u0026gt; graph = ...; graph.subgraph( new FilterFunction\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;() { public boolean filter(Vertex\u0026lt;Long, Long\u0026gt; vertex) { // keep only vertices with positive values return (vertex.getValue() \u0026gt; 0); } }, new FilterFunction\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt;() { public boolean filter(Edge\u0026lt;Long, Long\u0026gt; edge) { // keep only edges with negative values return (edge.getValue() \u0026lt; 0); } }) Scala val graph: Graph[Long, Long, Long] = ... // keep only vertices with positive values // and only edges with negative values graph.subgraph((vertex =\u0026gt; vertex.getValue \u0026gt; 0), (edge =\u0026gt; edge.getValue \u0026lt; 0)) Join: Gelly provides specialized methods for joining the vertex and edge datasets with other input datasets. joinWithVertices joins the vertices with a Tuple2 input data set. The join is performed using the vertex ID and the first field of the Tuple2 input as the join keys. The method returns a new Graph where the vertex values have been updated according to a provided user-defined transformation function. Similarly, an input dataset can be joined with the edges, using one of three methods. joinWithEdges expects an input DataSet of Tuple3 and joins on the composite key of both source and target vertex IDs. joinWithEdgesOnSource expects a DataSet of Tuple2 and joins on the source key of the edges and the first attribute of the input dataset and joinWithEdgesOnTarget expects a DataSet of Tuple2 and joins on the target key of the edges and the first attribute of the input dataset. All three methods apply a transformation function on the edge and the input data set values. Note that if the input dataset contains a key multiple times, all Gelly join methods will only consider the first value encountered. Java Graph\u0026lt;Long, Double, Double\u0026gt; network = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, LongValue\u0026gt;\u0026gt; vertexOutDegrees = network.outDegrees(); // assign the transition probabilities as the edge weights Graph\u0026lt;Long, Double, Double\u0026gt; networkWithWeights = network.joinWithEdgesOnSource(vertexOutDegrees, new VertexJoinFunction\u0026lt;Double, LongValue\u0026gt;() { public Double vertexJoin(Double vertexValue, LongValue inputValue) { return vertexValue / inputValue.getValue(); } }); Scala val network: Graph[Long, Double, Double] = ... val vertexOutDegrees: DataSet[(Long, LongValue)] = network.outDegrees // assign the transition probabilities as the edge weights val networkWithWeights = network.joinWithEdgesOnSource(vertexOutDegrees, (v1: Double, v2: LongValue) =\u0026gt; v1 / v2.getValue) Reverse: the reverse() method returns a new Graph where the direction of all edges has been reversed.
Undirected: In Gelly, a Graph is always directed. Undirected graphs can be represented by adding all opposite-direction edges to a graph. For this purpose, Gelly provides the getUndirected() method.
Union: Gelly\u0026rsquo;s union() method performs a union operation on the vertex and edge sets of the specified graph and the current graph. Duplicate vertices are removed from the resulting Graph, while if duplicate edges exist, these will be preserved.
Difference: Gelly\u0026rsquo;s difference() method performs a difference on the vertex and edge sets of the current graph and the specified graph.
Intersect: Gelly\u0026rsquo;s intersect() method performs an intersect on the edge sets of the current graph and the specified graph. The result is a new Graph that contains all edges that exist in both input graphs. Two edges are considered equal, if they have the same source identifier, target identifier and edge value. Vertices in the resulting graph have no value. If vertex values are required, one can for example retrieve them from one of the input graphs using the joinWithVertices() method. Depending on the parameter distinct, equal edges are either contained once in the resulting Graph or as often as there are pairs of equal edges in the input graphs.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create first graph from edges {(1, 3, 12) (1, 3, 13), (1, 3, 13)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges1 = ...; Graph\u0026lt;Long, NullValue, Long\u0026gt; graph1 = Graph.fromCollection(edges1, env); // create second graph from edges {(1, 3, 13)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges2 = ...; Graph\u0026lt;Long, NullValue, Long\u0026gt; graph2 = Graph.fromCollection(edges2, env); // Using distinct = true results in {(1,3,13)} Graph\u0026lt;Long, NullValue, Long\u0026gt; intersect1 = graph1.intersect(graph2, true); // Using distinct = false results in {(1,3,13),(1,3,13)} as there is one edge pair Graph\u0026lt;Long, NullValue, Long\u0026gt; intersect2 = graph1.intersect(graph2, false); Scala val env = ExecutionEnvironment.getExecutionEnvironment // create first graph from edges {(1, 3, 12) (1, 3, 13), (1, 3, 13)} val edges1: List[Edge[Long, Long]] = ... val graph1 = Graph.fromCollection(edges1, env) // create second graph from edges {(1, 3, 13)} val edges2: List[Edge[Long, Long]] = ... val graph2 = Graph.fromCollection(edges2, env) // Using distinct = true results in {(1,3,13)} val intersect1 = graph1.intersect(graph2, true) // Using distinct = false results in {(1,3,13),(1,3,13)} as there is one edge pair val intersect2 = graph1.intersect(graph2, false) - Back to top
Graph Mutations # Gelly includes the following methods for adding and removing vertices and edges from an input Graph:
Java // adds a Vertex to the Graph. If the Vertex already exists, it will not be added again. Graph\u0026lt;K, VV, EV\u0026gt; addVertex(final Vertex\u0026lt;K, VV\u0026gt; vertex) // adds a list of vertices to the Graph. If the vertices already exist in the graph, they will not be added once more. Graph\u0026lt;K, VV, EV\u0026gt; addVertices(List\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; verticesToAdd) // adds an Edge to the Graph. If the source and target vertices do not exist in the graph, they will also be added. Graph\u0026lt;K, VV, EV\u0026gt; addEdge(Vertex\u0026lt;K, VV\u0026gt; source, Vertex\u0026lt;K, VV\u0026gt; target, EV edgeValue) // adds a list of edges to the Graph. When adding an edge for a non-existing set of vertices, the edge is considered invalid and ignored. Graph\u0026lt;K, VV, EV\u0026gt; addEdges(List\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; newEdges) // removes the given Vertex and its edges from the Graph. Graph\u0026lt;K, VV, EV\u0026gt; removeVertex(Vertex\u0026lt;K, VV\u0026gt; vertex) // removes the given list of vertices and their edges from the Graph Graph\u0026lt;K, VV, EV\u0026gt; removeVertices(List\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; verticesToBeRemoved) // removes *all* edges that match the given Edge from the Graph. Graph\u0026lt;K, VV, EV\u0026gt; removeEdge(Edge\u0026lt;K, EV\u0026gt; edge) // removes *all* edges that match the edges in the given list Graph\u0026lt;K, VV, EV\u0026gt; removeEdges(List\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; edgesToBeRemoved) Scala // adds a Vertex to the Graph. If the Vertex already exists, it will not be added again. addVertex(vertex: Vertex[K, VV]) // adds a list of vertices to the Graph. If the vertices already exist in the graph, they will not be added once more. addVertices(verticesToAdd: List[Vertex[K, VV]]) // adds an Edge to the Graph. If the source and target vertices do not exist in the graph, they will also be added. addEdge(source: Vertex[K, VV], target: Vertex[K, VV], edgeValue: EV) // adds a list of edges to the Graph. When adding an edge for a non-existing set of vertices, the edge is considered invalid and ignored. addEdges(edges: List[Edge[K, EV]]) // removes the given Vertex and its edges from the Graph. removeVertex(vertex: Vertex[K, VV]) // removes the given list of vertices and their edges from the Graph removeVertices(verticesToBeRemoved: List[Vertex[K, VV]]) // removes *all* edges that match the given Edge from the Graph. removeEdge(edge: Edge[K, EV]) // removes *all* edges that match the edges in the given list removeEdges(edgesToBeRemoved: List[Edge[K, EV]]) Neighborhood Methods # Neighborhood methods allow vertices to perform an aggregation on their first-hop neighborhood. reduceOnEdges() can be used to compute an aggregation on the values of the neighboring edges of a vertex and reduceOnNeighbors() can be used to compute an aggregation on the values of the neighboring vertices. These methods assume associative and commutative aggregations and exploit combiners internally, significantly improving performance. The neighborhood scope is defined by the EdgeDirection parameter, which takes the values IN, OUT or ALL. IN will gather all in-coming edges (neighbors) of a vertex, OUT will gather all out-going edges (neighbors), while ALL will gather all edges (neighbors).
For example, assume that you want to select the minimum weight of all out-edges for each vertex in the following graph:
The following code will collect the out-edges for each vertex and apply the SelectMinWeight() user-defined function on each of the resulting neighborhoods:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; minWeights = graph.reduceOnEdges(new SelectMinWeight(), EdgeDirection.OUT); // user-defined function to select the minimum weight static final class SelectMinWeight implements ReduceEdgesFunction\u0026lt;Double\u0026gt; { @Override public Double reduceEdges(Double firstEdgeValue, Double secondEdgeValue) { return Math.min(firstEdgeValue, secondEdgeValue); } } Scala val graph: Graph[Long, Long, Double] = ... val minWeights = graph.reduceOnEdges(new SelectMinWeight, EdgeDirection.OUT) // user-defined function to select the minimum weight final class SelectMinWeight extends ReduceEdgesFunction[Double] { override def reduceEdges(firstEdgeValue: Double, secondEdgeValue: Double): Double = { Math.min(firstEdgeValue, secondEdgeValue) } } Similarly, assume that you would like to compute the sum of the values of all in-coming neighbors, for every vertex. The following code will collect the in-coming neighbors for each vertex and apply the SumValues() user-defined function on each neighborhood:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithSum = graph.reduceOnNeighbors(new SumValues(), EdgeDirection.IN); // user-defined function to sum the neighbor values static final class SumValues implements ReduceNeighborsFunction\u0026lt;Long\u0026gt; { @Override public Long reduceNeighbors(Long firstNeighbor, Long secondNeighbor) { return firstNeighbor + secondNeighbor; } } Scala val graph: Graph[Long, Long, Double] = ... val verticesWithSum = graph.reduceOnNeighbors(new SumValues, EdgeDirection.IN) // user-defined function to sum the neighbor values final class SumValues extends ReduceNeighborsFunction[Long] { override def reduceNeighbors(firstNeighbor: Long, secondNeighbor: Long): Long = { firstNeighbor + secondNeighbor } } When the aggregation function is not associative and commutative or when it is desirable to return more than one values per vertex, one can use the more general groupReduceOnEdges() and groupReduceOnNeighbors() methods. These methods return zero, one or more values per vertex and provide access to the whole neighborhood.
For example, the following code will output all the vertex pairs which are connected with an edge having a weight of 0.5 or more:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; vertexPairs = graph.groupReduceOnNeighbors(new SelectLargeWeightNeighbors(), EdgeDirection.OUT); // user-defined function to select the neighbors which have edges with weight \u0026gt; 0.5 static final class SelectLargeWeightNeighbors implements NeighborsFunctionWithVertexValue\u0026lt;Long, Long, Double, Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; { @Override public void iterateNeighbors(Vertex\u0026lt;Long, Long\u0026gt; vertex, Iterable\u0026lt;Tuple2\u0026lt;Edge\u0026lt;Long, Double\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; neighbors, Collector\u0026lt;Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; out) { for (Tuple2\u0026lt;Edge\u0026lt;Long, Double\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; neighbor : neighbors) { if (neighbor.f0.f2 \u0026gt; 0.5) { out.collect(new Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;(vertex, neighbor.f1)); } } } } Scala val graph: Graph[Long, Long, Double] = ... val vertexPairs = graph.groupReduceOnNeighbors(new SelectLargeWeightNeighbors, EdgeDirection.OUT) // user-defined function to select the neighbors which have edges with weight \u0026gt; 0.5 final class SelectLargeWeightNeighbors extends NeighborsFunctionWithVertexValue[Long, Long, Double, (Vertex[Long, Long], Vertex[Long, Long])] { override def iterateNeighbors(vertex: Vertex[Long, Long], neighbors: Iterable[(Edge[Long, Double], Vertex[Long, Long])], out: Collector[(Vertex[Long, Long], Vertex[Long, Long])]) = { for (neighbor \u0026lt;- neighbors) { if (neighbor._1.getValue() \u0026gt; 0.5) { out.collect(vertex, neighbor._2) } } } } When the aggregation computation does not require access to the vertex value (for which the aggregation is performed), it is advised to use the more efficient EdgesFunction and NeighborsFunction for the user-defined functions. When access to the vertex value is required, one should use EdgesFunctionWithVertexValue and NeighborsFunctionWithVertexValue instead.
Back to top
Graph Validation # Gelly provides a simple utility for performing validation checks on input graphs. Depending on the application context, a graph may or may not be valid according to certain criteria. For example, a user might need to validate whether their graph contains duplicate edges or whether its structure is bipartite. In order to validate a graph, one can define a custom GraphValidator and implement its validate() method. InvalidVertexIdsValidator is Gelly\u0026rsquo;s pre-defined validator. It checks that the edge set contains valid vertex IDs, i.e. that all edge IDs also exist in the vertex IDs set.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a list of vertices with IDs = {1, 2, 3, 4, 5} List\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; vertices = ...; // create a list of edges with IDs = {(1, 2) (1, 3), (2, 4), (5, 6)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromCollection(vertices, edges, env); // will return false: 6 is an invalid ID graph.validate(new InvalidVertexIdsValidator\u0026lt;Long, Long, Long\u0026gt;()); Scala val env = ExecutionEnvironment.getExecutionEnvironment // create a list of vertices with IDs = {1, 2, 3, 4, 5} val vertices: List[Vertex[Long, Long]] = ... // create a list of edges with IDs = {(1, 2) (1, 3), (2, 4), (5, 6)} val edges: List[Edge[Long, Long]] = ... val graph = Graph.fromCollection(vertices, edges, env) // will return false: 6 is an invalid ID graph.validate(new InvalidVertexIdsValidator[Long, Long, Long]) Back to top
`}),e.add({id:50,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/hints/",title:"Hints",section:"Queries",content:` SQL Hints # Batch Streaming
SQL hints can be used with SQL statements to alter execution plans. This chapter explains how to use hints to force various approaches.
Generally a hint can be used to:
Enforce planner: there\u0026rsquo;s no perfect planner, so it makes sense to implement hints to allow user better control the execution; Append meta data(or statistics): some statistics like “table index for scan” and “skew info of some shuffle keys” are somewhat dynamic for the query, it would be very convenient to config them with hints because our planning metadata from the planner is very often not that accurate; Operator resource constraints: for many cases, we would give a default resource configuration for the execution operators, i.e. min parallelism or managed memory (resource consuming UDF) or special resource requirement (GPU or SSD disk) and so on, it would be very flexible to profile the resource with hints per query(instead of the Job). Dynamic Table Options # Dynamic table options allows to specify or override table options dynamically, different with static table options defined with SQL DDL or connect API, these options can be specified flexibly in per-table scope within each query.
Thus it is very suitable to use for the ad-hoc queries in interactive terminal, for example, in the SQL-CLI, you can specify to ignore the parse error for a CSV source just by adding a dynamic option /*+ OPTIONS('csv.ignore-parse-errors'='true') */.
Syntax # In order to not break the SQL compatibility, we use the Oracle style SQL hint syntax:
table_path /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral Examples # CREATE TABLE kafka_table1 (id BIGINT, name STRING, age INT) WITH (...); CREATE TABLE kafka_table2 (id BIGINT, name STRING, age INT) WITH (...); -- override table options in query source select id, name from kafka_table1 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */; -- override table options in join select * from kafka_table1 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */ t1 join kafka_table2 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */ t2 on t1.id = t2.id; -- override table options for INSERT target table insert into kafka_table1 /*+ OPTIONS(\u0026#39;sink.partitioner\u0026#39;=\u0026#39;round-robin\u0026#39;) */ select * from kafka_table2; Back to top
`}),e.add({id:51,href:"/flink/flink-docs-master/docs/connectors/table/hive/hive_catalog/",title:"Hive Catalog",section:"Hive",content:` Hive Catalog # Hive Metastore has evolved into the de facto metadata hub over the years in Hadoop ecosystem. Many companies have a single Hive Metastore service instance in their production to manage all of their metadata, either Hive metadata or non-Hive metadata, as the source of truth.
For users who have both Hive and Flink deployments, HiveCatalog enables them to use Hive Metastore to manage Flink\u0026rsquo;s metadata.
For users who have just Flink deployment, HiveCatalog is the only persistent catalog provided out-of-box by Flink. Without a persistent catalog, users using Flink SQL CREATE DDL have to repeatedly create meta-objects like a Kafka table in each session, which wastes a lot of time. HiveCatalog fills this gap by empowering users to create tables and other meta-objects only once, and reference and manage them with convenience later on across sessions.
Set up HiveCatalog # Dependencies # Setting up a HiveCatalog in Flink requires the same dependencies as those of an overall Flink-Hive integration.
Configuration # Setting up a HiveCatalog in Flink requires the same configuration as those of an overall Flink-Hive integration.
How to use HiveCatalog # Once configured properly, HiveCatalog should just work out of box. Users can create Flink meta-objects with DDL, and should see them immediately afterwards.
HiveCatalog can be used to handle two kinds of tables: Hive-compatible tables and generic tables. Hive-compatible tables are those stored in a Hive-compatible way, in terms of both metadata and data in the storage layer. Therefore, Hive-compatible tables created via Flink can be queried from Hive side.
Generic tables, on the other hand, are specific to Flink. When creating generic tables with HiveCatalog, we\u0026rsquo;re just using HMS to persist the metadata. While these tables are visible to Hive, it\u0026rsquo;s unlikely Hive is able to understand the metadata. And therefore using such tables in Hive leads to undefined behavior.
It\u0026rsquo;s recommended to switch to Hive dialect to create Hive-compatible tables. If you want to create Hive-compatible tables with default dialect, make sure to set 'connector'='hive' in your table properties, otherwise a table is considered generic by default in HiveCatalog. Note that the connector property is not required if you use Hive dialect.
Example # We will walk through a simple example here.
step 1: set up a Hive Metastore # Have a Hive Metastore running.
Here, we set up a local Hive Metastore and our hive-site.xml file in local path /opt/hive-conf/hive-site.xml. We have some configs like the following:
\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;metadata is stored in a MySQL server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;MySQL JDBC driver class\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;user name for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;password for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://localhost:9083\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;IP address (or fully-qualified domain name) and port of the metastore host\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Test connection to the HMS with Hive Cli. Running some commands, we can see we have a database named default and there\u0026rsquo;s no table in it.
hive\u0026gt; show databases; OK default Time taken: 0.032 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.028 seconds, Fetched: 0 row(s) step 2: start SQL Client, and create a Hive catalog with Flink SQL DDL # Add all Hive dependencies to /lib dir in Flink distribution, and create a Hive catalog in Flink SQL CLI as following:
Flink SQL\u0026gt; CREATE CATALOG myhive WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39; ); step 3: set up a Kafka cluster # Bootstrap a local Kafka cluster with a topic named \u0026ldquo;test\u0026rdquo;, and produce some simple data to the topic as tuple of name and age.
localhost\$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test \u0026gt;tom,15 \u0026gt;john,21 These message can be seen by starting a Kafka console consumer.
localhost\$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 step 4: create a Kafka table with Flink SQL DDL # Create a simple Kafka table with Flink SQL DDL, and verify its schema.
Flink SQL\u0026gt; USE CATALOG myhive; Flink SQL\u0026gt; CREATE TABLE mykafka (name String, age Int) WITH ( \u0026#39;connector.type\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;connector.version\u0026#39; = \u0026#39;universal\u0026#39;, \u0026#39;connector.topic\u0026#39; = \u0026#39;test\u0026#39;, \u0026#39;connector.properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format.type\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;update-mode\u0026#39; = \u0026#39;append\u0026#39; ); [INFO] Table has been created. Flink SQL\u0026gt; DESCRIBE mykafka; root |-- name: STRING |-- age: INT Verify the table is also visible to Hive via Hive Cli:
hive\u0026gt; show tables; OK mykafka Time taken: 0.038 seconds, Fetched: 1 row(s) step 5: run Flink SQL to query the Kafka table # Run a simple select query from Flink SQL Client in a Flink cluster, either standalone or yarn-session.
Flink SQL\u0026gt; select * from mykafka; Produce some more messages in the Kafka topic
localhost\$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 kitty,30 amy,24 kaiky,18 You should see results produced by Flink in SQL Client now, as:
SQL Query Result (Table) Refresh: 1 s Page: Last of 1 name age tom 15 john 21 kitty 30 amy 24 kaiky 18 Supported Types # HiveCatalog supports all Flink types for generic tables.
For Hive-compatible tables, HiveCatalog needs to map Flink data types to corresponding Hive types as described in the following table:
Flink Data Type Hive Data Type CHAR(p) CHAR(p) VARCHAR(p) VARCHAR(p) STRING STRING BOOLEAN BOOLEAN TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT LONG FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) DATE DATE TIMESTAMP(9) TIMESTAMP BYTES BINARY ARRAY\u0026lt;T\u0026gt; LIST\u0026lt;T\u0026gt; MAP MAP ROW STRUCT Something to note about the type mapping:
Hive\u0026rsquo;s CHAR(p) has a maximum length of 255 Hive\u0026rsquo;s VARCHAR(p) has a maximum length of 65535 Hive\u0026rsquo;s MAP only supports primitive key types while Flink\u0026rsquo;s MAP can be any data type Hive\u0026rsquo;s UNION type is not supported Hive\u0026rsquo;s TIMESTAMP always has precision 9 and doesn\u0026rsquo;t support other precisions. Hive UDFs, on the other hand, can process TIMESTAMP values with a precision \u0026lt;= 9. Hive doesn\u0026rsquo;t support Flink\u0026rsquo;s TIMESTAMP_WITH_TIME_ZONE, TIMESTAMP_WITH_LOCAL_TIME_ZONE, and MULTISET Flink\u0026rsquo;s INTERVAL type cannot be mapped to Hive INTERVAL type yet `}),e.add({id:52,href:"/flink/flink-docs-master/docs/dev/python/installation/",title:"Installation",section:"Python API",content:` Installation # Environment Requirements # Python version (3.6, 3.7, 3.8 or 3.9) is required for PyFlink. Please run the following command to make sure that it meets the requirements: \` python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 Environment Setup # Your system may include multiple Python versions, and thus also include multiple Python binary executables. You can run the following ls command to find out what Python binary executables are available in your system:
\$ ls /usr/bin/python* To satisfy the PyFlink requirement regarding the Python environment version, you can choose to soft link python to point to your python3 interpreter:
ln -s /usr/bin/python3 python In addition to creating a soft link, you can also choose to create a Python virtual environment (venv). You can refer to the Preparing Python Virtual Environment documentation page for details on how to achieve that setup.
If you don’t want to use a soft link to change the system\u0026rsquo;s python interpreter point to, you can use the configuration way to specify the Python interpreter. For specifying the Python interpreter used to compile the jobs, you can refer to the configuration python.client.executable. For specifying the Python interpreter used to execute the Python UDF, you can refer to the configuration python.executable.
Installation of PyFlink # PyFlink is available in PyPi and can be installed as follows:
\$ python -m pip install apache-flink You can also build PyFlink from source by following the development guide.
Note Starting from Flink 1.11, it’s also supported to run PyFlink jobs locally on Windows and so you could develop and debug PyFlink jobs on Windows.
`}),e.add({id:53,href:"/flink/flink-docs-master/docs/learn-flink/",title:"Learn Flink",section:"Docs",content:" "}),e.add({id:54,href:"/flink/flink-docs-master/docs/ops/monitoring/checkpoint_monitoring/",title:"Monitoring Checkpointing",section:"Monitoring",content:` Monitoring Checkpointing # Overview # Flink\u0026rsquo;s web interface provides a tab to monitor the checkpoints of jobs. These stats are also available after the job has terminated. There are four different tabs to display information about your checkpoints: Overview, History, Summary, and Configuration. The following sections will cover all of these in turn.
Monitoring # Overview Tab # The overview tabs lists the following statistics. Note that these statistics don\u0026rsquo;t survive a JobManager loss and are reset to if your JobManager fails over.
Checkpoint Counts Triggered: The total number of checkpoints that have been triggered since the job started. In Progress: The current number of checkpoints that are in progress. Completed: The total number of successfully completed checkpoints since the job started. Failed: The total number of failed checkpoints since the job started. Restored: The number of restore operations since the job started. This also tells you how many times the job has restarted since submission. Note that the initial submission with a savepoint also counts as a restore and the count is reset if the JobManager was lost during operation. Latest Completed Checkpoint: The latest successfully completed checkpoints. Clicking on More details gives you detailed statistics down to the subtask level. Latest Failed Checkpoint: The latest failed checkpoint. Clicking on More details gives you detailed statistics down to the subtask level. Latest Savepoint: The latest triggered savepoint with its external path. Clicking on More details gives you detailed statistics down to the subtask level. Latest Restore: There are two types of restore operations. Restore from Checkpoint: We restored from a regular periodic checkpoint. Restore from Savepoint: We restored from a savepoint. History Tab # The checkpoint history keeps statistics about recently triggered checkpoints, including those that are currently in progress.
Note that for failed checkpoints, metrics are updated on a best efforts basis and may be not accurate.
ID: The ID of the triggered checkpoint. The IDs are incremented for each checkpoint, starting at 1. Status: The current status of the checkpoint, which is either In Progress, Completed, or Failed. If the triggered checkpoint is a savepoint, you will see a floppy-disk symbol. Acknowledged: The number of acknowledged subtask with total subtask. Trigger Time: The time when the checkpoint was triggered at the JobManager. Latest Acknowledgement: The time when the latest acknowledgement for any subtask was received at the JobManager (or n/a if no acknowledgement received yet). End to End Duration: The duration from the trigger timestamp until the latest acknowledgement (or n/a if no acknowledgement received yet). This end to end duration for a complete checkpoint is determined by the last subtask that acknowledges the checkpoint. This time is usually larger than single subtasks need to actually checkpoint the state. Checkpointed Data Size: The persisted data size during the sync and async phases of that checkpoint, the value could be different from full checkpoint data size if incremental checkpoint or changelog is enabled. Full Checkpoint Data Size: The accumulated checkpoint data size over all acknowledged subtasks. Processed (persisted) in-flight data: The approximate number of bytes processed/persisted during the alignment (time between receiving the first and the last checkpoint barrier) over all acknowledged subtasks. Persisted data could be larger than zero only if the unaligned checkpoints are enabled. For subtasks there are a couple of more detailed stats available.
Sync Duration: The duration of the synchronous part of the checkpoint. This includes snapshotting state of the operators and blocks all other activity on the subtask (processing records, firing timers, etc). Async Duration: The duration of the asynchronous part of the checkpoint. This includes time it took to write the checkpoint on to the selected filesystem. For unaligned checkpoints this also includes also the time the subtask had to wait for last of the checkpoint barriers to arrive (alignment duration) and the time it took to persist the in-flight data. Alignment Duration: The time between processing the first and the last checkpoint barrier. For aligned checkpoints, during the alignment, the channels that have already received checkpoint barrier are blocked from processing more data. Start Delay: The time it took for the first checkpoint barrier to reach this subtask since the checkpoint barrier has been created. Unaligned Checkpoint: Whether the checkpoint for the subtask is completed as an unaligned checkpoint. An aligned checkpoint can switch to an unaligned checkpoint if the alignment timeouts. History Size Configuration # You can configure the number of recent checkpoints that are remembered for the history via the following configuration key. The default is 10.
# Number of recent checkpoints that are remembered web.checkpoints.history: 15 Summary Tab # The summary computes a simple min/average/maximum statistics over all completed checkpoints for the End to End Duration, Incremental Checkpoint Data Size, Full Checkpoint Data Size, and Bytes Buffered During Alignment (see History for details about what these mean).
Note that these statistics don\u0026rsquo;t survive a JobManager loss and are reset to if your JobManager fails over.
Configuration Tab # The configuration list your streaming configuration:
Checkpointing Mode: Either Exactly Once or At least Once. Interval: The configured checkpointing interval. Trigger checkpoints in this interval. Timeout: Timeout after which a checkpoint is cancelled by the JobManager and a new checkpoint is triggered. Minimum Pause Between Checkpoints: Minimum required pause between checkpoints. After a checkpoint has completed successfully, we wait at least for this amount of time before triggering the next one, potentially delaying the regular interval. Maximum Concurrent Checkpoints: The maximum number of checkpoints that can be in progress concurrently. Persist Checkpoints Externally: Enabled or Disabled. If enabled, furthermore lists the cleanup config for externalized checkpoints (delete or retain on cancellation). Checkpoint Details # When you click on a More details link for a checkpoint, you get a Minimum/Average/Maximum summary over all its operators and also the detailed numbers per single subtask.
Summary per Operator # All Subtask Statistics # Back to top
`}),e.add({id:55,href:"/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/",title:"Overview",section:"Standalone",content:` Standalone # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate processes) of a Flink cluster. This can easily be expanded to set up a distributed standalone cluster, which we describe in the reference section.
Introduction # The standalone mode is the most barebone way of deploying Flink: The Flink services described in the deployment overview are just launched as processes on the operating system. Unlike deploying Flink with a resource provider such as Kubernetes or YARN, you have to take care of restarting failed processes, or allocation and de-allocation of resources during operation.
In the additional subpages of the standalone mode resource provider, we describe additional deployment methods which are based on the standalone mode: Deployment in Docker containers, and on Kubernetes.
Preparation # Flink runs on all UNIX-like environments, e.g. Linux, Mac OS X, and Cygwin (for Windows). Before you start to setup the system, make sure your system fulfils the following requirements.
Java 1.8.x or higher installed, Downloaded a recent Flink distribution from the download page and unpacked it. Starting a Standalone Cluster (Session Mode) # These steps show how to launch a Flink standalone cluster, and submit an example job:
# we assume to be in the root directory of the unzipped Flink distribution # (1) Start Cluster \$ ./bin/start-cluster.sh # (2) You can now access the Flink Web Interface on http://localhost:8081 # (3) Submit example job \$ ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar # (4) Stop the cluster again \$ ./bin/stop-cluster.sh In step (1), we\u0026rsquo;ve started 2 processes: A JVM for the JobManager, and a JVM for the TaskManager. The JobManager is serving the web interface accessible at localhost:8081. In step (3), we are starting a Flink Client (a short-lived JVM process) that submits an application to the JobManager.
Deployment Modes # Application Mode # For high-level intuition behind the application mode, please refer to the deployment mode overview. To start a Flink JobManager with an embedded application, we use the bin/standalone-job.sh script. We demonstrate this mode by locally starting the TopSpeedWindowing.jar example, running on a single TaskManager.
The application jar file needs to be available in the classpath. The easiest approach to achieve that is putting the jar into the lib/ folder:
\$ cp ./examples/streaming/TopSpeedWindowing.jar lib/ Then, we can launch the JobManager:
\$ ./bin/standalone-job.sh start --job-classname org.apache.flink.streaming.examples.windowing.TopSpeedWindowing The web interface is now available at localhost:8081. However, the application won\u0026rsquo;t be able to start, because there are no TaskManagers running yet:
\$ ./bin/taskmanager.sh start Note: You can start multiple TaskManagers, if your application needs more resources.
Stopping the services is also supported via the scripts. Call them multiple times if you want to stop multiple instances, or use stop-all:
\$ ./bin/taskmanager.sh stop \$ ./bin/standalone-job.sh stop Session Mode # For high-level intuition behind the session mode, please refer to the deployment mode overview. Local deployment in Session Mode has already been described in the introduction above.
Standalone Cluster Reference # Configuration # All available configuration options are listed on the configuration page, in particular the Basic Setup section contains good advise on configuring the ports, memory, parallelism etc.
The following scripts also allow configuration parameters to be set via dynamic properties:
jobmanager.sh standalone-job.sh taskmanager.sh historyserver.sh Example:
\$ ./bin/jobmanager.sh start -D jobmanager.rpc.address=localhost -D rest.port=8081 Options set via dynamic properties overwrite the options from flink-conf.yaml.
Debugging # If Flink is behaving unexpectedly, we recommend looking at Flink\u0026rsquo;s log files as a starting point for further investigations.
The log files are located in the logs/ directory. There\u0026rsquo;s a .log file for each Flink service running on this machine. In the default configuration, log files are rotated on each start of a Flink service \u0026ndash; older runs of a service will have a number suffixed to the log file.
Alternatively, logs are available from the Flink web frontend (both for the JobManager and each TaskManager).
By default, Flink is logging on the \u0026ldquo;INFO\u0026rdquo; log level, which provides basic information for all obvious issues. For cases where Flink seems to behave wrongly, reducing the log level to \u0026ldquo;DEBUG\u0026rdquo; is advised. The logging level is controlled via the conf/log4.properties file. Setting rootLogger.level = DEBUG will bootstrap Flink on the DEBUG log level.
There\u0026rsquo;s a dedicated page on the logging in Flink.
Component Management Scripts # Starting and Stopping a cluster # bin/start-cluster.sh and bin/stop-cluster.sh rely on conf/masters and conf/workers to determine the number of cluster component instances.
If password-less SSH access to the listed machines is configured, and they share the same directory structure, the scripts also support starting and stopping instances remotely.
Example 1: Start a cluster with 2 TaskManagers locally # conf/masters contents:
localhost conf/workers contents:
localhost localhost Example 2: Start a distributed cluster JobManagers # This assumes a cluster with 4 machines (master1, worker1, worker2, worker3), which all can reach each other over the network.
conf/masters contents:
master1 conf/workers contents:
worker1 worker2 worker3 Note that the configuration key jobmanager.rpc.address needs to be set to master1 for this to work.
We show a third example with a standby JobManager in the high-availability section.
Starting and Stopping Flink Components # The bin/jobmanager.sh and bin/taskmanager.sh scripts support starting the respective daemon in the background (using the start argument), or in the foreground (using start-foreground). In the foreground mode, the logs are printed to standard out. This mode is useful for deployment scenarios where another process is controlling the Flink daemon (e.g. Docker).
The scripts can be called multiple times, for example if multiple TaskManagers are needed. The instances are tracked by the scripts, and can be stopped one-by-one (using stop) or all together (using stop-all).
Windows Cygwin Users # If you are installing Flink from the git repository and you are using the Windows git shell, Cygwin can produce a failure similar to this one:
c:/flink/bin/start-cluster.sh: line 30: \$\u0026#39;\\r\u0026#39;: command not found This error occurs because git is automatically transforming UNIX line endings to Windows style line endings when running on Windows. The problem is that Cygwin can only deal with UNIX style line endings. The solution is to adjust the Cygwin settings to deal with the correct line endings by following these three steps:
Start a Cygwin shell.
Determine your home directory by entering
cd; pwd This will return a path under the Cygwin root path.
Using NotePad, WordPad or a different text editor open the file .bash_profile in the home directory and append the following (if the file does not exist you will have to create it):
\$ export SHELLOPTS \$ set -o igncr Save the file and open a new bash shell.
Setting up High-Availability # In order to enable HA for a standalone cluster, you have to use the ZooKeeper HA services.
Additionally, you have to configure your cluster to start multiple JobManagers.
In order to start an HA-cluster configure the masters file in conf/masters:
masters file: The masters file contains all hosts, on which JobManagers are started, and the ports to which the web user interface binds. master1:webUIPort1 [...] masterX:webUIPortX By default, the JobManager will pick a random port for inter process communication. You can change this via the high-availability.jobmanager.port key. This key accepts single ports (e.g. 50010), ranges (50000-50025), or a combination of both (50010,50011,50020-50025,50050-50075).
Example: Standalone HA Cluster with 2 JobManagers # Configure high availability mode and ZooKeeper quorum in conf/flink-conf.yaml: high-availability: zookeeper high-availability.zookeeper.quorum: localhost:2181 high-availability.zookeeper.path.root: /flink high-availability.cluster-id: /cluster_one # important: customize per cluster high-availability.storageDir: hdfs:///flink/recovery Configure masters in conf/masters: localhost:8081 localhost:8082 Configure ZooKeeper server in conf/zoo.cfg (currently it\u0026rsquo;s only possible to run a single ZooKeeper server per machine): server.0=localhost:2888:3888 Start ZooKeeper quorum: \$ ./bin/start-zookeeper-quorum.sh Starting zookeeper daemon on host localhost. Start an HA-cluster: \$ ./bin/start-cluster.sh Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum. Starting standalonesession daemon on host localhost. Starting standalonesession daemon on host localhost. Starting taskexecutor daemon on host localhost. Stop ZooKeeper quorum and cluster: \$ ./bin/stop-cluster.sh Stopping taskexecutor daemon (pid: 7647) on localhost. Stopping standalonesession daemon (pid: 7495) on host localhost. Stopping standalonesession daemon (pid: 7349) on host localhost. \$ ./bin/stop-zookeeper-quorum.sh Stopping zookeeper daemon (pid: 7101) on host localhost. User jars \u0026amp; Classpath # In Standalone mode, the following jars will be recognized as user-jars and included into user classpath:
Session Mode: The JAR file specified in startup command. Application Mode: The JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:56,href:"/flink/flink-docs-master/docs/deployment/resource-providers/",title:"Resource Providers",section:"Deployment",content:""}),e.add({id:57,href:"/flink/flink-docs-master/docs/deployment/security/security-ssl/",title:"SSL Setup",section:"Security",content:` SSL Setup # This page provides instructions on how to enable TLS/SSL authentication and encryption for network communication with and between Flink processes. NOTE: TLS/SSL authentication is not enabled by default.
Internal and External Connectivity # When securing network connections between machines processes through authentication and encryption, Apache Flink differentiates between internal and external connectivity. Internal Connectivity refers to all connections made between Flink processes. These connections run Flink custom protocols. Users never connect directly to internal connectivity endpoints. External / REST Connectivity endpoints refers to all connections made from the outside to Flink processes. This includes the web UI and REST commands to start and control running Flink jobs/applications, including the communication of the Flink CLI with the JobManager / Dispatcher.
For more flexibility, security for internal and external connectivity can be enabled and configured separately.
Internal Connectivity # Internal connectivity includes:
Control messages: RPC between JobManager / TaskManager / Dispatcher / ResourceManager The data plane: The connections between TaskManagers to exchange data during shuffles, broadcasts, redistribution, etc. The Blob Service (distribution of libraries and other artifacts). All internal connections are SSL authenticated and encrypted. The connections use mutual authentication, meaning both server and client side of each connection need to present the certificate to each other. The certificate acts effectively as a shared secret when a dedicated CA is used to exclusively sign an internal cert. The certificate for internal communication is not needed by any other party to interact with Flink, and can be simply added to the container images, or attached to the YARN deployment.
The easiest way to realize this setup is by generating a dedicated public/private key pair and self-signed certificate for the Flink deployment. The key- and truststore are identical and contains only that key pair / certificate. An example is shown below.
In an environment where operators are constrained to use firm-wide Internal CA (cannot generated self-signed certificates), the recommendation is to still have a dedicated key pair / certificate for the Flink deployment, signed by that CA. However, the TrustStore must then also contain the CA\u0026rsquo;s public certificate tho accept the deployment\u0026rsquo;s certificate during the SSL handshake (requirement in JDK TrustStore implementation).
NOTE: Because of that, it is critical that you specify the fingerprint of the deployment certificate (security.ssl.internal.cert.fingerprint), when it is not self-signed, to pin that certificate as the only trusted certificate and prevent the TrustStore from trusting all certificates signed by that CA.
Note: Because internal connections are mutually authenticated with shared certificates, Flink can skip hostname verification. This makes container-based setups easier.
External / REST Connectivity # All external connectivity is exposed via an HTTP/REST endpoint, used for example by the web UI and the CLI:
Communication with the Dispatcher to submit jobs (session clusters) Communication with the JobMaster to inspect and modify a running job/application The REST endpoints can be configured to require SSL connections. The server will, however, accept connections from any client by default, meaning the REST endpoint does not authenticate the client.
Simple mutual authentication may be enabled by configuration if authentication of connections to the REST endpoint is required, but we recommend to deploy a \u0026ldquo;side car proxy\u0026rdquo;: Bind the REST endpoint to the loopback interface (or the pod-local interface in Kubernetes) and start a REST proxy that authenticates and forwards the requests to Flink. Examples for proxies that Flink users have deployed are Envoy Proxy or NGINX with MOD_AUTH.
The rationale behind delegating authentication to a proxy is that such proxies offer a wide variety of authentication options and thus better integration into existing infrastructures.
Queryable State # Connections to the queryable state endpoints is currently not authenticated or encrypted.
Configuring SSL # SSL can be enabled separately for internal and external connectivity:
security.ssl.internal.enabled: Enable SSL for all internal connections. security.ssl.rest.enabled: Enable SSL for REST / external connections. Note: For backwards compatibility, the security.ssl.enabled option still exists and enables SSL for both internal and REST endpoints.
For internal connectivity, you can optionally disable security for different connection types separately. When security.ssl.internal.enabled is set to true, you can set the following parameters to false to disable SSL for that particular connection type:
taskmanager.data.ssl.enabled: Data communication between TaskManagers blob.service.ssl.enabled: Transport of BLOBs from JobManager to TaskManager akka.ssl.enabled: Akka-based RPC connections between JobManager / TaskManager / ResourceManager Keystores and Truststores # The SSL configuration requires to configure a keystore and a truststore. The keystore contains the public certificate (public key) and the private key, while the truststore contains the trusted certificates or the trusted authorities. Both stores need to be set up such that the truststore trusts the keystore\u0026rsquo;s certificate.
Internal Connectivity # Because internal communication is mutually authenticated between server and client side, keystore and truststore typically refer to a dedicated certificate that acts as a shared secret. In such a setup, the certificate can use wild card hostnames or addresses. WHen using self-signed certificates, it is even possible to use the same file as keystore and truststore.
security.ssl.internal.keystore: /path/to/file.keystore security.ssl.internal.keystore-password: keystore_password security.ssl.internal.key-password: key_password security.ssl.internal.truststore: /path/to/file.truststore security.ssl.internal.truststore-password: truststore_password When using a certificate that is not self-signed, but signed by a CA, you need to use certificate pinning to allow only a a specific certificate to be trusted when establishing the connectivity.
security.ssl.internal.cert.fingerprint: 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 REST Endpoints (external connectivity) # For REST endpoints, by default the keystore is used by the server endpoint, and the truststore is used by the REST clients (including the CLI client) to accept the server\u0026rsquo;s certificate. In the case where the REST keystore has a self-signed certificate, the truststore must trust that certificate directly. If the REST endpoint uses a certificate that is signed through a proper certification hierarchy, the roots of that hierarchy should be in the trust store.
If mutual authentication is enabled, the keystore and the truststore are used by both, the server endpoint and the REST clients as with internal connectivity.
security.ssl.rest.keystore: /path/to/file.keystore security.ssl.rest.keystore-password: keystore_password security.ssl.rest.key-password: key_password security.ssl.rest.truststore: /path/to/file.truststore security.ssl.rest.truststore-password: truststore_password security.ssl.rest.authentication-enabled: false Cipher suites # The IETF RFC 7525 recommends to use a specific set of cipher suites for strong security. Because these cipher suites were not available on many setups out of the box, Flink\u0026rsquo;s default value is set to a slightly weaker but more compatible cipher suite. We recommend that SSL setups update to the stronger cipher suites, if possible, by adding the below entry to the Flink configuration:
security.ssl.algorithms: TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 If these cipher suites are not supported on your setup, you will see that Flink processes will not be able to connect to each other.
Complete List of SSL Options # Key Default Type Description security.context.factory.classes "org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory";"org.apache.flink.runtime.security.contexts.NoOpSecurityContextFactory" List\u0026lt;String\u0026gt; List of factories that should be used to instantiate a security context. If multiple are configured, Flink will use the first compatible factory. You should have a NoOpSecurityContextFactory in this list as a fallback. security.kerberos.access.hadoopFileSystems (none) List\u0026lt;String\u0026gt; A comma-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens. security.kerberos.fetch.delegation-token true Boolean Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens. security.kerberos.krb5-conf.path (none) String Specify the local location of the krb5.conf file. If defined, this conf would be mounted on the JobManager and TaskManager containers/pods for Kubernetes and Yarn. Note: The KDC defined needs to be visible from inside the containers. security.kerberos.login.contexts (none) String A comma-separated list of login contexts to provide the Kerberos credentials to (for example, \`Client,KafkaClient\` to use the credentials for ZooKeeper authentication and for Kafka authentication) security.kerberos.login.keytab (none) String Absolute path to a Kerberos keytab file that contains the user credentials. security.kerberos.login.principal (none) String Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache true Boolean Indicates whether to read from your Kerberos ticket cache. security.kerberos.relogin.period 1 min Duration The time period when keytab login happens automatically in order to always have a valid TGT. security.kerberos.tokens.renewal.retry.backoff 1 h Duration The time period how long to wait before retrying to obtain new delegation tokens after a failure. security.kerberos.tokens.renewal.time-ratio 0.75 Double Ratio of the tokens's expiration time when new credentials should be re-obtained. security.module.factory.classes "org.apache.flink.runtime.security.modules.HadoopModuleFactory";"org.apache.flink.runtime.security.modules.JaasModuleFactory";"org.apache.flink.runtime.security.modules.ZookeeperModuleFactory" List\u0026lt;String\u0026gt; List of factories that should be used to instantiate security modules. All listed modules will be installed. Keep in mind that the configured security context might rely on some modules being present. security.ssl.algorithms "TLS_RSA_WITH_AES_128_CBC_SHA" String The comma separated list of standard SSL algorithms to be supported. Read more here security.ssl.internal.cert.fingerprint (none) String The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required security.ssl.internal.close-notify-flush-timeout -1 Integer The timeout (in ms) for flushing the \`close_notify\` that was triggered by closing a channel. If the \`close_notify\` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default) security.ssl.internal.enabled false Boolean Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc). security.ssl.internal.handshake-timeout -1 Integer The timeout (in ms) during SSL handshake. (-1 = use system default) security.ssl.internal.key-password (none) String The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.session-cache-size -1 Integer The size of the cache used for storing SSL session objects. According to here, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default). security.ssl.internal.session-timeout -1 Integer The timeout (in ms) for the cached SSL session objects. (-1 = use system default) security.ssl.internal.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore-password (none) String The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.protocol "TLSv1.2" String The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list. security.ssl.provider "JDK" String The SSL engine provider to use for the ssl transport:JDK: default Java-based SSL engineOPENSSL: openSSL-based SSL engine using system librariesOPENSSL is based on netty-tcnative and comes in two flavours:dynamically linked: This will use your system's openSSL libraries (if compatible) and requires opt/flink-shaded-netty-tcnative-dynamic-*.jar to be copied to lib/statically linked: Due to potential licensing issues with openSSL (see LEGAL-393), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into lib/:
git clone https://github.com/apache/flink-shaded.git \u0026amp;\u0026amp; cd flink-shaded \u0026amp;\u0026amp; mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static security.ssl.rest.authentication-enabled false Boolean Turns on mutual SSL authentication for external communication via the REST endpoints. security.ssl.rest.cert.fingerprint (none) String The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA security.ssl.rest.enabled false Boolean Turns on SSL for external communication via the REST endpoints. security.ssl.rest.key-password (none) String The secret to decrypt the key in the keystore for Flink's external REST endpoints. security.ssl.rest.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints. security.ssl.rest.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints. security.ssl.rest.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints. security.ssl.rest.truststore-password (none) String The password to decrypt the truststore for Flink's external REST endpoints. security.ssl.verify-hostname true Boolean Flag to enable peer’s hostname verification during ssl handshake. zookeeper.sasl.disable false Boolean zookeeper.sasl.login-context-name "Client" String zookeeper.sasl.service-name "zookeeper" String Creating and Deploying Keystores and Truststores # Keys, Certificates, and the Keystores and Truststores can be generatedd using the keytool utility. You need to have an appropriate Java Keystore and Truststore accessible from each node in the Flink cluster.
For standalone setups, this means copying the files to each node, or adding them to a shared mounted directory. For container based setups, add the keystore and truststore files to the container images. For Yarn setups, the cluster deployment phase can automatically distribute the keystore and truststore files. For the externally facing REST endpoint, the common name or subject alternative names in the certificate should match the node\u0026rsquo;s hostname and IP address.
Example SSL Setup Standalone and Kubernetes # Internal Connectivity
Execute the following keytool commands to create a key pair in a keystore:
\$ keytool -genkeypair \\ -alias flink.internal \\ -keystore internal.keystore \\ -dname \u0026#34;CN=flink.internal\u0026#34; \\ -storepass internal_store_password \\ -keyalg RSA \\ -keysize 4096 \\ -storetype PKCS12 The single key/certificate in the keystore is used the same way by the server and client endpoints (mutual authentication). The key pair acts as the shared secret for internal security, and we can directly use it as keystore and truststore.
security.ssl.internal.enabled: true security.ssl.internal.keystore: /path/to/flink/conf/internal.keystore security.ssl.internal.truststore: /path/to/flink/conf/internal.keystore security.ssl.internal.keystore-password: internal_store_password security.ssl.internal.truststore-password: internal_store_password security.ssl.internal.key-password: internal_store_password REST Endpoint
The REST endpoint may receive connections from external processes, including tools that are not part of Flink (for example curl request to the REST API). Setting up a proper certificate that is signed though a CA hierarchy may make sense for the REST endpoint.
However, as mentioned above, the REST endpoint does not authenticate clients and thus typically needs to be secured via a proxy anyways.
REST Endpoint (simple self signed certificate)
This example shows how to create a simple keystore / truststore pair. The truststore does not contain the primary key and can be shared with other applications. In this example, myhost.company.org / ip:10.0.2.15 is the node (or service) for the JobManager.
\$ keytool -genkeypair -alias flink.rest -keystore rest.keystore -dname \u0026#34;CN=myhost.company.org\u0026#34; -ext \u0026#34;SAN=dns:myhost.company.org,ip:10.0.2.15\u0026#34; -storepass rest_keystore_password -keyalg RSA -keysize 4096 -storetype PKCS12 \$ keytool -exportcert -keystore rest.keystore -alias flink.rest -storepass rest_keystore_password -file flink.cer \$ keytool -importcert -keystore rest.truststore -alias flink.rest -storepass rest_truststore_password -file flink.cer -noprompt security.ssl.rest.enabled: true security.ssl.rest.keystore: /path/to/flink/conf/rest.keystore security.ssl.rest.truststore: /path/to/flink/conf/rest.truststore security.ssl.rest.keystore-password: rest_keystore_password security.ssl.rest.truststore-password: rest_truststore_password security.ssl.rest.key-password: rest_keystore_password REST Endpoint (with a self signed CA)
Execute the following keytool commands to create a truststore with a self signed CA.
\$ keytool -genkeypair -alias ca -keystore ca.keystore -dname \u0026#34;CN=Sample CA\u0026#34; -storepass ca_keystore_password -keyalg RSA -keysize 4096 -ext \u0026#34;bc=ca:true\u0026#34; -storetype PKCS12 \$ keytool -exportcert -keystore ca.keystore -alias ca -storepass ca_keystore_password -file ca.cer \$ keytool -importcert -keystore ca.truststore -alias ca -storepass ca_truststore_password -file ca.cer -noprompt Now create a keystore for the REST endpoint with a certificate signed by the above CA. Let flink.company.org / ip:10.0.2.15 be the hostname of the JobManager.
\$ keytool -genkeypair -alias flink.rest -keystore rest.signed.keystore -dname \u0026#34;CN=flink.company.org\u0026#34; -ext \u0026#34;SAN=dns:flink.company.org\u0026#34; -storepass rest_keystore_password -keyalg RSA -keysize 4096 -storetype PKCS12 \$ keytool -certreq -alias flink.rest -keystore rest.signed.keystore -storepass rest_keystore_password -file rest.csr \$ keytool -gencert -alias ca -keystore ca.keystore -storepass ca_keystore_password -ext \u0026#34;SAN=dns:flink.company.org,ip:10.0.2.15\u0026#34; -infile rest.csr -outfile rest.cer \$ keytool -importcert -keystore rest.signed.keystore -storepass rest_keystore_password -file ca.cer -alias ca -noprompt \$ keytool -importcert -keystore rest.signed.keystore -storepass rest_keystore_password -file rest.cer -alias flink.rest -noprompt Now add the following configuration to your flink-conf.yaml:
security.ssl.rest.enabled: true security.ssl.rest.keystore: /path/to/flink/conf/rest.signed.keystore security.ssl.rest.truststore: /path/to/flink/conf/ca.truststore security.ssl.rest.keystore-password: rest_keystore_password security.ssl.rest.key-password: rest_keystore_password security.ssl.rest.truststore-password: ca_truststore_password Tips to query REST Endpoint with curl utility
You can convert the keystore into the PEM format using openssl:
\$ openssl pkcs12 -passin pass:rest_keystore_password -in rest.keystore -out rest.pem -nodes Then you can query REST Endpoint with curl:
\$ curl --cacert rest.pem flink_url If mutual SSL is enabled:
\$ curl --cacert rest.pem --cert rest.pem flink_url Tips for YARN Deployment # For YARN, you can use the tools of Yarn to help:
Configuring security for internal communication is exactly the same as in the example above.
To secure the REST endpoint, you need to issue the REST endpoint\u0026rsquo;s certificate such that it is valid for all hosts that the JobManager may get deployed to. This can be done with a wild card DNS name, or by adding multiple DNS names.
The easiest way to deploy keystores and truststore is by YARN client\u0026rsquo;s ship files option (-yt). Copy the keystore and truststore files into a local directory (say deploy-keys/) and start the YARN session as follows: flink run -m yarn-cluster -yt deploy-keys/ flinkapp.jar
When deployed using YARN, Flink\u0026rsquo;s web dashboard is accessible through YARN proxy\u0026rsquo;s Tracking URL. To ensure that the YARN proxy is able to access Flink\u0026rsquo;s HTTPS URL, you need to configure YARN proxy to accept Flink\u0026rsquo;s SSL certificates. For that, add the custom CA certificate into Java\u0026rsquo;s default truststore on the YARN Proxy node.
Back to top
`}),e.add({id:58,href:"/flink/flink-docs-master/docs/deployment/resource-providers/standalone/",title:"Standalone",section:"Resource Providers",content:" "}),e.add({id:59,href:"/flink/flink-docs-master/docs/libs/state_processor_api/",title:"State Processor API",section:"Libraries",content:` State Processor API # Apache Flink\u0026rsquo;s State Processor API provides powerful functionality to reading, writing, and modifying savepoints and checkpoints using Flink’s DataStream API under BATCH execution. Due to the interoperability of DataStream and Table API, you can even use relational Table API or SQL queries to analyze and process state data.
For example, you can take a savepoint of a running stream processing application and analyze it with a DataStream batch program to verify that the application behaves correctly. Or you can read a batch of data from any store, preprocess it, and write the result to a savepoint that you use to bootstrap the state of a streaming application. It is also possible to fix inconsistent state entries. Finally, the State Processor API opens up many ways to evolve a stateful application that was previously blocked by parameter and design choices that could not be changed without losing all the state of the application after it was started. For example, you can now arbitrarily modify the data types of states, adjust the maximum parallelism of operators, split or merge operator state, re-assign operator UIDs, and so on.
To get started with the state processor api, include the following library in your application.
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-state-processor-api\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Mapping Application State to DataSets # The State Processor API maps the state of a streaming application to one or more data sets that can be processed separately. In order to be able to use the API, you need to understand how this mapping works.
But let us first have a look at what a stateful Flink job looks like. A Flink job is composed of operators; typically one or more source operators, a few operators for the actual processing, and one or more sink operators. Each operator runs in parallel in one or more tasks and can work with different types of state. An operator can have zero, one, or more “operator states” which are organized as lists that are scoped to the operator\u0026rsquo;s tasks. If the operator is applied on a keyed stream, it can also have zero, one, or more “keyed states” which are scoped to a key that is extracted from each processed record. You can think of keyed state as a distributed key-value map.
The following figure shows the application “MyApp” which consists of three operators called “Src”, “Proc”, and “Snk”. Src has one operator state (os1), Proc has one operator state (os2) and two keyed states (ks1, ks2) and Snk is stateless.
A savepoint or checkpoint of MyApp consists of the data of all states, organized in a way that the states of each task can be restored. When processing the data of a savepoint (or checkpoint) with a batch job, we need a mental model that maps the data of the individual tasks\u0026rsquo; states into data sets or tables. In fact, we can think of a savepoint as a database. Every operator (identified by its UID) represents a namespace. Each operator state of an operator is mapped to a dedicated table in the namespace with a single column that holds the state\u0026rsquo;s data of all tasks. All keyed states of an operator are mapped to a single table consisting of a column for the key, and one column for each keyed state. The following figure shows how a savepoint of MyApp is mapped to a database.
The figure shows how the values of Src\u0026rsquo;s operator state are mapped to a table with one column and five rows, one row for each of the list entries across all parallel tasks of Src. Operator state os2 of the operator “Proc” is similarly mapped to an individual table. The keyed states ks1 and ks2 are combined to a single table with three columns, one for the key, one for ks1 and one for ks2. The keyed table holds one row for each distinct key of both keyed states. Since the operator “Snk” does not have any state, its namespace is empty.
Reading State # Reading state begins by specifying the path to a valid savepoint or checkpoint along with the StateBackend that should be used to restore the data. The compatibility guarantees for restoring state are identical to those when restoring a DataStream application.
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SavepointReader savepoint = SavepointReader.read(env, \u0026#34;hdfs://path/\u0026#34;, new HashMapStateBackend()); Operator State # Operator state is any non-keyed state in Flink. This includes, but is not limited to, any use of CheckpointedFunction or BroadcastState within an application. When reading operator state, users specify the operator uid, the state name, and the type information.
Operator List State # Operator state stored in a CheckpointedFunction using getListState can be read using ExistingSavepoint#readListState. The state name and type information should match those used to define the ListStateDescriptor that declared this state in the DataStream application.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readListState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;list-state\u0026#34;, Types.INT); Operator Union List State # Operator state stored in a CheckpointedFunction using getUnionListState can be read using ExistingSavepoint#readUnionState. The state name and type information should match those used to define the ListStateDescriptor that declared this state in the DataStream application. The framework will return a single copy of the state, equivalent to restoring a DataStream with parallelism 1.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readUnionState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;union-state\u0026#34;, Types.INT); Broadcast State # BroadcastState can be read using ExistingSavepoint#readBroadcastState. The state name and type information should match those used to define the MapStateDescriptor that declared this state in the DataStream application. The framework will return a single copy of the state, equivalent to restoring a DataStream with parallelism 1.
DataStream\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; broadcastState = savepoint.readBroadcastState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;broadcast-state\u0026#34;, Types.INT, Types.INT); Using Custom Serializers # Each of the operator state readers support using custom TypeSerializers if one was used to define the StateDescriptor that wrote out the state.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readListState\u0026lt;\u0026gt;( \u0026#34;uid\u0026#34;, \u0026#34;list-state\u0026#34;, Types.INT, new MyCustomIntSerializer()); Keyed State # Keyed state, or partitioned state, is any state that is partitioned relative to a key. When reading a keyed state, users specify the operator id and a KeyedStateReaderFunction\u0026lt;KeyType, OutputType\u0026gt;.
The KeyedStateReaderFunction allows users to read arbitrary columns and complex state types such as ListState, MapState, and AggregatingState. This means if an operator contains a stateful process function such as:
public class StatefulFunctionWithTime extends KeyedProcessFunction\u0026lt;Integer, Integer, Void\u0026gt; { ValueState\u0026lt;Integer\u0026gt; state; ListState\u0026lt;Long\u0026gt; updateTimes; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Integer\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT); state = getRuntimeContext().getState(stateDescriptor); ListStateDescriptor\u0026lt;Long\u0026gt; updateDescriptor = new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;times\u0026#34;, Types.LONG); updateTimes = getRuntimeContext().getListState(updateDescriptor); } @Override public void processElement(Integer value, Context ctx, Collector\u0026lt;Void\u0026gt; out) throws Exception { state.update(value + 1); updateTimes.add(System.currentTimeMillis()); } } Then it can read by defining an output type and corresponding KeyedStateReaderFunction.
DataStream\u0026lt;KeyedState\u0026gt; keyedState = savepoint.readKeyedState(\u0026#34;my-uid\u0026#34;, new ReaderFunction()); public class KeyedState { public int key; public int value; public List\u0026lt;Long\u0026gt; times; } public class ReaderFunction extends KeyedStateReaderFunction\u0026lt;Integer, KeyedState\u0026gt; { ValueState\u0026lt;Integer\u0026gt; state; ListState\u0026lt;Long\u0026gt; updateTimes; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Integer\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT); state = getRuntimeContext().getState(stateDescriptor); ListStateDescriptor\u0026lt;Long\u0026gt; updateDescriptor = new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;times\u0026#34;, Types.LONG); updateTimes = getRuntimeContext().getListState(updateDescriptor); } @Override public void readKey( Integer key, Context ctx, Collector\u0026lt;KeyedState\u0026gt; out) throws Exception { KeyedState data = new KeyedState(); data.key = key; data.value = state.value(); data.times = StreamSupport .stream(updateTimes.get().spliterator(), false) .collect(Collectors.toList()); out.collect(data); } } Along with reading registered state values, each key has access to a Context with metadata such as registered event time and processing time timers.
Note: When using a KeyedStateReaderFunction, all state descriptors must be registered eagerly inside of open. Any attempt to call a RuntimeContext#get*State will result in a RuntimeException.
Window State # The state processor api supports reading state from a window operator. When reading a window state, users specify the operator id, window assigner, and aggregation type.
Additionally, a WindowReaderFunction can be specified to enrich each read with additional information similar to a WindowFunction or ProcessWindowFunction.
Suppose a DataStream application that counts the number of clicks per user per minute.
class Click { public String userId; public LocalDateTime time; } class ClickCounter implements AggregateFunction\u0026lt;Click, Integer, Integer\u0026gt; { @Override public Integer createAccumulator() { return 0; } @Override public Integer add(Click value, Integer accumulator) { return 1 + accumulator; } @Override public Integer getResult(Integer accumulator) { return accumulator; } @Override public Integer merge(Integer a, Integer b) { return a + b; } } DataStream\u0026lt;Click\u0026gt; clicks = ...; clicks .keyBy(click -\u0026gt; click.userId) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .aggregate(new ClickCounter()) .uid(\u0026#34;click-window\u0026#34;) .addSink(new Sink()); This state can be read using the code below.
class ClickState { public String userId; public int count; public TimeWindow window; public Set\u0026lt;Long\u0026gt; triggerTimers; } class ClickReader extends WindowReaderFunction\u0026lt;Integer, ClickState, String, TimeWindow\u0026gt; { @Override public void readWindow( String key, Context\u0026lt;TimeWindow\u0026gt; context, Iterable\u0026lt;Integer\u0026gt; elements, Collector\u0026lt;ClickState\u0026gt; out) { ClickState state = new ClickState(); state.userId = key; state.count = elements.iterator().next(); state.window = context.window(); state.triggerTimers = context.registeredEventTimeTimers(); out.collect(state); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SavepointReader savepoint = SavepointReader.read(env, \u0026#34;hdfs://checkpoint-dir\u0026#34;, new HashMapStateBackend()); savepoint .window(TumblingEventTimeWindows.of(Time.minutes(1))) .aggregate(\u0026#34;click-window\u0026#34;, new ClickCounter(), new ClickReader(), Types.String, Types.INT, Types.INT) .print(); Additionally, trigger state - from CountTriggers or custom triggers - can be read using the method Context#triggerState inside the WindowReaderFunction.
Writing New Savepoints # Savepoint\u0026rsquo;s may also be written, which allows such use cases as bootstrapping state based on historical data. Each savepoint is made up of one or more StateBootstrapTransformation\u0026rsquo;s (explained below), each of which defines the state for an individual operator.
When using the SavepointWriter, your application must be executed under BATCH execution. Note The state processor api does not currently provide a Scala API. As a result it will always auto-derive serializers using the Java type stack. To bootstrap a savepoint for the Scala DataStream API please manually pass in all type information. int maxParallelism = 128; SavepointWriter .newSavepoint(new HashMapStateBackend(), maxParallelism) .withOperator(\u0026#34;uid1\u0026#34;, transformation1) .withOperator(\u0026#34;uid2\u0026#34;, transformation2) .write(savepointPath); The UIDs associated with each operator must match one to one with the UIDs assigned to the operators in your DataStream application; these are how Flink knows what state maps to which operator.
Operator State # Simple operator state, using CheckpointedFunction, can be created using the StateBootstrapFunction.
public class SimpleBootstrapFunction extends StateBootstrapFunction\u0026lt;Integer\u0026gt; { private ListState\u0026lt;Integer\u0026gt; state; @Override public void processElement(Integer value, Context ctx) throws Exception { state.add(value); } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { } @Override public void initializeState(FunctionInitializationContext context) throws Exception { state = context.getOperatorState().getListState(new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT)); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Integer\u0026gt; data = env.fromElements(1, 2, 3); StateBootstrapTransformation transformation = OperatorTransformation .bootstrapWith(data) .transform(new SimpleBootstrapFunction()); Broadcast State # BroadcastState can be written using a BroadcastStateBootstrapFunction. Similar to broadcast state in the DataStream API, the full state must fit in memory.
public class CurrencyRate { public String currency; public Double rate; } public class CurrencyBootstrapFunction extends BroadcastStateBootstrapFunction\u0026lt;CurrencyRate\u0026gt; { public static final MapStateDescriptor\u0026lt;String, Double\u0026gt; descriptor = new MapStateDescriptor\u0026lt;\u0026gt;(\u0026#34;currency-rates\u0026#34;, Types.STRING, Types.DOUBLE); @Override public void processElement(CurrencyRate value, Context ctx) throws Exception { ctx.getBroadcastState(descriptor).put(value.currency, value.rate); } } DataStream\u0026lt;CurrencyRate\u0026gt; currencyDataSet = env.fromCollection( new CurrencyRate(\u0026#34;USD\u0026#34;, 1.0), new CurrencyRate(\u0026#34;EUR\u0026#34;, 1.3)); StateBootstrapTransformation\u0026lt;CurrencyRate\u0026gt; broadcastTransformation = OperatorTransformation .bootstrapWith(currencyDataSet) .transform(new CurrencyBootstrapFunction()); Keyed State # Keyed state for ProcessFunction\u0026rsquo;s and other RichFunction types can be written using a KeyedStateBootstrapFunction.
public class Account { public int id; public double amount;	public long timestamp; } public class AccountBootstrapper extends KeyedStateBootstrapFunction\u0026lt;Integer, Account\u0026gt; { ValueState\u0026lt;Double\u0026gt; state; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Double\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;total\u0026#34;,Types.DOUBLE); state = getRuntimeContext().getState(descriptor); } @Override public void processElement(Account value, Context ctx) throws Exception { state.update(value.amount); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Account\u0026gt; accountDataSet = env.fromCollection(accounts); StateBootstrapTransformation\u0026lt;Account\u0026gt; transformation = OperatorTransformation .bootstrapWith(accountDataSet) .keyBy(acc -\u0026gt; acc.id) .transform(new AccountBootstrapper()); The KeyedStateBootstrapFunction supports setting event time and processing time timers. The timers will not fire inside the bootstrap function and only become active once restored within a DataStream application. If a processing time timer is set but the state is not restored until after that time has passed, the timer will fire immediately upon start.
Attention If your bootstrap function creates timers, the state can only be restored using one of the process type functions.
Window State # The state processor api supports writing state for the window operator. When writing window state, users specify the operator id, window assigner, evictor, optional trigger, and aggregation type. It is important the configurations on the bootstrap transformation match the configurations on the DataStream window.
public class Account { public int id; public double amount;	public long timestamp; } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Account\u0026gt; accountDataSet = env.fromCollection(accounts); StateBootstrapTransformation\u0026lt;Account\u0026gt; transformation = OperatorTransformation .bootstrapWith(accountDataSet) .keyBy(acc -\u0026gt; acc.id) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .reduce((left, right) -\u0026gt; left + right); Modifying Savepoints # Besides creating a savepoint from scratch, you can base one off an existing savepoint such as when bootstrapping a single new operator for an existing job.
SavepointWriter .fromExistingSavepoint(oldPath, new HashMapStateBackend()) .withOperator(\u0026#34;uid\u0026#34;, transformation) .write(newPath); `}),e.add({id:60,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/",title:"State Schema Evolution",section:"Data Types \u0026 Serialization",content:` State Schema Evolution # Apache Flink streaming applications are typically designed to run indefinitely or for long periods of time. As with all long-running services, the applications need to be updated to adapt to changing requirements. This goes the same for data schemas that the applications work against; they evolve along with the application.
This page provides an overview of how you can evolve your state type\u0026rsquo;s data schema. The current restrictions varies across different types and state structures (ValueState, ListState, etc.).
Note that the information on this page is relevant only if you are using state serializers that are generated by Flink\u0026rsquo;s own type serialization framework. That is, when declaring your state, the provided state descriptor is not configured to use a specific TypeSerializer or TypeInformation, in which case Flink infers information about the state type:
ListStateDescriptor\u0026lt;MyPojoType\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state-name\u0026#34;, MyPojoType.class); checkpointedState = getRuntimeContext().getListState(descriptor); Under the hood, whether or not the schema of state can be evolved depends on the serializer used to read / write persisted state bytes. Simply put, a registered state\u0026rsquo;s schema can only be evolved if its serializer properly supports it. This is handled transparently by serializers generated by Flink\u0026rsquo;s type serialization framework (current scope of support is listed below.
If you intend to implement a custom TypeSerializer for your state type and would like to learn how to implement the serializer to support state schema evolution, please refer to Custom State Serialization. The documentation there also covers necessary internal details about the interplay between state serializers and Flink\u0026rsquo;s state backends to support state schema evolution.
Evolving state schema # To evolve the schema of a given state type, you would take the following steps:
Take a savepoint of your Flink streaming job. Update state types in your application (e.g., modifying your Avro type schema). Restore the job from the savepoint. When accessing state for the first time, Flink will assess whether or not the schema had been changed for the state, and migrate state schema if necessary. The process of migrating state to adapt to changed schemas happens automatically, and independently for each state. This process is performed internally by Flink by first checking if the new serializer for the state has different serialization schema than the previous serializer; if so, the previous serializer is used to read the state to objects, and written back to bytes again with the new serializer.
Further details about the migration process is out of the scope of this documentation; please refer to here.
Supported data types for schema evolution # Currently, schema evolution is supported only for POJO and Avro types. Therefore, if you care about schema evolution for state, it is currently recommended to always use either Pojo or Avro for state data types.
There are plans to extend the support for more composite types; for more details, please refer to FLINK-10896.
POJO types # Flink supports evolving schema of POJO types, based on the following set of rules:
Fields can be removed. Once removed, the previous value for the removed field will be dropped in future checkpoints and savepoints. New fields can be added. The new field will be initialized to the default value for its type, as defined by Java. Declared fields types cannot change. Class name of the POJO type cannot change, including the namespace of the class. Note that the schema of POJO type state can only be evolved when restoring from a previous savepoint with Flink versions newer than 1.8.0. When restoring with Flink versions older than 1.8.0, the schema cannot be changed.
Avro types # Flink fully supports evolving schema of Avro type state, as long as the schema change is considered compatible by Avro\u0026rsquo;s rules for schema resolution.
One limitation is that Avro generated classes used as the state type cannot be relocated or have different namespaces when the job is restored.
Schema Migration Limitations # Flink\u0026rsquo;s schema migration has some limitations that are required to ensure correctness. For users that need to work around these limitations, and understand them to be safe in their specific use-case, consider using a custom serializer or the state processor api.
Schema evolution of keys is not supported. # The structure of a key cannot be migrated as this may lead to non-deterministic behavior. For example, if a POJO is used as a key and one field is dropped then there may suddenly be multiple separate keys that are now identical. Flink has no way to merge the corresponding values.
Additionally, the RocksDB state backend relies on binary object identity, rather than the hashCode method. Any change to the keys\u0026rsquo; object structure can lead to non-deterministic behavior.
Kryo cannot be used for schema evolution. # When Kryo is used, there is no possibility for the framework to verify if any incompatible changes have been made.
This means that if a data-structure containing a given type is serialized via Kryo, then that contained type can not undergo schema evolution.
For example, if a POJO contains a List\u0026lt;SomeOtherPojo\u0026gt;, then the List and its contents are serialized via Kryo and schema evolution is not supported for SomeOtherPojo.
Back to top
`}),e.add({id:61,href:"/flink/flink-docs-master/docs/connectors/table/",title:"Table API Connectors",section:"Connectors",content:""}),e.add({id:62,href:"/flink/flink-docs-master/docs/dev/dataset/transformations/",title:"Transformations",section:"DataSet API (Legacy)",content:` DataSet Transformations # This document gives a deep-dive into the available transformations on DataSets. For a general introduction to the Flink Java API, please refer to the Programming Guide.
For zipping elements in a data set with a dense index, please refer to the Zip Elements Guide.
Map # The Map transformation applies a user-defined map function on each element of a DataSet. It implements a one-to-one mapping, that is, exactly one element must be returned by the function.
The following code transforms a DataSet of Integer pairs into a DataSet of Integers:
Java // MapFunction that adds two integer values public class IntAdder implements MapFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Integer\u0026gt; { @Override public Integer map(Tuple2\u0026lt;Integer, Integer\u0026gt; in) { return in.f0 + in.f1; } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; intPairs = // [...] DataSet\u0026lt;Integer\u0026gt; intSums = intPairs.map(new IntAdder()); Scala val intPairs: DataSet[(Int, Int)] = // [...] val intSums = intPairs.map { pair =\u0026gt; pair._1 + pair._2 } FlatMap # The FlatMap transformation applies a user-defined flat-map function on each element of a DataSet. This variant of a map function can return arbitrary many result elements (including none) for each input element.
The following code transforms a DataSet of text lines into a DataSet of words:
Java // FlatMapFunction that tokenizes a String by whitespace characters and emits all String tokens. public class Tokenizer implements FlatMapFunction\u0026lt;String, String\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) { for (String token : value.split(\u0026#34;\\\\W\u0026#34;)) { out.collect(token); } } } // [...] DataSet\u0026lt;String\u0026gt; textLines = // [...] DataSet\u0026lt;String\u0026gt; words = textLines.flatMap(new Tokenizer()); Scala val textLines: DataSet[String] = // [...] val words = textLines.flatMap { _.split(\u0026#34; \u0026#34;) } MapPartition # MapPartition transforms a parallel partition in a single function call. The map-partition function gets the partition as Iterable and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.
The following code transforms a DataSet of text lines into a DataSet of counts per partition:
Java public class PartitionCounter implements MapPartitionFunction\u0026lt;String, Long\u0026gt; { public void mapPartition(Iterable\u0026lt;String\u0026gt; values, Collector\u0026lt;Long\u0026gt; out) { long c = 0; for (String s : values) { c++; } out.collect(c); } } // [...] DataSet\u0026lt;String\u0026gt; textLines = // [...] DataSet\u0026lt;Long\u0026gt; counts = textLines.mapPartition(new PartitionCounter()); Scala val textLines: DataSet[String] = // [...] // Some is required because the return value must be a Collection. // There is an implicit conversion from Option to a Collection. val counts = textLines.mapPartition { in =\u0026gt; Some(in.size) } Filter # The Filter transformation applies a user-defined filter function on each element of a DataSet and retains only those elements for which the function returns true.
The following code removes all Integers smaller than zero from a DataSet:
Java // FilterFunction that filters out all Integers smaller than zero. public class NaturalNumberFilter implements FilterFunction\u0026lt;Integer\u0026gt; { @Override public boolean filter(Integer number) { return number \u0026gt;= 0; } } // [...] DataSet\u0026lt;Integer\u0026gt; intNumbers = // [...] DataSet\u0026lt;Integer\u0026gt; naturalNumbers = intNumbers.filter(new NaturalNumberFilter()); Scala val intNumbers: DataSet[Int] = // [...] val naturalNumbers = intNumbers.filter { _ \u0026gt; 0 } IMPORTANT: The system assumes that the function does not modify the elements on which the predicate is applied. Violating this assumption can lead to incorrect results.
Projection of Tuple DataSet # The Project transformation removes or moves Tuple fields of a Tuple DataSet. The project(int...) method selects Tuple fields that should be retained by their index and defines their order in the output Tuple.
Projections do not require the definition of a user function.
The following code shows different ways to apply a Project transformation on a DataSet:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] // converts Tuple3\u0026lt;Integer, Double, String\u0026gt; into Tuple2\u0026lt;String, Integer\u0026gt; DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); Projection with Type Hint # Note that the Java compiler cannot infer the return type of project operator. This can cause a problem if you call another operator on a result of project operator such as:
DataSet\u0026lt;Tuple5\u0026lt;String,String,String,String,String\u0026gt;\u0026gt; ds = ....; DataSet\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt; ds2 = ds.project(0).distinct(0); This problem can be overcome by hinting the return type of project operator like this:
DataSet\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt; ds2 = ds.\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt;project(0).distinct(0); Scala Not supported. Transformations on Grouped DataSet # The reduce operations can operate on grouped data sets. Specifying the key to be used for grouping can be done in many ways:
key expressions a key-selector function one or more field position keys (Tuple DataSet only) Case Class fields (Case Classes only) Please look at the reduce examples to see how the grouping keys are specified.
Reduce on Grouped DataSet # A Reduce transformation that is applied on a grouped DataSet reduces each group to a single element using a user-defined reduce function. For each group of input elements, a reduce function successively combines pairs of elements into one element until only a single element for each group remains.
Note that for a ReduceFunction the keyed fields of the returned object should match the input values. This is because reduce is implicitly combinable and objects emitted from the combine operator are again grouped by key when passed to the reduce operator.
Reduce on DataSet Grouped by Key Expression # Key expressions specify one or more fields of each element of a DataSet. Each key expression is either the name of a public field or a getter method. A dot can be used to drill down into objects. The key expression \u0026ldquo;*\u0026rdquo; selects all fields. The following code shows how to group a POJO DataSet using key expressions and to reduce it with a reduce function.
Java // some ordinary POJO public class WC { public String word; public int count; // [...] } // ReduceFunction that sums Integer attributes of a POJO public class WordCounter implements ReduceFunction\u0026lt;WC\u0026gt; { @Override public WC reduce(WC in1, WC in2) { return new WC(in1.word, in1.count + in2.count); } } // [...] DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words // DataSet grouping on field \u0026#34;word\u0026#34; .groupBy(\u0026#34;word\u0026#34;) // apply ReduceFunction on grouped DataSet .reduce(new WordCounter()); Scala // some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;).reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } Reduce on DataSet Grouped by KeySelector Function # A key-selector function extracts a key value from each element of a DataSet. The extracted key value is used to group the DataSet. The following code shows how to group a POJO DataSet using a key-selector function and to reduce it with a reduce function.
Java // some ordinary POJO public class WC { public String word; public int count; // [...] } // ReduceFunction that sums Integer attributes of a POJO public class WordCounter implements ReduceFunction\u0026lt;WC\u0026gt; { @Override public WC reduce(WC in1, WC in2) { return new WC(in1.word, in1.count + in2.count); } } // [...] DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words // DataSet grouping on field \u0026#34;word\u0026#34; .groupBy(new SelectWord()) // apply ReduceFunction on grouped DataSet .reduce(new WordCounter()); public class SelectWord implements KeySelector\u0026lt;WC, String\u0026gt; { @Override public String getKey(Word w) { return w.word; } } Scala // some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy { _.word } reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } Reduce on DataSet Grouped by Field Position Keys (Tuple DataSets only) # Field position keys specify one or more fields of a Tuple DataSet that are used as grouping keys. The following code shows how to use field position keys and apply a reduce function
Java DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; tuples = // [...] DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; reducedTuples = tuples // group DataSet on first and second field of Tuple .groupBy(0, 1) // apply ReduceFunction on grouped DataSet .reduce(new MyTupleReducer()); Scala val tuples = DataSet[(String, Int, Double)] = // [...] // group on the first and second Tuple field val reducedTuples = tuples.groupBy(0, 1).reduce { ... } Reduce on DataSet grouped by Case Class Fields # When using Case Classes you can also specify the grouping key using the names of the fields:
Java Not supported. Scala case class MyClass(val a: String, b: Int, c: Double) val tuples = DataSet[MyClass] = // [...] // group on the first and second field val reducedTuples = tuples.groupBy(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;).reduce { ... } GroupReduce on Grouped DataSet # A GroupReduce transformation that is applied on a grouped DataSet calls a user-defined group-reduce function for each group. The difference between this and Reduce is that the user defined function gets the whole group at once. The function is invoked with an Iterable over all elements of a group and can return an arbitrary number of result elements.
GroupReduce on DataSet Grouped by Field Position Keys (Tuple DataSets only) # The following code shows how duplicate strings can be removed from a DataSet grouped by Integer.
Java public class DistinctReduce implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; out) { Set\u0026lt;String\u0026gt; uniqStrings = new HashSet\u0026lt;String\u0026gt;(); Integer key = null; // add all strings of the group to the set for (Tuple2\u0026lt;Integer, String\u0026gt; t : in) { key = t.f0; uniqStrings.add(t.f1); } // emit all unique strings. for (String s : uniqStrings) { out.collect(new Tuple2\u0026lt;Integer, String\u0026gt;(key, s)); } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; output = input .groupBy(0) // group DataSet by the first tuple field .reduceGroup(new DistinctReduce()); // apply GroupReduceFunction Scala val input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; in.toSet foreach (out.collect) } GroupReduce on DataSet Grouped by Key Expression, KeySelector Function, or Case Class Fields # Work analogous to key expressions, key-selector functions, and case class fields in Reduce transformations.
GroupReduce on sorted groups # A group-reduce function accesses the elements of a group using an Iterable. Optionally, the Iterable can hand out the elements of a group in a specified order. In many cases this can help to reduce the complexity of a user-defined group-reduce function and improve its efficiency.
The following code shows another example how to remove duplicate Strings in a DataSet grouped by an Integer and sorted by String.
Java // GroupReduceFunction that removes consecutive identical elements public class DistinctReduce implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; out) { Integer key = null; String comp = null; for (Tuple2\u0026lt;Integer, String\u0026gt; t : in) { key = t.f0; String next = t.f1; // check if strings are different if (comp == null || !next.equals(comp)) { out.collect(new Tuple2\u0026lt;Integer, String\u0026gt;(key, next)); comp = next; } } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Double\u0026gt; output = input .groupBy(0) // group DataSet by first field .sortGroup(1, Order.ASCENDING) // sort groups on second tuple field .reduceGroup(new DistinctReduce()); Scala val input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; var prev: (Int, String) = null for (t \u0026lt;- in) { if (prev == null || prev != t) out.collect(t) prev = t } } Note: A GroupSort often comes for free if the grouping is established using a sort-based execution strategy of an operator before the reduce operation.
Combinable GroupReduceFunctions # In contrast to a reduce function, a group-reduce function is not implicitly combinable. In order to make a group-reduce function combinable it must implement the GroupCombineFunction interface.
Important: The generic input and output types of the GroupCombineFunction interface must be equal to the generic input type of the GroupReduceFunction as shown in the following example:
Java // Combinable GroupReduceFunction that computes a sum. public class MyCombinableGroupReducer implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, String\u0026gt;, GroupCombineFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in, Collector\u0026lt;String\u0026gt; out) { String key = null; int sum = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; curr : in) { key = curr.f0; sum += curr.f1; } // concat key and sum and emit out.collect(key + \u0026#34;-\u0026#34; + sum); } @Override public void combine(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { String key = null; int sum = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; curr : in) { key = curr.f0; sum += curr.f1; } // emit tuple with key and sum out.collect(new Tuple2\u0026lt;\u0026gt;(key, sum)); } } Scala // Combinable GroupReduceFunction that computes two sums. class MyCombinableGroupReducer extends GroupReduceFunction[(String, Int), String] with GroupCombineFunction[(String, Int), (String, Int)] { override def reduce( in: java.lang.Iterable[(String, Int)], out: Collector[String]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // concat key and sum and emit out.collect (r._1 + \u0026#34;-\u0026#34; + r._2) } override def combine( in: java.lang.Iterable[(String, Int)], out: Collector[(String, Int)]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // emit tuple with key and sum out.collect(r) } } GroupCombine on a Grouped DataSet # The GroupCombine transformation is the generalized form of the combine step in the combinable GroupReduceFunction. It is generalized in the sense that it allows combining of input type I to an arbitrary output type O. In contrast, the combine step in the GroupReduce only allows combining from input type I to output type I. This is because the reduce step in the GroupReduceFunction expects input type I.
In some applications, it is desirable to combine a DataSet into an intermediate format before performing additional transformations (e.g. to reduce data size). This can be achieved with a CombineGroup transformation with very little costs.
Note: The GroupCombine on a Grouped DataSet is performed in memory with a greedy strategy which may not process all data at once but in multiple steps. It is also performed on the individual partitions without a data exchange like in a GroupReduce transformation. This may lead to partial results.
The following example demonstrates the use of a CombineGroup transformation for an alternative WordCount implementation.
Java DataSet\u0026lt;String\u0026gt; input = [..] // The words received as input DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; combinedWords = input .groupBy(0) // group identical words .combineGroup(new GroupCombineFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;() { public void combine(Iterable\u0026lt;String\u0026gt; words, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;) { // combine String key = null; int count = 0; for (String word : words) { key = word; count++; } // emit tuple with word and count out.collect(new Tuple2(key, count)); } }); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; output = combinedWords .groupBy(0) // group by words again .reduceGroup(new GroupReduceFunction() { // group reduce with full data exchange public void reduce(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;) { String key = null; int count = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; word : words) { key = word; count++; } // emit tuple with word and count out.collect(new Tuple2(key, count)); } }); Scala val input: DataSet[String] = [..] // The words received as input val combinedWords: DataSet[(String, Int)] = input .groupBy(0) .combineGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var count = 0 for (word \u0026lt;- words) { key = word count += 1 } out.collect((key, count)) } val output: DataSet[(String, Int)] = combinedWords .groupBy(0) .reduceGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var sum = 0 for ((word, sum) \u0026lt;- words) { key = word sum += count } out.collect((key, sum)) } The above alternative WordCount implementation demonstrates how the GroupCombine combines words before performing the GroupReduce transformation. The above example is just a proof of concept. Note, how the combine step changes the type of the DataSet which would normally require an additional Map transformation before executing the GroupReduce.
Aggregate on Grouped Tuple DataSet # There are some common aggregation operations that are frequently used. The Aggregate transformation provides the following build-in aggregation functions:
Sum, Min, and Max. The Aggregate transformation can only be applied on a Tuple DataSet and supports only field position keys for grouping.
The following code shows how to apply an Aggregation transformation on a DataSet grouped by field position keys:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .groupBy(1) // group DataSet on second field .aggregate(SUM, 0) // compute sum of the first field .and(MIN, 2); // compute minimum of the third field Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.groupBy(1).aggregate(SUM, 0).and(MIN, 2) To apply multiple aggregations on a DataSet it is necessary to use the .and() function after the first aggregate, that means .aggregate(SUM, 0).and(MIN, 2) produces the sum of field 0 and the minimum of field 2 of the original DataSet. In contrast to that .aggregate(SUM, 0).aggregate(MIN, 2) will apply an aggregation on an aggregation. In the given example it would produce the minimum of field 2 after calculating the sum of field 0 grouped by field 1.
Note: The set of aggregation functions will be extended in the future.
MinBy / MaxBy on Grouped Tuple DataSet # The MinBy (MaxBy) transformation selects a single tuple for each group of tuples. The selected tuple is the tuple whose values of one or more specified fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) fields values, an arbitrary tuple of these tuples is returned.
The following code shows how to select the tuple with the minimum values for the Integer and Double fields for each group of tuples with the same String value from a DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt;:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .groupBy(1) // group DataSet on second field .minBy(0, 2); // select tuple with minimum values for first and third field. Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .groupBy(1) // group DataSet on second field .minBy(0, 2) // select tuple with minimum values for first and third field. Reduce on full DataSet # The Reduce transformation applies a user-defined reduce function to all elements of a DataSet. The reduce function subsequently combines pairs of elements into one element until only a single element remains.
The following code shows how to sum all elements of an Integer DataSet:
Java // ReduceFunction that sums Integers public class IntSummer implements ReduceFunction\u0026lt;Integer\u0026gt; { @Override public Integer reduce(Integer num1, Integer num2) { return num1 + num2; } } // [...] DataSet\u0026lt;Integer\u0026gt; intNumbers = // [...] DataSet\u0026lt;Integer\u0026gt; sum = intNumbers.reduce(new IntSummer()); Scala val intNumbers = env.fromElements(1,2,3) val sum = intNumbers.reduce (_ + _) Reducing a full DataSet using the Reduce transformation implies that the final Reduce operation cannot be done in parallel. However, a reduce function is automatically combinable such that a Reduce transformation does not limit scalability for most use cases.
GroupReduce on full DataSet # The GroupReduce transformation applies a user-defined group-reduce function on all elements of a DataSet. A group-reduce can iterate over all elements of DataSet and return an arbitrary number of result elements.
The following example shows how to apply a GroupReduce transformation on a full DataSet:
Java DataSet\u0026lt;Integer\u0026gt; input = // [...] // apply a (preferably combinable) GroupReduceFunction to a DataSet DataSet\u0026lt;Double\u0026gt; output = input.reduceGroup(new MyGroupReducer()); Scala val input: DataSet[Int] = // [...] val output = input.reduceGroup(new MyGroupReducer()) Note: A GroupReduce transformation on a full DataSet cannot be done in parallel if the group-reduce function is not combinable. Therefore, this can be a very compute intensive operation. See the paragraph on \u0026ldquo;Combinable GroupReduceFunctions\u0026rdquo; above to learn how to implement a combinable group-reduce function.
GroupCombine on a full DataSet # The GroupCombine on a full DataSet works similar to the GroupCombine on a grouped DataSet. The data is partitioned on all nodes and then combined in a greedy fashion (i.e. only data fitting into memory is combined at once).
Aggregate on full Tuple DataSet # There are some common aggregation operations that are frequently used. The Aggregate transformation provides the following build-in aggregation functions:
Sum, Min, and Max. The Aggregate transformation can only be applied on a Tuple DataSet.
The following code shows how to apply an Aggregation transformation on a full DataSet:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; output = input .aggregate(SUM, 0) // compute sum of the first field .and(MIN, 1); // compute minimum of the second field Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.aggregate(SUM, 0).and(MIN, 2) Note: Extending the set of supported aggregation functions is on our roadmap.
MinBy / MaxBy on full Tuple DataSet # The MinBy (MaxBy) transformation selects a single tuple from a DataSet of tuples. The selected tuple is the tuple whose values of one or more specified fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) fields values, an arbitrary tuple of these tuples is returned.
The following code shows how to select the tuple with the maximum values for the Integer and Double fields from a DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt;:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .maxBy(0, 2); // select tuple with maximum values for first and third field. Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .maxBy(0, 2) // select tuple with maximum values for first and third field. Distinct # The Distinct transformation computes the DataSet of the distinct elements of the source DataSet. The following code removes all duplicate elements from the DataSet:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; output = input.distinct(); Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.distinct() It is also possible to change how the distinction of the elements in the DataSet is decided, using:
one or more field position keys (Tuple DataSets only), a key-selector function, or a key expression. Distinct with field position keys # Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double, String\u0026gt;\u0026gt; output = input.distinct(0,2); Scala val input: DataSet[(Int, Double, String)] = // [...] val output = input.distinct(0,2) Distinct with KeySelector function # Java private static class AbsSelector implements KeySelector\u0026lt;Integer, Integer\u0026gt; { private static final long serialVersionUID = 1L; @Override public Integer getKey(Integer t) { return Math.abs(t); } } DataSet\u0026lt;Integer\u0026gt; input = // [...] DataSet\u0026lt;Integer\u0026gt; output = input.distinct(new AbsSelector()); Scala val input: DataSet[Int] = // [...] val output = input.distinct {x =\u0026gt; Math.abs(x)} Distinct with key expression # Java // some ordinary POJO public class CustomType { public String aName; public int aNumber; // [...] } DataSet\u0026lt;CustomType\u0026gt; input = // [...] DataSet\u0026lt;CustomType\u0026gt; output = input.distinct(\u0026#34;aName\u0026#34;, \u0026#34;aNumber\u0026#34;); Scala // some ordinary POJO case class CustomType(aName : String, aNumber : Int) { } val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;aName\u0026#34;, \u0026#34;aNumber\u0026#34;) It is also possible to indicate to use all the fields by the wildcard character:
Java DataSet\u0026lt;CustomType\u0026gt; input = // [...] DataSet\u0026lt;CustomType\u0026gt; output = input.distinct(\u0026#34;*\u0026#34;); Scala // some ordinary POJO val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;_\u0026#34;) Join # The Join transformation joins two DataSets into one DataSet. The elements of both DataSets are joined on one or more keys which can be specified using
a key expression a key-selector function one or more field position keys (Tuple DataSet only). Case Class Fields There are a few different ways to perform a Join transformation which are shown in the following.
Default Join (Join into Tuple2) # The default Join transformation produces a new Tuple DataSet with two fields. Each tuple holds a joined element of the first input DataSet in the first tuple field and a matching element of the second input DataSet in the second field.
The following code shows a default Join transformation using field position keys:
Java public static class User { public String name; public int zip; } public static class Store { public Manager mgr; public int zip; } DataSet\u0026lt;User\u0026gt; input1 = // [...] DataSet\u0026lt;Store\u0026gt; input2 = // [...] // result dataset is typed as Tuple2 DataSet\u0026lt;Tuple2\u0026lt;User, Store\u0026gt;\u0026gt; result = input1.join(input2) .where(\u0026#34;zip\u0026#34;) // key of the first input (users) .equalTo(\u0026#34;zip\u0026#34;); // key of the second input (stores) Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Double, Int)] = // [...] val result = input1.join(input2).where(0).equalTo(1) Join with Join Function # A Join transformation can also call a user-defined join function to process joining tuples. A join function receives one element of the first input DataSet and one element of the second input DataSet and returns exactly one element.
The following code performs a join of DataSet with custom java objects and a Tuple DataSet using key-selector functions and shows how to use a user-defined join function:
Java // some POJO public class Rating { public String name; public String category; public int points; } // Join function that joins a custom POJO with a Tuple public class PointWeighter implements JoinFunction\u0026lt;Rating, Tuple2\u0026lt;String, Double\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;String, Double\u0026gt; join(Rating rating, Tuple2\u0026lt;String, Double\u0026gt; weight) { // multiply the points and rating and construct a new output tuple return new Tuple2\u0026lt;String, Double\u0026gt;(rating.name, rating.points * weight.f1); } } DataSet\u0026lt;Rating\u0026gt; ratings = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weights = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weightedRatings = ratings.join(weights) // key of the first input .where(\u0026#34;category\u0026#34;) // key of the second input .equalTo(\u0026#34;f0\u0026#34;) // applying the JoinFunction on joining pairs .with(new PointWeighter()); Scala case class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...] val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight) =\u0026gt; (rating.name, rating.points * weight._2) } Join with Flat-Join Function # Analogous to Map and FlatMap, a FlatJoin behaves in the same way as a Join, but instead of returning one element, it can return (collect), zero, one, or more elements.
Java public class PointWeighter implements FlatJoinFunction\u0026lt;Rating, Tuple2\u0026lt;String, Double\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; { @Override public void join(Rating rating, Tuple2\u0026lt;String, Double\u0026gt; weight, Collector\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; out) { if (weight.f1 \u0026gt; 0.1) { out.collect(new Tuple2\u0026lt;String, Double\u0026gt;(rating.name, rating.points * weight.f1)); } } } DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weightedRatings = ratings.join(weights); Scala case class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...] val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight, out: Collector[(String, Double)]) =\u0026gt; if (weight._2 \u0026gt; 0.1) out.collect(rating.name, rating.points * weight._2) } Join with Projection (Java Only) # A Join transformation can construct result tuples using a projection as shown here:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Byte, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, String, Double, Byte\u0026gt;\u0026gt; result = input1.join(input2) // key definition on first DataSet using a field position key .where(0) // key definition of second DataSet using a field position key .equalTo(0) // select and reorder fields of matching tuples .projectFirst(0,2).projectSecond(1).projectFirst(1); projectFirst(int...) and projectSecond(int...) select the fields of the first and second joined input that should be assembled into an output Tuple. The order of indexes defines the order of fields in the output tuple. The join projection works also for non-Tuple DataSets. In this case, projectFirst() or projectSecond() must be called without arguments to add a joined element to the output Tuple.
Scala Not supported. Join with DataSet Size Hint # In order to guide the optimizer to pick the right execution strategy, you can hint the size of a DataSet to join as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt;\u0026gt; result1 = // hint that the second DataSet is very small input1.joinWithTiny(input2) .where(0) .equalTo(0); DataSet\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt;\u0026gt; result2 = // hint that the second DataSet is very large input1.joinWithHuge(input2) .where(0) .equalTo(0); Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...] // hint that the second DataSet is very small val result1 = input1.joinWithTiny(input2).where(0).equalTo(0) // hint that the second DataSet is very large val result1 = input1.joinWithHuge(input2).where(0).equalTo(0) Join Algorithm Hints # The Flink runtime can execute joins in various ways. Each possible way outperforms the others under different circumstances. The system tries to pick a reasonable way automatically, but allows you to manually pick a strategy, in case you want to enforce a specific way of executing the join.
Java DataSet\u0026lt;SomeType\u0026gt; input1 = // [...] DataSet\u0026lt;AnotherType\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); Scala val input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...] // hint that the second DataSet is very small val result1 = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) The following hints are available:
OPTIMIZER_CHOOSES: Equivalent to not giving a hint at all, leaves the choice to the system.
BROADCAST_HASH_FIRST: Broadcasts the first input and builds a hash table from it, which is probed by the second input. A good strategy if the first input is very small.
BROADCAST_HASH_SECOND: Broadcasts the second input and builds a hash table from it, which is probed by the first input. A good strategy if the second input is very small.
REPARTITION_HASH_FIRST: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the first input. This strategy is good if the first input is smaller than the second, but both inputs are still large. Note: This is the default fallback strategy that the system uses if no size estimates can be made and no pre-existing partitions and sort-orders can be re-used.
REPARTITION_HASH_SECOND: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the second input. This strategy is good if the second input is smaller than the first, but both inputs are still large.
REPARTITION_SORT_MERGE: The system partitions (shuffles) each input (unless the input is already partitioned) and sorts each input (unless it is already sorted). The inputs are joined by a streamed merge of the sorted inputs. This strategy is good if one or both of the inputs are already sorted.
OuterJoin # The OuterJoin transformation performs a left, right, or full outer join on two data sets. Outer joins are similar to regular (inner) joins and create all pairs of elements that are equal on their keys. In addition, records of the \u0026ldquo;outer\u0026rdquo; side (left, right, or both in case of full) are preserved if no matching key is found in the other side. Matching pair of elements (or one element and a null value for the other input) are given to a JoinFunction to turn the pair of elements into a single element, or to a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements.
The elements of both DataSets are joined on one or more keys which can be specified using
a key expression a key-selector function one or more field position keys (Tuple DataSet only). Case Class Fields OuterJoins are only supported for the Java and Scala DataSet API.
OuterJoin with Join Function # A OuterJoin transformation calls a user-defined join function to process joining tuples. A join function receives one element of the first input DataSet and one element of the second input DataSet and returns exactly one element. Depending on the type of the outer join (left, right, full) one of both input elements of the join function can be null.
The following code performs a left outer join of DataSet with custom java objects and a Tuple DataSet using key-selector functions and shows how to use a user-defined join function:
Java // some POJO public class Rating { public String name; public String category; public int points; } // Join function that joins a custom POJO with a Tuple public class PointAssigner implements JoinFunction\u0026lt;Tuple2\u0026lt;String, String\u0026gt;, Rating, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;String, Integer\u0026gt; join(Tuple2\u0026lt;String, String\u0026gt; movie, Rating rating) { // Assigns the rating points to the movie. // NOTE: rating might be null return new Tuple2\u0026lt;String, Double\u0026gt;(movie.f0, rating == null ? -1 : rating.points; } } DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; movies = // [...] DataSet\u0026lt;Rating\u0026gt; ratings = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; moviesWithPoints = movies.leftOuterJoin(ratings) // key of the first input .where(\u0026#34;f0\u0026#34;) // key of the second input .equalTo(\u0026#34;name\u0026#34;) // applying the JoinFunction on joining pairs .with(new PointAssigner()); Scala case class Rating(name: String, category: String, points: Int) val movies: DataSet[(String, String)] = // [...] val ratings: DataSet[Ratings] = // [...] val moviesWithPoints = movies.leftOuterJoin(ratings).where(0).equalTo(\u0026#34;name\u0026#34;) { (movie, rating) =\u0026gt; (movie._1, if (rating == null) -1 else rating.points) } OuterJoin with Flat-Join Function # Analogous to Map and FlatMap, an OuterJoin with flat-join function behaves in the same way as an OuterJoin with join function, but instead of returning one element, it can return (collect), zero, one, or more elements.
Java public class PointAssigner implements FlatJoinFunction\u0026lt;Tuple2\u0026lt;String, String\u0026gt;, Rating, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void join(Tuple2\u0026lt;String, String\u0026gt; movie, Rating rating, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { if (rating == null ) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(movie.f0, -1)); } else if (rating.points \u0026lt; 10) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(movie.f0, rating.points)); } else { // do not emit } } DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; moviesWithPoints = movies.leftOuterJoin(ratings) // [...] Scala Not supported. Join Algorithm Hints # The Flink runtime can execute outer joins in various ways. Each possible way outperforms the others under different circumstances. The system tries to pick a reasonable way automatically, but allows you to manually pick a strategy, in case you want to enforce a specific way of executing the outer join.
Java DataSet\u0026lt;SomeType\u0026gt; input1 = // [...] DataSet\u0026lt;AnotherType\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result1 = input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result2 = input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); Scala val input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...] // hint that the second DataSet is very small val result1 = input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) val result2 = input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) The following hints are available.
OPTIMIZER_CHOOSES: Equivalent to not giving a hint at all, leaves the choice to the system.
BROADCAST_HASH_FIRST: Broadcasts the first input and builds a hash table from it, which is probed by the second input. A good strategy if the first input is very small.
BROADCAST_HASH_SECOND: Broadcasts the second input and builds a hash table from it, which is probed by the first input. A good strategy if the second input is very small.
REPARTITION_HASH_FIRST: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the first input. This strategy is good if the first input is smaller than the second, but both inputs are still large.
REPARTITION_HASH_SECOND: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the second input. This strategy is good if the second input is smaller than the first, but both inputs are still large.
REPARTITION_SORT_MERGE: The system partitions (shuffles) each input (unless the input is already partitioned) and sorts each input (unless it is already sorted). The inputs are joined by a streamed merge of the sorted inputs. This strategy is good if one or both of the inputs are already sorted.
NOTE: Not all execution strategies are supported by every outer join type, yet.
LeftOuterJoin supports:
OPTIMIZER_CHOOSES BROADCAST_HASH_SECOND REPARTITION_HASH_SECOND REPARTITION_SORT_MERGE RightOuterJoin supports:
OPTIMIZER_CHOOSES BROADCAST_HASH_FIRST REPARTITION_HASH_FIRST REPARTITION_SORT_MERGE FullOuterJoin supports:
OPTIMIZER_CHOOSES REPARTITION_SORT_MERGE Cross # The Cross transformation combines two DataSets into one DataSet. It builds all pairwise combinations of the elements of both input DataSets, i.e., it builds a Cartesian product. The Cross transformation either calls a user-defined cross function on each pair of elements or outputs a Tuple2. Both modes are shown in the following.
Note: Cross is potentially a very compute-intensive operation which can challenge even large compute clusters!
Cross with User-Defined Function # A Cross transformation can call a user-defined cross function. A cross function receives one element of the first input and one element of the second input and returns exactly one result element.
The following code shows how to apply a Cross transformation on two DataSets using a cross function:
Java public class Coord { public int id; public int x; public int y; } // CrossFunction computes the Euclidean distance between two Coord objects. public class EuclideanDistComputer implements CrossFunction\u0026lt;Coord, Coord, Tuple3\u0026lt;Integer, Integer, Double\u0026gt;\u0026gt; { @Override public Tuple3\u0026lt;Integer, Integer, Double\u0026gt; cross(Coord c1, Coord c2) { // compute Euclidean distance of coordinates double dist = sqrt(pow(c1.x - c2.x, 2) + pow(c1.y - c2.y, 2)); return new Tuple3\u0026lt;Integer, Integer, Double\u0026gt;(c1.id, c2.id, dist); } } DataSet\u0026lt;Coord\u0026gt; coords1 = // [...] DataSet\u0026lt;Coord\u0026gt; coords2 = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, Integer, Double\u0026gt;\u0026gt; distances = coords1.cross(coords2) // apply CrossFunction .with(new EuclideanDistComputer()); Cross with Projection # A Cross transformation can also construct result tuples using a projection as shown here:
DataSet\u0026lt;Tuple3\u0026lt;Integer, Byte, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, Byte, Integer, Double\u0026gt; result = input1.cross(input2) // select and reorder fields of matching tuples .projectSecond(0).projectFirst(1,0).projectSecond(1); The field selection in a Cross projection works the same way as in the projection of Join results.
Scala case class Coord(id: Int, x: Int, y: Int) val coords1: DataSet[Coord] = // [...] val coords2: DataSet[Coord] = // [...] val distances = coords1.cross(coords2) { (c1, c2) =\u0026gt; val dist = sqrt(pow(c1.x - c2.x, 2) + pow(c1.y - c2.y, 2)) (c1.id, c2.id, dist) } Cross with DataSet Size Hint # In order to guide the optimizer to pick the right execution strategy, you can hint the size of a DataSet to cross as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, String, Integer, String\u0026gt;\u0026gt; udfResult = // hint that the second DataSet is very small input1.crossWithTiny(input2) // apply any Cross function (or projection) .with(new MyCrosser()); DataSet\u0026lt;Tuple3\u0026lt;Integer, Integer, String\u0026gt;\u0026gt; projectResult = // hint that the second DataSet is very large input1.crossWithHuge(input2) // apply a projection (or any Cross function) .projectFirst(0,1).projectSecond(1); Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...] // hint that the second DataSet is very small val result1 = input1.crossWithTiny(input2) // hint that the second DataSet is very large val result1 = input1.crossWithHuge(input2) CoGroup # The CoGroup transformation jointly processes groups of two DataSets. Both DataSets are grouped on a defined key and groups of both DataSets that share the same key are handed together to a user-defined co-group function. If for a specific key only one DataSet has a group, the co-group function is called with this group and an empty group. A co-group function can separately iterate over the elements of both groups and return an arbitrary number of result elements.
Similar to Reduce, GroupReduce, and Join, keys can be defined using the different key-selection methods.
CoGroup on DataSets # Java The example shows how to group by Field Position Keys (Tuple DataSets only). You can do the same with Pojo-types and key expressions.
// Some CoGroupFunction definition class MyCoGrouper implements CoGroupFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;, Double\u0026gt; { @Override public void coGroup(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; iVals, Iterable\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; dVals, Collector\u0026lt;Double\u0026gt; out) { Set\u0026lt;Integer\u0026gt; ints = new HashSet\u0026lt;Integer\u0026gt;(); // add all Integer values in group to set for (Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; val : iVals) { ints.add(val.f1); } // multiply each Double value with each unique Integer values of group for (Tuple2\u0026lt;String, Double\u0026gt; val : dVals) { for (Integer i : ints) { out.collect(val.f1 * i); } } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; iVals = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; dVals = // [...] DataSet\u0026lt;Double\u0026gt; output = iVals.coGroup(dVals) // group first DataSet on first tuple field .where(0) // group second DataSet on first tuple field .equalTo(0) // apply CoGroup function on each pair of groups .with(new MyCoGrouper()); Scala val iVals: DataSet[(String, Int)] = // [...] val dVals: DataSet[(String, Double)] = // [...] val output = iVals.coGroup(dVals).where(0).equalTo(0) { (iVals, dVals, out: Collector[Double]) =\u0026gt; val ints = iVals map { _._2 } toSet for (dVal \u0026lt;- dVals) { for (i \u0026lt;- ints) { out.collect(dVal._2 * i) } } } Union # Produces the union of two DataSets, which have to be of the same type. A union of more than two DataSets can be implemented with multiple union calls, as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals3 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; unioned = vals1.union(vals2).union(vals3); Scala val vals1: DataSet[(String, Int)] = // [...] val vals2: DataSet[(String, Int)] = // [...] val vals3: DataSet[(String, Int)] = // [...] val unioned = vals1.union(vals2).union(vals3) Rebalance # Evenly rebalances the parallel partitions of a DataSet to eliminate data skew.
Java DataSet\u0026lt;String\u0026gt; in = // [...] // rebalance DataSet and apply a Map transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.rebalance() .map(new Mapper()); Scala val in: DataSet[String] = // [...] // rebalance DataSet and apply a Map transformation. val out = in.rebalance().map { ... } Hash-Partition # Hash-partitions a DataSet on a given key. Keys can be specified as position keys, expression keys, and key selector functions (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // hash-partition DataSet by String value and apply a MapPartition transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // hash-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByHash(0).mapPartition { ... } Range-Partition # Range-partitions a DataSet on a given key. Keys can be specified as position keys, expression keys, and key selector functions (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // range-partition DataSet by String value and apply a MapPartition transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // range-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByRange(0).mapPartition { ... } Sort Partition # Locally sorts all partitions of a DataSet on a specified field in a specified order. Fields can be specified as field expressions or field positions (see Reduce examples for how to specify keys). Partitions can be sorted on multiple fields by chaining sortPartition() calls.
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // Locally sort partitions in ascending order on the second String field and // in descending order on the first String field. // Apply a MapPartition transformation on the sorted partitions. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.sortPartition(1, Order.ASCENDING) .sortPartition(0, Order.DESCENDING) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // Locally sort partitions in ascending order on the second String field and // in descending order on the first String field. // Apply a MapPartition transformation on the sorted partitions. val out = in.sortPartition(1, Order.ASCENDING) .sortPartition(0, Order.DESCENDING) .mapPartition { ... } First-n # Returns the first n (arbitrary) elements of a DataSet. First-n can be applied on a regular DataSet, a grouped DataSet, or a grouped-sorted DataSet. Grouping keys can be specified as key-selector functions or field position keys (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // Return the first five (arbitrary) elements of the DataSet DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out1 = in.first(5); // Return the first two (arbitrary) elements of each String group DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out2 = in.groupBy(0) .first(2); // Return the first three elements of each String group ordered by the Integer field DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); Scala val in: DataSet[(String, Int)] = // [...] // Return the first five (arbitrary) elements of the DataSet val out1 = in.first(5) // Return the first two (arbitrary) elements of each String group val out2 = in.groupBy(0).first(2) // Return the first three elements of each String group ordered by the Integer field val out3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) `}),e.add({id:63,href:"/flink/flink-docs-master/docs/dev/configuration/maven/",title:"Using Maven",section:"Project Configuration",content:` How to use Maven to configure your project # This guide will show you how to configure a Flink job project with Maven, an open-source build automation tool developed by the Apache Software Foundation that enables you to build, publish, and deploy projects. You can use it to manage the entire lifecycle of your software project.
Requirements # Maven 3.0.4 (or higher) Java 11 Importing the project into your IDE # Once the project folder and files have been created, we recommend that you import this project into your IDE for developing and testing.
IntelliJ IDEA supports Maven projects out-of-the-box. Eclipse offers the m2e plugin to import Maven projects.
Note: The default JVM heap size for Java may be too small for Flink and you have to manually increase it. In Eclipse, choose Run Configurations -\u0026gt; Arguments and write into the VM Arguments box: -Xmx800m. In IntelliJ IDEA recommended way to change JVM options is from the Help | Edit Custom VM Options menu. See this article for details.
Note on IntelliJ: To make the applications run within IntelliJ IDEA, it is necessary to tick the Include dependencies with \u0026quot;Provided\u0026quot; scope box in the run configuration. If this option is not available (possibly due to using an older IntelliJ IDEA version), then a workaround is to create a test that calls the application\u0026rsquo;s main() method.
Building the project # If you want to build/package your project, navigate to your project directory and run the \u0026lsquo;mvn clean package\u0026rsquo; command. You will find a JAR file that contains your application (plus connectors and libraries that you may have added as dependencies to the application) here:target/\u0026lt;artifact-id\u0026gt;-\u0026lt;version\u0026gt;.jar.
Note: If you used a different class than DataStreamJob as the application\u0026rsquo;s main class / entry point, we recommend you change the mainClass setting in the pom.xml file accordingly so that Flink can run the application from the JAR file without additionally specifying the main class.
Adding dependencies to the project # Open the pom.xml file in your project directory and add the dependency in between the dependencies tab.
For example, you can add the Kafka connector as a dependency like this:
\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Then execute mvn install on the command line.
Projects created from the Java Project Template, the Scala Project Template, or Gradle are configured to automatically include the application dependencies into the application JAR when you run mvn clean package. For projects that are not set up from those templates, we recommend adding the Maven Shade Plugin to build the application jar with all required dependencies.
Important: Note that all these core API dependencies should have their scope set to provided. This means that they are needed to compile against, but that they should not be packaged into the project\u0026rsquo;s resulting application JAR file. If not set to provided, the best case scenario is that the resulting JAR becomes excessively large, because it also contains all Flink core dependencies. The worst case scenario is that the Flink core dependencies that are added to the application\u0026rsquo;s JAR file clash with some of your own dependency versions (which is normally avoided through inverted classloading).
To correctly package the dependencies into the application JAR, the Flink API dependencies must be set to the compile scope.
Packaging the application # Depending on your use case, you may need to package your Flink application in different ways before it gets deployed to a Flink environment.
If you want to create a JAR for a Flink Job and use only Flink dependencies without any third-party dependencies (i.e. using the filesystem connector with JSON format), you do not need to create an uber/fat JAR or shade any dependencies.
If you want to create a JAR for a Flink Job and use external dependencies not built into the Flink distribution, you can either add them to the classpath of the distribution or shade them into your uber/fat application JAR.
With the generated uber/fat JAR, you can submit it to a local or remote cluster with:
bin/flink run -c org.example.MyJob myFatJar.jar To learn more about how to deploy Flink jobs, check out the deployment guide.
Template for creating an uber/fat JAR with dependencies # To build an application JAR that contains all dependencies required for declared connectors and libraries, you can use the following shade plugin definition:
\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;artifactSet\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/artifactSet\u0026gt; \u0026lt;filters\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; \u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/filters\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;!-- Replace this with the main class of your job --\u0026gt; \u0026lt;mainClass\u0026gt;my.programs.main.clazz\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; The Maven shade plugin will include, by default, all the dependencies in the \u0026ldquo;runtime\u0026rdquo; and \u0026ldquo;compile\u0026rdquo; scope.
`}),e.add({id:64,href:"/flink/flink-docs-master/docs/dev/datastream/operators/windows/",title:"Windows",section:"Operators",content:` Windows # Windows are at the heart of processing infinite streams. Windows split the stream into \u0026ldquo;buckets\u0026rdquo; of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink and how the programmer can benefit to the maximum from its offered functionality.
The general structure of a windowed Flink program is presented below. The first snippet refers to keyed streams, while the second to non-keyed ones. As one can see, the only difference is the keyBy(...) call for the keyed streams and the window(...) which becomes windowAll(...) for non-keyed streams. This is also going to serve as a roadmap for the rest of the page.
Keyed Windows
Java/Scala stream .keyBy(...) \u0026lt;- keyed versus non-keyed windows .window(...) \u0026lt;- required: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- optional: \u0026quot;trigger\u0026quot; (else default trigger) [.evictor(...)] \u0026lt;- optional: \u0026quot;evictor\u0026quot; (else no evictor) [.allowedLateness(...)] \u0026lt;- optional: \u0026quot;lateness\u0026quot; (else zero) [.sideOutputLateData(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- required: \u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; Python stream .key_by(...) .window(...) \u0026lt;- required: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- optional: \u0026quot;trigger\u0026quot; (else default trigger) [.allowed_lateness(...)] \u0026lt;- optional: \u0026quot;lateness\u0026quot; (else zero) [.side_output_late_data(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- required: \u0026quot;function\u0026quot; [.get_side_output(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; Non-Keyed Windows
Java/Scala stream .windowAll(...) \u0026lt;- required: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- optional: \u0026quot;trigger\u0026quot; (else default trigger) [.evictor(...)] \u0026lt;- optional: \u0026quot;evictor\u0026quot; (else no evictor) [.allowedLateness(...)] \u0026lt;- optional: \u0026quot;lateness\u0026quot; (else zero) [.sideOutputLateData(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- required: \u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; Python stream .window_all(...) \u0026lt;- required: \u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- optional: \u0026quot;trigger\u0026quot; (else default trigger) [.allowed_lateness(...)] \u0026lt;- optional: \u0026quot;lateness\u0026quot; (else zero) [.side_output_late_data(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- required: \u0026quot;function\u0026quot; [.get_side_output(...)] \u0026lt;- optional: \u0026quot;output tag\u0026quot; In the above, the commands in square brackets ([\u0026hellip;]) are optional. This reveals that Flink allows you to customize your windowing logic in many different ways so that it best fits your needs. Note: Evictor is still not supported in Python DataStream API. Window Lifecycle # In a nutshell, a window is created as soon as the first element that should belong to this window arrives, and the window is completely removed when the time (event or processing time) passes its end timestamp plus the user-specified allowed lateness (see Allowed Lateness). Flink guarantees removal only for time-based windows and not for other types, e.g. global windows (see Window Assigners). For example, with an event-time-based windowing strategy that creates non-overlapping (or tumbling) windows every 5 minutes and has an allowed lateness of 1 min, Flink will create a new window for the interval between 12:00 and 12:05 when the first element with a timestamp that falls into this interval arrives, and it will remove it when the watermark passes the 12:06 timestamp.
In addition, each window will have a Trigger (see Triggers) and a function (ProcessWindowFunction, ReduceFunction, or AggregateFunction) (see Window Functions) attached to it. The function will contain the computation to be applied to the contents of the window, while the Trigger specifies the conditions under which the window is considered ready for the function to be applied. A triggering policy might be something like \u0026ldquo;when the number of elements in the window is more than 4\u0026rdquo;, or \u0026ldquo;when the watermark passes the end of the window\u0026rdquo;. A trigger can also decide to purge a window\u0026rsquo;s contents any time between its creation and removal. Purging in this case only refers to the elements in the window, and not the window metadata. This means that new data can still be added to that window.
Apart from the above, you can specify an Evictor (see Evictors) which will be able to remove elements from the window after the trigger fires and before and/or after the function is applied.
In the following we go into more detail for each of the components above. We start with the required parts in the above snippet (see Keyed vs Non-Keyed Windows, Window Assigners, and Window Functions) before moving to the optional ones.
Keyed vs Non-Keyed Windows # The first thing to specify is whether your stream should be keyed or not. This has to be done before defining the window. Using the keyBy(...) will split your infinite stream into logical keyed streams. If keyBy(...) is not called, your stream is not keyed.
In the case of keyed streams, any attribute of your incoming events can be used as a key (more details here). Having a keyed stream will allow your windowed computation to be performed in parallel by multiple tasks, as each logical keyed stream can be processed independently from the rest. All elements referring to the same key will be sent to the same parallel task.
In case of non-keyed streams, your original stream will not be split into multiple logical streams and all the windowing logic will be performed by a single task, i.e. with parallelism of 1.
Window Assigners # After specifying whether your stream is keyed or not, the next step is to define a window assigner. The window assigner defines how elements are assigned to windows. This is done by specifying the WindowAssigner of your choice in the window(...) (for keyed streams) or the windowAll() (for non-keyed streams) call.
A WindowAssigner is responsible for assigning each incoming element to one or more windows. Flink comes with pre-defined window assigners for the most common use cases, namely tumbling windows, sliding windows, session windows and global windows. You can also implement a custom window assigner by extending the WindowAssigner class. All built-in window assigners (except the global windows) assign elements to windows based on time, which can either be processing time or event time. Please take a look at our section on event time to learn about the difference between processing time and event time and how timestamps and watermarks are generated.
Time-based windows have a start timestamp (inclusive) and an end timestamp (exclusive) that together describe the size of the window. In code, Flink uses TimeWindow when working with time-based windows which has methods for querying the start- and end-timestamp and also an additional method maxTimestamp() that returns the largest allowed timestamp for a given windows.
In the following, we show how Flink\u0026rsquo;s pre-defined window assigners work and how they are used in a DataStream program. The following figures visualize the workings of each assigner. The purple circles represent elements of the stream, which are partitioned by some key (in this case user 1, user 2 and user 3). The x-axis shows the progress of time.
Tumbling Windows # A tumbling windows assigner assigns each element to a window of a specified window size. Tumbling windows have a fixed size and do not overlap. For example, if you specify a tumbling window with a size of 5 minutes, the current window will be evaluated and a new window will be started every five minutes as illustrated by the following figure.
The following code snippets show how to use tumbling windows.
Java DataStream\u0026lt;T\u0026gt; input = ...; // tumbling event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // tumbling processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // daily tumbling event-time windows offset by -8 hours. input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // tumbling event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // tumbling processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // daily tumbling event-time windows offset by -8 hours. input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream # tumbling event-time windows input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # tumbling processing-time windows input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # daily tumbling event-time windows offset by -8 hours. input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Time intervals can be specified by using one of Time.milliseconds(x), Time.seconds(x), Time.minutes(x), and so on.
As shown in the last example, tumbling window assigners also take an optional offset parameter that can be used to change the alignment of windows. For example, without offsets hourly tumbling windows are aligned with epoch, that is you will get windows such as 1:00:00.000 - 1:59:59.999, 2:00:00.000 - 2:59:59.999 and so on. If you want to change that you can give an offset. With an offset of 15 minutes you would, for example, get 1:15:00.000 - 2:14:59.999, 2:15:00.000 - 3:14:59.999 etc. An important use case for offsets is to adjust windows to timezones other than UTC-0. For example, in China you would have to specify an offset of Time.hours(-8).
Sliding Windows # The sliding windows assigner assigns elements to windows of fixed length. Similar to a tumbling windows assigner, the size of the windows is configured by the window size parameter. An additional window slide parameter controls how frequently a sliding window is started. Hence, sliding windows can be overlapping if the slide is smaller than the window size. In this case elements are assigned to multiple windows.
For example, you could have windows of size 10 minutes that slides by 5 minutes. With this you get every 5 minutes a window that contains the events that arrived during the last 10 minutes as depicted by the following figure.
The following code snippets show how to use sliding windows.
Java DataStream\u0026lt;T\u0026gt; input = ...; // sliding event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // sliding processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // sliding processing-time windows offset by -8 hours input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // sliding event-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // sliding processing-time windows input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // sliding processing-time windows offset by -8 hours input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream # sliding event-time windows input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # sliding processing-time windows input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # sliding processing-time windows offset by -8 hours input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Time intervals can be specified by using one of Time.milliseconds(x), Time.seconds(x), Time.minutes(x), and so on.
As shown in the last example, sliding window assigners also take an optional offset parameter that can be used to change the alignment of windows. For example, without offsets hourly windows sliding by 30 minutes are aligned with epoch, that is you will get windows such as 1:00:00.000 - 1:59:59.999, 1:30:00.000 - 2:29:59.999 and so on. If you want to change that you can give an offset. With an offset of 15 minutes you would, for example, get 1:15:00.000 - 2:14:59.999, 1:45:00.000 - 2:44:59.999 etc. An important use case for offsets is to adjust windows to timezones other than UTC-0. For example, in China you would have to specify an offset of Time.hours(-8).
Session Windows # The session windows assigner groups elements by sessions of activity. Session windows do not overlap and do not have a fixed start and end time, in contrast to tumbling windows and sliding windows. Instead a session window closes when it does not receive elements for a certain period of time, i.e., when a gap of inactivity occurred. A session window assigner can be configured with either a static session gap or with a session gap extractor function which defines how long the period of inactivity is. When this period expires, the current session closes and subsequent elements are assigned to a new session window.
The following code snippets show how to use session windows.
Java DataStream\u0026lt;T\u0026gt; input = ...; // event-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // event-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withDynamicGap((element) -\u0026gt; { // determine and return session gap })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // processing-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // processing-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withDynamicGap((element) -\u0026gt; { // determine and return session gap })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // event-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // event-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // determine and return session gap } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // processing-time session windows with static gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // processing-time session windows with dynamic gap input .keyBy(\u0026lt;key selector\u0026gt;) .window(DynamicProcessingTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // determine and return session gap } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream class MySessionWindowTimeGapExtractor(SessionWindowTimeGapExtractor): def extract(self, element: tuple) -\u0026gt; int: # determine and return session gap # event-time session windows with static gap input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(EventTimeSessionWindows.with_gap(Time.minutes(10))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # event-time session windows with dynamic gap input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(EventTimeSessionWindows.with_dynamic_gap(MySessionWindowTimeGapExtractor())) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # processing-time session windows with static gap input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(ProcessingTimeSessionWindows.with_gap(Time.minutes(10))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # processing-time session windows with dynamic gap input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(DynamicProcessingTimeSessionWindows.with_dynamic_gap(MySessionWindowTimeGapExtractor())) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Static gaps can be specified by using one of Time.milliseconds(x), Time.seconds(x), Time.minutes(x), and so on.
Dynamic gaps are specified by implementing the SessionWindowTimeGapExtractor interface.
Since session windows do not have a fixed start and end, they are evaluated differently than tumbling and sliding windows. Internally, a session window operator creates a new window for each arriving record and merges windows together if they are closer to each other than the defined gap. In order to be mergeable, a session window operator requires a merging Trigger and a merging Window Function, such as ReduceFunction, AggregateFunction, or ProcessWindowFunction Global Windows # A global windows assigner assigns all elements with the same key to the same single global window. This windowing scheme is only useful if you also specify a custom trigger. Otherwise, no computation will be performed, as the global window does not have a natural end at which we could process the aggregated elements.
The following code snippets show how to use a global window.
Java DataStream\u0026lt;T\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(GlobalWindows.create()) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(GlobalWindows.create()) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(GlobalWindows.create()) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Window Functions # After defining the window assigner, we need to specify the computation that we want to perform on each of these windows. This is the responsibility of the window function, which is used to process the elements of each (possibly keyed) window once the system determines that a window is ready for processing (see triggers for how Flink determines when a window is ready).
The window function can be one of ReduceFunction, AggregateFunction, or ProcessWindowFunction. The first two can be executed more efficiently (see State Size section) because Flink can incrementally aggregate the elements for each window as they arrive. A ProcessWindowFunction gets an Iterable for all the elements contained in a window and additional meta information about the window to which the elements belong.
A windowed transformation with a ProcessWindowFunction cannot be executed as efficiently as the other cases because Flink has to buffer all elements for a window internally before invoking the function. This can be mitigated by combining a ProcessWindowFunction with a ReduceFunction, or AggregateFunction to get both incremental aggregation of window elements and the additional window metadata that the ProcessWindowFunction receives. We will look at examples for each of these variants.
ReduceFunction # A ReduceFunction specifies how two elements from the input are combined to produce an output element of the same type. Flink uses a ReduceFunction to incrementally aggregate the elements of a window.
A ReduceFunction can be defined and used like this:
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce(new ReduceFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt;() { public Tuple2\u0026lt;String, Long\u0026gt; reduce(Tuple2\u0026lt;String, Long\u0026gt; v1, Tuple2\u0026lt;String, Long\u0026gt; v2) { return new Tuple2\u0026lt;\u0026gt;(v1.f0, v1.f1 + v2.f1); } }); Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce { (v1, v2) =\u0026gt; (v1._1, v1._2 + v2._2) } Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .reduce(lambda v1, v2: (v1[0], v1[1] + v2[1]), output_type=Types.TUPLE([Types.STRING(), Types.LONG()])) The above example sums up the second fields of the tuples for all elements in a window.
AggregateFunction # An AggregateFunction is a generalized version of a ReduceFunction that has three types: an input type (IN), accumulator type (ACC), and an output type (OUT). The input type is the type of elements in the input stream and the AggregateFunction has a method for adding one input element to an accumulator. The interface also has methods for creating an initial accumulator, for merging two accumulators into one accumulator and for extracting an output (of type OUT) from an accumulator. We will see how this works in the example below.
Same as with ReduceFunction, Flink will incrementally aggregate input elements of a window as they arrive.
An AggregateFunction can be defined and used like this:
Java /** * The accumulator is used to keep a running sum and a count. The {@code getResult} method * computes the average. */ private static class AverageAggregate implements AggregateFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Double\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; createAccumulator() { return new Tuple2\u0026lt;\u0026gt;(0L, 0L); } @Override public Tuple2\u0026lt;Long, Long\u0026gt; add(Tuple2\u0026lt;String, Long\u0026gt; value, Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return new Tuple2\u0026lt;\u0026gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); } @Override public Double getResult(Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return ((double) accumulator.f0) / accumulator.f1; } @Override public Tuple2\u0026lt;Long, Long\u0026gt; merge(Tuple2\u0026lt;Long, Long\u0026gt; a, Tuple2\u0026lt;Long, Long\u0026gt; b) { return new Tuple2\u0026lt;\u0026gt;(a.f0 + b.f0, a.f1 + b.f1); } } DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate()); Scala /** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate) Python class AverageAggregate(AggregateFunction): def create_accumulator(self) -\u0026gt; Tuple[int, int]: return 0, 0 def add(self, value: Tuple[str, int], accumulator: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return accumulator[0] + value[1], accumulator[1] + 1 def get_result(self, accumulator: Tuple[int, int]) -\u0026gt; float: return accumulator[0] / accumulator[1] def merge(self, a: Tuple[int, int], b: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return a[0] + b[0], a[1] + b[1] input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .aggregate(AverageAggregate(), accumulator_type=Types.TUPLE([Types.LONG(), Types.LONG()]), output_type=Types.DOUBLE()) The above example computes the average of the second field of the elements in the window.
ProcessWindowFunction # A ProcessWindowFunction gets an Iterable containing all the elements of the window, and a Context object with access to time and state information, which enables it to provide more flexibility than other window functions. This comes at the cost of performance and resource consumption, because elements cannot be incrementally aggregated but instead need to be buffered internally until the window is considered ready for processing.
The signature of ProcessWindowFunction looks as follows:
Java public abstract class ProcessWindowFunction\u0026lt;IN, OUT, KEY, W extends Window\u0026gt; implements Function { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable\u0026lt;IN\u0026gt; elements, Collector\u0026lt;OUT\u0026gt; out) throws Exception; /** * Deletes any state in the {@code Context} when the Window expires (the watermark passes its * {@code maxTimestamp} + {@code allowedLateness}). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public void clear(Context context) throws Exception {} /** * The context holding window metadata. */ public abstract class Context implements java.io.Serializable { /** * Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;NOTE:\u0026lt;/b\u0026gt;If you use per-window state you have to ensure that you clean it up * by implementing {@link ProcessWindowFunction#clear(Context)}. */ public abstract KeyedStateStore windowState(); /** * State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); } } Scala abstract class ProcessWindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * Deletes any state in the [[Context]] when the Window expires * (the watermark passes its \`maxTimestamp\` + \`allowedLateness\`). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ @throws[Exception] def clear(context: Context) {} /** * The context holding window metadata */ abstract class Context { /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore } } Python class ProcessWindowFunction(Function, Generic[IN, OUT, KEY, W]): @abstractmethod def process(self, key: KEY, context: \u0026#39;ProcessWindowFunction.Context\u0026#39;, elements: Iterable[IN]) -\u0026gt; Iterable[OUT]: \u0026#34;\u0026#34;\u0026#34; Evaluates the window and outputs none or several elements. :param key: The key for which this window is evaluated. :param context: The context in which the window is being evaluated. :param elements: The elements in the window being evaluated. :return: The iterable object which produces the elements to emit. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def clear(self, context: \u0026#39;ProcessWindowFunction.Context\u0026#39;) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Deletes any state in the :class:\`Context\` when the Window expires (the watermark passes its max_timestamp + allowed_lateness). :param context: The context to which the window is being evaluated. \u0026#34;\u0026#34;\u0026#34; pass class Context(ABC, Generic[W2]): \u0026#34;\u0026#34;\u0026#34; The context holding window metadata. \u0026#34;\u0026#34;\u0026#34; @abstractmethod def window(self) -\u0026gt; W2: \u0026#34;\u0026#34;\u0026#34; :return: The window that is being evaluated. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def current_processing_time(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; :return: The current processing time. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def current_watermark(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; :return: The current event-time watermark. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def window_state(self) -\u0026gt; KeyedStateStore: \u0026#34;\u0026#34;\u0026#34; State accessor for per-key and per-window state. .. note:: If you use per-window state you have to ensure that you clean it up by implementing :func:\`~ProcessWindowFunction.clear\`. :return: The :class:\`KeyedStateStore\` used to access per-key and per-window states. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def global_state(self) -\u0026gt; KeyedStateStore: \u0026#34;\u0026#34;\u0026#34; State accessor for per-key global state. \u0026#34;\u0026#34;\u0026#34; pass The key parameter is the key that is extracted via the KeySelector that was specified for the keyBy() invocation. In case of tuple-index keys or string-field references this key type is always Tuple and you have to manually cast it to a tuple of the correct size to extract the key fields.
A ProcessWindowFunction can be defined and used like this:
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(t -\u0026gt; t.f0) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction()); /* ... */ public class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, String, String, TimeWindow\u0026gt; { @Override public void process(String key, Context context, Iterable\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input, Collector\u0026lt;String\u0026gt; out) { long count = 0; for (Tuple2\u0026lt;String, Long\u0026gt; in: input) { count++; } out.collect(\u0026#34;Window: \u0026#34; + context.window() + \u0026#34;count: \u0026#34; + count); } } Scala val input: DataStream[(String, Long)] = ... input .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction()) /* ... */ class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] { def process(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]) = { var count = 0L for (in \u0026lt;- input) { count = count + 1 } out.collect(s\u0026#34;Window \${context.window} count: \$count\u0026#34;) } } Python input = ... # type: DataStream input \\ .key_by(lambda v: v[0]) \\ .window(TumblingEventTimeWindows.of(Time.minutes(5))) \\ .process(MyProcessWindowFunction()) # ... class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, elements: Iterable[Tuple[str, int]]) -\u0026gt; Iterable[str]: count = 0 for _ in elements: count += 1 yield \u0026#34;Window: {} count: {}\u0026#34;.format(context.window(), count) The example shows a ProcessWindowFunction that counts the elements in a window. In addition, the window function adds information about the window to the output.
Note that using ProcessWindowFunction for simple aggregates such as count is quite inefficient. The next section shows how a ReduceFunction or AggregateFunction can be combined with a ProcessWindowFunction to get both incremental aggregation and the added information of a ProcessWindowFunction. ProcessWindowFunction with Incremental Aggregation # A ProcessWindowFunction can be combined with either a ReduceFunction, or an AggregateFunction to incrementally aggregate elements as they arrive in the window. When the window is closed, the ProcessWindowFunction will be provided with the aggregated result. This allows it to incrementally compute windows while having access to the additional window meta information of the ProcessWindowFunction.
You can also use the legacy WindowFunction instead of ProcessWindowFunction for incremental window aggregation.
Incremental Window Aggregation with ReduceFunction # The following example shows how an incremental ReduceFunction can be combined with a ProcessWindowFunction to return the smallest event in a window along with the start time of the window.
Java DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce(new MyReduceFunction(), new MyProcessWindowFunction()); // Function definitions private static class MyReduceFunction implements ReduceFunction\u0026lt;SensorReading\u0026gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() \u0026gt; r2.value() ? r2 : r1; } } private static class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;SensorReading, Tuple2\u0026lt;Long, SensorReading\u0026gt;, String, TimeWindow\u0026gt; { public void process(String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; minReadings, Collector\u0026lt;Tuple2\u0026lt;Long, SensorReading\u0026gt;\u0026gt; out) { SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2\u0026lt;Long, SensorReading\u0026gt;(context.window().getStart(), min)); } } Scala val input: DataStream[SensorReading] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce( (r1: SensorReading, r2: SensorReading) =\u0026gt; { if (r1.value \u0026gt; r2.value) r2 else r1 }, ( key: String, context: ProcessWindowFunction[_, _, _, TimeWindow]#Context, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =\u0026gt; { val min = minReadings.iterator.next() out.collect((context.window.getStart, min)) } ) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .reduce(lambda r1, r2: r2 if r1.value \u0026gt; r2.value else r1, window_function=MyProcessWindowFunction(), output_type=Types.TUPLE([Types.STRING(), Types.LONG()])) # Function definition class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, min_readings: Iterable[SensorReading]) -\u0026gt; Iterable[Tuple[int, SensorReading]]: min = next(iter(min_readings)) yield context.window().start, min Incremental Window Aggregation with AggregateFunction # The following example shows how an incremental AggregateFunction can be combined with a ProcessWindowFunction to compute the average and also emit the key and window along with the average.
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction()); // Function definitions /** * The accumulator is used to keep a running sum and a count. The {@code getResult} method * computes the average. */ private static class AverageAggregate implements AggregateFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Double\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; createAccumulator() { return new Tuple2\u0026lt;\u0026gt;(0L, 0L); } @Override public Tuple2\u0026lt;Long, Long\u0026gt; add(Tuple2\u0026lt;String, Long\u0026gt; value, Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return new Tuple2\u0026lt;\u0026gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); } @Override public Double getResult(Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return ((double) accumulator.f0) / accumulator.f1; } @Override public Tuple2\u0026lt;Long, Long\u0026gt; merge(Tuple2\u0026lt;Long, Long\u0026gt; a, Tuple2\u0026lt;Long, Long\u0026gt; b) { return new Tuple2\u0026lt;\u0026gt;(a.f0 + b.f0, a.f1 + b.f1); } } private static class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;Double, Tuple2\u0026lt;String, Double\u0026gt;, String, TimeWindow\u0026gt; { public void process(String key, Context context, Iterable\u0026lt;Double\u0026gt; averages, Collector\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; out) { Double average = averages.iterator().next(); out.collect(new Tuple2\u0026lt;\u0026gt;(key, average)); } } Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction()) // Function definitions /** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } class MyProcessWindowFunction extends ProcessWindowFunction[Double, (String, Double), String, TimeWindow] { def process(key: String, context: Context, averages: Iterable[Double], out: Collector[(String, Double)]) = { val average = averages.iterator.next() out.collect((key, average)) } } Python input = ... # type: DataStream input .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .aggregate(AverageAggregate(), window_function=MyProcessWindowFunction(), accumulator_type=Types.TUPLE([Types.LONG(), Types.LONG()]), output_type=Types.TUPLE([Types.STRING(), Types.DOUBLE()])) # Function definitions class AverageAggregate(AggregateFunction): \u0026#34;\u0026#34;\u0026#34; The accumulator is used to keep a running sum and a count. The :func:\`get_result\` method computes the average. \u0026#34;\u0026#34;\u0026#34; def create_accumulator(self) -\u0026gt; Tuple[int, int]: return 0, 0 def add(self, value: Tuple[str, int], accumulator: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return accumulator[0] + value[1], accumulator[1] + 1 def get_result(self, accumulator: Tuple[int, int]) -\u0026gt; float: return accumulator[0] / accumulator[1] def merge(self, a: Tuple[int, int], b: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return a[0] + b[0], a[1] + b[1] class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, averages: Iterable[float]) -\u0026gt; Iterable[Tuple[str, float]]: average = next(iter(averages)) yield key, average Using per-window state in ProcessWindowFunction # In addition to accessing keyed state (as any rich function can) a ProcessWindowFunction can also use keyed state that is scoped to the window that the function is currently processing. In this context it is important to understand what the window that per-window state is referring to is. There are different \u0026ldquo;windows\u0026rdquo; involved:
The window that was defined when specifying the windowed operation: This might be tumbling windows of 1 hour or sliding windows of 2 hours that slide by 1 hour. An actual instance of a defined window for a given key: This might be time window from 12:00 to 13:00 for user-id xyz. This is based on the window definition and there will be many windows based on the number of keys that the job is currently processing and based on what time slots the events fall into. Per-window state is tied to the latter of those two. Meaning that if we process events for 1000 different keys and events for all of them currently fall into the [12:00, 13:00) time window then there will be 1000 window instances that each have their own keyed per-window state.
There are two methods on the Context object that a process() invocation receives that allow access to the two types of state:
globalState(), which allows access to keyed state that is not scoped to a window windowState(), which allows access to keyed state that is also scoped to the window This feature is helpful if you anticipate multiple firing for the same window, as can happen when you have late firings for data that arrives late or when you have a custom trigger that does speculative early firings. In such a case you would store information about previous firings or the number of firings in per-window state.
When using windowed state it is important to also clean up that state when a window is cleared. This should happen in the clear() method.
WindowFunction (Legacy) # In some places where a ProcessWindowFunction can be used you can also use a WindowFunction. This is an older version of ProcessWindowFunction that provides less contextual information and does not have some advances features, such as per-window keyed state. This interface will be deprecated at some point.
The signature of a WindowFunction looks as follows:
Java public interface WindowFunction\u0026lt;IN, OUT, KEY, W extends Window\u0026gt; extends Function, Serializable { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ void apply(KEY key, W window, Iterable\u0026lt;IN\u0026gt; input, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } Scala trait WindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function with Serializable { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def apply(key: KEY, window: W, input: Iterable[IN], out: Collector[OUT]) } Python class WindowFunction(Function, Generic[IN, OUT, KEY, W]): @abstractmethod def apply(self, key: KEY, window: W, inputs: Iterable[IN]) -\u0026gt; Iterable[OUT]: \u0026#34;\u0026#34;\u0026#34; Evaluates the window and outputs none or several elements. :param key: The key for which this window is evaluated. :param window: The window that is being evaluated. :param inputs: The elements in the window being evaluated. \u0026#34;\u0026#34;\u0026#34; pass It can be used like this:
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .apply(new MyWindowFunction()); Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .apply(new MyWindowFunction()) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .apply(MyWindowFunction()) Triggers # A Trigger determines when a window (as formed by the window assigner) is ready to be processed by the window function. Each WindowAssigner comes with a default Trigger. If the default trigger does not fit your needs, you can specify a custom trigger using trigger(...).
The trigger interface has five methods that allow a Trigger to react to different events:
The onElement() method is called for each element that is added to a window. The onEventTime() method is called when a registered event-time timer fires. The onProcessingTime() method is called when a registered processing-time timer fires. The onMerge() method is relevant for stateful triggers and merges the states of two triggers when their corresponding windows merge, e.g. when using session windows. Finally the clear() method performs any action needed upon removal of the corresponding window. Two things to notice about the above methods are:
The first three decide how to act on their invocation event by returning a TriggerResult. The action can be one of the following: CONTINUE: do nothing, FIRE: trigger the computation, PURGE: clear the elements in the window, and FIRE_AND_PURGE: trigger the computation and clear the elements in the window afterwards. Any of these methods can be used to register processing- or event-time timers for future actions. Fire and Purge # Once a trigger determines that a window is ready for processing, it fires, i.e., it returns FIRE or FIRE_AND_PURGE. This is the signal for the window operator to emit the result of the current window. Given a window with a ProcessWindowFunction all elements are passed to the ProcessWindowFunction (possibly after passing them to an evictor). Windows with ReduceFunction, or AggregateFunction simply emit their eagerly aggregated result.
When a trigger fires, it can either FIRE or FIRE_AND_PURGE. While FIRE keeps the contents of the window, FIRE_AND_PURGE removes its content. By default, the pre-implemented triggers simply FIRE without purging the window state.
Purging will simply remove the contents of the window and will leave any potential meta-information about the window and any trigger state intact. Default Triggers of WindowAssigners # The default Trigger of a WindowAssigner is appropriate for many use cases. For example, all the event-time window assigners have an EventTimeTrigger as default trigger. This trigger simply fires once the watermark passes the end of a window.
The default trigger of the GlobalWindow is the NeverTrigger which does never fire. Consequently, you always have to define a custom trigger when using a GlobalWindow.
By specifying a trigger using trigger() you are overwriting the default trigger of a WindowAssigner. For example, if you specify a CountTrigger for TumblingEventTimeWindows you will no longer get window firings based on the progress of time but only by count. Right now, you have to write your own custom trigger if you want to react based on both time and count. Built-in and Custom Triggers # Flink comes with a few built-in triggers.
The (already mentioned) EventTimeTrigger fires based on the progress of event-time as measured by watermarks. The ProcessingTimeTrigger fires based on processing time. The CountTrigger fires once the number of elements in a window exceeds the given limit. The PurgingTrigger takes as argument another trigger and transforms it into a purging one. If you need to implement a custom trigger, you should check out the abstract Trigger class. Please note that the API is still evolving and might change in future versions of Flink.
Evictors # Flink’s windowing model allows specifying an optional Evictor in addition to the WindowAssigner and the Trigger. This can be done using the evictor(...) method (shown in the beginning of this document). The evictor has the ability to remove elements from a window after the trigger fires and before and/or after the window function is applied. To do so, the Evictor interface has two methods:
/** * Optionally evicts elements. Called before windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictBefore(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); /** * Optionally evicts elements. Called after windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictAfter(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); The evictBefore() contains the eviction logic to be applied before the window function, while the evictAfter() contains the one to be applied after the window function. Elements evicted before the application of the window function will not be processed by it.
Flink comes with three pre-implemented evictors. These are:
CountEvictor: keeps up to a user-specified number of elements from the window and discards the remaining ones from the beginning of the window buffer. DeltaEvictor: takes a DeltaFunction and a threshold, computes the delta between the last element in the window buffer and each of the remaining ones, and removes the ones with a delta greater or equal to the threshold. TimeEvictor: takes as argument an interval in milliseconds and for a given window, it finds the maximum timestamp max_ts among its elements and removes all the elements with timestamps smaller than max_ts - interval. By default, all the pre-implemented evictors apply their logic before the window function.
Specifying an evictor prevents any pre-aggregation, as all the elements of a window have to be passed to the evictor before applying the computation. This means windows with evictors will create significantly more state. Note: Evictor is still not supported in Python DataStream API. Flink provides no guarantees about the order of the elements within a window. This implies that although an evictor may remove elements from the beginning of the window, these are not necessarily the ones that arrive first or last.
Allowed Lateness # When working with event-time windowing, it can happen that elements arrive late, i.e. the watermark that Flink uses to keep track of the progress of event-time is already past the end timestamp of a window to which an element belongs. See event time and especially late elements for a more thorough discussion of how Flink deals with event time.
By default, late elements are dropped when the watermark is past the end of the window. However, Flink allows to specify a maximum allowed lateness for window operators. Allowed lateness specifies by how much time elements can be late before they are dropped, and its default value is 0. Elements that arrive after the watermark has passed the end of the window but before it passes the end of the window plus the allowed lateness, are still added to the window. Depending on the trigger used, a late but not dropped element may cause the window to fire again. This is the case for the EventTimeTrigger.
In order to make this work, Flink keeps the state of windows until their allowed lateness expires. Once this happens, Flink removes the window and deletes its state, as also described in the Window Lifecycle section.
By default, the allowed lateness is set to 0. That is, elements that arrive behind the watermark will be dropped.
You can specify an allowed lateness like this:
Java DataStream\u0026lt;T\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .allowed_lateness(\u0026lt;time\u0026gt;) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) When using the GlobalWindows window assigner no data is ever considered late because the end timestamp of the global window is Long.MAX_VALUE. Getting late data as a side output # Using Flink\u0026rsquo;s side output feature you can get a stream of the data that was discarded as late.
You first need to specify that you want to get late data using sideOutputLateData(OutputTag) on the windowed stream. Then, you can get the side-output stream on the result of the windowed operation:
Java final OutputTag\u0026lt;T\u0026gt; lateOutputTag = new OutputTag\u0026lt;T\u0026gt;(\u0026#34;late-data\u0026#34;){}; DataStream\u0026lt;T\u0026gt; input = ...; SingleOutputStreamOperator\u0026lt;T\u0026gt; result = input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .sideOutputLateData(lateOutputTag) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); DataStream\u0026lt;T\u0026gt; lateStream = result.getSideOutput(lateOutputTag); Scala val lateOutputTag = OutputTag[T](\u0026#34;late-data\u0026#34;) val input: DataStream[T] = ... val result = input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .sideOutputLateData(lateOutputTag) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) val lateStream = result.getSideOutput(lateOutputTag) Python late_output_tag = OutputTag(\u0026#34;late-data\u0026#34;, type_info) input = ... # type: DataStream result = input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .allowed_lateness(\u0026lt;time\u0026gt;) \\ .side_output_late_data(late_output_tag) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) late_stream = result.get_side_output(late_output_tag) Late elements considerations # When specifying an allowed lateness greater than 0, the window along with its content is kept after the watermark passes the end of the window. In these cases, when a late but not dropped element arrives, it could trigger another firing for the window. These firings are called late firings, as they are triggered by late events and in contrast to the main firing which is the first firing of the window. In case of session windows, late firings can further lead to merging of windows, as they may \u0026ldquo;bridge\u0026rdquo; the gap between two pre-existing, unmerged windows.
The elements emitted by a late firing should be treated as updated results of a previous computation, i.e., your data stream will contain multiple results for the same computation. Depending on your application, you need to take these duplicated results into account or deduplicate them. Working with window results # The result of a windowed operation is again a DataStream, no information about the windowed operations is retained in the result elements so if you want to keep meta-information about the window you have to manually encode that information in the result elements in your ProcessWindowFunction. The only relevant information that is set on the result elements is the element timestamp. This is set to the maximum allowed timestamp of the processed window, which is end timestamp - 1, since the window-end timestamp is exclusive. Note that this is true for both event-time windows and processing-time windows. i.e. after a windowed operations elements always have a timestamp, but this can be an event-time timestamp or a processing-time timestamp. For processing-time windows this has no special implications but for event-time windows this together with how watermarks interact with windows enables consecutive windowed operations with the same window sizes. We will cover this after taking a look how watermarks interact with windows.
Interaction of watermarks and windows # Before continuing in this section you might want to take a look at our section about event time and watermarks.
When watermarks arrive at the window operator this triggers two things:
the watermark triggers computation of all windows where the maximum timestamp (which is end-timestamp - 1) is smaller than the new watermark the watermark is forwarded (as is) to downstream operations Intuitively, a watermark \u0026ldquo;flushes\u0026rdquo; out any windows that would be considered late in downstream operations once they receive that watermark.
Consecutive windowed operations # As mentioned before, the way the timestamp of windowed results is computed and how watermarks interact with windows allows stringing together consecutive windowed operations. This can be useful when you want to do two consecutive windowed operations where you want to use different keys but still want elements from the same upstream window to end up in the same downstream window. Consider this example:
Java DataStream\u0026lt;Integer\u0026gt; input = ...; DataStream\u0026lt;Integer\u0026gt; resultsPerKey = input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new Summer()); DataStream\u0026lt;Integer\u0026gt; globalResults = resultsPerKey .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new TopKWindowFunction()); Scala val input: DataStream[Int] = ... val resultsPerKey = input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new Summer()) val globalResults = resultsPerKey .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new TopKWindowFunction()) Python input = ... # type: DataStream results_per_key = input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(Summer()) global_results = results_per_key \\ .window_all(TumblingProcessingTimeWindows.of(Time.seconds(5))) \\ .process(TopKWindowFunction()) In this example, the results for time window [0, 5) from the first operation will also end up in time window [0, 5) in the subsequent windowed operation. This allows calculating a sum per key and then calculating the top-k elements within the same window in the second operation.
Useful state size considerations # Windows can be defined over long periods of time (such as days, weeks, or months) and therefore accumulate very large state. There are a couple of rules to keep in mind when estimating the storage requirements of your windowing computation:
Flink creates one copy of each element per window to which it belongs. Given this, tumbling windows keep one copy of each element (an element belongs to exactly one window unless it is dropped late). In contrast, sliding windows create several of each element, as explained in the Window Assigners section. Hence, a sliding window of size 1 day and slide 1 second might not be a good idea.
ReduceFunction and AggregateFunction can significantly reduce the storage requirements, as they eagerly aggregate elements and store only one value per window. In contrast, just using a ProcessWindowFunction requires accumulating all elements.
Using an Evictor prevents any pre-aggregation, as all the elements of a window have to be passed through the evictor before applying the computation (see Evictors).
Back to top
`}),e.add({id:65,href:"/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/",title:"Windows",section:"Operators",content:" "}),e.add({id:66,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/",title:"Working with State",section:"State \u0026 Fault Tolerance",content:` Working with State # In this section you will learn about the APIs that Flink provides for writing stateful programs. Please take a look at Stateful Stream Processing to learn about the concepts behind stateful stream processing.
Keyed DataStream # If you want to use keyed state, you first need to specify a key on a DataStream that should be used to partition the state (and also the records in the stream themselves). You can specify a key using keyBy(KeySelector) in Java/Scala API or key_by(KeySelector) in Python API on a DataStream. This will yield a KeyedStream, which then allows operations that use keyed state.
A key selector function takes a single record as input and returns the key for that record. The key can be of any type and must be derived from deterministic computations.
The data model of Flink is not based on key-value pairs. Therefore, you do not need to physically pack the data set types into keys and values. Keys are \u0026ldquo;virtual\u0026rdquo;: they are defined as functions over the actual data to guide the grouping operator.
The following example shows a key selector function that simply returns the field of an object:
Java // some ordinary POJO public class WC { public String word; public int count; public String getWord() { return word; } } DataStream\u0026lt;WC\u0026gt; words = // [...] KeyedStream\u0026lt;WC\u0026gt; keyed = words .keyBy(WC::getWord); Scala // some ordinary case class case class WC(word: String, count: Int) val words: DataStream[WC] = // [...] val keyed = words.keyBy( _.word ) Python words = # type: DataStream[Row] keyed = words.key_by(lambda row: row[0]) Tuple Keys and Expression Keys # Flink also has two alternative ways of defining keys: tuple keys and expression keys in the Java/Scala API(still not supported in the Python API). With this you can specify keys using tuple field indices or expressions for selecting fields of objects. We don\u0026rsquo;t recommend using these today but you can refer to the Javadoc of DataStream to learn about them. Using a KeySelector function is strictly superior: with Java lambdas they are easy to use and they have potentially less overhead at runtime.
Back to top
Using Keyed State # The keyed state interfaces provides access to different types of state that are all scoped to the key of the current input element. This means that this type of state can only be used on a KeyedStream, which can be created via stream.keyBy(…) in Java/Scala API or stream.key_by(…) in Python API.
Now, we will first look at the different types of state available and then we will see how they can be used in a program. The available state primitives are:
ValueState\u0026lt;T\u0026gt;: This keeps a value that can be updated and retrieved (scoped to key of the input element as mentioned above, so there will possibly be one value for each key that the operation sees). The value can be set using update(T) and retrieved using T value().
ListState\u0026lt;T\u0026gt;: This keeps a list of elements. You can append elements and retrieve an Iterable over all currently stored elements. Elements are added using add(T) or addAll(List\u0026lt;T\u0026gt;), the Iterable can be retrieved using Iterable\u0026lt;T\u0026gt; get(). You can also override the existing list with update(List\u0026lt;T\u0026gt;)
ReducingState\u0026lt;T\u0026gt;: This keeps a single value that represents the aggregation of all values added to the state. The interface is similar to ListState but elements added using add(T) are reduced to an aggregate using a specified ReduceFunction.
AggregatingState\u0026lt;IN, OUT\u0026gt;: This keeps a single value that represents the aggregation of all values added to the state. Contrary to ReducingState, the aggregate type may be different from the type of elements that are added to the state. The interface is the same as for ListState but elements added using add(IN) are aggregated using a specified AggregateFunction.
MapState\u0026lt;UK, UV\u0026gt;: This keeps a list of mappings. You can put key-value pairs into the state and retrieve an Iterable over all currently stored mappings. Mappings are added using put(UK, UV) or putAll(Map\u0026lt;UK, UV\u0026gt;). The value associated with a user key can be retrieved using get(UK). The iterable views for mappings, keys and values can be retrieved using entries(), keys() and values() respectively. You can also use isEmpty() to check whether this map contains any key-value mappings.
All types of state also have a method clear() that clears the state for the currently active key, i.e. the key of the input element.
It is important to keep in mind that these state objects are only used for interfacing with state. The state is not necessarily stored inside but might reside on disk or somewhere else. The second thing to keep in mind is that the value you get from the state depends on the key of the input element. So the value you get in one invocation of your user function can differ from the value in another invocation if the keys involved are different.
To get a state handle, you have to create a StateDescriptor. This holds the name of the state (as we will see later, you can create several states, and they have to have unique names so that you can reference them), the type of the values that the state holds, and possibly a user-specified function, such as a ReduceFunction. Depending on what type of state you want to retrieve, you create either a ValueStateDescriptor, a ListStateDescriptor, an AggregatingStateDescriptor, a ReducingStateDescriptor, or a MapStateDescriptor.
State is accessed using the RuntimeContext, so it is only possible in rich functions. Please see here for information about that, but we will also see an example shortly. The RuntimeContext that is available in a RichFunction has these methods for accessing state:
ValueState\u0026lt;T\u0026gt; getState(ValueStateDescriptor\u0026lt;T\u0026gt;) ReducingState\u0026lt;T\u0026gt; getReducingState(ReducingStateDescriptor\u0026lt;T\u0026gt;) ListState\u0026lt;T\u0026gt; getListState(ListStateDescriptor\u0026lt;T\u0026gt;) AggregatingState\u0026lt;IN, OUT\u0026gt; getAggregatingState(AggregatingStateDescriptor\u0026lt;IN, ACC, OUT\u0026gt;) MapState\u0026lt;UK, UV\u0026gt; getMapState(MapStateDescriptor\u0026lt;UK, UV\u0026gt;) This is an example FlatMapFunction that shows how all of the parts fit together:
Java public class CountWindowAverage extends RichFlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; sum; @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; input, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) throws Exception { // access the state value Tuple2\u0026lt;Long, Long\u0026gt; currentSum = sum.value(); // update the count currentSum.f0 += 1; // add the second field of the input value currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 \u0026gt;= 2) { out.collect(new Tuple2\u0026lt;\u0026gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {}), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); } } // this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env) env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(value -\u0026gt; value.f0) .flatMap(new CountWindowAverage()) .print(); // the printed output will be (1,4) and (1,5) Scala class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] { private var sum: ValueState[(Long, Long)] = _ override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = { // access the state value val tmpCurrentSum = sum.value // If it hasn\u0026#39;t been used before, it will be null val currentSum = if (tmpCurrentSum != null) { tmpCurrentSum } else { (0L, 0L) } // update the count val newSum = (currentSum._1 + 1, currentSum._2 + input._2) // update the state sum.update(newSum) // if the count reaches 2, emit the average and clear the state if (newSum._1 \u0026gt;= 2) { out.collect((input._1, newSum._2 / newSum._1)) sum.clear() } } override def open(parameters: Configuration): Unit = { sum = getRuntimeContext.getState( new ValueStateDescriptor[(Long, Long)](\u0026#34;average\u0026#34;, createTypeInformation[(Long, Long)]) ) } } object ExampleCountWindowAverage extends App { val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromCollection(List( (1L, 3L), (1L, 5L), (1L, 7L), (1L, 4L), (1L, 2L) )).keyBy(_._1) .flatMap(new CountWindowAverage()) .print() // the printed output will be (1,4) and (1,5) env.execute(\u0026#34;ExampleKeyedState\u0026#34;) } Python from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment, FlatMapFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor class CountWindowAverage(FlatMapFunction): def __init__(self): self.sum = None def open(self, runtime_context: RuntimeContext): descriptor = ValueStateDescriptor( \u0026#34;average\u0026#34;, # the state name Types.PICKLED_BYTE_ARRAY() # type information ) self.sum = runtime_context.get_state(descriptor) def flat_map(self, value): # access the state value current_sum = self.sum.value() if current_sum is None: current_sum = (0, 0) # update the count current_sum = (current_sum[0] + 1, current_sum[1] + value[1]) # update the state self.sum.update(current_sum) # if the count reaches 2, emit the average and clear the state if current_sum[0] \u0026gt;= 2: self.sum.clear() yield value[0], int(current_sum[1] / current_sum[0]) env = StreamExecutionEnvironment.get_execution_environment() env.from_collection([(1, 3), (1, 5), (1, 7), (1, 4), (1, 2)]) \\ .key_by(lambda row: row[0]) \\ .flat_map(CountWindowAverage()) \\ .print() env.execute() # the printed output will be (1,4) and (1,5) This example implements a poor man\u0026rsquo;s counting window. We key the tuples by the first field (in the example all have the same key 1). The function stores the count and a running sum in a ValueState. Once the count reaches 2 it will emit the average and clear the state so that we start over from 0. Note that this would keep a different state value for each different input key if we had tuples with different values in the first field.
State Time-To-Live (TTL) # A time-to-live (TTL) can be assigned to the keyed state of any type. If a TTL is configured and a state value has expired, the stored value will be cleaned up on a best effort basis which is discussed in more detail below.
All state collection types support per-entry TTLs. This means that list elements and map entries expire independently.
In order to use state TTL one must first build a StateTtlConfig configuration object. The TTL functionality can then be enabled in any state descriptor by passing the configuration:
Java import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); ValueStateDescriptor\u0026lt;String\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;text state\u0026#34;, String.class); stateDescriptor.enableTimeToLive(ttlConfig); Scala import org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build val stateDescriptor = new ValueStateDescriptor[String](\u0026#34;text state\u0026#34;, classOf[String]) stateDescriptor.enableTimeToLive(ttlConfig) Python from pyflink.common.time import Time from pyflink.common.typeinfo import Types from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite) \\ .set_state_visibility(StateTtlConfig.StateVisibility.NeverReturnExpired) \\ .build() state_descriptor = ValueStateDescriptor(\u0026#34;text state\u0026#34;, Types.STRING()) state_descriptor.enable_time_to_live(ttl_config) The configuration has several options to consider:
The first parameter of the newBuilder method is mandatory, it is the time-to-live value.
The update type configures when the state TTL is refreshed (by default OnCreateAndWrite):
StateTtlConfig.UpdateType.OnCreateAndWrite - only on creation and write access
StateTtlConfig.UpdateType.OnReadAndWrite - also on read access
(Notes: If you set the state visibility to StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp at the same time, the state read cache will be disabled, which will cause some performance loss in PyFlink)
The state visibility configures whether the expired value is returned on read access if it is not cleaned up yet (by default NeverReturnExpired):
StateTtlConfig.StateVisibility.NeverReturnExpired - expired value is never returned
(Notes: The state read/write cache will be disabled, which will cause some performance loss in PyFlink)
StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp - returned if still available
In case of NeverReturnExpired, the expired state behaves as if it does not exist anymore, even if it still has to be removed. The option can be useful for use cases where data has to become unavailable for read access strictly after TTL, e.g. application working with privacy sensitive data.
Another option ReturnExpiredIfNotCleanedUp allows to return the expired state before its cleanup.
Notes:
The state backends store the timestamp of the last modification along with the user value, which means that enabling this feature increases consumption of state storage. Heap state backend stores an additional Java object with a reference to the user state object and a primitive long value in memory. The RocksDB state backend adds 8 bytes per stored value, list entry or map entry.
Only TTLs in reference to processing time are currently supported.
Trying to restore state, which was previously configured without TTL, using TTL enabled descriptor or vice versa will lead to compatibility failure and StateMigrationException.
The TTL configuration is not part of check- or savepoints but rather a way of how Flink treats it in the currently running job.
The map state with TTL currently supports null user values only if the user value serializer can handle null values. If the serializer does not support null values, it can be wrapped with NullableSerializer at the cost of an extra byte in the serialized form.
With TTL enabled configuration, the defaultValue in StateDescriptor, which is actually already deprecated, will no longer take an effect. This aims to make the semantics more clear and let user manually manage the default value if the contents of the state is null or expired.
Cleanup of Expired State # By default, expired values are explicitly removed on read, such as ValueState#value, and periodically garbage collected in the background if supported by the configured state backend. Background cleanup can be disabled in the StateTtlConfig:
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground() .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .disable_cleanup_in_background() \\ .build() For more fine-grained control over some special cleanup in background, you can configure it separately as described below. Currently, heap state backend relies on incremental cleanup and RocksDB backend uses compaction filter for background cleanup.
Cleanup in full snapshot # Additionally, you can activate the cleanup at the moment of taking the full state snapshot which will reduce its size. The local state is not cleaned up under the current implementation but it will not include the removed expired state in case of restoration from the previous snapshot. It can be configured in StateTtlConfig:
Java import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_full_snapshot() \\ .build() This option is not applicable for the incremental checkpointing in the RocksDB state backend.
For existing jobs, this cleanup strategy can be activated or deactivated anytime in StateTtlConfig, e.g. after restart from savepoint. Incremental cleanup # Another option is to trigger cleanup of some state entries incrementally. The trigger can be a callback from each state access or/and each record processing. If this cleanup strategy is active for certain state, The storage backend keeps a lazy global iterator for this state over all its entries. Every time incremental cleanup is triggered, the iterator is advanced. The traversed state entries are checked and expired ones are cleaned up.
This feature can be configured in StateTtlConfig:
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_incrementally(10, True) \\ .build() This strategy has two parameters. The first one is number of checked state entries per each cleanup triggering. It is always triggered per each state access. The second parameter defines whether to trigger cleanup additionally per each record processing. The default background cleanup for heap backend checks 5 entries without cleanup per record processing.
Notes:
If no access happens to the state or no records are processed, expired state will persist. Time spent for the incremental cleanup increases record processing latency. At the moment incremental cleanup is implemented only for Heap state backend. Setting it for RocksDB will have no effect. If heap state backend is used with synchronous snapshotting, the global iterator keeps a copy of all keys while iterating because of its specific implementation which does not support concurrent modifications. Enabling of this feature will increase memory consumption then. Asynchronous snapshotting does not have this problem. For existing jobs, this cleanup strategy can be activated or deactivated anytime in StateTtlConfig, e.g. after restart from savepoint. Cleanup during RocksDB compaction # If the RocksDB state backend is used, a Flink specific compaction filter will be called for the background cleanup. RocksDB periodically runs asynchronous compactions to merge state updates and reduce storage. Flink compaction filter checks expiration timestamp of state entries with TTL and excludes expired values.
This feature can be configured in StateTtlConfig:
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_in_rocksdb_compact_filter(1000) \\ .build() RocksDB compaction filter will query current timestamp, used to check expiration, from Flink every time after processing certain number of state entries. You can change it and pass a custom value to StateTtlConfig.newBuilder(...).cleanupInRocksdbCompactFilter(long queryTimeAfterNumEntries) method. Updating the timestamp more often can improve cleanup speed but it decreases compaction performance because it uses JNI call from native code. The default background cleanup for RocksDB backend queries the current timestamp each time 1000 entries have been processed.
You can activate debug logs from the native code of RocksDB filter by activating debug level for FlinkCompactionFilter:
log4j.logger.org.rocksdb.FlinkCompactionFilter=DEBUG
Notes:
Calling of TTL filter during compaction slows it down. The TTL filter has to parse timestamp of last access and check its expiration for every stored state entry per key which is being compacted. In case of collection state type (list or map) the check is also invoked per stored element. If this feature is used with a list state which has elements with non-fixed byte length, the native TTL filter has to call additionally a Flink java type serializer of the element over JNI per each state entry where at least the first element has expired to determine the offset of the next unexpired element. For existing jobs, this cleanup strategy can be activated or deactivated anytime in StateTtlConfig, e.g. after restart from savepoint. State in the Scala DataStream API # In addition to the interface described above, the Scala API has shortcuts for stateful map() or flatMap() functions with a single ValueState on KeyedStream. The user function gets the current value of the ValueState in an Option and must return an updated value that will be used to update the state.
val stream: DataStream[(String, Int)] = ... val counts: DataStream[(String, Int)] = stream .keyBy(_._1) .mapWithState((in: (String, Int), count: Option[Int]) =\u0026gt; count match { case Some(c) =\u0026gt; ( (in._1, c), Some(c + in._2) ) case None =\u0026gt; ( (in._1, 0), Some(in._2) ) }) Operator State # Operator State (or non-keyed state) is state that is bound to one parallel operator instance. The Kafka Connector is a good motivating example for the use of Operator State in Flink. Each parallel instance of the Kafka consumer maintains a map of topic partitions and offsets as its Operator State.
The Operator State interfaces support redistributing state among parallel operator instances when the parallelism is changed. There are different schemes for doing this redistribution.
In a typical stateful Flink Application you don\u0026rsquo;t need operators state. It is mostly a special type of state that is used in source/sink implementations and scenarios where you don\u0026rsquo;t have a key by which state can be partitioned.
Notes: Operator state is still not supported in Python DataStream API.
Broadcast State # Broadcast State is a special type of Operator State. It was introduced to support use cases where records of one stream need to be broadcasted to all downstream tasks, where they are used to maintain the same state among all subtasks. This state can then be accessed while processing records of a second stream. As an example where broadcast state can emerge as a natural fit, one can imagine a low-throughput stream containing a set of rules which we want to evaluate against all elements coming from another stream. Having the above type of use cases in mind, broadcast state differs from the rest of operator states in that:
it has a map format, it is only available to specific operators that have as inputs a broadcasted stream and a non-broadcasted one, and such an operator can have multiple broadcast states with different names. Notes: Broadcast state is still not supported in Python DataStream API.
Back to top
Using Operator State # To use operator state, a stateful function can implement the CheckpointedFunction interface.
CheckpointedFunction # The CheckpointedFunction interface provides access to non-keyed state with different redistribution schemes. It requires the implementation of two methods:
void snapshotState(FunctionSnapshotContext context) throws Exception; void initializeState(FunctionInitializationContext context) throws Exception; Whenever a checkpoint has to be performed, snapshotState() is called. The counterpart, initializeState(), is called every time the user-defined function is initialized, be that when the function is first initialized or be that when the function is actually recovering from an earlier checkpoint. Given this, initializeState() is not only the place where different types of state are initialized, but also where state recovery logic is included.
Currently, list-style operator state is supported. The state is expected to be a List of serializable objects, independent from each other, thus eligible for redistribution upon rescaling. In other words, these objects are the finest granularity at which non-keyed state can be redistributed. Depending on the state accessing method, the following redistribution schemes are defined:
Even-split redistribution: Each operator returns a List of state elements. The whole state is logically a concatenation of all lists. On restore/redistribution, the list is evenly divided into as many sublists as there are parallel operators. Each operator gets a sublist, which can be empty, or contain one or more elements. As an example, if with parallelism 1 the checkpointed state of an operator contains elements element1 and element2, when increasing the parallelism to 2, element1 may end up in operator instance 0, while element2 will go to operator instance 1.
Union redistribution: Each operator returns a List of state elements. The whole state is logically a concatenation of all lists. On restore/redistribution, each operator gets the complete list of state elements. Do not use this feature if your list may have high cardinality. Checkpoint metadata will store an offset to each list entry, which could lead to RPC framesize or out-of-memory errors.
Below is an example of a stateful SinkFunction that uses CheckpointedFunction to buffer elements before sending them to the outside world. It demonstrates the basic even-split redistribution list state:
Java public class BufferingSink implements SinkFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;, CheckpointedFunction { private final int threshold; private transient ListState\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; checkpointedState; private List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; bufferedElements; public BufferingSink(int threshold) { this.threshold = threshold; this.bufferedElements = new ArrayList\u0026lt;\u0026gt;(); } @Override public void invoke(Tuple2\u0026lt;String, Integer\u0026gt; value, Context contex) throws Exception { bufferedElements.add(value); if (bufferedElements.size() \u0026gt;= threshold) { for (Tuple2\u0026lt;String, Integer\u0026gt; element: bufferedElements) { // send it to the sink } bufferedElements.clear(); } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { checkpointedState.clear(); for (Tuple2\u0026lt;String, Integer\u0026gt; element : bufferedElements) { checkpointedState.add(element); } } @Override public void initializeState(FunctionInitializationContext context) throws Exception { ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() {})); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) { for (Tuple2\u0026lt;String, Integer\u0026gt; element : checkpointedState.get()) { bufferedElements.add(element); } } } } Scala class BufferingSink(threshold: Int = 0) extends SinkFunction[(String, Int)] with CheckpointedFunction { @transient private var checkpointedState: ListState[(String, Int)] = _ private val bufferedElements = ListBuffer[(String, Int)]() override def invoke(value: (String, Int), context: Context): Unit = { bufferedElements += value if (bufferedElements.size \u0026gt;= threshold) { for (element \u0026lt;- bufferedElements) { // send it to the sink } bufferedElements.clear() } } override def snapshotState(context: FunctionSnapshotContext): Unit = { checkpointedState.clear() for (element \u0026lt;- bufferedElements) { checkpointedState.add(element) } } override def initializeState(context: FunctionInitializationContext): Unit = { val descriptor = new ListStateDescriptor[(String, Int)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Int)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) if(context.isRestored) { for(element \u0026lt;- checkpointedState.get().asScala) { bufferedElements += element } } } } The initializeState method takes as argument a FunctionInitializationContext. This is used to initialize the non-keyed state \u0026ldquo;containers\u0026rdquo;. These are a container of type ListState where the non-keyed state objects are going to be stored upon checkpointing.
Note how the state is initialized, similar to keyed state, with a StateDescriptor that contains the state name and information about the type of the value that the state holds:
Java ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() {})); checkpointedState = context.getOperatorStateStore().getListState(descriptor); Scala val descriptor = new ListStateDescriptor[(String, Long)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Long)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) The naming convention of the state access methods contain its redistribution pattern followed by its state structure. For example, to use list state with the union redistribution scheme on restore, access the state by using getUnionListState(descriptor). If the method name does not contain the redistribution pattern, e.g. getListState(descriptor), it simply implies that the basic even-split redistribution scheme will be used.
After initializing the container, we use the isRestored() method of the context to check if we are recovering after a failure. If this is true, i.e. we are recovering, the restore logic is applied.
As shown in the code of the modified BufferingSink, this ListState recovered during state initialization is kept in a class variable for future use in snapshotState(). There the ListState is cleared of all objects included by the previous checkpoint, and is then filled with the new ones we want to checkpoint.
As a side note, the keyed state can also be initialized in the initializeState() method. This can be done using the provided FunctionInitializationContext.
Stateful Source Functions # Stateful sources require a bit more care as opposed to other operators. In order to make the updates to the state and output collection atomic (required for exactly-once semantics on failure/recovery), the user is required to get a lock from the source\u0026rsquo;s context.
Java public static class CounterSource extends RichParallelSourceFunction\u0026lt;Long\u0026gt; implements CheckpointedFunction { /** current offset for exactly once semantics */ private Long offset = 0L; /** flag for job cancellation */ private volatile boolean isRunning = true; /** Our state object. */ private ListState\u0026lt;Long\u0026gt; state; @Override public void run(SourceContext\u0026lt;Long\u0026gt; ctx) { final Object lock = ctx.getCheckpointLock(); while (isRunning) { // output and state update are atomic synchronized (lock) { ctx.collect(offset); offset += 1; } } } @Override public void cancel() { isRunning = false; } @Override public void initializeState(FunctionInitializationContext context) throws Exception { state = context.getOperatorStateStore().getListState(new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state\u0026#34;, LongSerializer.INSTANCE)); // restore any state that we might already have to our fields, initialize state // is also called in case of restore. for (Long l : state.get()) { offset = l; } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { state.clear(); state.add(offset); } } Scala class CounterSource extends RichParallelSourceFunction[Long] with CheckpointedFunction { @volatile private var isRunning = true private var offset = 0L private var state: ListState[Long] = _ override def run(ctx: SourceFunction.SourceContext[Long]): Unit = { val lock = ctx.getCheckpointLock while (isRunning) { // output and state update are atomic lock.synchronized({ ctx.collect(offset) offset += 1 }) } } override def cancel(): Unit = isRunning = false override def initializeState(context: FunctionInitializationContext): Unit = { state = context.getOperatorStateStore.getListState( new ListStateDescriptor[Long](\u0026#34;state\u0026#34;, classOf[Long])) for (l \u0026lt;- state.get().asScala) { offset = l } } override def snapshotState(context: FunctionSnapshotContext): Unit = { state.clear() state.add(offset) } } Some operators might need the information when a checkpoint is fully acknowledged by Flink to communicate that with the outside world. In this case see the org.apache.flink.api.common.state.CheckpointListener interface.
Back to top
`}),e.add({id:67,href:"/flink/flink-docs-master/docs/deployment/ha/zookeeper_ha/",title:"ZooKeeper HA Services",section:"High Availability",content:` ZooKeeper HA Services # Flink\u0026rsquo;s ZooKeeper HA services use ZooKeeper for high availability services.
Flink leverages ZooKeeper for distributed coordination between all running JobManager instances. ZooKeeper is a separate service from Flink, which provides highly reliable distributed coordination via leader election and light-weight consistent state storage. Check out ZooKeeper\u0026rsquo;s Getting Started Guide for more information about ZooKeeper. Flink includes scripts to bootstrap a simple ZooKeeper installation.
Configuration # In order to start an HA-cluster you have to configure the following configuration keys:
high-availability (required): The high-availability option has to be set to zookeeper.
high-availability: zookeeper high-availability.storageDir (required): JobManager metadata is persisted in the file system high-availability.storageDir and only a pointer to this state is stored in ZooKeeper.
high-availability.storageDir: hdfs:///flink/recovery The storageDir stores all metadata needed to recover a JobManager failure.
high-availability.zookeeper.quorum (required): A ZooKeeper quorum is a replicated group of ZooKeeper servers, which provide the distributed coordination service.
high-availability.zookeeper.quorum: address1:2181[,...],addressX:2181 Each addressX:port refers to a ZooKeeper server, which is reachable by Flink at the given address and port.
high-availability.zookeeper.path.root (recommended): The root ZooKeeper node, under which all cluster nodes are placed.
high-availability.zookeeper.path.root: /flink high-availability.cluster-id (recommended): The cluster-id ZooKeeper node, under which all required coordination data for a cluster is placed.
high-availability.cluster-id: /default_ns # important: customize per cluster Important: You should not set this value manually when running on YARN, native Kubernetes or on another cluster manager. In those cases a cluster-id is being automatically generated. If you are running multiple Flink HA clusters on bare metal, you have to manually configure separate cluster-ids for each cluster.
Example configuration # Configure high availability mode and ZooKeeper quorum in conf/flink-conf.yaml:
high-availability: zookeeper high-availability.zookeeper.quorum: localhost:2181 high-availability.zookeeper.path.root: /flink high-availability.cluster-id: /cluster_one # important: customize per cluster high-availability.storageDir: hdfs:///flink/recovery Back to top
Configuring for ZooKeeper Security # If ZooKeeper is running in secure mode with Kerberos, you can override the following configurations in flink-conf.yaml as necessary:
# default is \u0026#34;zookeeper\u0026#34;. If the ZooKeeper quorum is configured # with a different service name then it can be supplied here. zookeeper.sasl.service-name: zookeeper # default is \u0026#34;Client\u0026#34;. The value needs to match one of the values # configured in \u0026#34;security.kerberos.login.contexts\u0026#34;. zookeeper.sasl.login-context-name: Client For more information on Flink configuration for Kerberos security, please refer to the security section of the Flink configuration page. You can also find further details on how Flink sets up Kerberos-based security internally.
Back to top
Advanced Configuration # Tolerating Suspended ZooKeeper Connections # Per default, Flink\u0026rsquo;s ZooKeeper client treats suspended ZooKeeper connections as an error. This means that Flink will invalidate all leaderships of its components and thereby triggering a failover if a connection is suspended.
This behaviour might be too disruptive in some cases (e.g., unstable network environment). If you are willing to take a more aggressive approach, then you can tolerate suspended ZooKeeper connections and only treat lost connections as an error via high-availability.zookeeper.client.tolerate-suspended-connections. Enabling this feature will make Flink more resilient against temporary connection problems but also increase the risk of running into ZooKeeper timing problems.
For more information take a look at Curator\u0026rsquo;s error handling.
ZooKeeper Versions # Flink ships with separate ZooKeeper clients for 3.4 and 3.5, with 3.4 being in the lib directory of the distribution and thus used by default, whereas 3.5 is placed in the opt directory.
The 3.5 client allows you to secure the ZooKeeper connection via SSL, but may not work with 3.4- ZooKeeper installations.
You can control which version is used by Flink by placing either jar in the lib directory.
Back to top
Bootstrap ZooKeeper # If you don\u0026rsquo;t have a running ZooKeeper installation, you can use the helper scripts, which ship with Flink.
There is a ZooKeeper configuration template in conf/zoo.cfg. You can configure the hosts to run ZooKeeper on with the server.X entries, where X is a unique ID of each server:
server.X=addressX:peerPort:leaderPort [...] server.Y=addressY:peerPort:leaderPort The script bin/start-zookeeper-quorum.sh will start a ZooKeeper server on each of the configured hosts. The started processes start ZooKeeper servers via a Flink wrapper, which reads the configuration from conf/zoo.cfg and makes sure to set some required configuration values for convenience. In production setups, it is recommended to manage your own ZooKeeper installation.
Back to top
`}),e.add({id:68,href:"/flink/flink-docs-master/docs/deployment/filesystems/s3/",title:"Amazon S3",section:"File Systems",content:` Amazon S3 # Amazon Simple Storage Service (Amazon S3) provides cloud object storage for a variety of use cases. You can use S3 with Flink for reading and writing data as well in conjunction with the streaming state backends.
You can use S3 objects like regular files by specifying paths in the following format:
s3://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt; The endpoint can either be a single file or a directory, for example:
// Read from S3 bucket env.readTextFile(\u0026#34;s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Write to S3 bucket stream.writeAsText(\u0026#34;s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Use S3 as checkpoint storage env.getCheckpointConfig().setCheckpointStorage(\u0026#34;s3://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); Note that these examples are not exhaustive and you can use S3 in other places as well, including your high availability setup or the EmbeddedRocksDBStateBackend; everywhere that Flink expects a FileSystem URI (unless otherwise stated).
For most use cases, you may use one of our flink-s3-fs-hadoop and flink-s3-fs-presto S3 filesystem plugins which are self-contained and easy to set up. For some cases, however, e.g., for using S3 as YARN\u0026rsquo;s resource storage dir, it may be necessary to set up a specific Hadoop S3 filesystem implementation.
Hadoop/Presto S3 File Systems plugins # You don\u0026rsquo;t have to configure this manually if you are running Flink on EMR. Flink provides two file systems to talk to Amazon S3, flink-s3-fs-presto and flink-s3-fs-hadoop. Both implementations are self-contained with no dependency footprint, so there is no need to add Hadoop to the classpath to use them.
flink-s3-fs-presto, registered under the scheme s3:// and s3p://, is based on code from the Presto project. You can configure it using the same configuration keys as the Presto file system, by adding the configurations to your flink-conf.yaml. The Presto S3 implementation is the recommended file system for checkpointing to S3.
flink-s3-fs-hadoop, registered under s3:// and s3a://, based on code from the Hadoop Project. The file system can be configured using Hadoop\u0026rsquo;s s3a configuration keys by adding the configurations to your flink-conf.yaml.
For example, Hadoop has a fs.s3a.connection.maximum configuration key. If you want to change it, you need to put s3.connection.maximum: xyz to the flink-conf.yaml. Flink will internally translate this back to fs.s3a.connection.maximum. There is no need to pass configuration parameters using Hadoop\u0026rsquo;s XML configuration files.
It is the only S3 file system with support for the FileSystem.
Both flink-s3-fs-hadoop and flink-s3-fs-presto register default FileSystem wrappers for URIs with the s3:// scheme, flink-s3-fs-hadoop also registers for s3a:// and flink-s3-fs-presto also registers for s3p://, so you can use this to use both at the same time. For example, the job uses the FileSystem which only supports Hadoop, but uses Presto for checkpointing. In this case, you should explicitly use s3a:// as a scheme for the sink (Hadoop) and s3p:// for checkpointing (Presto).
To use flink-s3-fs-hadoop or flink-s3-fs-presto, copy the respective JAR file from the opt directory to the plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/s3-fs-presto cp ./opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar ./plugins/s3-fs-presto/ Configure Access Credentials # After setting up the S3 FileSystem wrapper, you need to make sure that Flink is allowed to access your S3 buckets.
Identity and Access Management (IAM) (Recommended) # The recommended way of setting up credentials on AWS is via Identity and Access Management (IAM). You can use IAM features to securely give Flink instances the credentials that they need to access S3 buckets. Details about how to do this are beyond the scope of this documentation. Please refer to the AWS user guide. What you are looking for are IAM Roles.
If you set this up correctly, you can manage access to S3 within AWS and don\u0026rsquo;t need to distribute any access keys to Flink.
Access Keys (Discouraged) # Access to S3 can be granted via your access and secret key pair. Please note that this is discouraged since the introduction of IAM roles.
You need to configure both s3.access-key and s3.secret-key in Flink\u0026rsquo;s flink-conf.yaml:
s3.access-key: your-access-key s3.secret-key: your-secret-key Configure Non-S3 Endpoint # The S3 Filesystems also support using S3 compliant object stores such as IBM\u0026rsquo;s Cloud Object Storage and MinIO. To do so, configure your endpoint in flink-conf.yaml.
s3.endpoint: your-endpoint-hostname Configure Path Style Access # Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access in flink-conf.yaml.
s3.path.style.access: true Entropy injection for S3 file systems # The bundled S3 file systems (flink-s3-fs-presto and flink-s3-fs-hadoop) support entropy injection. Entropy injection is a technique to improve the scalability of AWS S3 buckets through adding some random characters near the beginning of the key.
If entropy injection is activated, a configured substring in the path is replaced with random characters. For example, path s3://my-bucket/_entropy_/checkpoints/dashboard-job/ would be replaced by something like s3://my-bucket/gf36ikvg/checkpoints/dashboard-job/. This only happens when the file creation passes the option to inject entropy! Otherwise, the file path removes the entropy key substring entirely. See FileSystem.create(Path, WriteOption) for details.
The Flink runtime currently passes the option to inject entropy only to checkpoint data files. All other files, including checkpoint metadata and external URI, do not inject entropy to keep checkpoint URIs predictable. To enable entropy injection, configure the entropy key and the entropy length parameters.
s3.entropy.key: _entropy_ s3.entropy.length: 4 (default) The s3.entropy.key defines the string in paths that is replaced by the random characters. Paths that do not contain the entropy key are left unchanged. If a file system operation does not pass the \u0026ldquo;inject entropy\u0026rdquo; write option, the entropy key substring is simply removed. The s3.entropy.length defines the number of random alphanumeric characters used for entropy.
Back to top
`}),e.add({id:69,href:"/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/",title:"Builtin Watermark Generators",section:"Event Time",content:` Builtin Watermark Generators # As described in Generating Watermarks, Flink provides abstractions that allow the programmer to assign their own timestamps and emit their own watermarks. More specifically, one can do so by implementing the WatermarkGenerator interface.
In order to further ease the programming effort for such tasks, Flink comes with some pre-implemented timestamp assigners. This section provides a list of them. Apart from their out-of-the-box functionality, their implementation can serve as an example for custom implementations.
Monotonously Increasing Timestamps # The simplest special case for periodic watermark generation is the when timestamps seen by a given source task occur in ascending order. In that case, the current timestamp can always act as a watermark, because no earlier timestamps will arrive.
Note that it is only necessary that timestamps are ascending per parallel data source task. For example, if in a specific setup one Kafka partition is read by one parallel data source instance, then it is only necessary that timestamps are ascending within each Kafka partition. Flink\u0026rsquo;s watermark merging mechanism will generate correct watermarks whenever parallel streams are shuffled, unioned, connected, or merged.
Java WatermarkStrategy.forMonotonousTimestamps(); Scala WatermarkStrategy.forMonotonousTimestamps() Python WatermarkStrategy.for_monotonous_timestamps() Fixed Amount of Lateness # Another example of periodic watermark generation is when the watermark lags behind the maximum (event-time) timestamp seen in the stream by a fixed amount of time. This case covers scenarios where the maximum lateness that can be encountered in a stream is known in advance, e.g. when creating a custom source containing elements with timestamps spread within a fixed period of time for testing. For these cases, Flink provides the BoundedOutOfOrdernessWatermarks generator which takes as an argument the maxOutOfOrderness, i.e. the maximum amount of time an element is allowed to be late before being ignored when computing the final result for the given window. Lateness corresponds to the result of t - t_w, where t is the (event-time) timestamp of an element, and t_w that of the previous watermark. If lateness \u0026gt; 0 then the element is considered late and is, by default, ignored when computing the result of the job for its corresponding window. See the documentation about allowed lateness for more information about working with late elements.
Java WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); Scala WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)) Python WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(10)) Back to top
`}),e.add({id:70,href:"/flink/flink-docs-master/docs/concepts/",title:"Concepts",section:"Docs",content:""}),e.add({id:71,href:"/flink/flink-docs-master/docs/deployment/config/",title:"Configuration",section:"Deployment",content:` Configuration # All configuration is done in conf/flink-conf.yaml, which is expected to be a flat collection of YAML key value pairs with format key: value.
The configuration is parsed and evaluated when the Flink processes are started. Changes to the configuration file require restarting the relevant processes.
The out of the box configuration will use your default Java installation. You can manually set the environment variable JAVA_HOME or the configuration key env.java.home in conf/flink-conf.yaml if you want to manually override the Java runtime to use.
You can specify a different configuration directory location by defining the FLINK_CONF_DIR environment variable. For resource providers which provide non-session deployments, you can specify per-job configurations this way. Make a copy of the conf directory from the Flink distribution and modify the settings on a per-job basis. Note that this is not supported in Docker or standalone Kubernetes deployments. On Docker-based deployments, you can use the FLINK_PROPERTIES environment variable for passing configuration values.
On session clusters, the provided configuration will only be used for configuring execution parameters, e.g. configuration parameters affecting the job, not the underlying cluster.
Basic Setup # The default configuration supports starting a single-node Flink session cluster without any changes. The options in this section are the ones most commonly needed for a basic distributed Flink setup.
Hostnames / Ports
These options are only necessary for standalone application- or session deployments (simple standalone or Kubernetes).
If you use Flink with Yarn or the active Kubernetes integration, the hostnames and ports are automatically discovered.
rest.address, rest.port: These are used by the client to connect to Flink. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes) service in front of the JobManager\u0026rsquo;s REST interface.
The jobmanager.rpc.address (defaults to \u0026ldquo;localhost\u0026rdquo;) and jobmanager.rpc.port (defaults to 6123) config entries are used by the TaskManager to connect to the JobManager/ResourceManager. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes internal) service for the JobManager. This option is ignored on setups with high-availability where the leader election mechanism is used to discover this automatically.
Memory Sizes
The default memory sizes support simple streaming/batch applications, but are too low to yield good performance for more complex applications.
jobmanager.memory.process.size: Total size of the JobManager (JobMaster / ResourceManager / Dispatcher) process. taskmanager.memory.process.size: Total size of the TaskManager process. The total sizes include everything. Flink will subtract some memory for the JVM\u0026rsquo;s own memory requirements (metaspace and others), and divide and configure the rest automatically between its components (JVM Heap, Off-Heap, for Task Managers also network, managed memory etc.).
These value are configured as memory sizes, for example 1536m or 2g.
Parallelism
taskmanager.numberOfTaskSlots: The number of slots that a TaskManager offers (default: 1). Each slot can take one task or pipeline. Having multiple slots in a TaskManager can help amortize certain constant overheads (of the JVM, application libraries, or network connections) across parallel tasks or pipelines. See the Task Slots and Resources concepts section for details.
Running more smaller TaskManagers with one slot each is a good starting point and leads to the best isolation between tasks. Dedicating the same resources to fewer larger TaskManagers with more slots can help to increase resource utilization, at the cost of weaker isolation between the tasks (more tasks share the same JVM).
parallelism.default: The default parallelism used when no parallelism is specified anywhere (default: 1).
Checkpointing
You can configure checkpointing directly in code within your Flink job or application. Putting these values here in the configuration defines them as defaults in case the application does not configure anything.
state.backend: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are filesystem or rocksdb. state.checkpoints.dir: The directory to write checkpoints to. This takes a path URI like s3://mybucket/flink-app/checkpoints or hdfs://namenode:port/flink/checkpoints. state.savepoints.dir: The default directory for savepoints. Takes a path URI, similar to state.checkpoints.dir. execution.checkpointing.interval: The base interval setting. To enable checkpointing, you need to set this value larger than 0. Web UI
web.submit.enable: Enables uploading and starting jobs through the Flink UI (true by default). Please note that even when this is disabled, session clusters still accept jobs through REST requests (HTTP calls). This flag only guards the feature to upload jobs in the UI. web.cancel.enable: Enables canceling jobs through the Flink UI (true by default). Please note that even when this is disabled, session clusters still cancel jobs through REST requests (HTTP calls). This flag only guards the feature to cancel jobs in the UI. web.upload.dir: The directory where to store uploaded jobs. Only used when web.submit.enable is true. web.exception-history-size: Sets the size of the exception history that prints the most recent failures that were handled by Flink for a job. Other
io.tmp.dirs: The directories where Flink puts local data, defaults to the system temp directory (java.io.tmpdir property). If a list of directories is configured, Flink will rotate files across the directories.
The data put in these directories include by default the files created by RocksDB, spilled intermediate results (batch algorithms), and cached jar files.
This data is NOT relied upon for persistence/recovery, but if this data gets deleted, it typically causes a heavyweight recovery operation. It is hence recommended to set this to a directory that is not automatically periodically purged.
Yarn and Kubernetes setups automatically configure this value to the local working directories by default.
Common Setup Options # Common options to configure your Flink application or cluster.
Hosts and Ports # Options to configure hostnames and ports for the different Flink components.
The JobManager hostname and port are only relevant for standalone setups without high-availability. In that setup, the config values are used by the TaskManagers to find (and connect to) the JobManager. In all highly-available setups, the TaskManagers discover the JobManager via the High-Availability-Service (for example ZooKeeper).
Setups using resource orchestration frameworks (K8s, Yarn) typically use the framework\u0026rsquo;s service discovery facilities.
You do not need to configure any TaskManager hosts and ports, unless the setup requires the use of specific port ranges or specific network interfaces to bind to.
Key Default Type Description jobmanager.rpc.address (none) String The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.rpc.port 6123 Integer The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. metrics.internal.query-service.port "0" String The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. rest.address (none) String The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE. rest.bind-address (none) String The address that the server binds itself. rest.bind-port "8081" String The port that the server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine. rest.port 8081 Integer The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE. taskmanager.data.port 0 Integer The task manager’s external port used for data exchange operations. taskmanager.host (none) String The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. taskmanager.rpc.port "0" String The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. Fault Tolerance # These configuration options control Flink\u0026rsquo;s restart behaviour in case of failures during the execution. By configuring these options in your flink-conf.yaml, you define the cluster\u0026rsquo;s default restart strategy.
The default restart strategy will only take effect if no job specific restart strategy has been configured via the ExecutionConfig.
Key Default Type Description restart-strategy (none) String Defines the restart strategy to use in case of job failures.
Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy. More details can be found here.failurerate, failure-rate: Failure rate restart strategy. More details can be found here.exponentialdelay, exponential-delay: Exponential delay restart strategy. More details can be found here.If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay. Fixed Delay Restart Strategy
Key Default Type Description restart-strategy.fixed-delay.attempts 1 Integer The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay. restart-strategy.fixed-delay.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s" Failure Rate Restart Strategy
Key Default Type Description restart-strategy.failure-rate.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.failure-rate-interval 1 min Duration Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.max-failures-per-interval 1 Integer Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate. Retryable Cleanup # After jobs reach a globally-terminal state, a cleanup of all related resources is performed. This cleanup can be retried in case of failure. Different retry strategies can be configured to change this behavior:
Key Default Type Description cleanup-strategy "exponential-delay" String Defines the cleanup strategy to use in case of cleanup failures.
Accepted values are:none, disable, off: Cleanup is only performed once. No retry will be initiated in case of failure. The job artifacts (and the job's JobResultStore entry) have to be cleaned up manually in case of a failure.fixed-delay, fixeddelay: Cleanup attempts will be separated by a fixed interval up to the point where the cleanup is considered successful or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.exponential-delay, exponentialdelay: Exponential delay restart strategy triggers the cleanup with an exponentially increasing delay up to the point where the cleanup succeeded or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.The default configuration relies on an exponentially delayed retry strategy with the given default values. Fixed-Delay Cleanup Retry Strategy
Key Default Type Description cleanup-strategy.fixed-delay.attempts infinite Integer The number of times that Flink retries the cleanup before giving up if cleanup-strategy has been set to fixed-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. cleanup-strategy.fixed-delay.delay 1 min Duration Amount of time that Flink waits before re-triggering the cleanup after a failed attempt if the cleanup-strategy is set to fixed-delay. It can be specified using the following notation: "1 min", "20 s" Exponential-Delay Cleanup Retry Strategy
Key Default Type Description cleanup-strategy.exponential-delay.attempts infinite Integer The number of times a failed cleanup is retried if cleanup-strategy has been set to exponential-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. cleanup-strategy.exponential-delay.initial-backoff 1 s Duration Starting duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s" cleanup-strategy.exponential-delay.max-backoff 1 h Duration The highest possible duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s" Checkpoints and State Backends # These options control the basic setup of state backends and checkpointing behavior.
The options are only relevant for jobs/applications executing in a continuous streaming fashion. Jobs/applications executing in a batch fashion do not use state backends and checkpoints, but different internal data structures that are optimized for batch processing.
Key Default Type Description state.backend (none) String The state backend to be used to store state.
The implementation can be specified either via their shortcut name, or via the class name of a StateBackendFactory. If a factory is specified it is instantiated via its zero argument constructor and its StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'hashmap' and 'rocksdb'. state.checkpoint-storage (none) String The checkpoint storage implementation to be used to checkpoint state.
The implementation can be specified either via their shortcut name, or via the class name of a CheckpointStorageFactory. If a factory is specified it is instantiated via its zero argument constructor and its CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'jobmanager' and 'filesystem'. state.checkpoints.dir (none) String The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). state.savepoints.dir (none) String The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). state.backend.incremental false Boolean Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. state.backend.local-recovery false Boolean This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, the MemoryStateBackend does not support local recovery and ignores this option. state.checkpoints.num-retained 1 Integer The maximum number of completed checkpoints to retain. taskmanager.state.local.root-dirs (none) String The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignores this option. If not configured it will default to \u0026lt;WORKING_DIR\u0026gt;/localState. The \u0026lt;WORKING_DIR\u0026gt; can be configured via process.taskmanager.working-dir High Availability # High-availability here refers to the ability of the JobManager process to recover from failures.
The JobManager ensures consistency during recovery across TaskManagers. For the JobManager itself to recover consistently, an external service must store a minimal amount of recovery metadata (like \u0026ldquo;ID of last committed checkpoint\u0026rdquo;), as well as help to elect and lock which JobManager is the leader (to avoid split-brain situations).
Key Default Type Description high-availability "NONE" String Defines high-availability mode used for cluster execution. To enable high-availability, set this mode to "ZOOKEEPER", "KUBERNETES", or specify the fully qualified name of the factory class. high-availability.cluster-id "/default" String The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN. high-availability.storageDir (none) String File system path (URI) where Flink persists metadata in high-availability setups. Options for the JobResultStore in high-availability setups
Key Default Type Description job-result-store.delete-on-commit true Boolean Determines whether job results should be automatically removed from the underlying job result store when the corresponding entity transitions into a clean state. If false, the cleaned job results are, instead, marked as clean to indicate their state. In this case, Flink no longer has ownership and the resources need to be cleaned up by the user. job-result-store.storage-path (none) String Defines where job results should be stored. This should be an underlying file-system that provides read-after-write consistency. By default, this is {high-availability.storageDir}/job-result-store/{high-availability.cluster-id}. Options for high-availability setups with ZooKeeper
Key Default Type Description high-availability.zookeeper.path.root "/flink" String The root path under which Flink stores its entries in ZooKeeper. high-availability.zookeeper.quorum (none) String The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper. Memory Configuration # These configuration values control the way that TaskManagers and JobManagers use memory.
Flink tries to shield users as much as possible from the complexity of configuring the JVM for data-intensive processing. In most cases, users should only need to set the values taskmanager.memory.process.size or taskmanager.memory.flink.size (depending on how the setup), and possibly adjusting the ratio of JVM heap and Managed Memory via taskmanager.memory.managed.fraction. The other options below can be used for performance tuning and fixing memory related errors.
For a detailed explanation of how these options interact, see the documentation on TaskManager and JobManager memory configurations.
Key Default Type Description jobmanager.memory.enable-jvm-direct-memory-limit false Boolean Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option. jobmanager.memory.flink.size (none) MemorySize Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration. jobmanager.memory.heap.size (none) MemorySize JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes). jobmanager.memory.jvm-metaspace.size 256 mb MemorySize JVM Metaspace Size for the JobManager. jobmanager.memory.jvm-overhead.fraction 0.1 Float Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.jvm-overhead.max 1 gb MemorySize Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.jvm-overhead.min 192 mb MemorySize Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.off-heap.size 128 mb MemorySize Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'. jobmanager.memory.process.size (none) MemorySize Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration. taskmanager.memory.flink.size (none) MemorySize Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration. taskmanager.memory.framework.heap.size 128 mb MemorySize Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots. taskmanager.memory.framework.off-heap.batch-shuffle.size 64 mb MemorySize Size of memory used by blocking shuffle for shuffle data read (currently only used by sort-shuffle and hybrid shuffle). Notes: 1) The memory is cut from 'taskmanager.memory.framework.off-heap.size' so must be smaller than that, which means you may also need to increase 'taskmanager.memory.framework.off-heap.size' after you increase this config value; 2) This memory size can influence the shuffle performance and you can increase this config value for large-scale batch jobs (for example, to 128M or 256M). taskmanager.memory.framework.off-heap.size 128 mb MemorySize Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. taskmanager.memory.jvm-metaspace.size 256 mb MemorySize JVM Metaspace Size for the TaskExecutors. taskmanager.memory.jvm-overhead.fraction 0.1 Float Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.jvm-overhead.max 1 gb MemorySize Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.jvm-overhead.min 192 mb MemorySize Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.managed.consumer-weights OPERATOR:70,STATE_BACKEND:70,PYTHON:30 Map Managed memory weights for different kinds of consumers. A slot’s managed memory is shared by all kinds of consumers it contains, proportionally to the kinds’ weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are OPERATOR (for built-in algorithms), STATE_BACKEND (for RocksDB state backend) and PYTHON (for Python processes). taskmanager.memory.managed.fraction 0.4 Float Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified. taskmanager.memory.managed.size (none) MemorySize Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory. taskmanager.memory.network.fraction 0.1 Float Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.network.max 1 gb MemorySize Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. taskmanager.memory.network.min 64 mb MemorySize Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. taskmanager.memory.process.size (none) MemorySize Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration. taskmanager.memory.task.heap.size (none) MemorySize Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory. taskmanager.memory.task.off-heap.size 0 bytes MemorySize Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. Miscellaneous Options # Key Default Type Description fs.allowed-fallback-filesystems (none) String A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb) fs.default-scheme (none) String The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode. io.tmp.dirs 'LOCAL_DIRS' on Yarn. System.getProperty("java.io.tmpdir") in standalone. String Directories for temporary files, separated by",", "|", or the system's java.io.File.pathSeparator. Security # Options for configuring Flink\u0026rsquo;s security and secure interaction with external systems.
SSL # Flink\u0026rsquo;s network connections can be secured via SSL. Please refer to the SSL Setup Docs for detailed setup guide and background.
Key Default Type Description security.ssl.algorithms "TLS_RSA_WITH_AES_128_CBC_SHA" String The comma separated list of standard SSL algorithms to be supported. Read more here security.ssl.internal.cert.fingerprint (none) String The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required security.ssl.internal.enabled false Boolean Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc). security.ssl.internal.key-password (none) String The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore-password (none) String The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.protocol "TLSv1.2" String The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list. security.ssl.rest.authentication-enabled false Boolean Turns on mutual SSL authentication for external communication via the REST endpoints. security.ssl.rest.cert.fingerprint (none) String The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA security.ssl.rest.enabled false Boolean Turns on SSL for external communication via the REST endpoints. security.ssl.rest.key-password (none) String The secret to decrypt the key in the keystore for Flink's external REST endpoints. security.ssl.rest.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints. security.ssl.rest.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints. security.ssl.rest.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints. security.ssl.rest.truststore-password (none) String The password to decrypt the truststore for Flink's external REST endpoints. security.ssl.verify-hostname true Boolean Flag to enable peer’s hostname verification during ssl handshake. Auth with External Systems # ZooKeeper Authentication / Authorization
These options are necessary when connecting to a secured ZooKeeper quorum.
Key Default Type Description zookeeper.sasl.disable false Boolean zookeeper.sasl.login-context-name "Client" String zookeeper.sasl.service-name "zookeeper" String Kerberos-based Authentication / Authorization
Please refer to the Flink and Kerberos Docs for a setup guide and a list of external system to which Flink can authenticate itself via Kerberos.
Key Default Type Description security.kerberos.access.hadoopFileSystems (none) List\u0026lt;String\u0026gt; A comma-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens. security.kerberos.fetch.delegation-token true Boolean Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens. security.kerberos.login.contexts (none) String A comma-separated list of login contexts to provide the Kerberos credentials to (for example, \`Client,KafkaClient\` to use the credentials for ZooKeeper authentication and for Kafka authentication) security.kerberos.login.keytab (none) String Absolute path to a Kerberos keytab file that contains the user credentials. security.kerberos.login.principal (none) String Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache true Boolean Indicates whether to read from your Kerberos ticket cache. security.kerberos.relogin.period 1 min Duration The time period when keytab login happens automatically in order to always have a valid TGT. security.kerberos.tokens.renewal.retry.backoff 1 h Duration The time period how long to wait before retrying to obtain new delegation tokens after a failure. security.kerberos.tokens.renewal.time-ratio 0.75 Double Ratio of the tokens's expiration time when new credentials should be re-obtained. Resource Orchestration Frameworks # This section contains options related to integrating Flink with resource orchestration frameworks, like Kubernetes, Yarn, etc.
Note that is not always necessary to integrate Flink with the resource orchestration framework. For example, you can easily deploy Flink applications on Kubernetes without Flink knowing that it runs on Kubernetes (and without specifying any of the Kubernetes config options here.) See this setup guide for an example.
The options in this section are necessary for setups where Flink itself actively requests and releases resources from the orchestrators.
YARN # Key Default Type Description external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key (none) String If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount. flink.hadoop.\u0026lt;key\u0026gt; (none) String A general option to probe Hadoop configuration through prefix 'flink.hadoop.'. Flink will remove the prefix to get \u0026lt;key\u0026gt; (from core-default.xml and hdfs-default.xml) then set the \u0026lt;key\u0026gt; and value to Hadoop configuration. For example, flink.hadoop.dfs.replication=5 in Flink configuration and convert to dfs.replication=5 in Hadoop configuration. flink.yarn.\u0026lt;key\u0026gt; (none) String A general option to probe Yarn configuration through prefix 'flink.yarn.'. Flink will remove the prefix 'flink.' to get yarn.\u0026lt;key\u0026gt; (from yarn-default.xml) then set the yarn.\u0026lt;key\u0026gt; and value to Yarn configuration. For example, flink.yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Flink configuration and convert to yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Yarn configuration. yarn.application-attempt-failures-validity-interval 10000 Long Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See here for more information. yarn.application-attempts (none) String Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via yarn.resourcemanager.am.max-attempts). Note that that the entire Flink cluster will restart and the YARN Client will lose the connection. yarn.application-master.port "0" String With this configuration option, users can specify a port, a range of ports or a list of ports for the Application Master (and JobManager) RPC port. By default we recommend using the default value (0) to let the operating system choose an appropriate port. In particular when multiple AMs are running on the same physical host, fixed port assignments prevent the AM from starting. For example when running Flink on YARN on an environment with a restrictive firewall, this option allows specifying a range of allowed ports. yarn.application.id (none) String The YARN application id of the running yarn cluster. This is the YARN cluster where the pipeline is going to be executed. yarn.application.name (none) String A custom name for your YARN application. yarn.application.node-label (none) String Specify YARN node label for the YARN application. yarn.application.priority -1 Integer A non-negative integer indicating the priority for submitting a Flink YARN application. It will only take effect if YARN priority scheduling setting is enabled. Larger integer corresponds with higher priority. If priority is negative or set to '-1'(default), Flink will unset yarn priority setting and use cluster default priority. Please refer to YARN's official documentation for specific settings required to enable priority scheduling for the targeted YARN version. yarn.application.queue (none) String The YARN queue on which to put the current pipeline. yarn.application.type (none) String A custom type for your YARN application.. yarn.appmaster.vcores 1 Integer The number of virtual cores (vcores) used by YARN application master. yarn.classpath.include-user-jar ORDER Enum
Defines whether user-jars are included in the system class path as well as their positioning in the path.
Possible values:"DISABLED": Exclude user jars from the system class path"FIRST": Position at the beginning"LAST": Position at the end"ORDER": Position based on the name of the jar yarn.containers.vcores -1 Integer The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler. yarn.file-replication -1 Integer Number of file replication of each local resource file. If it is not configured, Flink will use the default replication value in hadoop configuration. yarn.flink-dist-jar (none) String The location of the Flink dist jar. yarn.heartbeat.container-request-interval 500 Integer Time between heartbeats with the ResourceManager in milliseconds if Flink requests containers:The lower this value is, the faster Flink will get notified about container allocations since requests and allocations are transmitted via heartbeats.The lower this value is, the more excessive containers might get allocated which will eventually be released but put pressure on Yarn.If you observe too many container allocations on the ResourceManager, then it is recommended to increase this value. See this link for more information. yarn.heartbeat.interval 5 Integer Time between heartbeats with the ResourceManager in seconds. yarn.properties-file.location (none) String When a Flink job is submitted to YARN, the JobManager’s host and the number of available processing slots is written into a properties file, so that the Flink client is able to pick those details up. This configuration parameter allows changing the default location of that file (for example for environments sharing a Flink installation between users). yarn.provided.lib.dirs (none) List\u0026lt;String\u0026gt; A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://\$namenode_address/path/of/flink/lib yarn.provided.usrlib.dir (none) String The provided usrlib directory in remote. It should be pre-uploaded and world-readable. Flink will use it to exclude the local usrlib directory(i.e. usrlib/ under the parent directory of FLINK_LIB_DIR). Unlike yarn.provided.lib.dirs, YARN will not cache it on the nodes as it is for each application. An example could be hdfs://\$namenode_address/path/of/flink/usrlib yarn.security.kerberos.localized-keytab-path "krb5.keytab" String Local (on NodeManager) path where kerberos keytab file will be localized to. If yarn.security.kerberos.ship-local-keytab set to true, Flink willl ship the keytab file as a YARN local resource. In this case, the path is relative to the local resource directory. If set to false, Flink will try to directly locate the keytab from the path itself. yarn.security.kerberos.ship-local-keytab true Boolean When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource. yarn.ship-archives (none) List\u0026lt;String\u0026gt; A semicolon-separated list of archives to be shipped to the YARN cluster. These archives will be un-packed when localizing and they can be any of the following types: ".tar.gz", ".tar", ".tgz", ".dst", ".jar", ".zip". yarn.ship-files (none) List\u0026lt;String\u0026gt; A semicolon-separated list of files and/or directories to be shipped to the YARN cluster. yarn.staging-directory (none) String Staging directory used to store YARN files while submitting applications. Per default, it uses the home directory of the configured file system. yarn.tags (none) String A comma-separated list of tags to apply to the Flink YARN application. yarn.taskmanager.node-label (none) String Specify YARN node label for the Flink TaskManagers, it will override the yarn.application.node-label for TaskManagers if both are set. Kubernetes # Key Default Type Description external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key (none) String If configured, Flink will add "resources.limits.\u0026lt;config-key\u0026gt;" and "resources.requests.\u0026lt;config-key\u0026gt;" to the main container of TaskExecutor and set the value to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount. kubernetes.client.io-pool.size 4 Integer The size of the IO executor pool used by the Kubernetes client to execute blocking IO operations (e.g. start/stop TaskManager pods, update leader related ConfigMaps, etc.). Increasing the pool size allows to run more IO operations concurrently. kubernetes.client.user-agent "flink" String The user agent to be used for contacting with Kubernetes APIServer. kubernetes.cluster-id (none) String The cluster-id, which should be no more than 45 characters, is used for identifying a unique Flink cluster. The id must only contain lowercase alphanumeric characters and "-". The required format is [a-z]([-a-z0-9]*[a-z0-9]). If not set, the client will automatically generate it with a random ID. kubernetes.config.file (none) String The kubernetes config file will be used to create the client. The default is located at ~/.kube/config kubernetes.container.image The default value depends on the actually running version. In general it looks like "flink:\u0026lt;FLINK_VERSION\u0026gt;-scala_\u0026lt;SCALA_VERSION\u0026gt;" String Image to use for Flink containers. The specified image must be based upon the same Apache Flink and Scala versions as used by the application. Visit here for the official docker images provided by the Flink project. The Flink project also publishes docker images to apache/flink DockerHub repository. kubernetes.container.image.pull-policy IfNotPresent Enum
The Kubernetes container image pull policy. The default policy is IfNotPresent to avoid putting pressure to image repository.
Possible values:"IfNotPresent""Always""Never" kubernetes.container.image.pull-secrets (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the Kubernetes secrets used to access private image registries. kubernetes.context (none) String The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts. kubernetes.entry.path "/docker-entrypoint.sh" String The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container. kubernetes.env.secretKeyRef (none) List\u0026lt;Map\u0026gt; The user-specified secrets to set env variables in Flink container. The value should be in the form of env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key. kubernetes.flink.conf.dir "/opt/flink/conf" String The flink conf directory that will be mounted in pod. The flink-conf.yaml, log4j.properties, logback.xml in this path will be overwritten from config map. kubernetes.flink.log.dir (none) String The directory that logs of jobmanager and taskmanager be saved in the pod. The default value is \$FLINK_HOME/log. kubernetes.hadoop.conf.config-map.name (none) String Specify the name of an existing ConfigMap that contains custom Hadoop configuration to be mounted on the JobManager(s) and TaskManagers. kubernetes.hostnetwork.enabled false Boolean Whether to enable HostNetwork mode. The HostNetwork allows the pod could use the node network namespace instead of the individual pod network namespace. Please note that the JobManager service account should have the permission to update Kubernetes service. kubernetes.jobmanager.annotations (none) Map The user-specified annotations that are set to the JobManager pod. The value could be in the form of a1:v1,a2:v2 kubernetes.jobmanager.cpu 1.0 Double The number of cpu used by job manager kubernetes.jobmanager.cpu.limit-factor 1.0 Double The limit factor of cpu used by job manager. The resources limit cpu will be set to cpu * limit-factor. kubernetes.jobmanager.labels (none) Map The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. kubernetes.jobmanager.memory.limit-factor 1.0 Double The limit factor of memory used by job manager. The resources limit memory will be set to memory * limit-factor. kubernetes.jobmanager.node-selector (none) Map The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. kubernetes.jobmanager.owner.reference (none) List\u0026lt;Map\u0026gt; The user-specified Owner References to be set to the JobManager Deployment. When all the owner resources are deleted, the JobManager Deployment will be deleted automatically, which also deletes all the resources created by this Flink cluster. The value should be formatted as a semicolon-separated list of owner references, where each owner reference is a comma-separated list of \`key:value\` pairs. E.g., apiVersion:v1,blockOwnerDeletion:true,controller:true,kind:FlinkApplication,name:flink-app-name,uid:flink-app-uid;apiVersion:v1,kind:Deployment,name:deploy-name,uid:deploy-uid kubernetes.jobmanager.replicas 1 Integer Specify how many JobManager pods will be started simultaneously. Configure the value to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers. kubernetes.jobmanager.service-account "default" String Service account that is used by jobmanager within kubernetes cluster. The job manager uses this service account when requesting taskmanager pods from the API server. If not explicitly configured, config option 'kubernetes.service-account' will be used. kubernetes.jobmanager.tolerations (none) List\u0026lt;Map\u0026gt; The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 kubernetes.namespace "default" String The namespace that will be used for running the jobmanager and taskmanager pods. kubernetes.pod-template-file (none) String Specify a local file that contains the pod template definition. It will be used to initialize the jobmanager and taskmanager pod. The main container should be defined with name 'flink-main-container'. Notice that this can be overwritten by config options 'kubernetes.pod-template-file.jobmanager' and 'kubernetes.pod-template-file.taskmanager' for jobmanager and taskmanager respectively. kubernetes.pod-template-file.jobmanager (none) String Specify a local file that contains the jobmanager pod template definition. It will be used to initialize the jobmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used. kubernetes.pod-template-file.taskmanager (none) String Specify a local file that contains the taskmanager pod template definition. It will be used to initialize the taskmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used. kubernetes.rest-service.annotations (none) Map The user-specified annotations that are set to the rest Service. The value should be in the form of a1:v1,a2:v2 kubernetes.rest-service.exposed.node-port-address-type InternalIP Enum
The user-specified address type that is used for filtering node IPs when constructing a node port connection string. This option is only considered when 'kubernetes.rest-service.exposed.type' is set to 'NodePort'.
Possible values:"InternalIP""ExternalIP" kubernetes.rest-service.exposed.type ClusterIP Enum
The exposed type of the rest service. The exposed rest service could be used to access the Flink’s Web UI and REST endpoint.
Possible values:"ClusterIP""NodePort""LoadBalancer""Headless_ClusterIP" kubernetes.secrets (none) Map The user-specified secrets that will be mounted into Flink container. The value should be in the form of foo:/opt/secrets-foo,bar:/opt/secrets-bar. kubernetes.service-account "default" String Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively. kubernetes.taskmanager.annotations (none) Map The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2 kubernetes.taskmanager.cpu -1.0 Double The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager kubernetes.taskmanager.cpu.limit-factor 1.0 Double The limit factor of cpu used by task manager. The resources limit cpu will be set to cpu * limit-factor. kubernetes.taskmanager.labels (none) Map The labels to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. kubernetes.taskmanager.memory.limit-factor 1.0 Double The limit factor of memory used by task manager. The resources limit memory will be set to memory * limit-factor. kubernetes.taskmanager.node-selector (none) Map The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. kubernetes.taskmanager.service-account "default" String Service account that is used by taskmanager within kubernetes cluster. The task manager uses this service account when watching config maps on the API server to retrieve leader address of jobmanager and resourcemanager. If not explicitly configured, config option 'kubernetes.service-account' will be used. kubernetes.taskmanager.tolerations (none) List\u0026lt;Map\u0026gt; The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 kubernetes.transactional-operation.max-retries 5 Integer Defines the number of Kubernetes transactional operation retries before the client gives up. For example, FlinkKubeClient#checkAndUpdateConfigMap. State Backends # Please refer to the State Backend Documentation for background on State Backends.
RocksDB State Backend # These are the options commonly needed to configure the RocksDB state backend. See the Advanced RocksDB Backend Section for options necessary for advanced low level configurations and trouble-shooting.
Key Default Type Description state.backend.rocksdb.memory.fixed-per-slot (none) MemorySize The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. If neither this option, nor the 'state.backend.rocksdb.memory.managed' optionare set, then each RocksDB column family state has its own memory caches (as controlled by the column family options). state.backend.rocksdb.memory.high-prio-pool-ratio 0.1 Double The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.memory.managed true Boolean If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped. state.backend.rocksdb.memory.partitioned-index-filters false Boolean With partitioning, the index/filter block of an SST file is partitioned into smaller blocks with an additional top-level index on them. When reading an index/filter, only top-level index is loaded into memory. The partitioned index/filter then uses the top-level index to load on demand into the block cache the partitions that are required to perform the index/filter query. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.memory.write-buffer-ratio 0.5 Double The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.timer-service.factory ROCKSDB Enum
This determines the factory for timer service state implementation.
Possible values:"HEAP": Heap-based"ROCKSDB": Implementation based on RocksDB Metrics # Please refer to the metrics system documentation for background on Flink\u0026rsquo;s metrics infrastructure.
Key Default Type Description metrics.fetcher.update-interval 10000 Long Update interval for the metric fetcher used by the web UI in milliseconds. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely. metrics.internal.query-service.port "0" String The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. metrics.internal.query-service.thread-priority 1 Integer The thread priority used for Flink's internal metric query service. The thread is created by Akka's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down. metrics.job.status.enable CURRENT_TIME List\u0026lt;Enum\u0026gt;
The selection of job status metrics that should be reported.
Possible values:"STATE": For a given state, return 1 if the job is currently in that state, otherwise return 0."CURRENT_TIME": For a given state, if the job is currently in that state, return the time since the job transitioned into that state, otherwise return 0."TOTAL_TIME": For a given state, return how much time the job has spent in that state in total. metrics.latency.granularity "operator" String Defines the granularity of latency metrics. Accepted values are:single - Track latency without differentiating between sources and subtasks.operator - Track latency while differentiating between sources, but not subtasks.subtask - Track latency while differentiating between sources and subtasks. metrics.latency.history-size 128 Integer Defines the number of measured latencies to maintain at each operator. metrics.latency.interval 0 Long Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster. metrics.reporter.\u0026lt;name\u0026gt;.\u0026lt;parameter\u0026gt; (none) String Configures the parameter \u0026lt;parameter\u0026gt; for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.factory.class (none) String The reporter factory class to use for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.filter.excludes List\u0026lt;String\u0026gt; The metrics that should be excluded for the reporter named \u0026lt;name\u0026gt;. The format is identical to filter.includes
metrics.reporter.\u0026lt;name\u0026gt;.filter.includes "*:*:*" List\u0026lt;String\u0026gt; The metrics that should be included for the reporter named \u0026lt;name\u0026gt;. Filters are specified as a list, with each filter following this format:
\u0026lt;scope\u0026gt;[:\u0026lt;name\u0026gt;[,\u0026lt;name\u0026gt;][:\u0026lt;type\u0026gt;[,\u0026lt;type\u0026gt;]]]
A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.
scope: Filters based on the logical scope.
Specified as a pattern where * matches any sequence of characters and . separates scope components.
For example:
"jobmanager.job" matches any job-related metrics on the JobManager,
"*.job" matches all job-related metrics and
"*.job.*" matches all metrics below the job-level (i.e., task/operator metrics etc.).
name: Filters based on the metric name.
Specified as a comma-separate list of patterns where * matches any sequence of characters.
For example, "*Records*,*Bytes*" matches any metrics where the name contains "Records" or "Bytes".
type: Filters based on the metric type. Specified as a comma-separated list of metric types: [counter, meter, gauge, histogram]Examples:"*:numRecords*" Matches metrics like numRecordsIn."*.job.task.operator:numRecords*" Matches metrics like numRecordsIn on the operator level."*.job.task.operator:numRecords*:meter" Matches meter metrics like numRecordsInPerSecond on the operator level."*:numRecords*,numBytes*:counter,meter" Matches all counter/meter metrics like or numRecordsInPerSecond. metrics.reporter.\u0026lt;name\u0026gt;.interval 10 s Duration The reporter interval to use for the reporter named \u0026lt;name\u0026gt;. Only applicable to push-based reporters. metrics.reporter.\u0026lt;name\u0026gt;.scope.delimiter "." String The delimiter used to assemble the metric identifier for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.scope.variables.additional Map The map of additional variables that should be included for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. metrics.reporter.\u0026lt;name\u0026gt;.scope.variables.excludes "." String The set of variables that should be excluded for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. metrics.reporters (none) String An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started. metrics.scope.delimiter "." String Delimiter used to assemble the metric identifier. metrics.scope.jm "\u0026lt;host\u0026gt;.jobmanager" String Defines the scope format string that is applied to all metrics scoped to a JobManager. Only effective when a identifier-based reporter is configured. metrics.scope.jm.job "\u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a job on a JobManager. Only effective when a identifier-based reporter is configured metrics.scope.operator "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to an operator. Only effective when a identifier-based reporter is configured metrics.scope.task "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;subtask_index\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a task. Only effective when a identifier-based reporter is configured metrics.scope.tm "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a TaskManager. Only effective when a identifier-based reporter is configured metrics.scope.tm.job "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager. Only effective when a identifier-based reporter is configured metrics.system-resource false Boolean Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage. metrics.system-resource-probing-interval 5000 Long Interval between probing of system resource metrics specified in milliseconds. Has an effect only when 'metrics.system-resource' is enabled. RocksDB Native Metrics # Flink can report metrics from RocksDB\u0026rsquo;s native code, for applications using the RocksDB state backend. The metrics here are scoped to the operators with unsigned longs and have two kinds of types：
RocksDB property-based metrics, which is broken down by column family, e.g. number of currently running compactions of one specific column family. RocksDB statistics-based metrics, which holds at the database level, e.g. total block cache hit count within the DB. Enabling RocksDB\u0026rsquo;s native metrics may cause degraded performance and should be set carefully. Key Default Type Description state.backend.rocksdb.metrics.actual-delayed-write-rate false Boolean Monitor the current actual delayed write rate. 0 means no delay. state.backend.rocksdb.metrics.background-errors false Boolean Monitor the number of background errors in RocksDB. state.backend.rocksdb.metrics.block-cache-capacity false Boolean Monitor block cache capacity. state.backend.rocksdb.metrics.block-cache-hit false Boolean Monitor the total count of block cache hit in RocksDB (BLOCK_CACHE_HIT == BLOCK_CACHE_INDEX_HIT + BLOCK_CACHE_FILTER_HIT + BLOCK_CACHE_DATA_HIT). state.backend.rocksdb.metrics.block-cache-miss false Boolean Monitor the total count of block cache misses in RocksDB (BLOCK_CACHE_MISS == BLOCK_CACHE_INDEX_MISS + BLOCK_CACHE_FILTER_MISS + BLOCK_CACHE_DATA_MISS). state.backend.rocksdb.metrics.block-cache-pinned-usage false Boolean Monitor the memory size for the entries being pinned in block cache. state.backend.rocksdb.metrics.block-cache-usage false Boolean Monitor the memory size for the entries residing in block cache. state.backend.rocksdb.metrics.bytes-read false Boolean Monitor the number of uncompressed bytes read (from memtables/cache/sst) from Get() operation in RocksDB. state.backend.rocksdb.metrics.bytes-written false Boolean Monitor the number of uncompressed bytes written by DB::{Put(), Delete(), Merge(), Write()} operations, which does not include the compaction written bytes, in RocksDB. state.backend.rocksdb.metrics.column-family-as-variable false Boolean Whether to expose the column family as a variable for RocksDB property based metrics. state.backend.rocksdb.metrics.compaction-pending false Boolean Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise. state.backend.rocksdb.metrics.compaction-read-bytes false Boolean Monitor the bytes read during compaction in RocksDB. state.backend.rocksdb.metrics.compaction-write-bytes false Boolean Monitor the bytes written during compaction in RocksDB. state.backend.rocksdb.metrics.cur-size-active-mem-table false Boolean Monitor the approximate size of the active memtable in bytes. state.backend.rocksdb.metrics.cur-size-all-mem-tables false Boolean Monitor the approximate size of the active and unflushed immutable memtables in bytes. state.backend.rocksdb.metrics.estimate-live-data-size false Boolean Estimate of the amount of live data in bytes (usually smaller than sst files size due to space amplification). state.backend.rocksdb.metrics.estimate-num-keys false Boolean Estimate the number of keys in RocksDB. state.backend.rocksdb.metrics.estimate-pending-compaction-bytes false Boolean Estimated total number of bytes compaction needs to rewrite to get all levels down to under target size. Not valid for other compactions than level-based. state.backend.rocksdb.metrics.estimate-table-readers-mem false Boolean Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g.,filter and index blocks) in bytes. state.backend.rocksdb.metrics.is-write-stopped false Boolean Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise. state.backend.rocksdb.metrics.iter-bytes-read false Boolean Monitor the number of uncompressed bytes read (from memtables/cache/sst) from an iterator operation in RocksDB. state.backend.rocksdb.metrics.live-sst-files-size false Boolean Monitor the total size (bytes) of all SST files belonging to the latest version.WARNING: may slow down online queries if there are too many files. state.backend.rocksdb.metrics.mem-table-flush-pending false Boolean Monitor the number of pending memtable flushes in RocksDB. state.backend.rocksdb.metrics.num-deletes-active-mem-table false Boolean Monitor the total number of delete entries in the active memtable. state.backend.rocksdb.metrics.num-deletes-imm-mem-tables false Boolean Monitor the total number of delete entries in the unflushed immutable memtables. state.backend.rocksdb.metrics.num-entries-active-mem-table false Boolean Monitor the total number of entries in the active memtable. state.backend.rocksdb.metrics.num-entries-imm-mem-tables false Boolean Monitor the total number of entries in the unflushed immutable memtables. state.backend.rocksdb.metrics.num-immutable-mem-table false Boolean Monitor the number of immutable memtables in RocksDB. state.backend.rocksdb.metrics.num-live-versions false Boolean Monitor number of live versions. Version is an internal data structure. See RocksDB file version_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions. state.backend.rocksdb.metrics.num-running-compactions false Boolean Monitor the number of currently running compactions. state.backend.rocksdb.metrics.num-running-flushes false Boolean Monitor the number of currently running flushes. state.backend.rocksdb.metrics.num-snapshots false Boolean Monitor the number of unreleased snapshots of the database. state.backend.rocksdb.metrics.size-all-mem-tables false Boolean Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes. state.backend.rocksdb.metrics.stall-micros false Boolean Monitor the duration of writer requiring to wait for compaction or flush to finish in RocksDB. state.backend.rocksdb.metrics.total-sst-files-size false Boolean Monitor the total size (bytes) of all SST files of all versions.WARNING: may slow down online queries if there are too many files. History Server # The history server keeps the information of completed jobs (graphs, runtimes, statistics). To enable it, you have to enable \u0026ldquo;job archiving\u0026rdquo; in the JobManager (jobmanager.archive.fs.dir).
See the History Server Docs for details.
Key Default Type Description historyserver.archive.clean-expired-jobs false Boolean Whether HistoryServer should cleanup jobs that are no longer present \`historyserver.archive.fs.dir\`. historyserver.archive.fs.dir (none) String Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via \`jobmanager.archive.fs.dir\`. historyserver.archive.fs.refresh-interval 10000 Long Interval in milliseconds for refreshing the archived job directories. historyserver.archive.retained-jobs -1 Integer The maximum number of jobs to retain in each archive directory defined by \`historyserver.archive.fs.dir\`. If set to \`-1\`(default), there is no limit to the number of archives. If set to \`0\` or less than \`-1\` HistoryServer will throw an IllegalConfigurationException. historyserver.log.jobmanager.url-pattern (none) String Pattern of the log URL of JobManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`\u0026lt;jobid\u0026gt;\`, to the id of job. Only http / https schemes are supported. historyserver.log.taskmanager.url-pattern (none) String Pattern of the log URL of TaskManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`\u0026lt;jobid\u0026gt;\` and \`\u0026lt;tmid\u0026gt;\`, to the id of job and TaskManager respectively. Only http / https schemes are supported. historyserver.web.address (none) String Address of the HistoryServer's web interface. historyserver.web.port 8082 Integer Port of the HistoryServers's web interface. historyserver.web.refresh-interval 10000 Long The refresh interval for the HistoryServer web-frontend in milliseconds. historyserver.web.ssl.enabled false Boolean Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true. historyserver.web.tmpdir (none) String Local directory that is used by the history server REST API for temporary files. Experimental # Options for experimental features in Flink.
Queryable State # Queryable State is an experimental features that gives lets you access Flink\u0026rsquo;s internal state like a key/value store. See the Queryable State Docs for details.
Key Default Type Description queryable-state.client.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state client. queryable-state.enable false Boolean Option whether the queryable state proxy and server should be enabled where possible and configurable. queryable-state.proxy.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state proxy. queryable-state.proxy.ports "9069" String The port range of the queryable state proxy. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". queryable-state.proxy.query-threads 0 Integer Number of query Threads for queryable state proxy. Uses the number of slots if set to 0. queryable-state.server.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state server. queryable-state.server.ports "9067" String The port range of the queryable state server. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". queryable-state.server.query-threads 0 Integer Number of query Threads for queryable state server. Uses the number of slots if set to 0. Client # Key Default Type Description client.retry-period 2 s Duration The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec). client.timeout 1 min Duration Timeout on the client side. Execution # Key Default Type Description execution.allow-client-job-configurations true Boolean Determines whether configurations in the user program are allowed. Depending on your deployment mode failing the job might have different affects. Either your client that is trying to submit the job to an external cluster (session cluster deployment) throws the exception or the Job manager (application mode deployment). execution.attached false Boolean Specifies if the pipeline is submitted in attached or detached mode. execution.job-listeners (none) List\u0026lt;String\u0026gt; Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments. execution.shutdown-on-application-finish true Boolean Whether a Flink Application cluster should shut down automatically after its application finishes (either successfully or as result of a failure). Has no effect for other deployment modes. execution.shutdown-on-attached-exit false Boolean If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. execution.submit-failed-job-on-application-error false Boolean If a failed job should be submitted (in the application mode) when there is an error in the application driver before an actual job submission. This is intended for providing a clean way of reporting failures back to the user and is especially useful in combination with 'execution.shutdown-on-application-finish'. This option only works when the single job submission is enforced ('high-availability' is enabled). Please note that this is an experimental option and may be changed in the future. execution.target (none) String The deployment target for the execution. This can take one of the following values when calling bin/flink run:remotelocalyarn-per-job (deprecated)yarn-sessionkubernetes-sessionAnd one of the following values when calling bin/flink run-application:yarn-applicationkubernetes-application Key Default Type Description execution.savepoint-restore-mode NO_CLAIM Enum
Describes the mode how Flink should restore from the given savepoint or retained checkpoint.
Possible values:"CLAIM": Flink will take ownership of the given snapshot. It will clean the snapshot once it is subsumed by newer ones."NO_CLAIM": Flink will not claim ownership of the snapshot files. However it will make sure it does not depend on any artefacts from the restored snapshot. In order to do that, Flink will take the first checkpoint as a full one, which means it might reupload/duplicate files that are part of the restored checkpoint."LEGACY": This is the mode in which Flink worked so far. It will not claim ownership of the snapshot and will not delete the files. However, it can directly depend on the existence of the files of the restored checkpoint. It might not be safe to delete checkpoints that were restored in legacy mode execution.savepoint.ignore-unclaimed-state false Boolean Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered. execution.savepoint.path (none) String Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). Key Default Type Description execution.batch-shuffle-mode ALL_EXCHANGES_BLOCKING Enum
Defines how data is exchanged between tasks in batch 'execution.runtime-mode' if the shuffling behavior has not been set explicitly for an individual exchange.
With pipelined exchanges, upstream and downstream tasks run simultaneously. In order to achieve lower latency, a result record is immediately sent to and processed by the downstream task. Thus, the receiver back-pressures the sender. The streaming mode always uses this exchange.
With blocking exchanges, upstream and downstream tasks run in stages. Records are persisted to some storage between stages. Downstream tasks then fetch these records after the upstream tasks finished. Such an exchange reduces the resources required to execute the job as it does not need to run upstream and downstream tasks simultaneously.
With hybrid exchanges (experimental), downstream tasks can run anytime as long as upstream tasks start running. When given sufficient resources, it can reduce the overall job execution time by running tasks simultaneously. Otherwise, it also allows jobs to be executed with very little resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies.
Possible values:"ALL_EXCHANGES_PIPELINED": Upstream and downstream tasks run simultaneously. This leads to lower latency and more evenly distributed (but higher) resource usage across tasks."ALL_EXCHANGES_BLOCKING": Upstream and downstream tasks run subsequently. This reduces the resource usage as downstream tasks are started after upstream tasks finished."ALL_EXCHANGES_HYBRID_FULL": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will spill all data to disk to support re-consume."ALL_EXCHANGES_HYBRID_SELECTIVE": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will selective spilling data to reduce disk writes as much as possible. execution.buffer-timeout 100 ms Duration The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes:A positive value triggers flushing periodically by that interval0 triggers flushing after every record thus minimizing latency-1 ms triggers flushing only when the output buffer is full thus maximizing throughput execution.checkpointing.snapshot-compression false Boolean Tells if we should use compression for the state snapshot data or not execution.runtime-mode STREAMING Enum
Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics.
Possible values:"STREAMING""BATCH""AUTOMATIC" Pipeline # Key Default Type Description pipeline.auto-generate-uids true Boolean When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications.
It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state. pipeline.auto-type-registration true Boolean Controls whether Flink is automatically registering all types in the user programs with Kryo. pipeline.auto-watermark-interval 0 ms Duration The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing. pipeline.cached-files (none) List\u0026lt;String\u0026gt; Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.
Example:
name:file1,path:\`file:///tmp/file1\`;name:file2,path:\`hdfs:///tmp/file2\` pipeline.classpaths (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs. pipeline.closure-cleaner-level RECURSIVE Enum
Configures the mode in which the closure cleaner works.
Possible values:"NONE": Disables the closure cleaner completely."TOP_LEVEL": Cleans only the top-level class without recursing into fields."RECURSIVE": Cleans all fields recursively. pipeline.default-kryo-serializers (none) List\u0026lt;String\u0026gt; Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializers
Example:
class:org.example.ExampleClass,serializer:org.example.ExampleSerializer1; class:org.example.ExampleClass2,serializer:org.example.ExampleSerializer2 pipeline.force-avro false Boolean Forces Flink to use the Apache Avro serializer for POJOs.
Important: Make sure to include the flink-avro module. pipeline.force-kryo false Boolean If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO. pipeline.generic-types true Boolean If the use of generic types is disabled, Flink will throw an UnsupportedOperationException whenever it encounters a data type that would go through Kryo for serialization.
Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used.
We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail. pipeline.global-job-parameters (none) Map Register a custom, serializable user configuration object. The configuration can be accessed in operators pipeline.jars (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths. pipeline.max-parallelism -1 Integer The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state. pipeline.name (none) String The job name used for printing and logging. pipeline.object-reuse false Boolean When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour. pipeline.operator-chaining true Boolean Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization. pipeline.registered-kryo-types (none) List\u0026lt;String\u0026gt; Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written. pipeline.registered-pojo-types (none) List\u0026lt;String\u0026gt; Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written. pipeline.vertex-description-mode TREE Enum
The mode how we organize description of a job vertex.
Possible values:"TREE""CASCADING" pipeline.vertex-name-include-index-prefix false Boolean Whether name of vertex includes topological index or not. When it is true, the name will have a prefix of index of the vertex, like '[vertex-0]Source: source'. It is false by default Key Default Type Description pipeline.time-characteristic ProcessingTime Enum
The time characteristic for all created streams, e.g., processingtime, event time, or ingestion time.
If you set the characteristic to IngestionTime or EventTime this will set a default watermark update interval of 200 ms. If this is not applicable for your application you should change it using pipeline.auto-watermark-interval.
Possible values:"ProcessingTime""IngestionTime""EventTime" Checkpointing # Key Default Type Description execution.checkpointing.aligned-checkpoint-timeout 0 ms Duration Only relevant if execution.checkpointing.unaligned is enabled.
If timeout is 0, checkpoints will always start unaligned.
If timeout has a positive value, checkpoints will start aligned. If during checkpointing, checkpoint start delay exceeds this timeout, alignment will timeout and checkpoint barrier will start working as unaligned checkpoint. execution.checkpointing.checkpoints-after-tasks-finish.enabled true Boolean Feature toggle for enabling checkpointing even if some of tasks have finished. Before you enable it, please take a look at the important considerations execution.checkpointing.externalized-checkpoint-retention NO_EXTERNALIZED_CHECKPOINTS Enum
Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status JobStatus#FAILED or JobStatus#SUSPENDED). In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state.
The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status JobStatus#CANCELED).
The target directory for externalized checkpoints is configured via state.checkpoints.dir.
Possible values:"DELETE_ON_CANCELLATION": Checkpoint state is only kept when the owning job fails. It is deleted if the job is cancelled."RETAIN_ON_CANCELLATION": Checkpoint state is kept when the owning job is cancelled or fails."NO_EXTERNALIZED_CHECKPOINTS": Externalized checkpoints are disabled. execution.checkpointing.interval (none) Duration Gets the interval in which checkpoints are periodically scheduled.
This setting defines the base interval. Checkpoint triggering may be delayed by the settings execution.checkpointing.max-concurrent-checkpoints and execution.checkpointing.min-pause execution.checkpointing.max-concurrent-checkpoints 1 Integer The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire. execution.checkpointing.min-pause 0 ms Duration The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see execution.checkpointing.max-concurrent-checkpoints).
If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all. execution.checkpointing.mode EXACTLY_ONCE Enum
The checkpointing mode (exactly-once vs. at-least-once).
Possible values:"EXACTLY_ONCE""AT_LEAST_ONCE" execution.checkpointing.recover-without-channel-state.checkpoint-id -1 Long Checkpoint id for which in-flight data should be ignored in case of the recovery from this checkpoint.
It is better to keep this value empty until there is explicit needs to restore from the specific checkpoint without in-flight data.
execution.checkpointing.timeout 10 min Duration The maximum time that a checkpoint may take before being discarded. execution.checkpointing.tolerable-failed-checkpoints (none) Integer The tolerable checkpoint consecutive failure number. If set to 0, that means we do not tolerance any checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored. execution.checkpointing.unaligned false Boolean Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure.
Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.
Unaligned checkpoints can only be enabled if execution.checkpointing.mode is EXACTLY_ONCE and if execution.checkpointing.max-concurrent-checkpoints is 1 execution.checkpointing.unaligned.forced false Boolean Forces unaligned checkpoints, particularly allowing them for iterative jobs. Debugging \u0026amp; Expert Tuning # The options below here are meant for expert users and for fixing/debugging problems. Most setups should not need to configure these options. Class Loading # Flink dynamically loads the code for jobs submitted to a session cluster. In addition, Flink tries to hide many dependencies in the classpath from the application. This helps to reduce dependency conflicts between the application code and the dependencies in the classpath.
Please refer to the Debugging Classloading Docs for details.
Key Default Type Description classloader.check-leaked-classloader true Boolean Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running. classloader.fail-on-metaspace-oom-error true Boolean Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class. classloader.parent-first-patterns.additional List\u0026lt;String\u0026gt; A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to "classloader.parent-first-patterns.default". classloader.parent-first-patterns.default "java.";"scala.";"org.apache.flink.";"com.esotericsoftware.kryo";"org.apache.hadoop.";"javax.annotation.";"org.xml";"javax.xml";"org.apache.xerces";"org.w3c";"org.rocksdb.";"org.slf4j";"org.apache.log4j";"org.apache.logging";"org.apache.commons.logging";"ch.qos.logback" List\u0026lt;String\u0026gt; A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use "classloader.parent-first-patterns.additional" instead. classloader.resolve-order "child-first" String Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar ("child-first") or the application classpath ("parent-first"). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively). Advanced Options for the debugging # Key Default Type Description jmx.server.port (none) String The port range for the JMX server to start the registry. The port config can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". This option overrides metrics.reporter.*.port option. Advanced State Backends Options # Key Default Type Description state.storage.fs.memory-threshold 20 kb MemorySize The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. state.storage.fs.write-buffer-size 4096 Integer The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.storage.fs.memory-threshold'. State Backends Latency Tracking Options # Key Default Type Description state.backend.latency-track.history-size 128 Integer Defines the number of measured latencies to maintain at each state access operation. state.backend.latency-track.keyed-state-enabled false Boolean Whether to track latency of keyed state operations, e.g value state put/get/clear. state.backend.latency-track.sample-interval 100 Integer The sample interval of latency track once 'state.backend.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests. state.backend.latency-track.state-name-as-variable true Boolean Whether to expose state name as a variable if tracking latency. Advanced RocksDB State Backends Options # Advanced options to tune RocksDB and RocksDB checkpoints.
Key Default Type Description state.backend.rocksdb.checkpoint.transfer.thread.num 4 Integer The number of threads (per stateful operator) used to transfer (download and upload) files in RocksDBStateBackend. state.backend.rocksdb.localdir (none) String The local directory (on the TaskManager) where RocksDB puts its files. Per default, it will be \u0026lt;WORKING_DIR\u0026gt;/tmp. See process.taskmanager.working-dir for more details. state.backend.rocksdb.options-factory (none) String The options factory class for users to add customized options in DBOptions and ColumnFamilyOptions for RocksDB. If set, the RocksDB state backend will load the class and apply configs to DBOptions and ColumnFamilyOptions after loading ones from 'RocksDBConfigurableOptions' and pre-defined options. state.backend.rocksdb.predefined-options "DEFAULT" String The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING_DISK_OPTIMIZED, SPINNING_DISK_OPTIMIZED_HIGH_MEM or FLASH_SSD_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones. State Changelog Options # Please refer to State Backends for information on using State Changelog. The feature is in experimental status. Key Default Type Description state.backend.changelog.enabled false Boolean Whether to enable state backend to write state changes to StateChangelog. If this config is not set explicitly, it means no preference for enabling the change log, and the value in lower config level will take effect. The default value 'false' here means if no value set (job or cluster), the change log will not be enabled. state.backend.changelog.max-failures-allowed 3 Integer Max number of consecutive materialization failures allowed. state.backend.changelog.periodic-materialize.interval 10 min Duration Defines the interval in milliseconds to perform periodic materialization for state backend. The periodic materialization will be disabled when the value is negative state.backend.changelog.storage "memory" String The storage to be used to store state changelog.
The implementation can be specified via their shortcut name.
The list of recognized shortcut names currently includes 'memory' and 'filesystem'. FileSystem-based Changelog options # These settings take effect when the state.backend.changelog.storage is set to filesystem (see above). Key Default Type Description dstl.dfs.base-path (none) String Base path to store changelog files. dstl.dfs.batch.persist-delay 10 ms Duration Delay before persisting changelog after receiving persist request (on checkpoint). Minimizes the number of files and requests if multiple operators (backends) or sub-tasks are using the same store. Correspondingly increases checkpoint time (async phase). dstl.dfs.batch.persist-size-threshold 10 mb MemorySize Size threshold for state changes that were requested to be persisted but are waiting for dstl.dfs.batch.persist-delay (from all operators). . Once reached, accumulated changes are persisted immediately. This is different from dstl.dfs.preemptive-persist-threshold as it happens AFTER the checkpoint and potentially for state changes of multiple operators. Must not exceed in-flight data limit (see below) dstl.dfs.compression.enabled false Boolean Whether to enable compression when serializing changelog. dstl.dfs.discard.num-threads 1 Integer Number of threads to use to discard changelog (e.g. pre-emptively uploaded unused state). dstl.dfs.download.local-cache.idle-timeout-ms 10 min Duration Maximum idle time for cache files of distributed changelog file, after which the cache files will be deleted. dstl.dfs.preemptive-persist-threshold 5 mb MemorySize Size threshold for state changes of a single operator beyond which they are persisted pre-emptively without waiting for a checkpoint. Improves checkpointing time by allowing quasi-continuous uploading of state changes (as opposed to uploading all accumulated changes on checkpoint). dstl.dfs.upload.buffer-size 1 mb MemorySize Buffer size used when uploading change sets dstl.dfs.upload.max-attempts 3 Integer Maximum number of attempts (including the initial one) to perform a particular upload. Only takes effect if dstl.dfs.upload.retry-policy is fixed. dstl.dfs.upload.max-in-flight 100 mb MemorySize Max amount of data allowed to be in-flight. Upon reaching this limit the task will be back-pressured. I.e., snapshotting will block; normal processing will block if dstl.dfs.preemptive-persist-threshold is set and reached. The limit is applied to the total size of in-flight changes if multiple operators/backends are using the same changelog storage. Must be greater than or equal to dstl.dfs.batch.persist-size-threshold dstl.dfs.upload.next-attempt-delay 500 ms Duration Delay before the next attempt (if the failure was not caused by a timeout). dstl.dfs.upload.num-threads 5 Integer Number of threads to use for upload. dstl.dfs.upload.retry-policy "fixed" String Retry policy for the failed uploads (in particular, timed out). Valid values: none, fixed. dstl.dfs.upload.timeout 1 s Duration Time threshold beyond which an upload is considered timed out. If a new attempt is made but this upload succeeds earlier then this upload result will be used. May improve upload times if tail latencies of upload requests are significantly high. Only takes effect if dstl.dfs.upload.retry-policy is fixed. Please note that timeout * max_attempts should be less than execution.checkpointing.timeout RocksDB Configurable Options
These options give fine-grained control over the behavior and resources of ColumnFamilies. With the introduction of state.backend.rocksdb.memory.managed and state.backend.rocksdb.memory.fixed-per-slot (Apache Flink 1.10), it should be only necessary to use the options here for advanced performance tuning. These options here can also be specified in the application program via RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory).
Key Default Type Description state.backend.rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'. state.backend.rocksdb.block.cache-size 8 mb MemorySize The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'. state.backend.rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'. state.backend.rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0. state.backend.rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'. state.backend.rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'. state.backend.rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'. state.backend.rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc. state.backend.rocksdb.compaction.style LEVEL Enum
The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.
Possible values:"LEVEL""UNIVERSAL""FIFO""NONE" state.backend.rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'. state.backend.rocksdb.log.dir (none) String The directory for RocksDB's information logging files. If empty (Flink default setting), log files will be in the same directory as the Flink log. If non-empty, this directory will be used and the data directory's absolute path will be used as the prefix of the log file name. If setting this option as a non-existing location, e.g '/dev/null', RocksDB will then create the log under its own database folder as before. state.backend.rocksdb.log.file-num 4 Integer The maximum number of files RocksDB should keep for information logging (Default setting: 4). state.backend.rocksdb.log.level INFO_LEVEL Enum
The specified information logging level for RocksDB. If unset, Flink will use INFO_LEVEL.
Note: RocksDB info logs will not be written to the TaskManager logs and there is no rolling strategy, unless you configure state.backend.rocksdb.log.dir, state.backend.rocksdb.log.max-file-size, and state.backend.rocksdb.log.file-num accordingly. Without a rolling strategy, long-running tasks may lead to uncontrolled disk space usage if configured with increased log levels!
There is no need to modify the RocksDB log level, unless for troubleshooting RocksDB.
Possible values:"DEBUG_LEVEL""INFO_LEVEL""WARN_LEVEL""ERROR_LEVEL""FATAL_LEVEL""HEADER_LEVEL""NUM_INFO_LOG_LEVELS" state.backend.rocksdb.log.max-file-size 25 mb MemorySize The maximum size of RocksDB's file used for information logging. If the log files becomes larger than this, a new file will be created. If 0, all logs will be written to one log file. The default maximum file size is '25MB'. state.backend.rocksdb.restore-overlap-fraction-threshold 0.0 Double The threshold of overlap fraction between the handle's key-group range and target key-group range. When restore base DB, only the handle which overlap fraction greater than or equal to threshold has a chance to be an initial handle. The default value is 0.0, there is always a handle will be selected for initialization. state.backend.rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'. state.backend.rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default. state.backend.rocksdb.write-batch-size 2 mb MemorySize The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0. state.backend.rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'. state.backend.rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'. state.backend.rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'. Advanced Fault Tolerance Options # These parameters can help with problems related to failover and to components erroneously considering each other as failed.
Key Default Type Description cluster.io-pool.size (none) Integer The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently. cluster.registration.error-delay 10000 Long The pause made after an registration attempt caused an exception (other than timeout) in milliseconds. cluster.registration.initial-timeout 100 Long Initial registration timeout between cluster components in milliseconds. cluster.registration.max-timeout 30000 Long Maximum registration timeout between cluster components in milliseconds. cluster.registration.refused-registration-delay 30000 Long The pause made after the registration attempt was refused in milliseconds. cluster.services.shutdown-timeout 30000 Long The shutdown timeout for cluster services like executors in milliseconds. heartbeat.interval 10000 Long Time interval between heartbeat RPC requests from the sender to the receiver side. heartbeat.rpc-failure-threshold 2 Integer The number of consecutive failed heartbeat RPCs until a heartbeat target is marked as unreachable. Failed heartbeat RPCs can be used to detect dead targets faster because they no longer receive the RPCs. The detection time is heartbeat.interval * heartbeat.rpc-failure-threshold. In environments with a flaky network, setting this value too low can produce false positives. In this case, we recommend to increase this value, but not higher than heartbeat.timeout / heartbeat.interval. The mechanism can be disabled by setting this option to -1 heartbeat.timeout 50000 Long Timeout for requesting and receiving heartbeats for both sender and receiver sides. jobmanager.execution.failover-strategy "region" String This option specifies how the job computation recovers from task failures. Accepted values are:'full': Restarts all tasks to recover the job.'region': Restarts all tasks that could be affected by the task failure. More details can be found here. Advanced Cluster Options # Key Default Type Description cluster.intercept-user-system-exit DISABLED Enum
Flag to check user code exiting system by terminating JVM (e.g., System.exit()). Note that this configuration option can interfere with cluster.processes.halt-on-fatal-error: In intercepted user-code, a call to System.exit() will not cause the JVM to halt, when THROW is configured.
Possible values:"DISABLED": Flink is not monitoring or intercepting calls to System.exit()"LOG": Log exit attempt with stack trace but still allowing exit to be performed"THROW": Throw exception when exit is attempted disallowing JVM termination cluster.processes.halt-on-fatal-error false Boolean Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See FLINK-16510 for details. cluster.thread-dump.stacktrace-max-depth 8 Integer The maximum stacktrace depth of TaskManager and JobManager's thread dump web-frontend displayed. cluster.uncaught-exception-handling LOG Enum
Defines whether cluster will handle any uncaught exceptions by just logging them (LOG mode), or by failing job (FAIL mode)
Possible values:"LOG""FAIL" process.jobmanager.working-dir (none) String Working directory for Flink JobManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir. process.taskmanager.working-dir (none) String Working directory for Flink TaskManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir. process.working-dir io.tmp.dirs String Local working directory for Flink processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to a randomly picked temporary directory defined via io.tmp.dirs. Advanced JobManager Options # Key Default Type Description jobmanager.future-pool.size (none) Integer The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.io-pool.size (none) Integer The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. Advanced Scheduling Options # These parameters can help with fine-tuning scheduling for specific situations.
Key Default Type Description cluster.evenly-spread-out-slots false Boolean Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors. cluster.fine-grained-resource-management.enabled false Boolean Defines whether the cluster uses fine-grained resource management. fine-grained.shuffle-mode.all-blocking false Boolean Whether to convert all PIPELINE edges to BLOCKING when apply fine-grained resource management in batch jobs. jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task 1 gb MemorySize The average size of data volume to expect each task instance to process if jobmanager.scheduler has been set to AdaptiveBatch. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs or the decided parallelism reaches the jobmanager.adaptive-batch-scheduler.max-parallelism (due to too much data), the data actually processed by some tasks may far exceed this value. jobmanager.adaptive-batch-scheduler.default-source-parallelism 1 Integer The default parallelism of source vertices if jobmanager.scheduler has been set to AdaptiveBatch jobmanager.adaptive-batch-scheduler.max-parallelism 128 Integer The upper bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded down to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.min-parallelism 1 Integer The lower bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded up to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 1 min Duration Controls how long an detected slow node should be blocked for. jobmanager.adaptive-batch-scheduler.speculative.enabled false Boolean Controls whether to enable speculative execution. jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions 2 Integer Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones. jobmanager.adaptive-scheduler.min-parallelism-increase 1 Integer Configure the minimum increase in parallelism for a job to scale up. jobmanager.adaptive-scheduler.resource-stabilization-timeout 10 s Duration The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.
If scheduler-mode is configured to REACTIVE, this configuration value will default to 0, so that jobs are starting immediately with the available resources. jobmanager.adaptive-scheduler.resource-wait-timeout 5 min Duration The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.
Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).
Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.
If scheduler-mode is configured to REACTIVE, this configuration value will default to a negative value to disable the resource timeout. jobmanager.scheduler Default Enum
Determines which scheduler implementation is used to schedule tasks. Accepted values are:'Default': Default scheduler'Adaptive': Adaptive scheduler. More details can be found here.'AdaptiveBatch': Adaptive batch scheduler. More details can be found here.
Possible values:"Default""Adaptive""AdaptiveBatch" scheduler-mode (none) Enum
Determines the mode of the scheduler. Note that scheduler-mode=REACTIVE is only supported by standalone application deployments, not by active resource managers (YARN, Kubernetes) or session clusters.
Possible values:"REACTIVE" slot.idle.timeout 50000 Long The timeout in milliseconds for a idle slot in Slot Pool. slot.request.timeout 300000 Long The timeout in milliseconds for requesting a slot from Slot Pool. slotmanager.max-total-resource.cpu (none) Double Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.max-total-resource.memory (none) MemorySize Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.number-of-slots.max 2147483647 Integer Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. slow-task-detector.check-interval 1 s Duration The interval to check slow tasks. slow-task-detector.execution-time.baseline-lower-bound 1 min Duration The lower bound of slow task detection baseline. slow-task-detector.execution-time.baseline-multiplier 1.5 Double The multiplier to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. slow-task-detector.execution-time.baseline-ratio 0.75 Double The finished execution ratio threshold to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. Advanced High-availability Options # Key Default Type Description high-availability.jobmanager.port "0" String The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery. Advanced High-availability ZooKeeper Options # Key Default Type Description high-availability.zookeeper.client.acl "open" String Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos). high-availability.zookeeper.client.connection-timeout 15000 Integer Defines the connection timeout for ZooKeeper in ms. high-availability.zookeeper.client.max-retry-attempts 3 Integer Defines the number of connection retries before the client gives up. high-availability.zookeeper.client.retry-wait 5000 Integer Defines the pause between consecutive retries in ms. high-availability.zookeeper.client.session-timeout 60000 Integer Defines the session timeout for the ZooKeeper session in ms. high-availability.zookeeper.client.tolerate-suspended-connections false Boolean Defines whether a suspended ZooKeeper connection will be treated as an error that causes the leader information to be invalidated or not. In case you set this option to true, Flink will wait until a ZooKeeper connection is marked as lost before it revokes the leadership of components. This has the effect that Flink is more resilient against temporary connection instabilities at the cost of running more likely into timing issues with ZooKeeper. high-availability.zookeeper.path.jobgraphs "/jobgraphs" String ZooKeeper root path (ZNode) for job graphs high-availability.zookeeper.path.running-registry "/running_job_registry/" String Advanced High-availability Kubernetes Options # Key Default Type Description high-availability.kubernetes.leader-election.lease-duration 15 s Duration Define the lease duration for the Kubernetes leader election. The leader will continuously renew its lease time to indicate its existence. And the followers will do a lease checking against the current time. "renewTime + leaseDuration \u0026gt; now" means the leader is alive. high-availability.kubernetes.leader-election.renew-deadline 15 s Duration Defines the deadline duration when the leader tries to renew the lease. The leader will give up its leadership if it cannot successfully renew the lease in the given time. high-availability.kubernetes.leader-election.retry-period 5 s Duration Defines the pause duration between consecutive retries. All the contenders, including the current leader and all other followers, periodically try to acquire/renew the leadership if possible at this interval. Advanced SSL Security Options # Key Default Type Description security.ssl.internal.close-notify-flush-timeout -1 Integer The timeout (in ms) for flushing the \`close_notify\` that was triggered by closing a channel. If the \`close_notify\` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default) security.ssl.internal.handshake-timeout -1 Integer The timeout (in ms) during SSL handshake. (-1 = use system default) security.ssl.internal.session-cache-size -1 Integer The size of the cache used for storing SSL session objects. According to here, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default). security.ssl.internal.session-timeout -1 Integer The timeout (in ms) for the cached SSL session objects. (-1 = use system default) security.ssl.provider "JDK" String The SSL engine provider to use for the ssl transport:JDK: default Java-based SSL engineOPENSSL: openSSL-based SSL engine using system librariesOPENSSL is based on netty-tcnative and comes in two flavours:dynamically linked: This will use your system's openSSL libraries (if compatible) and requires opt/flink-shaded-netty-tcnative-dynamic-*.jar to be copied to lib/statically linked: Due to potential licensing issues with openSSL (see LEGAL-393), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into lib/:
git clone https://github.com/apache/flink-shaded.git \u0026amp;\u0026amp; cd flink-shaded \u0026amp;\u0026amp; mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static Advanced Options for the REST endpoint and Client # Key Default Type Description rest.async.store-duration 5 min Duration Maximum duration that the result of an async operation is stored. Once elapsed the result of the operation can no longer be retrieved. rest.await-leader-timeout 30000 Long The time in ms that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint rest.client.max-content-length 104857600 Integer The maximum content length in bytes that the client will handle. rest.connection-timeout 15000 Long The maximum time in ms for the client to establish a TCP connection. rest.flamegraph.cleanup-interval 10 min Duration Time after which cached stats are cleaned up if not accessed. It can be specified using notation: "100 s", "10 m". rest.flamegraph.delay-between-samples 50 ms Duration Delay between individual stack trace samples taken for building a FlameGraph. It can be specified using notation: "100 ms", "1 s". rest.flamegraph.enabled false Boolean Enables the experimental flame graph feature. rest.flamegraph.num-samples 100 Integer Number of samples to take to build a FlameGraph. rest.flamegraph.refresh-interval 1 min Duration Time after which available stats are deprecated and need to be refreshed (by resampling). It can be specified using notation: "30 s", "1 m". rest.flamegraph.stack-depth 100 Integer Maximum depth of stack traces used to create FlameGraphs. rest.idleness-timeout 300000 Long The maximum time in ms for a connection to stay idle before failing. rest.retry.delay 3000 Long The time in ms that the client waits between retries (See also \`rest.retry.max-attempts\`). rest.retry.max-attempts 20 Integer The number of retries the client will attempt if a retryable operations fails. rest.server.max-content-length 104857600 Integer The maximum content length in bytes that the server will handle. rest.server.numThreads 4 Integer The number of threads for the asynchronous processing of requests. rest.server.thread-priority 5 Integer Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing. Advanced Options for Flink Web UI # Key Default Type Description web.access-control-allow-origin "*" String Access-Control-Allow-Origin header for all responses from the web-frontend. web.cancel.enable true Boolean Flag indicating whether jobs can be canceled from the web-frontend. web.checkpoints.history 10 Integer Number of checkpoints to remember for recent history. web.exception-history-size 16 Integer The maximum number of failures collected by the exception history per job. web.history 5 Integer Number of archived jobs for the JobManager. web.log.path (none) String Path to the log file (may be in /log for standalone but under log directory when using YARN). web.refresh-interval 3000 Long Refresh interval for the web-frontend in milliseconds. web.submit.enable true Boolean Flag indicating whether jobs can be uploaded and run from the web-frontend. web.timeout 600000 Long Timeout for asynchronous operations by the web monitor in milliseconds. web.tmpdir System.getProperty("java.io.tmpdir") String Local directory that is used by the REST API for temporary files. web.upload.dir (none) String Local directory that is used by the REST API for storing uploaded jars. If not specified a dynamic directory will be created under web.tmpdir. Full JobManager Options # JobManager
Key Default Type Description jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task 1 gb MemorySize The average size of data volume to expect each task instance to process if jobmanager.scheduler has been set to AdaptiveBatch. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs or the decided parallelism reaches the jobmanager.adaptive-batch-scheduler.max-parallelism (due to too much data), the data actually processed by some tasks may far exceed this value. jobmanager.adaptive-batch-scheduler.default-source-parallelism 1 Integer The default parallelism of source vertices if jobmanager.scheduler has been set to AdaptiveBatch jobmanager.adaptive-batch-scheduler.max-parallelism 128 Integer The upper bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded down to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.min-parallelism 1 Integer The lower bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded up to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 1 min Duration Controls how long an detected slow node should be blocked for. jobmanager.adaptive-batch-scheduler.speculative.enabled false Boolean Controls whether to enable speculative execution. jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions 2 Integer Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones. jobmanager.adaptive-scheduler.min-parallelism-increase 1 Integer Configure the minimum increase in parallelism for a job to scale up. jobmanager.adaptive-scheduler.resource-stabilization-timeout 10 s Duration The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.
If scheduler-mode is configured to REACTIVE, this configuration value will default to 0, so that jobs are starting immediately with the available resources. jobmanager.adaptive-scheduler.resource-wait-timeout 5 min Duration The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.
Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).
Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.
If scheduler-mode is configured to REACTIVE, this configuration value will default to a negative value to disable the resource timeout. jobmanager.archive.fs.dir (none) String Dictionary for JobManager to store the archives of completed jobs. jobmanager.execution.attempts-history-size 16 Integer The maximum number of historical execution attempts kept in history. jobmanager.execution.failover-strategy "region" String This option specifies how the job computation recovers from task failures. Accepted values are:'full': Restarts all tasks to recover the job.'region': Restarts all tasks that could be affected by the task failure. More details can be found here. jobmanager.future-pool.size (none) Integer The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.io-pool.size (none) Integer The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.resource-id (none) String The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly. jobmanager.retrieve-taskmanager-hostname true Boolean Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to "false", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted. jobmanager.rpc.address (none) String The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.rpc.port 6123 Integer The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.scheduler Default Enum
Determines which scheduler implementation is used to schedule tasks. Accepted values are:'Default': Default scheduler'Adaptive': Adaptive scheduler. More details can be found here.'AdaptiveBatch': Adaptive batch scheduler. More details can be found here.
Possible values:"Default""Adaptive""AdaptiveBatch" jobstore.cache-size 52428800 Long The job store cache size in bytes which is used to keep completed jobs in memory. jobstore.expiration-time 3600 Long The time in seconds after which a completed job expires and is purged from the job store. jobstore.max-capacity 2147483647 Integer The max number of completed jobs that can be kept in the job store. NOTICE: if memory store keeps too many jobs in session cluster, it may cause FullGC or OOM in jm. jobstore.type File Enum
Determines which job store implementation is used in session cluster. Accepted values are:'File': the file job store keeps the archived execution graphs in files'Memory': the memory job store keeps the archived execution graphs in memory. You may need to limit the jobstore.max-capacity to mitigate FullGC or OOM when there are too many graphs
Possible values:"File""Memory" web.exception-history-size 16 Integer The maximum number of failures collected by the exception history per job. Blob Server
The Blob Server is a component in the JobManager. It is used for distribution of objects that are too large to be attached to a RPC message and that benefit from caching (like Jar files or large serialized code objects).
Key Default Type Description blob.client.connect.timeout 0 Integer The connection timeout in milliseconds for the blob client. blob.client.socket.timeout 300000 Integer The socket timeout in milliseconds for the blob client. blob.fetch.backlog 1000 Integer The config parameter defining the desired backlog of BLOB fetches on the JobManager.Note that the operating system usually enforces an upper limit on the backlog size based on the SOMAXCONN setting. blob.fetch.num-concurrent 50 Integer The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves. blob.fetch.retries 5 Integer The config parameter defining number of retires for failed BLOB fetches. blob.offload.minsize 1048576 Integer The minimum size for messages to be offloaded to the BlobServer. blob.server.port "0" String The config parameter defining the server port of the blob service. blob.service.cleanup.interval 3600 Long Cleanup interval of the blob caches at the task managers (in seconds). blob.service.ssl.enabled true Boolean Flag to override ssl support for the blob service transport. blob.storage.directory (none) String The config parameter defining the local storage directory to be used by the blob server. If not configured, then it will default to \u0026lt;WORKING_DIR\u0026gt;/blobStorage. ResourceManager
These configuration keys control basic Resource Manager behavior, independent of the used resource orchestration management framework (YARN, etc.)
Key Default Type Description resourcemanager.job.timeout "5 minutes" String Timeout for jobs which don't have a job manager as leader assigned. resourcemanager.previous-worker.recovery.timeout 0 ms Duration Timeout for resource manager to recover all the previous attempts workers. If exceeded, resource manager will handle new resource requests by requesting new workers. If you would like to reuse the previous workers as much as possible, you should configure a longer timeout time to wait for previous workers to register. resourcemanager.rpc.port 0 Integer Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges. resourcemanager.standalone.start-up-time -1 Long Time in milliseconds of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, slot.request.timeout will be used by default. resourcemanager.start-worker.max-failure-rate 10.0 Double The maximum number of start worker failures (Native Kubernetes / Yarn) per minute before pausing requesting new workers. Once the threshold is reached, subsequent worker requests will be postponed to after a configured retry interval ('resourcemanager.start-worker.retry-interval'). resourcemanager.start-worker.retry-interval 3 s Duration The time to wait before requesting new workers (Native Kubernetes / Yarn) once the max failure rate of starting workers ('resourcemanager.start-worker.max-failure-rate') is reached. resourcemanager.taskmanager-registration.timeout 5 min Duration Timeout for TaskManagers to register at the active resource managers. If exceeded, active resource manager will release and try to re-request the resource for the worker. If not configured, fallback to 'taskmanager.registration.timeout'. resourcemanager.taskmanager-timeout 30000 Long The timeout for an idle task manager to be released. slotmanager.max-total-resource.cpu (none) Double Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.max-total-resource.memory (none) MemorySize Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.number-of-slots.max 2147483647 Integer Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. slotmanager.redundant-taskmanager-num 0 Integer The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn). Full TaskManagerOptions # Please refer to the network memory tuning guide for details on how to use the taskmanager.network.memory.buffer-debloat.* configuration.
Key Default Type Description task.cancellation.interval 30000 Long Time interval between two successive task cancellation attempts in milliseconds. task.cancellation.timeout 180000 Long Timeout in milliseconds after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog. Notice that a task cancellation is different from both a task failure and a clean shutdown. Task cancellation timeout only applies to task cancellation and does not apply to task closing/clean-up caused by a task failure or a clean shutdown. task.cancellation.timers.timeout 7500 Long Time we wait for the timers in milliseconds to finish all pending timer threads when the stream task is cancelled. taskmanager.data.port 0 Integer The task manager’s external port used for data exchange operations. taskmanager.data.ssl.enabled true Boolean Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true taskmanager.debug.memory.log false Boolean Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM. taskmanager.debug.memory.log-interval 5000 Long The interval (in ms) for the log thread to log the current memory usage. taskmanager.host (none) String The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. taskmanager.jvm-exit-on-oom false Boolean Whether to kill the TaskManager when the task thread throws an OutOfMemoryError. taskmanager.memory.min-segment-size 256 bytes MemorySize Minimum possible size of memory buffers used by the network stack and the memory manager. ex. can be used for automatic buffer size adjustment. taskmanager.memory.segment-size 32 kb MemorySize Size of memory buffers used by the network stack and the memory manager. taskmanager.network.bind-policy "ip" String The automatic address binding policy used by the TaskManager if "taskmanager.host" is not set. The value should be one of the following: "name" - uses hostname as binding address"ip" - uses host's ip address as binding address taskmanager.numberOfTaskSlots 1 Integer The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores). taskmanager.registration.timeout 5 min Duration Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates. taskmanager.resource-id (none) String The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the "RpcAddress:RpcPort" and a 6-character random string. Notice that this option is not valid in Yarn and Native Kubernetes mode. taskmanager.rpc.port "0" String The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. taskmanager.slot.timeout 10 s Duration Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no value is configured, then it will fall back to akka.ask.timeout. Data Transport Network Stack
These options are for the network stack that handles the streaming and batch data exchanges between TaskManagers.
Key Default Type Description taskmanager.network.batch-shuffle.compression.enabled true Boolean Boolean flag indicating whether the shuffle data will be compressed for batch shuffle mode. Note that data is compressed per buffer and compression can incur extra CPU overhead, so it is more effective for IO bounded scenario when compression ratio is high. taskmanager.network.blocking-shuffle.type "file" String The blocking shuffle type, either "mmap" or "file". The "auto" means selecting the property type automatically based on system memory architecture (64 bit for mmap and 32 bit for file). Note that the memory usage of mmap is not accounted by configured memory limits, but some resource frameworks like yarn would track this memory usage and kill the container once memory exceeding some threshold. Also note that this option is experimental and might be changed future. taskmanager.network.compression.codec "LZ4" String The codec to be used when compressing shuffle data, only "LZ4", "LZO" and "ZSTD" are supported now. Through tpc-ds test of these three algorithms, the results show that "LZ4" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. "ZSTD" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future. taskmanager.network.detailed-metrics false Boolean Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths. taskmanager.network.max-num-tcp-connections 1 Integer The maximum number of tpc connections between taskmanagers for data communication. taskmanager.network.memory.buffer-debloat.enabled false Boolean The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput. taskmanager.network.memory.buffer-debloat.period 200 ms Duration The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance. taskmanager.network.memory.buffer-debloat.samples 20 Integer The number of the last buffer size values that will be taken for the correct calculation of the new one. taskmanager.network.memory.buffer-debloat.target 1 s Duration The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data. taskmanager.network.memory.buffer-debloat.threshold-percentages 25 Integer The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments. taskmanager.network.memory.buffers-per-channel 2 Integer Number of exclusive network buffers to use for each outgoing/incoming channel (subpartition/input channel) in the credit-based flow control model. It should be configured at least 2 for good performance. 1 buffer is for receiving in-flight data in the subpartition and 1 buffer is for parallel serialization. The minimum valid value that can be configured is 0. When 0 buffers-per-channel is configured, the exclusive network buffers used per downstream incoming channel will be 0, but for each upstream outgoing channel, max(1, configured value) will be used. In other words we ensure that, for performance reasons, there is at least one buffer per outgoing channel regardless of the configuration. taskmanager.network.memory.floating-buffers-per-gate 8 Integer Number of extra network buffers to use for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the input channels. The floating buffers are distributed based on backlog (real-time output buffers in the subpartition) feedback, and can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. This value should be increased in case of higher round trip times between nodes and/or larger number of machines in the cluster. taskmanager.network.memory.max-buffers-per-channel 10 Integer Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data. taskmanager.network.memory.max-overdraft-buffers-per-gate 5 Integer Number of max overdraft network buffers to use for each ResultPartition. The overdraft buffers will be used when the subtask cannot apply to the normal buffers due to back pressure, while subtask is performing an action that can not be interrupted in the middle, like serializing a large record, flatMap operator producing multiple records for one single input record or processing time timer producing large output. In situations like that system will allow subtask to request overdraft buffers, so that the subtask can finish such uninterruptible action, without blocking unaligned checkpoints for long period of time. Overdraft buffers are provided on best effort basis only if the system has some unused buffers available. Subtask that has used overdraft buffers won't be allowed to process any more records until the overdraft buffers are returned to the pool. taskmanager.network.netty.client.connectTimeoutSec 120 Integer The Netty client connection timeout. taskmanager.network.netty.client.numThreads -1 Integer The number of Netty client threads. taskmanager.network.netty.num-arenas -1 Integer The number of Netty arenas. taskmanager.network.netty.sendReceiveBufferSize 0 Integer The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux. taskmanager.network.netty.server.backlog 0 Integer The netty server connection backlog. taskmanager.network.netty.server.numThreads -1 Integer The number of Netty server threads. taskmanager.network.netty.transport "auto" String The Netty transport type, either "nio" or "epoll". The "auto" means selecting the property mode automatically based on the platform. Note that the "epoll" mode can get better performance, less GC and have more advanced features which are only available on modern Linux. taskmanager.network.request-backoff.initial 100 Integer Minimum backoff in milliseconds for partition requests of input channels. taskmanager.network.request-backoff.max 10000 Integer Maximum backoff in milliseconds for partition requests of input channels. taskmanager.network.retries 0 Integer The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections taskmanager.network.sort-shuffle.min-buffers 512 Integer Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value. taskmanager.network.sort-shuffle.min-parallelism 1 Integer Parallelism threshold to switch between sort-based blocking shuffle and hash-based blocking shuffle, which means for batch jobs of smaller parallelism, hash-shuffle will be used and for batch jobs of larger or equal parallelism, sort-shuffle will be used. The value 1 means that sort-shuffle is the default option. Note: For production usage, you may also need to tune 'taskmanager.network.sort-shuffle.min-buffers' and 'taskmanager.memory.framework.off-heap.batch-shuffle.size' for better performance. taskmanager.network.tcp-connection.enable-reuse-across-jobs true Boolean Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. Note that to avoid connection leak, you must set taskmanager.network.max-num-tcp-connections to a smaller value before you enable tcp connection reuse. RPC / Akka # Flink uses Akka for RPC between components (JobManager/TaskManager/ResourceManager). Flink does not use Akka for data transport.
Key Default Type Description akka.ask.callstack true Boolean If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint. akka.ask.timeout 10 s Duration Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d). akka.client-socket-worker-pool.pool-size-factor 1.0 Double The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. akka.client-socket-worker-pool.pool-size-max 2 Integer Max number of threads to cap factor-based number to. akka.client-socket-worker-pool.pool-size-min 1 Integer Min number of threads to cap factor-based number to. akka.fork-join-executor.parallelism-factor 2.0 Double The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values. akka.fork-join-executor.parallelism-max 64 Integer Max number of threads to cap factor-based parallelism number to. akka.fork-join-executor.parallelism-min 8 Integer Min number of threads to cap factor-based parallelism number to. akka.framesize "10485760b" String Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier. akka.jvm-exit-on-fatal-error true Boolean Exit JVM on fatal Akka errors. akka.log.lifecycle.events false Boolean Turns on the Akka’s remote logging of events. Set this value to 'true' in case of debugging. akka.lookup.timeout 10 s Duration Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d). akka.retry-gate-closed-for 50 Long Milliseconds a gate should be closed for after a remote connection was disconnected. akka.server-socket-worker-pool.pool-size-factor 1.0 Double The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. akka.server-socket-worker-pool.pool-size-max 2 Integer Max number of threads to cap factor-based number to. akka.server-socket-worker-pool.pool-size-min 1 Integer Min number of threads to cap factor-based number to. akka.ssl.enabled true Boolean Turns on SSL for Akka’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true. akka.startup-timeout (none) String Timeout after which the startup of a remote component is considered being failed. akka.tcp.timeout "20 s" String Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value. akka.throughput 15 Integer Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness. JVM and Logging Options # Key Default Type Description env.hadoop.conf.dir (none) String Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable. env.hbase.conf.dir (none) String Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable. env.java.opts (none) String Java options to start the JVM of all Flink processes with. env.java.opts.client (none) String Java options to start the JVM of the Flink Client with. env.java.opts.historyserver (none) String Java options to start the JVM of the HistoryServer with. env.java.opts.jobmanager (none) String Java options to start the JVM of the JobManager with. env.java.opts.taskmanager (none) String Java options to start the JVM of the TaskManager with. env.log.dir (none) String Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink’s home) env.log.max 5 Integer The maximum number of old log files to keep. env.pid.dir "/tmp" String Defines the directory where the flink-\u0026lt;host\u0026gt;-\u0026lt;process\u0026gt;.pid files are saved. env.ssh.opts (none) String Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh). env.yarn.conf.dir (none) String Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable. Forwarding Environment Variables # You can configure environment variables to be set on the JobManager and TaskManager processes started on Yarn.
containerized.master.env.: Prefix for passing custom environment variables to Flink\u0026rsquo;s JobManager process. For example for passing LD_LIBRARY_PATH as an env variable to the JobManager, set containerized.master.env.LD_LIBRARY_PATH: \u0026ldquo;/usr/lib/native\u0026rdquo; in the flink-conf.yaml.
containerized.taskmanager.env.: Similar to the above, this configuration prefix allows setting custom environment variables for the workers (TaskManagers).
Deprecated Options # These options relate to parts of Flink that are not actively developed any more. These options may be removed in a future release.
DataSet API Optimizer
Key Default Type Description compiler.delimited-informat.max-line-samples 10 Integer The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters. compiler.delimited-informat.max-sample-len 2097152 Integer The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format’s parameters. compiler.delimited-informat.min-line-samples 2 Integer The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters DataSet API Runtime Algorithms
Key Default Type Description taskmanager.runtime.hashjoin-bloom-filters false Boolean Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles. taskmanager.runtime.large-record-handler false Boolean Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging. taskmanager.runtime.max-fan 128 Integer The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small. taskmanager.runtime.sort-spilling-threshold 0.8 Float A sort operation starts spilling when this fraction of its memory budget is full. DataSet File Sinks
Key Default Type Description fs.output.always-create-directory false Boolean File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to "true", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to "false", the writer will directly create the file directly at the output path, without creating a containing directory. fs.overwrite-files false Boolean Specifies whether file output writers should overwrite existing files by default. Set to "true" to overwrite by default,"false" otherwise. Back to top
`}),e.add({id:72,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/",title:"Custom State Serialization",section:"Data Types \u0026 Serialization",content:` Custom Serialization for Managed State # This page is targeted as a guideline for users who require the use of custom serialization for their state, covering how to provide a custom state serializer as well as guidelines and best practices for implementing serializers that allow state schema evolution.
If you\u0026rsquo;re simply using Flink\u0026rsquo;s own serializers, this page is irrelevant and can be ignored.
Using custom state serializers # When registering a managed operator or keyed state, a StateDescriptor is required to specify the state\u0026rsquo;s name, as well as information about the type of the state. The type information is used by Flink\u0026rsquo;s type serialization framework to create appropriate serializers for the state.
It is also possible to completely bypass this and let Flink use your own custom serializer to serialize managed states, simply by directly instantiating the StateDescriptor with your own TypeSerializer implementation:
Java public class CustomTypeSerializer extends TypeSerializer\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; {...}; ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state-name\u0026#34;, new CustomTypeSerializer()); checkpointedState = getRuntimeContext().getListState(descriptor); Scala class CustomTypeSerializer extends TypeSerializer[(String, Integer)] {...} val descriptor = new ListStateDescriptor[(String, Integer)]( \u0026#34;state-name\u0026#34;, new CustomTypeSerializer) ) checkpointedState = getRuntimeContext.getListState(descriptor) State serializers and schema evolution # This section explains the user-facing abstractions related to state serialization and schema evolution, and necessary internal details about how Flink interacts with these abstractions.
When restoring from savepoints, Flink allows changing the serializers used to read and write previously registered state, so that users are not locked in to any specific serialization schema. When state is restored, a new serializer will be registered for the state (i.e., the serializer that comes with the StateDescriptor used to access the state in the restored job). This new serializer may have a different schema than that of the previous serializer. Therefore, when implementing state serializers, besides the basic logic of reading / writing data, another important thing to keep in mind is how the serialization schema can be changed in the future.
When speaking of schema, in this context the term is interchangeable between referring to the data model of a state type and the serialized binary format of a state type. The schema, generally speaking, can change for a few cases:
Data schema of the state type has evolved, i.e. adding or removing a field from a POJO that is used as state. Generally speaking, after a change to the data schema, the serialization format of the serializer will need to be upgraded. Configuration of the serializer has changed. In order for the new execution to have information about the written schema of state and detect whether or not the schema has changed, upon taking a savepoint of an operator\u0026rsquo;s state, a snapshot of the state serializer needs to be written along with the state bytes. This is abstracted a TypeSerializerSnapshot, explained in the next subsection.
The TypeSerializerSnapshot abstraction # public interface TypeSerializerSnapshot\u0026lt;T\u0026gt; { int getCurrentVersion(); void writeSnapshot(DataOuputView out) throws IOException; void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException; TypeSerializerSchemaCompatibility\u0026lt;T\u0026gt; resolveSchemaCompatibility(TypeSerializer\u0026lt;T\u0026gt; newSerializer); TypeSerializer\u0026lt;T\u0026gt; restoreSerializer(); } public abstract class TypeSerializer\u0026lt;T\u0026gt; { // ... public abstract TypeSerializerSnapshot\u0026lt;T\u0026gt; snapshotConfiguration(); } A serializer\u0026rsquo;s TypeSerializerSnapshot is a point-in-time information that serves as the single source of truth about the state serializer\u0026rsquo;s write schema, as well as any additional information mandatory to restore a serializer that would be identical to the given point-in-time. The logic about what should be written and read at restore time as the serializer snapshot is defined in the writeSnapshot and readSnapshot methods.
Note that the snapshot\u0026rsquo;s own write schema may also need to change over time (e.g. when you wish to add more information about the serializer to the snapshot). To facilitate this, snapshots are versioned, with the current version number defined in the getCurrentVersion method. On restore, when the serializer snapshot is read from savepoints, the version of the schema in which the snapshot was written in will be provided to the readSnapshot method so that the read implementation can handle different versions.
At restore time, the logic that detects whether or not the new serializer\u0026rsquo;s schema has changed should be implemented in the resolveSchemaCompatibility method. When previous registered state is registered again with new serializers in the restored execution of an operator, the new serializer is provided to the previous serializer\u0026rsquo;s snapshot via this method. This method returns a TypeSerializerSchemaCompatibility representing the result of the compatibility resolution, which can be one of the following:
TypeSerializerSchemaCompatibility.compatibleAsIs(): this result signals that the new serializer is compatible, meaning that the new serializer has identical schema with the previous serializer. It is possible that the new serializer has been reconfigured in the resolveSchemaCompatibility method so that it is compatible. TypeSerializerSchemaCompatibility.compatibleAfterMigration(): this result signals that the new serializer has a different serialization schema, and it is possible to migrate from the old schema by using the previous serializer (which recognizes the old schema) to read bytes into state objects, and then rewriting the object back to bytes with the new serializer (which recognizes the new schema). TypeSerializerSchemaCompatibility.incompatible(): this result signals that the new serializer has a different serialization schema, but it is not possible to migrate from the old schema. The last bit of detail is how the previous serializer is obtained in the case that migration is required. Another important role of a serializer\u0026rsquo;s TypeSerializerSnapshot is that it serves as a factory to restore the previous serializer. More specifically, the TypeSerializerSnapshot should implement the restoreSerializer method to instantiate a serializer instance that recognizes the previous serializer\u0026rsquo;s schema and configuration, and can therefore safely read data written by the previous serializer.
How Flink interacts with the TypeSerializer and TypeSerializerSnapshot abstractions # To wrap up, this section concludes how Flink, or more specifically the state backends, interact with the abstractions. The interaction is slightly different depending on the state backend, but this is orthogonal to the implementation of state serializers and their serializer snapshots.
Off-heap state backends (e.g. EmbeddedRocksDBStateBackend) # Register new state with a state serializer that has schema A the registered TypeSerializer for the state is used to read / write state on every state access. State is written in schema A. Take a savepoint The serializer snapshot is extracted via the TypeSerializer#snapshotConfiguration method. The serializer snapshot is written to the savepoint, as well as the already-serialized state bytes (with schema A). Restored execution re-accesses restored state bytes with new state serializer that has schema B The previous state serializer\u0026rsquo;s snapshot is restored. State bytes are not deserialized on restore, only loaded back to the state backends (therefore, still in schema A). Upon receiving the new serializer, it is provided to the restored previous serializer\u0026rsquo;s snapshot via the TypeSerializer#resolveSchemaCompatibility to check for schema compatibility. Migrate state bytes in backend from schema A to schema B If the compatibility resolution reflects that the schema has changed and migration is possible, schema migration is performed. The previous state serializer which recognizes schema A will be obtained from the serializer snapshot, via TypeSerializerSnapshot#restoreSerializer(), and is used to deserialize state bytes to objects, which in turn are re-written again with the new serializer, which recognizes schema B to complete the migration. All entries of the accessed state is migrated all-together before processing continues. If the resolution signals incompatibility, then the state access fails with an exception. Heap state backends (e.g. HashMapStateBackend) # Register new state with a state serializer that has schema A the registered TypeSerializer is maintained by the state backend. Take a savepoint, serializing all state with schema A The serializer snapshot is extracted via the TypeSerializer#snapshotConfiguration method. The serializer snapshot is written to the savepoint. State objects are now serialized to the savepoint, written in schema A. On restore, deserialize state into objects in heap The previous state serializer\u0026rsquo;s snapshot is restored. The previous serializer, which recognizes schema A, is obtained from the serializer snapshot, via TypeSerializerSnapshot#restoreSerializer(), and is used to deserialize state bytes to objects. From now on, all of the state is already deserialized. Restored execution re-accesses previous state with new state serializer that has schema B Upon receiving the new serializer, it is provided to the restored previous serializer\u0026rsquo;s snapshot via the TypeSerializer#resolveSchemaCompatibility to check for schema compatibility. If the compatibility check signals that migration is required, nothing happens in this case since for heap backends, all state is already deserialized into objects. If the resolution signals incompatibility, then the state access fails with an exception. Take another savepoint, serializing all state with schema B Same as step 2., but now state bytes are all in schema B. Predefined convenient TypeSerializerSnapshot classes # Flink provides two abstract base TypeSerializerSnapshot classes that can be used for typical scenarios: SimpleTypeSerializerSnapshot and CompositeTypeSerializerSnapshot.
Serializers that provide these predefined snapshots as their serializer snapshot must always have their own, independent subclass implementation. This corresponds to the best practice of not sharing snapshot classes across different serializers, which is more thoroughly explained in the next section.
Implementing a SimpleTypeSerializerSnapshot # The SimpleTypeSerializerSnapshot is intended for serializers that do not have any state or configuration, essentially meaning that the serialization schema of the serializer is solely defined by the serializer\u0026rsquo;s class.
There will only be 2 possible results of the compatibility resolution when using the SimpleTypeSerializerSnapshot as your serializer\u0026rsquo;s snapshot class:
TypeSerializerSchemaCompatibility.compatibleAsIs(), if the new serializer class remains identical, or TypeSerializerSchemaCompatibility.incompatible(), if the new serializer class is different then the previous one. Below is an example of how the SimpleTypeSerializerSnapshot is used, using Flink\u0026rsquo;s IntSerializer as an example:
public class IntSerializerSnapshot extends SimpleTypeSerializerSnapshot\u0026lt;Integer\u0026gt; { public IntSerializerSnapshot() { super(() -\u0026gt; IntSerializer.INSTANCE); } } The IntSerializer has no state or configurations. Serialization format is solely defined by the serializer class itself, and can only be read by another IntSerializer. Therefore, it suits the use case of the SimpleTypeSerializerSnapshot.
The base super constructor of the SimpleTypeSerializerSnapshot expects a Supplier of instances of the corresponding serializer, regardless of whether the snapshot is currently being restored or being written during snapshots. That supplier is used to create the restore serializer, as well as type checks to verify that the new serializer is of the same expected serializer class.
Implementing a CompositeTypeSerializerSnapshot # The CompositeTypeSerializerSnapshot is intended for serializers that rely on multiple nested serializers for serialization.
Before further explanation, we call the serializer, which relies on multiple nested serializer(s), as the \u0026ldquo;outer\u0026rdquo; serializer in this context. Examples for this could be MapSerializer, ListSerializer, GenericArraySerializer, etc. Consider the MapSerializer, for example - the key and value serializers would be the nested serializers, while MapSerializer itself is the \u0026ldquo;outer\u0026rdquo; serializer.
In this case, the snapshot of the outer serializer should also contain snapshots of the nested serializers, so that the compatibility of the nested serializers can be independently checked. When resolving the compatibility of the outer serializer, the compatibility of each nested serializer needs to be considered.
CompositeTypeSerializerSnapshot is provided to assist in the implementation of snapshots for these kind of composite serializers. It deals with reading and writing the nested serializer snapshots, as well as resolving the final compatibility result taking into account the compatibility of all nested serializers.
Below is an example of how the CompositeTypeSerializerSnapshot is used, using Flink\u0026rsquo;s MapSerializer as an example:
public class MapSerializerSnapshot\u0026lt;K, V\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;Map\u0026lt;K, V\u0026gt;, MapSerializer\u0026gt; { private static final int CURRENT_VERSION = 1; public MapSerializerSnapshot() { super(MapSerializer.class); } public MapSerializerSnapshot(MapSerializer\u0026lt;K, V\u0026gt; mapSerializer) { super(mapSerializer); } @Override public int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected MapSerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;K\u0026gt; keySerializer = (TypeSerializer\u0026lt;K\u0026gt;) nestedSerializers[0]; TypeSerializer\u0026lt;V\u0026gt; valueSerializer = (TypeSerializer\u0026lt;V\u0026gt;) nestedSerializers[1]; return new MapSerializer\u0026lt;\u0026gt;(keySerializer, valueSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(MapSerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getKeySerializer(), outerSerializer.getValueSerializer() }; } } When implementing a new serializer snapshot as a subclass of CompositeTypeSerializerSnapshot, the following three methods must be implemented:
#getCurrentOuterSnapshotVersion(): This method defines the version of the current outer serializer snapshot\u0026rsquo;s serialized binary format. #getNestedSerializers(TypeSerializer): Given the outer serializer, returns its nested serializers. #createOuterSerializerWithNestedSerializers(TypeSerializer[]): Given the nested serializers, create an instance of the outer serializer. The above example is a CompositeTypeSerializerSnapshot where there are no extra information to be snapshotted apart from the nested serializers\u0026rsquo; snapshots. Therefore, its outer snapshot version can be expected to never require an uptick. Some other serializers, however, contains some additional static configuration that needs to be persisted along with the nested component serializer. An example for this would be Flink\u0026rsquo;s GenericArraySerializer, which contains as configuration the class of the array element type, besides the nested element serializer.
In these cases, an additional three methods need to be implemented on the CompositeTypeSerializerSnapshot:
#writeOuterSnapshot(DataOutputView): defines how the outer snapshot information is written. #readOuterSnapshot(int, DataInputView, ClassLoader): defines how the outer snapshot information is read. #resolveOuterSchemaCompatibility(TypeSerializer): checks the compatibility based on the outer snapshot information. By default, the CompositeTypeSerializerSnapshot assumes that there isn\u0026rsquo;t any outer snapshot information to read / write, and therefore have empty default implementations for the above methods. If the subclass has outer snapshot information, then all three methods must be implemented.
Below is an example of how the CompositeTypeSerializerSnapshot is used for composite serializer snapshots that do have outer snapshot information, using Flink\u0026rsquo;s GenericArraySerializer as an example:
public final class GenericArraySerializerSnapshot\u0026lt;C\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;C[], GenericArraySerializer\u0026gt; { private static final int CURRENT_VERSION = 1; private Class\u0026lt;C\u0026gt; componentClass; public GenericArraySerializerSnapshot() { super(GenericArraySerializer.class); } public GenericArraySerializerSnapshot(GenericArraySerializer\u0026lt;C\u0026gt; genericArraySerializer) { super(genericArraySerializer); this.componentClass = genericArraySerializer.getComponentClass(); } @Override protected int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected void writeOuterSnapshot(DataOutputView out) throws IOException { out.writeUTF(componentClass.getName()); } @Override protected void readOuterSnapshot(int readOuterSnapshotVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException { this.componentClass = InstantiationUtil.resolveClassByName(in, userCodeClassLoader); } @Override protected boolean resolveOuterSchemaCompatibility(GenericArraySerializer newSerializer) { return (this.componentClass == newSerializer.getComponentClass()) ? OuterSchemaCompatibility.COMPATIBLE_AS_IS : OuterSchemaCompatibility.INCOMPATIBLE; } @Override protected GenericArraySerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;C\u0026gt; componentSerializer = (TypeSerializer\u0026lt;C\u0026gt;) nestedSerializers[0]; return new GenericArraySerializer\u0026lt;\u0026gt;(componentClass, componentSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(GenericArraySerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getComponentSerializer() }; } } There are two important things to notice in the above code snippet. First of all, since this CompositeTypeSerializerSnapshot implementation has outer snapshot information that is written as part of the snapshot, the outer snapshot version, as defined by getCurrentOuterSnapshotVersion(), must be upticked whenever the serialization format of the outer snapshot information changes.
Second of all, notice how we avoid using Java serialization when writing the component class, by only writing the classname and dynamically loading it when reading back the snapshot. Avoiding Java serialization for writing contents of serializer snapshots is in general a good practice to follow. More details about this is covered in the next section.
Implementation notes and best practices # 1. Flink restores serializer snapshots by instantiating them with their classname # A serializer\u0026rsquo;s snapshot, being the single source of truth for how a registered state was serialized, serves as an entry point to reading state in savepoints. In order to be able to restore and access previous state, the previous state serializer\u0026rsquo;s snapshot must be able to be restored.
Flink restores serializer snapshots by first instantiating the TypeSerializerSnapshot with its classname (written along with the snapshot bytes). Therefore, to avoid being subject to unintended classname changes or instantiation failures, TypeSerializerSnapshot classes should:
avoid being implemented as anonymous classes or nested classes, have a public, nullary constructor for instantiation 2. Avoid sharing the same TypeSerializerSnapshot class across different serializers # Since schema compatibility checks goes through the serializer snapshots, having multiple serializers returning the same TypeSerializerSnapshot class as their snapshot would complicate the implementation for the TypeSerializerSnapshot#resolveSchemaCompatibility and TypeSerializerSnapshot#restoreSerializer() method.
This would also be a bad separation of concerns; a single serializer\u0026rsquo;s serialization schema, configuration, as well as how to restore it, should be consolidated in its own dedicated TypeSerializerSnapshot class.
3. Avoid using Java serialization for serializer snapshot content # Java serialization should not be used at all when writing contents of a persisted serializer snapshot. Take for example, a serializer which needs to persist a class of its target type as part of its snapshot. Information about the class should be persisted by writing the class name, instead of directly serializing the class using Java. When reading the snapshot, the class name is read, and used to dynamically load the class via the name.
This practice ensures that serializer snapshots can always be safely read. In the above example, if the type class was persisted using Java serialization, the snapshot may no longer be readable once the class implementation has changed and is no longer binary compatible according to Java serialization specifics.
Migrating from deprecated serializer snapshot APIs before Flink 1.7 # This section is a guide for API migration from serializers and serializer snapshots that existed before Flink 1.7.
Before Flink 1.7, serializer snapshots were implemented as a TypeSerializerConfigSnapshot (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new TypeSerializerSnapshot interface). Moreover, the responsibility of serializer schema compatibility checks lived within the TypeSerializer, implemented in the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method.
Another major difference between the new and old abstractions is that the deprecated TypeSerializerConfigSnapshot did not have the capability of instantiating the previous serializer. Therefore, in the case where your serializer still returns a subclass of TypeSerializerConfigSnapshot as its snapshot, the serializer instance itself will always be written to savepoints using Java serialization so that the previous serializer may be available at restore time. This is very undesirable, since whether or not restoring the job will be successful is susceptible to availability of the previous serializer\u0026rsquo;s class, or in general, whether or not the serializer instance can be read back at restore time using Java serialization. This means that you be limited to the same serializer for your state, and could be problematic once you want to upgrade serializer classes or perform schema migration.
To be future-proof and have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. The steps to do this is as follows:
Implement a new subclass of TypeSerializerSnapshot. This will be the new snapshot for your serializer. Return the new TypeSerializerSnapshot as the serializer snapshot for your serializer in the TypeSerializer#snapshotConfiguration() method. Restore the job from the savepoint that existed before Flink 1.7, and then take a savepoint again. Note that at this step, the old TypeSerializerConfigSnapshot of the serializer must still exist in the classpath, and the implementation for the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method must not be removed. The purpose of this process is to replace the TypeSerializerConfigSnapshot written in old savepoints with the newly implemented TypeSerializerSnapshot for the serializer. Once you have a savepoint taken with Flink 1.7, the savepoint will contain TypeSerializerSnapshot as the state serializer snapshot, and the serializer instance will no longer be written in the savepoint. At this point, it is now safe to remove all implementations of the old abstraction (remove the old TypeSerializerConfigSnapshot implementation as will as the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) from the serializer). Back to top
`}),e.add({id:73,href:"/flink/flink-docs-master/docs/dev/table/data_stream_api/",title:"DataStream API Integration",section:"Table API \u0026 SQL",content:" DataStream API Integration # Both Table API and DataStream API are equally important when it comes to defining a data processing pipeline.\nThe DataStream API offers the primitives of stream processing (namely time, state, and dataflow management) in a relatively low-level imperative programming API. The Table API abstracts away many internals and provides a structured and declarative API.\nBoth APIs can work with bounded and unbounded streams.\nBounded streams need to be managed when processing historical data. Unbounded streams occur in real-time processing scenarios that might be initialized with historical data first.\nFor efficient execution, both APIs offer processing bounded streams in an optimized batch execution mode. However, since batch is just a special case of streaming, it is also possible to run pipelines of bounded streams in regular streaming execution mode.\nPipelines in one API can be defined end-to-end without dependencies on the other API. However, it might be useful to mix both APIs for various reasons:\nUse the table ecosystem for accessing catalogs or connecting to external systems easily, before implementing the main pipeline in DataStream API. Access some of the SQL functions for stateless data normalization and cleansing, before implementing the main pipeline in DataStream API. Switch to DataStream API every now and then if a more low-level operation (e.g. custom timer handling) is not present in Table API. Flink provides special bridging functionalities to make the integration with DataStream API as smooth as possible.\nSwitching between DataStream and Table API adds some conversion overhead. For example, internal data structures of the table runtime (i.e. RowData) that partially work on binary data need to be converted to more user-friendly data structures (i.e. Row). Usually, this overhead can be neglected but is mentioned here for completeness. Back to top\nConverting between DataStream and Table # Flink provides a specialized StreamTableEnvironment for integrating with the DataStream API. Those environments extend the regular TableEnvironment with additional methods and take the StreamExecutionEnvironment used in the DataStream API as a parameter.\nThe following code shows an example of how to go back and forth between the two APIs. Column names and types of the Table are automatically derived from the TypeInformation of the DataStream. Since the DataStream API does not support changelog processing natively, the code assumes append-only/insert-only semantics during the stream-to-table and table-to-stream conversion.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a DataStream DataStream\u0026lt;String\u0026gt; dataStream = env.fromElements(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;); // interpret the insert-only DataStream as a Table Table inputTable = tableEnv.fromDataStream(dataStream); // register the Table object as a view and query it tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable); Table resultTable = tableEnv.sqlQuery(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;); // interpret the insert-only Table as a DataStream again DataStream\u0026lt;Row\u0026gt; resultStream = tableEnv.toDataStream(resultTable); // add a printing sink and execute in DataStream API resultStream.print(); env.execute(); // prints: // +I[Alice] // +I[Bob] // +I[John] Scala import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment // create environments of both APIs val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // create a DataStream val dataStream = env.fromElements(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;) // interpret the insert-only DataStream as a Table val inputTable = tableEnv.fromDataStream(dataStream) // register the Table object as a view and query it tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable) val resultTable = tableEnv.sqlQuery(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;) // interpret the insert-only Table as a DataStream again val resultStream = tableEnv.toDataStream(resultTable) // add a printing sink and execute in DataStream API resultStream.print() env.execute() // prints: // +I[Alice] // +I[Bob] // +I[John] Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.common.typeinfo import Types env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create a DataStream ds = env.from_collection([\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;], Types.STRING()) # interpret the insert-only DataStream as a Table t = t_env.from_data_stream(ds) # register the Table object as a view and query it t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, t) res_table = t_env.sql_query(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;) # interpret the insert-only Table as a DataStream again res_ds = t_env.to_data_stream(res_table) # add a printing sink and execute in DataStream API res_ds.print() env.execute() # prints: # +I[Alice] # +I[Bob] # +I[John] The complete semantics of fromDataStream and toDataStream can be found in the dedicated section below. In particular, the section discusses how to influence the schema derivation with more complex and nested types. It also covers working with event-time and watermarks.\nDepending on the kind of query, in many cases the resulting dynamic table is a pipeline that does not only produce insert-only changes when converting the Table to a DataStream but also produces retractions and other kinds of updates. During table-to-stream conversion, this could lead to an exception similar to\nTable sink \u0026#39;Unregistered_DataStream_Sink_1\u0026#39; doesn\u0026#39;t support consuming update changes [...]. in which case one needs to revise the query again or switch to toChangelogStream.\nThe following example shows how updating tables can be converted. Every result row represents an entry in a changelog with a change flag that can be queried by calling row.getKind() on it. In the example, the second score for Alice creates an update before (-U) and update after (+U) change.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.of(\u0026#34;Alice\u0026#34;, 12), Row.of(\u0026#34;Bob\u0026#34;, 10), Row.of(\u0026#34;Alice\u0026#34;, 100)); // interpret the insert-only DataStream as a Table Table inputTable = tableEnv.fromDataStream(dataStream).as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;); // register the Table object as a view and query it // the query contains an aggregation that produces updates tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable); Table resultTable = tableEnv.sqlQuery( \u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;); // interpret the updating Table as a changelog DataStream DataStream\u0026lt;Row\u0026gt; resultStream = tableEnv.toChangelogStream(resultTable); // add a printing sink and execute in DataStream API resultStream.print(); env.execute(); // prints: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Scala import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment import org.apache.flink.types.Row // create environments of both APIs val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // create a DataStream val dataStream = env.fromElements( Row.of(\u0026#34;Alice\u0026#34;, Int.box(12)), Row.of(\u0026#34;Bob\u0026#34;, Int.box(10)), Row.of(\u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the insert-only DataStream as a Table val inputTable = tableEnv.fromDataStream(dataStream).as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) // register the Table object as a view and query it // the query contains an aggregation that produces updates tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable) val resultTable = tableEnv.sqlQuery(\u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;) // interpret the updating Table as a changelog DataStream val resultStream = tableEnv.toChangelogStream(resultTable) // add a printing sink and execute in DataStream API resultStream.print() env.execute() // prints: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.common.typeinfo import Types # create environments of both APIs env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create a DataStream ds = env.from_collection([(\u0026#34;Alice\u0026#34;, 12), (\u0026#34;Bob\u0026#34;, 10), (\u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW_NAMED( [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;], [Types.STRING(), Types.INT()])) input_table = t_env.from_data_stream(ds).alias(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) # register the Table object as a view and query it # the query contains an aggregation that produces updates t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, input_table) res_table = t_env.sql_query(\u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;) # interpret the updating Table as a changelog DataStream res_stream = t_env.to_changelog_stream(res_table) # add a printing sink and execute in DataStream API res_stream.print() env.execute() # prints: # +I[Alice, 12] # +I[Bob, 10] # -U[Alice, 12] # +U[Alice, 112] The complete semantics of fromChangelogStream and toChangelogStream can be found in the dedicated section below. In particular, the section discusses how to influence the schema derivation with more complex and nested types. It covers working with event-time and watermarks. It discusses how to declare a primary key and changelog mode for the input and output streams.\nThe example above shows how the final result is computed incrementally by continuously emitting row-wise updates for each incoming record. However, in cases where the input streams are finite (i.e. bounded), a result can be computed more efficiently by leveraging batch processing principles.\nIn batch processing, operators can be executed in successive stages that consume the entire input table before emitting results. For example, a join operator can sort both bounded inputs before performing the actual joining (i.e. sort-merge join algorithm), or build a hash table from one input before consuming the other (i.e. build/probe phase of the hash join algorithm).\nBoth DataStream API and Table API offer a specialized batch runtime mode.\nThe following example illustrates that the unified pipeline is able to process both batch and streaming data by just switching a flag.\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; // setup DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set the batch runtime mode env.setRuntimeMode(RuntimeExecutionMode.BATCH); // uncomment this for streaming mode // env.setRuntimeMode(RuntimeExecutionMode.STREAMING); // setup Table API // the table environment adopts the runtime mode during initialization StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // define the same pipeline as above // prints in BATCH mode: // +I[Bob, 10] // +I[Alice, 112] // prints in STREAMING mode: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Scala import org.apache.flink.api.common.RuntimeExecutionMode import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment // setup DataStream API val env = StreamExecutionEnvironment.getExecutionEnvironment() // set the batch runtime mode env.setRuntimeMode(RuntimeExecutionMode.BATCH) // uncomment this for streaming mode // env.setRuntimeMode(RuntimeExecutionMode.STREAMING) // setup Table API // the table environment adopts the runtime mode during initialization val tableEnv = StreamTableEnvironment.create(env) // define the same pipeline as above // prints in BATCH mode: // +I[Bob, 10] // +I[Alice, 112] // prints in STREAMING mode: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Python from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.table import StreamTableEnvironment # setup DataStream API env = StreamExecutionEnvironment.get_execution_environment() # set the batch runtime mode env.set_runtime_mode(RuntimeExecutionMode.BATCH) # uncomment this for streaming mode # env.set_runtime_mode(RuntimeExecutionMode.STREAMING) # setup Table API # the table environment adopts the runtime mode during initialization table_env = StreamTableEnvironment.create(env) # define the same pipeline as above # prints in BATCH mode: # +I[Bob, 10] # +I[Alice, 112] # prints in STREAMING mode: # +I[Alice, 12] # +I[Bob, 10] # -U[Alice, 12] # +U[Alice, 112] Once the changelog is applied to an external system (e.g. a key-value store), one can see that both modes are able to produce exactly the same output table. By consuming all input data before emitting results, the changelog of the batch mode consists solely of insert-only changes. See also the dedicated batch mode section below for more insights.\nDependencies and Imports # Projects that combine Table API with DataStream API need to add one of the following bridging modules. They include transitive dependencies to flink-table-api-java or flink-table-api-scala and the corresponding language-specific DataStream API module.\nJava \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Scala \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-scala-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; The following imports are required to declare common pipelines using either the Java or Scala version of both DataStream API and Table API.\nJava // imports for Java DataStream API import org.apache.flink.streaming.api.*; import org.apache.flink.streaming.api.environment.*; // imports for Table API with bridging to Java DataStream API import org.apache.flink.table.api.*; import org.apache.flink.table.api.bridge.java.*; Scala // imports for Scala DataStream API import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala._ // imports for Table API with bridging to Scala DataStream API import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala._ Python # imports for Python DataStream API from pyflink.datastream import * # imports for Table API to Python DataStream API from pyflink.table import * Please refer to the configuration section for more information.\nConfiguration # The TableEnvironment will adopt all configuration options from the passed StreamExecutionEnvironment. However, it cannot be guaranteed that further changes to the configuration of StreamExecutionEnvironment are propagated to the StreamTableEnvironment after its instantiation. The propagation of options from Table API to DataStream API happens during planning.\nWe recommend setting all configuration options in DataStream API early before switching to Table API.\nJava import java.time.ZoneId; import org.apache.flink.streaming.api.CheckpointingMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; // create Java DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set various configuration early env.setMaxParallelism(256); env.getConfig().addDefaultKryoSerializer(MyCustomType.class, CustomKryoSerializer.class); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // then switch to Java Table API StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // set configuration early tableEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;Europe/Berlin\u0026#34;)); // start defining your pipelines in both APIs... Scala import java.time.ZoneId import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.api.CheckpointingMode import org.apache.flink.table.api.bridge.scala._ // create Scala DataStream API val env = StreamExecutionEnvironment.getExecutionEnvironment // set various configuration early env.setMaxParallelism(256) env.getConfig.addDefaultKryoSerializer(classOf[MyCustomType], classOf[CustomKryoSerializer]) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // then switch to Scala Table API val tableEnv = StreamTableEnvironment.create(env) // set configuration early tableEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;Europe/Berlin\u0026#34;)) // start defining your pipelines in both APIs... Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.datastream.checkpointing_mode import CheckpointingMode # create Python DataStream API env = StreamExecutionEnvironment.get_execution_environment() # set various configuration early env.set_max_parallelism(256) env.get_config().add_default_kryo_serializer(\u0026#34;type_class_name\u0026#34;, \u0026#34;serializer_class_name\u0026#34;) env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE) # then switch to Python Table API t_env = StreamTableEnvironment.create(env) # set configuration early t_env.get_config().set_local_timezone(\u0026#34;Europe/Berlin\u0026#34;) # start defining your pipelines in both APIs... Execution Behavior # Both APIs provide methods to execute pipelines. In other words: if requested, they compile a job graph that will be submitted to the cluster and triggered for execution. Results will be streamed to the declared sinks.\nUsually, both APIs mark such behavior with the term execute in method names. However, the execution behavior is slightly different between Table API and DataStream API.\nDataStream API\nThe DataStream API\u0026rsquo;s StreamExecutionEnvironment uses a builder pattern to construct a complex pipeline. The pipeline possibly splits into multiple branches that might or might not end with a sink. The environment buffers all these defined branches until it comes to job submission.\nStreamExecutionEnvironment.execute() submits the entire constructed pipeline and clears the builder afterward. In other words: no sources and sinks are declared anymore, and a new pipeline can be added to the builder. Thus, every DataStream program usually ends with a call to StreamExecutionEnvironment.execute(). Alternatively, DataStream.executeAndCollect() implicitly defines a sink for streaming the results to the local client.\nTable API\nIn the Table API, branching pipelines are only supported within a StatementSet where each branch must declare a final sink. Both TableEnvironment and also StreamTableEnvironment do not offer a dedicated general execute() method. Instead, they offer methods for submitting a single source-to-sink pipeline or a statement set:\nJava // execute with explicit sink tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable\u0026#34;).execute(); tableEnv.executeSql(\u0026#34;INSERT INTO OutputTable SELECT * FROM InputTable\u0026#34;); tableEnv.createStatementSet() .add(tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable\u0026#34;)) .add(tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable2\u0026#34;)) .execute(); tableEnv.createStatementSet() .addInsertSql(\u0026#34;INSERT INTO OutputTable SELECT * FROM InputTable\u0026#34;) .addInsertSql(\u0026#34;INSERT INTO OutputTable2 SELECT * FROM InputTable\u0026#34;) .execute(); // execute with implicit local sink tableEnv.from(\u0026#34;InputTable\u0026#34;).execute().print(); tableEnv.executeSql(\u0026#34;SELECT * FROM InputTable\u0026#34;).print(); Python # execute with explicit sink table_env.from_path(\u0026#34;input_table\u0026#34;).execute_insert(\u0026#34;output_table\u0026#34;) table_env.execute_sql(\u0026#34;INSERT INTO output_table SELECT * FROM input_table\u0026#34;) table_env.create_statement_set() \\ .add_insert(\u0026#34;output_table\u0026#34;, input_table) \\ .add_insert(\u0026#34;output_table2\u0026#34;, input_table) \\ .execute() table_env.create_statement_set() \\ .add_insert_sql(\u0026#34;INSERT INTO output_table SELECT * FROM input_table\u0026#34;) \\ .add_insert_sql(\u0026#34;INSERT INTO output_table2 SELECT * FROM input_table\u0026#34;) \\ .execute() # execute with implicit local sink table_env.from_path(\u0026#34;input_table\u0026#34;).execute().print() table_env.execute_sql(\u0026#34;SELECT * FROM input_table\u0026#34;).print() To combine both execution behaviors, every call to StreamTableEnvironment.toDataStream or StreamTableEnvironment.toChangelogStream will materialize (i.e. compile) the Table API sub-pipeline and insert it into the DataStream API pipeline builder. This means that StreamExecutionEnvironment.execute() or DataStream.executeAndCollect must be called afterwards. An execution in Table API will not trigger these \u0026ldquo;external parts\u0026rdquo;.\nJava // (1) // adds a branch with a printing sink to the StreamExecutionEnvironment tableEnv.toDataStream(table).print(); // (2) // executes a Table API end-to-end pipeline as a Flink job and prints locally, // thus (1) has still not been executed table.execute().print(); // executes the DataStream API pipeline with the sink defined in (1) as a // Flink job, (2) was already running before env.execute(); Python # (1) # adds a branch with a printing sink to the StreamExecutionEnvironment table_env.to_data_stream(table).print() # (2) # executes a Table API end-to-end pipeline as a Flink job and prints locally, # thus (1) has still not been executed table.execute().print() # executes the DataStream API pipeline with the sink defined in (1) as a # Flink job, (2) was already running before env.execute() Back to top\nBatch Runtime Mode # The batch runtime mode is a specialized execution mode for bounded Flink programs.\nGenerally speaking, boundedness is a property of a data source that tells us whether all the records coming from that source are known before execution or whether new data will show up, potentially indefinitely. A job, in turn, is bounded if all its sources are bounded, and unbounded otherwise.\nStreaming runtime mode, on the other hand, can be used for both bounded and unbounded jobs.\nFor more information on the different execution modes, see also the corresponding DataStream API section.\nThe Table API \u0026amp; SQL planner provides a set of specialized optimizer rules and runtime operators for either of the two modes.\nCurrently, the runtime mode is not derived automatically from sources, thus, it must be set explicitly or will be adopted from StreamExecutionEnvironment when instantiating a StreamTableEnvironment:\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.table.api.EnvironmentSettings; // adopt mode from StreamExecutionEnvironment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // or // set mode explicitly for StreamTableEnvironment // it will be propagated to StreamExecutionEnvironment during planning StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, EnvironmentSettings.inBatchMode()); Scala import org.apache.flink.api.common.RuntimeExecutionMode import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment import org.apache.flink.table.api.EnvironmentSettings // adopt mode from StreamExecutionEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment env.setRuntimeMode(RuntimeExecutionMode.BATCH) val tableEnv = StreamTableEnvironment.create(env) // or // set mode explicitly for StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env, EnvironmentSettings.inBatchMode) Python from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.table import EnvironmentSettings, StreamTableEnvironment # adopt mode from StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) table_env = StreamTableEnvironment.create(env) # or # set mode explicitly for StreamTableEnvironment # it will be propagated to StreamExecutionEnvironment during planning env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env, EnvironmentSettings.in_batch_mode()) One must meet the following prerequisites before setting the runtime mode to BATCH:\nAll sources must declare themselves as bounded.\nCurrently, table sources must emit insert-only changes.\nOperators need a sufficient amount of off-heap memory for sorting and other intermediate results.\nAll table operations must be available in batch mode. Currently, some of them are only available in streaming mode. Please check the corresponding Table API \u0026amp; SQL pages.\nA batch execution has the following implications (among others):\nProgressive watermarks are neither generated nor used in operators. However, sources emit a maximum watermark before shutting down.\nExchanges between tasks might be blocking according to the execution.batch-shuffle-mode. This also means potentially less resource requirements compared to executing the same pipeline in streaming mode.\nCheckpointing is disabled. Artificial state backends are inserted.\nTable operations don\u0026rsquo;t produce incremental updates but only a complete final result which converts to an insert-only changelog stream.\nSince batch processing can be considered as a special case of stream processing, we recommend implementing a streaming pipeline first as it is the most general implementation for both bounded and unbounded data.\nIn theory, a streaming pipeline can execute all operators. However, in practice, some operations might not make much sense as they would lead to ever-growing state and are therefore not supported. A global sort would be an example that is only available in batch mode. Simply put: it should be possible to run a working streaming pipeline in batch mode but not necessarily vice versa.\nThe following example shows how to play around with batch mode using the DataGen table source. Many sources offer options that implicitly make the connector bounded, for example, by defining a terminating offset or timestamp. In our example, we limit the number of rows with the number-of-rows option.\nJava import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.TableDescriptor; Table table = tableEnv.from( TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) // make the source bounded .schema( Schema.newBuilder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()); // convert the Table to a DataStream and further transform the pipeline tableEnv.toDataStream(table) .keyBy(r -\u0026gt; r.\u0026lt;Byte\u0026gt;getFieldAs(\u0026#34;uid\u0026#34;)) .map(r -\u0026gt; \u0026#34;My custom operator: \u0026#34; + r.\u0026lt;String\u0026gt;getFieldAs(\u0026#34;payload\u0026#34;)) .executeAndCollect() .forEachRemaining(System.out::println); // prints: // My custom operator: 9660912d30a43c7b035e15bd... // My custom operator: 29f5f706d2144f4a4f9f52a0... // ... Scala import org.apache.flink.api.scala._ import org.apache.flink.table.api._ val table = tableEnv.from( TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) // make the source bounded .schema( Schema.newBuilder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()) // convert the Table to a DataStream and further transform the pipeline tableEnv.toDataStream(table) .keyBy(r =\u0026gt; r.getFieldAs[Byte](\u0026#34;uid\u0026#34;)) .map(r =\u0026gt; \u0026#34;My custom operator: \u0026#34; + r.getFieldAs[String](\u0026#34;payload\u0026#34;)) .executeAndCollect() .foreach(println) // prints: // My custom operator: 9660912d30a43c7b035e15bd... // My custom operator: 29f5f706d2144f4a4f9f52a0... // ... Python from pyflink.table import TableDescriptor, Schema, DataTypes table = table_env.from_descriptor( TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) .schema( Schema.new_builder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()) # convert the Table to a DataStream and further transform the pipeline collect = table_env.to_data_stream(table) \\ .key_by(lambda r: r[0]) \\ .map(lambda r: \u0026#34;My custom operator: \u0026#34; + r[1]) \\ .execute_and_collect() for c in collect: print(c) # prints: # My custom operator: 9660912d30a43c7b035e15bd... # My custom operator: 29f5f706d2144f4a4f9f52a0... # ... Changelog Unification # In most cases, the pipeline definition itself can remain constant in both Table API and DataStream API when switching from streaming to batch mode and vice versa. However, as mentioned before, the resulting changelog streams might differ due to the avoidance of incremental operations in batch mode.\nTime-based operations that rely on event-time and leverage watermarks as a completeness marker are able to produce an insert-only changelog stream that is independent of the runtime mode.\nThe following Java example illustrates a Flink program that is not only unified on an API level but also in the resulting changelog stream. The example joins two tables in SQL (UserTable and OrderTable) using an interval join based on the time attributes in both tables (ts). It uses DataStream API to implement a custom operator that deduplicates the user name using a KeyedProcessFunction and value state.\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; import org.apache.flink.util.Collector; // setup DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // use BATCH or STREAMING mode env.setRuntimeMode(RuntimeExecutionMode.BATCH); // setup Table API StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a user stream DataStream\u0026lt;Row\u0026gt; userStream = env .fromElements( Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:00:00\u0026#34;), 1, \u0026#34;Alice\u0026#34;), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:05:00\u0026#34;), 2, \u0026#34;Bob\u0026#34;), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:10:00\u0026#34;), 2, \u0026#34;Bob\u0026#34;)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;ts\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;name\u0026#34;}, Types.LOCAL_DATE_TIME, Types.INT, Types.STRING)); // create an order stream DataStream\u0026lt;Row\u0026gt; orderStream = env .fromElements( Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:02:00\u0026#34;), 1, 122), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:07:00\u0026#34;), 2, 239), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:11:00\u0026#34;), 2, 999)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;ts\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;amount\u0026#34;}, Types.LOCAL_DATE_TIME, Types.INT, Types.INT)); // create corresponding tables tableEnv.createTemporaryView( \u0026#34;UserTable\u0026#34;, userStream, Schema.newBuilder() .column(\u0026#34;ts\u0026#34;, DataTypes.TIMESTAMP(3)) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()); tableEnv.createTemporaryView( \u0026#34;OrderTable\u0026#34;, orderStream, Schema.newBuilder() .column(\u0026#34;ts\u0026#34;, DataTypes.TIMESTAMP(3)) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()); // perform interval join Table joinedTable = tableEnv.sqlQuery( \u0026#34;SELECT U.name, O.amount \u0026#34; + \u0026#34;FROM UserTable U, OrderTable O \u0026#34; + \u0026#34;WHERE U.uid = O.uid AND O.ts BETWEEN U.ts AND U.ts + INTERVAL \u0026#39;5\u0026#39; MINUTES\u0026#34;); DataStream\u0026lt;Row\u0026gt; joinedStream = tableEnv.toDataStream(joinedTable); joinedStream.print(); // implement a custom operator using ProcessFunction and value state joinedStream .keyBy(r -\u0026gt; r.\u0026lt;String\u0026gt;getFieldAs(\u0026#34;name\u0026#34;)) .process( new KeyedProcessFunction\u0026lt;String, Row, String\u0026gt;() { ValueState\u0026lt;String\u0026gt; seen; @Override public void open(Configuration parameters) { seen = getRuntimeContext().getState( new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;seen\u0026#34;, String.class)); } @Override public void processElement(Row row, Context ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { String name = row.getFieldAs(\u0026#34;name\u0026#34;); if (seen.value() == null) { seen.update(name); out.collect(name); } } }) .print(); // execute unified pipeline env.execute(); // prints (in both BATCH and STREAMING mode): // +I[Bob, 239] // +I[Alice, 122] // +I[Bob, 999] // // Bob // Alice Python from datetime import datetime from pyflink.common import Row, Types from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode, KeyedProcessFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor from pyflink.table import StreamTableEnvironment, Schema, DataTypes # setup DataStream API env = StreamExecutionEnvironment.get_execution_environment() # use BATCH or STREAMING mode env.set_runtime_mode(RuntimeExecutionMode.BATCH) # setup Table API table_env = StreamTableEnvironment.create(env) # create a user stream t_format = \u0026#34;%Y-%m-%dT%H:%M:%S\u0026#34; user_stream = env.from_collection( [Row(datetime.strptime(\u0026#34;2021-08-21T13:00:00\u0026#34;, t_format), 1, \u0026#34;Alice\u0026#34;), Row(datetime.strptime(\u0026#34;2021-08-21T13:05:00\u0026#34;, t_format), 2, \u0026#34;Bob\u0026#34;), Row(datetime.strptime(\u0026#34;2021-08-21T13:10:00\u0026#34;, t_format), 2, \u0026#34;Bob\u0026#34;)], type_info=Types.ROW_NAMED([\u0026#34;ts1\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;name\u0026#34;], [Types.SQL_TIMESTAMP(), Types.INT(), Types.STRING()])) # create an order stream order_stream = env.from_collection( [Row(datetime.strptime(\u0026#34;2021-08-21T13:02:00\u0026#34;, t_format), 1, 122), Row(datetime.strptime(\u0026#34;2021-08-21T13:07:00\u0026#34;, t_format), 2, 239), Row(datetime.strptime(\u0026#34;2021-08-21T13:11:00\u0026#34;, t_format), 2, 999)], type_info=Types.ROW_NAMED([\u0026#34;ts1\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;amount\u0026#34;], [Types.SQL_TIMESTAMP(), Types.INT(), Types.INT()])) # # create corresponding tables table_env.create_temporary_view( \u0026#34;user_table\u0026#34;, user_stream, Schema.new_builder() .column_by_expression(\u0026#34;ts\u0026#34;, \u0026#34;CAST(ts1 AS TIMESTAMP(3))\u0026#34;) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()) table_env.create_temporary_view( \u0026#34;order_table\u0026#34;, order_stream, Schema.new_builder() .column_by_expression(\u0026#34;ts\u0026#34;, \u0026#34;CAST(ts1 AS TIMESTAMP(3))\u0026#34;) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()) # perform interval join joined_table = table_env.sql_query( \u0026#34;SELECT U.name, O.amount \u0026#34; + \u0026#34;FROM user_table U, order_table O \u0026#34; + \u0026#34;WHERE U.uid = O.uid AND O.ts BETWEEN U.ts AND U.ts + INTERVAL \u0026#39;5\u0026#39; MINUTES\u0026#34;) joined_stream = table_env.to_data_stream(joined_table) joined_stream.print() # implement a custom operator using ProcessFunction and value state class MyProcessFunction(KeyedProcessFunction): def __init__(self): self.seen = None def open(self, runtime_context: RuntimeContext): state_descriptor = ValueStateDescriptor(\u0026#34;seen\u0026#34;, Types.STRING()) self.seen = runtime_context.get_state(state_descriptor) def process_element(self, value, ctx): name = value[0] if self.seen.value() is None: self.seen.update(name) yield name joined_stream \\ .key_by(lambda r: r[0]) \\ .process(MyProcessFunction()) \\ .print() # execute unified pipeline env.execute() # prints (in both BATCH and STREAMING mode): # +I[Bob, 239] # +I[Alice, 122] # +I[Bob, 999] # # Bob # Alice Back to top\nHandling of (Insert-Only) Streams # A StreamTableEnvironment offers the following methods to convert from and to DataStream API:\nfromDataStream(DataStream): Interprets a stream of insert-only changes and arbitrary type as a table. Event-time and watermarks are not propagated by default.\nfromDataStream(DataStream, Schema): Interprets a stream of insert-only changes and arbitrary type as a table. The optional schema allows to enrich column data types and add time attributes, watermarks strategies, other computed columns, or primary keys.\ncreateTemporaryView(String, DataStream): Registers the stream under a name to access it in SQL. It is a shortcut for createTemporaryView(String, fromDataStream(DataStream)).\ncreateTemporaryView(String, DataStream, Schema): Registers the stream under a name to access it in SQL. It is a shortcut for createTemporaryView(String, fromDataStream(DataStream, Schema)).\ntoDataStream(Table): Converts a table into a stream of insert-only changes. The default stream record type is org.apache.flink.types.Row. A single rowtime attribute column is written back into the DataStream API\u0026rsquo;s record. Watermarks are propagated as well.\ntoDataStream(Table, AbstractDataType): Converts a table into a stream of insert-only changes. This method accepts a data type to express the desired stream record type. The planner might insert implicit casts and reorders columns to map columns to fields of the (possibly nested) data type.\ntoDataStream(Table, Class): A shortcut for toDataStream(Table, DataTypes.of(Class)) to quickly create the desired data type reflectively.\nFrom a Table API\u0026rsquo;s perspective, converting from and to DataStream API is similar to reading from or writing to a virtual table connector that has been defined using a CREATE TABLE DDL in SQL.\nThe schema part in the virtual CREATE TABLE name (schema) WITH (options) statement can be automatically derived from the DataStream\u0026rsquo;s type information, enriched, or entirely defined manually using org.apache.flink.table.api.Schema.\nThe virtual DataStream table connector exposes the following metadata for every row:\nKey Data Type Description R/W rowtime TIMESTAMP_LTZ(3) NOT NULL Stream record's timestamp. R/W The virtual DataStream table source implements SupportsSourceWatermark and thus allows calling the SOURCE_WATERMARK() built-in function as a watermark strategy to adopt watermarks from the DataStream API.\nExamples for fromDataStream # The following code shows how to use fromDataStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import java.time.Instant; // some example POJO public static class User { public String name; public Integer score; public Instant event_time; // default constructor for DataStream API public User() {} // fully assigning constructor for Table API public User(String name, Integer score, Instant event_time) { this.name = name; this.score = score; this.event_time = event_time; } } // create a DataStream DataStream\u0026lt;User\u0026gt; dataStream = env.fromElements( new User(\u0026#34;Alice\u0026#34;, 4, Instant.ofEpochMilli(1000)), new User(\u0026#34;Bob\u0026#34;, 6, Instant.ofEpochMilli(1001)), new User(\u0026#34;Alice\u0026#34;, 10, Instant.ofEpochMilli(1002))); // === EXAMPLE 1 === // derive all physical columns automatically Table table = tableEnv.fromDataStream(dataStream); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9) // ) // === EXAMPLE 2 === // derive all physical columns automatically // but add computed columns (in this case for creating a proctime attribute column) Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT NOT NULL, // `event_time` TIMESTAMP_LTZ(9), // `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() //) // === EXAMPLE 3 === // derive all physical columns automatically // but add computed columns (in this case for creating a rowtime attribute column) // and a custom watermark strategy Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND // ) // === EXAMPLE 4 === // derive all physical columns automatically // but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column // also rely on the watermarks generated in the DataStream API // we assume that a watermark strategy has been defined for `dataStream` before // (not part of this example) Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() // ) // === EXAMPLE 5 === // define physical columns manually // in this example, // - we can reduce the default precision of timestamps from 9 to 3 // - we also project the columns and put `event_time` to the beginning Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, // `name` VARCHAR(200), // `score` INT // ) // note: the watermark strategy is not shown due to the inserted column reordering projection Scala import org.apache.flink.api.scala._ import java.time.Instant // some example case class case class User(name: String, score: java.lang.Integer, event_time: java.time.Instant) // create a DataStream val dataStream = env.fromElements( User(\u0026#34;Alice\u0026#34;, 4, Instant.ofEpochMilli(1000)), User(\u0026#34;Bob\u0026#34;, 6, Instant.ofEpochMilli(1001)), User(\u0026#34;Alice\u0026#34;, 10, Instant.ofEpochMilli(1002))) // === EXAMPLE 1 === // derive all physical columns automatically val table = tableEnv.fromDataStream(dataStream) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9) // ) // === EXAMPLE 2 === // derive all physical columns automatically // but add computed columns (in this case for creating a proctime attribute column) val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT NOT NULL, // `event_time` TIMESTAMP_LTZ(9), // `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() //) // === EXAMPLE 3 === // derive all physical columns automatically // but add computed columns (in this case for creating a rowtime attribute column) // and a custom watermark strategy val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND // ) // === EXAMPLE 4 === // derive all physical columns automatically // but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column // also rely on the watermarks generated in the DataStream API // we assume that a watermark strategy has been defined for `dataStream` before // (not part of this example) val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() // ) // === EXAMPLE 5 === // define physical columns manually // in this example, // - we can reduce the default precision of timestamps from 9 to 3 // - we also project the columns and put `event_time` to the beginning val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.printSchema() // prints: // ( // `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, // `name` VARCHAR(200), // `score` INT // ) // note: the watermark strategy is not shown due to the inserted column reordering projection Python from pyflink.common.time import Instant from pyflink.common.types import Row from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) ds = env.from_collection([ Row(\u0026#34;Alice\u0026#34;, 12, Instant.of_epoch_milli(1000)), Row(\u0026#34;Bob\u0026#34;, 5, Instant.of_epoch_milli(1001)), Row(\u0026#34;Alice\u0026#34;, 10, Instant.of_epoch_milli(1002))], type_info=Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;score\u0026#39;, \u0026#39;event_time\u0026#39;], [Types.STRING(), Types.INT(), Types.INSTANT()])) # === EXAMPLE 1 === # derive all physical columns automatically table = t_env.from_data_stream(ds) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9) # ) # === EXAMPLE 2 === # derive all physical columns automatically # but add computed columns (in this case for creating a proctime attribute column) table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_expression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() # ) # === EXAMPLE 3 === # derive all physical columns automatically # but add computed columns (in this case for creating a rowtime attribute column) # and a custom watermark strategy table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_expression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), # WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND # ) # === EXAMPLE 4 === # derive all physical columns automatically # but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column # also rely on the watermarks generated in the DataStream API # we assume that a watermark strategy has been defined for `dataStream` before # (not part of this example) table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_metadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, # WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() # ) # === EXAMPLE 5 === # define physical columns manually # in this example, # - we can reduce the default precision of timestamps from 9 to 3 # - we also project the columns and put `event_time` to the beginning table = t_env.from_data_stream( ds, Schema.new_builder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.print_schema() # prints: # ( # `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, # `name` STRING, # `score` INT # ) # note: the watermark strategy is not shown due to the inserted column reordering projection Example 1 illustrates a simple use case when no time-based operations are needed.\nExample 4 is the most common use case when time-based operations such as windows or interval joins should be part of the pipeline. Example 2 is the most common use case when these time-based operations should work in processing time.\nExample 5 entirely relies on the declaration of the user. This can be useful to replace generic types from the DataStream API (which would be RAW in the Table API) with proper data types.\nSince DataType is richer than TypeInformation, we can easily enable immutable POJOs and other complex data structures. The following example in Java shows what is possible. Check also the Data Types \u0026amp; Serialization page of the DataStream API for more information about the supported types there.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; // the DataStream API does not support immutable POJOs yet, // the class will result in a generic type that is a RAW type in Table API by default public static class User { public final String name; public final Integer score; public User(String name, Integer score) { this.name = name; this.score = score; } } // create a DataStream DataStream\u0026lt;User\u0026gt; dataStream = env.fromElements( new User(\u0026#34;Alice\u0026#34;, 4), new User(\u0026#34;Bob\u0026#34;, 6), new User(\u0026#34;Alice\u0026#34;, 10)); // since fields of a RAW type cannot be accessed, every stream record is treated as an atomic type // leading to a table with a single column `f0` Table table = tableEnv.fromDataStream(dataStream); table.printSchema(); // prints: // ( // `f0` RAW(\u0026#39;User\u0026#39;, \u0026#39;...\u0026#39;) // ) // instead, declare a more useful data type for columns using the Table API\u0026#39;s type system // in a custom schema and rename the columns in a following `as` projection Table table = tableEnv .fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.of(User.class)) .build()) .as(\u0026#34;user\u0026#34;); table.printSchema(); // prints: // ( // `user` *User\u0026lt;`name` STRING,`score` INT\u0026gt;* // ) // data types can be extracted reflectively as above or explicitly defined Table table3 = tableEnv .fromDataStream( dataStream, Schema.newBuilder() .column( \u0026#34;f0\u0026#34;, DataTypes.STRUCTURED( User.class, DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()))) .build()) .as(\u0026#34;user\u0026#34;); table.printSchema(); // prints: // ( // `user` *User\u0026lt;`name` STRING,`score` INT\u0026gt;* // ) Python Custom PoJo Class is unsupported in PyFlink now. Examples for createTemporaryView # A DataStream can be registered directly as a view (possibly enriched with a schema).\nViews created from a DataStream can only be registered as temporary views. Due to their inline/anonymous nature, it is not possible to register them in a permanent catalog. The following code shows how to use createTemporaryView for different scenarios.\nJava import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; // create some DataStream DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; dataStream = env.fromElements( Tuple2.of(12L, \u0026#34;Alice\u0026#34;), Tuple2.of(0L, \u0026#34;Bob\u0026#34;)); // === EXAMPLE 1 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session // all columns are derived automatically tableEnv.createTemporaryView(\u0026#34;MyView\u0026#34;, dataStream); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `f0` BIGINT NOT NULL, // `f1` STRING // ) // === EXAMPLE 2 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, // provide a schema to adjust the columns similar to `fromDataStream` // in this example, the derived NOT NULL information has been removed tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, dataStream, Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;f1\u0026#34;, \u0026#34;STRING\u0026#34;) .build()); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `f0` BIGINT, // `f1` STRING // ) // === EXAMPLE 3 === // use the Table API before creating the view if it is only about renaming columns tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, tableEnv.fromDataStream(dataStream).as(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `id` BIGINT NOT NULL, // `name` STRING // ) Scala // create some DataStream val dataStream: DataStream[(Long, String)] = env.fromElements( (12L, \u0026#34;Alice\u0026#34;), (0L, \u0026#34;Bob\u0026#34;)) // === EXAMPLE 1 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session // all columns are derived automatically tableEnv.createTemporaryView(\u0026#34;MyView\u0026#34;, dataStream) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `_1` BIGINT NOT NULL, // `_2` STRING // ) // === EXAMPLE 2 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, // provide a schema to adjust the columns similar to `fromDataStream` // in this example, the derived NOT NULL information has been removed tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, dataStream, Schema.newBuilder() .column(\u0026#34;_1\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;_2\u0026#34;, \u0026#34;STRING\u0026#34;) .build()) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `_1` BIGINT, // `_2` STRING // ) // === EXAMPLE 3 === // use the Table API before creating the view if it is only about renaming columns tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, tableEnv.fromDataStream(dataStream).as(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `id` BIGINT NOT NULL, // `name` STRING // ) Python from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import DataTypes, StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) ds = env.from_collection([(12, \u0026#34;Alice\u0026#34;), (0, \u0026#34;Bob\u0026#34;)], type_info=Types.TUPLE([Types.LONG(), Types.STRING()])) # === EXAMPLE 1 === # register the DataStream as view \u0026#34;MyView\u0026#34; in the current session # all columns are derived automatically t_env.create_temporary_view(\u0026#34;MyView\u0026#34;, ds) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `f0` BIGINT NOT NULL, # `f1` STRING # ) # === EXAMPLE 2 === # register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, # provide a schema to adjust the columns similar to `fromDataStream` # in this example, the derived NOT NULL information has been removed t_env.create_temporary_view( \u0026#34;MyView\u0026#34;, ds, Schema.new_builder() .column(\u0026#34;f0\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;f1\u0026#34;, \u0026#34;STRING\u0026#34;) .build()) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `f0` BIGINT, # `f1` STRING # ) # === EXAMPLE 3 === # use the Table API before creating the view if it is only about renaming columns t_env.create_temporary_view( \u0026#34;MyView\u0026#34;, t_env.from_data_stream(ds).alias(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `id` BIGINT NOT NULL, # `name` STRING # ) Back to top\nExamples for toDataStream # The following code shows how to use toDataStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Table; import org.apache.flink.types.Row; import java.time.Instant; // POJO with mutable fields // since no fully assigning constructor is defined, the field order // is alphabetical [event_time, name, score] public static class User { public String name; public Integer score; public Instant event_time; } tableEnv.executeSql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); Table table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;); // === EXAMPLE 1 === // use the default conversion to instances of Row // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toDataStream(table); // === EXAMPLE 2 === // a data type is extracted from class `User`, // the planner reorders fields and inserts implicit casts where possible to convert internal // data structures to the desired structured type // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated DataStream\u0026lt;User\u0026gt; dataStream = tableEnv.toDataStream(table, User.class); // data types can be extracted reflectively as above or explicitly defined DataStream\u0026lt;User\u0026gt; dataStream = tableEnv.toDataStream( table, DataTypes.STRUCTURED( User.class, DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3)))); Scala import org.apache.flink.streaming.api.scala.DataStream import org.apache.flink.table.api.DataTypes case class User(name: String, score: java.lang.Integer, event_time: java.time.Instant) tableEnv.executeSql( \u0026#34;\u0026#34;\u0026#34; CREATE TABLE GeneratedTable ( name STRING, score INT, event_time TIMESTAMP_LTZ(3), WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;) \u0026#34;\u0026#34;\u0026#34; ) val table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;) // === EXAMPLE 1 === // use the default conversion to instances of Row // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated val dataStream: DataStream[Row] = tableEnv.toDataStream(table) // === EXAMPLE 2 === // a data type is extracted from class `User`, // the planner reorders fields and inserts implicit casts where possible to convert internal // data structures to the desired structured type // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated val dataStream: DataStream[User] = tableEnv.toDataStream(table, classOf[User]) // data types can be extracted reflectively as above or explicitly defined val dataStream: DataStream[User] = tableEnv.toDataStream( table, DataTypes.STRUCTURED( classOf[User], DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3)))) Python t_env.execute_sql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); table = t_env.from_path(\u0026#34;GeneratedTable\u0026#34;); # === EXAMPLE 1 === # use the default conversion to instances of Row # since `event_time` is a single rowtime attribute, it is inserted into the DataStream # metadata and watermarks are propagated ds = t_env.to_data_stream(table) Note that only non-updating tables are supported by toDataStream. Usually, time-based operations such as windows, interval joins, or the MATCH_RECOGNIZE clause are a good fit for insert-only pipelines next to simple operations like projections and filters.\nPipelines with operations that produce updates can use toChangelogStream.\nBack to top\nHandling of Changelog Streams # Internally, Flink\u0026rsquo;s table runtime is a changelog processor. The concepts page describes how dynamic tables and streams relate to each other.\nA StreamTableEnvironment offers the following methods to expose these change data capture (CDC) functionalities:\nfromChangelogStream(DataStream): Interprets a stream of changelog entries as a table. The stream record type must be org.apache.flink.types.Row since its RowKind flag is evaluated during runtime. Event-time and watermarks are not propagated by default. This method expects a changelog containing all kinds of changes (enumerated in org.apache.flink.types.RowKind) as the default ChangelogMode.\nfromChangelogStream(DataStream, Schema): Allows to define a schema for the DataStream similar to fromDataStream(DataStream, Schema). Otherwise the semantics are equal to fromChangelogStream(DataStream).\nfromChangelogStream(DataStream, Schema, ChangelogMode): Gives full control about how to interpret a stream as a changelog. The passed ChangelogMode helps the planner to distinguish between insert-only, upsert, or retract behavior.\ntoChangelogStream(Table): Reverse operation of fromChangelogStream(DataStream). It produces a stream with instances of org.apache.flink.types.Row and sets the RowKind flag for every record at runtime. All kinds of updating tables are supported by this method. If the input table contains a single rowtime column, it will be propagated into a stream record\u0026rsquo;s timestamp. Watermarks will be propagated as well.\ntoChangelogStream(Table, Schema): Reverse operation of fromChangelogStream(DataStream, Schema). The method can enrich the produced column data types. The planner might insert implicit casts if necessary. It is possible to write out the rowtime as a metadata column.\ntoChangelogStream(Table, Schema, ChangelogMode): Gives full control about how to convert a table to a changelog stream. The passed ChangelogMode helps the planner to distinguish between insert-only, upsert, or retract behavior.\nFrom a Table API\u0026rsquo;s perspective, converting from and to DataStream API is similar to reading from or writing to a virtual table connector that has been defined using a CREATE TABLE DDL in SQL.\nBecause fromChangelogStream behaves similar to fromDataStream, we recommend reading the previous section before continuing here.\nThis virtual connector also supports reading and writing the rowtime metadata of the stream record.\nThe virtual table source implements SupportsSourceWatermark.\nExamples for fromChangelogStream # The following code shows how to use fromChangelogStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; // === EXAMPLE 1 === // interpret the stream as a retract stream // create a changelog DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)); // interpret the DataStream as a Table Table table = tableEnv.fromChangelogStream(dataStream); // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print(); // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -D | Alice | 12 | // | +I | Alice | 100 | // +----+--------------------------------+-------------+ // === EXAMPLE 2 === // interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) // create a changelog DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)); // interpret the DataStream as a Table Table table = tableEnv.fromChangelogStream( dataStream, Schema.newBuilder().primaryKey(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()); // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print(); // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -U | Alice | 12 | // | +U | Alice | 100 | // +----+--------------------------------+-------------+ Scala import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.table.api.Schema import org.apache.flink.table.connector.ChangelogMode import org.apache.flink.types.{Row, RowKind} // === EXAMPLE 1 === // interpret the stream as a retract stream // create a changelog DataStream val dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, Int.box(5)), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the DataStream as a Table val table = tableEnv.fromChangelogStream(dataStream) // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table) tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print() // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -D | Alice | 12 | // | +I | Alice | 100 | // +----+--------------------------------+-------------+ // === EXAMPLE 2 === // interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) // create a changelog DataStream val dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, Int.box(5)), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the DataStream as a Table val table = tableEnv.fromChangelogStream( dataStream, Schema.newBuilder().primaryKey(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()) // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table) tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print() // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -U | Alice | 12 | // | +U | Alice | 100 | // +----+--------------------------------+-------------+ Python from pyflink.common import Row, RowKind from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import DataTypes, StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # === EXAMPLE 1 === # create a changelog DataStream ds = env.from_collection([ Row.of_kind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.of_kind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW([Types.STRING(),Types.INT()])) # interpret the DataStream as a Table table = t_env.from_changelog_stream(ds) # register the table under a name and perform an aggregation t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, table) t_env.execute_sql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;).print() # prints: # +----+--------------------------------+-------------+ # | op | name | score | # +----+--------------------------------+-------------+ # | +I | Bob | 5 | # | +I | Alice | 12 | # | -D | Alice | 12 | # | +I | Alice | 100 | # +----+--------------------------------+-------------+ # === EXAMPLE 2 === # interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) # create a changelog DataStream ds = env.from_collection([ Row.of_kind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.of_kind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW([Types.STRING(),Types.INT()])) # interpret the DataStream as a Table table = t_env.from_changelog_stream( ds, Schema.new_builder().primary_key(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()) # register the table under a name and perform an aggregation t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, table) t_env.execute_sql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;).print() # prints: # +----+--------------------------------+-------------+ # | op | name | score | # +----+--------------------------------+-------------+ # | +I | Bob | 5 | # | +I | Alice | 12 | # | -U | Alice | 12 | # | +U | Alice | 100 | # +----+--------------------------------+-------------+ The default ChangelogMode shown in example 1 should be sufficient for most use cases as it accepts all kinds of changes.\nHowever, example 2 shows how to limit the kinds of incoming changes for efficiency by reducing the number of update messages by 50% using upsert mode. The number of result messages can be reduced by defining a primary key and upsert changelog mode for toChangelogStream.\nExamples for toChangelogStream # The following code shows how to use toChangelogStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.functions.ProcessFunction; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.data.StringData; import org.apache.flink.types.Row; import org.apache.flink.util.Collector; import static org.apache.flink.table.api.Expressions.*; // create Table with event-time tableEnv.executeSql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); Table table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;); // === EXAMPLE 1 === // convert to DataStream in the simplest and most general way possible (no event-time) Table simpleTable = tableEnv .fromValues(row(\u0026#34;Alice\u0026#34;, 12), row(\u0026#34;Alice\u0026#34;, 2), row(\u0026#34;Bob\u0026#34;, 12)) .as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) .groupBy($(\u0026#34;name\u0026#34;)) .select($(\u0026#34;name\u0026#34;), $(\u0026#34;score\u0026#34;).sum()); tableEnv .toChangelogStream(simpleTable) .executeAndCollect() .forEachRemaining(System.out::println); // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] // === EXAMPLE 2 === // convert to DataStream in the simplest and most general way possible (with event-time) DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream(table); // since `event_time` is a single time attribute in the schema, it is set as the // stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row dataStream.process( new ProcessFunction\u0026lt;Row, Void\u0026gt;() { @Override public void processElement(Row row, Context ctx, Collector\u0026lt;Void\u0026gt; out) { // prints: [name, score, event_time] System.out.println(row.getFieldNames(true)); // timestamp exists twice assert ctx.timestamp() == row.\u0026lt;Instant\u0026gt;getFieldAs(\u0026#34;event_time\u0026#34;).toEpochMilli(); } }); env.execute(); // === EXAMPLE 3 === // convert to DataStream but write out the time attribute as a metadata column which means // it is not part of the physical schema anymore DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream( table, Schema.newBuilder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()); // the stream record\u0026#39;s timestamp is defined by the metadata; it is not part of the Row dataStream.process( new ProcessFunction\u0026lt;Row, Void\u0026gt;() { @Override public void processElement(Row row, Context ctx, Collector\u0026lt;Void\u0026gt; out) { // prints: [name, score] System.out.println(row.getFieldNames(true)); // timestamp exists once System.out.println(ctx.timestamp()); } }); env.execute(); // === EXAMPLE 4 === // for advanced users, it is also possible to use more internal data structures for efficiency // note that this is only mentioned here for completeness because using internal data structures // adds complexity and additional type handling // however, converting a TIMESTAMP_LTZ column to `Long` or STRING to `byte[]` might be convenient, // also structured types can be represented as `Row` if needed DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream( table, Schema.newBuilder() .column( \u0026#34;name\u0026#34;, DataTypes.STRING().bridgedTo(StringData.class)) .column( \u0026#34;score\u0026#34;, DataTypes.INT()) .column( \u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3).bridgedTo(Long.class)) .build()); // leads to a stream of Row(name: StringData, score: Integer, event_time: Long) Scala import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.functions.ProcessFunction import org.apache.flink.streaming.api.scala.DataStream import org.apache.flink.table.api._ import org.apache.flink.types.Row import org.apache.flink.util.Collector import java.time.Instant // create Table with event-time tableEnv.executeSql( \u0026#34;\u0026#34;\u0026#34; CREATE TABLE GeneratedTable ( name STRING, score INT, event_time TIMESTAMP_LTZ(3), WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;) \u0026#34;\u0026#34;\u0026#34; ) val table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;) // === EXAMPLE 1 === // convert to DataStream in the simplest and most general way possible (no event-time) val simpleTable = tableEnv .fromValues(row(\u0026#34;Alice\u0026#34;, 12), row(\u0026#34;Alice\u0026#34;, 2), row(\u0026#34;Bob\u0026#34;, 12)) .as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) .groupBy($\u0026#34;name\u0026#34;) .select($\u0026#34;name\u0026#34;, $\u0026#34;score\u0026#34;.sum()) tableEnv .toChangelogStream(simpleTable) .executeAndCollect() .foreach(println) // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] // === EXAMPLE 2 === // convert to DataStream in the simplest and most general way possible (with event-time) val dataStream: DataStream[Row] = tableEnv.toChangelogStream(table) // since `event_time` is a single time attribute in the schema, it is set as the // stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row dataStream.process(new ProcessFunction[Row, Unit] { override def processElement( row: Row, ctx: ProcessFunction[Row, Unit]#Context, out: Collector[Unit]): Unit = { // prints: [name, score, event_time] println(row.getFieldNames(true)) // timestamp exists twice assert(ctx.timestamp() == row.getFieldAs[Instant](\u0026#34;event_time\u0026#34;).toEpochMilli) } }) env.execute() // === EXAMPLE 3 === // convert to DataStream but write out the time attribute as a metadata column which means // it is not part of the physical schema anymore val dataStream: DataStream[Row] = tableEnv.toChangelogStream( table, Schema.newBuilder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()) // the stream record\u0026#39;s timestamp is defined by the metadata; it is not part of the Row dataStream.process(new ProcessFunction[Row, Unit] { override def processElement( row: Row, ctx: ProcessFunction[Row, Unit]#Context, out: Collector[Unit]): Unit = { // prints: [name, score] println(row.getFieldNames(true)) // timestamp exists once println(ctx.timestamp()) } }) env.execute() // === EXAMPLE 4 === // for advanced users, it is also possible to use more internal data structures for better // efficiency // note that this is only mentioned here for completeness because using internal data structures // adds complexity and additional type handling // however, converting a TIMESTAMP_LTZ column to `Long` or STRING to `byte[]` might be convenient, // also structured types can be represented as `Row` if needed val dataStream: DataStream[Row] = tableEnv.toChangelogStream( table, Schema.newBuilder() .column( \u0026#34;name\u0026#34;, DataTypes.STRING().bridgedTo(classOf[StringData])) .column( \u0026#34;score\u0026#34;, DataTypes.INT()) .column( \u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3).bridgedTo(class[Long])) .build()) // leads to a stream of Row(name: StringData, score: Integer, event_time: Long) Python from pyflink.common import Row from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.functions import ProcessFunction from pyflink.table import DataTypes, StreamTableEnvironment, Schema from pyflink.table.expressions import col env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create Table with event-time t_env.execute_sql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;) table = t_env.from_path(\u0026#34;GeneratedTable\u0026#34;) # === EXAMPLE 1 === # convert to DataStream in the simplest and most general way possible (no event-time) simple_table = t_env.from_elements([Row(\u0026#34;Alice\u0026#34;, 12), Row(\u0026#34;Alice\u0026#34;, 2), Row(\u0026#34;Bob\u0026#34;, 12)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT())])) simple_table = simple_table.group_by(col(\u0026#39;name\u0026#39;)).select(col(\u0026#39;name\u0026#39;), col(\u0026#39;score\u0026#39;).sum) t_env.to_changelog_stream(simple_table).print() env.execute() # prints: # +I[Bob, 12] # +I[Alice, 12] # -U[Alice, 12] # +U[Alice, 14] # === EXAMPLE 2 === # convert to DataStream in the simplest and most general way possible (with event-time) ds = t_env.to_changelog_stream(table) # since `event_time` is a single time attribute in the schema, it is set as the # stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row class MyProcessFunction(ProcessFunction): def process_element(self, row, ctx): print(row) assert ctx.timestamp() == row.event_time.to_epoch_milli() ds.process(MyProcessFunction()) env.execute() # === EXAMPLE 3 === # convert to DataStream but write out the time attribute as a metadata column which means # it is not part of the physical schema anymore ds = t_env.to_changelog_stream( table, Schema.new_builder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .column_by_metadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()) class MyProcessFunction(ProcessFunction): def process_element(self, row, ctx): print(row) print(ctx.timestamp()) ds.process(MyProcessFunction()) env.execute() For more information about which conversions are supported for data types in Example 4, see the Table API\u0026rsquo;s Data Types page.\nThe behavior of toChangelogStream(Table).executeAndCollect() is equal to calling Table.execute().collect(). However, toChangelogStream(Table) might be more useful for tests because it allows to access the produced watermarks in a subsequent ProcessFunction in DataStream API.\nBack to top\nAdding Table API Pipelines to DataStream API # A single Flink job can consist of multiple disconnected pipelines that run next to each other.\nSource-to-sink pipelines defined in Table API can be attached as a whole to the StreamExecutionEnvironment and will be submitted when calling one of the execute methods in the DataStream API.\nHowever, a source does not necessarily have to be a table source but can also be another DataStream pipeline that was converted to Table API before. Thus, it is possible to use table sinks for DataStream API programs.\nThe functionality is available through a specialized StreamStatementSet instance created with StreamTableEnvironment.createStatementSet(). By using a statement set, the planner can optimize all added statements together and come up with one or more end-to-end pipelines that are added to the StreamExecutionEnvironment when calling StreamStatementSet.attachAsDataStream().\nThe following example shows how to add table programs to a DataStream API program within one job.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.sink.DiscardingSink; import org.apache.flink.table.api.*; import org.apache.flink.table.api.bridge.java.*; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); StreamStatementSet statementSet = tableEnv.createStatementSet(); // create some source TableDescriptor sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) .schema( Schema.newBuilder() .column(\u0026#34;myCol\u0026#34;, DataTypes.INT()) .column(\u0026#34;myOtherCol\u0026#34;, DataTypes.BOOLEAN()) .build()) .build(); // create some sink TableDescriptor sinkDescriptor = TableDescriptor.forConnector(\u0026#34;print\u0026#34;).build(); // add a pure Table API pipeline Table tableFromSource = tableEnv.from(sourceDescriptor); statementSet.add(tableFromSource.insertInto(sinkDescriptor)); // use table sinks for the DataStream API pipeline DataStream\u0026lt;Integer\u0026gt; dataStream = env.fromElements(1, 2, 3); Table tableFromStream = tableEnv.fromDataStream(dataStream); statementSet.add(tableFromStream.insertInto(sinkDescriptor)); // attach both pipelines to StreamExecutionEnvironment // (the statement set will be cleared after calling this method) statementSet.attachAsDataStream(); // define other DataStream API parts env.fromElements(4, 5, 6).addSink(new DiscardingSink\u0026lt;\u0026gt;()); // use DataStream API to submit the pipelines env.execute(); // prints similar to: // +I[1618440447, false] // +I[1259693645, true] // +I[158588930, false] // +I[1] // +I[2] // +I[3] Scala import org.apache.flink.streaming.api.functions.sink.DiscardingSink import org.apache.flink.streaming.api.scala._ import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) val statementSet = tableEnv.createStatementSet() // create some source val sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) .schema(Schema.newBuilder .column(\u0026#34;myCol\u0026#34;, DataTypes.INT) .column(\u0026#34;myOtherCol\u0026#34;, DataTypes.BOOLEAN).build) .build // create some sink val sinkDescriptor = TableDescriptor.forConnector(\u0026#34;print\u0026#34;).build // add a pure Table API pipeline val tableFromSource = tableEnv.from(sourceDescriptor) statementSet.add(tableFromSource.insertInto(sinkDescriptor)) // use table sinks for the DataStream API pipeline val dataStream = env.fromElements(1, 2, 3) val tableFromStream = tableEnv.fromDataStream(dataStream) statementSet.add(tableFromStream.insertInto(sinkDescriptor)) // attach both pipelines to StreamExecutionEnvironment // (the statement set will be cleared calling this method) statementSet.attachAsDataStream() // define other DataStream API parts env.fromElements(4, 5, 6).addSink(new DiscardingSink[Int]()) // now use DataStream API to submit the pipelines env.execute() // prints similar to: // +I[1618440447, false] // +I[1259693645, true] // +I[158588930, false] // +I[1] // +I[2] // +I[3] Python from pyflink.common import Encoder from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink from pyflink.table import StreamTableEnvironment, TableDescriptor, Schema, DataTypes env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) statement_set = table_env.create_statement_set() # create some source source_descriptor = TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) \\ .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) \\ .schema( Schema.new_builder() .column(\u0026#34;my_col\u0026#34;, DataTypes.INT()) .column(\u0026#34;my_other_col\u0026#34;, DataTypes.BOOLEAN()) .build()) \\ .build() # create some sink sink_descriptor = TableDescriptor.for_connector(\u0026#34;print\u0026#34;).build() # add a pure Table API pipeline table_from_source = table_env.from_descriptor(source_descriptor) statement_set.add_insert(sink_descriptor, table_from_source) # use table sinks for the DataStream API pipeline data_stream = env.from_collection([1, 2, 3]) table_from_stream = table_env.from_data_stream(data_stream) statement_set.add_insert(sink_descriptor, table_from_stream) # attach both pipelines to StreamExecutionEnvironment # (the statement set will be cleared after calling this method) statement_set.attach_as_datastream() # define other DataStream API parts env.from_collection([4, 5, 6]) \\ .add_sink(FileSink .for_row_format(\u0026#39;/tmp/output\u0026#39;, Encoder.simple_string_encoder()) .build()) # use DataStream API to submit the pipelines env.execute() # prints similar to: # +I[1618440447, false] # +I[1259693645, true] # +I[158588930, false] # +I[1] # +I[2] # +I[3] Back to top\nImplicit Conversions in Scala # Users of the Scala API can use all the conversion methods above in a more fluent way by leveraging Scala\u0026rsquo;s implicit feature.\nThose implicits are available in the API when importing the package object via org.apache.flink.table.api.bridge.scala._.\nIf enabled, methods such as toTable or toChangelogTable can be called directly on a DataStream object. Similarly, toDataStream and toChangelogStream are available on Table objects. Furthermore, Table objects will be converted to a changelog stream when requesting a DataStream API specific method for DataStream[Row].\nThe use of an implicit conversion should always be a conscious decision. One should pay attention whether the IDE proposes an actual Table API method, or a DataStream API method via implicits.\nFor example, a table.execute().collect() stays in Table API whereas table.executeAndCollect() implicitly uses the DataStream API\u0026rsquo;s executeAndCollect() method and therefore forces an API conversion.\nimport org.apache.flink.streaming.api.scala._ import org.apache.flink.table.api.bridge.scala._ import org.apache.flink.types.Row val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) val dataStream: DataStream[(Int, String)] = env.fromElements((42, \u0026#34;hello\u0026#34;)) // call toChangelogTable() implicitly on the DataStream object val table: Table = dataStream.toChangelogTable(tableEnv) // force implicit conversion val dataStreamAgain1: DataStream[Row] = table // call toChangelogStream() implicitly on the Table object val dataStreamAgain2: DataStream[Row] = table.toChangelogStream Back to top\nMapping between TypeInformation and DataType # The DataStream API uses instances of org.apache.flink.api.common.typeinfo.TypeInformation to describe the record type that travels in the stream. In particular, it defines how to serialize and deserialize records from one DataStream operator to the other. It also helps in serializing state into savepoints and checkpoints.\nThe Table API uses custom data structures to represent records internally and exposes org.apache.flink.table.types.DataType to users for declaring the external format into which the data structures are converted for easier usage in sources, sinks, UDFs, or DataStream API.\nDataType is richer than TypeInformation as it also includes details about the logical SQL type. Therefore, some details will be added implicitly during the conversion.\nColumn names and types of a Table are automatically derived from the TypeInformation of the DataStream. Use DataStream.getType() to check whether the type information has been detected correctly via the DataStream API\u0026rsquo;s reflective type extraction facilities. If the outermost record\u0026rsquo;s TypeInformation is a CompositeType, it will be flattened in the first level when deriving a table\u0026rsquo;s schema.\nThe DataStream API is not always able to extract a more specific TypeInformation based on reflection. This often happens silently and leads to GenericTypeInfo that is backed by the generic Kryo serializer.\nFor example, the Row class cannot be analyzed reflectively and always needs an explicit type information declaration. If no proper type information is declared in DataStream API, the row will show as RAW data type and the Table API is unable to access its fields. Use .map(...).returns(TypeInformation) in Java or .map(...)(TypeInformation) in Scala to declare type information explicitly.\nTypeInformation to DataType # The following rules apply when converting TypeInformation to a DataType:\nAll subclasses of TypeInformation are mapped to logical types, including nullability that is aligned with Flink\u0026rsquo;s built-in serializers.\nSubclasses of TupleTypeInfoBase are translated into a row (for Row) or structured type (for tuples, POJOs, and case classes).\nBigDecimal is converted to DECIMAL(38, 18) by default.\nThe order of PojoTypeInfo fields is determined by a constructor with all fields as its parameters. If that is not found during the conversion, the field order will be alphabetical.\nGenericTypeInfo and other TypeInformation that cannot be represented as one of the listed org.apache.flink.table.api.DataTypes will be treated as a black-box RAW type. The current session configuration is used to materialize the serializer of the raw type. Composite nested fields will not be accessible then.\nSee TypeInfoDataTypeConverter for the full translation logic.\nUse DataTypes.of(TypeInformation) to call the above logic in custom schema declaration or in UDFs.\nDataType to TypeInformation # The table runtime will make sure to properly serialize the output records to the first operator of the DataStream API.\nAfterward, the type information semantics of the DataStream API need to be considered. Back to top\nLegacy Conversion # The following section describes outdated parts of the API that will be removed in future versions.\nIn particular, these parts might not be well integrated into many recent new features and refactorings (e.g. RowKind is not correctly set, type systems don\u0026rsquo;t integrate smoothly).\nConvert a DataStream into a Table # A DataStream can be directly converted to a Table in a StreamTableEnvironment. The schema of the resulting view depends on the data type of the registered collection.\nJava StreamTableEnvironment tableEnv = ...; DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; stream = ...; Table table2 = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myString\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? val stream: DataStream[(Long, String)] = ??? val table2: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) Python t_env = ... # type: StreamTableEnvironment stream = ... # type: DataStream of Types.TUPLE([Types.LONG(), Types.STRING()]) table2 = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_stream\u0026#39;)) Back to top\nConvert a Table into a DataStream # The results of a Table can be converted into a DataStream. In this way, custom DataStream programs can be run on the result of a Table API or SQL query.\nWhen converting a Table into a DataStream you need to specify the data type of the resulting records, i.e., the data type into which the rows of the Table are to be converted. Often the most convenient conversion type is Row. The following list gives an overview of the features of the different options:\nRow: fields are mapped by position, arbitrary number of fields, support for null values, no type-safe access. POJO: fields are mapped by name (POJO fields must be named as Table fields), arbitrary number of fields, support for null values, type-safe access. Case Class: fields are mapped by position, no support for null values, type-safe access. Tuple: fields are mapped by position, limitation to 22 (Scala) or 25 (Java) fields, no support for null values, type-safe access. Atomic Type: Table must have a single field, no support for null values, type-safe access. Convert a Table into a DataStream # A Table that is the result of a streaming query will be updated dynamically, i.e., it is changing as new records arrive on the query\u0026rsquo;s input streams. Hence, the DataStream into which such a dynamic query is converted needs to encode the updates of the table.\nThere are two modes to convert a Table into a DataStream:\nAppend Mode: This mode can only be used if the dynamic Table is only modified by INSERT changes, i.e., it is append-only and previously emitted results are never updated. Retract Mode: This mode can always be used. It encodes INSERT and DELETE changes with a boolean flag. Java StreamTableEnvironment tableEnv = ...; Table table = tableEnv.fromValues( DataTypes.Row( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT()), row(\u0026#34;john\u0026#34;, 35), row(\u0026#34;sarah\u0026#34;, 32)); // Convert the Table into an append DataStream of Row by specifying the class DataStream\u0026lt;Row\u0026gt; dsRow = tableEnv.toAppendStream(table, Row.class); // Convert the Table into an append DataStream of Tuple2\u0026lt;String, Integer\u0026gt; with TypeInformation TupleTypeInfo\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; tupleType = new TupleTypeInfo\u0026lt;\u0026gt;(Types.STRING(), Types.INT()); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dsTuple = tableEnv.toAppendStream(table, tupleType); // Convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream\u0026lt;Tuple2\u0026lt;Boolean, X\u0026gt;\u0026gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. DataStream\u0026lt;Tuple2\u0026lt;Boolean, Row\u0026gt;\u0026gt; retractStream = tableEnv.toRetractStream(table, Row.class); Scala val tableEnv: StreamTableEnvironment = ??? // Table with two fields (String name, Integer age) val table: Table = tableEnv.fromValues( DataTypes.Row( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT()), row(\u0026#34;john\u0026#34;, 35), row(\u0026#34;sarah\u0026#34;, 32)) // Convert the Table into an append DataStream of Row by specifying the class val dsRow: DataStream[Row] = tableEnv.toAppendStream[Row](table) // Convert the Table into an append DataStream of (String, Integer) with TypeInformation val dsTuple: DataStream[(String, Int)] dsTuple = tableEnv.toAppendStream[(String, Int)](table) // Convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream\u0026lt;Tuple2\u0026lt;Boolean, X\u0026gt;\u0026gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. val retractStream: DataStream[(Boolean, Row)] = tableEnv.toRetractStream[Row](table) Python from pyflink.table import DataTypes from pyflink.common.typeinfo import Types t_env = ... table = t_env.from_elements([(\u0026#34;john\u0026#34;, 35), (\u0026#34;sarah\u0026#34;, 32)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT())])) # Convert the Table into an append DataStream of Row by specifying the type information ds_row = t_env.to_append_stream(table, Types.ROW([Types.STRING(), Types.INT()])) # Convert the Table into an append DataStream of Tuple[str, int] with TypeInformation ds_tuple = t_env.to_append_stream(table, Types.TUPLE([Types.STRING(), Types.INT()])) # Convert the Table into a retract DataStream of Row by specifying the type information # A retract stream of type X is a DataStream of Tuple[bool, X]. # The boolean field indicates the type of the change. # True is INSERT, false is DELETE. retract_stream = t_env.to_retract_stream(table, Types.ROW([Types.STRING(), Types.INT()])) Note: A detailed discussion about dynamic tables and their properties is given in the Dynamic Tables document.\nOnce the Table is converted to a DataStream, please use the StreamExecutionEnvironment.execute() method to execute the DataStream program. Back to top\nMapping of Data Types to Table Schema # Flink\u0026rsquo;s DataStream API supports many diverse types. Composite types such as Tuples (built-in Scala , Flink Java tuples and Python tuples), POJOs, Scala case classes, and Flink\u0026rsquo;s Row type allow for nested data structures with multiple fields that can be accessed in table expressions. Other types are treated as atomic types. In the following, we describe how the Table API converts these types into an internal row representation and show examples of converting a DataStream into a Table.\nThe mapping of a data type to a table schema can happen in two ways: based on the field positions or based on the field names.\nPosition-based Mapping\nPosition-based mapping can be used to give fields a more meaningful name while keeping the field order. This mapping is available for composite data types with a defined field order and atomic types. Composite data types such as tuples, rows, and case classes have such a field order. However, fields of a POJO must be mapped based on the field names (see next section). Fields can be projected out but can\u0026rsquo;t be renamed using an alias as(Java and Scala) or alias(Python).\nWhen defining a position-based mapping, the specified names must not exist in the input data type, otherwise, the API will assume that the mapping should happen based on the field names. If no field names are specified, the default field names and field order of the composite type are used or f0 for atomic types.\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section; DataStream\u0026lt;Tuple2\u0026lt;Long, Integer\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with field \u0026#34;myLong\u0026#34; only Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;)); // convert DataStream into Table with field names \u0026#34;myLong\u0026#34; and \u0026#34;myInt\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myInt\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with field \u0026#34;myLong\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) // convert DataStream into Table with field names \u0026#34;myLong\u0026#34; and \u0026#34;myInt\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myInt\u0026#34;) Python from pyflink.table.expressions import col # get a TableEnvironment t_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section stream = ... # type: DataStream of Types.Tuple([Types.LONG(), Types.INT()]) # convert DataStream into Table with field \u0026#34;my_long\u0026#34; only table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;)) # convert DataStream into Table with field names \u0026#34;my_long\u0026#34; and \u0026#34;my_int\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_int\u0026#39;)) Name-based Mapping\nName-based mapping can be used for any data type, including POJOs. It is the most flexible way of defining a table schema mapping. All fields in the mapping are referenced by name and can be possibly renamed using an alias as. Fields can be reordered and projected out.\nIf no field names are specified, the default field names and field order of the composite type are used or f0 for atomic types.\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section DataStream\u0026lt;Tuple2\u0026lt;Long, Integer\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with field \u0026#34;f1\u0026#34; only Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;)); // convert DataStream into Table with swapped fields Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;), $(\u0026#34;f0\u0026#34;)); // convert DataStream into Table with swapped fields and field names \u0026#34;myInt\u0026#34; and \u0026#34;myLong\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;).as(\u0026#34;myInt\u0026#34;), $(\u0026#34;f0\u0026#34;).as(\u0026#34;myLong\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with field \u0026#34;_2\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with swapped fields val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with swapped fields and field names \u0026#34;myInt\u0026#34; and \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myInt\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) Python from pyflink.table.expressions import col # get a TableEnvironment t_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section stream = ... # type: DataStream of Types.Tuple([Types.LONG(), Types.INT()]) # convert DataStream into Table with field \u0026#34;f1\u0026#34; only table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;)) # convert DataStream into Table with swapped fields table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;), col(\u0026#39;f0\u0026#39;)) # convert DataStream into Table with swapped fields and field names \u0026#34;my_int\u0026#34; and \u0026#34;my_long\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;).alias(\u0026#39;my_int\u0026#39;), col(\u0026#39;f0\u0026#39;).alias(\u0026#39;my_long\u0026#39;)) Atomic Types # Flink treats primitives (Integer, Double, String) or generic types (types that cannot be analyzed and decomposed) as atomic types. A DataStream of an atomic type is converted into a Table with a single column. The type of the column is inferred from the atomic type. The name of the column can be specified.\nJava StreamTableEnvironment tableEnv = ...; DataStream\u0026lt;Long\u0026gt; stream = ...; // Convert DataStream into Table with field name \u0026#34;myLong\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? val stream: DataStream[Long] = ... // Convert DataStream into Table with default field name \u0026#34;f0\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // Convert DataStream into Table with field name \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) Python from pyflink.table.expressions import col t_env = ... stream = ... # types: DataStream of Types.Long() # Convert DataStream into Table with default field name \u0026#34;f0\u0026#34; table = t_env.from_data_stream(stream) # Convert DataStream into Table with field name \u0026#34;my_long\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;)) Tuples (Scala, Java, Python) and Case Classes (Scala only) # Java Flink provides its own tuple classes for Java. DataStreams of the the Java tuple classes can be converted into tables. Fields can be renamed by providing names for all fields (mapping based on position). If no field names are specified, the default field names are used. If the original field names (f0, f1, \u0026hellip; for Flink Tuples) are referenced, the API assumes that the mapping is name-based instead of position-based. Name-based mapping allows for reordering fields and projection with alias (as).\nStreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with renamed field names \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; (position-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myString\u0026#34;)); // convert DataStream into Table with reordered fields \u0026#34;f1\u0026#34;, \u0026#34;f0\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;), $(\u0026#34;f0\u0026#34;)); // convert DataStream into Table with projected field \u0026#34;f1\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;)); // convert DataStream into Table with reordered and aliased fields \u0026#34;myString\u0026#34;, \u0026#34;myLong\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;).as(\u0026#34;myString\u0026#34;), $(\u0026#34;f0\u0026#34;).as(\u0026#34;myLong\u0026#34;)); Scala Flink supports Scala\u0026rsquo;s built-in tuples. DataStreams of Scala\u0026rsquo;s built-in tuples can be converted into tables. Fields can be renamed by providing names for all fields (mapping based on position). If no field names are specified, the default field names are used. If the original field names (_1, _2, \u0026hellip; for Scala Tuples) are referenced, the API assumes that the mapping is name-based instead of position-based. Name-based mapping allows for reordering fields and projection with alias (as).\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, String)] = ... // convert DataStream into Table with field names \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) // convert DataStream into Table with reordered fields \u0026#34;_2\u0026#34;, \u0026#34;_1\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with projected field \u0026#34;_2\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myString\u0026#34;, \u0026#34;myLong\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myString\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) // define case class case class Person(name: String, age: Int) val streamCC: DataStream[Person] = ... // convert DataStream into Table with field names \u0026#39;myName, \u0026#39;myAge (position-based) val table = tableEnv.fromDataStream(streamCC, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python Flink supports Python\u0026rsquo;s built-in Tuples. DataStreams of tuples can be converted into tables. Fields can be renamed by providing names for all fields (mapping based on position). If no field names are specified, the default field names are used. If the original field names (f0, f1, \u0026hellip; ) are referenced, the API assumes that the mapping is name-based instead of position-based. Name-based mapping allows for reordering fields and projection with alias (alias).\nfrom pyflink.table.expressions import col stream = ... # type: DataStream of Types.TUPLE([Types.LONG(), Types.STRING()]) # convert DataStream into Table with renamed field names \u0026#34;my_long\u0026#34;, \u0026#34;my_string\u0026#34; (position-based) table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_string\u0026#39;)) # convert DataStream into Table with reordered fields \u0026#34;f1\u0026#34;, \u0026#34;f0\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;), col(\u0026#39;f0\u0026#39;)) # convert DataStream into Table with projected field \u0026#34;f1\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;)) # convert DataStream into Table with reordered and aliased fields \u0026#34;my_string\u0026#34;, \u0026#34;my_long\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;).alias(\u0026#39;my_string\u0026#39;), col(\u0026#39;f0\u0026#39;).alias(\u0026#39;my_long\u0026#39;)) POJO (Java and Scala) # Flink supports POJOs as composite types. The rules for what determines a POJO are documented here.\nWhen converting a POJO DataStream into a Table without specifying field names, the names of the original POJO fields are used. The name mapping requires the original names and cannot be done by positions. Fields can be renamed using an alias (with the as keyword), reordered, and projected.\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // Person is a POJO with fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; DataStream\u0026lt;Person\u0026gt; stream = ...; // convert DataStream into Table with renamed fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;age\u0026#34;).as(\u0026#34;myAge\u0026#34;), $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;)); // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // Person is a POJO with field names \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; val stream: DataStream[Person] = ... // convert DataStream into Table with renamed fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python Custom PoJo Class is unsupported in PyFlink now. Row # The Row data type supports an arbitrary number of fields and fields with null values. Field names can be specified via a RowTypeInfo or when converting a Row DataStream into a Table. The row type supports mapping of fields by position and by name. Fields can be renamed by providing names for all fields (mapping based on position) or selected individually for projection/ordering/renaming (mapping based on name).\nJava StreamTableEnvironment tableEnv = ...; // DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` DataStream\u0026lt;Row\u0026gt; stream = ...; // Convert DataStream into Table with renamed field names \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (position-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myName\u0026#34;), $(\u0026#34;myAge\u0026#34;)); // Convert DataStream into Table with renamed fields \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;), $(\u0026#34;age\u0026#34;).as(\u0026#34;myAge\u0026#34;)); // Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;)); // Convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? // DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` val stream: DataStream[Row] = ... // Convert DataStream into Table with renamed field names \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // Convert DataStream into Table with renamed fields \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;) // Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // Convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python from pyflink.table.expressions import col t_env = ...; # DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` stream = ... # Convert DataStream into Table with renamed field names \u0026#34;my_name\u0026#34;, \u0026#34;my_age\u0026#34; (position-based) table = t_env.from_data_stream(stream, col(\u0026#39;my_name\u0026#39;), col(\u0026#39;my_age\u0026#39;)) # Convert DataStream into Table with renamed fields \u0026#34;my_name\u0026#34;, \u0026#34;my_age\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;).alias(\u0026#39;my_name\u0026#39;), col(\u0026#39;age\u0026#39;).alias(\u0026#39;my_age\u0026#39;)) # Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;)) # Convert DataStream into Table with projected and renamed field \u0026#34;my_name\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;).alias(\u0026#34;my_name\u0026#34;)) Back to top\n"}),e.add({id:74,href:"/flink/flink-docs-master/docs/ops/debugging/debugging_classloading/",title:"Debugging Classloading",section:"Debugging",content:` Debugging Classloading # Overview of Classloading in Flink # When running Flink applications, the JVM will load various classes over time. These classes can be divided into three groups based on their origin:
The Java Classpath: This is Java\u0026rsquo;s common classpath, and it includes the JDK libraries, and all code in Flink\u0026rsquo;s /lib folder (the classes of Apache Flink and some dependencies). They are loaded by AppClassLoader.
The Flink Plugin Components: The plugins code in folders under Flink\u0026rsquo;s /plugins folder. Flink\u0026rsquo;s plugin mechanism will dynamically load them once during startup.
The Dynamic User Code: These are all classes that are included in the JAR files of dynamically submitted jobs, (via REST, CLI, web UI). They are loaded (and unloaded) dynamically by FlinkUserCodeClassLoader per job.
As a general rule, whenever you start the Flink processes first and submit jobs later, the job\u0026rsquo;s classes are loaded dynamically. If the Flink processes are started together with the job/application, or if the application spawns the Flink components (JobManager, TaskManager, etc.), then all job\u0026rsquo;s classes are in the Java classpath.
Code in plugin components is loaded dynamically once by a dedicated class loader per plugin.
In the following are some more details about the different deployment modes:
Session Mode (Standalone/Yarn/Kubernetes)
When starting a Flink session(Standalone/Yarn/Kubernetes) cluster, the JobManagers and TaskManagers are started with the Flink framework classes in the Java classpath. The classes from all jobs/applications that are submitted against the session (via REST / CLI) are loaded dynamically by FlinkUserCodeClassLoader.
Per-Job Mode (deprecated) (Yarn)
Currently, only Yarn supports Per-Job mode. By default, running a Flink cluster in Per-Job mode will include the user jars (the JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder) into the system classpath (the AppClassLoader). This behavior can be controlled with the yarn.classpath.include-user-jar config option. When setting it to DISABLED, Flink will include the user jars in the user classpath and load them dynamically by FlinkUserCodeClassLoader. See Flink on Yarn for more details.
Application Mode (Standalone/Yarn/Kubernetes)
When run a Standalone/Kubernetes Flink cluster in Application Mode, the user jars (the JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder) will be loaded dynamically by FlinkUserCodeClassLoader.
When run a Yarn Flink cluster in Application Mode, the user jars (the JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder) will be included into the system classpath (the AppClassLoader) by default. Same as Per-Job mode, when setting the yarn.classpath.include-user-jar to DISABLED, Flink will include the user jars in the user classpath and load them dynamically by FlinkUserCodeClassLoader.
Inverted Class Loading and ClassLoader Resolution Order # In setups where dynamic classloading is involved (plugin components, Flink jobs in session setups), there is a hierarchy of typically two ClassLoaders: (1) Java\u0026rsquo;s application classloader, which has all classes in the classpath, and (2) the dynamic plugin/user code classloader. for loading classes from the plugin or the user-code jar(s). The dynamic ClassLoader has the application classloader as its parent.
By default, Flink inverts classloading order, meaning it looks into the dynamic classloader first, and only looks into the parent (application classloader) if the class is not part of the dynamically loaded code.
The benefit of inverted classloading is that plugins and jobs can use different library versions than Flink\u0026rsquo;s core itself, which is very useful when the different versions of the libraries are not compatible. The mechanism helps to avoid the common dependency conflict errors like IllegalAccessError or NoSuchMethodError. Different parts of the code simply have separate copies of the classes (Flink\u0026rsquo;s core or one of its dependencies can use a different copy than the user code or plugin code). In most cases, this works well and no additional configuration from the user is needed.
However, there are cases when the inverted classloading causes problems (see below, \u0026ldquo;X cannot be cast to X\u0026rdquo;). For user code classloading, you can revert back to Java\u0026rsquo;s default mode by configuring the ClassLoader resolution order via classloader.resolve-order in the Flink config to parent-first (from Flink\u0026rsquo;s default child-first).
Please note that certain classes are always resolved in a parent-first way (through the parent ClassLoader first), because they are shared between Flink\u0026rsquo;s core and the plugin/user code or the plugin/user-code facing APIs. The packages for these classes are configured via classloader.parent-first-patterns-default and classloader.parent-first-patterns-additional. To add new packages to be parent-first loaded, please set the classloader.parent-first-patterns-additional config option.
Avoiding Dynamic Classloading for User Code # All components (JobManager, TaskManager, Client, ApplicationMaster, \u0026hellip;) log their classpath setting on startup. They can be found as part of the environment information at the beginning of the log.
When running a setup where the JobManager and TaskManagers are exclusive to one particular job, one can put user code JAR files directly into the /lib folder to make sure they are part of the classpath and not loaded dynamically.
It usually works to put the job\u0026rsquo;s JAR file into the /lib directory. The JAR will be part of both the classpath (the AppClassLoader) and the dynamic class loader (FlinkUserCodeClassLoader). Because the AppClassLoader is the parent of the FlinkUserCodeClassLoader (and Java loads parent-first, by default), this should result in classes being loaded only once.
For setups where the job\u0026rsquo;s JAR file cannot be put to the /lib folder (for example because the setup is a session that is used by multiple jobs), it may still be possible to put common libraries to the /lib folder, and avoid dynamic class loading for those.
Manual Classloading in User Code # In some cases, a transformation function, source, or sink needs to manually load classes (dynamically via reflection). To do that, it needs the classloader that has access to the job\u0026rsquo;s classes.
In that case, the functions (or sources or sinks) can be made a RichFunction (for example RichMapFunction or RichWindowFunction) and access the user code class loader via getRuntimeContext().getUserCodeClassLoader().
X cannot be cast to X exceptions # In setups with dynamic classloading, you may see an exception in the style com.foo.X cannot be cast to com.foo.X. This means that multiple versions of the class com.foo.X have been loaded by different class loaders, and types of that class are attempted to be assigned to each other.
One common reason is that a library is not compatible with Flink\u0026rsquo;s inverted classloading approach. You can turn off inverted classloading to verify this (set classloader.resolve-order: parent-first in the Flink config) or exclude the library from inverted classloading (set classloader.parent-first-patterns-additional in the Flink config).
Another cause can be cached object instances, as produced by some libraries like Apache Avro, or by interning objects (for example via Guava\u0026rsquo;s Interners). The solution here is to either have a setup without any dynamic classloading, or to make sure that the respective library is fully part of the dynamically loaded code. The latter means that the library must not be added to Flink\u0026rsquo;s /lib folder, but must be part of the application\u0026rsquo;s fat-jar/uber-jar
Unloading of Dynamically Loaded Classes in User Code # All scenarios that involve dynamic user code classloading (sessions) rely on classes being unloaded again. Class unloading means that the Garbage Collector finds that no objects from a class exist and more, and thus removes the class (the code, static variable, metadata, etc).
Whenever a TaskManager starts (or restarts) a task, it will load that specific task\u0026rsquo;s code. Unless classes can be unloaded, this will become a memory leak, as new versions of classes are loaded and the total number of loaded classes accumulates over time. This typically manifests itself though a OutOfMemoryError: Metaspace.
Common causes for class leaks and suggested fixes:
Lingering Threads: Make sure the application functions/sources/sinks shuts down all threads. Lingering threads cost resources themselves and additionally typically hold references to (user code) objects, preventing garbage collection and unloading of the classes.
Interners: Avoid caching objects in special structures that live beyond the lifetime of the functions/sources/sinks. Examples are Guava\u0026rsquo;s interners, or Avro\u0026rsquo;s class/object caches in the serializers.
JDBC: JDBC drivers leak references outside the user code classloader. To ensure that these classes are only loaded once you should add the driver jars to Flink\u0026rsquo;s lib/ folder instead of bundling them in the user-jar. If you can\u0026rsquo;t guarantee that none of your user-jars bundle the driver, you have to additionally add the driver classes to the list of parent-first loaded classes via classloader.parent-first-patterns-additional.
A helpful tool for unloading dynamically loaded classes are the user code class loader release hooks. These are hooks which are executed prior to the unloading of a classloader. It is generally recommended to shutdown and unload resources as part of the regular function lifecycle (typically the close() methods). But in some cases (for example for static fields), it is better to unload once a classloader is certainly not needed anymore.
Class loader release hooks can be registered via the RuntimeContext.registerUserCodeClassLoaderReleaseHookIfAbsent() method.
Resolving Dependency Conflicts with Flink using the maven-shade-plugin. # A way to address dependency conflicts from the application developer\u0026rsquo;s side is to avoid exposing dependencies by shading them away.
Apache Maven offers the maven-shade-plugin, which allows one to change the package of a class after compiling it (so the code you are writing is not affected by the shading). For example if you have the com.amazonaws packages from the aws sdk in your user code jar, the shade plugin would relocate them into the org.myorg.shaded.com.amazonaws package, so that your code is calling your aws sdk version.
This documentation page explains relocating classes using the shade plugin.
Note that most of Flink\u0026rsquo;s dependencies, such as guava, netty, jackson, etc. are shaded away by the maintainers of Flink, so users usually don\u0026rsquo;t have to worry about it.
Back to top
`}),e.add({id:75,href:"/flink/flink-docs-master/docs/ops/debugging/flame_graphs/",title:"Flame Graphs",section:"Debugging",content:` Flame Graphs # Flame Graphs are a visualization that effectively surfaces answers to questions like:
Which methods are currently consuming CPU resources? How does consumption by one method compare to the others? Which series of calls on the stack led to executing a particular method? Flame Graph Flame Graphs are constructed by sampling stack traces a number of times. Each method call is presented by a bar, where the length of the bar is proportional to the number of times it is present in the samples.
Starting with Flink 1.13, Flame Graphs are natively supported in Flink. In order to produce a Flame Graph, navigate to the job graph of a running job, select an operator of interest and in the menu to the right click on the Flame Graph tab:
Operator\u0026rsquo;s On-CPU Flame Graph Any measurement process in and of itself inevitably affects the subject of measurement (see the double-split experiment). Sampling CPU stack traces is no exception. In order to prevent unintended impacts on production environments, Flame Graphs are currently available as an opt-in feature. To enable it, you\u0026rsquo;ll need to set rest.flamegraph.enabled: true in conf/flink-conf.yaml. We recommend enabling it in development and pre-production environments, but you should treat it as an experimental feature in production. Apart from the On-CPU Flame Graphs, Off-CPU and Mixed visualizations are available and can be switched between by using the selector at the top of the pane:
The Off-CPU Flame Graph visualizes blocking calls found in the samples. A distinction is made as follows:
On-CPU: Thread.State in [RUNNABLE, NEW] Off-CPU: Thread.State in [TIMED_WAITING, WAITING, BLOCKED] Off-CPU Flame Graph Mixed mode Flame Graphs are constructed from stack traces of threads in all possible states.
Flame Graph in Mixed Mode Sampling process # The collection of stack traces is done purely within the JVM, so only method calls within the Java runtime are visible (no system calls).
Flame Graph construction is performed at the level of an individual operator, i.e. all task threads of that operator are sampled in parallel and their stack traces are combined.
Note: Stack trace samples from all threads of an operator are combined together. If a method call consumes 100% of the resources in one of the parallel tasks but none in the others, the bottleneck might be obscured by being averaged out.
There are plans to address this limitation in the future by providing \u0026ldquo;drill down\u0026rdquo; visualizations to the task level.
`}),e.add({id:76,href:"/flink/flink-docs-master/docs/deployment/filesystems/gcs/",title:"Google Cloud Storage",section:"File Systems",content:` Google Cloud Storage # Google Cloud Storage (GCS) provides cloud storage for a variety of use cases. You can use it for reading and writing data, and for checkpoint storage when using FileSystemCheckpointStorage) with the streaming state backends.
You can use GCS objects like regular files by specifying paths in the following format:
gs://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt; The endpoint can either be a single file or a directory, for example:
// Read from GCS bucket env.readTextFile(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Write to GCS bucket stream.writeAsText(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Use GCS as checkpoint storage env.getCheckpointConfig().setCheckpointStorage(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); Note that these examples are not exhaustive and you can use GCS in other places as well, including your high availability setup or the EmbeddedRocksDBStateBackend; everywhere that Flink expects a FileSystem URI.
GCS File System plugin # Flink provides the flink-gs-fs-hadoop file system to write to GCS. This implementation is self-contained with no dependency footprint, so there is no need to add Hadoop to the classpath to use it.
flink-gs-fs-hadoop registers a FileSystem wrapper for URIs with the gs:// scheme. It uses Google\u0026rsquo;s gcs-connector Hadoop library to access GCS. It also uses Google\u0026rsquo;s google-cloud-storage library to provide RecoverableWriter support.
This file system can be used with the FileSystem connector.
To use flink-gs-fs-hadoop, copy the JAR file from the opt directory to the plugins directory of your Flink distribution before starting Flink, i.e.
mkdir ./plugins/gs-fs-hadoop cp ./opt/flink-gs-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/gs-fs-hadoop/ Configuration # The underlying Hadoop file system can be configured using the Hadoop configuration keys for gcs-connector by adding the configurations to your flink-conf.yaml.
For example, gcs-connector has a fs.gs.http.connect-timeout configuration key. If you want to change it, you need to set gs.http.connect-timeout: xyz in flink-conf.yaml. Flink will internally translate this back to fs.gs.http.connect-timeout.
You can also set gcs-connector options directly in the Hadoop core-site.xml configuration file, so long as the Hadoop configuration directory is made known to Flink via the env.hadoop.conf.dir Flink option or via the HADOOP_CONF_DIR environment variable.
flink-gs-fs-hadoop can also be configured by setting the following options in flink-conf.yaml:
Key Description gs.writer.temporary.bucket.name Set this property to choose a bucket to hold temporary blobs for in-progress writes via RecoverableWriter. If this property is not set, temporary blobs will be written to same bucket as the final file being written. In either case, temporary blobs are written with the prefix .inprogress/. It is recommended to choose a separate bucket in order to assign it a TTL, to provide a mechanism to clean up orphaned blobs that can occur when restoring from check/savepoints.
If you do use a separate bucket with a TTL for temporary blobs, attempts to restart jobs from check/savepoints after the TTL interval expires may fail. gs.writer.chunk.size Set this property to set the chunk size for writes via RecoverableWriter. If not set, a Google-determined default chunk size will be used. Authentication to access GCS # Most operations on GCS require authentication. To provide authentication credentials, either:
Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of the JSON credentials file, as described here, where JobManagers and TaskManagers run. This is the recommended method.
Set the google.cloud.auth.service.account.json.keyfile property in core-site.xml to the path to the JSON credentials file (and make sure that the Hadoop configuration directory is specified to Flink as described above):
\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;google.cloud.auth.service.account.json.keyfile\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;PATH TO GOOGLE AUTHENTICATION JSON FILE\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; For flink-gs-fs-hadoop to use credentials via either of these two methods, the use of service accounts for authentication must be enabled. This is enabled by default; however, it can be disabled in core-site.xml by setting:
\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;google.cloud.auth.service.account.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; gcs-connector supports additional options to provide authentication credentials besides the google.cloud.auth.service.account.json.keyfile option described above.
However, if you use any of those other options, the provided credentials will not be used by the google-cloud-storage library, which provides RecoverableWriter support, so Flink recoverable-write operations would be expected to fail.
For this reason, use of the gcs-connector authentication-credentials options other than google.cloud.auth.service.account.json.keyfile is not recommended.
Back to top
`}),e.add({id:77,href:"/flink/flink-docs-master/docs/libs/gelly/",title:"Graphs",section:"Libraries",content:""}),e.add({id:78,href:"/flink/flink-docs-master/docs/deployment/advanced/historyserver/",title:"History Server",section:"Advanced",content:` History Server # Flink has a history server that can be used to query the statistics of completed jobs after the corresponding Flink cluster has been shut down.
Furthermore, it exposes a REST API that accepts HTTP requests and responds with JSON data.
Overview # The HistoryServer allows you to query the status and statistics of completed jobs that have been archived by a JobManager.
After you have configured the HistoryServer and JobManager, you start and stop the HistoryServer via its corresponding startup script:
# Start or stop the HistoryServer bin/historyserver.sh (start|start-foreground|stop) By default, this server binds to localhost and listens at port 8082.
Currently, you can only run it as a standalone process.
Configuration # The configuration keys jobmanager.archive.fs.dir and historyserver.archive.fs.refresh-interval need to be adjusted for archiving and displaying archived jobs.
JobManager
The archiving of completed jobs happens on the JobManager, which uploads the archived job information to a file system directory. You can configure the directory to archive completed jobs in flink-conf.yaml by setting a directory via jobmanager.archive.fs.dir.
# Directory to upload completed job information jobmanager.archive.fs.dir: hdfs:///completed-jobs HistoryServer
The HistoryServer can be configured to monitor a comma-separated list of directories in via historyserver.archive.fs.dir. The configured directories are regularly polled for new archives; the polling interval can be configured via historyserver.archive.fs.refresh-interval.
# Monitor the following directories for completed jobs historyserver.archive.fs.dir: hdfs:///completed-jobs # Refresh every 10 seconds historyserver.archive.fs.refresh-interval: 10000 The contained archives are downloaded and cached in the local filesystem. The local directory for this is configured via historyserver.web.tmpdir.
Check out the configuration page for a complete list of configuration options.
Log Integration # Flink does not provide built-in methods for archiving logs of completed jobs. However, if you already have log archiving and browsing services, you can configure HistoryServer to integrate them (via historyserver.log.jobmanager.url-pattern and historyserver.log.taskmanager.url-pattern). In this way, you can directly link from HistoryServer WebUI to logs of the relevant JobManager / TaskManagers.
# HistoryServer will replace \u0026lt;jobid\u0026gt; with the relevant job id historyserver.log.jobmanager.url-pattern: http://my.log-browsing.url/\u0026lt;jobid\u0026gt; # HistoryServer will replace \u0026lt;jobid\u0026gt; and \u0026lt;tmid\u0026gt; with the relevant job id and taskmanager id historyserver.log.taskmanager.url-pattern: http://my.log-browsing.url/\u0026lt;jobid\u0026gt;/\u0026lt;tmid\u0026gt; Available Requests # Below is a list of available requests, with a sample JSON response. All requests are of the sample form http://hostname:8082/jobs, below we list only the path part of the URLs.
Values in angle brackets are variables, for example http://hostname:port/jobs/\u0026lt;jobid\u0026gt;/exceptions will have to requested for example as http://hostname:port/jobs/7684be6004e4e955c2a558a9bc463f65/exceptions.
/config /jobs/overview /jobs/\u0026lt;jobid\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices /jobs/\u0026lt;jobid\u0026gt;/config /jobs/\u0026lt;jobid\u0026gt;/exceptions /jobs/\u0026lt;jobid\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasktimes /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/taskmanagers /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt;/attempts/\u0026lt;attempt\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt;/attempts/\u0026lt;attempt\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/plan /jobs/\u0026lt;jobid\u0026gt;/jobmanager/config /jobs/\u0026lt;jobid\u0026gt;/jobmanager/environment /jobs/\u0026lt;jobid\u0026gt;/jobmanager/log-url /jobs/\u0026lt;jobid\u0026gt;/taskmanagers/\u0026lt;taskmanagerid\u0026gt;/log-url Back to top
`}),e.add({id:79,href:"/flink/flink-docs-master/docs/connectors/table/hive/hive_dialect/",title:"Hive Dialect",section:"Hive",content:` Hive Dialect # Flink allows users to write SQL statements in Hive syntax when Hive dialect is used. By providing compatibility with Hive syntax, we aim to improve the interoperability with Hive and reduce the scenarios when users need to switch between Flink and Hive in order to execute different statements.
Use Hive Dialect # Flink currently supports two SQL dialects: default and hive. You need to switch to Hive dialect before you can write in Hive syntax. The following describes how to set dialect with SQL Client and Table API. Also notice that you can dynamically switch dialect for each statement you execute. There\u0026rsquo;s no need to restart a session to use a different dialect.
SQL Client # SQL dialect can be specified via the table.sql-dialect property. Therefore you can set the initial dialect to use in the configuration section of the yaml file for your SQL Client.
execution: type: batch result-mode: table configuration: table.sql-dialect: hive You can also set the dialect after the SQL Client has launched.
Flink SQL\u0026gt; SET \u0026#39;table.sql-dialect\u0026#39; = \u0026#39;hive\u0026#39;; -- to use hive dialect [INFO] Session property has been set. Flink SQL\u0026gt; SET \u0026#39;table.sql-dialect\u0026#39; = \u0026#39;default\u0026#39;; -- to use default dialect [INFO] Session property has been set. Table API # You can set dialect for your TableEnvironment with Table API.
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // to use hive dialect tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE); // to use default dialect tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT); Python from pyflink.table import * settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) # to use hive dialect t_env.get_config().set_sql_dialect(SqlDialect.HIVE) # to use default dialect t_env.get_config().set_sql_dialect(SqlDialect.DEFAULT) DDL # This section lists the supported DDLs with the Hive dialect. We\u0026rsquo;ll mainly focus on the syntax here. You can refer to Hive doc for the semantics of each DDL statement.
CATALOG # Show # SHOW CURRENT CATALOG; DATABASE # Show # SHOW DATABASES; SHOW CURRENT DATABASE; Create # CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION fs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; Alter # Update Properties # ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); Update Owner # ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; Update Location # ALTER (DATABASE|SCHEMA) database_name SET LOCATION fs_path; Drop # DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; Use # USE database_name; TABLE # Show # SHOW TABLES; Create # CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [column_constraint] [COMMENT col_comment], ... [table_constraint])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [ [ROW FORMAT row_format] [STORED AS file_format] ] [LOCATION fs_path] [TBLPROPERTIES (property_name=property_value, ...)] row_format: : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE | RCFILE | ORC | PARQUET | AVRO | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname column_constraint: : NOT NULL [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]] table_constraint: : [CONSTRAINT constraint_name] PRIMARY KEY (col_name, ...) [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]] Alter # Rename # ALTER TABLE table_name RENAME TO new_table_name; Update Properties # ALTER TABLE table_name SET TBLPROPERTIES (property_name = property_value, property_name = property_value, ... ); Update Location # ALTER TABLE table_name [PARTITION partition_spec] SET LOCATION fs_path; The partition_spec, if present, needs to be a full spec, i.e. has values for all partition columns. And when it\u0026rsquo;s present, the operation will be applied to the corresponding partition instead of the table.
Update File Format # ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format; The partition_spec, if present, needs to be a full spec, i.e. has values for all partition columns. And when it\u0026rsquo;s present, the operation will be applied to the corresponding partition instead of the table.
Update SerDe Properties # ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, ... ) The partition_spec, if present, needs to be a full spec, i.e. has values for all partition columns. And when it\u0026rsquo;s present, the operation will be applied to the corresponding partition instead of the table.
Add Partitions # ALTER TABLE table_name ADD [IF NOT EXISTS] (PARTITION partition_spec [LOCATION fs_path])+; Drop Partitions # ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]; Add/Replace Columns # ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) [CASCADE|RESTRICT] Change Column # ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT]; Drop # DROP TABLE [IF EXISTS] table_name; VIEW # Create # CREATE VIEW [IF NOT EXISTS] view_name [(column_name, ...) ] [COMMENT view_comment] [TBLPROPERTIES (property_name = property_value, ...)] AS SELECT ...; Alter # Rename # ALTER VIEW view_name RENAME TO new_view_name; Update Properties # ALTER VIEW view_name SET TBLPROPERTIES (property_name = property_value, ... ); Update As Select # ALTER VIEW view_name AS select_statement; Drop # DROP VIEW [IF EXISTS] view_name; FUNCTION # Show # SHOW FUNCTIONS; Create # CREATE FUNCTION function_name AS class_name; Drop # DROP FUNCTION [IF EXISTS] function_name; DML \u0026amp; DQL Beta # Hive dialect supports a commonly-used subset of Hive\u0026rsquo;s DML and DQL. The following lists some examples of HiveQL supported by the Hive dialect.
SORT/CLUSTER/DISTRIBUTE BY Group By Join Union LATERAL VIEW Window Functions SubQueries CTE INSERT INTO dest schema Implicit type conversions In order to have better syntax and semantic compatibility, it\u0026rsquo;s highly recommended to use HiveModule and place it first in the module list, so that Hive built-in functions can be picked up during function resolution.
Hive dialect no longer supports Flink SQL queries. Please switch to default dialect if you\u0026rsquo;d like to write in Flink syntax.
Following is an example of using hive dialect to run some queries.
Flink SQL\u0026gt; create catalog myhive with (\u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39;); [INFO] Execute statement succeed. Flink SQL\u0026gt; use catalog myhive; [INFO] Execute statement succeed. Flink SQL\u0026gt; load module hive; [INFO] Execute statement succeed. Flink SQL\u0026gt; use modules hive,core; [INFO] Execute statement succeed. Flink SQL\u0026gt; set table.sql-dialect=hive; [INFO] Session property has been set. Flink SQL\u0026gt; select explode(array(1,2,3)); -- call hive udtf +-----+ | col | +-----+ | 1 | | 2 | | 3 | +-----+ 3 rows in set Flink SQL\u0026gt; create table tbl (key int,value string); [INFO] Execute statement succeed. Flink SQL\u0026gt; insert overwrite table tbl values (5,\u0026#39;e\u0026#39;),(1,\u0026#39;a\u0026#39;),(1,\u0026#39;a\u0026#39;),(3,\u0026#39;c\u0026#39;),(2,\u0026#39;b\u0026#39;),(3,\u0026#39;c\u0026#39;),(3,\u0026#39;c\u0026#39;),(4,\u0026#39;d\u0026#39;); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Flink SQL\u0026gt; select * from tbl cluster by key; -- run cluster by 2021-04-22 16:13:57,005 INFO org.apache.hadoop.mapred.FileInputFormat [] - Total input paths to process : 1 +-----+-------+ | key | value | +-----+-------+ | 1 | a | | 1 | a | | 5 | e | | 2 | b | | 3 | c | | 3 | c | | 3 | c | | 4 | d | +-----+-------+ 8 rows in set Notice # The following are some precautions for using the Hive dialect.
Hive dialect should only be used to process Hive meta objects, and requires the current catalog to be a HiveCatalog. Hive dialect only supports 2-part identifiers, so you can\u0026rsquo;t specify catalog for an identifier. While all Hive versions support the same syntax, whether a specific feature is available still depends on the Hive version you use. For example, updating database location is only supported in Hive-2.4.0 or later. Use HiveModule to run DML and DQL. Since Flink 1.15 you need to swap flink-table-planner-loader located in /lib with flink-table-planner_2.12 located in /opt to avoid the following exception. Please see FLINK-25128 for more details. `}),e.add({id:80,href:"/flink/flink-docs-master/docs/learn-flink/datastream_api/",title:"Intro to the DataStream API",section:"Learn Flink",content:` Intro to the DataStream API # The focus of this training is to broadly cover the DataStream API well enough that you will be able to get started writing streaming applications.
What can be Streamed? # Flink\u0026rsquo;s DataStream APIs for Java and Scala will let you stream anything they can serialize. Flink\u0026rsquo;s own serializer is used for
basic types, i.e., String, Long, Integer, Boolean, Array composite types: Tuples, POJOs, and Scala case classes and Flink falls back to Kryo for other types. It is also possible to use other serializers with Flink. Avro, in particular, is well supported.
Java tuples and POJOs # Flink\u0026rsquo;s native serializer can operate efficiently on tuples and POJOs.
Tuples # For Java, Flink defines its own Tuple0 thru Tuple25 types.
Tuple2\u0026lt;String, Integer\u0026gt; person = Tuple2.of(\u0026#34;Fred\u0026#34;, 35); // zero based index! String name = person.f0; Integer age = person.f1; POJOs # Flink recognizes a data type as a POJO type (and allows “by-name” field referencing) if the following conditions are fulfilled:
The class is public and standalone (no non-static inner class) The class has a public no-argument constructor All non-static, non-transient fields in the class (and all superclasses) are either public (and non-final) or have public getter- and setter- methods that follow the Java beans naming conventions for getters and setters. Example:
public class Person { public String name; public Integer age; public Person() {} public Person(String name, Integer age) { . . . } } Person person = new Person(\u0026#34;Fred Flintstone\u0026#34;, 35); Flink\u0026rsquo;s serializer supports schema evolution for POJO types.
Scala tuples and case classes # These work just as you\u0026rsquo;d expect.
Back to top
A Complete Example # This example takes a stream of records about people as input, and filters it to only include the adults.
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.api.common.functions.FilterFunction; public class Example { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Person\u0026gt; flintstones = env.fromElements( new Person(\u0026#34;Fred\u0026#34;, 35), new Person(\u0026#34;Wilma\u0026#34;, 35), new Person(\u0026#34;Pebbles\u0026#34;, 2)); DataStream\u0026lt;Person\u0026gt; adults = flintstones.filter(new FilterFunction\u0026lt;Person\u0026gt;() { @Override public boolean filter(Person person) throws Exception { return person.age \u0026gt;= 18; } }); adults.print(); env.execute(); } public static class Person { public String name; public Integer age; public Person() {} public Person(String name, Integer age) { this.name = name; this.age = age; } public String toString() { return this.name.toString() + \u0026#34;: age \u0026#34; + this.age.toString(); } } } Stream execution environment # Every Flink application needs an execution environment, env in this example. Streaming applications need to use a StreamExecutionEnvironment.
The DataStream API calls made in your application build a job graph that is attached to the StreamExecutionEnvironment. When env.execute() is called this graph is packaged up and sent to the JobManager, which parallelizes the job and distributes slices of it to the Task Managers for execution. Each parallel slice of your job will be executed in a task slot.
Note that if you don\u0026rsquo;t call execute(), your application won\u0026rsquo;t be run.
This distributed runtime depends on your application being serializable. It also requires that all dependencies are available to each node in the cluster.
Basic stream sources # The example above constructs a DataStream\u0026lt;Person\u0026gt; using env.fromElements(...). This is a convenient way to throw together a simple stream for use in a prototype or test. There is also a fromCollection(Collection) method on StreamExecutionEnvironment. So instead, you could do this:
List\u0026lt;Person\u0026gt; people = new ArrayList\u0026lt;Person\u0026gt;(); people.add(new Person(\u0026#34;Fred\u0026#34;, 35)); people.add(new Person(\u0026#34;Wilma\u0026#34;, 35)); people.add(new Person(\u0026#34;Pebbles\u0026#34;, 2)); DataStream\u0026lt;Person\u0026gt; flintstones = env.fromCollection(people); Another convenient way to get some data into a stream while prototyping is to use a socket
DataStream\u0026lt;String\u0026gt; lines = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999); or a file
DataStream\u0026lt;String\u0026gt; lines = env.readTextFile(\u0026#34;file:///path\u0026#34;); In real applications the most commonly used data sources are those that support low-latency, high throughput parallel reads in combination with rewind and replay \u0026ndash; the prerequisites for high performance and fault tolerance \u0026ndash; such as Apache Kafka, Kinesis, and various filesystems. REST APIs and databases are also frequently used for stream enrichment.
Basic stream sinks # The example above uses adults.print() to print its results to the task manager logs (which will appear in your IDE\u0026rsquo;s console, when running in an IDE). This will call toString() on each element of the stream.
The output looks something like this
1\u0026gt; Fred: age 35 2\u0026gt; Wilma: age 35 where 1\u0026gt; and 2\u0026gt; indicate which sub-task (i.e., thread) produced the output.
In production, commonly used sinks include the FileSink, various databases, and several pub-sub systems.
Debugging # In production, your application will run in a remote cluster or set of containers. And if it fails, it will fail remotely. The JobManager and TaskManager logs can be very helpful in debugging such failures, but it is much easier to do local debugging inside an IDE, which is something that Flink supports. You can set breakpoints, examine local variables, and step through your code. You can also step into Flink\u0026rsquo;s code, which can be a great way to learn more about its internals if you are curious to see how Flink works.
Back to top
Hands-on # At this point you know enough to get started coding and running a simple DataStream application. Clone the flink-training-repo , and after following the instructions in the README, do the first exercise: Filtering a Stream (Ride Cleansing) .
Back to top
Further Reading # Flink Serialization Tuning Vol. 1: Choosing your Serializer — if you can Anatomy of a Flink Program Data Sources Data Sinks DataStream Connectors Back to top
`}),e.add({id:81,href:"/flink/flink-docs-master/docs/dev/dataset/iterations/",title:"Iterations",section:"DataSet API (Legacy)",content:` Iterations # Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. Such algorithms are crucial in order to realize the promise of Big Data to extract meaningful information out of your data. With increasing interest to run these kinds of algorithms on very large data sets, there is a need to execute iterations in a massively parallel fashion.
Flink programs implement iterative algorithms by defining a step function and embedding it into a special iteration operator. There are two variants of this operator: Iterate and Delta Iterate. Both operators repeatedly invoke the step function on the current iteration state until a certain termination condition is reached.
Here, we provide background on both operator variants and outline their usage. The programming guide explains how to implement the operators in both Scala and Java. We also support both vertex-centric and gather-sum-apply iterations through Flink\u0026rsquo;s graph processing API, Gelly.
The following table provides an overview of both operators:
Iterate Delta Iterate Iteration Input Partial Solution Workset and Solution Set Step Function Arbitrary Data Flows State Update Next partial solution Next workset Changes to solution set Iteration Result Last partial solution Solution set state after last iteration Termination Maximum number of iterations (default) Custom aggregator convergence Maximum number of iterations or empty workset (default) Custom aggregator convergence Iterate Operator # The iterate operator covers the simple form of iterations: in each iteration, the step function consumes the entire input (the result of the previous iteration, or the initial data set), and computes the next version of the partial solution (e.g. map, reduce, join, etc.).
Iteration Input: Initial input for the first iteration from a data source or previous operators. Step Function: The step function will be executed in each iteration. It is an arbitrary data flow consisting of operators like map, reduce, join, etc. and depends on your specific task at hand. Next Partial Solution: In each iteration, the output of the step function will be fed back into the next iteration. Iteration Result: Output of the last iteration is written to a data sink or used as input to the following operators. There are multiple options to specify termination conditions for an iteration:
Maximum number of iterations: Without any further conditions, the iteration will be executed this many times. Custom aggregator convergence: Iterations allow to specify custom aggregators and convergence criteria like sum aggregate the number of emitted records (aggregator) and terminate if this number is zero (convergence criterion). You can also think about the iterate operator in pseudo-code:
IterationState state = getInitialState(); while (!terminationCriterion()) { state = step(state); } setFinalState(state); See the Programming Guide for details and code examples. Example: Incrementing Numbers # In the following example, we iteratively increment a set numbers:
Iteration Input: The initial input is read from a data source and consists of five single-field records (integers 1 to 5). Step function: The step function is a single map operator, which increments the integer field from i to i+1. It will be applied to every record of the input. Next Partial Solution: The output of the step function will be the output of the map operator, i.e. records with incremented integers. Iteration Result: After ten iterations, the initial numbers will have been incremented ten times, resulting in integers 11 to 15. // 1st 2nd 10th map(1) -\u0026gt; 2 map(2) -\u0026gt; 3 ... map(10) -\u0026gt; 11 map(2) -\u0026gt; 3 map(3) -\u0026gt; 4 ... map(11) -\u0026gt; 12 map(3) -\u0026gt; 4 map(4) -\u0026gt; 5 ... map(12) -\u0026gt; 13 map(4) -\u0026gt; 5 map(5) -\u0026gt; 6 ... map(13) -\u0026gt; 14 map(5) -\u0026gt; 6 map(6) -\u0026gt; 7 ... map(14) -\u0026gt; 15 Note that 1, 2, and 4 can be arbitrary data flows.
Delta Iterate Operator # The delta iterate operator covers the case of incremental iterations. Incremental iterations selectively modify elements of their solution and evolve the solution rather than fully recompute it.
Where applicable, this leads to more efficient algorithms, because not every element in the solution set changes in each iteration. This allows to focus on the hot parts of the solution and leave the cold parts untouched. Frequently, the majority of the solution cools down comparatively fast and the later iterations operate only on a small subset of the data.
Iteration Input: The initial workset and solution set are read from data sources or previous operators as input to the first iteration. Step Function: The step function will be executed in each iteration. It is an arbitrary data flow consisting of operators like map, reduce, join, etc. and depends on your specific task at hand. Next Workset/Update Solution Set: The next workset drives the iterative computation and will be fed back into the next iteration. Furthermore, the solution set will be updated and implicitly forwarded (it is not required to be rebuild). Both data sets can be updated by different operators of the step function. Iteration Result: After the last iteration, the solution set is written to a data sink or used as input to the following operators. The default termination condition for delta iterations is specified by the empty workset convergence criterion and a maximum number of iterations. The iteration will terminate when a produced next workset is empty or when the maximum number of iterations is reached. It is also possible to specify a custom aggregator and convergence criterion.
You can also think about the iterate operator in pseudo-code:
IterationState workset = getInitialState(); IterationState solution = getInitialSolution(); while (!terminationCriterion()) { (delta, workset) = step(workset, solution); solution.update(delta); } setFinalState(solution); See the Programming Guide for details and code examples. Example: Propagate Minimum in Graph # In the following example, every vertex has an ID and a coloring. Each vertex will propagate its vertex ID to neighboring vertices. The goal is to assign the minimum ID to every vertex in a subgraph. If a received ID is smaller then the current one, it changes to the color of the vertex with the received ID. One application of this can be found in community analysis or connected components computation.
The initial input is set as both workset and solution set. In the above figure, the colors visualize the evolution of the solution set. With each iteration, the color of the minimum ID is spreading in the respective subgraph. At the same time, the amount of work (exchanged and compared vertex IDs) decreases with each iteration. This corresponds to the decreasing size of the workset, which goes from all seven vertices to zero after three iterations, at which time the iteration terminates. The important observation is that the lower subgraph converges before the upper half does and the delta iteration is able to capture this with the workset abstraction.
In the upper subgraph ID 1 (orange) is the minimum ID. In the first iteration, it will get propagated to vertex 2, which will subsequently change its color to orange. Vertices 3 and 4 will receive ID 2 (in yellow) as their current minimum ID and change to yellow. Because the color of vertex 1 didn\u0026rsquo;t change in the first iteration, it can be skipped it in the next workset.
In the lower subgraph ID 5 (cyan) is the minimum ID. All vertices of the lower subgraph will receive it in the first iteration. Again, we can skip the unchanged vertices (vertex 5) for the next workset.
In the 2nd iteration, the workset size has already decreased from seven to five elements (vertices 2, 3, 4, 6, and 7). These are part of the iteration and further propagate their current minimum IDs. After this iteration, the lower subgraph has already converged (cold part of the graph), as it has no elements in the workset, whereas the upper half needs a further iteration (hot part of the graph) for the two remaining workset elements (vertices 3 and 4).
The iteration terminates, when the workset is empty after the 3rd iteration.
Superstep Synchronization # We referred to each execution of the step function of an iteration operator as a single iteration. In parallel setups, multiple instances of the step function are evaluated in parallel on different partitions of the iteration state. In many settings, one evaluation of the step function on all parallel instances forms a so called superstep, which is also the granularity of synchronization. Therefore, all parallel tasks of an iteration need to complete the superstep, before a next superstep will be initialized. Termination criteria will also be evaluated at superstep barriers.
`}),e.add({id:82,href:"/flink/flink-docs-master/docs/libs/gelly/iterative_graph_processing/",title:"Iterative Graph Processing",section:"Graphs",content:` Iterative Graph Processing # Gelly exploits Flink\u0026rsquo;s efficient iteration operators to support large-scale iterative graph processing. Currently, we provide implementations of the vertex-centric, scatter-gather, and gather-sum-apply models. In the following sections, we describe these abstractions and show how you can use them in Gelly.
Vertex-Centric Iterations # The vertex-centric model, also known as \u0026ldquo;think like a vertex\u0026rdquo; or \u0026ldquo;Pregel\u0026rdquo;, expresses computation from the perspective of a vertex in the graph. The computation proceeds in synchronized iteration steps, called supersteps. In each superstep, each vertex executes one user-defined function. Vertices communicate with other vertices through messages. A vertex can send a message to any other vertex in the graph, as long as it knows its unique ID.
The computational model is shown in the figure below. The dotted boxes correspond to parallelization units. In each superstep, all active vertices execute the same user-defined computation in parallel. Supersteps are executed synchronously, so that messages sent during one superstep are guaranteed to be delivered in the beginning of the next superstep.
To use vertex-centric iterations in Gelly, the user only needs to define the vertex compute function, ComputeFunction. This function and the maximum number of iterations to run are given as parameters to Gelly\u0026rsquo;s runVertexCentricIteration. This method will execute the vertex-centric iteration on the input Graph and return a new Graph, with updated vertex values. An optional message combiner, MessageCombiner, can be defined to reduce communication costs.
Let us consider computing Single-Source-Shortest-Paths with vertex-centric iterations. Initially, each vertex has a value of infinite distance, except from the source vertex, which has a value of zero. During the first superstep, the source propagates distances to its neighbors. During the following supersteps, each vertex checks its received messages and chooses the minimum distance among them. If this distance is smaller than its current value, it updates its state and produces messages for its neighbors. If a vertex does not change its value during a superstep, then it does not produce any messages for its neighbors for the next superstep. The algorithm converges when there are no value updates or the maximum number of supersteps has been reached. In this algorithm, a message combiner can be used to reduce the number of messages sent to a target vertex.
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the vertex-centric iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runVertexCentricIteration( new SSSPComputeFunction(), new SSSPCombiner(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // public static final class SSSPComputeFunction extends ComputeFunction\u0026lt;Long, Double, Double, Double\u0026gt; { public void compute(Vertex\u0026lt;Long, Double\u0026gt; vertex, MessageIterator\u0026lt;Double\u0026gt; messages) { double minDistance = (vertex.getId().equals(srcId)) ? 0d : Double.POSITIVE_INFINITY; for (Double msg : messages) { minDistance = Math.min(minDistance, msg); } if (minDistance \u0026lt; vertex.getValue()) { setNewVertexValue(minDistance); for (Edge\u0026lt;Long, Double\u0026gt; e: getEdges()) { sendMessageTo(e.getTarget(), minDistance + e.getValue()); } } } // message combiner public static final class SSSPCombiner extends MessageCombiner\u0026lt;Long, Double\u0026gt; { public void combineMessages(MessageIterator\u0026lt;Double\u0026gt; messages) { double minMessage = Double.POSITIVE_INFINITY; for (Double msg: messages) { minMessage = Math.min(minMessage, msg); } sendCombinedMessage(minMessage); } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the vertex-centric iteration val result = graph.runVertexCentricIteration(new SSSPComputeFunction, new SSSPCombiner, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // final class SSSPComputeFunction extends ComputeFunction[Long, Double, Double, Double] { override def compute(vertex: Vertex[Long, Double], messages: MessageIterator[Double]) = { var minDistance = if (vertex.getId.equals(srcId)) 0 else Double.MaxValue while (messages.hasNext) { val msg = messages.next if (msg \u0026lt; minDistance) { minDistance = msg } } if (vertex.getValue \u0026gt; minDistance) { setNewVertexValue(minDistance) for (edge: Edge[Long, Double] \u0026lt;- getEdges) { sendMessageTo(edge.getTarget, vertex.getValue + edge.getValue) } } } // message combiner final class SSSPCombiner extends MessageCombiner[Long, Double] { override def combineMessages(messages: MessageIterator[Double]) { var minDistance = Double.MaxValue while (messages.hasNext) { val msg = inMessages.next if (msg \u0026lt; minDistance) { minDistance = msg } } sendCombinedMessage(minMessage) } } Back to top
Configuring a Vertex-Centric Iteration # A vertex-centric iteration can be configured using a VertexCentricConfiguration object. Currently, the following parameters can be specified:
Name: The name for the vertex-centric iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined ComputeFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the ComputeFunction, using the addBroadcastSet() method.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration VertexCentricConfiguration parameters = new VertexCentricConfiguration(); // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;); // set the parallelism parameters.setParallelism(16); // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator()); // run the vertex-centric iteration, also passing the configuration parameters Graph\u0026lt;Long, Long, Double\u0026gt; result = graph.runVertexCentricIteration( new Compute(), null, maxIterations, parameters); // user-defined function public static final class Compute extends ComputeFunction { LongSumAggregator aggregator = new LongSumAggregator(); public void preSuperstep() { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;); } public void compute(Vertex\u0026lt;Long, Long\u0026gt; vertex, MessageIterator inMessages) { //do some computation Long partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue); // update the vertex value setNewVertexValue(...); } } Scala val graph: Graph[Long, Long, Double] = ... val parameters = new VertexCentricConfiguration // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;) // set the parallelism parameters.setParallelism(16) // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator) // run the vertex-centric iteration, also passing the configuration parameters val result = graph.runVertexCentricIteration(new Compute, new Combiner, maxIterations, parameters) // user-defined function final class Compute extends ComputeFunction { var aggregator = new LongSumAggregator override def preSuperstep { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;) } override def compute(vertex: Vertex[Long, Long], inMessages: MessageIterator[Long]) { //do some computation val partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue) // update the vertex value setNewVertexValue(...) } } Back to top
Scatter-Gather Iterations # The scatter-gather model, also known as \u0026ldquo;signal/collect\u0026rdquo; model, expresses computation from the perspective of a vertex in the graph. The computation proceeds in synchronized iteration steps, called supersteps. In each superstep, a vertex produces messages for other vertices and updates its value based on the messages it receives. To use scatter-gather iterations in Gelly, the user only needs to define how a vertex behaves in each superstep:
Scatter: produces the messages that a vertex will send to other vertices. Gather: updates the vertex value using received messages. Gelly provides methods for scatter-gather iterations. The user only needs to implement two functions, corresponding to the scatter and gather phases. The first function is a ScatterFunction, which allows a vertex to send out messages to other vertices. Messages are received during the same superstep as they are sent. The second function is GatherFunction, which defines how a vertex will update its value based on the received messages. These functions and the maximum number of iterations to run are given as parameters to Gelly\u0026rsquo;s runScatterGatherIteration. This method will execute the scatter-gather iteration on the input Graph and return a new Graph, with updated vertex values.
A scatter-gather iteration can be extended with information such as the total number of vertices, the in degree and out degree. Additionally, the neighborhood type (in/out/all) over which to run the scatter-gather iteration can be specified. By default, the updates from the in-neighbors are used to modify the current vertex\u0026rsquo;s state and messages are sent to out-neighbors.
Let us consider computing Single-Source-Shortest-Paths with scatter-gather iterations on the following graph and let vertex 1 be the source. In each superstep, each vertex sends a candidate distance message to all its neighbors. The message value is the sum of the current value of the vertex and the edge weight connecting this vertex with its neighbor. Upon receiving candidate distance messages, each vertex calculates the minimum distance and, if a shorter path has been discovered, it updates its value. If a vertex does not change its value during a superstep, then it does not produce messages for its neighbors for the next superstep. The algorithm converges when there are no value updates.
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the scatter-gather iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new MinDistanceMessenger(), new VertexDistanceUpdater(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // // scatter: messaging public static final class MinDistanceMessenger extends ScatterFunction\u0026lt;Long, Double, Double, Double\u0026gt; { public void sendMessages(Vertex\u0026lt;Long, Double\u0026gt; vertex) { for (Edge\u0026lt;Long, Double\u0026gt; edge : getEdges()) { sendMessageTo(edge.getTarget(), vertex.getValue() + edge.getValue()); } } } // gather: vertex update public static final class VertexDistanceUpdater extends GatherFunction\u0026lt;Long, Double, Double\u0026gt; { public void updateVertex(Vertex\u0026lt;Long, Double\u0026gt; vertex, MessageIterator\u0026lt;Double\u0026gt; inMessages) { Double minDistance = Double.MAX_VALUE; for (double msg : inMessages) { if (msg \u0026lt; minDistance) { minDistance = msg; } } if (vertex.getValue() \u0026gt; minDistance) { setNewVertexValue(minDistance); } } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the scatter-gather iteration val result = graph.runScatterGatherIteration(new MinDistanceMessenger, new VertexDistanceUpdater, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // // messaging final class MinDistanceMessenger extends ScatterFunction[Long, Double, Double, Double] { override def sendMessages(vertex: Vertex[Long, Double]) = { for (edge: Edge[Long, Double] \u0026lt;- getEdges) { sendMessageTo(edge.getTarget, vertex.getValue + edge.getValue) } } } // vertex update final class VertexDistanceUpdater extends GatherFunction[Long, Double, Double] { override def updateVertex(vertex: Vertex[Long, Double], inMessages: MessageIterator[Double]) = { var minDistance = Double.MaxValue while (inMessages.hasNext) { val msg = inMessages.next if (msg \u0026lt; minDistance) { minDistance = msg } } if (vertex.getValue \u0026gt; minDistance) { setNewVertexValue(minDistance) } } } Back to top
Configuring a Scatter-Gather Iteration # A scatter-gather iteration can be configured using a ScatterGatherConfiguration object. Currently, the following parameters can be specified:
Name: The name for the scatter-gather iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined ScatterFunction and GatherFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the ScatterFunction and GatherFunction, using the addBroadcastSetForUpdateFunction() and addBroadcastSetForMessagingFunction() methods, respectively.
Number of Vertices: Accessing the total number of vertices within the iteration. This property can be set using the setOptNumVertices() method. The number of vertices can then be accessed in the vertex update function and in the messaging function using the getNumberOfVertices() method. If the option is not set in the configuration, this method will return -1.
Degrees: Accessing the in/out degree for a vertex within an iteration. This property can be set using the setOptDegrees() method. The in/out degrees can then be accessed in the vertex update function and in the messaging function, per vertex using the getInDegree() and getOutDegree() methods. If the degrees option is not set in the configuration, these methods will return -1.
Messaging Direction: By default, a vertex sends messages to its out-neighbors and updates its value based on messages received from its in-neighbors. This configuration option allows users to change the messaging direction to either EdgeDirection.IN, EdgeDirection.OUT, EdgeDirection.ALL. The messaging direction also dictates the update direction which would be EdgeDirection.OUT, EdgeDirection.IN and EdgeDirection.ALL, respectively. This property can be set using the setDirection() method.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;); // set the parallelism parameters.setParallelism(16); // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator()); // run the scatter-gather iteration, also passing the configuration parameters Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters); // user-defined functions public static final class Messenger extends ScatterFunction {...} public static final class VertexUpdater extends GatherFunction { LongSumAggregator aggregator = new LongSumAggregator(); public void preSuperstep() { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;); } public void updateVertex(Vertex\u0026lt;Long, Long\u0026gt; vertex, MessageIterator inMessages) { //do some computation Long partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue); // update the vertex value setNewVertexValue(...); } } Scala val graph: Graph[Long, Double, Double] = ... val parameters = new ScatterGatherConfiguration // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;) // set the parallelism parameters.setParallelism(16) // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) // user-defined functions final class Messenger extends ScatterFunction {...} final class VertexUpdater extends GatherFunction { var aggregator = new LongSumAggregator override def preSuperstep { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;) } override def updateVertex(vertex: Vertex[Long, Long], inMessages: MessageIterator[Long]) { //do some computation val partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue) // update the vertex value setNewVertexValue(...) } } The following example illustrates the usage of the degree as well as the number of vertices options.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the number of vertices option to true parameters.setOptNumVertices(true); // set the degree option to true parameters.setOptDegrees(true); // run the scatter-gather iteration, also passing the configuration parameters Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters); // user-defined functions public static final class Messenger extends ScatterFunction { ... // retrieve the vertex out-degree outDegree = getOutDegree(); ... } public static final class VertexUpdater extends GatherFunction { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } Scala val graph: Graph[Long, Double, Double] = ... // configure the iteration val parameters = new ScatterGatherConfiguration // set the number of vertices option to true parameters.setOptNumVertices(true) // set the degree option to true parameters.setOptDegrees(true) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) // user-defined functions final class Messenger extends ScatterFunction { ... // retrieve the vertex out-degree val outDegree = getOutDegree ... } final class VertexUpdater extends GatherFunction { ... // get the number of vertices val numVertices = getNumberOfVertices ... } The following example illustrates the usage of the edge direction option. Vertices update their values to contain a list of all their in-neighbors.
Java Graph\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the messaging direction parameters.setDirection(EdgeDirection.IN); // run the scatter-gather iteration, also passing the configuration parameters DataSet\u0026lt;Vertex\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;\u0026gt;\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters) .getVertices(); // user-defined functions public static final class Messenger extends GatherFunction {...} public static final class VertexUpdater extends ScatterFunction {...} Scala val graph: Graph[Long, HashSet[Long], Double] = ... // configure the iteration val parameters = new ScatterGatherConfiguration // set the messaging direction parameters.setDirection(EdgeDirection.IN) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) .getVertices // user-defined functions final class Messenger extends ScatterFunction {...} final class VertexUpdater extends GatherFunction {...} Back to top
Gather-Sum-Apply Iterations # Like in the scatter-gather model, Gather-Sum-Apply also proceeds in synchronized iterative steps, called supersteps. Each superstep consists of the following three phases:
Gather: a user-defined function is invoked in parallel on the edges and neighbors of each vertex, producing a partial value. Sum: the partial values produced in the Gather phase are aggregated to a single value, using a user-defined reducer. Apply: each vertex value is updated by applying a function on the current value and the aggregated value produced by the Sum phase. Let us consider computing Single-Source-Shortest-Paths with GSA on the following graph and let vertex 1 be the source. During the Gather phase, we calculate the new candidate distances, by adding each vertex value with the edge weight. In Sum, the candidate distances are grouped by vertex ID and the minimum distance is chosen. In Apply, the newly calculated distance is compared to the current vertex value and the minimum of the two is assigned as the new value of the vertex.
Notice that, if a vertex does not change its value during a superstep, it will not calculate candidate distance during the next superstep. The algorithm converges when no vertex changes value.
To implement this example in Gelly GSA, the user only needs to call the runGatherSumApplyIteration method on the input graph and provide the GatherFunction, SumFunction and ApplyFunction UDFs. Iteration synchronization, grouping, value updates and convergence are handled by the system:
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the GSA iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runGatherSumApplyIteration( new CalculateDistances(), new ChooseMinDistance(), new UpdateDistance(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // // Gather private static final class CalculateDistances extends GatherFunction\u0026lt;Double, Double, Double\u0026gt; { public Double gather(Neighbor\u0026lt;Double, Double\u0026gt; neighbor) { return neighbor.getNeighborValue() + neighbor.getEdgeValue(); } } // Sum private static final class ChooseMinDistance extends SumFunction\u0026lt;Double, Double, Double\u0026gt; { public Double sum(Double newValue, Double currentValue) { return Math.min(newValue, currentValue); } } // Apply private static final class UpdateDistance extends ApplyFunction\u0026lt;Long, Double, Double\u0026gt; { public void apply(Double newDistance, Double oldDistance) { if (newDistance \u0026lt; oldDistance) { setResult(newDistance); } } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the GSA iteration val result = graph.runGatherSumApplyIteration(new CalculateDistances, new ChooseMinDistance, new UpdateDistance, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // // Gather final class CalculateDistances extends GatherFunction[Double, Double, Double] { override def gather(neighbor: Neighbor[Double, Double]): Double = { neighbor.getNeighborValue + neighbor.getEdgeValue } } // Sum final class ChooseMinDistance extends SumFunction[Double, Double, Double] { override def sum(newValue: Double, currentValue: Double): Double = { Math.min(newValue, currentValue) } } // Apply final class UpdateDistance extends ApplyFunction[Long, Double, Double] { override def apply(newDistance: Double, oldDistance: Double) = { if (newDistance \u0026lt; oldDistance) { setResult(newDistance) } } } Note that gather takes a Neighbor type as an argument. This is a convenience type which simply wraps a vertex with its neighboring edge.
For more examples of how to implement algorithms with the Gather-Sum-Apply model, check the GSAPageRank and GSAConnectedComponents library methods of Gelly.
Back to top
Configuring a Gather-Sum-Apply Iteration # A GSA iteration can be configured using a GSAConfiguration object. Currently, the following parameters can be specified:
Name: The name for the GSA iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined GatherFunction, SumFunction and ApplyFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the GatherFunction, SumFunction and ApplyFunction, using the methods addBroadcastSetForGatherFunction(), addBroadcastSetForSumFunction() and addBroadcastSetForApplyFunction methods, respectively.
Number of Vertices: Accessing the total number of vertices within the iteration. This property can be set using the setOptNumVertices() method. The number of vertices can then be accessed in the gather, sum and/or apply functions by using the getNumberOfVertices() method. If the option is not set in the configuration, this method will return -1.
Neighbor Direction: By default values are gathered from the out neighbors of the Vertex. This can be modified using the setDirection() method.
The following example illustrates the usage of the number of vertices option.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration GSAConfiguration parameters = new GSAConfiguration(); // set the number of vertices option to true parameters.setOptNumVertices(true); // run the gather-sum-apply iteration, also passing the configuration parameters Graph\u0026lt;Long, Long, Long\u0026gt; result = graph.runGatherSumApplyIteration( new Gather(), new Sum(), new Apply(), maxIterations, parameters); // user-defined functions public static final class Gather { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } public static final class Sum { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } public static final class Apply { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } Scala val graph: Graph[Long, Double, Double] = ... // configure the iteration val parameters = new GSAConfiguration // set the number of vertices option to true parameters.setOptNumVertices(true) // run the gather-sum-apply iteration, also passing the configuration parameters val result = graph.runGatherSumApplyIteration(new Gather, new Sum, new Apply, maxIterations, parameters) // user-defined functions final class Gather { ... // get the number of vertices val numVertices = getNumberOfVertices ... } final class Sum { ... // get the number of vertices val numVertices = getNumberOfVertices ... } final class Apply { ... // get the number of vertices val numVertices = getNumberOfVertices ... } The following example illustrates the usage of the edge direction option. Java Graph\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;, Double\u0026gt; graph = ...; // configure the iteration GSAConfiguration parameters = new GSAConfiguration(); // set the messaging direction parameters.setDirection(EdgeDirection.IN); // run the gather-sum-apply iteration, also passing the configuration parameters DataSet\u0026lt;Vertex\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;\u0026gt;\u0026gt; result = graph.runGatherSumApplyIteration( new Gather(), new Sum(), new Apply(), maxIterations, parameters) .getVertices(); Scala val graph: Graph[Long, HashSet[Long], Double] = ... // configure the iteration val parameters = new GSAConfiguration // set the messaging direction parameters.setDirection(EdgeDirection.IN) // run the gather-sum-apply iteration, also passing the configuration parameters val result = graph.runGatherSumApplyIteration(new Gather, new Sum, new Apply, maxIterations, parameters) .getVertices() Back to top
Iteration Abstractions Comparison # Although the three iteration abstractions in Gelly seem quite similar, understanding their differences can lead to more performant and maintainable programs. Among the three, the vertex-centric model is the most general model and supports arbitrary computation and messaging for each vertex. In the scatter-gather model, the logic of producing messages is decoupled from the logic of updating vertex values. Thus, programs written using scatter-gather are sometimes easier to follow and maintain. Separating the messaging phase from the vertex value update logic not only makes some programs easier to follow but might also have a positive impact on performance. Scatter-gather implementations typically have lower memory requirements, because concurrent access to the inbox (messages received) and outbox (messages to send) data structures is not required. However, this characteristic also limits expressiveness and makes some computation patterns non-intuitive. Naturally, if an algorithm requires a vertex to concurrently access its inbox and outbox, then the expression of this algorithm in scatter-gather might be problematic. Strongly Connected Components and Approximate Maximum Weight Matching are examples of such graph algorithms. A direct consequence of this restriction is that vertices cannot generate messages and update their states in the same phase. Thus, deciding whether to propagate a message based on its content would require storing it in the vertex value, so that the gather phase has access to it, in the following iteration step. Similarly, if the vertex update logic includes computation over the values of the neighboring edges, these have to be included inside a special message passed from the scatter to the gather phase. Such workarounds often lead to higher memory requirements and non-elegant, hard to understand algorithm implementations.
Gather-sum-apply iterations are also quite similar to scatter-gather iterations. In fact, any algorithm which can be expressed as a GSA iteration can also be written in the scatter-gather model. The messaging phase of the scatter-gather model is equivalent to the Gather and Sum steps of GSA: Gather can be seen as the phase where the messages are produced and Sum as the phase where they are routed to the target vertex. Similarly, the value update phase corresponds to the Apply step.
The main difference between the two implementations is that the Gather phase of GSA parallelizes the computation over the edges, while the messaging phase distributes the computation over the vertices. Using the SSSP examples above, we see that in the first superstep of the scatter-gather case, vertices 1, 2 and 3 produce messages in parallel. Vertex 1 produces 3 messages, while vertices 2 and 3 produce one message each. In the GSA case on the other hand, the computation is parallelized over the edges: the three candidate distance values of vertex 1 are produced in parallel. Thus, if the Gather step contains \u0026ldquo;heavy\u0026rdquo; computation, it might be a better idea to use GSA and spread out the computation, instead of burdening a single vertex. Another case when parallelizing over the edges might prove to be more efficient is when the input graph is skewed (some vertices have a lot more neighbors than others).
Another difference between the two implementations is that the scatter-gather implementation uses a coGroup operator internally, while GSA uses a reduce. Therefore, if the function that combines neighbor values (messages) requires the whole group of values for the computation, scatter-gather should be used. If the update function is associative and commutative, then the GSA\u0026rsquo;s reducer is expected to give a more efficient implementation, as it can make use of a combiner.
Another thing to note is that GSA works strictly on neighborhoods, while in the vertex-centric and scatter-gather models, a vertex can send a message to any vertex, given that it knows its vertex ID, regardless of whether it is a neighbor. Finally, in Gelly\u0026rsquo;s scatter-gather implementation, one can choose the messaging direction, i.e. the direction in which updates propagate. GSA does not support this yet, so each vertex will be updated based on the values of its in-neighbors only.
The main differences among the Gelly iteration models are shown in the table below.
Iteration Model Update Function Update Logic Communication Scope Communication Logic Vertex-Centric arbitrary arbitrary any vertex arbitrary Scatter-Gather arbitrary based on received messages any vertex based on vertex state Gather-Sum-Apply associative and commutative based on neighbors' values neighborhood based on vertex state Back to top
`}),e.add({id:83,href:"/flink/flink-docs-master/docs/dev/datastream/operators/joining/",title:"Joining",section:"Operators",content:` Joining # Window Join # A window join joins the elements of two streams that share a common key and lie in the same window. These windows can be defined by using a window assigner and are evaluated on elements from both of the streams.
The elements from both sides are then passed to a user-defined JoinFunction or FlatJoinFunction where the user can emit results that meet the join criteria.
The general usage can be summarized as follows:
stream.join(otherStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(\u0026lt;WindowAssigner\u0026gt;) .apply(\u0026lt;JoinFunction\u0026gt;); Some notes on semantics:
The creation of pairwise combinations of elements of the two streams behaves like an inner-join, meaning elements from one stream will not be emitted if they don\u0026rsquo;t have a corresponding element from the other stream to be joined with. Those elements that do get joined will have as their timestamp the largest timestamp that still lies in the respective window. For example a window with [5, 10) as its boundaries would result in the joined elements having 9 as their timestamp. In the following section we are going to give an overview over how different kinds of window joins behave using some exemplary scenarios.
Tumbling Window Join # When performing a tumbling window join, all elements with a common key and a common tumbling window are joined as pairwise combinations and passed on to a JoinFunction or FlatJoinFunction. Because this behaves like an inner join, elements of one stream that do not have elements from another stream in their tumbling window are not emitted!
As illustrated in the figure, we define a tumbling window with the size of 2 milliseconds, which results in windows of the form [0,1], [2,3], .... The image shows the pairwise combinations of all elements in each window which will be passed on to the JoinFunction. Note that in the tumbling window [6,7] nothing is emitted because no elements exist in the green stream to be joined with the orange elements ⑥ and ⑦.
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } Sliding Window Join # When performing a sliding window join, all elements with a common key and common sliding window are joined as pairwise combinations and passed on to the JoinFunction or FlatJoinFunction. Elements of one stream that do not have elements from the other stream in the current sliding window are not emitted! Note that some elements might be joined in one sliding window but not in another!
In this example we are using sliding windows with a size of two milliseconds and slide them by one millisecond, resulting in the sliding windows [-1, 0],[0,1],[1,2],[2,3], …. The joined elements below the x-axis are the ones that are passed to the JoinFunction for each sliding window. Here you can also see how for example the orange ② is joined with the green ③ in the window [2,3], but is not joined with anything in the window [1,2].
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } Session Window Join # When performing a session window join, all elements with the same key that when \u0026ldquo;combined\u0026rdquo; fulfill the session criteria are joined in pairwise combinations and passed on to the JoinFunction or FlatJoinFunction. Again this performs an inner join, so if there is a session window that only contains elements from one stream, no output will be emitted!
Here we define a session window join where each session is divided by a gap of at least 1ms. There are three sessions, and in the first two sessions the joined elements from both streams are passed to the JoinFunction. In the third session there are no elements in the green stream, so ⑧ and ⑨ are not joined!
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } Interval Join # The interval join joins elements of two streams (we\u0026rsquo;ll call them A \u0026amp; B for now) with a common key and where elements of stream B have timestamps that lie in a relative time interval to timestamps of elements in stream A.
This can also be expressed more formally as b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] or a.timestamp + lowerBound \u0026lt;= b.timestamp \u0026lt;= a.timestamp + upperBound
where a and b are elements of A and B that share a common key. Both the lower and upper bound can be either negative or positive as long as the lower bound is always smaller or equal to the upper bound. The interval join currently only performs inner joins.
When a pair of elements are passed to the ProcessJoinFunction, they will be assigned with the larger timestamp (which can be accessed via the ProcessJoinFunction.Context) of the two elements.
The interval join currently only supports event time. In the example above, we join two streams \u0026lsquo;orange\u0026rsquo; and \u0026lsquo;green\u0026rsquo; with a lower bound of -2 milliseconds and an upper bound of +1 millisecond. Be default, these boundaries are inclusive, but .lowerBoundExclusive() and .upperBoundExclusive() can be applied to change the behaviour.
Using the more formal notation again this will translate to
orangeElem.ts + lowerBound \u0026lt;= greenElem.ts \u0026lt;= orangeElem.ts + upperBound
as indicated by the triangles.
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream .keyBy(\u0026lt;KeySelector\u0026gt;) .intervalJoin(greenStream.keyBy(\u0026lt;KeySelector\u0026gt;)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process (new ProcessJoinFunction\u0026lt;Integer, Integer, String(){ @Override public void processElement(Integer left, Integer right, Context ctx, Collector\u0026lt;String\u0026gt; out) { out.collect(left + \u0026#34;,\u0026#34; + right); } }); Scala import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream .keyBy(elem =\u0026gt; /* select key */) .intervalJoin(greenStream.keyBy(elem =\u0026gt; /* select key */)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process(new ProcessJoinFunction[Integer, Integer, String] { override def processElement(left: Integer, right: Integer, ctx: ProcessJoinFunction[Integer, Integer, String]#Context, out: Collector[String]): Unit = { out.collect(left + \u0026#34;,\u0026#34; + right) } }) Back to top
`}),e.add({id:84,href:"/flink/flink-docs-master/docs/connectors/table/formats/json/",title:"JSON",section:"Formats",content:` JSON Format # Format: Serialization Schema Format: Deserialization Schema
The JSON format allows to read and write JSON data based on an JSON schema. Currently, the JSON schema is derived from table schema.
The JSON format supports append-only streams, unless you\u0026rsquo;re using a connector that explicitly support retract streams and/or upsert streams like the Upsert Kafka connector. If you need to write retract streams and/or upsert streams, we suggest you to look at CDC JSON formats like Debezium JSON and Canal JSON.
Dependencies # In order to use the Json format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in How to create a table with JSON format # Here is an example to create a table using Kafka connector and JSON format.
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'json'. json.fail-on-missing-field optional no false Boolean Whether to fail if a field is missing or not. json.ignore-parse-errors optional no false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. json.timestamp-format.standard optional yes 'SQL' String Specify the input and output timestamp format for TIMESTAMP and TIMESTAMP_LTZ type. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input TIMESTAMP values in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g "2020-12-30 12:13:14.123", parse input TIMESTAMP_LTZ values in "yyyy-MM-dd HH:mm:ss.s{precision}'Z'" format, e.g "2020-12-30 12:13:14.123Z" and output timestamp in the same format. Option 'ISO-8601'will parse input TIMESTAMP in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g "2020-12-30T12:13:14.123" parse input TIMESTAMP_LTZ in "yyyy-MM-ddTHH:mm:ss.s{precision}'Z'" format, e.g "2020-12-30T12:13:14.123Z" and output timestamp in the same format. json.map-null-key.mode optional yes 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by json.map-null-key.literal option. json.map-null-key.literal optional yes 'null' String Specify string literal to replace null key when 'json.map-null-key.mode' is LITERAL. json.encode.decimal-as-plain-number optional yes false Boolean Encode all decimals as plain numbers instead of possible scientific notations. By default, decimals may be written using scientific notation. For example, 0.000000027 is encoded as 2.7E-8 by default, and will be written as 0.000000027 if set this option to true. Data Type Mapping # Currently, the JSON schema is always derived from table schema. Explicitly defining an JSON schema is not supported yet.
Flink JSON format uses jackson databind API to parse and generate JSON string.
The following table lists the type mapping from Flink type to JSON type.
Flink SQL type JSON type CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY string with encoding: base64 DECIMAL number TINYINT number SMALLINT number INT number BIGINT number FLOAT number DOUBLE number DATE string with format: date TIME string with format: time TIMESTAMP string with format: date-time TIMESTAMP_WITH_LOCAL_TIME_ZONE string with format: date-time (with UTC time zone) INTERVAL number ARRAY array MAP / MULTISET object ROW object `}),e.add({id:85,href:"/flink/flink-docs-master/docs/connectors/datastream/kafka/",title:"Kafka",section:"DataStream Connectors",content:` Apache Kafka Connector # Flink provides an Apache Kafka connector for reading data from and writing data to Kafka topics with exactly-once guarantees.
Dependency # Apache Flink ships with a universal Kafka connector which attempts to track the latest version of the Kafka client. The version of the client it uses may change between Flink releases. Modern Kafka clients are backwards compatible with broker versions 0.10.0 or later. For details on Kafka compatibility, please refer to the official Kafka documentation.
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Flink\u0026rsquo;s streaming connectors are not part of the binary distribution. See how to link with them for cluster execution here.
In order to use the Kafka connector in PyFlink jobs, the following dependencies are required: Kafka version PyFlink JAR universal Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. Kafka Source # This part describes the Kafka source based on the new data source API. Usage # Kafka source provides a builder class for constructing instance of KafkaSource. The code snippet below shows how to build a KafkaSource to consume messages from the earliest offset of topic \u0026ldquo;input-topic\u0026rdquo;, with consumer group \u0026ldquo;my-group\u0026rdquo; and deserialize only the value of message as string.
Java KafkaSource\u0026lt;String\u0026gt; source = KafkaSource.\u0026lt;String\u0026gt;builder() .setBootstrapServers(brokers) .setTopics(\u0026#34;input-topic\u0026#34;) .setGroupId(\u0026#34;my-group\u0026#34;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .build(); env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Kafka Source\u0026#34;); Python source = KafkaSource.builder() \\ .set_bootstrap_servers(brokers) \\ .set_topics(\u0026#34;input-topic\u0026#34;) \\ .set_group_id(\u0026#34;my-group\u0026#34;) \\ .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\ .set_value_only_deserializer(SimpleStringSchema()) \\ .build() env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;Kafka Source\u0026#34;) The following properties are required for building a KafkaSource:
Bootstrap servers, configured by setBootstrapServers(String) Topics / partitions to subscribe, see the following Topic-partition subscription for more details. Deserializer to parse Kafka messages, see the following Deserializer for more details. Topic-partition Subscription # Kafka source provide 3 ways of topic-partition subscription:
Topic list, subscribing messages from all partitions in a list of topics. For example: Java KafkaSource.builder().setTopics(\u0026#34;topic-a\u0026#34;, \u0026#34;topic-b\u0026#34;); Python KafkaSource.builder().set_topics(\u0026#34;topic-a\u0026#34;, \u0026#34;topic-b\u0026#34;) Topic pattern, subscribing messages from all topics whose name matches the provided regular expression. For example: Java KafkaSource.builder().setTopicPattern(\u0026#34;topic.*\u0026#34;); Python KafkaSource.builder().set_topic_pattern(\u0026#34;topic.*\u0026#34;) Partition set, subscribing partitions in the provided partition set. For example: Java final HashSet\u0026lt;TopicPartition\u0026gt; partitionSet = new HashSet\u0026lt;\u0026gt;(Arrays.asList( new TopicPartition(\u0026#34;topic-a\u0026#34;, 0), // Partition 0 of topic \u0026#34;topic-a\u0026#34; new TopicPartition(\u0026#34;topic-b\u0026#34;, 5))); // Partition 5 of topic \u0026#34;topic-b\u0026#34; KafkaSource.builder().setPartitions(partitionSet); Python partition_set = { KafkaTopicPartition(\u0026#34;topic-a\u0026#34;, 0), KafkaTopicPartition(\u0026#34;topic-b\u0026#34;, 5) } KafkaSource.builder().set_partitions(partition_set) Deserializer # A deserializer is required for parsing Kafka messages. Deserializer (Deserialization schema) can be configured by setDeserializer(KafkaRecordDeserializationSchema), where KafkaRecordDeserializationSchema defines how to deserialize a Kafka ConsumerRecord.
If only the value of Kafka ConsumerRecord is needed, you can use setValueOnlyDeserializer(DeserializationSchema) in the builder, where DeserializationSchema defines how to deserialize binaries of Kafka message value.
You can also use a Kafka Deserializer for deserializing Kafka message value. For example using StringDeserializer for deserializing Kafka message value as string:
import org.apache.kafka.common.serialization.StringDeserializer; KafkaSource.\u0026lt;String\u0026gt;builder() .setDeserializer(KafkaRecordDeserializationSchema.valueOnly(StringDeserializer.class)); Currently, PyFlink only supports set_value_only_deserializer to customize deserialization of the value of a Kafka record.
KafkaSource.builder().set_value_only_deserializer(SimpleStringSchema()) Starting Offset # Kafka source is able to consume messages starting from different offsets by specifying OffsetsInitializer. Built-in initializers include:
Java KafkaSource.builder() // Start from committed offset of the consuming group, without reset strategy .setStartingOffsets(OffsetsInitializer.committedOffsets()) // Start from committed offset, also use EARLIEST as reset strategy if committed offset doesn\u0026#39;t exist .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST)) // Start from the first record whose timestamp is greater than or equals a timestamp (milliseconds) .setStartingOffsets(OffsetsInitializer.timestamp(1657256176000L)) // Start from earliest offset .setStartingOffsets(OffsetsInitializer.earliest()) // Start from latest offset .setStartingOffsets(OffsetsInitializer.latest()); Python KafkaSource.builder() \\ # Start from committed offset of the consuming group, without reset strategy .set_starting_offsets(KafkaOffsetsInitializer.committed_offsets()) \\ # Start from committed offset, also use EARLIEST as reset strategy if committed offset doesn\u0026#39;t exist .set_starting_offsets(KafkaOffsetsInitializer.committed_offsets(KafkaOffsetResetStrategy.EARLIEST)) \\ # Start from the first record whose timestamp is greater than or equals a timestamp (milliseconds) .set_starting_offsets(KafkaOffsetsInitializer.timestamp(1657256176000)) \\ # Start from the earliest offset .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\ # Start from the latest offset .set_starting_offsets(KafkaOffsetsInitializer.latest()) You can also implement a custom offsets initializer if built-in initializers above cannot fulfill your requirement. (Not supported in PyFlink)
If offsets initializer is not specified, OffsetsInitializer.earliest() will be used by default.
Boundedness # Kafka source is designed to support both streaming and batch running mode. By default, the KafkaSource is set to run in streaming manner, thus never stops until Flink job fails or is cancelled. You can use setBounded(OffsetsInitializer) to specify stopping offsets and set the source running in batch mode. When all partitions have reached their stopping offsets, the source will exit.
You can also set KafkaSource running in streaming mode, but still stop at the stopping offset by using setUnbounded(OffsetsInitializer). The source will exit when all partitions reach their specified stopping offset.
Additional Properties # In addition to properties described above, you can set arbitrary properties for KafkaSource and KafkaConsumer by using setProperties(Properties) and setProperty(String, String). KafkaSource has following options for configuration:
client.id.prefix defines the prefix to use for Kafka consumer\u0026rsquo;s client ID partition.discovery.interval.ms defines the interval im milliseconds for Kafka source to discover new partitions. See Dynamic Partition Discovery below for more details. register.consumer.metrics specifies whether to register metrics of KafkaConsumer in Flink metric group commit.offsets.on.checkpoint specifies whether to commit consuming offsets to Kafka brokers on checkpoint For configurations of KafkaConsumer, you can refer to Apache Kafka documentation for more details.
Please note that the following keys will be overridden by the builder even if it is configured:
key.deserializer is always set to ByteArrayDeserializer value.deserializer is always set to ByteArrayDeserializer auto.offset.reset.strategy is overridden by OffsetsInitializer#getAutoOffsetResetStrategy() for the starting offsets partition.discovery.interval.ms is overridden to -1 when setBounded(OffsetsInitializer) has been invoked Dynamic Partition Discovery # In order to handle scenarios like topic scaling-out or topic creation without restarting the Flink job, Kafka source can be configured to periodically discover new partitions under provided topic-partition subscribing pattern. To enable partition discovery, set a non-negative value for property partition.discovery.interval.ms:
Java KafkaSource.builder() .setProperty(\u0026#34;partition.discovery.interval.ms\u0026#34;, \u0026#34;10000\u0026#34;); // discover new partitions per 10 seconds Python KafkaSource.builder() \\ .set_property(\u0026#34;partition.discovery.interval.ms\u0026#34;, \u0026#34;10000\u0026#34;) # discover new partitions per 10 seconds Partition discovery is disabled by default. You need to explicitly set the partition discovery interval to enable this feature. Event Time and Watermarks # By default, the record will use the timestamp embedded in Kafka ConsumerRecord as the event time. You can define your own WatermarkStrategy for extract event time from the record itself, and emit watermark downstream:
env.fromSource(kafkaSource, new CustomWatermarkStrategy(), \u0026#34;Kafka Source With Custom Watermark Strategy\u0026#34;); This documentation describes details about how to define a WatermarkStrategy. (Not supported in PyFlink)
Idleness # The Kafka Source does not go automatically in an idle state if the parallelism is higher than the number of partitions. You will either need to lower the parallelism or add an idle timeout to the watermark strategy. If no records flow in a partition of a stream for that amount of time, then that partition is considered \u0026ldquo;idle\u0026rdquo; and will not hold back the progress of watermarks in downstream operators.
This documentation describes details about how to define a WatermarkStrategy#withIdleness.
Consumer Offset Committing # Kafka source commits the current consuming offset when checkpoints are completed, for ensuring the consistency between Flink\u0026rsquo;s checkpoint state and committed offsets on Kafka brokers.
If checkpointing is not enabled, Kafka source relies on Kafka consumer\u0026rsquo;s internal automatic periodic offset committing logic, configured by enable.auto.commit and auto.commit.interval.ms in the properties of Kafka consumer.
Note that Kafka source does NOT rely on committed offsets for fault tolerance. Committing offset is only for exposing the progress of consumer and consuming group for monitoring.
Monitoring # Kafka source exposes the following metrics in the respective scope.
Scope of Metric # Scope Metrics User Variables Description Type Operator currentEmitEventTimeLag n/a The time span from the record event timestamp to the time the record is emitted by the source connector¹: currentEmitEventTimeLag = EmitTime - EventTime. Gauge watermarkLag n/a The time span that the watermark lags behind the wall clock time: watermarkLag = CurrentTime - Watermark Gauge sourceIdleTime n/a The time span that the source has not processed any record: sourceIdleTime = CurrentTime - LastRecordProcessTime Gauge pendingRecords n/a The number of records that have not been fetched by the source. e.g. the available records after the consumer offset in a Kafka partition. Gauge KafkaSourceReader.commitsSucceeded n/a The total number of successful offset commits to Kafka, if offset committing is turned on and checkpointing is enabled. Counter KafkaSourceReader.commitsFailed n/a The total number of offset commit failures to Kafka, if offset committing is turned on and checkpointing is enabled. Note that committing offsets back to Kafka is only a means to expose consumer progress, so a commit failure does not affect the integrity of Flink's checkpointed partition offsets. Counter KafkaSourceReader.committedOffsets topic, partition The last successfully committed offsets to Kafka, for each partition. A particular partition's metric can be specified by topic name and partition id. Gauge KafkaSourceReader.currentOffsets topic, partition The consumer's current read offset, for each partition. A particular partition's metric can be specified by topic name and partition id. Gauge ¹ This metric is an instantaneous value recorded for the last processed record. This metric is provided because latency histogram could be expensive. The instantaneous latency value is usually a good enough indication of the latency.
Kafka Consumer Metrics # All metrics of Kafka consumer are also registered under group KafkaSourceReader.KafkaConsumer. For example, Kafka consumer metric \u0026ldquo;records-consumed-total\u0026rdquo; will be reported in metric: \u0026lt;some_parent_groups\u0026gt;.operator.KafkaSourceReader.KafkaConsumer.records-consumed-total .
You can configure whether to register Kafka consumer\u0026rsquo;s metric by configuring option register.consumer.metrics. This option will be set as true by default.
For metrics of Kafka consumer, you can refer to Apache Kafka Documentation for more details.
In case you experience a warning with a stack trace containing javax.management.InstanceAlreadyExistsException: kafka.consumer:[...], you are probably trying to register multiple KafkaConsumers with the same client.id. The warning indicates that not all available metrics are correctly forwarded to the metrics system. You must ensure that a different client.id.prefix for every KafkaSource is configured and that no other KafkaConsumer in your job uses the same client.id.
Security # In order to enable security configurations including encryption and authentication, you just need to setup security configurations as additional properties to the Kafka source. The code snippet below shows configuring Kafka source to use PLAIN as SASL mechanism and provide JAAS configuration:
Java KafkaSource.builder() .setProperty(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_PLAINTEXT\u0026#34;) .setProperty(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;PLAIN\u0026#34;) .setProperty(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;); Python KafkaSource.builder() \\ .set_property(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_PLAINTEXT\u0026#34;) \\ .set_property(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;PLAIN\u0026#34;) \\ .set_property(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;) For a more complex example, use SASL_SSL as the security protocol and use SCRAM-SHA-256 as SASL mechanism:
Java KafkaSource.builder() .setProperty(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_SSL\u0026#34;) // SSL configurations // Configure the path of truststore (CA) provided by the server .setProperty(\u0026#34;ssl.truststore.location\u0026#34;, \u0026#34;/path/to/kafka.client.truststore.jks\u0026#34;) .setProperty(\u0026#34;ssl.truststore.password\u0026#34;, \u0026#34;test1234\u0026#34;) // Configure the path of keystore (private key) if client authentication is required .setProperty(\u0026#34;ssl.keystore.location\u0026#34;, \u0026#34;/path/to/kafka.client.keystore.jks\u0026#34;) .setProperty(\u0026#34;ssl.keystore.password\u0026#34;, \u0026#34;test1234\u0026#34;) // SASL configurations // Set SASL mechanism as SCRAM-SHA-256 .setProperty(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;SCRAM-SHA-256\u0026#34;) // Set JAAS configurations .setProperty(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;); Python KafkaSource.builder() \\ .set_property(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_SSL\u0026#34;) \\ # SSL configurations # Configure the path of truststore (CA) provided by the server .set_property(\u0026#34;ssl.truststore.location\u0026#34;, \u0026#34;/path/to/kafka.client.truststore.jks\u0026#34;) \\ .set_property(\u0026#34;ssl.truststore.password\u0026#34;, \u0026#34;test1234\u0026#34;) \\ # Configure the path of keystore (private key) if client authentication is required .set_property(\u0026#34;ssl.keystore.location\u0026#34;, \u0026#34;/path/to/kafka.client.keystore.jks\u0026#34;) \\ .set_property(\u0026#34;ssl.keystore.password\u0026#34;, \u0026#34;test1234\u0026#34;) \\ # SASL configurations # Set SASL mechanism as SCRAM-SHA-256 .set_property(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;SCRAM-SHA-256\u0026#34;) \\ # Set JAAS configurations .set_property(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;) Please note that the class path of the login module in sasl.jaas.config might be different if you relocate Kafka client dependencies in the job JAR, so you may need to rewrite it with the actual class path of the module in the JAR.
For detailed explanations of security configurations, please refer to the \u0026ldquo;Security\u0026rdquo; section in Apache Kafka documentation.
Behind the Scene # If you are interested in how Kafka source works under the design of new data source API, you may want to read this part as a reference. For details about the new data source API, documentation of data source and FLIP-27 provide more descriptive discussions. Under the abstraction of the new data source API, Kafka source consists of the following components:
Source Split # A source split in Kafka source represents a partition of Kafka topic. A Kafka source split consists of:
TopicPartition the split representing Starting offset of the partition Stopping offset of the partition, only available when the source is running in bounded mode The state of Kafka source split also stores current consuming offset of the partition, and the state will be converted to immutable split when Kafka source reader is snapshot, assigning current offset to the starting offset of the immutable split.
You can check class KafkaPartitionSplit and KafkaPartitionSplitState for more details.
Split Enumerator # The split enumerator of Kafka is responsible for discovering new splits (partitions) under the provided topic partition subscription pattern, and assigning splits to readers, uniformly distributed across subtasks, in round-robin style. Note that the split enumerator of Kafka source pushes splits eagerly to source readers, so it won\u0026rsquo;t need to handle split requests from source reader.
Source Reader # The source reader of Kafka source extends the provided SourceReaderBase, and use single-thread-multiplexed thread model, which read multiple assigned splits (partitions) with one KafkaConsumer driven by one SplitReader. Messages are deserialized right after they are fetched from Kafka in SplitReader. The state of split, or current progress of message consuming is updated by KafkaRecordEmitter , which is also responsible for assigning event time when the record is emitted downstream.
Kafka SourceFunction # FlinkKafkaConsumer is deprecated and will be removed with Flink 1.15, please use KafkaSource instead. For older references you can look at the Flink 1.13 documentation.
Kafka Sink # KafkaSink allows writing a stream of records to one or more Kafka topics.
Usage # Kafka sink provides a builder class to construct an instance of a KafkaSink. The code snippet below shows how to write String records to a Kafka topic with a delivery guarantee of at least once.
Java DataStream\u0026lt;String\u0026gt; stream = ...; KafkaSink\u0026lt;String\u0026gt; sink = KafkaSink.\u0026lt;String\u0026gt;builder() .setBootstrapServers(brokers) .setRecordSerializer(KafkaRecordSerializationSchema.builder() .setTopic(\u0026#34;topic-name\u0026#34;) .setValueSerializationSchema(new SimpleStringSchema()) .build() ) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build(); stream.sinkTo(sink); Python sink = KafkaSink.builder() \\ .set_bootstrap_servers(brokers) \\ .set_record_serializer( KafkaRecordSerializationSchema.builder() .set_topic(\u0026#34;topic-name\u0026#34;) .set_value_serialization_schema(SimpleStringSchema()) .build() ) \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .build() stream.sink_to(sink) The following properties are required to build a KafkaSink:
Bootstrap servers, setBootstrapServers(String) Record serializer, setRecordSerializer(KafkaRecordSerializationSchema) If you configure the delivery guarantee with DeliveryGuarantee.EXACTLY_ONCE you also have use setTransactionalIdPrefix(String) Serializer # You always need to supply a KafkaRecordSerializationSchema to transform incoming elements from the data stream to Kafka producer records. Flink offers a schema builder to provide some common building blocks i.e. key/value serialization, topic selection, partitioning. You can also implement the interface on your own to exert more control.
Java KafkaRecordSerializationSchema.builder() .setTopicSelector((element) -\u0026gt; {\u0026lt;your-topic-selection-logic\u0026gt;}) .setValueSerializationSchema(new SimpleStringSchema()) .setKeySerializationSchema(new SimpleStringSchema()) .setPartitioner(new FlinkFixedPartitioner()) .build(); Python KafkaRecordSerializationSchema.builder() \\ .set_topic_selector(lambda element: \u0026lt;your-topic-selection-logic\u0026gt;) \\ .set_value_serialization_schema(SimpleStringSchema()) \\ .set_key_serialization_schema(SimpleStringSchema()) \\ # set partitioner is not supported in PyFlink .build() It is required to always set a value serialization method and a topic (selection method). Moreover, it is also possible to use Kafka serializers instead of Flink serializer by using setKafkaKeySerializer(Serializer) or setKafkaValueSerializer(Serializer).
Fault Tolerance # Overall the KafkaSink supports three different DeliveryGuarantees. For DeliveryGuarantee.AT_LEAST_ONCE and DeliveryGuarantee.EXACTLY_ONCE Flink\u0026rsquo;s checkpointing must be enabled. By default the KafkaSink uses DeliveryGuarantee.NONE. Below you can find an explanation of the different guarantees.
DeliveryGuarantee.NONE does not provide any guarantees: messages may be lost in case of issues on the Kafka broker and messages may be duplicated in case of a Flink failure. DeliveryGuarantee.AT_LEAST_ONCE: The sink will wait for all outstanding records in the Kafka buffers to be acknowledged by the Kafka producer on a checkpoint. No messages will be lost in case of any issue with the Kafka brokers but messages may be duplicated when Flink restarts because Flink reprocesses old input records. DeliveryGuarantee.EXACTLY_ONCE: In this mode, the KafkaSink will write all messages in a Kafka transaction that will be committed to Kafka on a checkpoint. Thus, if the consumer reads only committed data (see Kafka consumer config isolation.level), no duplicates will be seen in case of a Flink restart. However, this delays record visibility effectively until a checkpoint is written, so adjust the checkpoint duration accordingly. Please ensure that you use unique transactionalIdPrefix across your applications running on the same Kafka cluster such that multiple running jobs do not interfere in their transactions! Additionally, it is highly recommended to tweak Kafka transaction timeout (see Kafka producer transaction.timeout.ms)\u0026raquo; maximum checkpoint duration + maximum restart duration or data loss may happen when Kafka expires an uncommitted transaction. Monitoring # Kafka sink exposes the following metrics in the respective scope.
Scope Metrics User Variables Description Type Operator currentSendTime n/a The time it takes to send the last record. This metric is an instantaneous value recorded for the last processed record. Gauge Kafka Producer # FlinkKafkaProducer is deprecated and will be removed with Flink 1.15, please use KafkaSink instead. For older references you can look at the Flink 1.13 documentation.
Kafka Connector Metrics # Flink\u0026rsquo;s Kafka connectors provide some metrics through Flink\u0026rsquo;s metrics system to analyze the behavior of the connector. The producers and consumers export Kafka\u0026rsquo;s internal metrics through Flink\u0026rsquo;s metric system for all supported versions. The Kafka documentation lists all exported metrics in its documentation.
It is also possible to disable the forwarding of the Kafka metrics by either configuring register.consumer.metrics outlined by this section for the KafkaSource or when using the KafkaSink you can set the configuration register.producer.metrics to false via the producer properties.
Enabling Kerberos Authentication # Flink provides first-class support through the Kafka connector to authenticate to a Kafka installation configured for Kerberos. Simply configure Flink in flink-conf.yaml to enable Kerberos authentication for Kafka like so:
Configure Kerberos credentials by setting the following - security.kerberos.login.use-ticket-cache: By default, this is true and Flink will attempt to use Kerberos credentials in ticket caches managed by kinit. Note that when using the Kafka connector in Flink jobs deployed on YARN, Kerberos authorization using ticket caches will not work. security.kerberos.login.keytab and security.kerberos.login.principal: To use Kerberos keytabs instead, set values for both of these properties. Append KafkaClient to security.kerberos.login.contexts: This tells Flink to provide the configured Kerberos credentials to the Kafka login context to be used for Kafka authentication. Once Kerberos-based Flink security is enabled, you can authenticate to Kafka with either the Flink Kafka Consumer or Producer by simply including the following two settings in the provided properties configuration that is passed to the internal Kafka client:
Set security.protocol to SASL_PLAINTEXT (default NONE): The protocol used to communicate to Kafka brokers. When using standalone Flink deployment, you can also use SASL_SSL; please see how to configure the Kafka client for SSL here. Set sasl.kerberos.service.name to kafka (default kafka): The value for this should match the sasl.kerberos.service.name used for Kafka broker configurations. A mismatch in service name between client and server configuration will cause the authentication to fail. For more information on Flink configuration for Kerberos security, please see here. You can also find here further details on how Flink internally setups Kerberos-based security.
Upgrading to the Latest Connector Version # The generic upgrade steps are outlined in upgrading jobs and Flink versions guide. For Kafka, you additionally need to follow these steps:
Do not upgrade Flink and the Kafka Connector version at the same time. Make sure you have a group.id configured for your Consumer. Set setCommitOffsetsOnCheckpoints(true) on the consumer so that read offsets are committed to Kafka. It\u0026rsquo;s important to do this before stopping and taking the savepoint. You might have to do a stop/restart cycle on the old connector version to enable this setting. Set setStartFromGroupOffsets(true) on the consumer so that we get read offsets from Kafka. This will only take effect when there is no read offset in Flink state, which is why the next step is very important. Change the assigned uid of your source/sink. This makes sure the new source/sink doesn\u0026rsquo;t read state from the old source/sink operators. Start the new job with --allow-non-restored-state because we still have the state of the previous connector version in the savepoint. Troubleshooting # If you have a problem with Kafka when using Flink, keep in mind that Flink only wraps KafkaConsumer or KafkaProducer and your problem might be independent of Flink and sometimes can be solved by upgrading Kafka brokers, reconfiguring Kafka brokers or reconfiguring KafkaConsumer or KafkaProducer in Flink. Some examples of common problems are listed below. Data loss # Depending on your Kafka configuration, even after Kafka acknowledges writes you can still experience data loss. In particular keep in mind about the following properties in Kafka config:
acks log.flush.interval.messages log.flush.interval.ms log.flush.* Default values for the above options can easily lead to data loss. Please refer to the Kafka documentation for more explanation.
UnknownTopicOrPartitionException # One possible cause of this error is when a new leader election is taking place, for example after or during restarting a Kafka broker. This is a retriable exception, so Flink job should be able to restart and resume normal operation. It also can be circumvented by changing retries property in the producer settings. However this might cause reordering of messages, which in turn if undesired can be circumvented by setting max.in.flight.requests.per.connection to 1.
ProducerFencedException # The reason for this exception is most likely a transaction timeout on the broker side. With the implementation of KAFKA-6119, the (producerId, epoch) will be fenced off after a transaction timeout and all of its pending transactions are aborted (each transactional.id is mapped to a single producerId; this is described in more detail in the following blog post).
Back to top
`}),e.add({id:86,href:"/flink/flink-docs-master/docs/connectors/table/kafka/",title:"Kafka",section:"Table API Connectors",content:" Apache Kafka SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode\nThe Kafka connector allows for reading data from and writing data into Kafka topics.\nDependencies # In order to use the Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nKafka version Maven dependency SQL Client JAR universal \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The Kafka connector is not part of the binary distribution. See how to link with it for cluster execution here.\nHow to create a Kafka table # The example below shows how to create a Kafka table:\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ) Available Metadata # The following connector metadata can be accessed as metadata columns in a table definition.\nThe R/W column defines whether a metadata field is readable (R) and/or writable (W). Read-only columns must be declared VIRTUAL to exclude them during an INSERT INTO operation.\nKey Data Type Description R/W topic STRING NOT NULL Topic name of the Kafka record. R partition INT NOT NULL Partition ID of the Kafka record. R headers MAP NOT NULL Headers of the Kafka record as a map of raw bytes. R/W leader-epoch INT NULL Leader epoch of the Kafka record if available. R offset BIGINT NOT NULL Offset of the Kafka record in the partition. R timestamp TIMESTAMP_LTZ(3) NOT NULL Timestamp of the Kafka record. R/W timestamp-type STRING NOT NULL Timestamp type of the Kafka record. Either \"NoTimestampType\", \"CreateTime\" (also set when writing metadata), or \"LogAppendTime\". R The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:\nCREATE TABLE KafkaTable ( `event_time` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `partition` BIGINT METADATA VIRTUAL, `offset` BIGINT METADATA VIRTUAL, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Format Metadata\nThe connector is able to expose metadata of the value format for reading. Format metadata keys are prefixed with 'value.'.\nThe following example shows how to access both Kafka and Debezium metadata fields:\nCREATE TABLE KafkaTable ( `event_time` TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, -- from Debezium format `origin_table` STRING METADATA FROM \u0026#39;value.source.table\u0026#39; VIRTUAL, -- from Debezium format `partition_id` BIGINT METADATA FROM \u0026#39;partition\u0026#39; VIRTUAL, -- from Kafka connector `offset` BIGINT METADATA VIRTUAL, -- from Kafka connector `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39; ); Connector Options # Option Required Forwarded Default Type Description connector required no (none) String Specify what connector to use, for Kafka use 'kafka'. topic required for sink yes (none) String Topic name(s) to read data from when the table is used as source. It also supports topic list for source by separating topic by semicolon like 'topic-1;topic-2'. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources. When the table is used as sink, the topic name is the topic to write data to. Note topic list is not supported for sinks. topic-pattern optional yes (none) String The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of \"topic-pattern\" and \"topic\" can be specified for sources. properties.bootstrap.servers required yes (none) String Comma separated list of Kafka brokers. properties.group.id optional for source, not applicable for sink yes (none) String The id of the consumer group for Kafka source. If group ID is not specified, an automatically generated id \"KafkaSource-{tableIdentifier}\" will be used. properties.* optional no (none) String This can set and pass arbitrary Kafka configurations. Suffix names must match the configuration key defined in Kafka Configuration documentation. Flink will remove the \"properties.\" key prefix and pass the transformed key and values to the underlying KafkaClient. For example, you can disable automatic topic creation via 'properties.allow.auto.create.topics' = 'false'. But there are some configurations that do not support to set, because Flink will override them, e.g. 'key.deserializer' and 'value.deserializer'. format required no (none) String The format used to deserialize and serialize the value part of Kafka messages. Please refer to the formats page for more details and more format options. Note: Either this option or the 'value.format' option are required. key.format optional no (none) String The format used to deserialize and serialize the key part of Kafka messages. Please refer to the formats page for more details and more format options. Note: If a key format is defined, the 'key.fields' option is required as well. Otherwise the Kafka records will have an empty key. key.fields optional no [] List\u0026lt;String\u0026gt; Defines an explicit list of physical columns from the table schema that configure the data type for the key format. By default, this list is empty and thus a key is undefined. The list should look like 'field1;field2'. key.fields-prefix optional no (none) String Defines a custom prefix for all fields of the key format to avoid name clashes with fields of the value format. By default, the prefix is empty. If a custom prefix is defined, both the table schema and 'key.fields' will work with prefixed names. When constructing the data type of the key format, the prefix will be removed and the non-prefixed names will be used within the key format. Please note that this option requires that 'value.fields-include' must be set to 'EXCEPT_KEY'. value.format required no (none) String The format used to deserialize and serialize the value part of Kafka messages. Please refer to the formats page for more details and more format options. Note: Either this option or the 'format' option are required. value.fields-include optional no ALL Enum\nPossible values: [ALL, EXCEPT_KEY] Defines a strategy how to deal with key columns in the data type of the value format. By default, 'ALL' physical columns of the table schema will be included in the value format which means that key columns appear in the data type for both the key and value format. scan.startup.mode optional yes group-offsets String Startup mode for Kafka consumer, valid values are 'earliest-offset', 'latest-offset', 'group-offsets', 'timestamp' and 'specific-offsets'. See the following Start Reading Position for more details. scan.startup.specific-offsets optional yes (none) String Specify offsets for each partition in case of 'specific-offsets' startup mode, e.g. 'partition:0,offset:42;partition:1,offset:300'. scan.startup.timestamp-millis optional yes (none) Long Start from the specified epoch timestamp (milliseconds) used in case of 'timestamp' startup mode. scan.topic-partition-discovery.interval optional yes (none) Duration Interval for consumer to discover dynamically created Kafka topics and partitions periodically. sink.partitioner optional yes 'default' String Output partitioning from Flink's partitions into Kafka's partitions. Valid values are default: use the kafka default partitioner to partition records. fixed: each Flink partition ends up in at most one Kafka partition. round-robin: a Flink partition is distributed to Kafka partitions sticky round-robin. It only works when record's keys are not specified. Custom FlinkKafkaPartitioner subclass: e.g. 'org.mycompany.MyPartitioner'. See the following Sink Partitioning for more details. sink.semantic optional no at-least-once String Deprecated: Please use sink.delivery-guarantee. sink.delivery-guarantee optional no at-least-once String Defines the delivery semantic for the Kafka sink. Valid enumerationns are 'at-least-once', 'exactly-once' and 'none'. See Consistency guarantees for more details. sink.transactional-id-prefix optional yes (none) String If the delivery guarantee is configured as 'exactly-once' this value must be set and is used a prefix for the identifier of all opened Kafka transactions. sink.parallelism optional no (none) Integer Defines the parallelism of the Kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator. Features # Key and Value Formats # Both the key and value part of a Kafka record can be serialized to and deserialized from raw bytes using one of the given formats.\nValue Format\nSince a key is optional in Kafka records, the following statement reads and writes records with a configured value format but without a key format. The 'format' option is a synonym for 'value.format'. All format options are prefixed with the format identifier.\nCREATE TABLE KafkaTable ( `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) The value format will be configured with the following data type:\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT, `behavior` STRING\u0026gt; Key and Value Format\nThe following example shows how to specify and configure key and value formats. The format options are prefixed with either the 'key' or 'value' plus format identifier.\nCREATE TABLE KafkaTable ( `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;user_id;item_id\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;ALL\u0026#39; ) The key format includes the fields listed in 'key.fields' (using ';' as the delimiter) in the same order. Thus, it will be configured with the following data type:\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT\u0026gt; Since the value format is configured with 'value.fields-include' = 'ALL', key fields will also end up in the value format\u0026rsquo;s data type:\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT, `behavior` STRING\u0026gt; Overlapping Format Fields\nThe connector cannot split the table\u0026rsquo;s columns into key and value fields based on schema information if both key and value formats contain fields of the same name. The 'key.fields-prefix' option allows to give key columns a unique name in the table schema while keeping the original names when configuring the key format.\nThe following example shows a key and value format that both contain a version field:\nCREATE TABLE KafkaTable ( `k_version` INT, `k_user_id` BIGINT, `k_item_id` BIGINT, `version` INT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;k_\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;k_version;k_user_id;k_item_id\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) The value format must be configured in 'EXCEPT_KEY' mode. The formats will be configured with the following data types:\nkey format: ROW\u0026lt;`version` INT, `user_id` BIGINT, `item_id` BIGINT\u0026gt; value format: ROW\u0026lt;`version` INT, `behavior` STRING\u0026gt; Topic and Partition Discovery # The config option topic and topic-pattern specifies the topics or topic pattern to consume for source. The config option topic can accept topic list using semicolon separator like \u0026rsquo;topic-1;topic-2\u0026rsquo;. The config option topic-pattern will use regular expression to discover the matched topic. For example, if the topic-pattern is test-topic-[0-9], then all topics with names that match the specified regular expression (starting with test-topic- and ending with a single digit)) will be subscribed by the consumer when the job starts running.\nTo allow the consumer to discover dynamically created topics after the job started running, set a non-negative value for scan.topic-partition-discovery.interval. This allows the consumer to discover partitions of new topics with names that also match the specified pattern.\nPlease refer to Kafka DataStream Connector documentation for more about topic and partition discovery.\nNote that topic list and topic pattern only work in sources. In sinks, Flink currently only supports a single topic.\nStart Reading Position # The config option scan.startup.mode specifies the startup mode for Kafka consumer. The valid enumerations are:\n`group-offsets`: start from committed offsets in ZK / Kafka brokers of a specific consumer group. `earliest-offset`: start from the earliest offset possible. `latest-offset`: start from the latest offset. `timestamp`: start from user-supplied timestamp for each partition. `specific-offsets`: start from user-supplied specific offsets for each partition. The default option value is group-offsets which indicates to consume from last committed offsets in ZK / Kafka brokers.\nIf timestamp is specified, another config option scan.startup.timestamp-millis is required to specify a specific startup timestamp in milliseconds since January 1, 1970 00:00:00.000 GMT.\nIf specific-offsets is specified, another config option scan.startup.specific-offsets is required to specify specific startup offsets for each partition, e.g. an option value partition:0,offset:42;partition:1,offset:300 indicates offset 42 for partition 0 and offset 300 for partition 1.\nCDC Changelog Source # Flink natively supports Kafka as a CDC changelog source. If messages in a Kafka topic are change event captured from other databases using a CDC tool, you can use the corresponding Flink CDC format to interpret the messages as INSERT/UPDATE/DELETE statements into a Flink SQL table.\nThe changelog source is a very useful feature in many cases, such as synchronizing incremental data from databases to other systems, auditing logs, materialized views on databases, temporal join changing history of a database table and so on.\nFlink provides several CDC formats:\ndebezium canal maxwell Sink Partitioning # The config option sink.partitioner specifies output partitioning from Flink\u0026rsquo;s partitions into Kafka\u0026rsquo;s partitions. By default, Flink uses the Kafka default partitioner to partition records. It uses the sticky partition strategy for records with null keys and uses a murmur2 hash to compute the partition for a record with the key defined.\nIn order to control the routing of rows into partitions, a custom sink partitioner can be provided. The \u0026lsquo;fixed\u0026rsquo; partitioner will write the records in the same Flink partition into the same Kafka partition, which could reduce the cost of the network connections.\nConsistency guarantees # By default, a Kafka sink ingests data with at-least-once guarantees into a Kafka topic if the query is executed with checkpointing enabled.\nWith Flink\u0026rsquo;s checkpointing enabled, the kafka connector can provide exactly-once delivery guarantees.\nBesides enabling Flink\u0026rsquo;s checkpointing, you can also choose three different modes of operating chosen by passing appropriate sink.delivery-guarantee option:\nnone: Flink will not guarantee anything. Produced records can be lost or they can be duplicated. at-least-once (default setting): This guarantees that no records will be lost (although they can be duplicated). exactly-once: Kafka transactions will be used to provide exactly-once semantic. Whenever you write to Kafka using transactions, do not forget about setting desired isolation.level (read_committed or read_uncommitted - the latter one is the default value) for any application consuming records from Kafka. Please refer to Kafka documentation for more caveats about delivery guarantees.\nSource Per-Partition Watermarks # Flink supports to emit per-partition watermarks for Kafka. Watermarks are generated inside the Kafka consumer. The per-partition watermarks are merged in the same way as watermarks are merged during streaming shuffles. The output watermark of the source is determined by the minimum watermark among the partitions it reads. If some partitions in the topics are idle, the watermark generator will not advance. You can alleviate this problem by setting the 'table.exec.source.idle-timeout' option in the table configuration.\nPlease refer to Kafka watermark strategies for more details.\nSecurity # In order to enable security configurations including encryption and authentication, you just need to setup security configurations with \u0026ldquo;properties.\u0026rdquo; prefix in table options. The code snippet below shows configuring Kafka table to use PLAIN as SASL mechanism and provide JAAS configuration:\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_PLAINTEXT\u0026#39;, \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;PLAIN\u0026#39;, \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#39; ) For a more complex example, use SASL_SSL as the security protocol and use SCRAM-SHA-256 as SASL mechanism:\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_SSL\u0026#39;, /* SSL configurations */ /* Configure the path of truststore (CA) provided by the server */ \u0026#39;properties.ssl.truststore.location\u0026#39; = \u0026#39;/path/to/kafka.client.truststore.jks\u0026#39;, \u0026#39;properties.ssl.truststore.password\u0026#39; = \u0026#39;test1234\u0026#39;, /* Configure the path of keystore (private key) if client authentication is required */ \u0026#39;properties.ssl.keystore.location\u0026#39; = \u0026#39;/path/to/kafka.client.keystore.jks\u0026#39;, \u0026#39;properties.ssl.keystore.password\u0026#39; = \u0026#39;test1234\u0026#39;, /* SASL configurations */ /* Set SASL mechanism as SCRAM-SHA-256 */ \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;SCRAM-SHA-256\u0026#39;, /* Set JAAS configurations */ \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#39; ) Please note that the class path of the login module in sasl.jaas.config might be different if you relocate Kafka client dependencies, so you may need to rewrite it with the actual class path of the module in the JAR. For example if you are using SQL client JAR, which has relocate Kafka client dependencies to org.apache.flink.kafka.shaded.org.apache.kafka, the path of plain login module should be org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule instead.\nFor detailed explanations of security configurations, please refer to the \u0026ldquo;Security\u0026rdquo; section in Apache Kafka documentation.\nData Type Mapping # Kafka stores message keys and values as bytes, so Kafka doesn\u0026rsquo;t have schema or data types. The Kafka messages are deserialized and serialized by formats, e.g. csv, json, avro. Thus, the data type mapping is determined by specific formats. Please refer to Formats pages for more details.\nBack to top\n"}),e.add({id:87,href:"/flink/flink-docs-master/docs/deployment/security/security-kerberos/",title:"Kerberos",section:"Security",content:` Kerberos Authentication Setup and Configuration # This document briefly describes how Flink security works in the context of various deployment mechanisms (Standalone, native Kubernetes, YARN), filesystems, connectors, and state backends.
Objective # The primary goals of the Flink Kerberos security infrastructure are:
to enable secure data access for jobs within a cluster via connectors (e.g. Kafka) to authenticate to ZooKeeper (if configured to use SASL) to authenticate to Hadoop components (e.g. HDFS, HBase) In a production deployment scenario, streaming jobs are understood to run for long periods of time (days/weeks/months) and be able to authenticate to secure data sources throughout the life of the job. Kerberos keytabs do not expire in that timeframe, unlike a Hadoop delegation token or ticket cache entry.
The current implementation supports running Flink clusters (JobManager / TaskManager / jobs) with either a configured keytab credential or with Hadoop delegation tokens. Keep in mind that all jobs share the credential configured for a given cluster. To use a different keytab for a certain job, simply launch a separate Flink cluster with a different configuration. Numerous Flink clusters may run side-by-side in a Kubernetes or YARN environment.
How Flink Security works # In concept, a Flink program may use first- or third-party connectors (Kafka, HDFS, Cassandra, Flume, Kinesis etc.) necessitating arbitrary authentication methods (Kerberos, SSL/TLS, username/password, etc.). While satisfying the security requirements for all connectors is an ongoing effort, Flink provides first-class support for Kerberos authentication only. The following services and connectors are supported for Kerberos authentication:
Kafka (0.9+) HDFS HBase ZooKeeper Note that it is possible to enable the use of Kerberos independently for each service or connector. For example, the user may enable Hadoop security without necessitating the use of Kerberos for ZooKeeper, or vice versa. The shared element is the configuration of Kerberos credentials, which is then explicitly used by each component.
The internal architecture is based on security modules (implementing org.apache.flink.runtime.security.modules.SecurityModule) which are installed at startup. The following sections describes each security module.
Hadoop Security Module # This module uses the Hadoop UserGroupInformation (UGI) class to establish a process-wide login user context. The login user is then used for all interactions with Hadoop, including HDFS, HBase, and YARN.
If Hadoop security is enabled (in core-site.xml), the login user will have whatever Kerberos credential is configured. Otherwise, the login user conveys only the user identity of the OS account that launched the cluster.
JAAS Security Module # This module provides a dynamic JAAS configuration to the cluster, making available the configured Kerberos credential to ZooKeeper, Kafka, and other such components that rely on JAAS.
Note that the user may also provide a static JAAS configuration file using the mechanisms described in the Java SE Documentation. Static entries override any dynamic entries provided by this module.
ZooKeeper Security Module # This module configures certain process-wide ZooKeeper security-related settings, namely the ZooKeeper service name (default: zookeeper) and the JAAS login context name (default: Client).
Deployment Modes # Here is some information specific to each deployment mode.
Standalone Mode # Steps to run a secure Flink cluster in standalone/cluster mode:
Add security-related configuration options to the Flink configuration file (on all cluster nodes) (see here). Ensure that the keytab file exists at the path indicated by security.kerberos.login.keytab on all cluster nodes. Deploy Flink cluster as normal. Native Kubernetes and YARN Mode # Steps to run a secure Flink cluster in native Kubernetes and YARN mode:
Add security-related configuration options to the Flink configuration file on the client (see here). Ensure that the keytab file exists at the path as indicated by security.kerberos.login.keytab on the client node. Deploy Flink cluster as normal. In YARN and native Kubernetes mode, the keytab is automatically copied from the client to the Flink containers.
To enable Kerberos authentication, the Kerberos configuration file is also required. This file can be either fetched from the cluster environment or uploaded by Flink. In the latter case, you need to configure the security.kerberos.krb5-conf.path to indicate the path of the Kerberos configuration file and Flink will copy this file to its containers/pods.
For more information, see YARN security documentation.
Using kinit (YARN only) # In YARN mode, it is possible to deploy a secure Flink cluster without a keytab, using only the ticket cache (as managed by kinit). This avoids the complexity of generating a keytab and avoids entrusting the cluster manager with it. In this scenario, the Flink CLI acquires Hadoop delegation tokens (for HDFS and for HBase). The main drawback is that the cluster is necessarily short-lived since the generated delegation tokens will expire (typically within a week).
Steps to run a secure Flink cluster using kinit:
Add security-related configuration options to the Flink configuration file on the client (see here). Login using the kinit command. Deploy Flink cluster as normal. Further Details # Ticket Renewal # Each component that uses Kerberos is independently responsible for renewing the Kerberos ticket-granting-ticket (TGT). Hadoop, ZooKeeper, and Kafka all renew the TGT automatically when provided a keytab. In the delegation token scenario, YARN itself renews the token (up to its maximum lifespan).
Back to top
`}),e.add({id:88,href:"/flink/flink-docs-master/docs/deployment/ha/kubernetes_ha/",title:"Kubernetes HA Services",section:"High Availability",content:` Kubernetes HA Services # Flink\u0026rsquo;s Kubernetes HA services use Kubernetes for high availability services.
Kubernetes high availability services can only be used when deploying to Kubernetes. Consequently, they can be configured when using standalone Flink on Kubernetes or the native Kubernetes integration
Prerequisites # In order to use Flink\u0026rsquo;s Kubernetes HA services you must fulfill the following prerequisites:
Kubernetes \u0026gt;= 1.9. Service account with permissions to create, edit, delete ConfigMaps. Take a look at how to configure a service account for Flink\u0026rsquo;s native Kubernetes integration and standalone Flink on Kubernetes for more information. Configuration # In order to start an HA-cluster you have to configure the following configuration keys:
high-availability (required): The high-availability option has to be set to KubernetesHaServicesFactory. high-availability: kubernetes high-availability.storageDir (required): JobManager metadata is persisted in the file system high-availability.storageDir and only a pointer to this state is stored in Kubernetes. high-availability.storageDir: s3://flink/recovery The storageDir stores all metadata needed to recover a JobManager failure.
kubernetes.cluster-id (required): In order to identify the Flink cluster, you have to specify a kubernetes.cluster-id. kubernetes.cluster-id: cluster1337 Example configuration # Configure high availability mode in conf/flink-conf.yaml:
kubernetes.cluster-id: \u0026lt;cluster-id\u0026gt; high-availability: kubernetes high-availability.storageDir: hdfs:///flink/recovery Back to top
High availability data clean up # To keep HA data while restarting the Flink cluster, simply delete the deployment (via kubectl delete deployment \u0026lt;cluster-id\u0026gt;). All the Flink cluster related resources will be deleted (e.g. JobManager Deployment, TaskManager pods, services, Flink conf ConfigMap). HA related ConfigMaps will be retained because they do not set the owner reference. When restarting the cluster, all previously running jobs will be recovered and restarted from the latest successful checkpoint.
Back to top
`}),e.add({id:89,href:"/flink/flink-docs-master/docs/ops/monitoring/back_pressure/",title:"Monitoring Back Pressure",section:"Monitoring",content:` Monitoring Back Pressure # Flink\u0026rsquo;s web interface provides a tab to monitor the back pressure behaviour of running jobs.
Back Pressure # If you see a back pressure warning (e.g. High) for a task, this means that it is producing data faster than the downstream operators can consume. Records in your job flow downstream (e.g. from sources to sinks) and back pressure is propagated in the opposite direction, up the stream.
Take a simple Source -\u0026gt; Sink job as an example. If you see a warning for Source, this means that Sink is consuming data slower than Source is producing. Sink is back pressuring the upstream operator Source.
Task performance metrics # Every parallel instance of a task (subtask) is exposing a group of three metrics:
backPressureTimeMsPerSecond, time that subtask spent being back pressured idleTimeMsPerSecond, time that subtask spent waiting for something to process busyTimeMsPerSecond, time that subtask was busy doing some actual work At any point of time these three metrics are adding up approximately to 1000ms. These metrics are being updated every couple of seconds, and the reported value represents the average time that subtask was back pressured (or idle or busy) during the last couple of seconds. Keep this in mind if your job has a varying load. For example, a subtask with a constant load of 50% and another subtask that is alternating every second between fully loaded and idling will both have the same value of busyTimeMsPerSecond: around 500ms.
Internally, back pressure is judged based on the availability of output buffers. If a task has no available output buffers, then that task is considered back pressured. Idleness, on the other hand, is determined by whether or not there is input available.
Example # The WebUI aggregates the maximum value of the back pressure and busy metrics from all of the subtasks and presents those aggregated values inside the JobGraph. Besides displaying the raw values, tasks are also color-coded to make the investigation easier.
Idling tasks are blue, fully back pressured tasks are black, and fully busy tasks are colored red. All values in between are represented as shades between those three colors.
Back Pressure Status # In the Back Pressure tab next to the job overview you can find more detailed metrics.
For subtasks whose status is OK, there is no indication of back pressure. HIGH, on the other hand, means that a subtask is back pressured. Status is defined in the following way:
OK: 0% \u0026lt;= back pressured \u0026lt;= 10% LOW: 10% \u0026lt; back pressured \u0026lt;= 50% HIGH: 50% \u0026lt; back pressured \u0026lt;= 100% Additionally, you can find the percentage of time each subtask is back pressured, idle, or busy.
Back to top
`}),e.add({id:90,href:"/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/",title:"Native Kubernetes",section:"Resource Providers",content:` Native Kubernetes # This page describes how to deploy Flink natively on Kubernetes.
Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on Kubernetes.
Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink\u0026rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.
Preparation # The Getting Started section assumes a running Kubernetes cluster fulfilling the following requirements:
Kubernetes \u0026gt;= 1.9. KubeConfig, which has access to list, create, delete pods and services, configurable via ~/.kube/config. You can verify permissions by running kubectl auth can-i \u0026lt;list|create|edit|delete\u0026gt; pods. Enabled Kubernetes DNS. default service account with RBAC permissions to create, delete pods. If you have problems setting up a Kubernetes cluster, then take a look at how to setup a Kubernetes cluster.
Starting a Flink Session on Kubernetes # Once you have your Kubernetes cluster running and kubectl is configured to point to it, you can launch a Flink cluster in Session Mode via
# (1) Start Kubernetes session \$ ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster # (2) Submit example job \$ ./bin/flink run \\ --target kubernetes-session \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ ./examples/streaming/TopSpeedWindowing.jar # (3) Stop Kubernetes session by deleting cluster deployment \$ kubectl delete deployment/my-first-flink-cluster In default, Flink’s Web UI and REST endpoint are exposed as ClusterIP service. To access the service, please refer to Accessing Flink’s Web UI for instructions. Congratulations! You have successfully run a Flink application by deploying Flink on Kubernetes.
Back to top
Deployment Modes # For production use, we recommend deploying Flink Applications in the Application Mode, as these modes provide a better isolation for the Applications.
Application Mode # For high-level intuition behind the application mode, please refer to the deployment mode overview. The Application Mode requires that the user code is bundled together with the Flink image because it runs the user code\u0026rsquo;s main() method on the cluster. The Application Mode makes sure that all Flink components are properly cleaned up after the termination of the application.
The Flink community provides a base Docker image which can be used to bundle the user code:
FROM flink RUN mkdir -p \$FLINK_HOME/usrlib COPY /path/of/my-flink-job.jar \$FLINK_HOME/usrlib/my-flink-job.jar After creating and publishing the Docker image under custom-image-name, you can start an Application cluster with the following command:
\$ ./bin/flink run-application \\ --target kubernetes-application \\ -Dkubernetes.cluster-id=my-first-application-cluster \\ -Dkubernetes.container.image=custom-image-name \\ local:///opt/flink/usrlib/my-flink-job.jar Note local is the only supported scheme in Application Mode.
The kubernetes.cluster-id option specifies the cluster name and must be unique. If you do not specify this option, then Flink will generate a random name.
The kubernetes.container.image option specifies the image to start the pods with.
Once the application cluster is deployed you can interact with it:
# List running job on the cluster \$ ./bin/flink list --target kubernetes-application -Dkubernetes.cluster-id=my-first-application-cluster # Cancel running job \$ ./bin/flink cancel --target kubernetes-application -Dkubernetes.cluster-id=my-first-application-cluster \u0026lt;jobId\u0026gt; You can override configurations set in conf/flink-conf.yaml by passing key-value pairs -Dkey=value to bin/flink.
Session Mode # For high-level intuition behind the session mode, please refer to the deployment mode overview. You have seen the deployment of a Session cluster in the Getting Started guide at the top of this page.
The Session Mode can be executed in two modes:
detached mode (default): The kubernetes-session.sh deploys the Flink cluster on Kubernetes and then terminates.
attached mode (-Dexecution.attached=true): The kubernetes-session.sh stays alive and allows entering commands to control the running Flink cluster. For example, stop stops the running Session cluster. Type help to list all supported commands.
In order to re-attach to a running Session cluster with the cluster id my-first-flink-cluster use the following command:
\$ ./bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ -Dexecution.attached=true You can override configurations set in conf/flink-conf.yaml by passing key-value pairs -Dkey=value to bin/kubernetes-session.sh.
Stop a Running Session Cluster # In order to stop a running Session Cluster with cluster id my-first-flink-cluster you can either delete the Flink deployment or use:
\$ echo \u0026#39;stop\u0026#39; | ./bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ -Dexecution.attached=true Back to top
Flink on Kubernetes Reference # Configuring Flink on Kubernetes # The Kubernetes-specific configuration options are listed on the configuration page.
Flink uses Fabric8 Kubernetes client to communicate with Kubernetes APIServer to create/delete Kubernetes resources(e.g. Deployment, Pod, ConfigMap, Service, etc.), as well as watch the Pods and ConfigMaps. Except for the above Flink config options, some expert options of Fabric8 Kubernetes client could be configured via system properties or environment variables.
For example, users could use the following Flink config options to set the concurrent max requests.
containerized.master.env.KUBERNETES_MAX_CONCURRENT_REQUESTS: 200 env.java.opts.jobmanager: \u0026#34;-Dkubernetes.max.concurrent.requests=200\u0026#34; Accessing Flink\u0026rsquo;s Web UI # Flink\u0026rsquo;s Web UI and REST endpoint can be exposed in several ways via the kubernetes.rest-service.exposed.type configuration option.
ClusterIP: Exposes the service on a cluster-internal IP. The Service is only reachable within the cluster. If you want to access the JobManager UI or submit job to the existing session, you need to start a local proxy. You can then use localhost:8081 to submit a Flink job to the session or view the dashboard. \$ kubectl port-forward service/\u0026lt;ServiceName\u0026gt; 8081 NodePort: Exposes the service on each Node’s IP at a static port (the NodePort). \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt; can be used to contact the JobManager service.
LoadBalancer: Exposes the service externally using a cloud provider’s load balancer. Since the cloud provider and Kubernetes needs some time to prepare the load balancer, you may get a NodePort JobManager Web Interface in the client log. You can use kubectl get services/\u0026lt;cluster-id\u0026gt;-rest to get EXTERNAL-IP and construct the load balancer JobManager Web Interface manually http://\u0026lt;EXTERNAL-IP\u0026gt;:8081.
Please refer to the official documentation on publishing services in Kubernetes for more information.
Depending on your environment, starting a Flink cluster with LoadBalancer REST service exposed type might make the cluster accessible publicly (usually with the ability to execute arbitrary code). Logging # The Kubernetes integration exposes conf/log4j-console.properties and conf/logback-console.xml as a ConfigMap to the pods. Changes to these files will be visible to a newly started cluster.
Accessing the Logs # By default, the JobManager and TaskManager will output the logs to the console and /opt/flink/log in each pod simultaneously. The STDOUT and STDERR output will only be redirected to the console. You can access them via
\$ kubectl logs \u0026lt;pod-name\u0026gt; If the pod is running, you can also use kubectl exec -it \u0026lt;pod-name\u0026gt; bash to tunnel in and view the logs or debug the process.
Accessing the Logs of the TaskManagers # Flink will automatically de-allocate idling TaskManagers in order to not waste resources. This behaviour can make it harder to access the logs of the respective pods. You can increase the time before idling TaskManagers are released by configuring resourcemanager.taskmanager-timeout so that you have more time to inspect the log files.
Changing the Log Level Dynamically # If you have configured your logger to detect configuration changes automatically, then you can dynamically adapt the log level by changing the respective ConfigMap (assuming that the cluster id is my-first-flink-cluster):
\$ kubectl edit cm flink-config-my-first-flink-cluster Using Plugins # In order to use plugins, you must copy them to the correct location in the Flink JobManager/TaskManager pod. You can use the built-in plugins without mounting a volume or building a custom Docker image. For example, use the following command to enable the S3 plugin for your Flink session cluster.
\$ ./bin/kubernetes-session.sh -Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.16-SNAPSHOT.jar \\ -Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.16-SNAPSHOT.jar Custom Docker Image # If you want to use a custom Docker image, then you can specify it via the configuration option kubernetes.container.image. The Flink community provides a rich Flink Docker image which can be a good starting point. See how to customize Flink\u0026rsquo;s Docker image for how to enable plugins, add dependencies and other options.
Using Secrets # Kubernetes Secrets is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a pod specification or in an image. Flink on Kubernetes can use Secrets in two ways:
Using Secrets as files from a pod;
Using Secrets as environment variables;
Using Secrets as Files From a Pod # The following command will mount the secret mysecret under the path /path/to/secret in the started pods:
\$ ./bin/kubernetes-session.sh -Dkubernetes.secrets=mysecret:/path/to/secret The username and password of the secret mysecret can then be found stored in the files /path/to/secret/username and /path/to/secret/password. For more details see the official Kubernetes documentation.
Using Secrets as Environment Variables # The following command will expose the secret mysecret as environment variable in the started pods:
\$ ./bin/kubernetes-session.sh -Dkubernetes.env.secretKeyRef=\\ env:SECRET_USERNAME,secret:mysecret,key:username;\\ env:SECRET_PASSWORD,secret:mysecret,key:password The env variable SECRET_USERNAME contains the username and the env variable SECRET_PASSWORD contains the password of the secret mysecret. For more details see the official Kubernetes documentation.
High-Availability on Kubernetes # For high availability on Kubernetes, you can use the existing high availability services.
Configure the value of kubernetes.jobmanager.replicas to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers.
Manual Resource Cleanup # Flink uses Kubernetes OwnerReference\u0026rsquo;s to clean up all cluster components. All the Flink created resources, including ConfigMap, Service, and Pod, have the OwnerReference being set to deployment/\u0026lt;cluster-id\u0026gt;. When the deployment is deleted, all related resources will be deleted automatically.
\$ kubectl delete deployment/\u0026lt;cluster-id\u0026gt; Supported Kubernetes Versions # Currently, all Kubernetes versions \u0026gt;= 1.9 are supported.
Namespaces # Namespaces in Kubernetes divide cluster resources between multiple users via resource quotas. Flink on Kubernetes can use namespaces to launch Flink clusters. The namespace can be configured via kubernetes.namespace.
RBAC # Role-based access control (RBAC) is a method of regulating access to compute or network resources based on the roles of individual users within an enterprise. Users can configure RBAC roles and service accounts used by JobManager to access the Kubernetes API server within the Kubernetes cluster.
Every namespace has a default service account. However, the default service account may not have the permission to create or delete pods within the Kubernetes cluster. Users may need to update the permission of the default service account or specify another service account that has the right role bound.
\$ kubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=default:default If you do not want to use the default service account, use the following command to create a new flink-service-account service account and set the role binding. Then use the config option -Dkubernetes.service-account=flink-service-account to make the JobManager pod use the flink-service-account service account to create/delete TaskManager pods and leader ConfigMaps. Also this will allow the TaskManager to watch leader ConfigMaps to retrieve the address of JobManager and ResourceManager.
\$ kubectl create serviceaccount flink-service-account \$ kubectl create clusterrolebinding flink-role-binding-flink --clusterrole=edit --serviceaccount=default:flink-service-account Please refer to the official Kubernetes documentation on RBAC Authorization for more information.
Pod Template # Flink allows users to define the JobManager and TaskManager pods via template files. This allows to support advanced features that are not supported by Flink Kubernetes config options directly. Use kubernetes.pod-template-file to specify a local file that contains the pod definition. It will be used to initialize the JobManager and TaskManager. The main container should be defined with name flink-main-container. Please refer to the pod template example for more information.
Fields Overwritten by Flink # Some fields of the pod template will be overwritten by Flink. The mechanism for resolving effective field values can be categorized as follows:
Defined by Flink: User cannot configure it.
Defined by the user: User can freely specify this value. Flink framework won\u0026rsquo;t set any additional values and the effective value derives from the config option and the template.
Precedence order: First an explicit config option value is taken, then the value in pod template and at last the default value of a config option if nothing is specified.
Merged with Flink: Flink will merge values for a setting with a user defined value (see precedence order for \u0026ldquo;Defined by the user\u0026rdquo;). Flink values have precedence in case of same name fields.
Refer to the following tables for the full list of pod fields that will be overwritten. All the fields defined in the pod template that are not listed in the tables will be unaffected.
Pod Metadata
Key Category Related Config Options Description name Defined by Flink The JobManager pod name will be overwritten with the deployment which is defined by kubernetes.cluster-id. The TaskManager pod names will be overwritten with the pattern \u0026lt;clusterID\u0026gt;-\u0026lt;attempt\u0026gt;-\u0026lt;index\u0026gt; which is generated by Flink ResourceManager. namespace Defined by the user kubernetes.namespace Both the JobManager deployment and TaskManager pods will be created in the user specified namespace. ownerReferences Defined by Flink The owner reference of JobManager and TaskManager pods will always be set to the JobManager deployment. Please use kubernetes.jobmanager.owner.reference to control when the deployment is deleted. annotations Defined by the user kubernetes.jobmanager.annotations kubernetes.taskmanager.annotations Flink will add additional annotations specified by the Flink configuration options. labels Merged with Flink kubernetes.jobmanager.labels kubernetes.taskmanager.labels Flink will add some internal labels to the user defined values. Pod Spec
Key Category Related Config Options Description imagePullSecrets Defined by the user kubernetes.container.image.pull-secrets Flink will add additional pull secrets specified by the Flink configuration options. nodeSelector Defined by the user kubernetes.jobmanager.node-selector kubernetes.taskmanager.node-selector Flink will add additional node selectors specified by the Flink configuration options. tolerations Defined by the user kubernetes.jobmanager.tolerations kubernetes.taskmanager.tolerations Flink will add additional tolerations specified by the Flink configuration options. restartPolicy Defined by Flink "always" for JobManager pod and "never" for TaskManager pod. The JobManager pod will always be restarted by deployment. And the TaskManager pod should not be restarted. serviceAccount Defined by the user kubernetes.service-account The JobManager and TaskManager pods will be created with the user defined service account. volumes Merged with Flink Flink will add some internal ConfigMap volumes(e.g. flink-config-volume, hadoop-config-volume) which is necessary for shipping the Flink configuration and hadoop configuration. Main Container Spec
Key Category Related Config Options Description env Merged with Flink containerized.master.env.{ENV_NAME} containerized.taskmanager.env.{ENV_NAME} Flink will add some internal environment variables to the user defined values. image Defined by the user kubernetes.container.image The container image will be resolved with respect to the defined precedence order for user defined values. imagePullPolicy Defined by the user kubernetes.container.image.pull-policy The container image pull policy will be resolved with respect to the defined precedence order for user defined values. name Defined by Flink The container name will be overwritten by Flink with "flink-main-container". resources Defined by the user Memory: jobmanager.memory.process.size taskmanager.memory.process.size CPU: kubernetes.jobmanager.cpu kubernetes.taskmanager.cpu The memory and cpu resources(including requests and limits) will be overwritten by Flink configuration options. All other resources(e.g. ephemeral-storage) will be retained. containerPorts Merged with Flink Flink will add some internal container ports(e.g. rest, jobmanager-rpc, blob, taskmanager-rpc). volumeMounts Merged with Flink Flink will add some internal volume mounts(e.g. flink-config-volume, hadoop-config-volume) which is necessary for shipping the Flink configuration and hadoop configuration. Example of Pod Template # pod-template.yaml
apiVersion: v1 kind: Pod metadata: name: jobmanager-pod-template spec: initContainers: - name: artifacts-fetcher image: busybox:latest # Use wget or other tools to get user jars from remote storage command: [ \u0026#39;wget\u0026#39;, \u0026#39;https://path/of/StateMachineExample.jar\u0026#39;, \u0026#39;-O\u0026#39;, \u0026#39;/flink-artifact/myjob.jar\u0026#39; ] volumeMounts: - mountPath: /flink-artifact name: flink-artifact containers: # Do not change the main container name - name: flink-main-container resources: requests: ephemeral-storage: 2048Mi limits: ephemeral-storage: 2048Mi volumeMounts: - mountPath: /opt/flink/volumes/hostpath name: flink-volume-hostpath - mountPath: /opt/flink/artifacts name: flink-artifact - mountPath: /opt/flink/log name: flink-logs # Use sidecar container to push logs to remote storage or do some other debugging things - name: sidecar-log-collector image: sidecar-log-collector:latest command: [ \u0026#39;command-to-upload\u0026#39;, \u0026#39;/remote/path/of/flink-logs/\u0026#39; ] volumeMounts: - mountPath: /flink-logs name: flink-logs volumes: - name: flink-volume-hostpath hostPath: path: /tmp type: Directory - name: flink-artifact emptyDir: { } - name: flink-logs emptyDir: { } User jars \u0026amp; Classpath # When deploying Flink natively on Kubernetes, the following jars will be recognized as user-jars and included into user classpath:
Session Mode: The JAR file specified in startup command. Application Mode: The JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:91,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/",title:"Queries",section:"SQL",content:""}),e.add({id:92,href:"/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/",title:"Set up TaskManager Memory",section:"Memory Configuration",content:` Set up TaskManager Memory # The TaskManager runs user code in Flink. Configuring memory usage for your needs can greatly reduce Flink\u0026rsquo;s resource footprint and improve Job stability.
The further described memory configuration is applicable starting with the release version 1.10. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.10 release.
This memory setup guide is relevant only for TaskManagers! The TaskManager memory components have a similar but more sophisticated structure compared to the memory model of the JobManager process. Configure Total Memory # The total process memory of Flink JVM processes consists of memory consumed by Flink application (total Flink memory) and by the JVM to run the process. The total Flink memory consumption includes usage of JVM Heap, managed memory (managed by Flink) and other direct (or native) memory.
If you run Flink locally (e.g. from your IDE) without creating a cluster, then only a subset of the memory configuration options are relevant, see also local execution for more details.
Otherwise, the simplest way to setup memory for TaskManagers is to configure the total memory. A more fine-grained approach is described in more detail here.
The rest of the memory components will be adjusted automatically, based on default values or additionally configured options. See next chapters for more details about the other memory components.
Configure Heap and Managed Memory # As mentioned before in total memory description, another way to setup memory in Flink is to specify explicitly both task heap and managed memory. It gives more control over the available JVM Heap to Flink’s tasks and its managed memory.
The rest of the memory components will be adjusted automatically, based on default values or additionally configured options. Here are more details about the other memory components.
If you have configured the task heap and managed memory explicitly, it is recommended to set neither total process memory nor total Flink memory. Otherwise, it may easily lead to memory configuration conflicts. Task (Operator) Heap Memory # If you want to guarantee that a certain amount of JVM Heap is available for your user code, you can set the task heap memory explicitly (taskmanager.memory.task.heap.size). It will be added to the JVM Heap size and will be dedicated to Flink’s operators running the user code.
Managed Memory # Managed memory is managed by Flink and is allocated as native memory (off-heap). The following workloads use managed memory:
Streaming jobs can use it for RocksDB state backend. Both streaming and batch jobs can use it for sorting, hash tables, caching of intermediate results. Both streaming and batch jobs can use it for executing User Defined Functions in Python processes. The size of managed memory can be
either configured explicitly via taskmanager.memory.managed.size or computed as a fraction of total Flink memory via taskmanager.memory.managed.fraction. Size will override fraction, if both are set. If neither size nor fraction is explicitly configured, the default fraction will be used.
See also how to configure memory for state backends and batch jobs.
Consumer Weights # If your job contains multiple types of managed memory consumers, you can also control how managed memory should be shared across these types. The configuration option taskmanager.memory.managed.consumer-weights allows you to set a weight for each type, to which Flink will reserve managed memory proportionally. Valid consumer types are:
OPERATOR: for built-in algorithms. STATE_BACKEND: for RocksDB state backend in streaming PYTHON: for Python processes. E.g. if a streaming job uses both RocksDB state backend and Python UDFs, and the consumer weights are configured as STATE_BACKEND:70,PYTHON:30, Flink will reserve 70% of the total managed memory for RocksDB state backend and 30% for Python processes.
For each type, Flink reserves managed memory only if the job contains managed memory consumers of that type. E.g, if a streaming job uses the heap state backend and Python UDFs, and the consumer weights are configured as STATE_BACKEND:70,PYTHON:30, Flink will use all of its managed memory for Python processes, because the heap state backend does not use managed memory.
Flink will not reserve managed memory for consumer types that are not included in the consumer weights. If the missing type is actually needed by the job, it can lead to memory allocation failures. By default, all consumer types are included. This could only happen when the weights are explicitly configured/overwritten. Configure Off-heap Memory (direct or native) # The off-heap memory which is allocated by user code should be accounted for in task off-heap memory (taskmanager.memory.task.off-heap.size).
You can also adjust the framework off-heap memory. You should only change this value if you are sure that the Flink framework needs more memory.
Flink includes the framework off-heap memory and task off-heap memory into the direct memory limit of the JVM, see also JVM parameters.
Note Although, native non-direct memory usage can be accounted for as a part of the framework off-heap memory or task off-heap memory, it will result in a higher JVM\u0026rsquo;s direct memory limit in this case.
Note The network memory is also part of JVM direct memory, but it is managed by Flink and guaranteed to never exceed its configured size. Therefore, resizing the network memory will not help in this situation.
See also the detailed memory model.
Detailed Memory Model # The following table lists all memory components, depicted above, and references Flink configuration options which affect the size of the respective components:
Component Configuration options Description Framework Heap Memory taskmanager.memory.framework.heap.size JVM Heap memory dedicated to Flink framework (advanced option) Task Heap Memory taskmanager.memory.task.heap.size JVM Heap memory dedicated to Flink application to run operators and user code Managed memory taskmanager.memory.managed.size taskmanager.memory.managed.fraction Native memory managed by Flink, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend Framework Off-heap Memory taskmanager.memory.framework.off-heap.size Off-heap direct (or native) memory dedicated to Flink framework (advanced option) Task Off-heap Memory taskmanager.memory.task.off-heap.size Off-heap direct (or native) memory dedicated to Flink application to run operators Network Memory taskmanager.memory.network.min taskmanager.memory.network.max taskmanager.memory.network.fraction Direct memory reserved for data record exchange between tasks (e.g. buffering for the transfer over the network), is a capped fractionated component of the total Flink memory. This memory is used for allocation of network buffers JVM metaspace taskmanager.memory.jvm-metaspace.size Metaspace size of the Flink JVM process JVM Overhead taskmanager.memory.jvm-overhead.min taskmanager.memory.jvm-overhead.max taskmanager.memory.jvm-overhead.fraction Native memory reserved for other JVM overhead: e.g. thread stacks, code cache, garbage collection space etc, it is a capped fractionated component of the total process memory As you can see, the size of some memory components can be simply set by the respective option. Other components can be tuned using multiple options.
Framework Memory # You should not change the framework heap memory and framework off-heap memory without a good reason. Adjust them only if you are sure that Flink needs more memory for some internal data structures or operations. It can be related to a particular deployment environment or job structure, like high parallelism. In addition, Flink dependencies, such as Hadoop may consume more direct or native memory in certain setups.
Note Flink neither isolates heap nor off-heap versions of framework and task memory at the moment. The separation of framework and task memory can be used in future releases for further optimizations.
Local Execution # If you start Flink locally on your machine as a single java program without creating a cluster (e.g. from your IDE) then all components are ignored except for the following:
Memory component Relevant options Default value for the local execution Task heap taskmanager.memory.task.heap.size infinite Task off-heap taskmanager.memory.task.off-heap.size infinite Managed memory taskmanager.memory.managed.size 128MB Network memory taskmanager.memory.network.min taskmanager.memory.network.max 64MB All of the components listed above can be but do not have to be explicitly configured for local execution. If they are not configured they are set to their default values. Task heap memory and task off-heap memory are considered to be infinite (Long.MAX_VALUE bytes) and managed memory has a default value of 128MB only for the local execution mode.
Note The task heap size is not related in any way to the real heap size in this case. It can become relevant for future optimizations coming with next releases. The actual JVM Heap size of the started local process is not controlled by Flink and depends on how you start the process. If you want to control the JVM Heap size you have to explicitly pass the corresponding JVM arguments, e.g. -Xmx, -Xms.
`}),e.add({id:93,href:"/flink/flink-docs-master/docs/concepts/stateful-stream-processing/",title:"Stateful Stream Processing",section:"Concepts",content:` Stateful Stream Processing # What is State? # While many operations in a dataflow simply look at one individual event at a time (for example an event parser), some operations remember information across multiple events (for example window operators). These operations are called stateful.
Some examples of stateful operations:
When an application searches for certain event patterns, the state will store the sequence of events encountered so far. When aggregating events per minute/hour/day, the state holds the pending aggregates. When training a machine learning model over a stream of data points, the state holds the current version of the model parameters. When historic data needs to be managed, the state allows efficient access to events that occurred in the past. Flink needs to be aware of the state in order to make it fault tolerant using checkpoints and savepoints.
Knowledge about the state also allows for rescaling Flink applications, meaning that Flink takes care of redistributing state across parallel instances.
Queryable state allows you to access state from outside of Flink during runtime.
When working with state, it might also be useful to read about Flink\u0026rsquo;s state backends. Flink provides different state backends that specify how and where state is stored.
Back to top
Keyed State # Keyed state is maintained in what can be thought of as an embedded key/value store. The state is partitioned and distributed strictly together with the streams that are read by the stateful operators. Hence, access to the key/value state is only possible on keyed streams, i.e. after a keyed/partitioned data exchange, and is restricted to the values associated with the current event\u0026rsquo;s key. Aligning the keys of streams and state makes sure that all state updates are local operations, guaranteeing consistency without transaction overhead. This alignment also allows Flink to redistribute the state and adjust the stream partitioning transparently.
Keyed State is further organized into so-called Key Groups. Key Groups are the atomic unit by which Flink can redistribute Keyed State; there are exactly as many Key Groups as the defined maximum parallelism. During execution each parallel instance of a keyed operator works with the keys for one or more Key Groups.
State Persistence # Flink implements fault tolerance using a combination of stream replay and checkpointing. A checkpoint marks a specific point in each of the input streams along with the corresponding state for each of the operators. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and replaying the records from the point of the checkpoint.
The checkpoint interval is a means of trading off the overhead of fault tolerance during execution with the recovery time (the number of records that need to be replayed).
The fault tolerance mechanism continuously draws snapshots of the distributed streaming data flow. For streaming applications with small state, these snapshots are very light-weight and can be drawn frequently without much impact on performance. The state of the streaming applications is stored at a configurable place, usually in a distributed file system.
In case of a program failure (due to machine-, network-, or software failure), Flink stops the distributed streaming dataflow. The system then restarts the operators and resets them to the latest successful checkpoint. The input streams are reset to the point of the state snapshot. Any records that are processed as part of the restarted parallel dataflow are guaranteed to not have affected the previously checkpointed state.
By default, checkpointing is disabled. See Checkpointing for details on how to enable and configure checkpointing. For this mechanism to realize its full guarantees, the data stream source (such as message queue or broker) needs to be able to rewind the stream to a defined recent point. Apache Kafka has this ability and Flink\u0026rsquo;s connector to Kafka exploits this. See Fault Tolerance Guarantees of Data Sources and Sinks for more information about the guarantees provided by Flink\u0026rsquo;s connectors. Because Flink\u0026rsquo;s checkpoints are realized through distributed snapshots, we use the words snapshot and checkpoint interchangeably. Often we also use the term snapshot to mean either checkpoint or savepoint. Checkpointing # The central part of Flink\u0026rsquo;s fault tolerance mechanism is drawing consistent snapshots of the distributed data stream and operator state. These snapshots act as consistent checkpoints to which the system can fall back in case of a failure. Flink\u0026rsquo;s mechanism for drawing these snapshots is described in \u0026ldquo;Lightweight Asynchronous Snapshots for Distributed Dataflows\u0026rdquo;. It is inspired by the standard Chandy-Lamport algorithm for distributed snapshots and is specifically tailored to Flink\u0026rsquo;s execution model.
Keep in mind that everything to do with checkpointing can be done asynchronously. The checkpoint barriers don\u0026rsquo;t travel in lock step and operations can asynchronously snapshot their state.
Since Flink 1.11, checkpoints can be taken with or without alignment. In this section, we describe aligned checkpoints first.
Barriers # A core element in Flink\u0026rsquo;s distributed snapshotting are the stream barriers. These barriers are injected into the data stream and flow with the records as part of the data stream. Barriers never overtake records, they flow strictly in line. A barrier separates the records in the data stream into the set of records that goes into the current snapshot, and the records that go into the next snapshot. Each barrier carries the ID of the snapshot whose records it pushed in front of it. Barriers do not interrupt the flow of the stream and are hence very lightweight. Multiple barriers from different snapshots can be in the stream at the same time, which means that various snapshots may happen concurrently.
Stream barriers are injected into the parallel data flow at the stream sources. The point where the barriers for snapshot n are injected (let\u0026rsquo;s call it Sn) is the position in the source stream up to which the snapshot covers the data. For example, in Apache Kafka, this position would be the last record\u0026rsquo;s offset in the partition. This position Sn is reported to the checkpoint coordinator (Flink\u0026rsquo;s JobManager).
The barriers then flow downstream. When an intermediate operator has received a barrier for snapshot n from all of its input streams, it emits a barrier for snapshot n into all of its outgoing streams. Once a sink operator (the end of a streaming DAG) has received the barrier n from all of its input streams, it acknowledges that snapshot n to the checkpoint coordinator. After all sinks have acknowledged a snapshot, it is considered completed.
Once snapshot n has been completed, the job will never again ask the source for records from before Sn, since at that point these records (and their descendant records) will have passed through the entire data flow topology.
Operators that receive more than one input stream need to align the input streams on the snapshot barriers. The figure above illustrates this:
As soon as the operator receives snapshot barrier n from an incoming stream, it cannot process any further records from that stream until it has received the barrier n from the other inputs as well. Otherwise, it would mix records that belong to snapshot n and with records that belong to snapshot n+1. Once the last stream has received barrier n, the operator emits all pending outgoing records, and then emits snapshot n barriers itself. It snapshots the state and resumes processing records from all input streams, processing records from the input buffers before processing the records from the streams. Finally, the operator writes the state asynchronously to the state backend. Note that the alignment is needed for all operators with multiple inputs and for operators after a shuffle when they consume output streams of multiple upstream subtasks.
Snapshotting Operator State # When operators contain any form of state, this state must be part of the snapshots as well.
Operators snapshot their state at the point in time when they have received all snapshot barriers from their input streams, and before emitting the barriers to their output streams. At that point, all updates to the state from records before the barriers have been made, and no updates that depend on records from after the barriers have been applied. Because the state of a snapshot may be large, it is stored in a configurable state backend. By default, this is the JobManager\u0026rsquo;s memory, but for production use a distributed reliable storage should be configured (such as HDFS). After the state has been stored, the operator acknowledges the checkpoint, emits the snapshot barrier into the output streams, and proceeds.
The resulting snapshot now contains:
For each parallel stream data source, the offset/position in the stream when the snapshot was started For each operator, a pointer to the state that was stored as part of the snapshot Recovery # Recovery under this mechanism is straightforward: Upon a failure, Flink selects the latest completed checkpoint k. The system then re-deploys the entire distributed dataflow, and gives each operator the state that was snapshotted as part of checkpoint k. The sources are set to start reading the stream from position Sk. For example in Apache Kafka, that means telling the consumer to start fetching from offset Sk.
If state was snapshotted incrementally, the operators start with the state of the latest full snapshot and then apply a series of incremental snapshot updates to that state.
See Restart Strategies for more information.
Unaligned Checkpointing # Checkpointing can also be performed unaligned. The basic idea is that checkpoints can overtake all in-flight data as long as the in-flight data becomes part of the operator state.
Note that this approach is actually closer to the Chandy-Lamport algorithm , but Flink still inserts the barrier in the sources to avoid overloading the checkpoint coordinator.
The figure depicts how an operator handles unaligned checkpoint barriers:
The operator reacts on the first barrier that is stored in its input buffers. It immediately forwards the barrier to the downstream operator by adding it to the end of the output buffers. The operator marks all overtaken records to be stored asynchronously and creates a snapshot of its own state. Consequently, the operator only briefly stops the processing of input to mark the buffers, forwards the barrier, and creates the snapshot of the other state.
Unaligned checkpointing ensures that barriers are arriving at the sink as fast as possible. It\u0026rsquo;s especially suited for applications with at least one slow moving data path, where alignment times can reach hours. However, since it\u0026rsquo;s adding additional I/O pressure, it doesn\u0026rsquo;t help when the I/O to the state backends is the bottleneck. See the more in-depth discussion in ops for other limitations.
Note that savepoints will always be aligned.
Unaligned Recovery # Operators first recover the in-flight data before starting processing any data from upstream operators in unaligned checkpointing. Aside from that, it performs the same steps as during recovery of aligned checkpoints.
State Backends # The exact data structures in which the key/values indexes are stored depends on the chosen state backend. One state backend stores data in an in-memory hash map, another state backend uses RocksDB as the key/value store. In addition to defining the data structure that holds the state, the state backends also implement the logic to take a point-in-time snapshot of the key/value state and store that snapshot as part of a checkpoint. State backends can be configured without changing your application logic.
Back to top
Savepoints # All programs that use checkpointing can resume execution from a savepoint. Savepoints allow both updating your programs and your Flink cluster without losing any state.
Savepoints are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend. They rely on the regular checkpointing mechanism for this.
Savepoints are similar to checkpoints except that they are triggered by the user and don\u0026rsquo;t automatically expire when newer checkpoints are completed. To make proper use of savepoints, it\u0026rsquo;s important to understand the differences between checkpoints and savepoints which is described in checkpoints vs. savepoints.
Back to top
Exactly Once vs. At Least Once # The alignment step may add latency to the streaming program. Usually, this extra latency is on the order of a few milliseconds, but we have seen cases where the latency of some outliers increased noticeably. For applications that require consistently super low latencies (few milliseconds) for all records, Flink has a switch to skip the stream alignment during a checkpoint. Checkpoint snapshots are still drawn as soon as an operator has seen the checkpoint barrier from each input.
When the alignment is skipped, an operator keeps processing all inputs, even after some checkpoint barriers for checkpoint n arrived. That way, the operator also processes elements that belong to checkpoint n+1 before the state snapshot for checkpoint n was taken. On a restore, these records will occur as duplicates, because they are both included in the state snapshot of checkpoint n, and will be replayed as part of the data after checkpoint n.
Alignment happens only for operators with multiple predecessors (joins) as well as operators with multiple senders (after a stream repartitioning/shuffle). Because of that, dataflows with only embarrassingly parallel streaming operations (map(), flatMap(), filter(), \u0026hellip;) actually give exactly once guarantees even in at least once mode. Back to top
State and Fault Tolerance in Batch Programs # Flink executes batch programs as a special case of streaming programs, where the streams are bounded (finite number of elements). A DataSet is treated internally as a stream of data. The concepts above thus apply to batch programs in the same way as well as they apply to streaming programs, with minor exceptions:
Fault tolerance for batch programs does not use checkpointing. Recovery happens by fully replaying the streams. That is possible, because inputs are bounded. This pushes the cost more towards the recovery, but makes the regular processing cheaper, because it avoids checkpoints.
Stateful operations in the DataSet API use simplified in-memory/out-of-core data structures, rather than key/value indexes.
The DataSet API introduces special synchronized (superstep-based) iterations, which are only possible on bounded streams. For details, check out the iteration docs.
Back to top
`}),e.add({id:94,href:"/flink/flink-docs-master/docs/dev/table/",title:"Table API \u0026 SQL",section:"Application Development",content:" "}),e.add({id:95,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/",title:"The Broadcast State Pattern",section:"State \u0026 Fault Tolerance",content:` The Broadcast State Pattern # In this section you will learn about how to use broadcast state in practise. Please refer to Stateful Stream Processing to learn about the concepts behind stateful stream processing.
Provided APIs # To show the provided APIs, we will start with an example before presenting their full functionality. As our running example, we will use the case where we have a stream of objects of different colors and shapes and we want to find pairs of objects of the same color that follow a certain pattern, e.g. a rectangle followed by a triangle. We assume that the set of interesting patterns evolves over time.
In this example, the first stream will contain elements of type Item with a Color and a Shape property. The other stream will contain the Rules.
Starting from the stream of Items, we just need to key it by Color, as we want pairs of the same color. This will make sure that elements of the same color end up on the same physical machine.
Java // key the items by color KeyedStream\u0026lt;Item, Color\u0026gt; colorPartitionedStream = itemStream .keyBy(new KeySelector\u0026lt;Item, Color\u0026gt;(){...}); Python # key the items by color color_partitioned_stream = item_stream.key_by(lambda item: ...) Moving on to the Rules, the stream containing them should be broadcasted to all downstream tasks, and these tasks should store them locally so that they can evaluate them against all incoming Items. The snippet below will i) broadcast the stream of rules and ii) using the provided MapStateDescriptor, it will create the broadcast state where the rules will be stored.
Java // a map descriptor to store the name of the rule (string) and the rule itself. MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); // broadcast the rules and create the broadcast state BroadcastStream\u0026lt;Rule\u0026gt; ruleBroadcastStream = ruleStream .broadcast(ruleStateDescriptor); Python # a map descriptor to store the name of the rule (string) and the rule (Python object) itself. rule_state_descriptor = MapStateDescriptor(\u0026#34;RuleBroadcastState\u0026#34;, Types.STRING(), Types.PICKLED_BYTE_ARRAY()) # broadcast the rules and create the broadcast state rule_broadcast_stream = rule_stream.broadcast(rule_state_descriptor) Finally, in order to evaluate the Rules against the incoming elements from the Item stream, we need to:
connect the two streams, and specify our match detecting logic. Connecting a stream (keyed or non-keyed) with a BroadcastStream can be done by calling connect() on the non-broadcasted stream, with the BroadcastStream as an argument. This will return a BroadcastConnectedStream, on which we can call process() with a special type of CoProcessFunction. The function will contain our matching logic. The exact type of the function depends on the type of the non-broadcasted stream:
if that is keyed, then the function is a KeyedBroadcastProcessFunction. if it is non-keyed, the function is a BroadcastProcessFunction. Given that our non-broadcasted stream is keyed, the following snippet includes the above calls:
The connect should be called on the non-broadcasted stream, with the BroadcastStream as an argument. Java DataStream\u0026lt;String\u0026gt; output = colorPartitionedStream .connect(ruleBroadcastStream) .process( // type arguments in our KeyedBroadcastProcessFunction represent: // 1. the key of the keyed stream // 2. the type of elements in the non-broadcast side // 3. the type of elements in the broadcast side // 4. the type of the result, here a string new KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // my matching logic } ); Python class MyKeyedBroadcastProcessFunction(KeyedBroadcastProcessFunction): # my matching logic ... output = color_partitioned_stream \\ .connect(rule_broadcast_stream) \\ .process(MyKeyedBroadcastProcessFunction()) BroadcastProcessFunction and KeyedBroadcastProcessFunction # As in the case of a CoProcessFunction, these functions have two process methods to implement; the processBroadcastElement() which is responsible for processing incoming elements in the broadcasted stream and the processElement() which is used for the non-broadcasted one. The full signatures of the methods are presented below:
Java public abstract class BroadcastProcessFunction\u0026lt;IN1, IN2, OUT\u0026gt; extends BaseBroadcastProcessFunction { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } public abstract class KeyedBroadcastProcessFunction\u0026lt;KS, IN1, IN2, OUT\u0026gt; { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } Python class BroadcastProcessFunction(BaseBroadcastProcessFunction, Generic[IN1, IN2, OUT]): @abstractmethod def process_element(value: IN1, ctx: ReadOnlyContext): pass @abstractmethod def process_broadcast_element(value: IN2, ctx: Context): pass class KeyedBroadcastProcessFunction(BaseBrodcastProcessFunction, Generic[KEY, IN1, IN2, OUT]): @abstractmethod def process_element(value: IN1, ctx: ReadOnlyContext): pass @abstractmethod def process_broadcast_element(value: IN2, ctx: Context): pass def on_timer(timestamp: int, ctx: OnTimerContext): pass The first thing to notice is that both functions require the implementation of the processBroadcastElement() method for processing elements in the broadcast side and the processElement() for elements in the non-broadcasted side.
The two methods differ in the context they are provided. The non-broadcast side has a ReadOnlyContext, while the broadcasted side has a Context.
Both of these contexts (ctx in the following enumeration):
Java give access to the broadcast state: ctx.getBroadcastState(MapStateDescriptor\u0026lt;K, V\u0026gt; stateDescriptor) allow to query the timestamp of the element: ctx.timestamp(), get the current watermark: ctx.currentWatermark() get the current processing time: ctx.currentProcessingTime(), and emit elements to side-outputs: ctx.output(OutputTag\u0026lt;X\u0026gt; outputTag, X value). Python give access to the broadcast state: ctx.get_broadcast_state(state_descriptor: MapStateDescriptor) allow to query the timestamp of the element: ctx.timestamp(), get the current watermark: ctx.current_watermark() get the current processing time: ctx.current_processing_time(), and emit elements to side-outputs: yield output_tag, value. The stateDescriptor in the getBroadcastState() should be identical to the one in the .broadcast(ruleStateDescriptor) above.
The difference lies in the type of access each one gives to the broadcast state. The broadcasted side has read-write access to it, while the non-broadcast side has read-only access (thus the names). The reason for this is that in Flink there is no cross-task communication. So, to guarantee that the contents in the Broadcast State are the same across all parallel instances of our operator, we give read-write access only to the broadcast side, which sees the same elements across all tasks, and we require the computation on each incoming element on that side to be identical across all tasks. Ignoring this rule would break the consistency guarantees of the state, leading to inconsistent and often difficult to debug results.
The logic implemented in processBroadcastElement() must have the same deterministic behavior across all parallel instances! Finally, due to the fact that the KeyedBroadcastProcessFunction is operating on a keyed stream, it exposes some functionality which is not available to the BroadcastProcessFunction. That is:
the ReadOnlyContext in the processElement() method gives access to Flink\u0026rsquo;s underlying timer service, which allows to register event and/or processing time timers. When a timer fires, the onTimer() (shown above) is invoked with an OnTimerContext which exposes the same functionality as the ReadOnlyContext plus the ability to ask if the timer that fired was an event or processing time one and to query the key associated with the timer. the Context in the processBroadcastElement() method contains the method applyToKeyedState(StateDescriptor\u0026lt;S, VS\u0026gt; stateDescriptor, KeyedStateFunction\u0026lt;KS, S\u0026gt; function). This allows to register a KeyedStateFunction to be applied to all states of all keys associated with the provided stateDescriptor. Note that apply_to_keyed_state is not supported in PyFlink yet. Registering timers is only possible at processElement() of the KeyedBroadcastProcessFunction and only there. It is not possible in the processBroadcastElement() method, as there is no key associated to the broadcasted elements. Coming back to our original example, our KeyedBroadcastProcessFunction could look like the following:
Java new KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // store partial matches, i.e. first elements of the pair waiting for their second element // we keep a list as we may have many first elements waiting private final MapStateDescriptor\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; mapStateDesc = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;items\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, new ListTypeInfo\u0026lt;\u0026gt;(Item.class)); // identical to our ruleStateDescriptor above private final MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); @Override public void processBroadcastElement(Rule value, Context ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { ctx.getBroadcastState(ruleStateDescriptor).put(value.name, value); } @Override public void processElement(Item value, ReadOnlyContext ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { final MapState\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; state = getRuntimeContext().getMapState(mapStateDesc); final Shape shape = value.getShape(); for (Map.Entry\u0026lt;String, Rule\u0026gt; entry : ctx.getBroadcastState(ruleStateDescriptor).immutableEntries()) { final String ruleName = entry.getKey(); final Rule rule = entry.getValue(); List\u0026lt;Item\u0026gt; stored = state.get(ruleName); if (stored == null) { stored = new ArrayList\u0026lt;\u0026gt;(); } if (shape == rule.second \u0026amp;\u0026amp; !stored.isEmpty()) { for (Item i : stored) { out.collect(\u0026#34;MATCH: \u0026#34; + i + \u0026#34; - \u0026#34; + value); } stored.clear(); } // there is no else{} to cover if rule.first == rule.second if (shape.equals(rule.first)) { stored.add(value); } if (stored.isEmpty()) { state.remove(ruleName); } else { state.put(ruleName, stored); } } } } Python class MyKeyedBroadcastProcessFunction(KeyedBroadcastProcessFunction): def __init__(self): self._map_state_desc = MapStateDescriptor(\u0026#34;item\u0026#34;, Types.STRING(), Types.LIST(Types.PICKLED_BYTE_ARRAY())) self._rule_state_desc = MapStateDescriptor(\u0026#34;RulesBroadcastState\u0026#34;, Types.STRING(), Types.PICKLED_BYTE_ARRAY()) self._map_state = None def open(self, ctx: RuntimeContext): self._map_state = ctx.get_map_state(self._map_state_desc) def process_broadcast_element(value: Rule, ctx: KeyedBroadcastProcessFunction.Context): ctx.get_broadcast_state(self._rule_state_desc).put(value.name, value) def process_element(value: Item, ctx: KeyedBroadcastProcessFunction.ReadOnlyContext): shape = value.get_shape() for rule_name, rule in ctx.get_broadcast_state(self._rule_state_desc).items(): stored = self._map_state.get(rule_name) if stored is None: stored = [] if shape == rule.second and len(stored) \u0026gt; 0: for i in stored: yield \u0026#34;MATCH: {} - {}\u0026#34;.format(i, value) stored = [] if shape == rule.first: stored.append(value) if len(stored) == 0: self._map_state.remove(rule_name) else: self._map_state.put(rule_name, stored) Important Considerations # After describing the offered APIs, this section focuses on the important things to keep in mind when using broadcast state. These are:
There is no cross-task communication: As stated earlier, this is the reason why only the broadcast side of a (Keyed)-BroadcastProcessFunction can modify the contents of the broadcast state. In addition, the user has to make sure that all tasks modify the contents of the broadcast state in the same way for each incoming element. Otherwise, different tasks might have different contents, leading to inconsistent results.
Order of events in Broadcast State may differ across tasks: Although broadcasting the elements of a stream guarantees that all elements will (eventually) go to all downstream tasks, elements may arrive in a different order to each task. So the state updates for each incoming element MUST NOT depend on the ordering of the incoming events.
All tasks checkpoint their broadcast state: Although all tasks have the same elements in their broadcast state when a checkpoint takes place (checkpoint barriers do not overpass elements), all tasks checkpoint their broadcast state, and not just one of them. This is a design decision to avoid having all tasks read from the same file during a restore (thus avoiding hotspots), although it comes at the expense of increasing the size of the checkpointed state by a factor of p (= parallelism). Flink guarantees that upon restoring/rescaling there will be no duplicates and no missing data. In case of recovery with the same or smaller parallelism, each task reads its checkpointed state. Upon scaling up, each task reads its own state, and the remaining tasks (p_new-p_old) read checkpoints of previous tasks in a round-robin manner.
No RocksDB state backend: Broadcast state is kept in-memory at runtime and memory provisioning should be done accordingly. This holds for all operator states.
Back to top
`}),e.add({id:96,href:"/flink/flink-docs-master/docs/dev/table/concepts/time_attributes/",title:"Time Attributes",section:"Streaming Concepts",content:" Time Attributes # Flink can process data based on different notions of time.\nProcessing time refers to the machine\u0026rsquo;s system time (also known as epoch time, e.g. Java\u0026rsquo;s System.currentTimeMillis()) that is executing the respective operation. Event time refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened. For more information about time handling in Flink, see the introduction about event time and watermarks.\nIntroduction to Time Attributes # Time attributes can be part of every table schema. They are defined when creating a table from a CREATE TABLE DDL or a DataStream. Once a time attribute is defined, it can be referenced as a field and used in time-based operations. As long as a time attribute is not modified, and is simply forwarded from one part of a query to another, it remains a valid time attribute. Time attributes behave like regular timestamps, and are accessible for calculations. When used in calculations, time attributes are materialized and act as standard timestamps. However, ordinary timestamps cannot be used in place of, or be converted to, time attributes.\nEvent Time # Event time allows a table program to produce results based on timestamps in every record, allowing for consistent results despite out-of-order or late events. It also ensures the replayability of the results of the table program when reading records from persistent storage.\nAdditionally, event time allows for unified syntax for table programs in both batch and streaming environments. A time attribute in a streaming environment can be a regular column of a row in a batch environment.\nTo handle out-of-order events and to distinguish between on-time and late events in streaming, Flink needs to know the timestamp for each row, and it also needs regular indications of how far along in event time the processing has progressed so far (via so-called watermarks).\nEvent time attributes can be defined in CREATE table DDL or during DataStream-to-Table conversion.\nDefining in DDL # The event time attribute is defined using a WATERMARK statement in CREATE table DDL. A watermark statement defines a watermark generation expression on an existing event time field, which marks the event time field as the event time attribute. Please see CREATE TABLE DDL for more information about watermark statement and watermark strategies.\nFlink supports defining event time attribute on TIMESTAMP column and TIMESTAMP_LTZ column. If the timestamp data in the source is represented as year-month-day-hour-minute-second, usually a string value without time-zone information, e.g. 2020-04-15 20:13:40.564, it\u0026rsquo;s recommended to define the event time attribute as a TIMESTAMP column::\nCREATE TABLE user_actions ( user_name STRING, data STRING, user_action_time TIMESTAMP(3), -- declare user_action_time as event time attribute and use 5 seconds delayed watermark strategy WATERMARK FOR user_action_time AS user_action_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( ... ); SELECT TUMBLE_START(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE); If the timestamp data in the source is represented as a epoch time, usually a long value, e.g. 1618989564564, it\u0026rsquo;s recommended to define event time attribute as a TIMESTAMP_LTZ column:\nCREATE TABLE user_actions ( user_name STRING, data STRING, ts BIGINT, time_ltz AS TO_TIMESTAMP_LTZ(ts, 3), -- declare time_ltz as event time attribute and use 5 seconds delayed watermark strategy WATERMARK FOR time_ltz AS time_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( ... ); SELECT TUMBLE_START(time_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(time_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTE); During DataStream-to-Table Conversion # When converting a DataStream to a table, an event time attribute can be defined with the .rowtime property during schema definition. Timestamps and watermarks must have already been assigned in the DataStream being converted. During the conversion, Flink always derives rowtime attribute as TIMESTAMP WITHOUT TIME ZONE, because DataStream doesn\u0026rsquo;t have time zone notion, and treats all event time values as in UTC.\nThere are two ways of defining the time attribute when converting a DataStream into a Table. Depending on whether the specified .rowtime field name exists in the schema of the DataStream, the timestamp is either (1) appended as a new column, or it (2) replaces an existing column.\nIn either case, the event time timestamp field will hold the value of the DataStream event time timestamp.\nJava // Option 1: // extract timestamp and assign watermarks based on knowledge of the stream DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = inputStream.assignTimestampsAndWatermarks(...); // declare an additional logical field as an event time attribute Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;), $(\u0026#34;user_action_time\u0026#34;).rowtime()); // Option 2: // extract timestamp from first field, and assign watermarks based on knowledge of the stream DataStream\u0026lt;Tuple3\u0026lt;Long, String, String\u0026gt;\u0026gt; stream = inputStream.assignTimestampsAndWatermarks(...); // the first field has been used for timestamp extraction, and is no longer necessary // replace first field with a logical event time attribute Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_action_time\u0026#34;).rowtime(), $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;)); // Usage: WindowedTable windowedTable = table.window(Tumble .over(lit(10).minutes()) .on($(\u0026#34;user_action_time\u0026#34;)) .as(\u0026#34;userActionWindow\u0026#34;)); Scala // Option 1: // extract timestamp and assign watermarks based on knowledge of the stream val stream: DataStream[(String, String)] = inputStream.assignTimestampsAndWatermarks(...) // declare an additional logical field as an event time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.rowtime) // Option 2: // extract timestamp from first field, and assign watermarks based on knowledge of the stream val stream: DataStream[(Long, String, String)] = inputStream.assignTimestampsAndWatermarks(...) // the first field has been used for timestamp extraction, and is no longer necessary // replace first field with a logical event time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;user_action_time\u0026#34;.rowtime, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;) // Usage: val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) Python # Option 1: # extract timestamp and assign watermarks based on knowledge of the stream stream = input_stream.assign_timestamps_and_watermarks(...) table = t_env.from_data_stream(stream, col(\u0026#39;user_name\u0026#39;), col(\u0026#39;data\u0026#39;), col(\u0026#39;user_action_time\u0026#39;).rowtime) # Option 2: # extract timestamp from first field, and assign watermarks based on knowledge of the stream stream = input_stream.assign_timestamps_and_watermarks(...) # the first field has been used for timestamp extraction, and is no longer necessary # replace first field with a logical event time attribute table = t_env.from_data_stream(stream, col(\u0026#34;user_action_time\u0026#34;).rowtime, col(\u0026#39;user_name\u0026#39;), col(\u0026#39;data\u0026#39;)) # Usage: table.window(Tumble.over(lit(10).minutes).on(col(\u0026#34;user_action_time\u0026#34;)).alias(\u0026#34;userActionWindow\u0026#34;)) Processing Time # Processing time allows a table program to produce results based on the time of the local machine. It is the simplest notion of time, but it will generate non-deterministic results. Processing time does not require timestamp extraction or watermark generation.\nThere are two ways to define a processing time attribute.\nDefining in DDL # The processing time attribute is defined as a computed column in CREATE table DDL using the system PROCTIME() function, the function return type is TIMESTAMP_LTZ. Please see CREATE TABLE DDL for more information about computed column.\nCREATE TABLE user_actions ( user_name STRING, data STRING, user_action_time AS PROCTIME() -- declare an additional field as a processing time attribute ) WITH ( ... ); SELECT TUMBLE_START(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE); During DataStream-to-Table Conversion # The processing time attribute is defined with the .proctime property during schema definition. The time attribute must only extend the physical schema by an additional logical field. Thus, it is only definable at the end of the schema definition.\nJava DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = ...; // declare an additional logical field as a processing time attribute Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;), $(\u0026#34;user_action_time\u0026#34;).proctime()); WindowedTable windowedTable = table.window( Tumble.over(lit(10).minutes()) .on($(\u0026#34;user_action_time\u0026#34;)) .as(\u0026#34;userActionWindow\u0026#34;)); Scala val stream: DataStream[(String, String)] = ... // declare an additional logical field as a processing time attribute val table = tEnv.fromDataStream(stream, $\u0026#34;UserActionTimestamp\u0026#34;, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.proctime) val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) Python stream = ... # declare an additional logical field as a processing time attribute table = t_env.from_data_stream(stream, col(\u0026#34;UserActionTimestamp\u0026#34;), col(\u0026#34;user_name\u0026#34;), col(\u0026#34;data\u0026#34;), col(\u0026#34;user_action_time\u0026#34;).proctime) windowed_table = table.window(Tumble.over(lit(10).minutes).on(col(\u0026#34;user_action_time\u0026#34;)).alias(\u0026#34;userActionWindow\u0026#34;)) "}),e.add({id:97,href:"/flink/flink-docs-master/docs/concepts/time/",title:"Timely Stream Processing",section:"Concepts",content:` Timely Stream Processing # Introduction # Timely stream processing is an extension of stateful stream processing in which time plays some role in the computation. Among other things, this is the case when you do time series analysis, when doing aggregations based on certain time periods (typically called windows), or when you do event processing where the time when an event occurred is important.
In the following sections we will highlight some of the topics that you should consider when working with timely Flink Applications.
Back to top
Notions of Time: Event Time and Processing Time # When referring to time in a streaming program (for example to define windows), one can refer to different notions of time:
Processing time: Processing time refers to the system time of the machine that is executing the respective operation.
When a streaming program runs on processing time, all time-based operations (like time windows) will use the system clock of the machines that run the respective operator. An hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour. For example, if an application begins running at 9:15am, the first hourly processing time window will include events processed between 9:15am and 10:00am, the next window will include events processed between 10:00am and 11:00am, and so on.
Processing time is the simplest notion of time and requires no coordination between streams and machines. It provides the best performance and the lowest latency. However, in distributed and asynchronous environments processing time does not provide determinism, because it is susceptible to the speed at which records arrive in the system (for example from the message queue), to the speed at which the records flow between operators inside the system, and to outages (scheduled, or otherwise).
Event time: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink, and that event timestamp can be extracted from each record. In event time, the progress of time depends on the data, not on any wall clocks. Event time programs must specify how to generate Event Time Watermarks, which is the mechanism that signals progress in event time. This watermarking mechanism is described in a later section, below.
In a perfect world, event time processing would yield completely consistent and deterministic results, regardless of when events arrive, or their ordering. However, unless the events are known to arrive in-order (by timestamp), event time processing incurs some latency while waiting for out-of-order events. As it is only possible to wait for a finite period of time, this places a limit on how deterministic event time applications can be.
Assuming all of the data has arrived, event time operations will behave as expected, and produce correct and consistent results even when working with out-of-order or late events, or when reprocessing historic data. For example, an hourly event time window will contain all records that carry an event timestamp that falls into that hour, regardless of the order in which they arrive, or when they are processed. (See the section on lateness for more information.)
Note that sometimes when event time programs are processing live data in real-time, they will use some processing time operations in order to guarantee that they are progressing in a timely fashion.
Back to top
Event Time and Watermarks # Note: Flink implements many techniques from the Dataflow Model. For a good introduction to event time and watermarks, have a look at the articles below.
Streaming 101 by Tyler Akidau The Dataflow Model paper A stream processor that supports event time needs a way to measure the progress of event time. For example, a window operator that builds hourly windows needs to be notified when event time has passed beyond the end of an hour, so that the operator can close the window in progress.
Event time can progress independently of processing time (measured by wall clocks). For example, in one program the current event time of an operator may trail slightly behind the processing time (accounting for a delay in receiving the events), while both proceed at the same speed. On the other hand, another streaming program might progress through weeks of event time with only a few seconds of processing, by fast-forwarding through some historic data already buffered in a Kafka topic (or another message queue).
The mechanism in Flink to measure progress in event time is watermarks. Watermarks flow as part of the data stream and carry a timestamp t. A Watermark(t) declares that event time has reached time t in that stream, meaning that there should be no more elements from the stream with a timestamp t\u0026rsquo; \u0026lt;= t (i.e. events with timestamps older or equal to the watermark).
The figure below shows a stream of events with (logical) timestamps, and watermarks flowing inline. In this example the events are in order (with respect to their timestamps), meaning that the watermarks are simply periodic markers in the stream.
Watermarks are crucial for out-of-order streams, as illustrated below, where the events are not ordered by their timestamps. In general a watermark is a declaration that by that point in the stream, all events up to a certain timestamp should have arrived. Once a watermark reaches an operator, the operator can advance its internal event time clock to the value of the watermark.
Note that event time is inherited by a freshly created stream element (or elements) from either the event that produced them or from watermark that triggered creation of those elements.
Watermarks in Parallel Streams # Watermarks are generated at, or directly after, source functions. Each parallel subtask of a source function usually generates its watermarks independently. These watermarks define the event time at that particular parallel source.
As the watermarks flow through the streaming program, they advance the event time at the operators where they arrive. Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.
Some operators consume multiple input streams; a union, for example, or operators following a keyBy(\u0026hellip;) or partition(\u0026hellip;) function. Such an operator\u0026rsquo;s current event time is the minimum of its input streams\u0026rsquo; event times. As its input streams update their event times, so does the operator.
The figure below shows an example of events and watermarks flowing through parallel streams, and operators tracking event time.
Lateness # It is possible that certain elements will violate the watermark condition, meaning that even after the Watermark(t) has occurred, more elements with timestamp t\u0026rsquo; \u0026lt;= t will occur. In fact, in many real world setups, certain elements can be arbitrarily delayed, making it impossible to specify a time by which all elements of a certain event timestamp will have occurred. Furthermore, even if the lateness can be bounded, delaying the watermarks by too much is often not desirable, because it causes too much delay in the evaluation of event time windows.
For this reason, streaming programs may explicitly expect some late elements. Late elements are elements that arrive after the system\u0026rsquo;s event time clock (as signaled by the watermarks) has already passed the time of the late element\u0026rsquo;s timestamp. See Allowed Lateness for more information on how to work with late elements in event time windows.
Windowing # Aggregating events (e.g., counts, sums) works differently on streams than in batch processing. For example, it is impossible to count all elements in a stream, because streams are in general infinite (unbounded). Instead, aggregates on streams (counts, sums, etc), are scoped by windows, such as \u0026ldquo;count over the last 5 minutes\u0026rdquo;, or \u0026ldquo;sum of the last 100 elements\u0026rdquo;.
Windows can be time driven (example: every 30 seconds) or data driven (example: every 100 elements). One typically distinguishes different types of windows, such as tumbling windows (no overlap), sliding windows (with overlap), and session windows (punctuated by a gap of inactivity).
Please check out this blog post for additional examples of windows or take a look a window documentation of the DataStream API.
Back to top
`}),e.add({id:98,href:"/flink/flink-docs-master/docs/dev/configuration/gradle/",title:"Using Gradle",section:"Project Configuration",content:` How to use Gradle to configure your project # You will likely need a build tool to configure your Flink project. This guide will show you how to do so with Gradle, an open-source general-purpose build tool that can be used to automate tasks in the development process.
Requirements # Gradle 7.x Java 11 Importing the project into your IDE # Once the project folder and files have been created, we recommend that you import this project into your IDE for developing and testing.
IntelliJ IDEA supports Gradle projects via the Gradle plugin.
Eclipse does so via the Eclipse Buildship plugin (make sure to specify a Gradle version \u0026gt;= 3.0 in the last step of the import wizard; the shadow plugin requires it). You may also use Gradle\u0026rsquo;s IDE integration to create project files with Gradle.
Note: The default JVM heap size for Java may be too small for Flink and you have to manually increase it. In Eclipse, choose Run Configurations -\u0026gt; Arguments and write into the VM Arguments box: -Xmx800m. In IntelliJ IDEA recommended way to change JVM options is from the Help | Edit Custom VM Options menu. See this article for details.
Note on IntelliJ: To make the applications run within IntelliJ IDEA, it is necessary to tick the Include dependencies with \u0026quot;Provided\u0026quot; scope box in the run configuration. If this option is not available (possibly due to using an older IntelliJ IDEA version), then a workaround is to create a test that calls the application\u0026rsquo;s main() method.
Building the project # If you want to build/package your project, go to your project directory and run the \u0026lsquo;gradle clean shadowJar\u0026rsquo; command. You will find a JAR file that contains your application, plus connectors and libraries that you may have added as dependencies to the application: build/libs/\u0026lt;project-name\u0026gt;-\u0026lt;version\u0026gt;-all.jar.
Note: If you use a different class than StreamingJob as the application\u0026rsquo;s main class / entry point, we recommend you change the mainClassName setting in the build.gradle file accordingly. That way, Flink can run the application from the JAR file without additionally specifying the main class.
Adding dependencies to the project # Specify a dependency configuration in the dependencies block of your build.gradle file.
For example, if you created your project using our Gradle build script or quickstart script, you can add the Kafka connector as a dependency like this:
build.gradle
... dependencies { ... flinkShadowJar \u0026#34;org.apache.flink:flink-connector-kafka:\${flinkVersion}\u0026#34; ... } ... Important: Note that all these (core) dependencies should have their scope set to provided. This means that they are needed to compile against, but that they should not be packaged into the project\u0026rsquo;s resulting application JAR file. If not set to provided, the best case scenario is that the resulting JAR becomes excessively large, because it also contains all Flink core dependencies. The worst case scenario is that the Flink core dependencies that are added to the application\u0026rsquo;s JAR file clash with some of your own dependency versions (which is normally avoided through inverted classloading).
To correctly package the dependencies into the application JAR, these application dependencies must be set to the compile scope.
Packaging the application # Depending on your use case, you may need to package your Flink application in different ways before it gets deployed to a Flink environment.
If you want to create a JAR for a Flink Job and use only Flink dependencies without any third-party dependencies (i.e. using the filesystem connector with JSON format), you do not need to create an uber/fat JAR or shade any dependencies.
You can use the command gradle clean installDist. If you are using a Gradle Wrapper, this would be ./gradlew clean installDist.
If you want to create a JAR for a Flink Job and use external dependencies not built into the Flink distribution, you can either add them to the classpath of the distribution or shade them into your uber/fat application JAR.
You can use the command gradle clean installShadowDist, which will produce a single fat JAR in /build/install/yourProject/lib. If you are using a Gradle Wrapper, this would be ./gradlew clean installShadowDist.
With the generated uber/fat JAR, you can submit it to a local or remote cluster with:
bin/flink run -c org.example.MyJob myFatJar.jar To learn more about how to deploy Flink jobs, check out the deployment guide.
`}),e.add({id:99,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/with/",title:"WITH clause",section:"Queries",content:` WITH clause # Batch Streaming
WITH provides a way to write auxiliary statements for use in a larger query. These statements, which are often referred to as Common Table Expression (CTE), can be thought of as defining temporary views that exist just for one query.
The syntax of WITH statement is:
WITH \u0026lt;with_item_definition\u0026gt; [ , ... ] SELECT ... FROM ...; \u0026lt;with_item_defintion\u0026gt;: with_item_name (column_name[, ...n]) AS ( \u0026lt;select_query\u0026gt; ) The following example defines a common table expression orders_with_total and use it in a GROUP BY query.
WITH orders_with_total AS ( SELECT order_id, price + tax AS total FROM Orders ) SELECT order_id, SUM(total) FROM orders_with_total GROUP BY order_id; Back to top
`}),e.add({id:100,href:"/flink/flink-docs-master/docs/deployment/resource-providers/standalone/working_directory/",title:"Working Directory",section:"Standalone",content:` Working Directory # Flink supports to configure a working directory (FLIP-198) for Flink processes (JobManager and TaskManager). The working directory is used by the processes to store information that can be recovered upon a process restart. The requirement for this to work is that the process is started with the same identity and has access to the volume on which the working directory is stored.
Configuring the Working Directory # The working directories for the Flink processes are:
JobManager working directory: \u0026lt;WORKING_DIR_BASE\u0026gt;/jm_\u0026lt;JM_RESOURCE_ID\u0026gt; TaskManager working directory: \u0026lt;WORKING_DIR_BASE\u0026gt;/tm_\u0026lt;TM_RESOURCE_ID\u0026gt; with \u0026lt;WORKING_DIR_BASE\u0026gt; being the working directory base, \u0026lt;JM_RESOURCE_ID\u0026gt; being the resource id of the JobManager process and \u0026lt;TM_RESOURCE_ID\u0026gt; being the resource id of the TaskManager process.
The \u0026lt;WORKING_DIR_BASE\u0026gt; can be configured by process.working-dir. It needs to point to a local directory. If not explicitly configured, then it defaults to a randomly picked directory from io.tmp.dirs.
It is also possible to configure a JobManager and TaskManager specific \u0026lt;WORKING_DIR_BASE\u0026gt; via process.jobmanager.working-dir and process.taskmanager.working-dir respectively.
The JobManager resource id can be configured via jobmanager.resource-id. If not explicitly configured, then it will be a random UUID.
Similarly, the TaskManager resource id can be configured via taskmanager.resource-id. If not explicitly configured, then it will be a random value containing the host and port of the running process.
Artifacts Stored in the Working Directory # Flink processes will use the working directory to store the following artifacts:
Blobs stored by the BlobServer and BlobCache Local state if state.backend.local-recovery is enabled RocksDB\u0026rsquo;s working directory Local Recovery Across Process Restarts # The working directory can be used to enable local recovery across process restarts (FLIP-201). This means that Flink does not have to recover state information from remote storage.
In order to use this feature, local recovery has to be enabled via state.backend.local-recovery. Moreover, the TaskManager processes need to get a deterministic resource id assigned via taskmanager.resource-id. Last but not least, a failed TaskManager process needs to be restarted with the same working directory.
process.working-dir: /path/to/working/dir/base state.backend.local-recovery: true taskmanager.resource-id: TaskManager_1 # important: Change for every TaskManager process Back to top
`}),e.add({id:101,href:"/flink/flink-docs-master/docs/dev/dataset/zip_elements_guide/",title:"Zipping Elements",section:"DataSet API (Legacy)",content:` Zipping Elements in a DataSet # In certain algorithms, one may need to assign unique identifiers to data set elements. This document shows how DataSetUtils can be used for that purpose.
Zip with a Dense Index # zipWithIndex assigns consecutive labels to the elements, receiving a data set as input and returning a new data set of (unique id, initial value) 2-tuples. This process requires two passes, first counting then labeling elements, and cannot be pipelined due to the synchronization of counts. The alternative zipWithUniqueId works in a pipelined fashion and is preferred when a unique labeling is sufficient. For example, the following code:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataSet\u0026lt;String\u0026gt; in = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; result = DataSetUtils.zipWithIndex(in); result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;); env.execute(); Scala import org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithIndex result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() Python from flink.plan.Environment import get_environment env = get_environment() env.set_parallelism(2) input = env.from_elements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) result = input.zip_with_index() result.write_text(result_path) env.execute() may yield the tuples: (0,G), (1,H), (2,A), (3,B), (4,C), (5,D), (6,E), (7,F)
Back to top
Zip with a Unique Identifier # In many cases one may not need to assign consecutive labels. zipWithUniqueId works in a pipelined fashion, speeding up the label assignment process. This method receives a data set as input and returns a new data set of (unique id, initial value) 2-tuples. For example, the following code:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataSet\u0026lt;String\u0026gt; in = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; result = DataSetUtils.zipWithUniqueId(in); result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;); env.execute(); Scala import org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithUniqueId result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() may yield the tuples: (0,G), (1,A), (2,H), (3,B), (5,C), (7,D), (9,E), (11,F)
Back to top
`}),e.add({id:102,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/third_party_serializers/",title:"3rd Party Serializers",section:"Data Types \u0026 Serialization",content:` 3rd Party Serializers # If you use a custom type in your Flink program which cannot be serialized by the Flink type serializer, Flink falls back to using the generic Kryo serializer. You may register your own serializer or a serialization system like Google Protobuf or Apache Thrift with Kryo. To do that, simply register the type class and the serializer in the ExecutionConfig of your Flink program.
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // register the class of the serializer as serializer for a type env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, MyCustomSerializer.class); // register an instance as serializer for a type MySerializer mySerializer = new MySerializer(); env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, mySerializer); Note that your custom serializer has to extend Kryo\u0026rsquo;s Serializer class. In the case of Google Protobuf or Apache Thrift, this has already been done for you:
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // register the Google Protobuf serializer with Kryo env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class); // register the serializer included with Apache Thrift as the standard serializer // TBaseSerializer states it should be initialized as a default Kryo serializer env.getConfig().addDefaultKryoSerializer(MyCustomType.class, TBaseSerializer.class); For the above example to work, you need to include the necessary dependencies in your Maven project file (pom.xml). In the dependency section, add the following for Apache Thrift:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-thrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- libthrift is required by chill-thrift --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.thrift\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;libthrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.11.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; For Google Protobuf you need the following Maven dependency:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-protobuf\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- We need protobuf for chill-protobuf --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Please adjust the versions of both libraries as needed.
Issue with using Kryo\u0026rsquo;s JavaSerializer # If you register Kryo\u0026rsquo;s JavaSerializer for your custom type, you may encounter ClassNotFoundExceptions even though your custom type class is included in the submitted user code jar. This is due to a know issue with Kryo\u0026rsquo;s JavaSerializer, which may incorrectly use the wrong classloader.
In this case, you should use org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer instead to resolve the issue. This is a reimplemented JavaSerializer in Flink that makes sure the user code classloader is used.
Please refer to FLINK-6025 for more details.
Back to top
`}),e.add({id:103,href:"/flink/flink-docs-master/docs/deployment/filesystems/oss/",title:"Aliyun OSS",section:"File Systems",content:` Aliyun Object Storage Service (OSS) # OSS: Object Storage Service # Aliyun Object Storage Service (Aliyun OSS) is widely used, particularly popular among China’s cloud users, and it provides cloud object storage for a variety of use cases. You can use OSS with Flink for reading and writing data as well in conjunction with the streaming state backends
You can use OSS objects like regular files by specifying paths in the following format:
oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt; Below shows how to use OSS in a Flink job:
// Read from OSS bucket env.readTextFile(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;); // Write to OSS bucket stream.writeAsText(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;); // Use OSS as checkpoint storage env.getCheckpointConfig().setCheckpointStorage(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;); Shaded Hadoop OSS file system # To use flink-oss-fs-hadoop, copy the respective JAR file from the opt directory to a directory in plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/oss-fs-hadoop cp ./opt/flink-oss-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/oss-fs-hadoop/ flink-oss-fs-hadoop registers default FileSystem wrappers for URIs with the oss:// scheme.
Configurations setup # After setting up the OSS FileSystem wrapper, you need to add some configurations to make sure that Flink is allowed to access your OSS buckets.
To allow for easy adoption, you can use the same configuration keys in flink-conf.yaml as in Hadoop\u0026rsquo;s core-site.xml
You can see the configuration keys in the Hadoop OSS documentation.
There are some required configurations that must be added to flink-conf.yaml (Other configurations defined in Hadoop OSS documentation are advanced configurations which used by performance tuning):
fs.oss.endpoint: Aliyun OSS endpoint to connect to fs.oss.accessKeyId: Aliyun access key ID fs.oss.accessKeySecret: Aliyun access key secret An alternative CredentialsProvider can also be configured in the flink-conf.yaml, e.g.
# Read Credentials from OSS_ACCESS_KEY_ID and OSS_ACCESS_KEY_SECRET fs.oss.credentials.provider: com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider Other credential providers can be found under here.
Back to top
`}),e.add({id:104,href:"/flink/flink-docs-master/docs/dev/",title:"Application Development",section:"Docs",content:" "}),e.add({id:105,href:"/flink/flink-docs-master/docs/ops/debugging/application_profiling/",title:"Application Profiling \u0026 Debugging",section:"Debugging",content:` Application Profiling \u0026amp; Debugging # Overview of Custom Logging with Apache Flink # Each standalone JobManager, TaskManager, HistoryServer, and ZooKeeper daemon redirects stdout and stderr to a file with a .out filename suffix and writes internal logging to a file with a .log suffix. Java options configured by the user in env.java.opts, env.java.opts.jobmanager, env.java.opts.taskmanager, env.java.opts.historyserver and env.java.opts.client can likewise define log files with use of the script variable FLINK_LOG_PREFIX and by enclosing the options in double quotes for late evaluation. Log files using FLINK_LOG_PREFIX are rotated along with the default .out and .log files.
Profiling with Java Flight Recorder # Java Flight Recorder is a profiling and event collection framework built into the Oracle JDK. Java Mission Control is an advanced set of tools that enables efficient and detailed analysis of the extensive of data collected by Java Flight Recorder. Example configuration:
env.java.opts: \u0026#34;-XX:+UnlockCommercialFeatures -XX:+UnlockDiagnosticVMOptions -XX:+FlightRecorder -XX:+DebugNonSafepoints -XX:FlightRecorderOptions=defaultrecording=true,dumponexit=true,dumponexitpath=\${FLINK_LOG_PREFIX}.jfr\u0026#34; Profiling with JITWatch # JITWatch is a log analyser and visualizer for the Java HotSpot JIT compiler used to inspect inlining decisions, hot methods, bytecode, and assembly. Example configuration:
env.java.opts: \u0026#34;-XX:+UnlockDiagnosticVMOptions -XX:+TraceClassLoading -XX:+LogCompilation -XX:LogFile=\${FLINK_LOG_PREFIX}.jit -XX:+PrintAssembly\u0026#34; Analyzing Out of Memory Problems # If you encounter OutOfMemoryExceptions with your Flink application, then it is a good idea to enable heap dumps on out of memory errors.
env.java.opts: \u0026#34;-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=\${FLINK_LOG_PREFIX}.hprof\u0026#34; The heap dump will allow you to analyze potential memory leaks in your user code. If the memory leak should be caused by Flink, then please reach out to the dev mailing list.
Analyzing Memory \u0026amp; Garbage Collection Behaviour # Memory usage and garbage collection can have a profound impact on your application. The effects can range from slight performance degradation to a complete cluster failure if the GC pauses are too long. If you want to better understand the memory and GC behaviour of your application, then you can enable memory logging on the TaskManagers.
taskmanager.debug.memory.log: true taskmanager.debug.memory.log-interval: 10000 // 10s interval If you are interested in more detailed GC statistics, then you can activate the JVM\u0026rsquo;s GC logging via:
env.java.opts: \u0026#34;-Xloggc:\${FLINK_LOG_PREFIX}.gc.log -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+PrintPromotionFailure -XX:+PrintGCCause\u0026#34; Back to top
`}),e.add({id:106,href:"/flink/flink-docs-master/docs/connectors/dataset/formats/avro/",title:"Avro",section:"Formats",content:` Avro format # Flink has built-in support for Apache Avro. This allows to easily read and write Avro data based on an Avro schema with Flink. The serialization framework of Flink is able to handle classes generated from Avro schemas. In order to use the Avro format the following dependencies are required for projects using a build automation tool (such as Maven or SBT).
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; In order to read data from an Avro file, you have to specify an AvroInputFormat.
Example:
AvroInputFormat\u0026lt;User\u0026gt; users = new AvroInputFormat\u0026lt;User\u0026gt;(in, User.class); DataSet\u0026lt;User\u0026gt; usersDS = env.createInput(users); Note that User is a POJO generated by Avro. Flink also allows to perform string-based key selection of these POJOs. For example:
usersDS.keyBy(\u0026#34;name\u0026#34;); Note that using the GenericData.Record type is possible with Flink, but not recommended. Since the record contains the full schema, its very data intensive and thus probably slow to use.
Flink\u0026rsquo;s POJO field selection also works with POJOs generated from Avro. However, the usage is only possible if the field types are written correctly to the generated class. If a field is of type Object you can not use the field as a join or grouping key. Specifying a field in Avro like this {\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot;}, works fine, however specifying it as a UNION-type with only one field ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;double\u0026quot;]},) will generate a field of type Object. Note that specifying nullable types ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;null\u0026quot;, \u0026quot;double\u0026quot;]},) is possible!
`}),e.add({id:107,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/avro/",title:"Avro",section:"Formats",content:` Avro format # Flink has built-in support for Apache Avro. This allows to easily read and write Avro data based on an Avro schema with Flink. The serialization framework of Flink is able to handle classes generated from Avro schemas. In order to use the Avro format the following dependencies are required for projects using a build automation tool (such as Maven or SBT).
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; In order to use the Avro format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. In order to read data from an Avro file, you have to specify an AvroInputFormat.
Example:
AvroInputFormat\u0026lt;User\u0026gt; users = new AvroInputFormat\u0026lt;User\u0026gt;(in, User.class); DataStream\u0026lt;User\u0026gt; usersDS = env.createInput(users); Note that User is a POJO generated by Avro. Flink also allows to perform string-based key selection of these POJOs. For example:
usersDS.keyBy(\u0026#34;name\u0026#34;); Note that using the GenericData.Record type is possible with Flink, but not recommended. Since the record contains the full schema, its very data intensive and thus probably slow to use.
Flink\u0026rsquo;s POJO field selection also works with POJOs generated from Avro. However, the usage is only possible if the field types are written correctly to the generated class. If a field is of type Object you can not use the field as a join or grouping key. Specifying a field in Avro like this {\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot;}, works fine, however specifying it as a UNION-type with only one field ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;double\u0026quot;]},) will generate a field of type Object. Note that specifying nullable types ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;null\u0026quot;, \u0026quot;double\u0026quot;]},) is possible!
For Python jobs, an Avro schema should be defined to read from Avro files, and the elements will be vanilla Python objects. For example:
schema = AvroSchema.parse_string(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;int\u0026#34;, \u0026#34;null\u0026#34;]}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteColor\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;string\u0026#34;, \u0026#34;null\u0026#34;]} ] } \u0026#34;\u0026#34;\u0026#34;) env = StreamExecutionEnvironment.get_execution_environment() ds = env.create_input(AvroInputFormat(AVRO_FILE_PATH, schema)) def json_dumps(record): import json return json.dumps(record) ds.map(json_dumps).print() `}),e.add({id:108,href:"/flink/flink-docs-master/docs/connectors/table/formats/avro/",title:"Avro",section:"Formats",content:` Avro Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Avro format allows to read and write Avro data based on an Avro schema. Currently, the Avro schema is derived from table schema.
Dependencies # In order to use the Avro format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a table with Avro format # Here is an example to create a table using Kafka connector and Avro format.
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'avro'. avro.codec optional yes (none) String For Filesystem only, the compression codec for avro. Snappy compression as default. The valid enumerations are: null, deflate, snappy, bzip2, xz. Data Type Mapping # Currently, the Avro schema is always derived from table schema. Explicitly defining an Avro schema is not supported yet. So the following table lists the type mapping from Flink type to Avro type.
Flink SQL type Avro type Avro logical type CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY bytes DECIMAL fixed decimal TINYINT int SMALLINT int INT int BIGINT long FLOAT float DOUBLE double DATE int date TIME int time-millis TIMESTAMP long timestamp-millis ARRAY array MAP
(key must be string/char/varchar type) map MULTISET
(element must be string/char/varchar type) map ROW record In addition to the types listed above, Flink supports reading/writing nullable types. Flink maps nullable types to Avro union(something, null), where something is the Avro type converted from Flink type.
You can refer to Avro Specification for more information about Avro types.
`}),e.add({id:109,href:"/flink/flink-docs-master/docs/deployment/filesystems/azure/",title:"Azure Blob Storage",section:"File Systems",content:` Azure Blob Storage # Azure Blob Storage is a Microsoft-managed service providing cloud storage for a variety of use cases. You can use Azure Blob Storage with Flink for reading and writing data as well in conjunction with the streaming state backends
Flink supports accessing Azure Blob Storage using both wasb:// or abfs://.
Azure recommends using abfs:// for accessing ADLS Gen2 storage accounts even though wasb:// works through backward compatibility. abfs:// can be used for accessing the ADLS Gen2 storage accounts only. Please visit Azure documentation on how to identify ADLS Gen2 storage account. You can use Azure Blob Storage objects like regular files by specifying paths in the following format:
// WASB unencrypted access wasb://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt; // WASB SSL encrypted access wasbs://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt; // ABFS unecrypted access abfs://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.dfs.core.windows.net/\u0026lt;object-path\u0026gt; // ABFS SSL encrypted access abfss://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.dfs.core.windows.net/\u0026lt;object-path\u0026gt; See below for how to use Azure Blob Storage in a Flink job:
// Read from Azure Blob storage env.readTextFile(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;); // Write to Azure Blob storage stream.writeAsText(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;); // Use Azure Blob Storage as checkpoint storage env.getCheckpointConfig().setCheckpointStorage(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;); Shaded Hadoop Azure Blob Storage file system # To use flink-azure-fs-hadoop, copy the respective JAR file from the opt directory to the plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/azure-fs-hadoop cp ./opt/flink-azure-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/azure-fs-hadoop/ flink-azure-fs-hadoop registers default FileSystem wrappers for URIs with the wasb:// and wasbs:// (SSL encrypted access) scheme.
Credentials Configuration # WASB # Hadoop\u0026rsquo;s WASB Azure Filesystem supports configuration of credentials via the Hadoop configuration as outlined in the Hadoop Azure Blob Storage documentation. For convenience Flink forwards all Flink configurations with a key prefix of fs.azure to the Hadoop configuration of the filesystem. Consequently, the azure blob storage key can be configured in flink-conf.yaml via:
fs.azure.account.key.\u0026lt;account_name\u0026gt;.blob.core.windows.net: \u0026lt;azure_storage_key\u0026gt; Alternatively, the filesystem can be configured to read the Azure Blob Storage key from an environment variable AZURE_STORAGE_KEY by setting the following configuration keys in flink-conf.yaml.
fs.azure.account.keyprovider.\u0026lt;account_name\u0026gt;.blob.core.windows.net: org.apache.flink.fs.azurefs.EnvironmentVariableKeyProvider ABFS # Hadoop\u0026rsquo;s ABFS Azure Filesystem supports several ways of configuring authentication. Please visit the Hadoop ABFS documentation documentation on how to configure.
Azure recommends using Azure managed identity to access the ADLS Gen2 storage accounts using abfs. Please refer to Azure managed identities documentation for more details.
Please visit the page for the list of services that support Managed Identities. Flink clusters deployed in those Azure services can take advantage of Managed Identities.
Accessing ABFS using storage Keys (Discouraged) # Azure blob storage key can be configured in flink-conf.yaml via:
fs.azure.account.key.\u0026lt;account_name\u0026gt;.dfs.core.windows.net: \u0026lt;azure_storage_key\u0026gt; Back to top
`}),e.add({id:110,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/azure_table_storage/",title:"Azure Table storage",section:"Formats",content:` Azure Table Storage # This example is using the HadoopInputFormat wrapper to use an existing Hadoop input format implementation for accessing Azure\u0026rsquo;s Table Storage.
Download and compile the azure-tables-hadoop project. The input format developed by the project is not yet available in Maven Central, therefore, we have to build the project ourselves. Execute the following commands: git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install Setup a new Flink project using the quickstarts: curl https://flink.apache.org/q/quickstart.sh | bash Add the following dependencies (in the \u0026lt;dependencies\u0026gt; section) to your pom.xml file: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;microsoft-hadoop-azure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; flink-hadoop-compatibility is a Flink package that provides the Hadoop input format wrappers. microsoft-hadoop-azure is adding the project we\u0026rsquo;ve build before to our project.
The project is now ready for starting to code. We recommend to import the project into an IDE, such as IntelliJ. You should import it as a Maven project. Browse to the file Job.java. This is an empty skeleton for a Flink job.
Paste the following code:
import java.util.Map; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataStream; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import com.microsoft.hadoop.azure.AzureTableConfiguration; import com.microsoft.hadoop.azure.AzureTableInputFormat; import com.microsoft.hadoop.azure.WritableEntity; import com.microsoft.windowsazure.storage.table.EntityProperty; public class AzureTableExample { public static void main(String[] args) throws Exception { // set up the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); // create a AzureTableInputFormat, using a Hadoop input format wrapper HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt; hdIf = new HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt;(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job()); // set the Account URI, something like: https://apacheflink.table.core.windows.net hdIf.getConfiguration().set(azuretableconfiguration.Keys.ACCOUNT_URI.getKey(), \u0026#34;TODO\u0026#34;); // set the secret storage key here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), \u0026#34;TODO\u0026#34;); // set the table name here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), \u0026#34;TODO\u0026#34;); DataStream\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt; input = env.createInput(hdIf); // a little example how to use the data in a mapper. DataStream\u0026lt;String\u0026gt; fin = input.map(new MapFunction\u0026lt;Tuple2\u0026lt;Text,WritableEntity\u0026gt;, String\u0026gt;() { @Override public String map(Tuple2\u0026lt;Text, WritableEntity\u0026gt; arg0) throws Exception { System.err.println(\u0026#34;--------------------------------\\nKey = \u0026#34;+arg0.f0); WritableEntity we = arg0.f1; for(Map.Entry\u0026lt;String, EntityProperty\u0026gt; prop : we.getProperties().entrySet()) { System.err.println(\u0026#34;key=\u0026#34;+prop.getKey() + \u0026#34; ; value (asString)=\u0026#34;+prop.getValue().getValueAsString()); } return arg0.f0.toString(); } }); // emit result (this works only locally) fin.print(); // execute program env.execute(\u0026#34;Azure Example\u0026#34;); } } The example shows how to access an Azure table and turn data into Flink\u0026rsquo;s DataStream (more specifically, the type of the set is DataStream\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt;). With the DataStream, you can apply all known transformations to the DataStream.
Back to top
`}),e.add({id:111,href:"/flink/flink-docs-master/docs/ops/batch/batch_shuffle/",title:"Batch Shuffle",section:"Batch",content:` Batch Shuffle # Overview # Flink supports a batch execution mode in both DataStream API and Table / SQL for jobs executing across bounded input. In batch execution mode, Flink offers two modes for network exchanges: Blocking Shuffle and Hybrid Shuffle.
Blocking Shuffle is the default data exchange mode for batch executions. It persists all intermediate data, and can be consumed only after fully produced. Hybrid Shuffle is the next generation data exchange mode for batch executions. It persists data more smartly, and allows consuming while being produced. This feature is still experimental and has some known limitations. Blocking Shuffle # Unlike the pipeline shuffle used for streaming applications, blocking exchanges persists data to some storage. Downstream tasks then fetch these values via the network. Such an exchange reduces the resources required to execute the job as it does not need the upstream and downstream tasks to run simultaneously.
As a whole, Flink provides two different types of blocking shuffles: Hash shuffle and Sort shuffle.
Hash Shuffle # The default blocking shuffle implementation for 1.14 and lower, Hash Shuffle, has each upstream task persist its results in a separate file for each downstream task on the local disk of the TaskManager. When the downstream tasks run, they will request partitions from the upstream TaskManager\u0026rsquo;s, which read the files and transmit data via the network.
Hash Shuffle provides different mechanisms for writing and reading files:
file: Writes files with the normal File IO, reads and transmits files with Netty FileRegion. FileRegion relies on sendfile system call to reduce the number of data copies and memory consumption. mmap: Writes and reads files with mmap system call. auto: Writes files with the normal File IO, for file reading, it falls back to normal file option on 32 bit machine and use mmap on 64 bit machine. This is to avoid file size limitation of java mmap implementation on 32 bit machine. The different mechanism could be chosen via TaskManager configurations.
This option is experimental and might be changed future. If SSL is enabled, the file mechanism can not use FileRegion and instead uses an un-pooled buffer to cache data before transmitting. This might cause direct memory OOM. Additionally, since the synchronous file reading might block Netty threads for some time, the SSL handshake timeout needs to be increased to avoid connection reset errors. The memory usage of mmap is not accounted for by configured memory limits, but some resource frameworks like Yarn will track this memory usage and kill the container if memory exceeds some threshold. Hash Shuffle works well for small scale jobs with SSD, but it also have some disadvantages:
If the job scale is large, it might create too many files, and it requires a large write buffer to write these files at the same time. On HDD, when multiple downstream tasks fetch their data simultaneously, it might incur the issue of random IO. Sort Shuffle # Sort Shuffle is another blocking shuffle implementation introduced in version 1.13 and it becomes the default blocking shuffle implementation in 1.15. Different from Hash Shuffle, Sort Shuffle writes only one file for each result partition. When the result partition is read by multiple downstream tasks concurrently, the data file is opened only once and shared by all readers. As a result, the cluster uses fewer resources like inode and file descriptors, which improves stability. Furthermore, by writing fewer files and making a best effort to read data sequentially, Sort Shuffle can achieve better performance than Hash Shuffle, especially on HDD. Additionally, Sort Shuffle uses extra managed memory as data reading buffer and does not rely on sendfile or mmap mechanism, thus it also works well with SSL. Please refer to FLINK-19582 and FLINK-19614 for more details about Sort Shuffle.
Here are some config options that might need adjustment when using sort blocking shuffle:
taskmanager.network.sort-shuffle.min-buffers: Config option to control data writing buffer size. For large scale jobs, you may need to increase this value, usually, several hundreds of megabytes memory is enough. Because this memory is allocated from network memory, to increase this value, you may also need to increase the total network memory by adjusting taskmanager.memory.network.fraction, taskmanager.memory.network.min and taskmanager.memory.network.max to avoid the potential \u0026ldquo;Insufficient number of network buffers\u0026rdquo; error. taskmanager.memory.framework.off-heap.batch-shuffle.size: Config option to control data reading buffer size. For large scale jobs, you may need to increase this value, usually, several hundreds of megabytes memory is enough. Because this memory is cut from the framework off-heap memory, to increase this value, you need also to increase the total framework off-heap memory by adjusting taskmanager.memory.framework.off-heap.size to avoid the potential direct memory OOM error. Currently Sort Shuffle only sort records by partition index instead of the records themselves, that is to say, the sort is only used as a data clustering algorithm. Choices of Blocking Shuffle # As a summary,
For small scale jobs running on SSD, both implementation should work. For large scale jobs or for jobs running on HDD, Sort Shuffle should be more suitable. To switch between Sort Shuffle and Hash Shuffle, you need to adjust this config option: taskmanager.network.sort-shuffle.min-parallelism. It controls which shuffle implementation to use based on the parallelism of downstream tasks, if the parallelism is lower than the configured value, Hash Shuffle will be used, otherwise Sort Shuffle will be used. For versions lower than 1.15, its default value is Integer.MAX_VALUE, so Hash Shuffle will be used by default. Since 1.15, its default value is 1, so Sort Shuffle will be used by default.
Hybrid Shuffle # This feature is still experimental and has some known limitations. Hybrid shuffle is the next generation of batch data exchanges. It combines the advantages of blocking shuffle and pipelined shuffle (in streaming mode).
Like blocking shuffle, it does not require upstream and downstream tasks to run simultaneously, which allows executing a job with little resources. Like pipelined shuffle, it does not require downstream tasks to be executed after upstream tasks finish, which reduces the overall execution time of the job when given sufficient resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies. Spilling Strategy # Hybrid shuffle provides two spilling strategies:
Selective Spilling Strategy persists data only if they are not consumed by downstream tasks timely. This reduces the amount of data to persist, at the price that in case of failures upstream tasks need to be restarted to reproduce the complete intermediate results. Full Spilling Strategy persists all data, no matter they are consumed by downstream tasks or not. In case of failures, the persisted complete intermediate result can be re-consumed, without having to restart upstream tasks. Usage # To use hybrid shuffle mode, you need to configure the execution.batch-shuffle-mode to ALL_EXCHANGES_HYBRID_FULL (full spilling strategy) or ALL_EXCHANGES_HYBRID_SELECTIVE (selective spilling strategy).
Limitations # Hybrid shuffle mode is still experimental and has some known limitations, which the Flink community is still working on eliminating.
No support for Slot Sharing. In hybrid shuffle mode, Flink currently forces each task to be executed in a dedicated slot exclusively. If slot sharing is explicitly specified, an error will occur. No support for Adaptive Batch Scheduler and Speculative Execution. If adaptive batch scheduler is used in hybrid shuffle mode, an error will occur. Performance Tuning # The following guidelines may help you to achieve better performance especially for large scale batch jobs:
Blocking Shuffle Always use Sort Shuffle on HDD because Sort Shuffle can largely improve stability and IO performance. Since 1.15, Sort Shuffle is already the default blocking shuffle implementation, for 1.14 and lower version, you need to enable it manually by setting taskmanager.network.sort-shuffle.min-parallelism to 1. For both blocking shuffle implementations, you may consider enabling data compression to improve the performance unless the data is hard to compress. Since 1.15, data compression is already enabled by default, for 1.14 and lower version, you need to enable it manually. When Sort Shuffle is used, decreasing the number of exclusive buffers per channel and increasing the number of floating buffers per gate can help. For 1.14 and higher version, it is suggested to set taskmanager.network.memory.buffers-per-channel to 0 and set taskmanager.network.memory.floating-buffers-per-gate to a larger value (for example, 4096). This setting has two main advantages: 1) It decouples the network memory consumption from parallelism so for large scale jobs, the possibility of \u0026ldquo;Insufficient number of network buffers\u0026rdquo; error can be decreased; 2) Networker buffers are distributed among different channels according to needs, which can improve the network buffer utilization and further improve performance. Increase the total size of network memory. Currently, the default network memory size is pretty modest. For large scale jobs, it\u0026rsquo;s suggested to increase the total network memory fraction to at least 0.2 to achieve better performance. At the same time, you may also need to adjust the lower bound and upper bound of the network memory size, please refer to the memory configuration document for more information. Increase the memory size for shuffle data write. As mentioned in the above section, for large scale jobs, it\u0026rsquo;s suggested to increase the number of write buffers per result partition to at least (2 * parallelism) if you have enough memory. Note that you may also need to increase the total size of network memory to avoid the \u0026ldquo;Insufficient number of network buffers\u0026rdquo; error after you increase this config value. Increase the memory size for shuffle data read. As mentioned in the above section, for large scale jobs, it\u0026rsquo;s suggested to increase the size of the shared read memory to a larger value (for example, 256M or 512M). Because this memory is cut from the framework off-heap memory, you must increase taskmanager.memory.framework.off-heap.size by the same size to avoid the direct memory OOM error. Hybrid Shuffle Increase the total size of network memory. Currently, the default network memory size is pretty modest. For large scale jobs, it\u0026rsquo;s suggested to increase the total network memory fraction to at least 0.2 to achieve better performance. At the same time, you may also need to adjust the lower bound and upper bound of the network memory size, please refer to the memory configuration document for more information. Increase the memory size for shuffle data write. For large scale jobs, it\u0026rsquo;s suggested to increase the total size of network memory, the larger the memory that can be used in the shuffle write phase, the more opportunities downstream to read data directly from memory. You need to ensure that each Result Partition can be allocated to at least numSubpartition + 1 buffers, otherwise the \u0026ldquo;Insufficient number of network buffers\u0026rdquo; will be encountered. Increase the memory size for shuffle data read. For large scale jobs, it\u0026rsquo;s suggested to increase the size of the shared read memory to a larger value (for example, 256M or 512M). Because this memory is cut from the framework off-heap memory, you must increase taskmanager.memory.framework.off-heap.size by the same size to avoid the direct memory OOM error. Trouble Shooting # Here are some exceptions you may encounter (rarely) and the corresponding solutions that may help:
Blocking Shuffle Exceptions Potential Solutions Insufficient number of network buffers This means the amount of network memory is not enough to run the target job and you need to increase the total network memory size. Note that since 1.15, Sort Shuffle has become the default blocking shuffle implementation and for some cases, it may need more network memory than before, which means there is a small possibility that your batch jobs may suffer from this issue after upgrading to 1.15. If this is the case, you just need to increase the total network memory size. Too many open files This means that the file descriptors is not enough. If you are using Hash Shuffle, please switch to Sort Shuffle. If you are already using Sort Shuffle, please consider increasing the system limit for file descriptor and check if the user code consumes too many file descriptors. Connection reset by peer This usually means that the network is unstable or or under heavy burden. Other issues like SSL handshake timeout mentioned above may also cause this problem. If you are using Hash Shuffle, please switch to Sort Shuffle. If you are already using Sort Shuffle, increasing the network backlog may help. Network connection timeout This usually means that the network is unstable or under heavy burden and increasing the network connection timeout or enable connection retry may help. Socket read/write timeout This may indicate that the network is slow or under heavy burden and increasing the network send/receive buffer size may help. If the job is running in Kubernetes environment, using host network may also help. Read buffer request timeout This can happen only when you are using Sort Shuffle and it means a fierce contention of the shuffle read memory. To solve the issue, you can increase taskmanager.memory.framework.off-heap.batch-shuffle.size together with taskmanager.memory.framework.off-heap.size. No space left on device This usually means that the disk space or the inodes have been exhausted. Please consider extending the storage space or do some cleanup. Out of memory error If you are using Hash Shuffle, please switch to Sort Shuffle. If you are already using Sort Shuffle and following the above guidelines, please consider increasing the corresponding memory size. For heap memory, you can increase taskmanager.memory.task.heap.size and for direct memory, you can increase taskmanager.memory.task.off-heap.size. Container killed by external resource manger There are several reasons which can lead to the killing of a container, for example, kill a low priority container to make room for high priority container or the container uses too many resources like memory and disk space. As mentioned in the above section, Hash Shuffle may use too much memory and gets killed by YARN. So if you are using Hash Shuffle, please switch to Sort Shuffle. If Sort Shuffle is already used, you may need to check both Flink log and the external resource manager log to find out the root cause and resolve it accordingly. Hybrid Shuffle Exceptions Potential Solutions Insufficient number of network buffers This means the amount of network memory is not enough to run the target job and you need to increase the total network memory size. Connection reset by peer This usually means that the network is unstable or or under heavy burden. Other issues like SSL handshake timeout may also cause this problem. Increasing the network backlog may help. Network connection timeout This usually means that the network is unstable or under heavy burden and increasing the network connection timeout or enable connection retry may help. Socket read/write timeout This may indicate that the network is slow or under heavy burden and increasing the network send/receive buffer size may help. If the job is running in Kubernetes environment, using host network may also help. Read buffer request timeout This means a fierce contention of the shuffle read memory. To solve the issue, you can increase taskmanager.memory.framework.off-heap.batch-shuffle.size together with taskmanager.memory.framework.off-heap.size. No space left on device This usually means that the disk space or the inodes have been exhausted. Please consider extending the storage space or do some cleanup. `}),e.add({id:112,href:"/flink/flink-docs-master/docs/connectors/datastream/cassandra/",title:"Cassandra",section:"DataStream Connectors",content:` Apache Cassandra Connector # This connector provides sinks that writes data into a Apache Cassandra database.
To use this connector, add the following dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-cassandra_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here.
Installing Apache Cassandra # There are multiple ways to bring up a Cassandra instance on local machine:
Follow the instructions from Cassandra Getting Started page. Launch a container running Cassandra from Official Docker Repository Cassandra Sinks # Configurations # Flink\u0026rsquo;s Cassandra sink are created by using the static CassandraSink.addSink(DataStream input) method. This method returns a CassandraSinkBuilder, which offers methods to further configure the sink, and finally build() the sink instance.
The following configuration methods can be used:
setQuery(String query) Sets the upsert query that is executed for every record the sink receives. The query is internally treated as CQL statement. DO set the upsert query for processing Tuple data type. DO NOT set the query for processing POJO data types. setClusterBuilder(ClusterBuilder clusterBuilder) Sets the cluster builder that is used to configure the connection to cassandra with more sophisticated settings such as consistency level, retry policy and etc. setHost(String host[, int port]) Simple version of setClusterBuilder() with host/port information to connect to Cassandra instances setMapperOptions(MapperOptions options) Sets the mapper options that are used to configure the DataStax ObjectMapper. Only applies when processing POJO data types. setMaxConcurrentRequests(int maxConcurrentRequests, Duration timeout) Sets the maximum allowed number of concurrent requests with a timeout for acquiring permits to execute. Only applies when enableWriteAheadLog() is not configured. enableWriteAheadLog([CheckpointCommitter committer]) An optional setting Allows exactly-once processing for non-deterministic algorithms. setFailureHandler([CassandraFailureHandler failureHandler]) An optional setting Sets the custom failure handler. setDefaultKeyspace(String keyspace) Sets the default keyspace to be used. enableIgnoreNullFields() Enables ignoring null values, treats null values as unset and avoids writing null fields and creating tombstones. build() Finalizes the configuration and constructs the CassandraSink instance. Write-ahead Log # A checkpoint committer stores additional information about completed checkpoints in some resource. This information is used to prevent a full replay of the last completed checkpoint in case of a failure. You can use a CassandraCommitter to store these in a separate table in cassandra. Note that this table will NOT be cleaned up by Flink.
Flink can provide exactly-once guarantees if the query is idempotent (meaning it can be applied multiple times without changing the result) and checkpointing is enabled. In case of a failure the failed checkpoint will be replayed completely.
Furthermore, for non-deterministic programs the write-ahead log has to be enabled. For such a program the replayed checkpoint may be completely different than the previous attempt, which may leave the database in an inconsistent state since part of the first attempt may already be written. The write-ahead log guarantees that the replayed checkpoint is identical to the first attempt. Note that that enabling this feature will have an adverse impact on latency.
Note: The write-ahead log functionality is currently experimental. In many cases it is sufficient to use the connector without enabling it. Please report problems to the development mailing list.
Checkpointing and Fault Tolerance # With checkpointing enabled, Cassandra Sink guarantees at-least-once delivery of action requests to C* instance.
More details on checkpoints docs and fault tolerance guarantee docs
Examples # The Cassandra sink currently supports both Tuple and POJO data types, and Flink automatically detects which type of input is used. For general use of those streaming data types, please refer to Supported Data Types. We show two implementations based on SocketWindowWordCount , for POJO and Tuple data types respectively.
In all these examples, we assumed the associated Keyspace example and Table wordcount have been created.
CQL CREATE KEYSPACE IF NOT EXISTS example WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;: \u0026#39;1\u0026#39;}; CREATE TABLE IF NOT EXISTS example.wordcount ( word text, count bigint, PRIMARY KEY(word) ); Cassandra Sink Example for Streaming Tuple Data Type # While storing the result with Java/Scala Tuple data type to a Cassandra sink, it is required to set a CQL upsert statement (via setQuery(\u0026lsquo;stmt\u0026rsquo;)) to persist each record back to the database. With the upsert query cached as PreparedStatement, each Tuple element is converted to parameters of the statement.
For details about PreparedStatement and BoundStatement, please visit DataStax Java Driver manual
Java // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream\u0026lt;String\u0026gt; text = env.socketTextStream(hostname, port, \u0026#34;\\n\u0026#34;); // parse the data, group it, window it, and aggregate the counts DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; result = text .flatMap(new FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Long\u0026gt;\u0026gt;() { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) { // normalize and split the line String[] words = value.toLowerCase().split(\u0026#34;\\\\s\u0026#34;); // emit the pairs for (String word : words) { //Do not accept empty word, since word is defined as primary key in C* table if (!word.isEmpty()) { out.collect(new Tuple2\u0026lt;String, Long\u0026gt;(word, 1L)); } } } }) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); CassandraSink.addSink(result) .setQuery(\u0026#34;INSERT INTO example.wordcount(word, count) values (?, ?);\u0026#34;) .setHost(\u0026#34;127.0.0.1\u0026#34;) .build(); Scala val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // get input data by connecting to the socket val text: DataStream[String] = env.socketTextStream(hostname, port, \u0026#39;\\n\u0026#39;) // parse the data, group it, window it, and aggregate the counts val result: DataStream[(String, Long)] = text // split up the lines in pairs (2-tuples) containing: (word,1) .flatMap(_.toLowerCase.split(\u0026#34;\\\\s\u0026#34;)) .filter(_.nonEmpty) .map((_, 1L)) // group by the tuple field \u0026#34;0\u0026#34; and sum up tuple field \u0026#34;1\u0026#34; .keyBy(_._1) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1) CassandraSink.addSink(result) .setQuery(\u0026#34;INSERT INTO example.wordcount(word, count) values (?, ?);\u0026#34;) .setHost(\u0026#34;127.0.0.1\u0026#34;) .build() result.print().setParallelism(1) Cassandra Sink Example for Streaming POJO Data Type # An example of streaming a POJO data type and store the same POJO entity back to Cassandra. In addition, this POJO implementation needs to follow DataStax Java Driver Manual to annotate the class as each field of this entity is mapped to an associated column of the designated table using the DataStax Java Driver com.datastax.driver.mapping.Mapper class.
The mapping of each table column can be defined through annotations placed on a field declaration in the Pojo class. For details of the mapping, please refer to CQL documentation on Definition of Mapped Classes and CQL Data types
Java // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream\u0026lt;String\u0026gt; text = env.socketTextStream(hostname, port, \u0026#34;\\n\u0026#34;); // parse the data, group it, window it, and aggregate the counts DataStream\u0026lt;WordCount\u0026gt; result = text .flatMap(new FlatMapFunction\u0026lt;String, WordCount\u0026gt;() { public void flatMap(String value, Collector\u0026lt;WordCount\u0026gt; out) { // normalize and split the line String[] words = value.toLowerCase().split(\u0026#34;\\\\s\u0026#34;); // emit the pairs for (String word : words) { if (!word.isEmpty()) { //Do not accept empty word, since word is defined as primary key in C* table out.collect(new WordCount(word, 1L)); } } } }) .keyBy(WordCount::getWord) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .reduce(new ReduceFunction\u0026lt;WordCount\u0026gt;() { @Override public WordCount reduce(WordCount a, WordCount b) { return new WordCount(a.getWord(), a.getCount() + b.getCount()); } }); CassandraSink.addSink(result) .setHost(\u0026#34;127.0.0.1\u0026#34;) .setMapperOptions(() -\u0026gt; new Mapper.Option[]{Mapper.Option.saveNullFields(true)}) .build(); @Table(keyspace = \u0026#34;example\u0026#34;, name = \u0026#34;wordcount\u0026#34;) public class WordCount { @Column(name = \u0026#34;word\u0026#34;) private String word = \u0026#34;\u0026#34;; @Column(name = \u0026#34;count\u0026#34;) private long count = 0; public WordCount() {} public WordCount(String word, long count) { this.setWord(word); this.setCount(count); } public String getWord() { return word; } public void setWord(String word) { this.word = word; } public long getCount() { return count; } public void setCount(long count) { this.count = count; } @Override public String toString() { return getWord() + \u0026#34; : \u0026#34; + getCount(); } } Back to top
`}),e.add({id:113,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/",title:"Checkpointing",section:"State \u0026 Fault Tolerance",content:` Checkpointing # Every function and operator in Flink can be stateful (see working with state for details). Stateful functions store data across the processing of individual elements/events, making state a critical building block for any type of more elaborate operation.
In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution.
The documentation on streaming fault tolerance describes in detail the technique behind Flink\u0026rsquo;s streaming fault tolerance mechanism.
Prerequisites # Flink\u0026rsquo;s checkpointing mechanism interacts with durable storage for streams and state. In general, it requires:
A persistent (or durable) data source that can replay records for a certain amount of time. Examples for such sources are persistent messages queues (e.g., Apache Kafka, RabbitMQ, Amazon Kinesis, Google PubSub) or file systems (e.g., HDFS, S3, GFS, NFS, Ceph, \u0026hellip;). A persistent storage for state, typically a distributed filesystem (e.g., HDFS, S3, GFS, NFS, Ceph, \u0026hellip;) Enabling and Configuring Checkpointing # By default, checkpointing is disabled. To enable checkpointing, call enableCheckpointing(n) on the StreamExecutionEnvironment, where n is the checkpoint interval in milliseconds.
Other parameters for checkpointing include:
checkpoint storage: You can set the location where checkpoint snapshots are made durable. By default Flink will use the JobManager\u0026rsquo;s heap. For production deployments it is recommended to instead use a durable filesystem. See checkpoint storage for more details on the available options for job-wide and cluster-wide configuration.
exactly-once vs. at-least-once: You can optionally pass a mode to the enableCheckpointing(n) method to choose between the two guarantee levels. Exactly-once is preferable for most applications. At-least-once may be relevant for certain super-low-latency (consistently few milliseconds) applications.
checkpoint timeout: The time after which a checkpoint-in-progress is aborted, if it did not complete by then.
minimum time between checkpoints: To make sure that the streaming application makes a certain amount of progress between checkpoints, one can define how much time needs to pass between checkpoints. If this value is set for example to 5000, the next checkpoint will be started no sooner than 5 seconds after the previous checkpoint completed, regardless of the checkpoint duration and the checkpoint interval. Note that this implies that the checkpoint interval will never be smaller than this parameter.
It is often easier to configure applications by defining the \u0026ldquo;time between checkpoints\u0026rdquo; than the checkpoint interval, because the \u0026ldquo;time between checkpoints\u0026rdquo; is not susceptible to the fact that checkpoints may sometimes take longer than on average (for example if the target storage system is temporarily slow).
Note that this value also implies that the number of concurrent checkpoints is one.
tolerable checkpoint failure number: This defines how many consecutive checkpoint failures will be tolerated, before the whole job is failed over. The default value is 0, which means no checkpoint failures will be tolerated, and the job will fail on first reported checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored.
number of concurrent checkpoints: By default, the system will not trigger another checkpoint while one is still in progress. This ensures that the topology does not spend too much time on checkpoints and not make progress with processing the streams. It is possible to allow for multiple overlapping checkpoints, which is interesting for pipelines that have a certain processing delay (for example because the functions call external services that need some time to respond) but that still want to do very frequent checkpoints (100s of milliseconds) to re-process very little upon failures.
This option cannot be used when a minimum time between checkpoints is defined.
externalized checkpoints: You can configure periodic checkpoints to be persisted externally. Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the job fails. This way, you will have a checkpoint around to resume from if your job fails. There are more details in the deployment notes on externalized checkpoints.
unaligned checkpoints: You can enable unaligned checkpoints to greatly reduce checkpointing times under backpressure. This only works for exactly-once checkpoints and with one concurrent checkpoint.
checkpoints with finished tasks: By default Flink will continue performing checkpoints even if parts of the DAG have finished processing all of their records. Please refer to important considerations for details.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // start a checkpoint every 1000 ms env.enableCheckpointing(1000); // advanced options: // set mode to exactly-once (this is the default) env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // make sure 500 ms of progress happen between checkpoints env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500); // checkpoints have to complete within one minute, or are discarded env.getCheckpointConfig().setCheckpointTimeout(60000); // only two consecutive checkpoint failures are tolerated env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2); // allow only one checkpoint to be in progress at the same time env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // enable externalized checkpoints which are retained // after job cancellation env.getCheckpointConfig().setExternalizedCheckpointCleanup( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); // enables the unaligned checkpoints env.getCheckpointConfig().enableUnalignedCheckpoints(); // sets the checkpoint storage where checkpoint snapshots will be written env.getCheckpointConfig().setCheckpointStorage(\u0026#34;hdfs:///my/checkpoint/dir\u0026#34;); // enable checkpointing with finished tasks Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, true); env.configure(config); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() // start a checkpoint every 1000 ms env.enableCheckpointing(1000) // advanced options: // set mode to exactly-once (this is the default) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // make sure 500 ms of progress happen between checkpoints env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500) // checkpoints have to complete within one minute, or are discarded env.getCheckpointConfig.setCheckpointTimeout(60000) // only two consecutive checkpoint failures are tolerated env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2) // allow only one checkpoint to be in progress at the same time env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // enable externalized checkpoints which are retained // after job cancellation env.getCheckpointConfig().setExternalizedCheckpointCleanup( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) // enables the unaligned checkpoints env.getCheckpointConfig.enableUnalignedCheckpoints() // sets the checkpoint storage where checkpoint snapshots will be written env.getCheckpointConfig.setCheckpointStorage(\u0026#34;hdfs:///my/checkpoint/dir\u0026#34;) // enable checkpointing with finished tasks val config = new Configuration() config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, true) env.configure(config) Python env = StreamExecutionEnvironment.get_execution_environment() # start a checkpoint every 1000 ms env.enable_checkpointing(1000) # advanced options: # set mode to exactly-once (this is the default) env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE) # make sure 500 ms of progress happen between checkpoints env.get_checkpoint_config().set_min_pause_between_checkpoints(500) # checkpoints have to complete within one minute, or are discarded env.get_checkpoint_config().set_checkpoint_timeout(60000) # only two consecutive checkpoint failures are tolerated env.get_checkpoint_config().set_tolerable_checkpoint_failure_number(2) # allow only one checkpoint to be in progress at the same time env.get_checkpoint_config().set_max_concurrent_checkpoints(1) # enable externalized checkpoints which are retained after job cancellation env.get_checkpoint_config().enable_externalized_checkpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) # enables the unaligned checkpoints env.get_checkpoint_config().enable_unaligned_checkpoints() Related Config Options # Some more parameters and/or defaults may be set via conf/flink-conf.yaml (see configuration for a full guide):
Key Default Type Description state.backend.incremental false Boolean Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. state.backend.local-recovery false Boolean This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, the MemoryStateBackend does not support local recovery and ignores this option. state.checkpoint-storage (none) String The checkpoint storage implementation to be used to checkpoint state.
The implementation can be specified either via their shortcut name, or via the class name of a CheckpointStorageFactory. If a factory is specified it is instantiated via its zero argument constructor and its CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'jobmanager' and 'filesystem'. state.checkpoints.dir (none) String The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). state.checkpoints.num-retained 1 Integer The maximum number of completed checkpoints to retain. state.savepoints.dir (none) String The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). state.storage.fs.memory-threshold 20 kb MemorySize The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. state.storage.fs.write-buffer-size 4096 Integer The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.storage.fs.memory-threshold'. taskmanager.state.local.root-dirs (none) String The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignores this option. If not configured it will default to \u0026lt;WORKING_DIR\u0026gt;/localState. The \u0026lt;WORKING_DIR\u0026gt; can be configured via process.taskmanager.working-dir Back to top
Selecting Checkpoint Storage # Flink\u0026rsquo;s checkpointing mechanism stores consistent snapshots of all the state in timers and stateful operators, including connectors, windows, and any user-defined state. Where the checkpoints are stored (e.g., JobManager memory, file system, database) depends on the configured Checkpoint Storage.
By default, checkpoints are stored in memory in the JobManager. For proper persistence of large state, Flink supports various approaches for checkpointing state in other locations. The choice of checkpoint storage can be configured via StreamExecutionEnvironment.getCheckpointConfig().setCheckpointStorage(…). It is strongly encouraged that checkpoints be stored in a highly-available filesystem for production deployments.
See checkpoint storage for more details on the available options for job-wide and cluster-wide configuration.
State Checkpoints in Iterative Jobs # Flink currently only provides processing guarantees for jobs without iterations. Enabling checkpointing on an iterative job causes an exception. In order to force checkpointing on an iterative program the user needs to set a special flag when enabling checkpointing: env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true).
Please note that records in flight in the loop edges (and the state changes associated with them) will be lost during failure.
Checkpointing with parts of the graph finished # Starting from Flink 1.14 it is possible to continue performing checkpoints even if parts of the job graph have finished processing all data, which might happen if it contains bounded sources. This feature is enabled by default since 1.15, and it could be disabled via a feature flag:
Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, false); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config); Once the tasks/subtasks are finished, they do not contribute to the checkpoints any longer. This is an important consideration when implementing any custom operators or UDFs (User-Defined Functions).
In order to support checkpointing with tasks that finished, we adjusted the task lifecycle and introduced the StreamOperator#finish method. This method is expected to be a clear cutoff point for flushing any remaining buffered state. All checkpoints taken after the finish method has been called should be empty (in most cases) and should not contain any buffered data since there will be no way to emit this data. One notable exception is if your operator has some pointers to transactions in external systems (i.e. order to implement the exactly-once semantic). In such a case, checkpoints taken after invoking the finish() method should keep a pointer to the last transaction(s) that will be committed in the final checkpoint before the operator is closed. A good built-in example of this are exactly-once sinks and the TwoPhaseCommitSinkFunction.
How does this impact the operator state? # There is a special handling for UnionListState, which has often been used to implement a global view over offsets in an external system (i.e. storing current offsets of Kafka partitions). If we had discarded a state for a single subtask that had its close method called, we would have lost the offsets for partitions that it had been assigned. In order to work around this problem, we let checkpoints succeed only if none or all subtasks that use UnionListState are finished.
We have not seen ListState used in a similar way, but you should be aware that any state checkpointed after the close method will be discarded and not be available after a restore.
Any operator that is prepared to be rescaled should work well with tasks that partially finish. Restoring from a checkpoint where only a subset of tasks finished is equivalent to restoring such a task with the number of new subtasks equal to the number of running tasks.
Waiting for the final checkpoint before task exit # To ensure all the records could be committed for operators using the two-phase commit, the tasks would wait for the final checkpoint completed successfully after all the operators finished. It needs to be noted that this behavior would prolong the execution time of tasks. If the checkpoint interval is long, the execution time would also be prolonged largely. For the worst case, if the checkpoint interval is set to Long.MAX_VALUE, the tasks would in fact be blocked forever since the final checkpoint would never happen.
Back to top
`}),e.add({id:114,href:"/flink/flink-docs-master/docs/connectors/table/formats/avro-confluent/",title:"Confluent Avro",section:"Formats",content:` Confluent Avro Format # Format: Serialization Schema Format: Deserialization Schema
The Avro Schema Registry (avro-confluent) format allows you to read records that were serialized by the io.confluent.kafka.serializers.KafkaAvroSerializer and to write records that can in turn be read by the io.confluent.kafka.serializers.KafkaAvroDeserializer.
When reading (deserializing) a record with this format the Avro writer schema is fetched from the configured Confluent Schema Registry based on the schema version id encoded in the record while the reader schema is inferred from table schema.
When writing (serializing) a record with this format the Avro schema is inferred from the table schema and used to retrieve a schema id to be encoded with the data. The lookup is performed with in the configured Confluent Schema Registry under the subject given in avro-confluent.subject.
The Avro Schema Registry format can only be used in conjunction with the Apache Kafka SQL connector or the Upsert Kafka SQL Connector.
Dependencies # In order to use the Avro Schema Registry format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-confluent-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. For Maven, SBT, Gradle, or other build automation tools, please also ensure that Confluent\u0026rsquo;s maven repository at https://packages.confluent.io/maven/ is configured in your project\u0026rsquo;s build files.
How to create tables with Avro-Confluent format # Example of a table using raw UTF-8 string as Kafka key and Avro records registered in the Schema Registry as Kafka values:
CREATE TABLE user_created ( -- one column mapped to the Kafka raw UTF-8 key the_kafka_key STRING, -- a few columns mapped to the Avro fields of the Kafka value id STRING, name STRING, email STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example1\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- UTF-8 string as Kafka keys, using the \u0026#39;the_kafka_key\u0026#39; table column \u0026#39;key.format\u0026#39; = \u0026#39;raw\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;the_kafka_key\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) We can write data into the kafka table as follows:
INSERT INTO user_created SELECT -- replicating the user id into a column mapped to the kafka key id as the_kafka_key, -- all values id, name, email FROM some_table Example of a table with both the Kafka key and value registered as Avro records in the Schema Registry:
CREATE TABLE user_created ( -- one column mapped to the \u0026#39;id\u0026#39; Avro field of the Kafka key kafka_key_id STRING, -- a few columns mapped to the Avro fields of the Kafka value id STRING, name STRING, email STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example2\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- Watch out: schema evolution in the context of a Kafka key is almost never backward nor -- forward compatible due to hash partitioning. \u0026#39;key.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;key.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;kafka_key_id\u0026#39;, -- In this example, we want the Avro types of both the Kafka key and value to contain the field \u0026#39;id\u0026#39; -- =\u0026gt; adding a prefix to the table column associated to the Kafka key field avoids clashes \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;kafka_key_\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39;, -- subjects have a default value since Flink 1.13, though can be overridden: \u0026#39;key.avro-confluent.subject\u0026#39; = \u0026#39;user_events_example2-key2\u0026#39;, \u0026#39;value.avro-confluent.subject\u0026#39; = \u0026#39;user_events_example2-value2\u0026#39; ) Example of a table using the upsert-kafka connector with the Kafka value registered as an Avro record in the Schema Registry:
CREATE TABLE user_created ( -- one column mapped to the Kafka raw UTF-8 key kafka_key_id STRING, -- a few columns mapped to the Avro fields of the Kafka value id STRING, name STRING, email STRING, -- upsert-kafka connector requires a primary key to define the upsert behavior PRIMARY KEY (kafka_key_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example3\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- UTF-8 string as Kafka keys -- We don\u0026#39;t specify \u0026#39;key.fields\u0026#39; in this case since it\u0026#39;s dictated by the primary key of the table \u0026#39;key.format\u0026#39; = \u0026#39;raw\u0026#39;, -- In this example, we want the Avro types of both the Kafka key and value to contain the field \u0026#39;id\u0026#39; -- =\u0026gt; adding a prefix to the table column associated to the kafka key field avoids clashes \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;kafka_key_\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'avro-confluent'. avro-confluent.basic-auth.credentials-source optional yes (none) String Basic auth credentials source for Schema Registry avro-confluent.basic-auth.user-info optional yes (none) String Basic auth user info for schema registry avro-confluent.bearer-auth.credentials-source optional yes (none) String Bearer auth credentials source for Schema Registry avro-confluent.bearer-auth.token optional yes (none) String Bearer auth token for Schema Registry avro-confluent.properties optional yes (none) Map Properties map that is forwarded to the underlying Schema Registry. This is useful for options that are not officially exposed via Flink config options. However, note that Flink options have higher precedence. avro-confluent.ssl.keystore.location optional yes (none) String Location / File of SSL keystore avro-confluent.ssl.keystore.password optional yes (none) String Password for SSL keystore avro-confluent.ssl.truststore.location optional yes (none) String Location / File of SSL truststore avro-confluent.ssl.truststore.password optional yes (none) String Password for SSL truststore avro-confluent.subject optional yes (none) String The Confluent Schema Registry subject under which to register the schema used by this format during serialization. By default, 'kafka' and 'upsert-kafka' connectors use '\u0026lt;topic_name\u0026gt;-value' or '\u0026lt;topic_name\u0026gt;-key' as the default subject name if this format is used as the value or key format. But for other connectors (e.g. 'filesystem'), the subject option is required when used as sink. avro-confluent.url required yes (none) String The URL of the Confluent Schema Registry to fetch/register schemas. Data Type Mapping # Currently, Apache Flink always uses the table schema to derive the Avro reader schema during deserialization and Avro writer schema during serialization. Explicitly defining an Avro schema is not supported yet. See the Apache Avro Format for the mapping between Avro and Flink DataTypes.
In addition to the types listed there, Flink supports reading/writing nullable types. Flink maps nullable types to Avro union(something, null), where something is the Avro type converted from Flink type.
You can refer to Avro Specification for more information about Avro types.
`}),e.add({id:115,href:"/flink/flink-docs-master/docs/dev/table/sql/create/",title:"CREATE Statements",section:"SQL",content:` CREATE Statements # CREATE statements are used to register a table/view/function into current or specified Catalog. A registered table/view/function can be used in SQL queries.
Flink SQL supports the following CREATE statements for now:
CREATE TABLE CREATE CATALOG CREATE DATABASE CREATE VIEW CREATE FUNCTION Run a CREATE statement # Java CREATE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful CREATE operation, otherwise will throw an exception.
The following examples show how to run a CREATE statement in TableEnvironment.
Scala CREATE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful CREATE operation, otherwise will throw an exception.
The following examples show how to run a CREATE statement in TableEnvironment.
Python CREATE statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful CREATE operation, otherwise will throw an exception.
The following examples show how to run a CREATE statement in TableEnvironment.
SQL CLI CREATE statements can be executed in SQL CLI.
The following examples show how to run a CREATE statement in SQL CLI.
Java TableEnvironment tableEnv = TableEnvironment.create(...); // SQL query with a registered table // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // run a SQL query on the Table and retrieve the result as a new Table Table result = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // Execute insert SQL with a registered table // register a TableSink tableEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;); // run an insert SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); Scala val tableEnv = TableEnvironment.create(...) // SQL query with a registered table // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // run a SQL query on the Table and retrieve the result as a new Table val result = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // Execute insert SQL with a registered table // register a TableSink tableEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (\u0026#39;connector.path\u0026#39;=\u0026#39;/path/to/file\u0026#39; ...)\u0026#34;) // run an insert SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Python table_env = TableEnvironment.create(...) # SQL query with a registered table # register a table named \u0026#34;Orders\u0026#34; table_env.execute_sql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); # run a SQL query on the Table and retrieve the result as a new Table result = table_env.sql_query( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); # Execute an INSERT SQL with a registered table # register a TableSink table_env.execute_sql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) # run an INSERT SQL on the Table and emit the result to the TableSink table_env \\ .execute_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders (product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... Back to top
CREATE TABLE # The following grammar gives an overview about the available syntax:
CREATE TABLE [IF NOT EXISTS] [catalog_name.][db_name.]table_name ( { \u0026lt;physical_column_definition\u0026gt; | \u0026lt;metadata_column_definition\u0026gt; | \u0026lt;computed_column_definition\u0026gt; }[ , ...n] [ \u0026lt;watermark_definition\u0026gt; ] [ \u0026lt;table_constraint\u0026gt; ][ , ...n] ) [COMMENT table_comment] [PARTITIONED BY (partition_column_name1, partition_column_name2, ...)] WITH (key1=val1, key2=val2, ...) [ LIKE source_table [( \u0026lt;like_options\u0026gt; )] ] \u0026lt;physical_column_definition\u0026gt;: column_name column_type [ \u0026lt;column_constraint\u0026gt; ] [COMMENT column_comment] \u0026lt;column_constraint\u0026gt;: [CONSTRAINT constraint_name] PRIMARY KEY NOT ENFORCED \u0026lt;table_constraint\u0026gt;: [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED \u0026lt;metadata_column_definition\u0026gt;: column_name column_type METADATA [ FROM metadata_key ] [ VIRTUAL ] \u0026lt;computed_column_definition\u0026gt;: column_name AS computed_column_expression [COMMENT column_comment] \u0026lt;watermark_definition\u0026gt;: WATERMARK FOR rowtime_column_name AS watermark_strategy_expression \u0026lt;source_table\u0026gt;: [catalog_name.][db_name.]table_name \u0026lt;like_options\u0026gt;: { { INCLUDING | EXCLUDING } { ALL | CONSTRAINTS | PARTITIONS } | { INCLUDING | EXCLUDING | OVERWRITING } { GENERATED | OPTIONS | WATERMARKS } }[, ...] The statement above creates a table with the given name. If a table with the same name already exists in the catalog, an exception is thrown.
Columns # Physical / Regular Columns
Physical columns are regular columns known from databases. They define the names, the types, and the order of fields in the physical data. Thus, physical columns represent the payload that is read from and written to an external system. Connectors and formats use these columns (in the defined order) to configure themselves. Other kinds of columns can be declared between physical columns but will not influence the final physical schema.
The following statement creates a table with only regular columns:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING ) WITH ( ... ); Metadata Columns
Metadata columns are an extension to the SQL standard and allow to access connector and/or format specific fields for every row of a table. A metadata column is indicated by the METADATA keyword. For example, a metadata column can be be used to read and write the timestamp from and to Kafka records for time-based operations. The connector and format documentation lists the available metadata fields for every component. However, declaring a metadata column in a table\u0026rsquo;s schema is optional.
The following statement creates a table with an additional metadata column that references the metadata field timestamp:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`record_time\` TIMESTAMP_LTZ(3) METADATA FROM \u0026#39;timestamp\u0026#39; -- reads and writes a Kafka record\u0026#39;s timestamp ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); Every metadata field is identified by a string-based key and has a documented data type. For example, the Kafka connector exposes a metadata field with key timestamp and data type TIMESTAMP_LTZ(3) that can be used for both reading and writing records.
In the example above, the metadata column record_time becomes part of the table\u0026rsquo;s schema and can be transformed and stored like a regular column:
INSERT INTO MyTable SELECT user_id, name, record_time + INTERVAL \u0026#39;1\u0026#39; SECOND FROM MyTable; For convenience, the FROM clause can be omitted if the column name should be used as the identifying metadata key:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`timestamp\` TIMESTAMP_LTZ(3) METADATA -- use column name as metadata key ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); For convenience, the runtime will perform an explicit cast if the data type of the column differs from the data type of the metadata field. Of course, this requires that the two data types are compatible.
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`timestamp\` BIGINT METADATA -- cast the timestamp as BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); By default, the planner assumes that a metadata column can be used for both reading and writing. However, in many cases an external system provides more read-only metadata fields than writable fields. Therefore, it is possible to exclude metadata columns from persisting using the VIRTUAL keyword.
CREATE TABLE MyTable ( \`timestamp\` BIGINT METADATA, -- part of the query-to-sink schema \`offset\` BIGINT METADATA VIRTUAL, -- not part of the query-to-sink schema \`user_id\` BIGINT, \`name\` STRING, ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); In the example above, the offset is a read-only metadata column and excluded from the query-to-sink schema. Thus, source-to-query schema (for SELECT) and query-to-sink (for INSERT INTO) schema differ:
source-to-query schema: MyTable(\`timestamp\` BIGINT, \`offset\` BIGINT, \`user_id\` BIGINT, \`name\` STRING) query-to-sink schema: MyTable(\`timestamp\` BIGINT, \`user_id\` BIGINT, \`name\` STRING) Computed Columns
Computed columns are virtual columns that are generated using the syntax column_name AS computed_column_expression.
A computed column evaluates an expression that can reference other columns declared in the same table. Both physical columns and metadata columns can be accessed. The column itself is not physically stored within the table. The column\u0026rsquo;s data type is derived automatically from the given expression and does not have to be declared manually.
The planner will transform computed columns into a regular projection after the source. For optimization or watermark strategy push down, the evaluation might be spread across operators, performed multiple times, or skipped if not needed for the given query.
For example, a computed column could be defined as:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE, \`cost\` AS price * quanitity, -- evaluate expression and supply the result to queries ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); The expression may contain any combination of columns, constants, or functions. The expression cannot contain a subquery.
Computed columns are commonly used in Flink for defining time attributes in CREATE TABLE statements.
A processing time attribute can be defined easily via proc AS PROCTIME() using the system\u0026rsquo;s PROCTIME() function. An event time attribute timestamp can be pre-processed before the WATERMARK declaration. For example, the computed column can be used if the original field is not TIMESTAMP(3) type or is nested in a JSON string. Similar to virtual metadata columns, computed columns are excluded from persisting. Therefore, a computed column cannot be the target of an INSERT INTO statement. Thus, source-to-query schema (for SELECT) and query-to-sink (for INSERT INTO) schema differ:
source-to-query schema: MyTable(\`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE, \`cost\` DOUBLE) query-to-sink schema: MyTable(\`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE) WATERMARK # The WATERMARK clause defines the event time attributes of a table and takes the form WATERMARK FOR rowtime_column_name AS watermark_strategy_expression.
The rowtime_column_name defines an existing column that is marked as the event time attribute of the table. The column must be of type TIMESTAMP(3) and be a top-level column in the schema. It may be a computed column.
The watermark_strategy_expression defines the watermark generation strategy. It allows arbitrary non-query expression, including computed columns, to calculate the watermark. The expression return type must be TIMESTAMP(3), which represents the timestamp since the Epoch. The returned watermark will be emitted only if it is non-null and its value is larger than the previously emitted local watermark (to preserve the contract of ascending watermarks). The watermark generation expression is evaluated by the framework for every record. The framework will periodically emit the largest generated watermark. If the current watermark is still identical to the previous one, or is null, or the value of the returned watermark is smaller than that of the last emitted one, then no new watermark will be emitted. Watermark is emitted in an interval defined by pipeline.auto-watermark-interval configuration. If watermark interval is 0ms, the generated watermarks will be emitted per-record if it is not null and greater than the last emitted one.
When using event time semantics, tables must contain an event time attribute and watermarking strategy.
Flink provides several commonly used watermark strategies.
Strictly ascending timestamps: WATERMARK FOR rowtime_column AS rowtime_column.
Emits a watermark of the maximum observed timestamp so far. Rows that have a timestamp bigger to the max timestamp are not late.
Ascending timestamps: WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND.
Emits a watermark of the maximum observed timestamp so far minus 1. Rows that have a timestamp bigger or equal to the max timestamp are not late.
Bounded out of orderness timestamps: WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL 'string' timeUnit.
Emits watermarks, which are the maximum observed timestamp minus the specified delay, e.g., WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '5' SECOND is a 5 seconds delayed watermark strategy.
CREATE TABLE Orders ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( . . . ); PRIMARY KEY # Primary key constraint is a hint for Flink to leverage for optimizations. It tells that a column or a set of columns of a table or a view are unique and they do not contain null. Neither of columns in a primary can be nullable. Primary key therefore uniquely identify a row in a table.
Primary key constraint can be either declared along with a column definition (a column constraint) or as a single line (a table constraint). For both cases, it should only be declared as a singleton. If you define multiple primary key constraints at the same time, an exception would be thrown.
Validity Check
SQL standard specifies that a constraint can either be ENFORCED or NOT ENFORCED. This controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data therefore the only mode we want to support is the NOT ENFORCED mode. It is up to the user to ensure that the query enforces key integrity.
Flink will assume correctness of the primary key by assuming that the columns nullability is aligned with the columns in primary key. Connectors should ensure those are aligned.
Notes: In a CREATE TABLE statement, creating a primary key constraint will alter the columns nullability, that means, a column with primary key constraint is not nullable.
PARTITIONED BY # Partition the created table by the specified columns. A directory is created for each partition if this table is used as a filesystem sink.
WITH Options # Table properties used to create a table source/sink. The properties are usually used to find and create the underlying connector.
The key and value of expression key1=val1 should both be string literal. See details in Connect to External Systems for all the supported table properties of different connectors.
Notes: The table name can be of three formats: 1. catalog_name.db_name.table_name 2. db_name.table_name 3. table_name. For catalog_name.db_name.table_name, the table would be registered into metastore with catalog named \u0026ldquo;catalog_name\u0026rdquo; and database named \u0026ldquo;db_name\u0026rdquo;; for db_name.table_name, the table would be registered into the current catalog of the execution table environment and database named \u0026ldquo;db_name\u0026rdquo;; for table_name, the table would be registered into the current catalog and database of the execution table environment.
Notes: The table registered with CREATE TABLE statement can be used as both table source and table sink, we can not decide if it is used as a source or sink until it is referenced in the DMLs.
LIKE # The LIKE clause is a variant/combination of SQL features (Feature T171, “LIKE clause in table definition” and Feature T173, “Extended LIKE clause in table definition”). The clause can be used to create a table based on a definition of an existing table. Additionally, users can extend the original table or exclude certain parts of it. In contrast to the SQL standard the clause must be defined at the top-level of a CREATE statement. That is because the clause applies to multiple parts of the definition and not only to the schema part.
You can use the clause to reuse (and potentially overwrite) certain connector properties or add watermarks to tables defined externally. For example, you can add a watermark to a table defined in Apache Hive.
Consider the example statement below:
CREATE TABLE Orders ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39; ); CREATE TABLE Orders_with_watermark ( -- Add watermark definition WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( -- Overwrite the startup-mode \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39; ) LIKE Orders; The resulting table Orders_with_watermark will be equivalent to a table created with a following statement:
CREATE TABLE Orders_with_watermark ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39; ); The merging logic of table features can be controlled with like options.
You can control the merging behavior of:
CONSTRAINTS - constraints such as primary and unique keys GENERATED - computed columns METADATA - metadata columns OPTIONS - connector options that describe connector and format properties PARTITIONS - partition of the tables WATERMARKS - watermark declarations with three different merging strategies:
INCLUDING - Includes the feature of the source table, fails on duplicate entries, e.g. if an option with the same key exists in both tables. EXCLUDING - Does not include the given feature of the source table. OVERWRITING - Includes the feature of the source table, overwrites duplicate entries of the source table with properties of the new table, e.g. if an option with the same key exists in both tables, the one from the current statement will be used. Additionally, you can use the INCLUDING/EXCLUDING ALL option to specify what should be the strategy if there was no specific strategy defined, i.e. if you use EXCLUDING ALL INCLUDING WATERMARKS only the watermarks will be included from the source table.
Example:
-- A source table stored in a filesystem CREATE TABLE Orders_in_file ( \`user\` BIGINT, product STRING, order_time_string STRING, order_time AS to_timestamp(order_time) ) PARTITIONED BY (\`user\`) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;...\u0026#39; ); -- A corresponding table we want to store in kafka CREATE TABLE Orders_in_kafka ( -- Add watermark definition WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... ) LIKE Orders_in_file ( -- Exclude everything besides the computed columns which we need to generate the watermark for. -- We do not want to have the partitions or filesystem options as those do not apply to kafka. EXCLUDING ALL INCLUDING GENERATED ); If you provide no like options, INCLUDING ALL OVERWRITING OPTIONS will be used as a default.
NOTE You cannot control the behavior of merging physical columns. Those will be merged as if you applied the INCLUDING strategy.
NOTE The source_table can be a compound identifier. Thus, it can be a table from a different catalog or database: e.g. my_catalog.my_db.MyTable specifies table MyTable from catalog MyCatalog and database my_db; my_db.MyTable specifies table MyTable from current catalog and database my_db.
Back to top
CREATE CATALOG # CREATE CATALOG catalog_name WITH (key1=val1, key2=val2, ...) Create a catalog with the given catalog properties. If a catalog with the same name already exists, an exception is thrown.
WITH OPTIONS
Catalog properties used to store extra information related to this catalog. The key and value of expression key1=val1 should both be string literal.
Check out more details at Catalogs.
Back to top
CREATE DATABASE # CREATE DATABASE [IF NOT EXISTS] [catalog_name.]db_name [COMMENT database_comment] WITH (key1=val1, key2=val2, ...) Create a database with the given database properties. If a database with the same name already exists in the catalog, an exception is thrown.
IF NOT EXISTS
If the database already exists, nothing happens.
WITH OPTIONS
Database properties used to store extra information related to this database. The key and value of expression key1=val1 should both be string literal.
Back to top
CREATE VIEW # CREATE [TEMPORARY] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [( columnName [, columnName ]* )] [COMMENT view_comment] AS query_expression Create a view with the given query expression. If a view with the same name already exists in the catalog, an exception is thrown.
TEMPORARY
Create temporary view that has catalog and database namespaces and overrides views.
IF NOT EXISTS
If the view already exists, nothing happens.
Back to top
CREATE FUNCTION # CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF NOT EXISTS] [catalog_name.][db_name.]function_name AS identifier [LANGUAGE JAVA|SCALA|PYTHON] Create a catalog function that has catalog and database namespaces with the identifier and optional language tag. If a function with the same name already exists in the catalog, an exception is thrown.
If the language tag is JAVA/SCALA, the identifier is the full classpath of the UDF. For the implementation of Java/Scala UDF, please refer to User-defined Functions for more details.
If the language tag is PYTHON, the identifier is the fully qualified name of the UDF, e.g. pyflink.table.tests.test_udf.add. For the implementation of Python UDF, please refer to Python UDFs for more details.
If the language tag is PYTHON, however the current program is written in Java/Scala or pure SQL, then you need to configure the Python dependencies.
TEMPORARY
Create temporary catalog function that has catalog and database namespaces and overrides catalog functions.
TEMPORARY SYSTEM
Create temporary system function that has no namespace and overrides built-in functions
IF NOT EXISTS
If the function already exists, nothing happens.
LANGUAGE JAVA|SCALA|PYTHON
Language tag to instruct Flink runtime how to execute the function. Currently only JAVA, SCALA and PYTHON are supported, the default language for a function is JAVA.
`}),e.add({id:116,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/csv/",title:"CSV",section:"Formats",content:` CSV format # To use the CSV format you need to add the Flink CSV dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-csv\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading CSV files using CsvReaderFormat. The reader utilizes Jackson library and allows passing the corresponding configuration for the CSV schema and parsing options.
CsvReaderFormat can be initialized and used like this:
CsvReaderFormat\u0026lt;SomePojo\u0026gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class); FileSource\u0026lt;SomePojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); The schema for CSV parsing, in this case, is automatically derived based on the fields of the SomePojo class using the Jackson library.
Note: you might need to add @JsonPropertyOrder({field1, field2, ...}) annotation to your class definition with the fields order exactly matching those of the CSV file columns. Advanced configuration # If you need more fine-grained control over the CSV schema or the parsing options, use the more low-level forSchema static factory method of CsvReaderFormat:
CsvReaderFormat\u0026lt;T\u0026gt; forSchema(Supplier\u0026lt;CsvMapper\u0026gt; mapperFactory, Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator, TypeInformation\u0026lt;T\u0026gt; typeInformation) Below is an example of reading a POJO with a custom columns\u0026rsquo; separator:
//Has to match the exact order of columns in the CSV file @JsonPropertyOrder({\u0026#34;city\u0026#34;,\u0026#34;lat\u0026#34;,\u0026#34;lng\u0026#34;,\u0026#34;country\u0026#34;,\u0026#34;iso2\u0026#34;, \u0026#34;adminName\u0026#34;,\u0026#34;capital\u0026#34;,\u0026#34;population\u0026#34;}) public static class CityPojo { public String city; public BigDecimal lat; public BigDecimal lng; public String country; public String iso2; public String adminName; public String capital; public long population; } Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator = mapper -\u0026gt; mapper.schemaFor(CityPojo.class).withoutQuoteChar().withColumnSeparator(\u0026#39;|\u0026#39;); CsvReaderFormat\u0026lt;CityPojo\u0026gt; csvFormat = CsvReaderFormat.forSchema(() -\u0026gt; new CsvMapper(), schemaGenerator, TypeInformation.of(CityPojo.class)); FileSource\u0026lt;CityPojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); The corresponding CSV file:
Berlin|52.5167|13.3833|Germany|DE|Berlin|primary|3644826 San Francisco|37.7562|-122.443|United States|US|California||3592294 Beijing|39.905|116.3914|China|CN|Beijing|primary|19433000 It is also possible to read more complex data types using fine-grained Jackson settings:
public static class ComplexPojo { private long id; private int[] array; } CsvReaderFormat\u0026lt;ComplexPojo\u0026gt; csvFormat = CsvReaderFormat.forSchema( CsvSchema.builder() .addColumn( new CsvSchema.Column(0, \u0026#34;id\u0026#34;, CsvSchema.ColumnType.NUMBER)) .addColumn( new CsvSchema.Column(4, \u0026#34;array\u0026#34;, CsvSchema.ColumnType.ARRAY) .withArrayElementSeparator(\u0026#34;#\u0026#34;)) .build(), TypeInformation.of(ComplexPojo.class)); For PyFlink users, a csv schema can be defined by manually adding columns, and the output type of the csv source will be a Row with each column mapped to a field.
schema = CsvSchema.builder() \\ .add_number_column(\u0026#39;id\u0026#39;, number_type=DataTypes.BIGINT()) \\ .add_array_column(\u0026#39;array\u0026#39;, separator=\u0026#39;#\u0026#39;, element_type=DataTypes.INT()) \\ .set_column_separator(\u0026#39;,\u0026#39;) \\ .build() source = FileSource.for_record_stream_format( CsvReaderFormat.for_schema(schema), CSV_FILE_PATH).build() # the type of record will be Types.ROW_NAMED([\u0026#39;id\u0026#39;, \u0026#39;array\u0026#39;], [Types.LONG(), Types.LIST(Types.INT())]) ds = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#39;csv-source\u0026#39;) The corresponding CSV file:
0,1#2#3 1, 2,1 Similarly to the TextLineInputFormat, CsvReaderFormat can be used in both continues and batch modes (see TextLineInputFormat for examples).
For PyFlink users, CsvBulkWriters could be used to create BulkWriterFactory to write records to files in CSV format.
schema = CsvSchema.builder() \\ .add_number_column(\u0026#39;id\u0026#39;, number_type=DataTypes.BIGINT()) \\ .add_array_column(\u0026#39;array\u0026#39;, separator=\u0026#39;#\u0026#39;, element_type=DataTypes.INT()) \\ .set_column_separator(\u0026#39;,\u0026#39;) \\ .build() sink = FileSink.for_bulk_format( OUTPUT_DIR, CsvBulkWriters.for_schema(schema)).build() ds.sink_to(sink) `}),e.add({id:117,href:"/flink/flink-docs-master/docs/learn-flink/etl/",title:"Data Pipelines \u0026 ETL",section:"Learn Flink",content:` Data Pipelines \u0026amp; ETL # One very common use case for Apache Flink is to implement ETL (extract, transform, load) pipelines that take data from one or more sources, perform some transformations and/or enrichments, and then store the results somewhere. In this section we are going to look at how to use Flink\u0026rsquo;s DataStream API to implement this kind of application.
Note that Flink\u0026rsquo;s Table and SQL APIs are well suited for many ETL use cases. But regardless of whether you ultimately use the DataStream API directly, or not, having a solid understanding the basics presented here will prove valuable.
Stateless Transformations # This section covers map() and flatmap(), the basic operations used to implement stateless transformations. The examples in this section assume you are familiar with the Taxi Ride data used in the hands-on exercises in the flink-training-repo .
map() # In the first exercise you filtered a stream of taxi ride events. In that same code base there\u0026rsquo;s a GeoUtils class that provides a static method GeoUtils.mapToGridCell(float lon, float lat) which maps a location (longitude, latitude) to a grid cell that refers to an area that is approximately 100x100 meters in size.
Now let\u0026rsquo;s enrich our stream of taxi ride objects by adding startCell and endCell fields to each event. You can create an EnrichedRide object that extends TaxiRide, adding these fields:
public static class EnrichedRide extends TaxiRide { public int startCell; public int endCell; public EnrichedRide() {} public EnrichedRide(TaxiRide ride) { this.rideId = ride.rideId; this.isStart = ride.isStart; ... this.startCell = GeoUtils.mapToGridCell(ride.startLon, ride.startLat); this.endCell = GeoUtils.mapToGridCell(ride.endLon, ride.endLat); } public String toString() { return super.toString() + \u0026#34;,\u0026#34; + Integer.toString(this.startCell) + \u0026#34;,\u0026#34; + Integer.toString(this.endCell); } } You can then create an application that transforms the stream
DataStream\u0026lt;TaxiRide\u0026gt; rides = env.addSource(new TaxiRideSource(...)); DataStream\u0026lt;EnrichedRide\u0026gt; enrichedNYCRides = rides .filter(new RideCleansingSolution.NYCFilter()) .map(new Enrichment()); enrichedNYCRides.print(); with this MapFunction:
public static class Enrichment implements MapFunction\u0026lt;TaxiRide, EnrichedRide\u0026gt; { @Override public EnrichedRide map(TaxiRide taxiRide) throws Exception { return new EnrichedRide(taxiRide); } } flatmap() # A MapFunction is suitable only when performing a one-to-one transformation: for each and every stream element coming in, map() will emit one transformed element. Otherwise, you will want to use flatmap()
DataStream\u0026lt;TaxiRide\u0026gt; rides = env.addSource(new TaxiRideSource(...)); DataStream\u0026lt;EnrichedRide\u0026gt; enrichedNYCRides = rides .flatMap(new NYCEnrichment()); enrichedNYCRides.print(); together with a FlatMapFunction:
public static class NYCEnrichment implements FlatMapFunction\u0026lt;TaxiRide, EnrichedRide\u0026gt; { @Override public void flatMap(TaxiRide taxiRide, Collector\u0026lt;EnrichedRide\u0026gt; out) throws Exception { FilterFunction\u0026lt;TaxiRide\u0026gt; valid = new RideCleansing.NYCFilter(); if (valid.filter(taxiRide)) { out.collect(new EnrichedRide(taxiRide)); } } } With the Collector provided in this interface, the flatmap() method can emit as many stream elements as you like, including none at all.
Back to top
Keyed Streams # keyBy() # It is often very useful to be able to partition a stream around one of its attributes, so that all events with the same value of that attribute are grouped together. For example, suppose you wanted to find the longest taxi rides starting in each of the grid cells. Thinking in terms of a SQL query, this would mean doing some sort of GROUP BY with the startCell, while in Flink this is done with keyBy(KeySelector)
rides .flatMap(new NYCEnrichment()) .keyBy(enrichedRide -\u0026gt; enrichedRide.startCell); Every keyBy causes a network shuffle that repartitions the stream. In general this is pretty expensive, since it involves network communication along with serialization and deserialization.
Keys are computed # KeySelectors aren\u0026rsquo;t limited to extracting a key from your events. They can, instead, compute the key in whatever way you want, so long as the resulting key is deterministic, and has valid implementations of hashCode() and equals(). This restriction rules out KeySelectors that generate random numbers, or that return Arrays or Enums, but you can have composite keys using Tuples or POJOs, for example, so long as their elements follow these same rules.
The keys must be produced in a deterministic way, because they are recomputed whenever they are needed, rather than being attached to the stream records.
For example, rather than creating a new EnrichedRide class with a startCell field that we then use as a key via
keyBy(enrichedRide -\u0026gt; enrichedRide.startCell); we could do this, instead:
keyBy(ride -\u0026gt; GeoUtils.mapToGridCell(ride.startLon, ride.startLat)); Aggregations on Keyed Streams # This bit of code creates a new stream of tuples containing the startCell and duration (in minutes) for each end-of-ride event:
import org.joda.time.Interval; DataStream\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; minutesByStartCell = enrichedNYCRides .flatMap(new FlatMapFunction\u0026lt;EnrichedRide, Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt;() { @Override public void flatMap(EnrichedRide ride, Collector\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; out) throws Exception { if (!ride.isStart) { Interval rideInterval = new Interval(ride.startTime, ride.endTime); Minutes duration = rideInterval.toDuration().toStandardMinutes(); out.collect(new Tuple2\u0026lt;\u0026gt;(ride.startCell, duration)); } } }); Now it is possible to produce a stream that contains only those rides that are the longest rides ever seen (to that point) for each startCell.
There are a variety of ways that the field to use as the key can be expressed. Earlier you saw an example with an EnrichedRide POJO, where the field to use as the key was specified with its name. This case involves Tuple2 objects, and the index within the tuple (starting from 0) is used to specify the key.
minutesByStartCell .keyBy(value -\u0026gt; value.f0) // .keyBy(value -\u0026gt; value.startCell) .maxBy(1) // duration .print(); The output stream now contains a record for each key every time the duration reaches a new maximum \u0026ndash; as shown here with cell 50797:
... 4\u0026gt; (64549,5M) 4\u0026gt; (46298,18M) 1\u0026gt; (51549,14M) 1\u0026gt; (53043,13M) 1\u0026gt; (56031,22M) 1\u0026gt; (50797,6M) ... 1\u0026gt; (50797,8M) ... 1\u0026gt; (50797,11M) ... 1\u0026gt; (50797,12M) (Implicit) State # This is the first example in this training that involves stateful streaming. Though the state is being handled transparently, Flink has to keep track of the maximum duration for each distinct key.
Whenever state gets involved in your application, you should think about how large the state might become. Whenever the key space is unbounded, then so is the amount of state Flink will need.
When working with streams, it generally makes more sense to think in terms of aggregations over finite windows, rather than over the entire stream.
reduce() and other aggregators # maxBy(), used above, is just one example of a number of aggregator functions available on Flink\u0026rsquo;s KeyedStreams. There is also a more general purpose reduce() function that you can use to implement your own custom aggregations.
Back to top
Stateful Transformations # Why is Flink Involved in Managing State? # Your applications are certainly capable of using state without getting Flink involved in managing it \u0026ndash; but Flink offers some compelling features for the state it manages:
local: Flink state is kept local to the machine that processes it, and can be accessed at memory speed durable: Flink state is fault-tolerant, i.e., it is automatically checkpointed at regular intervals, and is restored upon failure vertically scalable: Flink state can be kept in embedded RocksDB instances that scale by adding more local disk horizontally scalable: Flink state is redistributed as your cluster grows and shrinks queryable: Flink state can be queried externally via the Queryable State API. In this section you will learn how to work with Flink\u0026rsquo;s APIs that manage keyed state.
Rich Functions # At this point you have already seen several of Flink\u0026rsquo;s function interfaces, including FilterFunction, MapFunction, and FlatMapFunction. These are all examples of the Single Abstract Method pattern.
For each of these interfaces, Flink also provides a so-called \u0026ldquo;rich\u0026rdquo; variant, e.g., RichFlatMapFunction, which has some additional methods, including:
open(Configuration c) close() getRuntimeContext() open() is called once, during operator initialization. This is an opportunity to load some static data, or to open a connection to an external service, for example.
getRuntimeContext() provides access to a whole suite of potentially interesting things, but most notably it is how you can create and access state managed by Flink.
An Example with Keyed State # In this example, imagine you have a stream of events that you want to de-duplicate, so that you only keep the first event with each key. Here\u0026rsquo;s an application that does that, using a RichFlatMapFunction called Deduplicator:
private static class Event { public final String key; public final long timestamp; ... } public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new EventSource()) .keyBy(e -\u0026gt; e.key) .flatMap(new Deduplicator()) .print(); env.execute(); } To accomplish this, Deduplicator will need to somehow remember, for each key, whether or not there has already been an event for that key. It will do so using Flink\u0026rsquo;s keyed state interface.
When you are working with a keyed stream like this one, Flink will maintain a key/value store for each item of state being managed.
Flink supports several different types of keyed state, and this example uses the simplest one, namely ValueState. This means that for each key, Flink will store a single object \u0026ndash; in this case, an object of type Boolean.
Our Deduplicator class has two methods: open() and flatMap(). The open method establishes the use of managed state by defining a ValueStateDescriptor\u0026lt;Boolean\u0026gt;. The arguments to the constructor specify a name for this item of keyed state (\u0026ldquo;keyHasBeenSeen\u0026rdquo;), and provide information that can be used to serialize these objects (in this case, Types.BOOLEAN).
public static class Deduplicator extends RichFlatMapFunction\u0026lt;Event, Event\u0026gt; { ValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; @Override public void open(Configuration conf) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; desc = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;keyHasBeenSeen\u0026#34;, Types.BOOLEAN); keyHasBeenSeen = getRuntimeContext().getState(desc); } @Override public void flatMap(Event event, Collector\u0026lt;Event\u0026gt; out) throws Exception { if (keyHasBeenSeen.value() == null) { out.collect(event); keyHasBeenSeen.update(true); } } } When the flatMap method calls keyHasBeenSeen.value(), Flink\u0026rsquo;s runtime looks up the value of this piece of state for the key in context, and only if it is null does it go ahead and collect the event to the output. It also updates keyHasBeenSeen to true in this case.
This mechanism for accessing and updating key-partitioned state may seem rather magical, since the key is not explicitly visible in the implementation of our Deduplicator. When Flink\u0026rsquo;s runtime calls the open method of our RichFlatMapFunction, there is no event, and thus no key in context at that moment. But when it calls the flatMap method, the key for the event being processed is available to the runtime, and is used behind the scenes to determine which entry in Flink\u0026rsquo;s state backend is being operated on.
When deployed to a distributed cluster, there will be many instances of this Deduplicator, each of which will responsible for a disjoint subset of the entire keyspace. Thus, when you see a single item of ValueState, such as
ValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; understand that this represents not just a single Boolean, but rather a distributed, sharded, key/value store.
Clearing State # There\u0026rsquo;s a potential problem with the example above: What will happen if the key space is unbounded? Flink is storing somewhere an instance of Boolean for every distinct key that is used. If there\u0026rsquo;s a bounded set of keys then this will be fine, but in applications where the set of keys is growing in an unbounded way, it\u0026rsquo;s necessary to clear the state for keys that are no longer needed. This is done by calling clear() on the state object, as in:
keyHasBeenSeen.clear(); You might want to do this, for example, after a period of inactivity for a given key. You\u0026rsquo;ll see how to use Timers to do this when you learn about ProcessFunctions in the section on event-driven applications.
There\u0026rsquo;s also a State Time-to-Live (TTL) option that you can configure with the state descriptor that specifies when you want the state for stale keys to be automatically cleared.
Non-keyed State # It is also possible to work with managed state in non-keyed contexts. This is sometimes called operator state. The interfaces involved are somewhat different, and since it is unusual for user-defined functions to need non-keyed state, it is not covered here. This feature is most often used in the implementation of sources and sinks.
Back to top
Connected Streams # Sometimes instead of applying a pre-defined transformation like this:
you want to be able to dynamically alter some aspects of the transformation \u0026ndash; by streaming in thresholds, or rules, or other parameters. The pattern in Flink that supports this is something called connected streams, wherein a single operator has two input streams, like this:
Connected streams can also be used to implement streaming joins.
Example # In this example, a control stream is used to specify words which must be filtered out of the streamOfWords. A RichCoFlatMapFunction called ControlFunction is applied to the connected streams to get this done.
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; control = env .fromElements(\u0026#34;DROP\u0026#34;, \u0026#34;IGNORE\u0026#34;) .keyBy(x -\u0026gt; x); DataStream\u0026lt;String\u0026gt; streamOfWords = env .fromElements(\u0026#34;Apache\u0026#34;, \u0026#34;DROP\u0026#34;, \u0026#34;Flink\u0026#34;, \u0026#34;IGNORE\u0026#34;) .keyBy(x -\u0026gt; x); control .connect(streamOfWords) .flatMap(new ControlFunction()) .print(); env.execute(); } Note that the two streams being connected must be keyed in compatible ways. The role of a keyBy is to partition a stream\u0026rsquo;s data, and when keyed streams are connected, they must be partitioned in the same way. This ensures that all of the events from both streams with the same key are sent to the same instance. This makes it possible, then, to join the two streams on that key, for example.
In this case the streams are both of type DataStream\u0026lt;String\u0026gt;, and both streams are keyed by the string. As you will see below, this RichCoFlatMapFunction is storing a Boolean value in keyed state, and this Boolean is shared by the two streams.
public static class ControlFunction extends RichCoFlatMapFunction\u0026lt;String, String, String\u0026gt; { private ValueState\u0026lt;Boolean\u0026gt; blocked; @Override public void open(Configuration config) { blocked = getRuntimeContext() .getState(new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;blocked\u0026#34;, Boolean.class)); } @Override public void flatMap1(String control_value, Collector\u0026lt;String\u0026gt; out) throws Exception { blocked.update(Boolean.TRUE); } @Override public void flatMap2(String data_value, Collector\u0026lt;String\u0026gt; out) throws Exception { if (blocked.value() == null) { out.collect(data_value); } } } A RichCoFlatMapFunction is a kind of FlatMapFunction that can be applied to a pair of connected streams, and it has access to the rich function interface. This means that it can be made stateful.
The blocked Boolean is being used to remember the keys (words, in this case) that have been mentioned on the control stream, and those words are being filtered out of the streamOfWords stream. This is keyed state, and it is shared between the two streams, which is why the two streams have to share the same keyspace.
flatMap1 and flatMap2 are called by the Flink runtime with elements from each of the two connected streams \u0026ndash; in our case, elements from the control stream are passed into flatMap1, and elements from streamOfWords are passed into flatMap2. This was determined by the order in which the two streams are connected with control.connect(streamOfWords).
It is important to recognize that you have no control over the order in which the flatMap1 and flatMap2 callbacks are called. These two input streams are racing against each other, and the Flink runtime will do what it wants to regarding consuming events from one stream or the other. In cases where timing and/or ordering matter, you may find it necessary to buffer events in managed Flink state until your application is ready to process them. (Note: if you are truly desperate, it is possible to exert some limited control over the order in which a two-input operator consumes its inputs by using a custom Operator that implements the InputSelectable Back to top
Hands-on # The hands-on exercise that goes with this section is the Rides and Fares .
Back to top
Further Reading # DataStream Transformations Stateful Stream Processing Back to top
`}),e.add({id:118,href:"/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/",title:"Docker",section:"Standalone",content:` Docker Setup # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate containers) of a Flink cluster using Docker containers.
Introduction # Docker is a popular container runtime. There are official Docker images for Apache Flink available on Docker Hub. You can use the Docker images to deploy a Session or Application cluster on Docker. This page focuses on the setup of Flink on Docker and Docker Compose.
Deployment into managed containerized environments, such as standalone Kubernetes or native Kubernetes, are described on separate pages.
Starting a Session Cluster on Docker # A Flink Session cluster can be used to run multiple jobs. Each job needs to be submitted to the cluster after the cluster has been deployed. To deploy a Flink Session cluster with Docker, you need to start a JobManager container. To enable communication between the containers, we first set a required Flink configuration property and create a network:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: jobmanager\u0026#34; \$ docker network create flink-network Then we launch the JobManager:
\$ docker run \\ --rm \\ --name=jobmanager \\ --network flink-network \\ --publish 8081:8081 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest jobmanager and one or more TaskManager containers:
\$ docker run \\ --rm \\ --name=taskmanager \\ --network flink-network \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest taskmanager The web interface is now available at localhost:8081.
Submission of a job is now possible like this (assuming you have a local distribution of Flink available):
\$ ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar To shut down the cluster, either terminate (e.g. with CTRL-C) the JobManager and TaskManager processes, or use docker ps to identify and docker stop to terminate the containers.
Deployment Modes # The Flink image contains a regular Flink distribution with its default configuration and a standard entry point script. You can run its entry point in the following modes:
JobManager for a Session cluster JobManager for an Application cluster TaskManager for any cluster This allows you to deploy a standalone cluster (Session or Application Mode) in any containerised environment, for example:
manually in a local Docker setup, in a Kubernetes cluster, with Docker Compose, Note The native Kubernetes also runs the same image by default and deploys TaskManagers on demand so that you do not have to do it manually.
The next chapters describe how to start a single Flink Docker container for various purposes.
Once you\u0026rsquo;ve started Flink on Docker, you can access the Flink Web UI on localhost:8081 or submit jobs like this ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar.
We recommend using Docker Compose for deploying Flink in Session Mode to ease system configuration.
Application Mode # For high-level intuition behind the application mode, please refer to the deployment mode overview. A Flink Application cluster is a dedicated cluster which runs a single job. In this case, you deploy the cluster with the job as one step, thus, there is no extra job submission needed.
The job artifacts are included into the class path of Flink\u0026rsquo;s JVM process within the container and consist of:
your job jar, which you would normally submit to a Session cluster and all other necessary dependencies or resources, not included into Flink. To deploy a cluster for a single job with Docker, you need to
make job artifacts available locally in all containers under /opt/flink/usrlib, start a JobManager container in the Application cluster mode start the required number of TaskManager containers. To make the job artifacts available locally in the container, you can
either mount a volume (or multiple volumes) with the artifacts to /opt/flink/usrlib when you start the JobManager and TaskManagers:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: jobmanager\u0026#34; \$ docker network create flink-network \$ docker run \\ --mount type=bind,src=/host/path/to/job/artifacts1,target=/opt/flink/usrlib/artifacts1 \\ --mount type=bind,src=/host/path/to/job/artifacts2,target=/opt/flink/usrlib/artifacts2 \\ --rm \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --name=jobmanager \\ --network flink-network \\ flink:latest standalone-job \\ --job-classname com.job.ClassName \\ [--job-id \u0026lt;job id\u0026gt;] \\ [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] \\ [job arguments] \$ docker run \\ --mount type=bind,src=/host/path/to/job/artifacts1,target=/opt/flink/usrlib/artifacts1 \\ --mount type=bind,src=/host/path/to/job/artifacts2,target=/opt/flink/usrlib/artifacts2 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest taskmanager or extend the Flink image by writing a custom Dockerfile, build it and use it for starting the JobManager and TaskManagers:
FROM flink ADD /host/path/to/job/artifacts/1 /opt/flink/usrlib/artifacts/1 ADD /host/path/to/job/artifacts/2 /opt/flink/usrlib/artifacts/2 \$ docker build --tag flink_with_job_artifacts . \$ docker run \\ flink_with_job_artifacts standalone-job \\ --job-classname com.job.ClassName \\ [--job-id \u0026lt;job id\u0026gt;] \\ [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] \\ [job arguments] \$ docker run flink_with_job_artifacts taskmanager The standalone-job argument starts a JobManager container in the Application Mode.
JobManager additional command line arguments # You can provide the following additional command line arguments to the cluster entrypoint:
--job-classname \u0026lt;job class name\u0026gt;: Class name of the job to run.
By default, Flink scans its class path for a JAR with a Main-Class or program-class manifest entry and chooses it as the job class. Use this command line argument to manually set the job class. This argument is required in case that no or more than one JAR with such a manifest entry is available on the class path.
--job-id \u0026lt;job id\u0026gt; (optional): Manually set a Flink job ID for the job (default: 00000000000000000000000000000000)
--fromSavepoint /path/to/savepoint (optional): Restore from a savepoint
In order to resume from a savepoint, you also need to pass the savepoint path. Note that /path/to/savepoint needs to be accessible in all Docker containers of the cluster (e.g., storing it on a DFS or from the mounted volume or adding it to the image).
--allowNonRestoredState (optional): Skip broken savepoint state
Additionally you can specify this argument to allow that savepoint state is skipped which cannot be restored.
If the main function of the user job main class accepts arguments, you can also pass them at the end of the docker run command.
Session Mode # For high-level intuition behind the session mode, please refer to the deployment mode overview. Local deployment in the Session Mode has already been described in the Getting Started section above.
Back to top
Flink Docker Images # Image Hosting # There are two distribution channels for the Flink Docker images:
Official Flink images on Docker Hub (reviewed and build by Docker) Flink images on Docker Hub apache/flink (managed by the Flink developers) We recommend using the official images on Docker Hub, as they are reviewed by Docker. The images on apache/flink are provided in case of delays in the review process by Docker.
Launching an image named flink:latest will pull the latest image from Docker Hub. In order to use the images hosted in apache/flink, replace flink by apache/flink. Any of the image tags (starting from Flink 1.11.3) are available on apache/flink as well.
Image Tags # The Flink Docker repository is hosted on Docker Hub and serves images of Flink version 1.2.1 and later. The source for these images can be found in the Apache flink-docker repository.
Images for each supported combination of Flink and Scala versions are available, and tag aliases are provided for convenience.
For example, you can use the following aliases:
flink:latest → flink:\u0026lt;latest-flink\u0026gt;-scala_\u0026lt;latest-scala\u0026gt; flink:1.11 → flink:1.11.\u0026lt;latest-flink-1.11\u0026gt;-scala_2.12 Note It is recommended to always use an explicit version tag of the docker image that specifies both the needed Flink and Scala versions (for example flink:1.11-scala_2.12). This will avoid some class conflicts that can occur if the Flink and/or Scala versions used in the application are different from the versions provided by the docker image.
Note Prior to Flink 1.5 version, Hadoop dependencies were always bundled with Flink. You can see that certain tags include the version of Hadoop, e.g. (e.g. -hadoop28). Beginning with Flink 1.5, image tags that omit the Hadoop version correspond to Hadoop-free releases of Flink that do not include a bundled Hadoop distribution.
Flink with Docker Compose # Docker Compose is a way to run a group of Docker containers locally. The next sections show examples of configuration files to run Flink.
General # Create the docker-compose.yaml file. Please check the examples in the sections below:
Application Mode Session Mode Session Mode with SQL Client Launch a cluster in the foreground (use -d for background)
\$ docker-compose up Scale the cluster up or down to N TaskManagers
\$ docker-compose scale taskmanager=\u0026lt;N\u0026gt; Access the JobManager container
\$ docker exec -it \$(docker ps --filter name=jobmanager --format={{.ID}}) /bin/sh Kill the cluster
\$ docker-compose down Access Web UI
When the cluster is running, you can visit the web UI at http://localhost:8081.
Application Mode # In application mode you start a Flink cluster that is dedicated to run only the Flink Jobs which have been bundled with the images. Hence, you need to build a dedicated Flink Image per application. Please check here for the details. See also how to specify the JobManager arguments in the command for the jobmanager service.
docker-compose.yml for Application Mode.
version: \u0026#34;2.2\u0026#34; services: jobmanager: image: flink:latest ports: - \u0026#34;8081:8081\u0026#34; command: standalone-job --job-classname com.job.ClassName [--job-id \u0026lt;job id\u0026gt;] [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] [job arguments] volumes: - /host/path/to/job/artifacts:/opt/flink/usrlib environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager parallelism.default: 2 taskmanager: image: flink:latest depends_on: - jobmanager command: taskmanager scale: 1 volumes: - /host/path/to/job/artifacts:/opt/flink/usrlib environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 parallelism.default: 2 Session Mode # In Session Mode you use docker-compose to spin up a long-running Flink Cluster to which you can then submit Jobs.
docker-compose.yml for Session Mode:
version: \u0026#34;2.2\u0026#34; services: jobmanager: image: flink:latest ports: - \u0026#34;8081:8081\u0026#34; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager: image: flink:latest depends_on: - jobmanager command: taskmanager scale: 1 environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 Flink SQL Client with Session Cluster # In this example, you spin up a long-running session cluster and a Flink SQL CLI which uses this clusters to submit jobs to.
docker-compose.yml for Flink SQL Client with Session Cluster:
version: \u0026#34;2.2\u0026#34; services: jobmanager: image: flink:latest ports: - \u0026#34;8081:8081\u0026#34; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager: image: flink:latest depends_on: - jobmanager command: taskmanager scale: 1 environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 sql-client: image: flink:latest command: bin/sql-client.sh depends_on: - jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager In order to start the SQL Client run
docker-compose run sql-client You can then start creating tables and queries those.
Note, that all required dependencies (e.g. for connectors) need to be available in the cluster as well as the client. For example, if you would like to use the Kafka Connector create a custom image with the following Dockerfile
FROM flink:latest RUN wget -P /opt/flink/lib https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.12/1.16-SNAPSHOT/flink-sql-connector-kafka_scala_2.12-1.16-SNAPSHOT.jar and reference it (e.g via the build) command in the Dockerfile. and reference it (e.g via the build) command in the Dockerfile. SQL Commands like ADD JAR will not work for JARs located on the host machine as they only work with the local filesystem, which in this case is Docker\u0026rsquo;s overlay filesystem.
Using Flink Python on Docker # To build a custom image which has Python and PyFlink prepared, you can refer to the following Dockerfile:
FROM flink:latest # install python3: it has updated Python to 3.9 in Debian 11 and so install Python 3.7 from source # it currently only supports Python 3.6, 3.7 and 3.8 in PyFlink officially. RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev \u0026amp;\u0026amp; \\ wget https://www.python.org/ftp/python/3.7.9/Python-3.7.9.tgz \u0026amp;\u0026amp; \\ tar -xvf Python-3.7.9.tgz \u0026amp;\u0026amp; \\ cd Python-3.7.9 \u0026amp;\u0026amp; \\ ./configure --without-tests --enable-shared \u0026amp;\u0026amp; \\ make -j6 \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ ldconfig /usr/local/lib \u0026amp;\u0026amp; \\ cd .. \u0026amp;\u0026amp; rm -f Python-3.7.9.tgz \u0026amp;\u0026amp; rm -rf Python-3.7.9 \u0026amp;\u0026amp; \\ ln -s /usr/local/bin/python3 /usr/local/bin/python \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* # install PyFlink COPY apache-flink*.tar.gz / RUN pip3 install /apache-flink-libraries*.tar.gz \u0026amp;\u0026amp; pip3 install /apache-flink*.tar.gz Note For Debian 10 and below, Python 3 could also be installed alternatively as following:
RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install -y python3.7 python3-pip python3.7-dev \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN ln -s /usr/bin/python3 /usr/bin/python Note PyFlink packages could be built-in according to development guide Build the image named as pyflink:latest:
\$ docker build --tag pyflink:latest . Configuring Flink on Docker # Via dynamic properties # \$ docker run flink:latest \\ \u0026lt;jobmanager|standalone-job|taskmanager|historyserver\u0026gt; \\ -D jobmanager.rpc.address=host \\ -D taskmanager.numberOfTaskSlots=3 \\ -D blob.server.port=6124 Options set via dynamic properties overwrite the options from flink-conf.yaml.
Via Environment Variables # When you run Flink image, you can also change its configuration options by setting the environment variable FLINK_PROPERTIES:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: host taskmanager.numberOfTaskSlots: 3 blob.server.port: 6124 \u0026#34; \$ docker run --env FLINK_PROPERTIES=\${FLINK_PROPERTIES} flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; The jobmanager.rpc.address option must be configured, others are optional to set.
The environment variable FLINK_PROPERTIES should contain a list of Flink cluster configuration options separated by new line, the same way as in the flink-conf.yaml. FLINK_PROPERTIES takes precedence over configurations in flink-conf.yaml.
Via flink-conf.yaml # The configuration files (flink-conf.yaml, logging, hosts etc) are located in the /opt/flink/conf directory in the Flink image. To provide a custom location for the Flink configuration files, you can
either mount a volume with the custom configuration files to this path /opt/flink/conf when you run the Flink image:
\$ docker run \\ --mount type=bind,src=/host/path/to/custom/conf,target=/opt/flink/conf \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; or add them to your custom Flink image, build and run it:
FROM flink ADD /host/path/to/flink-conf.yaml /opt/flink/conf/flink-conf.yaml ADD /host/path/to/log4j.properties /opt/flink/conf/log4j.properties The mounted volume must contain all necessary configuration files. The flink-conf.yaml file must have write permission so that the Docker entry point script can modify it in certain cases. Using Filesystem Plugins # As described in the plugins documentation page: In order to use plugins they must be copied to the correct location in the Flink installation in the Docker container for them to work.
If you want to enable plugins provided with Flink (in the opt/ directory of the Flink distribution), you can pass the environment variable ENABLE_BUILT_IN_PLUGINS when you run the Flink image. The ENABLE_BUILT_IN_PLUGINS should contain a list of plugin jar file names separated by ;. A valid plugin name is for example flink-s3-fs-hadoop-1.16-SNAPSHOT.jar
\$ docker run \\ --env ENABLE_BUILT_IN_PLUGINS=flink-plugin1.jar;flink-plugin2.jar \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; There are also more advanced ways for customizing the Flink image.
Switching the Memory Allocator # Flink introduced jemalloc as default memory allocator to resolve memory fragmentation problem (please refer to FLINK-19125).
You could switch back to use glibc as the memory allocator to restore the old behavior or if any unexpected memory consumption or problem observed (and please report the issue via JIRA or mailing list if you found any), by setting environment variable DISABLE_JEMALLOC as true:
\$ docker run \\ --env DISABLE_JEMALLOC=true \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; For users that are still using glibc memory allocator, the glibc bug can easily be reproduced, especially while savepoints or full checkpoints with RocksDBStateBackend are created. Setting the environment variable MALLOC_ARENA_MAX can avoid unlimited memory growth:
\$ docker run \\ --env MALLOC_ARENA_MAX=1 \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; Further Customization # There are several ways in which you can further customize the Flink image:
install custom software (e.g. python) enable (symlink) optional libraries or plugins from /opt/flink/opt into /opt/flink/lib or /opt/flink/plugins add other libraries to /opt/flink/lib (e.g. Hadoop) add other plugins to /opt/flink/plugins You can customize the Flink image in several ways:
override the container entry point with a custom script where you can run any bootstrap actions. At the end you can call the standard /docker-entrypoint.sh script of the Flink image with the same arguments as described in supported deployment modes.
The following example creates a custom entry point script which enables more libraries and plugins. The custom script, custom library and plugin are provided from a mounted volume. Then it runs the standard entry point script of the Flink image:
# create custom_lib.jar # create custom_plugin.jar \$ echo \u0026#34; # enable an optional library ln -fs /opt/flink/opt/flink-queryable-state-runtime-*.jar /opt/flink/lib/ # enable a custom library ln -fs /mnt/custom_lib.jar /opt/flink/lib/ mkdir -p /opt/flink/plugins/flink-s3-fs-hadoop # enable an optional plugin ln -fs /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/flink-s3-fs-hadoop/ mkdir -p /opt/flink/plugins/custom_plugin # enable a custom plugin ln -fs /mnt/custom_plugin.jar /opt/flink/plugins/custom_plugin/ /docker-entrypoint.sh \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; \u0026#34; \u0026gt; custom_entry_point_script.sh \$ chmod 755 custom_entry_point_script.sh \$ docker run \\ --mount type=bind,src=\$(pwd),target=/mnt flink:latest /mnt/custom_entry_point_script.sh extend the Flink image by writing a custom Dockerfile and build a custom image:
FROM flink RUN set -ex; apt-get update; apt-get -y install python ADD /host/path/to/flink-conf.yaml /container/local/path/to/custom/conf/flink-conf.yaml ADD /host/path/to/log4j.properties /container/local/path/to/custom/conf/log4j.properties RUN ln -fs /opt/flink/opt/flink-queryable-state-runtime-*.jar /opt/flink/lib/. RUN mkdir -p /opt/flink/plugins/flink-s3-fs-hadoop RUN ln -fs /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/flink-s3-fs-hadoop/. ENV VAR_NAME value Commands for building:
\`\`\`sh \$ docker build --tag custom_flink_image . # optional push to your docker image registry if you have it, # e.g. to distribute the custom image to your cluster \$ docker push custom_flink_image \`\`\` Back to top
`}),e.add({id:119,href:"/flink/flink-docs-master/docs/dev/datastream/event-time/",title:"Event Time",section:"DataStream API",content:""}),e.add({id:120,href:"/flink/flink-docs-master/docs/concepts/flink-architecture/",title:"Flink Architecture",section:"Concepts",content:` Flink Architecture # Flink is a distributed system and requires effective allocation and management of compute resources in order to execute streaming applications. It integrates with all common cluster resource managers such as Hadoop YARN and Kubernetes, but can also be set up to run as a standalone cluster or even as a library.
This section contains an overview of Flink’s architecture and describes how its main components interact to execute applications and recover from failures.
Anatomy of a Flink Cluster # The Flink runtime consists of two types of processes: a JobManager and one or more TaskManagers.
The Client is not part of the runtime and program execution, but is used to prepare and send a dataflow to the JobManager. After that, the client can disconnect (detached mode), or stay connected to receive progress reports (attached mode). The client runs either as part of the Java/Scala program that triggers the execution, or in the command line process ./bin/flink run ....
The JobManager and TaskManagers can be started in various ways: directly on the machines as a standalone cluster, in containers, or managed by resource frameworks like YARN. TaskManagers connect to JobManagers, announcing themselves as available, and are assigned work.
JobManager # The JobManager has a number of responsibilities related to coordinating the distributed execution of Flink Applications: it decides when to schedule the next task (or set of tasks), reacts to finished tasks or execution failures, coordinates checkpoints, and coordinates recovery on failures, among others. This process consists of three different components:
ResourceManager
The ResourceManager is responsible for resource de-/allocation and provisioning in a Flink cluster — it manages task slots, which are the unit of resource scheduling in a Flink cluster (see TaskManagers). Flink implements multiple ResourceManagers for different environments and resource providers such as YARN, Kubernetes and standalone deployments. In a standalone setup, the ResourceManager can only distribute the slots of available TaskManagers and cannot start new TaskManagers on its own.
Dispatcher
The Dispatcher provides a REST interface to submit Flink applications for execution and starts a new JobMaster for each submitted job. It also runs the Flink WebUI to provide information about job executions.
JobMaster
A JobMaster is responsible for managing the execution of a single JobGraph. Multiple jobs can run simultaneously in a Flink cluster, each having its own JobMaster.
There is always at least one JobManager. A high-availability setup might have multiple JobManagers, one of which is always the leader, and the others are standby (see High Availability (HA)).
TaskManagers # The TaskManagers (also called workers) execute the tasks of a dataflow, and buffer and exchange the data streams.
There must always be at least one TaskManager. The smallest unit of resource scheduling in a TaskManager is a task slot. The number of task slots in a TaskManager indicates the number of concurrent processing tasks. Note that multiple operators may execute in a task slot (see Tasks and Operator Chains).
Back to top
Tasks and Operator Chains # For distributed execution, Flink chains operator subtasks together into tasks. Each task is executed by one thread. Chaining operators together into tasks is a useful optimization: it reduces the overhead of thread-to-thread handover and buffering, and increases overall throughput while decreasing latency. The chaining behavior can be configured; see the chaining docs for details.
The sample dataflow in the figure below is executed with five subtasks, and hence with five parallel threads.
Back to top
Task Slots and Resources # Each worker (TaskManager) is a JVM process, and may execute one or more subtasks in separate threads. To control how many tasks a TaskManager accepts, it has so called task slots (at least one).
Each task slot represents a fixed subset of resources of the TaskManager. A TaskManager with three slots, for example, will dedicate 1/3 of its managed memory to each slot. Slotting the resources means that a subtask will not compete with subtasks from other jobs for managed memory, but instead has a certain amount of reserved managed memory. Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks.
By adjusting the number of task slots, users can define how subtasks are isolated from each other. Having one slot per TaskManager means that each task group runs in a separate JVM (which can be started in a separate container, for example). Having multiple slots means more subtasks share the same JVM. Tasks in the same JVM share TCP connections (via multiplexing) and heartbeat messages. They may also share data sets and data structures, thus reducing the per-task overhead.
By default, Flink allows subtasks to share slots even if they are subtasks of different tasks, so long as they are from the same job. The result is that one slot may hold an entire pipeline of the job. Allowing this slot sharing has two main benefits:
A Flink cluster needs exactly as many task slots as the highest parallelism used in the job. No need to calculate how many tasks (with varying parallelism) a program contains in total.
It is easier to get better resource utilization. Without slot sharing, the non-intensive source/map() subtasks would block as many resources as the resource intensive window subtasks. With slot sharing, increasing the base parallelism in our example from two to six yields full utilization of the slotted resources, while making sure that the heavy subtasks are fairly distributed among the TaskManagers.
Flink Application Execution # A Flink Application is any user program that spawns one or multiple Flink jobs from its main() method. The execution of these jobs can happen in a local JVM (LocalEnvironment) or on a remote setup of clusters with multiple machines (RemoteEnvironment). For each program, the ExecutionEnvironment provides methods to control the job execution (e.g. setting the parallelism) and to interact with the outside world (see Anatomy of a Flink Program).
The jobs of a Flink Application can either be submitted to a long-running Flink Session Cluster, a dedicated Flink Job Cluster (deprecated), or a Flink Application Cluster. The difference between these options is mainly related to the cluster’s lifecycle and to resource isolation guarantees.
Flink Application Cluster # Cluster Lifecycle: a Flink Application Cluster is a dedicated Flink cluster that only executes jobs from one Flink Application and where the main() method runs on the cluster rather than the client. The job submission is a one-step process: you don’t need to start a Flink cluster first and then submit a job to the existing cluster session; instead, you package your application logic and dependencies into a executable job JAR and the cluster entrypoint (ApplicationClusterEntryPoint) is responsible for calling the main() method to extract the JobGraph. This allows you to deploy a Flink Application like any other application on Kubernetes, for example. The lifetime of a Flink Application Cluster is therefore bound to the lifetime of the Flink Application.
Resource Isolation: in a Flink Application Cluster, the ResourceManager and Dispatcher are scoped to a single Flink Application, which provides a better separation of concerns than the Flink Session Cluster.
Flink Session Cluster # Cluster Lifecycle: in a Flink Session Cluster, the client connects to a pre-existing, long-running cluster that can accept multiple job submissions. Even after all jobs are finished, the cluster (and the JobManager) will keep running until the session is manually stopped. The lifetime of a Flink Session Cluster is therefore not bound to the lifetime of any Flink Job.
Resource Isolation: TaskManager slots are allocated by the ResourceManager on job submission and released once the job is finished. Because all jobs are sharing the same cluster, there is some competition for cluster resources — like network bandwidth in the submit-job phase. One limitation of this shared setup is that if one TaskManager crashes, then all jobs that have tasks running on this TaskManager will fail; in a similar way, if some fatal error occurs on the JobManager, it will affect all jobs running in the cluster.
Other considerations: having a pre-existing cluster saves a considerable amount of time applying for resources and starting TaskManagers. This is important in scenarios where the execution time of jobs is very short and a high startup time would negatively impact the end-to-end user experience — as is the case with interactive analysis of short queries, where it is desirable that jobs can quickly perform computations using existing resources.
Formerly, a Flink Session Cluster was also known as a Flink Cluster in session mode. Flink Job Cluster (deprecated) # Per-job mode is only supported by YARN and has been deprecated in Flink 1.15. It will be dropped in FLINK-26000. Please consider application mode to launch a dedicated cluster per-job on YARN. Cluster Lifecycle: in a Flink Job Cluster, the available cluster manager (like YARN) is used to spin up a cluster for each submitted job and this cluster is available to that job only. Here, the client first requests resources from the cluster manager to start the JobManager and submits the job to the Dispatcher running inside this process. TaskManagers are then lazily allocated based on the resource requirements of the job. Once the job is finished, the Flink Job Cluster is torn down.
Resource Isolation: a fatal error in the JobManager only affects the one job running in that Flink Job Cluster.
Other considerations: because the ResourceManager has to apply and wait for external resource management components to start the TaskManager processes and allocate resources, Flink Job Clusters are more suited to large jobs that are long-running, have high-stability requirements and are not sensitive to longer startup times.
Formerly, a Flink Job Cluster was also known as a Flink Cluster in job (or per-job) mode. Flink Job Clusters are only supperted with YARN. Back to top
`}),e.add({id:121,href:"/flink/flink-docs-master/docs/connectors/dataset/formats/hadoop/",title:"Hadoop",section:"Formats",content:` Hadoop formats # Project Configuration # Support for Hadoop is contained in the flink-hadoop-compatibility Maven module.
Add the following dependency to your pom.xml to use hadoop
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Using Hadoop InputFormats # To use Hadoop InputFormats with Flink the format must first be wrapped using either readHadoopFile or createHadoopInput of the HadoopInputs utility class. The former is used for input formats derived from FileInputFormat while the latter has to be used for general purpose input formats. The resulting InputFormat can be used to create a data source by using ExecutionEnvironment#createInput.
The resulting DataSet contains 2-tuples where the first field is the key and the second field is the value retrieved from the Hadoop InputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextInputFormat.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath)); // Do something with the data. [...] Scala val env = ExecutionEnvironment.getExecutionEnvironment val input: DataSet[(LongWritable, Text)] = env.createInput(HadoopInputs.readHadoopFile( new TextInputFormat, classOf[LongWritable], classOf[Text], textPath)) // Do something with the data. [...] Using Hadoop OutputFormats # Flink provides a compatibility wrapper for Hadoop OutputFormats. Any class that implements org.apache.hadoop.mapred.OutputFormat or extends org.apache.hadoop.mapreduce.OutputFormat is supported. The OutputFormat wrapper expects its input data to be a DataSet containing 2-tuples of key and value. These are to be processed by the Hadoop OutputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextOutputFormat.
Java // Obtain the result we want to emit DataSet\u0026lt;Tuple2\u0026lt;Text, IntWritable\u0026gt;\u0026gt; hadoopResult = [...]; // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt; hadoopOF = // create the Flink wrapper. new HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt;( // set the Hadoop OutputFormat and specify the job. new TextOutputFormat\u0026lt;Text, IntWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. hadoopResult.output(hadoopOF); Scala // Obtain your result to emit. val hadoopResult: DataSet[(Text, IntWritable)] = [...] val hadoopOF = new HadoopOutputFormat[Text,IntWritable]( new TextOutputFormat[Text, IntWritable], new JobConf) hadoopOF.getJobConf.set(\u0026#34;mapred.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;) FileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) hadoopResult.output(hadoopOF) Back to top
`}),e.add({id:122,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/hadoop/",title:"Hadoop",section:"Formats",content:` Hadoop formats # Project Configuration # Support for Hadoop is contained in the flink-hadoop-compatibility Maven module.
Add the following dependency to your pom.xml to use hadoop
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Using Hadoop InputFormats # To use Hadoop InputFormats with Flink the format must first be wrapped using either readHadoopFile or createHadoopInput of the HadoopInputs utility class. The former is used for input formats derived from FileInputFormat while the latter has to be used for general purpose input formats. The resulting InputFormat can be used to create a data source by using ExecutionEnvironment#createInput.
The resulting DataStream contains 2-tuples where the first field is the key and the second field is the value retrieved from the Hadoop InputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextInputFormat.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath)); // Do something with the data. [...] Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val input: DataStream[(LongWritable, Text)] = env.createInput(HadoopInputs.readHadoopFile( new TextInputFormat, classOf[LongWritable], classOf[Text], textPath)) // Do something with the data. [...] Using Hadoop OutputFormats # Flink provides a compatibility wrapper for Hadoop OutputFormats. Any class that implements org.apache.hadoop.mapred.OutputFormat or extends org.apache.hadoop.mapreduce.OutputFormat is supported. The OutputFormat wrapper expects its input data to be a DataSet containing 2-tuples of key and value. These are to be processed by the Hadoop OutputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextOutputFormat.
Java // Obtain the result we want to emit DataStream\u0026lt;Tuple2\u0026lt;Text, IntWritable\u0026gt;\u0026gt; hadoopResult = [...] // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt; hadoopOF = // create the Flink wrapper. new HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt;( // set the Hadoop OutputFormat and specify the job. new TextOutputFormat\u0026lt;Text, IntWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. hadoopResult.output(hadoopOF); Scala // Obtain your result to emit. val hadoopResult: DataStream[(Text, IntWritable)] = [...] val hadoopOF = new HadoopOutputFormat[Text,IntWritable]( new TextOutputFormat[Text, IntWritable], new JobConf) hadoopOF.getJobConf.set(\u0026#34;mapred.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;) FileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) hadoopResult.output(hadoopOF) Back to top
`}),e.add({id:123,href:"/flink/flink-docs-master/docs/connectors/table/hive/hive_read_write/",title:"Hive Read \u0026 Write",section:"Hive",content:` Hive Read \u0026amp; Write # Using the HiveCatalog, Apache Flink can be used for unified BATCH and STREAM processing of Apache Hive Tables. This means Flink can be used as a more performant alternative to Hive’s batch engine, or to continuously read and write data into and out of Hive tables to power real-time data warehousing applications.
Reading # Flink supports reading data from Hive in both BATCH and STREAMING modes. When run as a BATCH application, Flink will execute its query over the state of the table at the point in time when the query is executed. STREAMING reads will continuously monitor the table and incrementally fetch new data as it is made available. Flink will read tables as bounded by default.
STREAMING reads support consuming both partitioned and non-partitioned tables. For partitioned tables, Flink will monitor the generation of new partitions, and read them incrementally when available. For non-partitioned tables, Flink will monitor the generation of new files in the folder and read new files incrementally.
Key Default Type Description streaming-source.enable false Boolean Enable streaming source or not. NOTES: Please make sure that each partition/file should be written atomically, otherwise the reader may get incomplete data. streaming-source.partition.include all String Option to set the partitions to read, the supported option are \`all\` and \`latest\`, the \`all\` means read all partitions; the \`latest\` means read latest partition in order of 'streaming-source.partition.order', the \`latest\` only works\` when the streaming hive source table used as temporal table. By default the option is \`all\`. Flink supports temporal join the latest hive partition by enabling 'streaming-source.enable' and setting 'streaming-source.partition.include' to 'latest', at the same time, user can assign the partition compare order and data update interval by configuring following partition-related options. streaming-source.monitor-interval None Duration Time interval for consecutively monitoring partition/file. Notes: The default interval for hive streaming reading is '1 min', the default interval for hive streaming temporal join is '60 min', this is because there's one framework limitation that every TM will visit the Hive metaStore in current hive streaming temporal join implementation which may produce pressure to metaStore, this will improve in the future. streaming-source.partition-order partition-name String The partition order of streaming source, support create-time, partition-time and partition-name. create-time compares partition/file creation time, this is not the partition create time in Hive metaStore, but the folder/file modification time in filesystem, if the partition folder somehow gets updated, e.g. add new file into folder, it can affect how the data is consumed. partition-time compares the time extracted from partition name. partition-name compares partition name's alphabetical order. For non-partition table, this value should always be 'create-time'. By default the value is partition-name. The option is equality with deprecated option 'streaming-source.consume-order'. streaming-source.consume-start-offset None String Start offset for streaming consuming. How to parse and compare offsets depends on your order. For create-time and partition-time, should be a timestamp string (yyyy-[m]m-[d]d [hh:mm:ss]). For partition-time, will use partition time extractor to extract time from partition. For partition-name, is the partition name string (e.g. pt_year=2020/pt_mon=10/pt_day=01). SQL Hints can be used to apply configurations to a Hive table without changing its definition in the Hive metastore.
SELECT * FROM hive_table /*+ OPTIONS(\u0026#39;streaming-source.enable\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;streaming-source.consume-start-offset\u0026#39;=\u0026#39;2020-05-20\u0026#39;) */; Notes
Monitor strategy is to scan all directories/files currently in the location path. Many partitions may cause performance degradation. Streaming reads for non-partitioned tables requires that each file be written atomically into the target directory. Streaming reading for partitioned tables requires that each partition should be added atomically in the view of hive metastore. If not, new data added to an existing partition will be consumed. Streaming reads do not support watermark grammar in Flink DDL. These tables cannot be used for window operators. Reading Hive Views # Flink is able to read from Hive defined views, but some limitations apply:
The Hive catalog must be set as the current catalog before you can query the view. This can be done by either tableEnv.useCatalog(...) in Table API or USE CATALOG ... in SQL Client.
Hive and Flink SQL have different syntax, e.g. different reserved keywords and literals. Make sure the view’s query is compatible with Flink grammar.
Vectorized Optimization upon Read # Flink will automatically used vectorized reads of Hive tables when the following conditions are met:
Format: ORC or Parquet. Columns without complex data type, like hive types: List, Map, Struct, Union. This feature is enabled by default. It may be disabled with the following configuration.
table.exec.hive.fallback-mapred-reader=true Source Parallelism Inference # By default, Flink will infer the optimal parallelism for its Hive readers based on the number of files, and number of blocks in each file.
Flink allows you to flexibly configure the policy of parallelism inference. You can configure the following parameters in TableConfig (note that these parameters affect all sources of the job):
Key Default Type Description table.exec.hive.infer-source-parallelism true Boolean If is true, source parallelism is inferred according to splits number. If is false, parallelism of source are set by config. table.exec.hive.infer-source-parallelism.max 1000 Integer Sets max infer parallelism for source operator. Tuning Split Size While Reading Hive Table # While reading Hive table, the data files will be enumerated into splits, one of which is a portion of data consumed by the source. Splits are granularity by which the source distributes the work and parallelize the data reading. Users can do some performance tuning by tuning the split\u0026rsquo;s size with the follow configurations.
Key Default Type Description table.exec.hive.split-max-size 128mb MemorySize The maximum number of bytes (default is 128MB) to pack into a split while reading Hive table. table.exec.hive.file-open-cost 4mb MemorySize The estimated cost (default is 4MB) to open a file. Used to enumerate Hive's files to splits. If the value is overestimated, Flink will tend to pack Hive's data into less splits, which will helpful when Hive's table contains many small files. If the value is underestimated, Flink will tend to pack Hive's data into more splits, which will help improve parallelism. NOTE: Currently, these two configurations only works for the Hive table stored as ORC format.
Load Partition Splits # Multi-thread is used to split hive\u0026rsquo;s partitions. You can use table.exec.hive.load-partition-splits.thread-num to configure the thread number. The default value is 3 and the configured value should be bigger than 0.
Read Partition With Subdirectory # In some case, you may create an external table referring another table, but the partition columns is a subset of the referred table. For example, you have a partitioned table fact_tz with partition day and hour:
CREATE TABLE fact_tz(x int) PARTITIONED BY (day STRING, hour STRING); And you have an external table fact_daily referring to table fact_tz with a coarse-grained partition day:
CREATE EXTERNAL TABLE fact_daily(x int) PARTITIONED BY (ds STRING) LOCATION \u0026#39;/path/to/fact_tz\u0026#39;; Then when reading the external table fact_daily, there will be sub-directories (hour=1 to hour=24) in the partition directory of the table.
By default, you can add partition with sub-directories to the external table. Flink SQL can recursively scan all sub-directories and fetch all the data from all sub-directories.
ALTER TABLE fact_daily ADD PARTITION (ds=\u0026#39;2022-07-07\u0026#39;) location \u0026#39;/path/to/fact_tz/ds=2022-07-07\u0026#39;; You can set job configuration table.exec.hive.read-partition-with-subdirectory.enabled (true by default) to false to disallow Flink to read the sub-directories. If the configuration is false and the directory does not contain files, rather consists of sub directories Flink blows up with the exception: java.io.IOException: Not a file: /path/to/data/*.
Temporal Table Join # You can use a Hive table as a temporal table, and then a stream can correlate the Hive table by temporal join. Please see temporal join for more information about the temporal join.
Flink supports processing-time temporal join Hive Table, the processing-time temporal join always joins the latest version of temporal table. Flink supports temporal join both partitioned table and Hive non-partitioned table, for partitioned table, Flink supports tracking the latest partition of Hive table automatically.
NOTE: Flink does not support event-time temporal join Hive table yet.
Temporal Join The Latest Partition # For a partitioned table which is changing over time, we can read it out as an unbounded stream, the partition can be acted as a version of the temporal table if every partition contains complete data of a version, the version of temporal table keeps the data of the partition.
Flink supports tracking the latest partition (version) of temporal table automatically in processing time temporal join, the latest partition (version) is defined by \u0026lsquo;streaming-source.partition-order\u0026rsquo; option, This is the most common user cases that use Hive table as dimension table in a Flink stream application job.
NOTE: This feature is only supported in Flink STREAMING Mode.
The following demo shows a classical business pipeline, the dimension table comes from Hive and it\u0026rsquo;s updated once every day by a batch pipeline job or a Flink job, the kafka stream comes from real time online business data or log and need to join with the dimension table to enrich stream.
-- Assume the data in hive table is updated per day, every day contains the latest and complete dimension data SET table.sql-dialect=hive; CREATE TABLE dimension_table ( product_id STRING, product_name STRING, unit_price DECIMAL(10, 4), pv_count BIGINT, like_count BIGINT, comment_count BIGINT, update_time TIMESTAMP(3), update_user STRING, ... ) PARTITIONED BY (pt_year STRING, pt_month STRING, pt_day STRING) TBLPROPERTIES ( -- using default partition-name order to load the latest partition every 12h (the most recommended and convenient way) \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;partition-name\u0026#39;, -- option with default value, can be ignored. -- using partition file create-time order to load the latest partition every 12h \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;create-time\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39; -- using partition-time order to load the latest partition every 12h \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;partition-time\u0026#39;, \u0026#39;partition.time-extractor.kind\u0026#39; = \u0026#39;default\u0026#39;, \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39; = \u0026#39;\$pt_year-\$pt_month-\$pt_day 00:00:00\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE orders_table ( order_id STRING, order_amount DOUBLE, product_id STRING, log_ts TIMESTAMP(3), proctime as PROCTIME() ) WITH (...); -- streaming sql, kafka temporal join a hive dimension table. Flink will automatically reload data from the -- configured latest partition in the interval of \u0026#39;streaming-source.monitor-interval\u0026#39;. SELECT * FROM orders_table AS o JOIN dimension_table FOR SYSTEM_TIME AS OF o.proctime AS dim ON o.product_id = dim.product_id; Temporal Join The Latest Table # For a Hive table, we can read it out as a bounded stream. In this case, the Hive table can only track its latest version at the time when we query. The latest version of table keep all data of the Hive table.
When performing the temporal join the latest Hive table, the Hive table will be cached in Slot memory and each record from the stream is joined against the table by key to decide whether a match is found. Using the latest Hive table as a temporal table does not require any additional configuration. Optionally, you can configure the TTL of the Hive table cache with the following property. After the cache expires, the Hive table will be scanned again to load the latest data.
Key Default Type Description lookup.join.cache.ttl 60 min Duration The cache TTL (e.g. 10min) for the build table in lookup join. By default the TTL is 60 minutes. NOTES: The option only works when lookup bounded hive table source, if you're using streaming hive source as temporal table, please use 'streaming-source.monitor-interval' to configure the interval of data update. The following demo shows load all data of hive table as a temporal table.
-- Assume the data in hive table is overwrite by batch pipeline. SET table.sql-dialect=hive; CREATE TABLE dimension_table ( product_id STRING, product_name STRING, unit_price DECIMAL(10, 4), pv_count BIGINT, like_count BIGINT, comment_count BIGINT, update_time TIMESTAMP(3), update_user STRING, ... ) TBLPROPERTIES ( \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;false\u0026#39;, -- option with default value, can be ignored. \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;all\u0026#39;, -- option with default value, can be ignored. \u0026#39;lookup.join.cache.ttl\u0026#39; = \u0026#39;12 h\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE orders_table ( order_id STRING, order_amount DOUBLE, product_id STRING, log_ts TIMESTAMP(3), proctime as PROCTIME() ) WITH (...); -- streaming sql, kafka join a hive dimension table. Flink will reload all data from dimension_table after cache ttl is expired. SELECT * FROM orders_table AS o JOIN dimension_table FOR SYSTEM_TIME AS OF o.proctime AS dim ON o.product_id = dim.product_id; Note:
Each joining subtask needs to keep its own cache of the Hive table. Please make sure the Hive table can fit into the memory of a TM task slot. It is encouraged to set a relatively large value both for streaming-source.monitor-interval(latest partition as temporal table) or lookup.join.cache.ttl(all partitions as temporal table). Otherwise, Jobs are prone to performance issues as the table needs to be updated and reloaded too frequently. Currently we simply load the whole Hive table whenever the cache needs refreshing. There\u0026rsquo;s no way to differentiate new data from the old. Writing # Flink supports writing data from Hive in both BATCH and STREAMING modes. When run as a BATCH application, Flink will write to a Hive table only making those records visible when the Job finishes. BATCH writes support both appending to and overwriting existing tables.
# ------ INSERT INTO will append to the table or partition, keeping the existing data intact ------ Flink SQL\u0026gt; INSERT INTO mytable SELECT \u0026#39;Tom\u0026#39;, 25; # ------ INSERT OVERWRITE will overwrite any existing data in the table or partition ------ Flink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u0026#39;Tom\u0026#39;, 25; Data can also be inserted into particular partitions.
# ------ Insert with static partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;, my_date=\u0026#39;2019-08-08\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25; # ------ Insert with dynamic partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;type_1\u0026#39;, \u0026#39;2019-08-08\u0026#39;; # ------ Insert with static(my_type) and dynamic(my_date) partition ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;2019-08-08\u0026#39;; STREAMING writes continuously adding new data to Hive, committing records - making them visible - incrementally. Users control when/how to trigger commits with several properties. Insert overwrite is not supported for streaming write.
The below examples show how the streaming sink can be used to write a streaming query to write data from Kafka into a Hive table with partition-commit, and runs a batch query to read that data back out.
Please see the streaming sink for a full list of available configurations.
SET table.sql-dialect=hive; CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hr:00:00\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL \u0026#39;5\u0026#39; SECOND -- Define watermark on TIMESTAMP column ) WITH (...); -- streaming sql, insert into hive table INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(log_ts, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM hive_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and hr=\u0026#39;12\u0026#39;; If the watermark is defined on TIMESTAMP_LTZ column and used partition-time to commit, the sink.partition-commit.watermark-time-zone is required to set to the session time zone, otherwise the partition committed may happen after a few hours.
SET table.sql-dialect=hive; CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hr:00:00\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.watermark-time-zone\u0026#39;=\u0026#39;Asia/Shanghai\u0026#39;, -- Assume user configured time zone is \u0026#39;Asia/Shanghai\u0026#39; \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, ts BIGINT, -- time in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND -- Define watermark on TIMESTAMP_LTZ column ) WITH (...); -- streaming sql, insert into hive table INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(ts_ltz, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(ts_ltz, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM hive_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and hr=\u0026#39;12\u0026#39;; By default, for streaming writes, Flink only supports renaming committers, meaning the S3 filesystem cannot support exactly-once streaming writes. Exactly-once writes to S3 can be achieved by configuring the following parameter to false. This will instruct the sink to use Flink\u0026rsquo;s native writers but only works for parquet and orc file types. This configuration is set in the TableConfig and will affect all sinks of the job.
Key Default Type Description table.exec.hive.fallback-mapred-writer true Boolean If it is false, using flink native writer to write parquet and orc files; if it is true, using hadoop mapred record writer to write parquet and orc files. Dynamic Partition Writing # Different from static partition writing which requires users to specify the partition column value, dynamic partition writing allow users not to specify partition column value. For example, for a partitioned table like:
CREATE TABLE fact_tz(x int) PARTITIONED BY (day STRING, hour STRING); Users can use the follow SQL statement to write data to partitioned table fact_tz:
INSERT INTO TABLE fact_tz PARTITION (day, hour) select 1, \u0026#39;2022-8-8\u0026#39;, \u0026#39;14\u0026#39;; It\u0026rsquo;s a typical case for dynamic partition writing since user does not specify any partition column value in the SQL statement.
By default, if it\u0026rsquo;s for dynamic partition writing, Flink will sort the data additionally by dynamic partition columns before writing into sink table. That means the sink will receive all elements of one partition and then all elements of another partition. Elements of different partitions will not be mixed. This is helpful for Hive sink to reduce the number of partition writers and improve writing performance by writing one partition at a time. Otherwise, too many partition writers may cause the OutOfMemory exception.
To avoid the extra sorting, you can set job configuration table.exec.hive.sink.sort-by-dynamic-partition.enable (true by default) to false. But with such a configuration, as said before, it may throw OutOfMemory exception if there are too many partitions fall into same sink node.
To relieve the problem of too many partition writers, if data is not skewed, you can add DISTRIBUTED BY \u0026lt;partition_field\u0026gt; in your SQL statement to shuffle the data with same partition into same node.
Also, you can manually add SORTED BY \u0026lt;partition_field\u0026gt; in your SQL statement to achieve the same purpose as table.exec.hive.sink.sort-by-dynamic-partition.enable=true.
NOTE:
The configuration table.exec.hive.sink.sort-by-dynamic-partition.enable only works in Flink BATCH mode. Currently, DISTRIBUTED BY and SORTED BY is only supported when using Hive dialect in Flink BATCH mode. Auto Gather Statistic # By default, Flink will gather the statistic automatically and then committed to Hive metastore during writing Hive table.
But in some case, you may want to disable it as it may be time-consuming to gather the statistic. Then, you can set the job configuration table.exec.hive.sink.statistic-auto-gather.enable (true by default) to false to disable it.
If the Hive table is stored as Parquet or ORC format, numFiles/totalSize/numRows/rawDataSize can be gathered. Otherwise, only numFiles/totalSize can be gathered.
To gather statistic numRows/rawDataSize for Parquet and ORC format, Flink will only read the file\u0026rsquo;s footer to do fast gathering. But it may still time-consuming when there are too many files, then you can configure the job configuration table.exec.hive.sink.statistic-auto-gather.thread-num (3 by default) to use more threads to speed the gathering.
NOTE:
Only BATCH mode supports to auto gather statistic, STREAMING mode doesn\u0026rsquo;t support it yet. Formats # Flink\u0026rsquo;s Hive integration has been tested against the following file formats:
Text CSV SequenceFile ORC Parquet `}),e.add({id:124,href:"/flink/flink-docs-master/docs/flinkdev/ide_setup/",title:"Importing Flink into an IDE",section:"Flink Development",content:` Importing Flink into an IDE # The sections below describe how to import the Flink project into an IDE for the development of Flink itself. For writing Flink programs, please refer to the Java API and the Scala API quickstart guides.
Whenever something is not working in your IDE, try with the Maven command line first (mvn clean package -DskipTests) as it might be your IDE that has a bug or is not properly set up. Preparation # To get started, please first checkout the Flink sources from one of our repositories, e.g.
git clone https://github.com/apache/flink.git Ignoring Refactoring Commits # We keep a list of big refactoring commits in .git-blame-ignore-revs. When looking at change annotations using git blame it\u0026rsquo;s helpful to ignore these. You can configure git and your IDE to do so using:
git config blame.ignoreRevsFile .git-blame-ignore-revs IntelliJ IDEA # The following guide has been written for IntelliJ IDEA 2021.2. Some details might differ in other versions. Please make sure to follow all steps accurately.
Importing Flink # Choose \u0026ldquo;New\u0026rdquo; → \u0026ldquo;Project from Existing Sources\u0026rdquo;. Select the root folder of the cloned Flink repository. Choose \u0026ldquo;Import project from external model\u0026rdquo; and select \u0026ldquo;Maven\u0026rdquo;. Leave the default options and successively click \u0026ldquo;Next\u0026rdquo; until you reach the SDK section. If there is no SDK listed, create one using the \u0026ldquo;+\u0026rdquo; sign on the top left. Select \u0026ldquo;JDK\u0026rdquo;, choose the JDK home directory and click \u0026ldquo;OK\u0026rdquo;. Select the most suitable JDK version. NOTE: A good rule of thumb is to select the JDK version matching the active Maven profile. Continue by clicking \u0026ldquo;Next\u0026rdquo; until the import is finished. Open the \u0026ldquo;Maven\u0026rdquo; tab (or right-click on the imported project and find \u0026ldquo;Maven\u0026rdquo;) and run \u0026ldquo;Generate Sources and Update Folders\u0026rdquo;. Alternatively, you can run mvn clean package -DskipTests. Build the Project (\u0026ldquo;Build\u0026rdquo; → \u0026ldquo;Build Project\u0026rdquo;). Copyright Profile # Every file needs to include the Apache license as a header. This can be automated in IntelliJ by adding a Copyright profile:
Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Editor\u0026rdquo; → \u0026ldquo;Copyright\u0026rdquo; → \u0026ldquo;Copyright Profiles\u0026rdquo;.
Add a new profile and name it \u0026ldquo;Apache\u0026rdquo;.
Add the following text as the license text:
Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Go to \u0026ldquo;Editor\u0026rdquo; → \u0026ldquo;Copyright\u0026rdquo; and choose the \u0026ldquo;Apache\u0026rdquo; profile as the default profile for this project.
Click \u0026ldquo;Apply\u0026rdquo;.
Required Plugins # Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Plugins\u0026rdquo; and select the \u0026ldquo;Marketplace\u0026rdquo; tab. Search for the following plugins, install them, and restart the IDE if prompted:
Scala Python – Required for PyFlink. If you do not intend to work on PyFlink, you can skip this. Save Actions Checkstyle-IDEA You will also need to install the google-java-format plugin. However, a specific version of this plugin is required. Download google-java-format v1.7.0.6 and install it as follows. Make sure to never update this plugin.
Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Plugins\u0026rdquo;. Click the gear icon and select \u0026ldquo;Install Plugin from Disk\u0026rdquo;. Navigate to the downloaded ZIP file and select it. Code Formatting # Flink uses Spotless together with google-java-format to format the Java code. For Scala, it uses Spotless with scalafmt.
It is recommended to automatically format your code by applying the following settings:
Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Other Settings\u0026rdquo; → \u0026ldquo;google-java-format Settings\u0026rdquo;. Tick the checkbox to enable the plugin. Change the code style to \u0026ldquo;Android Open Source Project (AOSP) style\u0026rdquo;. Go to \u0026ldquo;Settings\u0026rdquo; → Editor → Code Style → Scala. Change the \u0026ldquo;Formatter\u0026rdquo; to \u0026ldquo;scalafmt\u0026rdquo;. Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Tools\u0026rdquo; → \u0026ldquo;Actions on Save\u0026rdquo;. Under \u0026ldquo;Formatting Actions\u0026rdquo;, select \u0026ldquo;Optimize imports\u0026rdquo; and \u0026ldquo;Reformat file\u0026rdquo;. From the \u0026ldquo;All file types list\u0026rdquo; next to \u0026ldquo;Reformat code\u0026rdquo;, select Java and Scala. For earlier IntelliJ IDEA versions: 6. Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Other Settings\u0026rdquo; → \u0026ldquo;Save Actions\u0026rdquo;. 7. Under \u0026ldquo;General\u0026rdquo;, enable your preferred settings for when to format the code, e.g. \u0026ldquo;Activate save actions on save\u0026rdquo;. 8. Under \u0026ldquo;Formatting Actions\u0026rdquo;, select \u0026ldquo;Optimize imports\u0026rdquo; and \u0026ldquo;Reformat file\u0026rdquo;. 9. Under \u0026ldquo;File Path Inclusions\u0026rdquo;, add an entry for .*\\.java and .*\\.scala to avoid formatting other file types.
You can also format the whole project (both Java and Scala) via Maven by using mvn spotless:apply.
Checkstyle For Java # Checkstyle is used to enforce static coding guidelines.
Some modules are not covered by Checkstyle, e.g. flink-core, flink-optimizer, and flink-runtime. Nevertheless, please make sure to conform to the checkstyle rules in these modules if you work in any of these modules. Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Tools\u0026rdquo; → \u0026ldquo;Checkstyle\u0026rdquo;. Set \u0026ldquo;Scan Scope\u0026rdquo; to \u0026ldquo;Only Java sources (including tests)\u0026rdquo;. For \u0026ldquo;Checkstyle Version\u0026rdquo; select \u0026ldquo;8.14\u0026rdquo;. Under \u0026ldquo;Configuration File\u0026rdquo; click the \u0026ldquo;+\u0026rdquo; icon to add a new configuration. Set \u0026ldquo;Description\u0026rdquo; to \u0026ldquo;Flink\u0026rdquo;. Select \u0026ldquo;Use a local Checkstyle file\u0026rdquo; and point it to tools/maven/checkstyle.xml located within your cloned repository. Select \u0026ldquo;Store relative to project location\u0026rdquo; and click \u0026ldquo;Next\u0026rdquo;. Configure the property checkstyle.suppressions.file with the value suppressions.xml and click \u0026ldquo;Next\u0026rdquo;. Click \u0026ldquo;Finish\u0026rdquo;. Select \u0026ldquo;Flink\u0026rdquo; as the only active configuration file and click \u0026ldquo;Apply\u0026rdquo;. You can now import the Checkstyle configuration for the Java code formatter.
Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Editor\u0026rdquo; → \u0026ldquo;Code Style\u0026rdquo; → \u0026ldquo;Java\u0026rdquo;. Click the gear icon next to \u0026ldquo;Scheme\u0026rdquo; and select \u0026ldquo;Import Scheme\u0026rdquo; → \u0026ldquo;Checkstyle Configuration\u0026rdquo;. Navigate to and select tools/maven/checkstyle.xml located within your cloned repository. To verify the setup, click \u0026ldquo;View\u0026rdquo; → \u0026ldquo;Tool Windows\u0026rdquo; → \u0026ldquo;Checkstyle\u0026rdquo; and find the \u0026ldquo;Check Module\u0026rdquo; button in the opened tool window. It should report no violations.
Python for PyFlink # Working on the flink-python module requires both a Java SDK and a Python SDK. However, IntelliJ IDEA only supports one configured SDK per module. If you intend to work actively on PyFlink, it is recommended to import the flink-python module as a separate project either in PyCharm or IntelliJ IDEA for working with Python.
If you only occasionally need to work on flink-python and would like to get Python to work in IntelliJ IDEA, e.g. to run Python tests, you can use the following guide.
Follow Configure a virtual environment to create a new Virtualenv Python SDK in your Flink project. Find the flink-python module in the Project Explorer, right-click on it and choose \u0026ldquo;Open Module Settings\u0026rdquo;. Alternatively, go to \u0026ldquo;Project Structure\u0026rdquo; → \u0026ldquo;Modules\u0026rdquo; and find the module there. Change \u0026ldquo;Module SDK\u0026rdquo; to the Virtualenv Python SDK you created earlier. Open the file flink-python/setup.py and install the dependencies when IntelliJ prompts you to do so. You can verify your setup by running some of the Python tests located in flink-python.
Common Problems # This section lists issues that developers have run into in the past when working with IntelliJ.
Compilation fails with invalid flag: --add-exports=java.base/sun.net.util=ALL-UNNAMED # This happens if the \u0026ldquo;java11\u0026rdquo; Maven profile is active, but an older JDK version is used. Go to \u0026ldquo;View\u0026rdquo; → \u0026ldquo;Tool Windows\u0026rdquo; → \u0026ldquo;Maven\u0026rdquo; and uncheck the \u0026ldquo;java11\u0026rdquo; profile. Afterwards, reimport the project.
Compilation fails with cannot find symbol: symbol: method defineClass(...) location: class sun.misc.Unsafe # This happens if you are using JDK 11, but are working on a Flink version which doesn\u0026rsquo;t yet support Java 11 (\u0026lt;= 1.9). Go to \u0026ldquo;Project Structure\u0026rdquo; → \u0026ldquo;Project Settings\u0026rdquo; → \u0026ldquo;Project\u0026rdquo; and select JDK 8 as the Project SDK.
When switching back to newer Flink versions you may have to revert this change again.
Compilation fails with package sun.misc does not exist # This happens if you are using JDK 11 and compile to Java 8 with the --release option. This option is currently incompatible with our build setup. Go to \u0026ldquo;Settings\u0026rdquo; → \u0026ldquo;Build, Execution, Deployment\u0026rdquo; → \u0026ldquo;Compiler\u0026rdquo; → \u0026ldquo;Java Compiler\u0026rdquo; and uncheck the \u0026ldquo;Use \u0026lsquo;\u0026ndash;release\u0026rsquo; option for cross-compilation (Java 9 and later)\u0026rdquo;.
Examples fail with a NoClassDefFoundError for Flink classes. # This happens if Flink dependencies are set to \u0026ldquo;provided\u0026rdquo;, resulting in them not being available on the classpath. You can either check \u0026ldquo;Include dependencies with \u0026lsquo;Provided\u0026rsquo; scope\u0026rdquo; in your run configuration, or create a test that calls the main() method of the example.
Eclipse # Using Eclipse with Flink is currently not supported and discouraged. Please use IntelliJ IDEA instead.
PyCharm # If you intend to work on PyFlink, it is recommended to use PyCharm as a separate IDE for the flink-python module. The following guide has been written for 2019.1.3. Some details might differ in other versions.
Importing flink-python # Open the PyCharm IDE and choose (\u0026ldquo;File\u0026rdquo; →) \u0026ldquo;Open\u0026rdquo;. Select the \u0026ldquo;flink-python\u0026rdquo; folder within your located repository. Checkstyle For Python # Flake8 is used to enforce some coding guidelines.
Install flake8 for your Python interpreter using pip install flake8. In PyCharm go to \u0026ldquo;Preferences\u0026rdquo; → \u0026ldquo;Tools\u0026rdquo; → \u0026ldquo;External Tools\u0026rdquo;. Select the \u0026ldquo;+\u0026rdquo; button to add a new external tool. Set \u0026ldquo;Name\u0026rdquo; to \u0026ldquo;flake8\u0026rdquo;. Set \u0026ldquo;Description\u0026rdquo; to \u0026ldquo;Code Style Check\u0026rdquo;. Set \u0026ldquo;Program\u0026rdquo; to the path of your Python interpreter, e.g. /usr/bin/python. Set \u0026ldquo;Arguments\u0026rdquo; to -m flake8 --config=tox.ini. Set \u0026ldquo;Working Directory\u0026rdquo; to \$ProjectFileDir\$. You can verify the setup by right-clicking on any file or folder in the flink-python project and running \u0026ldquo;External Tools\u0026rdquo; → \u0026ldquo;flake8\u0026rdquo;.
Back to top
`}),e.add({id:125,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/json/",title:"JSON",section:"Formats",content:` Json format # To use the JSON format you need to add the Flink JSON dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-json\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading/writing JSON records via the JsonSerializationSchema/JsonDeserializationSchema. These utilize the Jackson library, and support any type that is supported by Jackson, including, but not limited to, POJOs and ObjectNode.
The JsonDeserializationSchema can be used with any connector that supports the DeserializationSchema.
For example, this is how you use it with a KafkaSource to deserialize a POJO:
JsonDeserializationSchema\u0026lt;SomePojo\u0026gt; jsonFormat=new JsonDeserializationSchema\u0026lt;\u0026gt;(SomePojo.class); KafkaSource\u0026lt;SomePojo\u0026gt; source= KafkaSource.\u0026lt;SomePojo\u0026gt;builder() .setValueOnlyDeserializer(jsonFormat) ... The JsonSerializationSchema can be used with any connector that supports the SerializationSchema.
For example, this is how you use it with a KafkaSink to serialize a POJO:
JsonSerializationSchema\u0026lt;SomePojo\u0026gt; jsonFormat=new JsonSerializationSchema\u0026lt;\u0026gt;(); KafkaSink\u0026lt;SomePojo\u0026gt; source = KafkaSink.\u0026lt;SomePojo\u0026gt;builder() .setRecordSerializer( new KafkaRecordSerializationSchemaBuilder\u0026lt;\u0026gt;() .setValueSerializationSchema(jsonFormat) ... Custom Mapper # Both schemas have constructors that accept a SerializableSupplier\u0026lt;ObjectMapper\u0026gt;, acting a factory for object mappers. With this factory you gain full control over the created mapper, and can enable/disable various Jackson features or register modules to extend the set of supported types or add additional functionality.
JsonSerializationSchema\u0026lt;SomeClass\u0026gt; jsonFormat=new JsonSerializationSchema\u0026lt;\u0026gt;( () -\u0026gt; new ObjectMapper() .enable(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS)) .registerModule(new ParameterNamesModule()); Python # In PyFlink, JsonRowSerializationSchema and JsonRowDeserializationSchema are built-in support for Row type. For example to use it in KafkaSource and KafkaSink:
row_type_info = Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;], [Types.STRING(), Types.INT()]) json_format = JsonRowDeserializationSchema.builder().type_info(row_type_info).build() source = KafkaSource.builder() \\ .set_value_only_deserializer(json_format) \\ .build() row_type_info = Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;], [Types.STRING(), Types.INT()]) json_format = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build() sink = KafkaSink.builder() \\ .set_record_serializer( KafkaRecordSerializationSchema.builder() .set_topic(\u0026#39;test\u0026#39;) .set_value_serialization_schema(json_format) .build() ) \\ .build() `}),e.add({id:126,href:"/flink/flink-docs-master/docs/libs/gelly/library_methods/",title:"Library Methods",section:"Graphs",content:` Library Methods # Gelly has a growing collection of graph algorithms for easily analyzing large-scale Graphs.
Gelly\u0026rsquo;s library methods can be used by simply calling the run() method on the input graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, NullValue\u0026gt; graph = ...; // run Label Propagation for 30 iterations to detect communities on the input graph DataSet\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithCommunity = graph.run(new LabelPropagation\u0026lt;Long\u0026gt;(30)); // print the result verticesWithCommunity.print(); Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph: Graph[java.lang.Long, java.lang.Long, NullValue] = ... // run Label Propagation for 30 iterations to detect communities on the input graph val verticesWithCommunity = graph.run(new LabelPropagation[java.lang.Long, java.lang.Long, NullValue](30)) // print the result verticesWithCommunity.print() Community Detection # Overview # In graph theory, communities refer to groups of nodes that are well connected internally, but sparsely connected to other groups. This library method is an implementation of the community detection algorithm described in the paper Towards real-time community detection in large networks.
Details # The algorithm is implemented using scatter-gather iterations. Initially, each vertex is assigned a Tuple2 containing its initial value along with a score equal to 1.0. In each iteration, vertices send their labels and scores to their neighbors. Upon receiving messages from its neighbors, a vertex chooses the label with the highest score and subsequently re-scores it using the edge values, a user-defined hop attenuation parameter, delta, and the superstep number. The algorithm converges when vertices no longer update their value or when the maximum number of iterations is reached.
Usage # The algorithm takes as input a Graph with any vertex type, Long vertex values, and Double edge values. It returns a Graph of the same type as the input, where the vertex values correspond to the community labels, i.e. two vertices belong to the same community if they have the same vertex value. The constructor takes two parameters:
maxIterations: the maximum number of iterations to run. delta: the hop attenuation parameter, with default value 0.5. Label Propagation # Overview # This is an implementation of the well-known Label Propagation algorithm described in this paper. The algorithm discovers communities in a graph, by iteratively propagating labels between neighbors. Unlike the Community Detection library method, this implementation does not use scores associated with the labels.
Details # The algorithm is implemented using scatter-gather iterations. Labels are expected to be of type Comparable and are initialized using the vertex values of the input Graph. The algorithm iteratively refines discovered communities by propagating labels. In each iteration, a vertex adopts the label that is most frequent among its neighbors\u0026rsquo; labels. In case of a tie (i.e. two or more labels appear with the same frequency), the algorithm picks the greater label. The algorithm converges when no vertex changes its value or the maximum number of iterations has been reached. Note that different initializations might lead to different results.
Usage # The algorithm takes as input a Graph with a Comparable vertex type, a Comparable vertex value type and an arbitrary edge value type. It returns a DataSet of vertices, where the vertex value corresponds to the community in which this vertex belongs after convergence. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. Connected Components # Overview # This is an implementation of the Weakly Connected Components algorithm. Upon convergence, two vertices belong to the same component, if there is a path from one to the other, without taking edge direction into account.
Details # The algorithm is implemented using scatter-gather iterations. This implementation uses a comparable vertex value as initial component identifier (ID). Vertices propagate their current value in each iteration. Upon receiving component IDs from its neighbors, a vertex adopts a new component ID if its value is lower than its current component ID. The algorithm converges when vertices no longer update their component ID value or when the maximum number of iterations has been reached.
Usage # The result is a DataSet of vertices, where the vertex value corresponds to the assigned component. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. GSA Connected Components # Overview # This is an implementation of the Weakly Connected Components algorithm. Upon convergence, two vertices belong to the same component, if there is a path from one to the other, without taking edge direction into account.
Details # The algorithm is implemented using gather-sum-apply iterations. This implementation uses a comparable vertex value as initial component identifier (ID). In the gather phase, each vertex collects the vertex value of their adjacent vertices. In the sum phase, the minimum among those values is selected. In the apply phase, the algorithm sets the minimum value as the new vertex value if it is smaller than the current value. The algorithm converges when vertices no longer update their component ID value or when the maximum number of iterations has been reached.
Usage # The result is a DataSet of vertices, where the vertex value corresponds to the assigned component. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. Single Source Shortest Paths # Overview # An implementation of the Single-Source-Shortest-Paths algorithm for weighted graphs. Given a source vertex, the algorithm computes the shortest paths from this source to all other nodes in the graph.
Details # The algorithm is implemented using scatter-gather iterations. In each iteration, a vertex sends to its neighbors a message containing the sum its current distance and the edge weight connecting this vertex with the neighbor. Upon receiving candidate distance messages, a vertex calculates the minimum distance and, if a shorter path has been discovered, it updates its value. If a vertex does not change its value during a superstep, then it does not produce messages for its neighbors for the next superstep. The computation terminates after the specified maximum number of supersteps or when there are no value updates.
Usage # The algorithm takes as input a Graph with any vertex type and Double edge values. The vertex values can be any type and are not used by this algorithm. The vertex type must implement equals(). The output is a DataSet of vertices where the vertex values correspond to the minimum distances from the given source vertex. The constructor takes two parameters:
srcVertexId The vertex ID of the source vertex. maxIterations: the maximum number of iterations to run. GSA Single Source Shortest Paths # The algorithm is implemented using gather-sum-apply iterations.
See the Single Source Shortest Paths library method for implementation details and usage information.
Triangle Enumerator # Overview # This library method enumerates unique triangles present in the input graph. A triangle consists of three edges that connect three vertices with each other. This implementation ignores edge directions.
Details # The basic triangle enumeration algorithm groups all edges that share a common vertex and builds triads, i.e., triples of vertices that are connected by two edges. Then, all triads are filtered for which no third edge exists that closes the triangle. For a group of n edges that share a common vertex, the number of built triads is quadratic ((n*(n-1))/2). Therefore, an optimization of the algorithm is to group edges on the vertex with the smaller output degree to reduce the number of triads. This implementation extends the basic algorithm by computing output degrees of edge vertices and grouping on edges on the vertex with the smaller degree.
Usage # The algorithm takes a directed graph as input and outputs a DataSet of Tuple3. The Vertex ID type has to be Comparable. Each Tuple3 corresponds to a triangle, with the fields containing the IDs of the vertices forming the triangle.
Summarization # Overview # The summarization algorithm computes a condensed version of the input graph by grouping vertices and edges based on their values. In doing so, the algorithm helps to uncover insights about patterns and distributions in the graph. One possible use case is the visualization of communities where the whole graph is too large and needs to be summarized based on the community identifier stored at a vertex.
Details # In the resulting graph, each vertex represents a group of vertices that share the same value. An edge, that connects a vertex with itself, represents all edges with the same edge value that connect vertices from the same vertex group. An edge between different vertices in the output graph represents all edges with the same edge value between members of different vertex groups in the input graph.
The algorithm is implemented using Flink data operators. First, vertices are grouped by their value and a representative is chosen from each group. For any edge, the source and target vertex identifiers are replaced with the corresponding representative and grouped by source, target and edge value. Output vertices and edges are created from their corresponding groupings.
Usage # The algorithm takes a directed, vertex (and possibly edge) attributed graph as input and outputs a new graph where each vertex represents a group of vertices and each edge represents a group of edges from the input graph. Furthermore, each vertex and edge in the output graph stores the common group value and the number of represented elements.
Clustering # Average Clustering Coefficient # Overview # The average clustering coefficient measures the mean connectedness of a graph. Scores range from 0.0 (no edges between neighbors) to 1.0 (complete graph).
Details # See the Local Clustering Coefficient library method for a detailed explanation of clustering coefficient. The Average Clustering Coefficient is the average of the Local Clustering Coefficient scores over all vertices with at least two neighbors. Each vertex, independent of degree, has equal weight for this score.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult containing the total number of vertices and average clustering coefficient of the graph. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Global Clustering Coefficient # Overview # The global clustering coefficient measures the connectedness of a graph. Scores range from 0.0 (no edges between neighbors) to 1.0 (complete graph).
Details # See the Local Clustering Coefficient library method for a detailed explanation of clustering coefficient. The Global Clustering Coefficient is the ratio of connected neighbors over the entire graph. Vertices with higher degrees have greater weight for this score because the count of neighbor pairs is quadratic in degree.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult containing the total number of triplets and triangles in the graph. The result class provides a method to compute the global clustering coefficient score. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Local Clustering Coefficient # Overview # The local clustering coefficient measures the connectedness of each vertex\u0026rsquo;s neighborhood. Scores range from 0.0 (no edges between neighbors) to 1.0 (neighborhood is a clique).
Details # An edge between neighbors of a vertex is a triangle. Counting edges between neighbors is equivalent to counting the number of triangles which include the vertex. The clustering coefficient score is the number of edges between neighbors divided by the number of potential edges between neighbors.
See the Triangle Listing library method for a detailed explanation of triangle enumeration.
Usage # Directed and undirected variants are provided. The algorithms take a simple graph as input and output a DataSet of UnaryResult containing the vertex ID, vertex degree, and number of triangles containing the vertex. The result class provides a method to compute the local clustering coefficient score. The graph ID type must be Comparable and Copyable.
setIncludeZeroDegreeVertices: include results for vertices with a degree of zero setParallelism: override the parallelism of operators processing small amounts of data Triadic Census # Overview # A triad is formed by any three vertices in a graph. Each triad contains three pairs of vertices which may be connected or unconnected. The Triadic Census counts the occurrences of each type of triad with the graph.
Details # This analytic counts the four undirected triad types (formed with 0, 1, 2, or 3 connecting edges) or 16 directed triad types by counting the triangles from Triangle Listing and running Vertex Metrics to obtain the number of triplets and edges. Triangle counts are then deducted from triplet counts, and triangle and triplet counts are removed from edge counts.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for querying the count of each triad type. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Triangle Listing # Overview # Enumerates all triangles in the graph. A triangle is composed of three edges connecting three vertices into cliques of size 3.
Details # Triangles are listed by joining open triplets (two edges with a common neighbor) against edges on the triplet endpoints. This implementation uses optimizations from Schank\u0026rsquo;s algorithm to improve performance with high-degree vertices. Triplets are generated from the lowest degree vertex since each triangle need only be listed once. This greatly reduces the number of generated triplets which is quadratic in vertex degree.
Usage # Directed and undirected variants are provided. The algorithms take a simple graph as input and output a DataSet of TertiaryResult containing the three triangle vertices and, for the directed algorithm, a bitmask marking each of the six potential edges connecting the three vertices. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data setSortTriangleVertices: normalize the triangle listing such that for each result (K0, K1, K2) the vertex IDs are sorted K0 \u0026lt; K1 \u0026lt; K2 Link Analysis # Hyperlink-Induced Topic Search # Overview # Hyperlink-Induced Topic Search (HITS, or \u0026ldquo;Hubs and Authorities\u0026rdquo;) computes two interdependent scores for every vertex in a directed graph. Good hubs are those which point to many good authorities and good authorities are those pointed to by many good hubs.
Details # Every vertex is assigned the same initial hub and authority scores. The algorithm then iteratively updates the scores until termination. During each iteration new hub scores are computed from the authority scores, then new authority scores are computed from the new hub scores. The scores are then normalized and optionally tested for convergence. HITS is similar to PageRank but vertex scores are emitted in full to each neighbor whereas in PageRank the vertex score is first divided by the number of neighbors.
Usage # The algorithm takes a simple directed graph as input and outputs a DataSet of UnaryResult containing the vertex ID, hub score, and authority score. Termination is configured by the number of iterations and/or a convergence threshold on the iteration sum of the change in scores over all vertices.
setIncludeZeroDegreeVertices: whether to include zero-degree vertices in the iterative computation setParallelism: override the operator parallelism PageRank # Overview # PageRank is an algorithm that was first used to rank web search engine results. Today, the algorithm and many variations are used in various graph application domains. The idea of PageRank is that important or relevant vertices tend to link to other important vertices.
Details # The algorithm operates in iterations, where pages distribute their scores to their neighbors (pages they have links to) and subsequently update their scores based on the sum of values they receive. In order to consider the importance of a link from one page to another, scores are divided by the total number of out-links of the source page. Thus, a page with 10 links will distribute 1/10 of its score to each neighbor, while a page with 100 links will distribute 1/100 of its score to each neighboring page.
Usage # The algorithm takes a directed graph as input and outputs a DataSet where each Result contains the vertex ID and PageRank score. Termination is configured with a maximum number of iterations and/or a convergence threshold on the sum of the change in score for each vertex between iterations.
setParallelism: override the operator parallelism Metric # Vertex Metrics # Overview # This graph analytic computes the following statistics for both directed and undirected graphs:
number of vertices number of edges average degree number of triplets maximum degree maximum number of triplets The following statistics are additionally computed for directed graphs:
number of unidirectional edges number of bidirectional edges maximum out degree maximum in degree Details # The statistics are computed over vertex degrees generated from degree.annotate.directed.VertexDegrees or degree.annotate.undirected.VertexDegree.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for the computed statistics. The graph ID type must be Comparable.
setIncludeZeroDegreeVertices: include results for vertices with a degree of zero setParallelism: override the operator parallelism setReduceOnTargetId (undirected only): the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID Edge Metrics # Overview # This graph analytic computes the following statistics:
number of triangle triplets number of rectangle triplets maximum number of triangle triplets maximum number of rectangle triplets Details # The statistics are computed over edge degrees generated from degree.annotate.directed.EdgeDegreesPair or degree.annotate.undirected.EdgeDegreePair and grouped by vertex.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for the computed statistics. The graph ID type must be Comparable.
setParallelism: override the operator parallelism setReduceOnTargetId (undirected only): the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID Similarity # Adamic-Adar # Overview # Adamic-Adar measures the similarity between pairs of vertices as the sum of the inverse logarithm of degree over shared neighbors. Scores are non-negative and unbounded. A vertex with higher degree has greater overall influence but is less influential to each pair of neighbors.
Details # The algorithm first annotates each vertex with the inverse of the logarithm of the vertex degree then joins this score onto edges by source vertex. Grouping on the source vertex, each pair of neighbors is emitted with the vertex score. Grouping on vertex pairs, the Adamic-Adar score is summed.
See the Jaccard Index library method for a similar algorithm.
Usage # The algorithm takes a simple undirected graph as input and outputs a DataSet of BinaryResult containing two vertex IDs and the Adamic-Adar similarity score. The graph ID type must be Copyable.
setMinimumRatio: filter out Adamic-Adar scores less than the given ratio times the average score setMinimumScore: filter out Adamic-Adar scores less than the given minimum setParallelism: override the parallelism of operators processing small amounts of data Jaccard Index # Overview # The Jaccard Index measures the similarity between vertex neighborhoods and is computed as the number of shared neighbors divided by the number of distinct neighbors. Scores range from 0.0 (no shared neighbors) to 1.0 (all neighbors are shared).
Details # Counting shared neighbors for pairs of vertices is equivalent to counting connecting paths of length two. The number of distinct neighbors is computed by storing the sum of degrees of the vertex pair and subtracting the count of shared neighbors, which are double-counted in the sum of degrees.
The algorithm first annotates each edge with the target vertex\u0026rsquo;s degree. Grouping on the source vertex, each pair of neighbors is emitted with the degree sum. Grouping on vertex pairs, the shared neighbors are counted.
Usage # The algorithm takes a simple undirected graph as input and outputs a DataSet of tuples containing two vertex IDs, the number of shared neighbors, and the number of distinct neighbors. The result class provides a method to compute the Jaccard Index score. The graph ID type must be Copyable.
setMaximumScore: filter out Jaccard Index scores greater than or equal to the given maximum fraction setMinimumScore: filter out Jaccard Index scores less than the given minimum fraction setParallelism: override the parallelism of operators processing small amounts of data Back to top
`}),e.add({id:127,href:"/flink/flink-docs-master/docs/deployment/advanced/logging/",title:"Logging",section:"Advanced",content:` How to use logging # All Flink processes create a log text file that contains messages for various events happening in that process. These logs provide deep insights into the inner workings of Flink, and can be used to detect problems (in the form of WARN/ERROR messages) and can help in debugging them.
The log files can be accessed via the Job-/TaskManager pages of the WebUI. The used Resource Provider (e.g., YARN) may provide additional means of accessing them.
The logging in Flink uses the SLF4J logging interface. This allows you to use any logging framework that supports SLF4J, without having to modify the Flink source code.
By default, Log4j 2 is used as the underlying logging framework.
Configuring Log4j 2 # Log4j 2 is controlled using property files.
The Flink distribution ships with the following log4j properties files in the conf directory, which are used automatically if Log4j 2 is enabled:
log4j-cli.properties: used by the command line interface (e.g., flink run) log4j-session.properties: used by the command line interface when starting a Kubernetes/Yarn session cluster (i.e., kubernetes-session.sh/yarn-session.sh) log4j-console.properties: used for Job-/TaskManagers if they are run in the foreground (e.g., Kubernetes) log4j.properties: used for Job-/TaskManagers by default Log4j periodically scans this file for changes and adjusts the logging behavior if necessary. By default this check happens every 30 seconds and is controlled by the monitorInterval setting in the Log4j properties files.
Compatibility with Log4j 1 # Flink ships with the Log4j API bridge, allowing existing applications that work against Log4j1 classes to continue working.
If you have custom Log4j 1 properties files or code that relies on Log4j 1, please check out the official Log4j compatibility and migration guides.
Configuring Log4j1 # To use Flink with Log4j 1 you must ensure that:
org.apache.logging.log4j:log4j-core, org.apache.logging.log4j:log4j-slf4j-impl and org.apache.logging.log4j:log4j-1.2-api are not on the classpath, log4j:log4j, org.slf4j:slf4j-log4j12, org.apache.logging.log4j:log4j-to-slf4j and org.apache.logging.log4j:log4j-api are on the classpath. In the IDE this means you have to replace such dependencies defined in your pom, and possibly add exclusions on dependencies that transitively depend on them.
For Flink distributions this means you have to
remove the log4j-core, log4j-slf4j-impl and log4j-1.2-api jars from the lib directory, add the log4j, slf4j-log4j12 and log4j-to-slf4j jars to the lib directory, replace all log4j properties files in the conf directory with Log4j1-compliant versions. Configuring logback # To use Flink with logback you must ensure that:
org.apache.logging.log4j:log4j-slf4j-impl is not on the classpath, ch.qos.logback:logback-core and ch.qos.logback:logback-classic are on the classpath. In the IDE this means you have to replace such dependencies defined in your pom, and possibly add exclusions on dependencies that transitively depend on them.
For Flink distributions this means you have to
remove the log4j-slf4j-impl jar from the lib directory, add the logback-core, and logback-classic jars to the lib directory. The Flink distribution ships with the following logback configuration files in the conf directory, which are used automatically if logback is enabled:
logback-session.properties: used by the command line interface when starting a Kubernetes/Yarn session cluster (i.e., kubernetes-session.sh/yarn-session.sh) logback-console.properties: used for Job-/TaskManagers if they are run in the foreground (e.g., Kubernetes) logback.xml: used for command line interface and Job-/TaskManagers by default Logback 1.3+ requires SLF4J 2, which is currently not supported. Best practices for developers # You can create an SLF4J logger by calling org.slf4j.LoggerFactory#LoggerFactory.getLogger with the Class of your class as an argument.
We highly recommend storing this logger in a private static final field.
import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class Foobar { private static final Logger LOG = LoggerFactory.getLogger(Foobar.class); public static void main(String[] args) { LOG.info(\u0026#34;Hello world!\u0026#34;); } } In order to benefit most from SLF4J, it is recommended to use its placeholder mechanism. Using placeholders allows avoiding unnecessary string constructions in case that the logging level is set so high that the message would not be logged.
The syntax of placeholders is the following:
LOG.info(\u0026#34;This message contains {} placeholders. {}\u0026#34;, 2, \u0026#34;Yippie\u0026#34;); Placeholders can also be used in conjunction with exceptions which shall be logged.
catch(Exception exception){ LOG.error(\u0026#34;An {} occurred.\u0026#34;, \u0026#34;error\u0026#34;, exception); } Back to top
`}),e.add({id:128,href:"/flink/flink-docs-master/docs/deployment/memory/",title:"Memory Configuration",section:"Deployment",content:""}),e.add({id:129,href:"/flink/flink-docs-master/docs/connectors/dataset/formats/azure_table_storage/",title:"Microsoft Azure table",section:"Formats",content:` Microsoft Azure Table Storage format # This example is using the HadoopInputFormat wrapper to use an existing Hadoop input format implementation for accessing Azure\u0026rsquo;s Table Storage.
Download and compile the azure-tables-hadoop project. The input format developed by the project is not yet available in Maven Central, therefore, we have to build the project ourselves. Execute the following commands: git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install Setup a new Flink project using the quickstarts: curl https://flink.apache.org/q/quickstart.sh | bash Add the following dependencies (in the \u0026lt;dependencies\u0026gt; section) to your pom.xml file: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;microsoft-hadoop-azure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; flink-hadoop-compatibility is a Flink package that provides the Hadoop input format wrappers. microsoft-hadoop-azure is adding the project we\u0026rsquo;ve build before to our project.
The project is now ready for starting to code. We recommend to import the project into an IDE, such as IntelliJ. You should import it as a Maven project. Browse to the file Job.java. This is an empty skeleton for a Flink job.
Paste the following code:
import java.util.Map; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataStream; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import com.microsoft.hadoop.azure.AzureTableConfiguration; import com.microsoft.hadoop.azure.AzureTableInputFormat; import com.microsoft.hadoop.azure.WritableEntity; import com.microsoft.windowsazure.storage.table.EntityProperty; public class AzureTableExample { public static void main(String[] args) throws Exception { // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a AzureTableInputFormat, using a Hadoop input format wrapper HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt; hdIf = new HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt;(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job()); // set the Account URI, something like: https://apacheflink.table.core.windows.net hdIf.getConfiguration().set(azuretableconfiguration.Keys.ACCOUNT_URI.getKey(), \u0026#34;TODO\u0026#34;); // set the secret storage key here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), \u0026#34;TODO\u0026#34;); // set the table name here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), \u0026#34;TODO\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt; input = env.createInput(hdIf); // a little example how to use the data in a mapper. DataSet\u0026lt;String\u0026gt; fin = input.map(new MapFunction\u0026lt;Tuple2\u0026lt;Text,WritableEntity\u0026gt;, String\u0026gt;() { @Override public String map(Tuple2\u0026lt;Text, WritableEntity\u0026gt; arg0) throws Exception { System.err.println(\u0026#34;--------------------------------\\nKey = \u0026#34;+arg0.f0); WritableEntity we = arg0.f1; for(Map.Entry\u0026lt;String, EntityProperty\u0026gt; prop : we.getProperties().entrySet()) { System.err.println(\u0026#34;key=\u0026#34;+prop.getKey() + \u0026#34; ; value (asString)=\u0026#34;+prop.getValue().getValueAsString()); } return arg0.f0.toString(); } }); // emit result (this works only locally) fin.print(); // execute program env.execute(\u0026#34;Azure Example\u0026#34;); } } The example shows how to access an Azure table and turn data into Flink\u0026rsquo;s DataSet (more specifically, the type of the set is DataSet\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt;). With the DataSet, you can apply all known transformations to the DataStream.
Back to top
`}),e.add({id:130,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/parquet/",title:"Parquet",section:"Formats",content:` Parquet format # Flink supports reading Parquet files, producing Flink RowData and producing Avro records. To use the format you need to add the flink-parquet dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-parquet\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; To read Avro records, you will need to add the parquet-avro dependency:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.parquet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;parquet-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.2\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;it.unimi.dsi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fastutil\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; In order to use the Parquet format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. This format is compatible with the new Source that can be used in both batch and streaming execution modes. Thus, you can use this format for two kinds of data:
Bounded data: lists all files and reads them all. Unbounded data: monitors a directory for new files that appear. When you start a File Source it is configured for bounded data by default. To configure the File Source for unbounded data, you must additionally call AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration). Vectorized reader
Java // Parquet rows are decoded in batches FileSource.forBulkFileFormat(BulkFormat,Path...) // Monitor the Paths to read data as unbounded data FileSource.forBulkFileFormat(BulkFormat,Path...) .monitorContinuously(Duration.ofMillis(5L)) .build(); Python # Parquet rows are decoded in batches FileSource.for_bulk_file_format(BulkFormat, Path...) # Monitor the Paths to read data as unbounded data FileSource.for_bulk_file_format(BulkFormat, Path...) \\ .monitor_continuously(Duration.of_millis(5)) \\ .build() Avro Parquet reader
Java // Parquet rows are decoded in batches FileSource.forRecordStreamFormat(StreamFormat,Path...) // Monitor the Paths to read data as unbounded data FileSource.forRecordStreamFormat(StreamFormat,Path...) .monitorContinuously(Duration.ofMillis(5L)) .build(); Python # Parquet rows are decoded in batches FileSource.for_record_stream_format(StreamFormat, Path...) # Monitor the Paths to read data as unbounded data FileSource.for_record_stream_format(StreamFormat, Path...) \\ .monitor_continuously(Duration.of_millis(5)) \\ .build() Following examples are all configured for bounded data. To configure the File Source for unbounded data, you must additionally call AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration). Flink RowData # In this example, you will create a DataStream containing Parquet records as Flink RowDatas. The schema is projected to read only the specified fields (\u0026ldquo;f7\u0026rdquo;, \u0026ldquo;f4\u0026rdquo; and \u0026ldquo;f99\u0026rdquo;).
Flink will read records in batches of 500 records. The first boolean parameter specifies that timestamp columns will be interpreted as UTC. The second boolean instructs the application that the projected Parquet fields names are case-sensitive. There is no watermark strategy defined as records do not contain event timestamps.
Java final LogicalType[] fieldTypes = new LogicalType[] { new DoubleType(), new IntType(), new VarCharType()}; final RowType rowType = RowType.of(fieldTypes, new String[] {\u0026#34;f7\u0026#34;, \u0026#34;f4\u0026#34;, \u0026#34;f99\u0026#34;}); final ParquetColumnarRowInputFormat\u0026lt;FileSourceSplit\u0026gt; format = new ParquetColumnarRowInputFormat\u0026lt;\u0026gt;( new Configuration(), rowType, InternalTypeInfo.of(rowType), 500, false, true); final FileSource\u0026lt;RowData\u0026gt; source = FileSource.forBulkFileFormat(format, /* Flink Path */) .build(); final DataStream\u0026lt;RowData\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python row_type = DataTypes.ROW([ DataTypes.FIELD(\u0026#39;f7\u0026#39;, DataTypes.DOUBLE()), DataTypes.FIELD(\u0026#39;f4\u0026#39;, DataTypes.INT()), DataTypes.FIELD(\u0026#39;f99\u0026#39;, DataTypes.VARCHAR()), ]) source = FileSource.for_bulk_file_format(ParquetColumnarRowInputFormat( row_type=row_type, hadoop_config=Configuration(), batch_size=500, is_utc_timestamp=False, is_case_sensitive=True, ), PARQUET_FILE_PATH).build() ds = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) Avro Records # Flink supports producing three types of Avro records by reading Parquet files (Only Generic record is supported in PyFlink):
Generic record Specific record Reflect record Generic record # Avro schemas are defined using JSON. You can get more information about Avro schemas and types from the Avro specification. This example uses an Avro schema example similar to the one described in the official Avro tutorial:
{\u0026#34;namespace\u0026#34;: \u0026#34;example.avro\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;int\u0026#34;, \u0026#34;null\u0026#34;]}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteColor\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;string\u0026#34;, \u0026#34;null\u0026#34;]} ] } This schema defines a record representing a user with three fields: name, favoriteNumber, and favoriteColor. You can find more details at record specification for how to define an Avro schema.
In the following example, you will create a DataStream containing Parquet records as Avro Generic records. It will parse the Avro schema based on the JSON string. There are many other ways to parse a schema, e.g. from java.io.File or java.io.InputStream. Please refer to Avro Schema for details. After that, you will create an AvroParquetRecordFormat via AvroParquetReaders for Avro Generic records.
Java // parsing avro schema final Schema schema = new Schema.Parser() .parse( \u0026#34;{\\\u0026#34;type\\\u0026#34;: \\\u0026#34;record\\\u0026#34;, \u0026#34; + \u0026#34;\\\u0026#34;name\\\u0026#34;: \\\u0026#34;User\\\u0026#34;, \u0026#34; + \u0026#34;\\\u0026#34;fields\\\u0026#34;: [\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;name\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: \\\u0026#34;string\\\u0026#34; },\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;favoriteNumber\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: [\\\u0026#34;int\\\u0026#34;, \\\u0026#34;null\\\u0026#34;] },\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;favoriteColor\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: [\\\u0026#34;string\\\u0026#34;, \\\u0026#34;null\\\u0026#34;] }\\n\u0026#34; + \u0026#34; ]\\n\u0026#34; + \u0026#34; }\u0026#34;); final FileSource\u0026lt;GenericRecord\u0026gt; source = FileSource.forRecordStreamFormat( AvroParquetReaders.forGenericRecord(schema), /* Flink Path */) .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(10L); final DataStream\u0026lt;GenericRecord\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python # parsing avro schema schema = AvroSchema.parse_string(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;int\u0026#34;, \u0026#34;null\u0026#34;]}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteColor\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;string\u0026#34;, \u0026#34;null\u0026#34;]} ] } \u0026#34;\u0026#34;\u0026#34;) source = FileSource.for_record_stream_format( AvroParquetReaders.for_generic_record(schema), # file paths ).build() env = StreamExecutionEnvironment.get_execution_environment() env.enable_checkpointing(10) stream = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) Specific record # Based on the previously defined schema, you can generate classes by leveraging Avro code generation. Once the classes have been generated, there is no need to use the schema directly in your programs. You can either use avro-tools.jar to generate code manually or you could use the Avro Maven plugin to perform code generation on any .avsc files present in the configured source directory. Please refer to Avro Getting Started for more information.
The following example uses the example schema testdata.avsc :
[ {\u0026#34;namespace\u0026#34;: \u0026#34;org.apache.flink.formats.parquet.generated\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Address\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;num\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;street\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;city\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;state\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;zip\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;} ] } ] You will use the Avro Maven plugin to generate the Address Java class:
@org.apache.avro.specific.AvroGenerated public class Address extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord { // generated code... } You will create an AvroParquetRecordFormat via AvroParquetReaders for Avro Specific record and then create a DataStream containing Parquet records as Avro Specific records.
final FileSource\u0026lt;GenericRecord\u0026gt; source = FileSource.forRecordStreamFormat( AvroParquetReaders.forSpecificRecord(Address.class), /* Flink Path */) .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(10L); final DataStream\u0026lt;GenericRecord\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Reflect record # Beyond Avro Generic and Specific record that requires a predefined Avro schema, Flink also supports creating a DataStream from Parquet files based on existing Java POJO classes. In this case, Avro will use Java reflection to generate schemas and protocols for these POJO classes. Java types are mapped to Avro schemas, please refer to the Avro reflect documentation for more details.
This example uses a simple Java POJO class Datum :
public class Datum implements Serializable { public String a; public int b; public Datum() {} public Datum(String a, int b) { this.a = a; this.b = b; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } Datum datum = (Datum) o; return b == datum.b \u0026amp;\u0026amp; (a != null ? a.equals(datum.a) : datum.a == null); } @Override public int hashCode() { int result = a != null ? a.hashCode() : 0; result = 31 * result + b; return result; } } You will create an AvroParquetRecordFormat via AvroParquetReaders for Avro Reflect record and then create a DataStream containing Parquet records as Avro Reflect records.
final FileSource\u0026lt;GenericRecord\u0026gt; source = FileSource.forRecordStreamFormat( AvroParquetReaders.forReflectRecord(Datum.class), /* Flink Path */) .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(10L); final DataStream\u0026lt;GenericRecord\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Prerequisite for Parquet files # In order to support reading Avro reflect records, the Parquet file must contain specific meta information. The Avro schema used for creating the Parquet data must contain a namespace, which will be used by the program to identify the concrete Java class for the reflection process.
The following example shows the User schema used previously. But this time it contains a namespace pointing to the location(in this case the package), where the User class for the reflection could be found.
// avro schema with namespace final String schema = \u0026#34;{\\\u0026#34;type\\\u0026#34;: \\\u0026#34;record\\\u0026#34;, \u0026#34; + \u0026#34;\\\u0026#34;name\\\u0026#34;: \\\u0026#34;User\\\u0026#34;, \u0026#34; + \u0026#34;\\\u0026#34;namespace\\\u0026#34;: \\\u0026#34;org.apache.flink.formats.parquet.avro\\\u0026#34;, \u0026#34; + \u0026#34;\\\u0026#34;fields\\\u0026#34;: [\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;name\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: \\\u0026#34;string\\\u0026#34; },\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;favoriteNumber\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: [\\\u0026#34;int\\\u0026#34;, \\\u0026#34;null\\\u0026#34;] },\\n\u0026#34; + \u0026#34; {\\\u0026#34;name\\\u0026#34;: \\\u0026#34;favoriteColor\\\u0026#34;, \\\u0026#34;type\\\u0026#34;: [\\\u0026#34;string\\\u0026#34;, \\\u0026#34;null\\\u0026#34;] }\\n\u0026#34; + \u0026#34; ]\\n\u0026#34; + \u0026#34; }\u0026#34;; Parquet files created with this schema will contain meta information like:
creator: parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94) extra: parquet.avro.schema = {\u0026#34;type\u0026#34;:\u0026#34;record\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;User\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;org.apache.flink.formats.parquet.avro\u0026#34;,\u0026#34;fields\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;favoriteNumber\u0026#34;,\u0026#34;type\u0026#34;:[\u0026#34;int\u0026#34;,\u0026#34;null\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;favoriteColor\u0026#34;,\u0026#34;type\u0026#34;:[\u0026#34;string\u0026#34;,\u0026#34;null\u0026#34;]}]} extra: writer.model.name = avro file schema: org.apache.flink.formats.parquet.avro.User -------------------------------------------------------------------------------- name: REQUIRED BINARY L:STRING R:0 D:0 favoriteNumber: OPTIONAL INT32 R:0 D:1 favoriteColor: OPTIONAL BINARY L:STRING R:0 D:1 row group 1: RC:3 TS:143 OFFSET:4 -------------------------------------------------------------------------------- name: BINARY UNCOMPRESSED DO:0 FPO:4 SZ:47/47/1.00 VC:3 ENC:PLAIN,BIT_PACKED ST:[min: Jack, max: Tom, num_nulls: 0] favoriteNumber: INT32 UNCOMPRESSED DO:0 FPO:51 SZ:41/41/1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 1, max: 3, num_nulls: 0] favoriteColor: BINARY UNCOMPRESSED DO:0 FPO:92 SZ:55/55/1.00 VC:3 ENC:RLE,PLAIN,BIT_PACKED ST:[min: green, max: yellow, num_nulls: 0] With the User class defined in the package org.apache.flink.formats.parquet.avro:
public class User { private String name; private Integer favoriteNumber; private String favoriteColor; public User() {} public User(String name, Integer favoriteNumber, String favoriteColor) { this.name = name; this.favoriteNumber = favoriteNumber; this.favoriteColor = favoriteColor; } public String getName() { return name; } public Integer getFavoriteNumber() { return favoriteNumber; } public String getFavoriteColor() { return favoriteColor; } } you can write the following program to read Avro Reflect records of User type from parquet files:
final FileSource\u0026lt;GenericRecord\u0026gt; source = FileSource.forRecordStreamFormat( AvroParquetReaders.forReflectRecord(User.class), /* Flink Path */) .build(); final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(10L); final DataStream\u0026lt;GenericRecord\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); `}),e.add({id:131,href:"/flink/flink-docs-master/docs/dev/datastream/operators/process_function/",title:"Process Function",section:"Operators",content:` Process Function # The ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).
For fault-tolerant state, the ProcessFunction gives access to Flink\u0026rsquo;s keyed state, accessible via the RuntimeContext, similar to the way other stateful functions can access keyed state.
The timers allow applications to react to changes in processing time and in event time. Every call to the function processElement(...) gets a Context object which gives access to the element\u0026rsquo;s event time timestamp, and to the TimerService. The TimerService can be used to register callbacks for future event-/processing-time instants. With event-time timers, the onTimer(...) method is called when the current watermark is advanced up to or beyond the timestamp of the timer, while with processing-time timers, onTimer(...) is called when wall clock time reaches the specified time. During that call, all states are again scoped to the key with which the timer was created, allowing timers to manipulate keyed state.
If you want to access keyed state and timers you have to apply the ProcessFunction on a keyed stream: stream.keyBy(...).process(new MyProcessFunction()); Low-level Joins # To realize low-level operations on two inputs, applications can use CoProcessFunction or KeyedCoProcessFunction. This function is bound to two different inputs and gets individual calls to processElement1(...) and processElement2(...) for records from the two different inputs.
Implementing a low level join typically follows this pattern:
Create a state object for one input (or both) Update the state upon receiving elements from its input Upon receiving elements from the other input, probe the state and produce the joined result For example, you might be joining customer data to financial trades, while keeping state for the customer data. If you care about having complete and deterministic joins in the face of out-of-order events, you can use a timer to evaluate and emit the join for a trade when the watermark for the customer data stream has passed the time of that trade.
Example # In the following example a KeyedProcessFunction maintains counts per key, and emits a key/count pair whenever a minute passes (in event time) without an update for that key:
The count, key, and last-modification-timestamp are stored in a ValueState, which is implicitly scoped by key. For each record, the KeyedProcessFunction increments the counter and sets the last-modification timestamp The function also schedules a callback one minute into the future (in event time) Upon each callback, it checks the callback\u0026rsquo;s event time timestamp against the last-modification time of the stored count and emits the key/count if they match (i.e., no further update occurred during that minute) This simple example could have been implemented with session windows. We use KeyedProcessFunction here to illustrate the basic pattern it provides. Java import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.streaming.api.functions.KeyedProcessFunction.Context; import org.apache.flink.streaming.api.functions.KeyedProcessFunction.OnTimerContext; import org.apache.flink.util.Collector; // the source data stream DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = ...; // apply the process function onto a keyed stream DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; result = stream .keyBy(value -\u0026gt; value.f0) .process(new CountWithTimeoutFunction()); /** * The data type stored in the state */ public class CountWithTimestamp { public String key; public long count; public long lastModified; } /** * The implementation of the ProcessFunction that maintains the count and timeouts */ public class CountWithTimeoutFunction extends KeyedProcessFunction\u0026lt;Tuple, Tuple2\u0026lt;String, String\u0026gt;, Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; { /** The state that is maintained by this process function */ private ValueState\u0026lt;CountWithTimestamp\u0026gt; state; @Override public void open(Configuration parameters) throws Exception { state = getRuntimeContext().getState(new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;myState\u0026#34;, CountWithTimestamp.class)); } @Override public void processElement( Tuple2\u0026lt;String, String\u0026gt; value, Context ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { // retrieve the current count CountWithTimestamp current = state.value(); if (current == null) { current = new CountWithTimestamp(); current.key = value.f0; } // update the state\u0026#39;s count current.count++; // set the state\u0026#39;s timestamp to the record\u0026#39;s assigned event time timestamp current.lastModified = ctx.timestamp(); // write the state back state.update(current); // schedule the next timer 60 seconds from the current event time ctx.timerService().registerEventTimeTimer(current.lastModified + 60000); } @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { // get the state for the key that scheduled the timer CountWithTimestamp result = state.value(); // check if this is an outdated timer or the latest timer if (timestamp == result.lastModified + 60000) { // emit the state on timeout out.collect(new Tuple2\u0026lt;String, Long\u0026gt;(result.key, result.count)); } } } Scala import org.apache.flink.api.common.state.ValueState import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.java.tuple.Tuple import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector // the source data stream val stream: DataStream[Tuple2[String, String]] = ... // apply the process function onto a keyed stream val result: DataStream[Tuple2[String, Long]] = stream .keyBy(_._1) .process(new CountWithTimeoutFunction()) /** * The data type stored in the state */ case class CountWithTimestamp(key: String, count: Long, lastModified: Long) /** * The implementation of the ProcessFunction that maintains the count and timeouts */ class CountWithTimeoutFunction extends KeyedProcessFunction[Tuple, (String, String), (String, Long)] { /** The state that is maintained by this process function */ lazy val state: ValueState[CountWithTimestamp] = getRuntimeContext .getState(new ValueStateDescriptor[CountWithTimestamp](\u0026#34;myState\u0026#34;, classOf[CountWithTimestamp])) override def processElement( value: (String, String), ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#Context, out: Collector[(String, Long)]): Unit = { // initialize or retrieve/update the state val current: CountWithTimestamp = state.value match { case null =\u0026gt; CountWithTimestamp(value._1, 1, ctx.timestamp) case CountWithTimestamp(key, count, lastModified) =\u0026gt; CountWithTimestamp(key, count + 1, ctx.timestamp) } // write the state back state.update(current) // schedule the next timer 60 seconds from the current event time ctx.timerService.registerEventTimeTimer(current.lastModified + 60000) } override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#OnTimerContext, out: Collector[(String, Long)]): Unit = { state.value match { case CountWithTimestamp(key, count, lastModified) if (timestamp == lastModified + 60000) =\u0026gt; out.collect((key, count)) case _ =\u0026gt; } } } Python import datetime from pyflink.common import Row, WatermarkStrategy from pyflink.common.typeinfo import Types from pyflink.common.watermark_strategy import TimestampAssigner from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor from pyflink.table import StreamTableEnvironment class CountWithTimeoutFunction(KeyedProcessFunction): def __init__(self): self.state = None def open(self, runtime_context: RuntimeContext): self.state = runtime_context.get_state(ValueStateDescriptor( \u0026#34;my_state\u0026#34;, Types.PICKLED_BYTE_ARRAY())) def process_element(self, value, ctx: \u0026#39;KeyedProcessFunction.Context\u0026#39;): # retrieve the current count current = self.state.value() if current is None: current = Row(value.f1, 0, 0) # update the state\u0026#39;s count current[1] += 1 # set the state\u0026#39;s timestamp to the record\u0026#39;s assigned event time timestamp current[2] = ctx.timestamp() # write the state back self.state.update(current) # schedule the next timer 60 seconds from the current event time ctx.timer_service().register_event_time_timer(current[2] + 60000) def on_timer(self, timestamp: int, ctx: \u0026#39;KeyedProcessFunction.OnTimerContext\u0026#39;): # get the state for the key that scheduled the timer result = self.state.value() # check if this is an outdated timer or the latest timer if timestamp == result[2] + 60000: # emit the state on timeout yield result[0], result[1] class MyTimestampAssigner(TimestampAssigner): def __init__(self): self.epoch = datetime.datetime.utcfromtimestamp(0) def extract_timestamp(self, value, record_timestamp) -\u0026gt; int: return int((value[0] - self.epoch).total_seconds() * 1000) if __name__ == \u0026#39;__main__\u0026#39;: env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a TIMESTAMP(3), b VARCHAR, c VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;rows-per-second\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) stream = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.SQL_TIMESTAMP(), Types.STRING(), Types.STRING()])) watermarked_stream = stream.assign_timestamps_and_watermarks( WatermarkStrategy.for_monotonous_timestamps() .with_timestamp_assigner(MyTimestampAssigner())) # apply the process function onto a keyed stream result = watermarked_stream.key_by(lambda value: value[1]) \\ .process(CountWithTimeoutFunction()) \\ .print() env.execute() Before Flink 1.4.0, when called from a processing-time timer, the ProcessFunction.onTimer() method sets the current processing time as event-time timestamp. This behavior is very subtle and might not be noticed by users. Well, it\u0026rsquo;s harmful because processing-time timestamps are indeterministic and not aligned with watermarks. Besides, user-implemented logic depends on this wrong timestamp highly likely is unintendedly faulty. So we\u0026rsquo;ve decided to fix it. Upon upgrading to 1.4.0, Flink jobs that are using this incorrect event-time timestamp will fail, and users should adapt their jobs to the correct logic. The KeyedProcessFunction # KeyedProcessFunction, as an extension of ProcessFunction, gives access to the key of timers in its onTimer(...) method.
Java @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception { K key = ctx.getCurrentKey(); // ... } Scala override def onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT]): Unit = { var key = ctx.getCurrentKey // ... } Python def on_timer(self, timestamp: int, ctx: \u0026#39;KeyedProcessFunction.OnTimerContext\u0026#39;): key = ctx.get_current_key() # ... Timers # Both types of timers (processing-time and event-time) are internally maintained by the TimerService and enqueued for execution.
The TimerService deduplicates timers per key and timestamp, i.e., there is at most one timer per key and timestamp. If multiple timers are registered for the same timestamp, the onTimer() method will be called just once.
Flink synchronizes invocations of onTimer() and processElement(). Hence, users do not have to worry about concurrent modification of state.
Fault Tolerance # Timers are fault tolerant and checkpointed along with the state of the application. In case of a failure recovery or when starting an application from a savepoint, the timers are restored.
Checkpointed processing-time timers that were supposed to fire before their restoration, will fire immediately. This might happen when an application recovers from a failure or when it is started from a savepoint. Timers are always asynchronously checkpointed, except for the combination of RocksDB backend / with incremental snapshots / with heap-based timers (will be resolved with FLINK-10026). Notice that large numbers of timers can increase the checkpointing time because timers are part of the checkpointed state. See the \u0026ldquo;Timer Coalescing\u0026rdquo; section for advice on how to reduce the number of timers. Timer Coalescing # Since Flink maintains only one timer per key and timestamp, you can reduce the number of timers by reducing the timer resolution to coalesce them.
For a timer resolution of 1 second (event or processing time), you can round down the target time to full seconds. Timers will fire at most 1 second earlier but not later than requested with millisecond accuracy. As a result, there are at most one timer per key and second.
Java long coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000; ctx.timerService().registerProcessingTimeTimer(coalescedTime); Scala val coalescedTime = ((ctx.timestamp + timeout) / 1000) * 1000 ctx.timerService.registerProcessingTimeTimer(coalescedTime) Python coalesced_time = ((ctx.timestamp() + timeout) // 1000) * 1000 ctx.timer_service().register_processing_time_timer(coalesced_time) Since event-time timers only fire with watermarks coming in, you may also schedule and coalesce these timers with the next watermark by using the current one:
Java long coalescedTime = ctx.timerService().currentWatermark() + 1; ctx.timerService().registerEventTimeTimer(coalescedTime); Scala val coalescedTime = ctx.timerService.currentWatermark + 1 ctx.timerService.registerEventTimeTimer(coalescedTime) Python coalesced_time = ctx.timer_service().current_watermark() + 1 ctx.timer_service().register_event_time_timer(coalesced_time) Timers can also be stopped and removed as follows:
Stopping a processing-time timer:
Java long timestampOfTimerToStop = ...; ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop); Scala val timestampOfTimerToStop = ... ctx.timerService.deleteProcessingTimeTimer(timestampOfTimerToStop) Python timestamp_of_timer_to_stop = ... ctx.timer_service().delete_processing_time_timer(timestamp_of_timer_to_stop) Stopping an event-time timer:
Java long timestampOfTimerToStop = ...; ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop); Scala val timestampOfTimerToStop = ... ctx.timerService.deleteEventTimeTimer(timestampOfTimerToStop) Python timestamp_of_timer_to_stop = ... ctx.timer_service().delete_event_time_timer(timestamp_of_timer_to_stop) Stopping a timer has no effect if no such timer with the given timestamp is registered. Back to top
`}),e.add({id:132,href:"/flink/flink-docs-master/docs/dev/python/datastream/operators/process_function/",title:"Process Function",section:"Operators",content:` Process Function # ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).
Please refer to Process Function for more details about the concept and usage of ProcessFunction.
Execution behavior of timer # Python user-defined functions are executed in a separate Python process from Flink\u0026rsquo;s operators which run in a JVM, the timer registration requests made in ProcessFunction will be sent to the Java operator asynchronously. Once received timer registration requests, the Java operator will register it into the underlying timer service.
If the registered timer has already passed the current time (the current system time for processing time timer, or the current watermark for event time), it will be triggered immediately.
Note that, due to the asynchronous processing characteristics, it may happen that the timer was triggered a little later than the actual time. For example, a registered processing time timer of 10:00:00 may be actually processed at 10:00:05.
`}),e.add({id:133,href:"/flink/flink-docs-master/docs/connectors/table/formats/protobuf/",title:"Protobuf",section:"Formats",content:` Protobuf Format # Format: Serialization Schema Format: Deserialization Schema
The Protocol Buffers Protobuf format allows you to read and write Protobuf data, based on Protobuf generated classes.
Dependencies # In order to use the Protobuf format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-protobuf\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a table with Protobuf format # Here is an example to create a table using the Kafka connector and Protobuf format.
Below is the proto definition file.
syntax = \u0026#34;proto2\u0026#34;; package com.example; option java_package = \u0026#34;com.example\u0026#34;; option java_multiple_files = true; message SimpleTest { optional int64 uid = 1; optional string name = 2; optional int32 category_type = 3; optional bytes content = 4; optional double price = 5; map\u0026lt;int64, InnerMessageTest\u0026gt; value_map = 6; repeated InnerMessageTest value_arr = 7; optional Corpus corpus_int = 8; optional Corpus corpus_str = 9; message InnerMessageTest{ optional int64 v1 =1; optional int32 v2 =2; } enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 7; } } Use protoc command to compile the .proto file to java classes Then compile and package the classes (there is no need to package proto-java into the jar) Finally you should provide the jar in your classpath, e.g. pass it using -j in sql-client CREATE TABLE simple_test ( uid BIGINT, name STRING, category_type INT, content BINARY, price DOUBLE, value_map map\u0026lt;BIGINT, row\u0026lt;v1 BIGINT, v2 INT\u0026gt;\u0026gt;, value_arr array\u0026lt;row\u0026lt;v1 BIGINT, v2 INT\u0026gt;\u0026gt;, corpus_int INT, corpus_str STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;protobuf\u0026#39;, \u0026#39;protobuf.message-class-name\u0026#39; = \u0026#39;com.example.SimpleTest\u0026#39;, \u0026#39;protobuf.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'protobuf'. protobuf.message-class-name required no (none) String The full name of a Protobuf generated class. The name must match the message name in the proto definition file. \$ is supported for inner class names, like 'com.exmample.OuterClass\$MessageClass' protobuf.ignore-parse-errors optional no false Boolean Optional flag to skip rows with parse errors instead of failing. protobuf.read-default-values optional yes false Boolean This option only works if the generated class's version is proto2. If this value is set to true, the format will read empty values as the default values defined in the proto file. If the value is set to false, the format will generate null values if the data element does not exist in the binary protobuf message. If the proto syntax is proto3, this value will forcibly be set to true, because proto3's standard is to use default values. protobuf.write-null-string-literal optional no "" String When serializing to protobuf data, this is the optional config to specify the string literal in Protobuf's array/map in case of null values. Data Type Mapping # The following table lists the type mapping from Flink type to Protobuf type.
Flink SQL type Protobuf type Description CHAR / VARCHAR / STRING string BOOLEAN bool BINARY / VARBINARY bytes INT int32 BIGINT int64 FLOAT float DOUBLE double ARRAY repeated Elements cannot be null, the string default value can be specified by write-null-string-literal MAP map Keys or values cannot be null, the string default value can be specified by write-null-string-literal ROW message VARCHAR / CHAR / TINYINT / SMALLINT / INTEGER / BIGINT enum The enum value of protobuf can be mapped to string or number of flink row accordingly. Null Values # As protobuf does not permit null values in maps and array, we need to auto-generate default values when converting from Flink Rows to Protobuf.
Protobuf Data Type Default Value int32 / int64 / float / double 0 string "" bool false enum first element of enum binary ByteString.EMPTY message MESSAGE.getDefaultInstance() OneOf field # In the serialization process, there\u0026rsquo;s no guarantee that the Flink fields of the same one-of group only contain at most one valid value. When serializing, each field is set in the order of Flink schema, so the field in the higher position will override the field in lower position in the same one-of group.
You can refer to Language Guide (proto2) or Language Guide (proto3) for more information about Protobuf types.
`}),e.add({id:134,href:"/flink/flink-docs-master/docs/dev/python/",title:"Python API",section:"Application Development",content:" "}),e.add({id:135,href:"/flink/flink-docs-master/docs/try-flink/table_api/",title:"Real Time Reporting with the Table API",section:"Try Flink",content:` Real Time Reporting with the Table API # Apache Flink offers a Table API as a unified, relational API for batch and stream processing, i.e., queries are executed with the same semantics on unbounded, real-time streams or bounded, batch data sets and produce the same results. The Table API in Flink is commonly used to ease the definition of data analytics, data pipelining, and ETL applications.
What Will You Be Building? # In this tutorial, you will learn how to build a real-time dashboard to track financial transactions by account. The pipeline will read data from Kafka and write the results to MySQL visualized via Grafana.
Prerequisites # This walkthrough assumes that you have some familiarity with Java or Scala, but you should be able to follow along even if you come from a different programming language. It also assumes that you are familiar with basic relational concepts such as SELECT and GROUP BY clauses.
Help, I’m Stuck! # If you get stuck, check out the community support resources. In particular, Apache Flink\u0026rsquo;s user mailing list consistently ranks as one of the most active of any Apache project and a great way to get help quickly.
If running docker on Windows and your data generator container is failing to start, then please ensure that you\u0026rsquo;re using the right shell. For example docker-entrypoint.sh for table-walkthrough_data-generator_1 container requires bash. If unavailable, it will throw an error standard_init_linux.go:211: exec user process caused \u0026ldquo;no such file or directory\u0026rdquo;. A workaround is to switch the shell to sh on the first line of docker-entrypoint.sh. How To Follow Along # If you want to follow along, you will require a computer with:
Java 11 Maven Docker Attention: The Apache Flink Docker images used for this playground are only available for released versions of Apache Flink.
Since you are currently looking at the latest SNAPSHOT version of the documentation, all version references below will not work. Please switch the documentation to the latest released version via the release picker which you find on the left side below the menu.
The required configuration files are available in the flink-playgrounds repository. Once downloaded, open the project flink-playground/table-walkthrough in your IDE and navigate to the file SpendReport.
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); Table transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); Breaking Down The Code # The Execution Environment # The first two lines set up your TableEnvironment. The table environment is how you can set properties for your Job, specify whether you are writing a batch or a streaming application, and create your sources. This walkthrough creates a standard table environment that uses the streaming execution.
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); Registering Tables # Next, tables are registered in the current catalog that you can use to connect to external systems for reading and writing both batch and streaming data. A table source provides access to data stored in external systems, such as a database, a key-value store, a message queue, or a file system. A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, JSON, Avro, or Parquet.
tEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); Two tables are registered; a transaction input table, and a spend report output table. The transactions (transactions) table lets us read credit card transactions, which contain account ID\u0026rsquo;s (account_id), timestamps (transaction_time), and US\$ amounts (amount). The table is a logical view over a Kafka topic called transactions containing CSV data.
tEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); The second table, spend_report, stores the final results of the aggregation. Its underlying storage is a table in a MySql database.
The Query # With the environment configured and tables registered, you are ready to build your first application. From the TableEnvironment you can read from an input table to read its rows and then write those results into an output table using executeInsert. The report function is where you will implement your business logic. It is currently unimplemented.
Table transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); Testing # The project contains a secondary testing class SpendReportTest that validates the logic of the report. It creates a table environment in batch mode.
EnvironmentSettings settings = EnvironmentSettings.inBatchMode(); TableEnvironment tEnv = TableEnvironment.create(settings); One of Flink\u0026rsquo;s unique properties is that it provides consistent semantics across batch and streaming. This means you can develop and test applications in batch mode on static datasets, and deploy to production as streaming applications.
Attempt One # Now with the skeleton of a Job set-up, you are ready to add some business logic. The goal is to build a report that shows the total spend for each account across each hour of the day. This means the timestamp column needs be be rounded down from millisecond to hour granularity.
Flink supports developing relational applications in pure SQL or using the Table API. The Table API is a fluent DSL inspired by SQL, that can be written in Python, Java, or Scala and supports strong IDE integration. Just like a SQL query, Table programs can select the required fields and group by your keys. These features, along with built-in functions like floor and sum, enable you to write this report.
public static Table report(Table transactions) { return transactions.select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;transaction_time\u0026#34;).floor(TimeIntervalUnit.HOUR).as(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } User Defined Functions # Flink contains a limited number of built-in functions, and sometimes you need to extend it with a user-defined function. If floor wasn\u0026rsquo;t predefined, you could implement it yourself.
import java.time.LocalDateTime; import java.time.temporal.ChronoUnit; import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.functions.ScalarFunction; public class MyFloor extends ScalarFunction { public @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime eval( @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime timestamp) { return timestamp.truncatedTo(ChronoUnit.HOURS); } } And then quickly integrate it in your application.
public static Table report(Table transactions) { return transactions.select( \$(\u0026#34;account_id\u0026#34;), call(MyFloor.class, \$(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } This query consumes all records from the transactions table, calculates the report, and outputs the results in an efficient, scalable manner. Running the test with this implementation will pass.
Adding Windows # Grouping data based on time is a typical operation in data processing, especially when working with infinite streams. A grouping based on time is called a window and Flink offers flexible windowing semantics. The most basic type of window is called a Tumble window, which has a fixed size and whose buckets do not overlap.
public static Table report(Table transactions) { return transactions .window(Tumble.over(lit(1).hour()).on(\$(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \`(\u0026#34;log_ts\u0026#34;).start().as(\u0026#34;log_ts\u0026#34;), \`(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } This defines your application as using one hour tumbling windows based on the timestamp column. So a row with timestamp 2019-06-01 01:23:47 is put in the 2019-06-01 01:00:00 window.
Aggregations based on time are unique because time, as opposed to other attributes, generally moves forward in a continuous streaming application. Unlike floor and your UDF, window functions are intrinsics, which allows the runtime to apply additional optimizations. In a batch context, windows offer a convenient API for grouping records by a timestamp attribute.
Running the test with this implementation will also pass.
Once More, With Streaming! # And that\u0026rsquo;s it, a fully functional, stateful, distributed streaming application! The query continuously consumes the stream of transactions from Kafka, computes the hourly spendings, and emits results as soon as they are ready. Since the input is unbounded, the query keeps running until it is manually stopped. And because the Job uses time window-based aggregations, Flink can perform specific optimizations such as state clean up when the framework knows that no more records will arrive for a particular window.
The table playground is fully dockerized and runnable locally as streaming application. The environment contains a Kafka topic, a continuous data generator, MySql, and Grafana.
From within the table-walkthrough folder start the docker-compose script.
\$ docker-compose build \$ docker-compose up -d You can see information on the running job via the Flink console.
Explore the results from inside MySQL.
\$ docker-compose exec mysql mysql -Dsql-demo -usql-demo -pdemo-sql mysql\u0026gt; use sql-demo; Database changed mysql\u0026gt; select count(*) from spend_report; +----------+ | count(*) | +----------+ | 110 | +----------+ Finally, go to Grafana to see the fully visualized result!
`}),e.add({id:136,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/select/",title:"SELECT \u0026 WHERE",section:"Queries",content:` SELECT \u0026amp; WHERE clause # Batch Streaming
The general syntax of the SELECT statement is:
SELECT select_list FROM table_expression [ WHERE boolean_expression ] The table_expression refers to any source of data. It could be an existing table, view, or VALUES clause, the joined results of multiple existing tables, or a subquery. Assuming that the table is available in the catalog, the following would read all rows from Orders.
SELECT * FROM Orders The select_list specification * means the query will resolve all columns. However, usage of * is discouraged in production because it makes queries less robust to catalog changes. Instead, a select_list can specify a subset of available columns or make calculations using said columns. For example, if Orders has columns named order_id, price, and tax you could write the following query:
SELECT order_id, price + tax FROM Orders Queries can also consume from inline data using the VALUES clause. Each tuple corresponds to one row and an alias may be provided to assign names to each column.
SELECT order_id, price FROM (VALUES (1, 2.0), (2, 3.1)) AS t (order_id, price) Rows can be filtered based on a WHERE clause.
SELECT price + tax FROM Orders WHERE id = 10 Additionally, built-in and user-defined scalar functions can be invoked on the columns of a single row. User-defined functions must be registered in a catalog before use.
SELECT PRETTY_PRINT(order_id) FROM Orders Back to top
`}),e.add({id:137,href:"/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/",title:"Set up JobManager Memory",section:"Memory Configuration",content:` Set up JobManager Memory # The JobManager is the controlling element of the Flink Cluster. It consists of three distinct components: Resource Manager, Dispatcher and one JobMaster per running Flink Job. This guide walks you through high level and fine-grained memory configurations for the JobManager.
The further described memory configuration is applicable starting with the release version 1.11. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.11 release.
This memory setup guide is relevant only for the JobManager! The JobManager memory components have a similar but simpler structure compared to the TaskManagers\u0026rsquo; memory configuration. Configure Total Memory # The simplest way to set up the memory configuration is to configure the total memory for the process. If you run the JobManager process using local execution mode you do not need to configure memory options, they will have no effect.
Detailed configuration # The following table lists all memory components, depicted above, and references Flink configuration options which affect the size of the respective components:
Component Configuration options Description JVM Heap jobmanager.memory.heap.size JVM Heap memory size for job manager. Off-heap Memory jobmanager.memory.off-heap.size Off-heap memory size for job manager. This option covers all off-heap memory usage including direct and native memory allocation. JVM metaspace jobmanager.memory.jvm-metaspace.size Metaspace size of the Flink JVM process JVM Overhead jobmanager.memory.jvm-overhead.min jobmanager.memory.jvm-overhead.max jobmanager.memory.jvm-overhead.fraction Native memory reserved for other JVM overhead: e.g. thread stacks, code cache, garbage collection space etc, it is a capped fractionated component of the total process memory Configure JVM Heap # As mentioned before in the total memory description, another way to set up the memory for the JobManager is to specify explicitly the JVM Heap size (jobmanager.memory.heap.size). It gives more control over the available JVM Heap which is used by:
Flink framework User code executed during job submission (e.g. for certain batch sources) or in checkpoint completion callbacks The required size of JVM Heap is mostly driven by the number of running jobs, their structure, and requirements for the mentioned user code.
Note If you have configured the JVM Heap explicitly, it is recommended to set neither total process memory nor total Flink memory. Otherwise, it may easily lead to memory configuration conflicts.
The Flink scripts and CLI set the JVM Heap size via the JVM parameters -Xms and -Xmx when they start the JobManager process, see also JVM parameters.
Configure Off-heap Memory # The Off-heap memory component accounts for any type of JVM direct memory and native memory usage. Therefore, you can also enable the JVM Direct Memory limit by setting the jobmanager.memory.enable-jvm-direct-memory-limit option. If this option is configured, Flink will set the limit to the Off-heap memory size via the corresponding JVM argument: -XX:MaxDirectMemorySize. See also JVM parameters.
The size of this component can be configured by jobmanager.memory.off-heap.size option. This option can be tuned e.g. if the JobManager process throws ‘OutOfMemoryError: Direct buffer memory’, see the troubleshooting guide for more information.
There can be the following possible sources of Off-heap memory consumption:
Flink framework dependencies (e.g. Akka network communication) User code executed during job submission (e.g. for certain batch sources) or in checkpoint completion callbacks Note If you have configured the Total Flink Memory and the JVM Heap explicitly but you have not configured the Off-heap memory, the size of the Off-heap memory will be derived as the Total Flink Memory minus the JVM Heap. The default value of the Off-heap memory option will be ignored.
Local Execution # If you run Flink locally (e.g. from your IDE) without creating a cluster, then the JobManager memory configuration options are ignored.
`}),e.add({id:138,href:"/flink/flink-docs-master/docs/dev/table/concepts/",title:"Streaming Concepts",section:"Table API \u0026 SQL",content:""}),e.add({id:139,href:"/flink/flink-docs-master/docs/connectors/datastream/formats/text_files/",title:"Text files",section:"Formats",content:` Text files format # Flink supports reading from text lines from a file using TextLineInputFormat. This format uses Java\u0026rsquo;s built-in InputStreamReader to decode the byte stream using various supported charset encodings. To use the format you need to add the Flink Connector Files dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-files\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; For PyFlink users, you could use it directly in your jobs.
This format is compatible with the new Source that can be used in both batch and streaming modes. Thus, you can use this format in two ways:
Bounded read for batch mode Continuous read for streaming mode: monitors a directory for new files that appear Bounded read example:
In this example we create a DataStream containing the lines of a text file as Strings. There is no need for a watermark strategy as records do not contain event timestamps.
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), /* Flink Path */) .build(); final DataStream\u0026lt;String\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python source = FileSource.for_record_stream_format(StreamFormat.text_line_format(), *path).build() stream = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) Continuous read example: In this example, we create a DataStream containing the lines of text files as Strings that will infinitely grow as new files are added to the directory. We monitor for new files each second. There is no need for a watermark strategy as records do not contain event timestamps.
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), /* Flink Path */) .monitorContinuously(Duration.ofSeconds(1L)) .build(); final DataStream\u0026lt;String\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python source = FileSource \\ .for_record_stream_format(StreamFormat.text_line_format(), *path) \\ .monitor_continously(Duration.of_seconds(1)) \\ .build() stream = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) `}),e.add({id:140,href:"/flink/flink-docs-master/docs/connectors/table/upsert-kafka/",title:"Upsert Kafka",section:"Table API Connectors",content:` Upsert Kafka SQL Connector # Scan Source: Unbounded Sink: Streaming Upsert Mode
The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.
As a source, the upsert-kafka connector produces a changelog stream, where each data record represents an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of the last value for the same key, if any (if a corresponding key doesn’t exist yet, the update will be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null values are interpreted in a special way: a record with a null value represents a “DELETE”.
As a sink, the upsert-kafka connector can consume a changelog stream. It will write INSERT/UPDATE_AFTER data as normal Kafka messages value, and write DELETE data as Kafka messages with null values (indicate tombstone for the key). Flink will guarantee the message ordering on the primary key by partition data on the values of the primary key columns, so the update/deletion messages on the same key will fall into the same partition.
Dependencies # In order to use the Upsert Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Upsert Kafka version Maven dependency SQL Client JAR universal \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The Upsert Kafka connector is not part of the binary distribution. See how to link with it for cluster execution here.
Full Example # The example below shows how to create and use an Upsert Kafka table:
CREATE TABLE pageviews_per_region ( user_region STRING, pv BIGINT, uv BIGINT, PRIMARY KEY (user_region) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews_per_region\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;key.format\u0026#39; = \u0026#39;avro\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro\u0026#39; ); CREATE TABLE pageviews ( user_id BIGINT, page_id BIGINT, viewtime TIMESTAMP, user_region STRING, WATERMARK FOR viewtime AS viewtime - INTERVAL \u0026#39;2\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ); -- calculate the pv, uv and insert into the upsert-kafka sink INSERT INTO pageviews_per_region SELECT user_region, COUNT(*), COUNT(DISTINCT user_id) FROM pageviews GROUP BY user_region; Attention Make sure to define the primary key in the DDL.
Available Metadata # See the regular Kafka connector for a list of all available metadata fields.
Connector Options # Option Required Default Type Description connector required (none) String Specify which connector to use, for the Upsert Kafka use: 'upsert-kafka'. topic required (none) String The Kafka topic name to read from and write to. properties.bootstrap.servers required (none) String Comma separated list of Kafka brokers. properties.* optional (none) String This can set and pass arbitrary Kafka configurations. Suffix names must match the configuration key defined in Kafka Configuration documentation. Flink will remove the "properties." key prefix and pass the transformed key and values to the underlying KafkaClient. For example, you can disable automatic topic creation via 'properties.allow.auto.create.topics' = 'false'. But there are some configurations that do not support to set, because Flink will override them, e.g. 'key.deserializer' and 'value.deserializer'. key.format required (none) String The format used to deserialize and serialize the key part of Kafka messages. Please refer to the formats page for more details and more format options.
Attention Compared to the regular Kafka connector, the key fields are specified by the PRIMARY KEY syntax. key.fields-prefix optional (none) String Defines a custom prefix for all fields of the key format to avoid name clashes with fields of the value format. By default, the prefix is empty. If a custom prefix is defined, both the table schema and 'key.fields' will work with prefixed names. When constructing the data type of the key format, the prefix will be removed and the non-prefixed names will be used within the key format. Please note that this option requires that 'value.fields-include' must be set to 'EXCEPT_KEY'. value.format required (none) String The format used to deserialize and serialize the value part of Kafka messages. Please refer to the formats page for more details and more format options. value.fields-include optional ALL Enum
Possible values: [ALL, EXCEPT_KEY] Defines a strategy how to deal with key columns in the data type of the value format. By default, 'ALL' physical columns of the table schema will be included in the value format which means that key columns appear in the data type for both the key and value format. sink.parallelism optional (none) Integer Defines the parallelism of the upsert-kafka sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator. sink.buffer-flush.max-rows optional 0 Integer The max size of buffered records before flush. When the sink receives many updates on the same key, the buffer will retain the last record of the same key. This can help to reduce data shuffling and avoid possible tombstone messages to Kafka topic. Can be set to '0' to disable it. By default, this is disabled. Note both 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero to enable sink buffer flushing. sink.buffer-flush.interval optional 0 Duration The flush interval mills, over this time, asynchronous threads will flush data. When the sink receives many updates on the same key, the buffer will retain the last record of the same key. This can help to reduce data shuffling and avoid possible tombstone messages to Kafka topic. Can be set to '0' to disable it. By default, this is disabled. Note both 'sink.buffer-flush.max-rows' and 'sink.buffer-flush.interval' must be set to be greater than zero to enable sink buffer flushing. Features # Key and Value Formats # See the regular Kafka connector for more explanation around key and value formats. However, note that this connector requires both a key and value format where the key fields are derived from the PRIMARY KEY constraint.
The following example shows how to specify and configure key and value formats. The format options are prefixed with either the 'key' or 'value' plus format identifier.
SQL CREATE TABLE KafkaTable ( \`ts\` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, \`user_id\` BIGINT, \`item_id\` BIGINT, \`behavior\` STRING, PRIMARY KEY (\`user_id\`) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) Primary Key Constraints # The Upsert Kafka always works in the upsert fashion and requires to define the primary key in the DDL. With the assumption that records with the same key should be ordered in the same partition, the primary key semantic on the changelog source means the materialized changelog is unique on the primary keys. The primary key definition will also control which fields should end up in Kafka’s key.
Consistency Guarantees # By default, an Upsert Kafka sink ingests data with at-least-once guarantees into a Kafka topic if the query is executed with checkpointing enabled.
This means, Flink may write duplicate records with the same key into the Kafka topic. But as the connector is working in the upsert mode, the last record on the same key will take effect when reading back as a source. Therefore, the upsert-kafka connector achieves idempotent writes just like the HBase sink.
Source Per-Partition Watermarks # Flink supports to emit per-partition watermarks for Upsert Kafka. Watermarks are generated inside the Kafka consumer. The per-partition watermarks are merged in the same way as watermarks are merged during streaming shuffles. The output watermark of the source is determined by the minimum watermark among the partitions it reads. If some partitions in the topics are idle, the watermark generator will not advance. You can alleviate this problem by setting the 'table.exec.source.idle-timeout' option in the table configuration.
Please refer to Kafka watermark strategies for more details.
Data Type Mapping # Upsert Kafka stores message keys and values as bytes, so Upsert Kafka doesn\u0026rsquo;t have schema or data types. The messages are serialized and deserialized by formats, e.g. csv, json, avro. Thus, the data type mapping is determined by specific formats. Please refer to Formats pages for more details.
Back to top
`}),e.add({id:141,href:"/flink/flink-docs-master/docs/dev/table/concepts/versioned_tables/",title:"Versioned Tables",section:"Streaming Concepts",content:` Versioned Tables # Flink SQL operates over dynamic tables that evolve, which may either be append-only or updating. Versioned tables represent a special type of updating table that remembers the past values for each key.
Concept # Dynamic tables define relations over time. Often, particularly when working with metadata, a key\u0026rsquo;s old value does not become irrelevant when it changes.
Flink SQL can define versioned tables over any dynamic table with a PRIMARY KEY constraint and time attribute.
A primary key constraint in Flink means that a column or set of columns of a table or view are unique and non-null. The primary key semantic on a upserting table means the materialized changes for a particular key (INSERT/UPDATE/DELETE) represent the changes to a single row over time. The time attribute on a upserting table defines when each change occurred.
Taken together, Flink can track the changes to a row over time and maintain the period for which each value was valid for that key.
Suppose a table tracks the prices of different products in a store.
(changelog kind) update_time product_id product_name price ================= =========== ========== ============ ===== +(INSERT) 00:01:00 p_001 scooter 11.11 +(INSERT) 00:02:00 p_002 basketball 23.11 -(UPDATE_BEFORE) 12:00:00 p_001 scooter 11.11 +(UPDATE_AFTER) 12:00:00 p_001 scooter 12.99 -(UPDATE_BEFORE) 12:00:00 p_002 basketball 23.11 +(UPDATE_AFTER) 12:00:00 p_002 basketball 19.99 -(DELETE) 18:00:00 p_001 scooter 12.99 Given this set of changes, we track how the price of a scooter changes over time. It is initially \$11.11 at 00:01:00 when added to the catalog. The price then rises to \$12.99 at 12:00:00 before being deleted from the catalog at 18:00:00.
If we queried the table for various products\u0026rsquo; prices at different times, we would retrieve different results. At 10:00:00 the table would show one set of prices.
update_time product_id product_name price =========== ========== ============ ===== 00:01:00 p_001 scooter 11.11 00:02:00 p_002 basketball 23.11 While at 13:00:00, we would find another set of prices.
update_time product_id product_name price =========== ========== ============ ===== 12:00:00 p_001 scooter 12.99 12:00:00 p_002 basketball 19.99 Versioned Table Sources # Versioned tables are defined implicitly for any tables whose underlying sources or formats directly define changelogs. Examples include the upsert Kafka source as well as database changelog formats such as debezium and canal. As discussed above, the only additional requirement is the CREATE table statement must contain a PRIMARY KEY and an event-time attribute.
CREATE TABLE products ( product_id STRING, product_name STRING, price DECIMAL(32, 2), update_time TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, PRIMARY KEY (product_id) NOT ENFORCED, WATERMARK FOR update_time AS update_time ) WITH (...); Versioned Table Views # Flink also supports defining versioned views if the underlying query contains a unique key constraint and event-time attribute. Imagine an append-only table of currency rates.
CREATE TABLE currency_rates ( currency STRING, rate DECIMAL(32, 10), update_time TIMESTAMP(3), WATERMARK FOR update_time AS update_time ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39;	= \u0026#39;rates\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ); The table currency_rates contains a row for each currency — with respect to USD — and receives a new row each time the rate changes. The JSON format does not support native changelog semantics, so Flink can only read this table as append-only.
(changelog kind) update_time currency rate ================ ============= ========= ==== +(INSERT) 09:00:00 Yen 102 +(INSERT) 09:00:00 Euro 114 +(INSERT) 09:00:00 USD 1 +(INSERT) 11:15:00 Euro 119 +(INSERT) 11:49:00 Pounds 108 Flink interprets each row as an INSERT to the table, meaning we cannot define a PRIMARY KEY over currency. However, it is clear to us (the query developer) that this table has all the necessary information to define a versioned table. Flink can reinterpret this table as a versioned table by defining a deduplication query which produces an ordered changelog stream with an inferred primary key (currency) and event time (update_time).
-- Define a versioned view CREATE VIEW versioned_rates AS SELECT currency, rate, update_time -- (1) \`update_time\` keeps the event time FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY currency -- (2) the inferred unique key \`currency\` can be a primary key ORDER BY update_time DESC) AS rownum FROM currency_rates) WHERE rownum = 1; -- the view \`versioned_rates\` will produce a changelog as the following. (changelog kind) update_time currency rate ================ ============= ========= ==== +(INSERT) 09:00:00 Yen 102 +(INSERT) 09:00:00 Euro 114 +(INSERT) 09:00:00 USD 1 +(UPDATE_AFTER) 10:45:00 Euro 116 +(UPDATE_AFTER) 11:15:00 Euro 119 +(INSERT) 11:49:00 Pounds 108 Flink has a special optimization step that will efficiently transform this query into a versioned table usable in subsequent queries. In general, the results of a query with the following format produces a versioned table:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]] ORDER BY time_attr DESC) AS rownum FROM table_name) WHERE rownum = 1 Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one. PARTITION BY col1[, col2...]: Specifies the partition columns, i.e. the deduplicate key. These columns form the primary key of the subsequent versioned table. ORDER BY time_attr DESC: Specifies the ordering column, it must be a time attribute. WHERE rownum = 1: The rownum = 1 is required for Flink to recognize this query is to generate a versioned table. Back to top
`}),e.add({id:142,href:"/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/",title:"Async I/O",section:"Operators",content:` Asynchronous I/O for External Data Access # This page explains the use of Flink\u0026rsquo;s API for asynchronous I/O with external data stores. For users not familiar with asynchronous or event-driven programming, an article about Futures and event-driven programming may be useful preparation.
Note: Details about the design and implementation of the asynchronous I/O utility can be found in the proposal and design document FLIP-12: Asynchronous I/O Design and Implementation. Details about the new retry support can be found in document FLIP-232: Add Retry Support For Async I/O In DataStream API.
The need for Asynchronous I/O Operations # When interacting with external systems (for example when enriching stream events with data stored in a database), one needs to take care that communication delay with the external system does not dominate the streaming application\u0026rsquo;s total work.
Naively accessing data in the external database, for example in a MapFunction, typically means synchronous interaction: A request is sent to the database and the MapFunction waits until the response has been received. In many cases, this waiting makes up the vast majority of the function\u0026rsquo;s time.
Asynchronous interaction with the database means that a single parallel function instance can handle many requests concurrently and receive the responses concurrently. That way, the waiting time can be overlaid with sending other requests and receiving responses. At the very least, the waiting time is amortized over multiple requests. This leads in most cased to much higher streaming throughput.
Note: Improving throughput by just scaling the MapFunction to a very high parallelism is in some cases possible as well, but usually comes at a very high resource cost: Having many more parallel MapFunction instances means more tasks, threads, Flink-internal network connections, network connections to the database, buffers, and general internal bookkeeping overhead.
Prerequisites # As illustrated in the section above, implementing proper asynchronous I/O to a database (or key/value store) requires a client to that database that supports asynchronous requests. Many popular databases offer such a client.
In the absence of such a client, one can try and turn a synchronous client into a limited concurrent client by creating multiple clients and handling the synchronous calls with a thread pool. However, this approach is usually less efficient than a proper asynchronous client.
Async I/O API # Flink\u0026rsquo;s Async I/O API allows users to use asynchronous request clients with data streams. The API handles the integration with data streams, well as handling order, event time, fault tolerance, retry support, etc.
Assuming one has an asynchronous client for the target database, three parts are needed to implement a stream transformation with asynchronous I/O against the database:
An implementation of AsyncFunction that dispatches the requests A callback that takes the result of the operation and hands it to the ResultFuture Applying the async I/O operation on a DataStream as a transformation with or without retry The following code example illustrates the basic pattern:
Java // This example implements the asynchronous request and callback with Futures that have the // interface of Java 8\u0026#39;s futures (which is the same one followed by Flink\u0026#39;s Future) /** * An implementation of the \u0026#39;AsyncFunction\u0026#39; that sends requests and sets the callback. */ class AsyncDatabaseRequest extends RichAsyncFunction\u0026lt;String, Tuple2\u0026lt;String, String\u0026gt;\u0026gt; { /** The database specific client that can issue concurrent requests with callbacks */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception { client = new DatabaseClient(host, post, credentials); } @Override public void close() throws Exception { client.close(); } @Override public void asyncInvoke(String key, final ResultFuture\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultFuture) throws Exception { // issue the asynchronous request, receive a future for result final Future\u0026lt;String\u0026gt; result = client.query(key); // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future CompletableFuture.supplyAsync(new Supplier\u0026lt;String\u0026gt;() { @Override public String get() { try { return result.get(); } catch (InterruptedException | ExecutionException e) { // Normally handled explicitly. return null; } } }).thenAccept( (String dbResult) -\u0026gt; { resultFuture.complete(Collections.singleton(new Tuple2\u0026lt;\u0026gt;(key, dbResult))); }); } } // create the original stream DataStream\u0026lt;String\u0026gt; stream = ...; // apply the async I/O transformation without retry DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100); // or apply the async I/O transformation with retry // create an async retry strategy via utility class or a user defined strategy AsyncRetryStrategy asyncRetryStrategy = new AsyncRetryStrategies.FixedDelayRetryStrategyBuilder(3, 100L) // maxAttempts=3, fixedDelay=100ms .retryIfResult(RetryPredicates.EMPTY_RESULT_PREDICATE) .retryIfException(RetryPredicates.HAS_EXCEPTION_PREDICATE) .build(); // apply the async I/O transformation with retry DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultStream = AsyncDataStream.unorderedWaitWithRetry(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100, asyncRetryStrategy); Scala /** * An implementation of the \u0026#39;AsyncFunction\u0026#39; that sends requests and sets the callback. */ class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] { /** The database specific client that can issue concurrent requests with callbacks */ lazy val client: DatabaseClient = new DatabaseClient(host, post, credentials) /** The context used for the future callbacks */ implicit lazy val executor: ExecutionContext = ExecutionContext.fromExecutor(Executors.directExecutor()) override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit = { // issue the asynchronous request, receive a future for the result val resultFutureRequested: Future[String] = client.query(str) // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future resultFutureRequested.onSuccess { case result: String =\u0026gt; resultFuture.complete(Iterable((str, result))) } } } // create the original stream val stream: DataStream[String] = ... // apply the async I/O transformation without retry val resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100) // apply the async I/O transformation with retry // create an AsyncRetryStrategy val asyncRetryStrategy: AsyncRetryStrategy[OUT] = ... // apply the async I/O transformation with retry val resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWaitWithRetry(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100, asyncRetryStrategy) Important note: The ResultFuture is completed with the first call of ResultFuture.complete. All subsequent complete calls will be ignored.
The following three parameters control the asynchronous operations:
Timeout: The timeout defines how long an asynchronous operation take before it is finally considered failed, may include multiple retry requests if retry enabled. This parameter guards against dead/failed requests.
Capacity: This parameter defines how many asynchronous requests may be in progress at the same time. Even though the async I/O approach leads typically to much better throughput, the operator can still be the bottleneck in the streaming application. Limiting the number of concurrent requests ensures that the operator will not accumulate an ever-growing backlog of pending requests, but that it will trigger backpressure once the capacity is exhausted.
AsyncRetryStrategy: The asyncRetryStrategy defines what conditions will trigger a delayed retry and the delay strategy, e.g., fixed-delay, exponential-backoff-delay, custom implementation, etc.
Timeout Handling # When an async I/O request times out, by default an exception is thrown and job is restarted. If you want to handle timeouts, you can override the AsyncFunction#timeout method. Make sure you call ResultFuture.complete() or ResultFuture.completeExceptionally() when overriding in order to indicate to Flink that the processing of this input record has completed. You can call ResultFuture.complete(Collections.emptyList()) if you do not want to emit any record when timeouts happen.
Order of Results # The concurrent requests issued by the AsyncFunction frequently complete in some undefined order, based on which request finished first. To control in which order the resulting records are emitted, Flink offers two modes:
Unordered: Result records are emitted as soon as the asynchronous request finishes. The order of the records in the stream is different after the async I/O operator than before. This mode has the lowest latency and lowest overhead, when used with processing time as the basic time characteristic. Use AsyncDataStream.unorderedWait(...) for this mode.
Ordered: In that case, the stream order is preserved. Result records are emitted in the same order as the asynchronous requests are triggered (the order of the operators input records). To achieve that, the operator buffers a result record until all its preceding records are emitted (or timed out). This usually introduces some amount of extra latency and some overhead in checkpointing, because records or results are maintained in the checkpointed state for a longer time, compared to the unordered mode. Use AsyncDataStream.orderedWait(...) for this mode.
Event Time # When the streaming application works with event time, watermarks will be handled correctly by the asynchronous I/O operator. That means concretely the following for the two order modes:
Unordered: Watermarks do not overtake records and vice versa, meaning watermarks establish an order boundary. Records are emitted unordered only between watermarks. A record occurring after a certain watermark will be emitted only after that watermark was emitted. The watermark in turn will be emitted only after all result records from inputs before that watermark were emitted.
That means that in the presence of watermarks, the unordered mode introduces some of the same latency and management overhead as the ordered mode does. The amount of that overhead depends on the watermark frequency.
Ordered: Order of watermarks and records is preserved, just like order between records is preserved. There is no significant change in overhead, compared to working with processing time.
Please recall that Ingestion Time is a special case of event time with automatically generated watermarks that are based on the sources processing time.
Fault Tolerance Guarantees # The asynchronous I/O operator offers full exactly-once fault tolerance guarantees. It stores the records for in-flight asynchronous requests in checkpoints and restores/re-triggers the requests when recovering from a failure.
Retry Support # The retry support introduces a built-in mechanism for async operator which being transparently to the user\u0026rsquo;s AsyncFunction.
AsyncRetryStrategy: The AsyncRetryStrategy contains the definition of the retry condition AsyncRetryPredicate and the interfaces to determine whether to continue retry and the retry interval based on the current attempt number. Note that after the trigger retry condition is met, it is possible to abandon the retry because the current attempt number exceeds the preset limit, or to be forced to terminate the retry at the end of the task (in this case, the system takes the last execution result or exception as the final state).
AsyncRetryPredicate: The retry condition can be triggered based on the return result or the execution exception.
Implementation Tips # For implementations with Futures that have an Executor (or ExecutionContext in Scala) for callbacks, we suggests to use a DirectExecutor, because the callback typically does minimal work, and a DirectExecutor avoids an additional thread-to-thread handover overhead. The callback typically only hands the result to the ResultFuture, which adds it to the output buffer. From there, the heavy logic that includes record emission and interaction with the checkpoint bookkeeping happens in a dedicated thread-pool anyways.
A DirectExecutor can be obtained via org.apache.flink.util.concurrent.Executors.directExecutor() or com.google.common.util.concurrent.MoreExecutors.directExecutor().
Caveats # The AsyncFunction is not called Multi-Threaded
A common confusion that we want to explicitly point out here is that the AsyncFunction is not called in a multi-threaded fashion. There exists only one instance of the AsyncFunction and it is called sequentially for each record in the respective partition of the stream. Unless the asyncInvoke(...) method returns fast and relies on a callback (by the client), it will not result in proper asynchronous I/O.
For example, the following patterns result in a blocking asyncInvoke(...) functions and thus void the asynchronous behavior:
Using a database client whose lookup/query method call blocks until the result has been received back
Blocking/waiting on the future-type objects returned by an asynchronous client inside the asyncInvoke(...) method
An AsyncFunction(AsyncWaitOperator) can be used anywhere in the job graph, except that it cannot be chained to a SourceFunction/SourceStreamTask.
May Need Larger Queue Capacity If Retry Enabled
The new retry feature may result in larger queue capacity requirements, the maximum number can be approximately evaluated as below:
inputRate * retryRate * avgRetryDuration For example, for a task with inputRate = 100 records/sec, where 1% of the elements will trigger 1 retry on average, and the average retry time is 60s, the additional queue capacity requirement will be:
100 records/sec * 1% * 60s = 60 That is, adding more 60 capacity to the work queue may not affect the throughput in unordered output mode , in case of ordered mode, the head element is the key point, and the longer it stays uncompleted, the longer the processing delay provided by the operator, the retry feature may increase the incomplete time of the head element, if in fact more retries are obtained with the same timeout constraint.
When the queue capacity grows(common way to ease the backpressure), the risk of OOM increases. Though in fact, for ListState storage, the theoretical upper limit is Integer.MAX_VALUE, so the queue capacity\u0026rsquo;s limit is the same, but we can\u0026rsquo;t increase the queue capacity too big in production, increase the task parallelism maybe a more viable way.
Back to top
`}),e.add({id:143,href:"/flink/flink-docs-master/docs/deployment/cli/",title:"Command-Line Interface",section:"Deployment",content:` Command-Line Interface # Flink provides a Command-Line Interface (CLI) bin/flink to run programs that are packaged as JAR files and to control their execution. The CLI is part of any Flink setup, available in local single node setups and in distributed setups. It connects to the running JobManager specified in conf/flink-conf.yaml.
Job Lifecycle Management # A prerequisite for the commands listed in this section to work is to have a running Flink deployment like Kubernetes, YARN or any other option available. Feel free to start a Flink cluster locally to try the commands on your own machine.
Submitting a Job # Submitting a job means uploading the job\u0026rsquo;s JAR and related dependencies to the Flink cluster and initiating the job execution. For the sake of this example, we select a long-running job like examples/streaming/StateMachineExample.jar. Feel free to select any other JAR archive from the examples/ folder or deploy your own job.
\$ ./bin/flink run \\ --detached \\ ./examples/streaming/StateMachineExample.jar Submitting the job using --detached will make the command return after the submission is done. The output contains (besides other things) the ID of the newly submitted job.
Usage with built-in data generator: StateMachineExample [--error-rate \u0026lt;probability-of-invalid-transition\u0026gt;] [--sleep \u0026lt;sleep-per-record-in-ms\u0026gt;] Usage with Kafka: StateMachineExample --kafka-topic \u0026lt;topic\u0026gt; [--brokers \u0026lt;brokers\u0026gt;] Options for both the above setups: [--backend \u0026lt;file|rocks\u0026gt;] [--checkpoint-dir \u0026lt;filepath\u0026gt;] [--async-checkpoints \u0026lt;true|false\u0026gt;] [--incremental-checkpoints \u0026lt;true|false\u0026gt;] [--output \u0026lt;filepath\u0026gt; OR null for stdout] Using standalone source with error rate 0.000000 and sleep delay 1 millis Job has been submitted with JobID cca7bc1061d61cf15238e92312c2fc20 The usage information printed lists job-related parameters that can be added to the end of the job submission command if necessary. For the purpose of readability, we assume that the returned JobID is stored in a variable JOB_ID for the commands below:
\$ export JOB_ID=\u0026#34;cca7bc1061d61cf15238e92312c2fc20\u0026#34; There is another action called run-application available to run the job in Application Mode. This documentation does not address this action individually as it works similarly to the run action in terms of the CLI frontend.
The run and run-application commands support passing additional configuration parameters via the -D argument. For example setting the maximum parallelism for a job can be done by setting -Dpipeline.max-parallelism=120. This argument is very useful for configuring application mode clusters, because you can pass any configuration parameter to the cluster without changing the configuration file.
When submitting a job to an existing session cluster, only execution configuration parameters are supported.
Job Monitoring # You can monitor any running jobs using the list action:
\$ ./bin/flink list Waiting for response... ------------------ Running/Restarting Jobs ------------------- 30.11.2020 16:02:29 : cca7bc1061d61cf15238e92312c2fc20 : State machine job (RUNNING) -------------------------------------------------------------- No scheduled jobs. Jobs that were submitted but not started, yet, would be listed under \u0026ldquo;Scheduled Jobs\u0026rdquo;.
Creating a Savepoint # Savepoints can be created to save the current state a job is in. All that\u0026rsquo;s needed is the JobID:
\$ ./bin/flink savepoint \\ \$JOB_ID \\ /tmp/flink-savepoints Triggering savepoint for job cca7bc1061d61cf15238e92312c2fc20. Waiting for response... Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab You can resume your program from this savepoint with the run command. The savepoint folder is optional and needs to be specified if state.savepoints.dir isn\u0026rsquo;t set.
Lastly, you can optionally provide what should be the binary format of the savepoint.
The path to the savepoint can be used later on to restart the Flink job.
Disposing a Savepoint # The savepoint action can be also used to remove savepoints. --dispose with the corresponding savepoint path needs to be added:
\$ ./bin/flink savepoint \\ --dispose \\ /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ \$JOB_ID Disposing savepoint \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;. Waiting for response... Savepoint \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39; disposed. If you use custom state instances (for example custom reducing state or RocksDB state), you have to specify the path to the program JAR with which the savepoint was triggered. Otherwise, you will run into a ClassNotFoundException:
\$ ./bin/flink savepoint \\ --dispose \u0026lt;savepointPath\u0026gt; \\ --jarfile \u0026lt;jarFile\u0026gt; Triggering the savepoint disposal through the savepoint action does not only remove the data from the storage but makes Flink clean up the savepoint-related metadata as well.
Terminating a Job # Stopping a Job Gracefully Creating a Final Savepoint # Another action for stopping a job is stop. It is a more graceful way of stopping a running streaming job as the stop flows from source to sink. When the user requests to stop a job, all sources will be requested to send the last checkpoint barrier that will trigger a savepoint, and after the successful completion of that savepoint, they will finish by calling their cancel() method.
\$ ./bin/flink stop \\ --savepointPath /tmp/flink-savepoints \\ \$JOB_ID Suspending job \u0026#34;cca7bc1061d61cf15238e92312c2fc20\u0026#34; with a savepoint. Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab We have to use --savepointPath to specify the savepoint folder if state.savepoints.dir isn\u0026rsquo;t set.
If the --drain flag is specified, then a MAX_WATERMARK will be emitted before the last checkpoint barrier. This will make all registered event-time timers fire, thus flushing out any state that is waiting for a specific watermark, e.g. windows. The job will keep running until all sources properly shut down. This allows the job to finish processing all in-flight data, which can produce some records to process after the savepoint taken while stopping.
Use the --drain flag if you want to terminate the job permanently. If you want to resume the job at a later point in time, then do not drain the pipeline because it could lead to incorrect results when the job is resumed. Lastly, you can optionally provide what should be the binary format of the savepoint.
Cancelling a Job Ungracefully # Cancelling a job can be achieved through the cancel action:
\$ ./bin/flink cancel \$JOB_ID Cancelling job cca7bc1061d61cf15238e92312c2fc20. Cancelled job cca7bc1061d61cf15238e92312c2fc20. The corresponding job\u0026rsquo;s state will be transitioned from Running to Cancelled. Any computations will be stopped.
The --withSavepoint flag allows creating a savepoint as part of the job cancellation. This feature is deprecated. Use the stop action instead. Starting a Job from a Savepoint # Starting a job from a savepoint can be achieved using the run (and run-application) action.
\$ ./bin/flink run \\ --detached \\ --fromSavepoint /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ ./examples/streaming/StateMachineExample.jar Usage with built-in data generator: StateMachineExample [--error-rate \u0026lt;probability-of-invalid-transition\u0026gt;] [--sleep \u0026lt;sleep-per-record-in-ms\u0026gt;] Usage with Kafka: StateMachineExample --kafka-topic \u0026lt;topic\u0026gt; [--brokers \u0026lt;brokers\u0026gt;] Options for both the above setups: [--backend \u0026lt;file|rocks\u0026gt;] [--checkpoint-dir \u0026lt;filepath\u0026gt;] [--async-checkpoints \u0026lt;true|false\u0026gt;] [--incremental-checkpoints \u0026lt;true|false\u0026gt;] [--output \u0026lt;filepath\u0026gt; OR null for stdout] Using standalone source with error rate 0.000000 and sleep delay 1 millis Job has been submitted with JobID 97b20a0a8ffd5c1d656328b0cd6436a6 See how the command is equal to the initial run command except for the --fromSavepoint parameter which is used to refer to the state of the previously stopped job. A new JobID is generated that can be used to maintain the job.
By default, we try to match the whole savepoint state to the job being submitted. If you want to allow to skip savepoint state that cannot be restored with the new job you can set the --allowNonRestoredState flag. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered and you still want to use the savepoint.
\$ ./bin/flink run \\ --fromSavepoint \u0026lt;savepointPath\u0026gt; \\ --allowNonRestoredState ... This is useful if your program dropped an operator that was part of the savepoint.
You can also select the restore mode which should be used for the savepoint. The mode controls who takes ownership of the files of the specified savepoint.
Back to top
CLI Actions # Here\u0026rsquo;s an overview of actions supported by Flink\u0026rsquo;s CLI tool:
Action Purpose run This action executes jobs. It requires at least the jar containing the job. Flink- or job-related arguments can be passed if necessary. run-application This action executes jobs in Application Mode. Other than that, it requires the same parameters as the run action. info This action can be used to print an optimized execution graph of the passed job. Again, the jar containing the job needs to be passed. list This action lists all running or scheduled jobs. savepoint This action can be used to create or disposing savepoints for a given job. It might be necessary to specify a savepoint directory besides the JobID, if the state.savepoints.dir parameter was not specified in conf/flink-conf.yaml. cancel This action can be used to cancel running jobs based on their JobID. stop This action combines the cancel and savepoint actions to stop a running job but also create a savepoint to start from again. A more fine-grained description of all actions and their parameters can be accessed through bin/flink --help or the usage information of each individual action bin/flink \u0026lt;action\u0026gt; --help.
Back to top
Advanced CLI # REST API # The Flink cluster can be also managed using the REST API. The commands described in previous sections are a subset of what is offered by Flink\u0026rsquo;s REST endpoints. Therefore, tools like curl can be used to get even more out of Flink.
Selecting Deployment Targets # Flink is compatible with multiple cluster management frameworks like Kubernetes or YARN which are described in more detail in the Resource Provider section. Jobs can be submitted in different Deployment Modes. The parameterization of a job submission differs based on the underlying framework and Deployment Mode.
bin/flink offers a parameter --target to handle the different options. In addition to that, jobs have to be submitted using either run (for Session and Per-Job Mode (deprecated)) or run-application (for Application Mode). See the following summary of parameter combinations:
YARN ./bin/flink run --target yarn-session: Submission to an already running Flink on YARN cluster ./bin/flink run --target yarn-per-job: Submission spinning up a Flink on YARN cluster in Per-Job Mode (deprecated) ./bin/flink run-application --target yarn-application: Submission spinning up Flink on YARN cluster in Application Mode Kubernetes ./bin/flink run --target kubernetes-session: Submission to an already running Flink on Kubernetes cluster ./bin/flink run-application --target kubernetes-application: Submission spinning up a Flink on Kubernetes cluster in Application Mode Standalone: ./bin/flink run --target local: Local submission using a MiniCluster in Session Mode ./bin/flink run --target remote: Submission to an already running Flink cluster The --target will overwrite the execution.target specified in the conf/flink-conf.yaml.
For more details on the commands and the available options, please refer to the Resource Provider-specific pages of the documentation.
Submitting PyFlink Jobs # Currently, users are able to submit a PyFlink job via the CLI. It does not require to specify the JAR file path or the entry main class, which is different from the Java job submission.
When submitting Python job via flink run, Flink will run the command \u0026ldquo;python\u0026rdquo;. Please run the following command to confirm that the python executable in current environment points to a supported Python version of 3.6+. \$ python --version # the version printed here must be 3.6+ The following commands show different PyFlink job submission use-cases:
Run a PyFlink job: \$ ./bin/flink run --python examples/python/table/word_count.py Run a PyFlink job with additional source and resource files. Files specified in --pyFiles will be added to the PYTHONPATH and, therefore, available in the Python code. \$ ./bin/flink run \\ --python examples/python/table/word_count.py \\ --pyFiles file:///user.txt,hdfs:///\$namenode_address/username.txt Run a PyFlink job which will reference Java UDF or external connectors. JAR file specified in --jarfile will be uploaded to the cluster. \$ ./bin/flink run \\ --python examples/python/table/word_count.py \\ --jarfile \u0026lt;jarFile\u0026gt; Run a PyFlink job with pyFiles and the main entry module specified in --pyModule: \$ ./bin/flink run \\ --pyModule table.word_count \\ --pyFiles examples/python/table Submit a PyFlink job on a specific JobManager running on host \u0026lt;jobmanagerHost\u0026gt; (adapt the command accordingly): \$ ./bin/flink run \\ --jobmanager \u0026lt;jobmanagerHost\u0026gt;:8081 \\ --python examples/python/table/word_count.py Run a PyFlink job using a YARN cluster in Per-Job Mode: \$ ./bin/flink run \\ --target yarn-per-job --python examples/python/table/word_count.py Run a PyFlink job using a YARN cluster in Application Mode: \$ ./bin/flink run-application -t yarn-application \\ -Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=1024m \\ -Dyarn.application.name=\u0026lt;ApplicationName\u0026gt; \\ -Dyarn.ship-files=/path/to/shipfiles \\ -pyarch shipfiles/venv.zip \\ -pyclientexec venv.zip/venv/bin/python3 \\ -pyexec venv.zip/venv/bin/python3 \\ -py shipfiles/word_count.py Note It assumes that the Python dependencies needed to execute the job are already placed in the directory /path/to/shipfiles. For example, it should contain venv.zip and word_count.py for the above example.
Note As it executes the job on the JobManager in YARN application mode, the paths specified in -pyarch and -py are paths relative to shipfiles which is the directory name of the shipped files.
Note The archive files specified via -pyarch will be distributed to the TaskManagers through blob server where the file size limit is 2 GB. If the size of an archive file is more than 2 GB, you could upload it to a distributed file system and then use the path in the command line option -pyarch.
Run a PyFlink application on a native Kubernetes cluster having the cluster ID \u0026lt;ClusterId\u0026gt;, it requires a docker image with PyFlink installed, please refer to Enabling PyFlink in docker: \$ ./bin/flink run-application \\ --target kubernetes-application \\ --parallelism 8 \\ -Dkubernetes.cluster-id=\u0026lt;ClusterId\u0026gt; \\ -Dtaskmanager.memory.process.size=4096m \\ -Dkubernetes.taskmanager.cpu=2 \\ -Dtaskmanager.numberOfTaskSlots=4 \\ -Dkubernetes.container.image=\u0026lt;PyFlinkImageName\u0026gt; \\ --pyModule word_count \\ --pyFiles /opt/flink/examples/python/table/word_count.py To learn more available options, please refer to Kubernetes or YARN which are described in more detail in the Resource Provider section.
Besides --pyFiles, --pyModule and --python mentioned above, there are also some other Python related options. Here\u0026rsquo;s an overview of all the Python related options for the actions run and run-application supported by Flink\u0026rsquo;s CLI tool:
Option Description -py,--python Python script with the program entry point. The dependent resources can be configured with the --pyFiles option. -pym,--pyModule Python module with the program entry point. This option must be used in conjunction with --pyFiles. -pyfs,--pyFiles Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (',') could be used as the separator to specify multiple files (e.g., --pyFiles file:///tmp/myresource.zip,hdfs:///\$namenode_address/myresource2.zip). -pyarch,--pyArchives Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g., --pyArchives file:///tmp/py37.zip,file:///tmp/data.zip#data --pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). -pyclientexec,--pyClientExecutable The path of the Python interpreter used to launch the Python process when submitting the Python jobs via \\"flink run\\" or compiling the Java/Scala jobs containing Python UDFs. (e.g., --pyArchives file:///tmp/py37.zip --pyClientExecutable py37.zip/py37/python) -pyexec,--pyExecutable Specify the path of the python interpreter used to execute the python UDF worker (e.g.: --pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u003e= 20.3) and SetupTools (version \u003e= 37.0.0). Please ensure that the specified environment meets the above requirements. -pyreq,--pyRequirements Specify the requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists (e.g., --pyRequirements file:///tmp/requirements.txt#file:///tmp/cached_dir). In addition to the command line options during submitting the job, it also supports to specify the dependencies via configuration or Python API inside the code. Please refer to the dependency management for more details.
Back to top
`}),e.add({id:144,href:"/flink/flink-docs-master/docs/dev/configuration/connector/",title:"Connectors and Formats",section:"Project Configuration",content:` Connectors and Formats # Flink applications can read from and write to various external systems via connectors. It supports multiple formats in order to encode and decode data to match Flink\u0026rsquo;s data structures.
An overview of available connectors and formats is available for both DataStream and Table API/SQL.
Available artifacts # In order to use connectors and formats, you need to make sure Flink has access to the artifacts implementing them. For each connector supported by the Flink community, we publish two artifacts on Maven Central:
flink-connector-\u0026lt;NAME\u0026gt; which is a thin JAR including only the connector code, but excluding eventual third-party dependencies flink-sql-connector-\u0026lt;NAME\u0026gt; which is an uber JAR ready to use with all the connector third-party dependencies The same applies for formats as well. Note that some connectors may not have a corresponding flink-sql-connector-\u0026lt;NAME\u0026gt; artifact because they do not require third-party dependencies.
The uber/fat JARs are supported mostly for being used in conjunction with the SQL client, but you can also use them in any DataStream/Table application. Using artifacts # In order to use a connector/format module, you can either:
Shade the thin JAR and its transitive dependencies in your job JAR Shade the uber JAR in your job JAR Copy the uber JAR directly in the /lib folder of the Flink distribution For shading dependencies, check out the specific Maven and Gradle guides. For a reference about the Flink distribution, check Anatomy of the Flink distribution.
Deciding whether to shade the uber JAR, the thin JAR or just include the dependency in the distribution is up to you and your use case. If you shade a dependency, you will have more control over the dependency version in the job JAR. In case of shading the thin JAR, you will have even more control over the transitive dependencies, since you can change the versions without changing the connector version (binary compatibility permitting). In case of embedding the connector uber JAR directly in the Flink distribution /lib folder, you will be able to control in one place connector versions for all jobs. `}),e.add({id:145,href:"/flink/flink-docs-master/docs/connectors/table/formats/debezium/",title:"Debezium",section:"Formats",content:` Debezium Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Debezium is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL, PostgreSQL, Oracle, Microsoft SQL Server and many other databases into Kafka. Debezium provides a unified format schema for changelog and supports to serialize messages using JSON and Apache Avro.
Flink supports to interpret Debezium JSON and Avro messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as
synchronizing incremental data from databases to other systems auditing logs real-time materialized views on databases temporal join changing history of a database table and so on. Flink also supports to encode the INSERT/UPDATE/DELETE messages in Flink SQL as Debezium JSON or Avro messages, and emit to external systems like Kafka. However, currently Flink can\u0026rsquo;t combine UPDATE_BEFORE and UPDATE_AFTER into a single UPDATE message. Therefore, Flink encodes UPDATE_BEFORE and UDPATE_AFTER as DELETE and INSERT Debezium messages.
Dependencies # Debezium Avro # In order to use the Debezium format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-confluent-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Debezium Json # In order to use the Debezium format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in Note: please refer to Debezium documentation about how to setup a Debezium Kafka Connect to synchronize changelog to Kafka topics.
How to use Debezium format # Debezium provides a unified format for changelog, here is a simple example for an update operation captured from a MySQL products table in JSON format:
{ \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;source\u0026#34;: {...}, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1589362330904, \u0026#34;transaction\u0026#34;: null } Note: please refer to Debezium documentation about the meaning of each fields.
The MySQL products table has 4 columns (id, name, description and weight). The above JSON message is an update change event on the products table where the weight value of the row with id = 111 is changed from 5.18 to 5.15. Assuming this messages is synchronized to Kafka topic products_binlog, then we can use the following DDL to consume this topic and interpret the change events.
CREATE TABLE topic_products ( -- schema is totally the same to the MySQL \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, -- using \u0026#39;debezium-json\u0026#39; as the format to interpret Debezium JSON messages -- please use \u0026#39;debezium-avro-confluent\u0026#39; if Debezium encodes messages in Avro format \u0026#39;format\u0026#39; = \u0026#39;debezium-json\u0026#39; ) In some cases, users may setup the Debezium Kafka Connect with the Kafka configuration 'value.converter.schemas.enable' enabled to include schema in the message. Then the Debezium JSON message may look like this:
{ \u0026#34;schema\u0026#34;: {...}, \u0026#34;payload\u0026#34;: { \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;source\u0026#34;: {...}, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1589362330904, \u0026#34;transaction\u0026#34;: null } } In order to interpret such messages, you need to add the option 'debezium-json.schema-include' = 'true' into above DDL WITH clause (false by default). Usually, this is not recommended to include schema because this makes the messages very verbose and reduces parsing performance.
After registering the topic as a Flink table, then you can consume the Debezium messages as a changelog source.
-- a real-time materialized view on the MySQL \u0026#34;products\u0026#34; -- which calculate the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of MySQL \u0026#34;products\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Attention Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format.
Key Data Type Description schema STRING NULL JSON string describing the schema of the payload. Null if the schema is not included in the Debezium record. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts_ms field in the Debezium record. source.timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the source system created the event. Corresponds to the source.ts_ms field in the Debezium record. source.database STRING NULL The originating database. Corresponds to the source.db field in the Debezium record if available. source.schema STRING NULL The originating database schema. Corresponds to the source.schema field in the Debezium record if available. source.table STRING NULL The originating database table. Corresponds to the source.table or source.collection field in the Debezium record if available. source.properties MAP\u0026lt;STRING, STRING\u0026gt; NULL Map of various source properties. Corresponds to the source field in the Debezium record. The following example shows how to access Debezium metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, event_time TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, origin_database STRING METADATA FROM \u0026#39;value.source.database\u0026#39; VIRTUAL, origin_schema STRING METADATA FROM \u0026#39;value.source.schema\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.source.table\u0026#39; VIRTUAL, origin_properties MAP\u0026lt;STRING, STRING\u0026gt; METADATA FROM \u0026#39;value.source.properties\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39; ); Format Options # Flink provides debezium-avro-confluent and debezium-json formats to interpret Avro or Json messages produced by Debezium. Use format debezium-avro-confluent to interpret Debezium Avro messages and format debezium-json to interpret Debezium Json messages.
Debezium Avro Option Required Default Type Description format required (none) String Specify what format to use, here should be 'debezium-avro-confluent'. debezium-avro-confluent.basic-auth.credentials-source optional (none) String Basic auth credentials source for Schema Registry debezium-avro-confluent.basic-auth.user-info optional (none) String Basic auth user info for schema registry debezium-avro-confluent.bearer-auth.credentials-source optional (none) String Bearer auth credentials source for Schema Registry debezium-avro-confluent.bearer-auth.token optional (none) String Bearer auth token for Schema Registry debezium-avro-confluent.properties optional (none) Map Properties map that is forwarded to the underlying Schema Registry. This is useful for options that are not officially exposed via Flink config options. However, note that Flink options have higher precedence. debezium-avro-confluent.ssl.keystore.location optional (none) String Location / File of SSL keystore debezium-avro-confluent.ssl.keystore.password optional (none) String Password for SSL keystore debezium-avro-confluent.ssl.truststore.location optional (none) String Location / File of SSL truststore debezium-avro-confluent.ssl.truststore.password optional (none) String Password for SSL truststore debezium-avro-confluent.subject optional (none) String The Confluent Schema Registry subject under which to register the schema used by this format during serialization. By default, 'kafka' and 'upsert-kafka' connectors use '\u0026lt;topic_name\u0026gt;-value' or '\u0026lt;topic_name\u0026gt;-key' as the default subject name if this format is used as the value or key format. But for other connectors (e.g. 'filesystem'), the subject option is required when used as sink. debezium-avro-confluent.url required (none) String The URL of the Confluent Schema Registry to fetch/register schemas. Debezium Json Option Required Default Type Description format required (none) String Specify what format to use, here should be 'debezium-json'. debezium-json.schema-include optional false Boolean When setting up a Debezium Kafka Connect, users may enable a Kafka configuration 'value.converter.schemas.enable' to include schema in the message. This option indicates whether the Debezium JSON message includes the schema or not. debezium-json.ignore-parse-errors optional false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. debezium-json.timestamp-format.standard optional 'SQL' String Specify the input and output timestamp format. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input timestamp in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g '2020-12-30 12:13:14.123' and output timestamp in the same format. Option 'ISO-8601'will parse input timestamp in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g '2020-12-30T12:13:14.123' and output timestamp in the same format. debezium-json.map-null-key.mode optional 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by debezium-json.map-null-key.literal option. debezium-json.map-null-key.literal optional 'null' String Specify string literal to replace null key when 'debezium-json.map-null-key.mode' is LITERAL. debezium-json.encode.decimal-as-plain-number optional false Boolean Encode all decimals as plain numbers instead of possible scientific notations. By default, decimals may be written using scientific notation. For example, 0.000000027 is encoded as 2.7E-8 by default, and will be written as 0.000000027 if set this option to true. `}),e.add({id:146,href:"/flink/flink-docs-master/docs/dev/table/sql/drop/",title:"DROP Statements",section:"SQL",content:` DROP Statements # DROP statements are used to remove a catalog with the given catalog name or to remove a registered table/view/function from the current or specified Catalog.
Flink SQL supports the following DROP statements for now:
DROP CATALOG DROP TABLE DROP DATABASE DROP VIEW DROP FUNCTION Run a DROP statement # Java DROP statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful DROP operation, otherwise will throw an exception.
The following examples show how to run a DROP statement in TableEnvironment.
Scala DROP statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful DROP operation, otherwise will throw an exception.
The following examples show how to run a DROP statement in TableEnvironment.
Python DROP statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful DROP operation, otherwise will throw an exception.
The following examples show how to run a DROP statement in TableEnvironment.
SQL CLI DROP statements can be in SQL CLI.
The following examples show how to run a DROP statement in SQL CLI.
Java TableEnvironment tableEnv = TableEnvironment.create(...); // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // a string array: [\u0026#34;Orders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // drop \u0026#34;Orders\u0026#34; table from catalog tableEnv.executeSql(\u0026#34;DROP TABLE Orders\u0026#34;); // an empty string array String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // a string array: [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // drop \u0026#34;Orders\u0026#34; table from catalog tableEnv.executeSql(\u0026#34;DROP TABLE Orders\u0026#34;) // an empty string array val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() Python table_env = TableEnvironment.create(...) # a string array: [\u0026#34;Orders\u0026#34;] tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() # drop \u0026#34;Orders\u0026#34; table from catalog table_env.execute_sql(\u0026#34;DROP TABLE Orders\u0026#34;) # an empty string array tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; Orders Flink SQL\u0026gt; DROP TABLE Orders; [INFO] Table has been removed. Flink SQL\u0026gt; SHOW TABLES; [INFO] Result was empty. DROP CATALOG # DROP CATALOG [IF EXISTS] catalog_name Drop a catalog with the given catalog name.
IF EXISTS
If the catalog does not exist, nothing happens.
DROP TABLE # DROP [TEMPORARY] TABLE [IF EXISTS] [catalog_name.][db_name.]table_name Drop a table with the given table name. If the table to drop does not exist, an exception is thrown.
TEMPORARY
Drop temporary table that has catalog and database namespaces.
IF EXISTS
If the table does not exist, nothing happens.
DROP DATABASE # DROP DATABASE [IF EXISTS] [catalog_name.]db_name [ (RESTRICT | CASCADE) ] Drop a database with the given database name. If the database to drop does not exist, an exception is thrown.
IF EXISTS
If the database does not exist, nothing happens.
RESTRICT
Dropping a non-empty database triggers an exception. Enabled by default.
CASCADE
Dropping a non-empty database also drops all associated tables and functions.
DROP VIEW # DROP [TEMPORARY] VIEW [IF EXISTS] [catalog_name.][db_name.]view_name Drop a view that has catalog and database namespaces. If the view to drop does not exist, an exception is thrown.
TEMPORARY
Drop temporary view that has catalog and database namespaces.
IF EXISTS
If the view does not exist, nothing happens.
MAINTAIN DEPENDENCIES Flink does not maintain dependencies of view by CASCADE/RESTRICT keywords, the current way is producing postpone error message when user tries to use the view under the scenarios like the underlying table of view has been dropped.
DROP FUNCTION # DROP [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF EXISTS] [catalog_name.][db_name.]function_name; Drop a catalog function that has catalog and database namespaces. If the function to drop does not exist, an exception is thrown.
TEMPORARY
Drop temporary catalog function that has catalog and database namespaces.
TEMPORARY SYSTEM
Drop temporary system function that has no namespace.
IF EXISTS
If the function doesn\u0026rsquo;t exists, nothing happens.
`}),e.add({id:147,href:"/flink/flink-docs-master/docs/deployment/elastic_scaling/",title:"Elastic Scaling",section:"Deployment",content:` Elastic Scaling # Apache Flink allows you to rescale your jobs. You can do this manually by stopping the job and restarting from the savepoint created during shutdown with a different parallelism.
This page describes options where Flink automatically adjusts the parallelism instead.
Reactive Mode # Reactive mode is an MVP (\u0026ldquo;minimum viable product\u0026rdquo;) feature. The Flink community is actively looking for feedback by users through our mailing lists. Please check the limitations listed on this page. Reactive Mode configures a job so that it always uses all resources available in the cluster. Adding a TaskManager will scale up your job, removing resources will scale it down. Flink will manage the parallelism of the job, always setting it to the highest possible values.
Reactive Mode restarts a job on a rescaling event, restoring it from the latest completed checkpoint. This means that there is no overhead of creating a savepoint (which is needed for manually rescaling a job). Also, the amount of data that is reprocessed after rescaling depends on the checkpointing interval, and the restore time depends on the state size.
The Reactive Mode allows Flink users to implement a powerful autoscaling mechanism, by having an external service monitor certain metrics, such as consumer lag, aggregate CPU utilization, throughput or latency. As soon as these metrics are above or below a certain threshold, additional TaskManagers can be added or removed from the Flink cluster. This could be implemented through changing the replica factor of a Kubernetes deployment, or an autoscaling group on AWS. This external service only needs to handle the resource allocation and deallocation. Flink will take care of keeping the job running with the resources available.
Getting started # If you just want to try out Reactive Mode, follow these instructions. They assume that you are deploying Flink on a single machine.
# these instructions assume you are in the root directory of a Flink distribution. # Put Job into lib/ directory cp ./examples/streaming/TopSpeedWindowing.jar lib/ # Submit Job in Reactive Mode ./bin/standalone-job.sh start -Dscheduler-mode=reactive -Dexecution.checkpointing.interval=\u0026#34;10s\u0026#34; -j org.apache.flink.streaming.examples.windowing.TopSpeedWindowing # Start first TaskManager ./bin/taskmanager.sh start Let\u0026rsquo;s quickly examine the used submission command:
./bin/standalone-job.sh start deploys Flink in Application Mode -Dscheduler-mode=reactive enables Reactive Mode. -Dexecution.checkpointing.interval=\u0026quot;10s\u0026quot; configure checkpointing and restart strategy. the last argument is passing the Job\u0026rsquo;s main class name. You have now started a Flink job in Reactive Mode. The web interface shows that the job is running on one TaskManager. If you want to scale up the job, simply add another TaskManager to the cluster:
# Start additional TaskManager ./bin/taskmanager.sh start To scale down, remove a TaskManager instance.
# Remove a TaskManager ./bin/taskmanager.sh stop Usage # Configuration # To enable Reactive Mode, you need to configure scheduler-mode to reactive.
The parallelism of individual operators in a job will be determined by the scheduler. It is not configurable and will be ignored if explicitly set, either on individual operators or the entire job.
The only way of influencing the parallelism is by setting a max parallelism for an operator (which will be respected by the scheduler). The maxParallelism is bounded by 2^15 (32768). If you do not set a max parallelism for individual operators or the entire job, the default parallelism rules will be applied, potentially applying lower bounds than the max possible value. As with the default scheduling mode, please take the best practices for parallelism into consideration.
Note that such a high max parallelism might affect performance of the job, since more internal structures are needed to maintain some internal structures of Flink.
When enabling Reactive Mode, the jobmanager.adaptive-scheduler.resource-wait-timeout configuration key will default to -1. This means that the JobManager will run forever waiting for sufficient resources. If you want the JobManager to stop after a certain time without enough TaskManagers to run the job, configure jobmanager.adaptive-scheduler.resource-wait-timeout.
With Reactive Mode enabled, the jobmanager.adaptive-scheduler.resource-stabilization-timeout configuration key will default to 0: Flink will start running the job, as soon as there are sufficient resources available. In scenarios where TaskManagers are not connecting at the same time, but slowly one after another, this behavior leads to a job restart whenever a TaskManager connects. Increase this configuration value if you want to wait for the resources to stabilize before scheduling the job. Additionally, one can configure jobmanager.adaptive-scheduler.min-parallelism-increase: This configuration option specifics the minimum amount of additional, aggregate parallelism increase before triggering a scale-up. For example if you have a job with a source (parallelism=2) and a sink (parallelism=2), the aggregate parallelism is 4. By default, the configuration key is set to 1, so any increase in the aggregate parallelism will trigger a restart.
Recommendations # Configure periodic checkpointing for stateful jobs: Reactive mode restores from the latest completed checkpoint on a rescale event. If no periodic checkpointing is enabled, your program will lose its state. Checkpointing also configures a restart strategy. Reactive Mode will respect the configured restarting strategy: If no restarting strategy is configured, reactive mode will fail your job, instead of scaling it.
Downscaling in Reactive Mode might take longer if the TaskManager is not properly shutdown (i.e., if a SIGKILL signal is used instead of a SIGTERM signal). In this case, Flink waits for the heartbeat between JobManager and the stopped TaskManager(s) to time out. You will see that your Flink job is stuck for roughly 50 seconds before redeploying your job with a lower parallelism.
The default timeout is configured to 50 seconds. Adjust the heartbeat.timeout configuration to a lower value, if your infrastructure permits this. Setting a low heartbeat timeout can lead to failures if a TaskManager fails to respond to a heartbeat, for example due to a network congestion or a long garbage collection pause. Note that the heartbeat.interval always needs to be lower than the timeout.
Limitations # Since Reactive Mode is a new, experimental feature, not all features supported by the default scheduler are also available with Reactive Mode (and its adaptive scheduler). The Flink community is working on addressing these limitations.
Deployment is only supported as a standalone application deployment. Active resource providers (such as native Kubernetes, YARN) are explicitly not supported. Standalone session clusters are not supported either. The application deployment is limited to single job applications.
The only supported deployment options are Standalone in Application Mode (described on this page), Docker in Application Mode and Standalone Kubernetes Application Cluster.
The limitations of Adaptive Scheduler also apply to Reactive Mode.
Adaptive Scheduler # Using Adaptive Scheduler directly (not through Reactive Mode) is only advised for advanced users because slot allocation on a session cluster with multiple jobs is not defined. The Adaptive Scheduler can adjust the parallelism of a job based on available slots. It will automatically reduce the parallelism if not enough slots are available to run the job with the originally configured parallelism; be it due to not enough resources being available at the time of submission, or TaskManager outages during the job execution. If new slots become available the job will be scaled up again, up to the configured parallelism. In Reactive Mode (see above) the configured parallelism is ignored and treated as if it was set to infinity, letting the job always use as many resources as possible. You can also use Adaptive Scheduler without Reactive Mode, but there are some practical limitations:
If you are using Adaptive Scheduler on a session cluster, there are no guarantees regarding the distribution of slots between multiple running jobs in the same session. One benefit of the Adaptive Scheduler over the default scheduler is that it can handle TaskManager losses gracefully, since it would just scale down in these cases.
Usage # The following configuration parameters need to be set:
jobmanager.scheduler: adaptive: Change from the default scheduler to adaptive scheduler cluster.declarative-resource-management.enabled Declarative resource management must be enabled (enabled by default). The behavior of Adaptive Scheduler is configured by all configuration options containing adaptive-scheduler in their name.
Limitations # Streaming jobs only: The first version of Adaptive Scheduler runs with streaming jobs only. When submitting a batch job, we will automatically fall back to the default scheduler. No support for local recovery: Local recovery is a feature that schedules tasks to machines so that the state on that machine gets re-used if possible. The lack of this feature means that Adaptive Scheduler will always need to download the entire state from the checkpoint storage. No support for partial failover: Partial failover means that the scheduler is able to restart parts (\u0026ldquo;regions\u0026rdquo; in Flink\u0026rsquo;s internals) of a failed job, instead of the entire job. This limitation impacts only recovery time of embarrassingly parallel jobs: Flink\u0026rsquo;s default scheduler can restart failed parts, while Adaptive Scheduler will restart the entire job. Limited integration with Flink\u0026rsquo;s Web UI: Adaptive Scheduler allows that a job\u0026rsquo;s parallelism can change over its lifetime. The web UI only shows the current parallelism the job. Unused slots: If the max parallelism for slot sharing groups is not equal, slots offered to Adaptive Scheduler might be unused. Scaling events trigger job and task restarts, which will increase the number of Task attempts. Adaptive Batch Scheduler # The Adaptive Batch Scheduler can automatically decide parallelisms of operators for batch jobs. If an operator is not set with a parallelism, the scheduler will decide parallelism for it according to the size of its consumed datasets. This can bring many benefits:
Batch job users can be relieved from parallelism tuning Automatically tuned parallelisms can better fit consumed datasets which have a varying volume size every day Operators from SQL batch jobs can be assigned with different parallelisms which are automatically tuned Usage # To automatically decide parallelisms for operators with Adaptive Batch Scheduler, you need to:
Configure to use Adaptive Batch Scheduler. Set the parallelism of operators to -1. Configure to use Adaptive Batch Scheduler # To use Adaptive Batch Scheduler, you need to:
Set jobmanager.scheduler: AdaptiveBatch. Leave the execution.batch-shuffle-mode unset or explicitly set it to ALL-EXCHANGES-BLOCKING (default value) due to \u0026ldquo;ALL-EXCHANGES-BLOCKING jobs only\u0026rdquo;. In addition, there are several related configuration options that may need adjustment when using Adaptive Batch Scheduler:
jobmanager.adaptive-batch-scheduler.min-parallelism: The lower bound of allowed parallelism to set adaptively. Currently, this option should be configured as a power of 2, otherwise it will be rounded up to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.max-parallelism: The upper bound of allowed parallelism to set adaptively. Currently, this option should be configured as a power of 2, otherwise it will be rounded down to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task: The average size of data volume to expect each task instance to process. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs, or the decided parallelism reaches the max parallelism (due to too much data), the data actually processed by some tasks may far exceed this value. jobmanager.adaptive-batch-scheduler.default-source-parallelism: The default parallelism of data source. Set the parallelism of operators to -1 # Adaptive Batch Scheduler will only decide parallelism for operators whose parallelism is not specified by users (parallelism is -1). So if you want the parallelism of operators to be decided automatically, you should configure as follows:
Set parallelism.default: -1 Set table.exec.resource.default-parallelism: -1 in SQL jobs. Don\u0026rsquo;t call setParallelism() for operators in DataStream/DataSet jobs. Don\u0026rsquo;t call setParallelism() on StreamExecutionEnvironment/ExecutionEnvironment in DataStream/DataSet jobs. Performance tuning # It\u0026rsquo;s recommended to use Sort Shuffle and set taskmanager.network.memory.buffers-per-channel to 0. This can decouple the required network memory from parallelism, so that for large scale jobs, the \u0026ldquo;Insufficient number of network buffers\u0026rdquo; errors are less likely to happen. It\u0026rsquo;s recommended to set jobmanager.adaptive-batch-scheduler.max-parallelism to the parallelism you expect to need in the worst case. Values larger than that are not recommended, because excessive value may affect the performance. This option can affect the number of subpartitions produced by upstream tasks, large number of subpartitions may degrade the performance of hash shuffle and the performance of network transmission due to small packets. Limitations # Batch jobs only: Adaptive Batch Scheduler only supports batch jobs. Exception will be thrown if a streaming job is submitted. ALL-EXCHANGES-BLOCKING jobs only: At the moment, Adaptive Batch Scheduler only supports jobs whose shuffle mode is ALL-EXCHANGES-BLOCKING. The decided parallelism will be a power of 2: In order to ensure downstream tasks to consume the same count of subpartitions, the configuration option jobmanager.adaptive-batch-scheduler.max-parallelism should be set to be a power of 2 (2^N), and the decided parallelism will also be a power of 2 (2^M and M \u0026lt;= N). FileInputFormat sources are not supported: FileInputFormat sources are not supported, including StreamExecutionEnvironment#readFile(...) StreamExecutionEnvironment#readTextFile(...) and StreamExecutionEnvironment#createInput(FileInputFormat, ...). Users should use the new sources(FileSystem DataStream Connector or FileSystem SQL Connector) to read files when using the Adaptive Batch Scheduler. Inconsistent broadcast results metrics on WebUI: In Adaptive Batch Scheduler, for broadcast results, the number of bytes/records sent by the upstream task counted by metric is not equal to the number of bytes/records received by the downstream task, which may confuse users when displayed on the Web UI. See FLIP-187 for details. Back to top
`}),e.add({id:148,href:"/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/",title:"Elasticsearch",section:"DataStream Connectors",content:` Elasticsearch Connector # This connector provides sinks that can request document actions to an Elasticsearch Index. To use this connector, add one of the following dependencies to your project, depending on the version of the Elasticsearch installation:
Elasticsearch version Maven Dependency 6.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch6\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 7.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch7\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the Elasticsearch connector in PyFlink jobs, the following dependencies are required: Elasticsearch version PyFlink JAR 6.x Only available for stable releases. 7.x and later versions Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. Note that the streaming connectors are currently not part of the binary distribution. See here for information about how to package the program with the libraries for cluster execution.
Installing Elasticsearch # Instructions for setting up an Elasticsearch cluster can be found here.
Elasticsearch Sink # The example below shows how to configure and create a sink:
Java Elasticsearch 6:
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.HashMap; import java.util.Map; DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch6SinkBuilder\u0026lt;String\u0026gt;() .setBulkFlushMaxActions(1) // Instructs the sink to emit after every element, otherwise they would be buffered .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) .build()); private static IndexRequest createIndexRequest(String element) { Map\u0026lt;String, Object\u0026gt; json = new HashMap\u0026lt;\u0026gt;(); json.put(\u0026#34;data\u0026#34;, element); return Requests.indexRequest() .index(\u0026#34;my-index\u0026#34;) .type(\u0026#34;my-type\u0026#34;) .id(element) .source(json); } Elasticsearch 7:
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.HashMap; import java.util.Map; DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch7SinkBuilder\u0026lt;String\u0026gt;() .setBulkFlushMaxActions(1) // Instructs the sink to emit after every element, otherwise they would be buffered .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) .build()); private static IndexRequest createIndexRequest(String element) { Map\u0026lt;String, Object\u0026gt; json = new HashMap\u0026lt;\u0026gt;(); json.put(\u0026#34;data\u0026#34;, element); return Requests.indexRequest() .index(\u0026#34;my-index\u0026#34;) .id(element) .source(json); } Scala Elasticsearch 6:
import org.apache.flink.api.connector.sink.SinkWriter import org.apache.flink.connector.elasticsearch.sink.{Elasticsearch6SinkBuilder, RequestIndexer} import org.apache.flink.streaming.api.datastream.DataStream import org.apache.http.HttpHost import org.elasticsearch.action.index.IndexRequest import org.elasticsearch.client.Requests val input: DataStream[String] = ... input.sinkTo( new Elasticsearch6SinkBuilder[String] .setBulkFlushMaxActions(1) // Instructs the sink to emit after every element, otherwise they would be buffered .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) .build()) def createIndexRequest(element: (String)): IndexRequest = { val json = Map( \u0026#34;data\u0026#34; -\u0026gt; element.asInstanceOf[AnyRef] ) Requests.indexRequest.index(\u0026#34;my-index\u0026#34;).\`type\`(\u0026#34;my-type\u0026#34;).source(mapAsJavaMap(json)) } Elasticsearch 7:
import org.apache.flink.api.connector.sink.SinkWriter import org.apache.flink.connector.elasticsearch.sink.{Elasticsearch7SinkBuilder, RequestIndexer} import org.apache.flink.streaming.api.datastream.DataStream import org.apache.http.HttpHost import org.elasticsearch.action.index.IndexRequest import org.elasticsearch.client.Requests val input: DataStream[String] = ... input.sinkTo( new Elasticsearch7SinkBuilder[String] .setBulkFlushMaxActions(1) // Instructs the sink to emit after every element, otherwise they would be buffered .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) .build()) def createIndexRequest(element: (String)): IndexRequest = { val json = Map( \u0026#34;data\u0026#34; -\u0026gt; element.asInstanceOf[AnyRef] ) Requests.indexRequest.index(\u0026#34;my-index\u0026#34;).source(mapAsJavaMap(json)) } Python Elasticsearch 6 static index:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch6SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... # The set_bulk_flush_max_actions instructs the sink to emit after every element, otherwise they would be buffered es6_sink = Elasticsearch6SinkBuilder() \\ .set_bulk_flush_max_actions(1) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es6_sink).name(\u0026#39;es6 sink\u0026#39;) Elasticsearch 6 dynamic index:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch6SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... es_sink = Elasticsearch6SinkBuilder() \\ .set_emitter(ElasticsearchEmitter.dynamic_index(\u0026#39;name\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es6_sink).name(\u0026#39;es6 dynamic index sink\u0026#39;) Elasticsearch 7 static index:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... # The set_bulk_flush_max_actions instructs the sink to emit after every element, otherwise they would be buffered es7_sink = Elasticsearch7SinkBuilder() \\ .set_bulk_flush_max_actions(1) \\ .set_emitter(ElasticsearchEmitter.static(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 sink\u0026#39;) Elasticsearch 7 dynamic index:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... es7_sink = Elasticsearch7SinkBuilder() \\ .set_emitter(ElasticsearchEmitter.dynamic_index(\u0026#39;name\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 dynamic index sink\u0026#39;) Note that the example only demonstrates performing a single index request for each incoming element. Generally, the ElasticsearchEmitter can be used to perform requests of different types (ex., DeleteRequest, UpdateRequest, etc.).
Internally, each parallel instance of the Flink Elasticsearch Sink uses a BulkProcessor to send action requests to the cluster. This will buffer elements before sending them in bulk to the cluster. The BulkProcessor executes bulk requests one at a time, i.e. there will be no two concurrent flushes of the buffered actions in progress.
Elasticsearch Sinks and Fault Tolerance # With Flink’s checkpointing enabled, the Flink Elasticsearch Sink guarantees at-least-once delivery of action requests to Elasticsearch clusters. It does so by waiting for all pending action requests in the BulkProcessor at the time of checkpoints. This effectively assures that all requests before the checkpoint was triggered have been successfully acknowledged by Elasticsearch, before proceeding to process more records sent to the sink.
More details on checkpoints and fault tolerance are in the fault tolerance docs.
To use fault tolerant Elasticsearch Sinks, checkpointing of the topology needs to be enabled at the execution environment:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); // checkpoint every 5000 msecs Scala Elasticsearch 6:
val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableCheckpointing(5000) // checkpoint every 5000 msecs Python env = StreamExecutionEnvironment.get_execution_environment() # checkpoint every 5000 msecs env.enable_checkpointing(5000) IMPORTANT: Checkpointing is not enabled by default but the default delivery guarantee is AT_LEAST_ONCE. This causes the sink to buffer requests until it either finishes or the BulkProcessor flushes automatically. By default, the BulkProcessor will flush after 1000 added Actions. To configure the processor to flush more frequently, please refer to the BulkProcessor configuration section. Using UpdateRequests with deterministic ids and the upsert method it is possible to achieve exactly-once semantics in Elasticsearch when AT_LEAST_ONCE delivery is configured for the connector. Handling Failing Elasticsearch Requests # Elasticsearch action requests may fail due to a variety of reasons, including temporarily saturated node queue capacity or malformed documents to be indexed. The Flink Elasticsearch Sink allows the user to retry requests by specifying a backoff-policy.
Below is an example:
Java Elasticsearch 6:
DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch6SinkBuilder\u0026lt;String\u0026gt;() .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) // This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()); Elasticsearch 7:
DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch7SinkBuilder\u0026lt;String\u0026gt;() .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) // This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()); Scala Elasticsearch 6:
val input: DataStream[String] = ... input.sinkTo( new Elasticsearch6SinkBuilder[String] .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) // This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()) Elasticsearch 7:
val input: DataStream[String] = ... input.sinkTo( new Elasticsearch7SinkBuilder[String] .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) // This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()) Python Elasticsearch 6:
input = ... # This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds es_sink = Elasticsearch6SinkBuilder() \\ .set_bulk_flush_backoff_strategy(FlushBackoffType.CONSTANT, 5, 1000) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es_sink).name(\u0026#39;es6 sink\u0026#39;) Elasticsearch 7:
input = ... # This enables an exponential backoff retry mechanism, with a maximum of 5 retries and an initial delay of 1000 milliseconds es7_sink = Elasticsearch7SinkBuilder() \\ .set_bulk_flush_backoff_strategy(FlushBackoffType.EXPONENTIAL, 5, 1000) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 sink\u0026#39;) The above example will let the sink re-add requests that failed due to resource constrains (e.g. queue capacity saturation). For all other failures, such as malformed documents, the sink will fail. If no BulkFlushBackoffStrategy (or FlushBackoffType.NONE) is configured, the sink will fail for any kind of error.
IMPORTANT: Re-adding requests back to the internal BulkProcessor on failures will lead to longer checkpoints, as the sink will also need to wait for the re-added requests to be flushed when checkpointing. For example, when using FlushBackoffType.EXPONENTIAL, checkpoints will need to wait until Elasticsearch node queues have enough capacity for all the pending requests, or until the maximum number of retries has been reached. Configuring the Internal Bulk Processor # The internal BulkProcessor can be further configured for its behaviour on how buffered action requests are flushed, by using the following methods of the Elasticsearch6SinkBuilder:
setBulkFlushMaxActions(int numMaxActions): Maximum amount of actions to buffer before flushing. setBulkFlushMaxSizeMb(int maxSizeMb): Maximum size of data (in megabytes) to buffer before flushing. setBulkFlushInterval(long intervalMillis): Interval at which to flush regardless of the amount or size of buffered actions. Configuring how temporary request errors are retried is also supported:
setBulkFlushBackoffStrategy(FlushBackoffType flushBackoffType, int maxRetries, long delayMillis): The type of backoff delay, either CONSTANT or EXPONENTIAL, the amount of backoff retries to attempt, the amount of delay for backoff. For constant backoff, this is simply the delay between each retry. For exponential backoff, this is the initial base delay. More information about Elasticsearch can be found here.
Packaging the Elasticsearch Connector into an Uber-Jar # For the execution of your Flink program, it is recommended to build a so-called uber-jar (executable jar) containing all your dependencies (see here for further information).
Alternatively, you can put the connector\u0026rsquo;s jar file into Flink\u0026rsquo;s lib/ folder to make it available system-wide, i.e. for all job being run.
Back to top
`}),e.add({id:149,href:"/flink/flink-docs-master/docs/deployment/finegrained_resource/",title:"Fine-Grained Resource Management",section:"Deployment",content:` Fine-Grained Resource Management # Apache Flink works hard to auto-derive sensible default resource requirements for all applications out of the box. For users who wish to fine-tune their resource consumption, based on knowledge of their specific scenarios, Flink offers fine-grained resource management.
This page describes the fine-grained resource management’s usage, applicable scenarios, and how it works.
Note: This feature is currently an MVP (“minimum viable product”) feature and only available to DataStream API. Applicable Scenarios # Typical scenarios that potentially benefit from fine-grained resource management are where:
Tasks have significantly different parallelisms.
The resource needed for an entire pipeline is too much to fit into a single slot/task manager.
Batch jobs where resources needed for tasks of different stages are significantly different
An in-depth discussion on why fine-grained resource management can improve resource efficiency for the above scenarios is presented in How it improves resource efficiency.
How it works # As described in Flink Architecture, task execution resources in a TaskManager are split into many slots. The slot is the basic unit of both resource scheduling and resource requirement in Flink\u0026rsquo;s runtime.
With fine-grained resource management, the slots requests contain specific resource profiles, which users can specify. Flink will respect those user-specified resource requirements and dynamically cut an exactly-matched slot out of the TaskManager’s available resources. As shown above, there is a requirement for a slot with 0.25 Core and 1GB memory, and Flink allocates Slot 1 for it.
Previously in Flink, the resource requirement only contained the required slots, without fine-grained resource profiles, namely coarse-grained resource management. The TaskManager had a fixed number of identical slots to fulfill those requirements. For the resource requirement without a specified resource profile, Flink will automatically decide a resource profile. Currently, the resource profile of it is calculated from TaskManager’s total resource and taskmanager.numberOfTaskSlots, just like in coarse-grained resource management. As shown above, the total resource of TaskManager is 1 Core and 4 GB memory and the number of task slots is set to 2, Slot 2 is created with 0.5 Core and 2 GB memory for the requirement without a specified resource profile.
After the allocation of Slot 1 and Slot 2, there is 0.25 Core and 1 GB memory remaining as the free resources in the TaskManager. These free resources can be further partitioned to fulfill the following resource requirements.
Please refer to Resource Allocation Strategy for more details.
Usage # To use fine-grained resource management, you need to:
Configure to enable fine-grained resource management.
Specify the resource requirement.
Enable Fine-Grained Resource Management # To enable fine-grained resource management, you need to configure the cluster.fine-grained-resource-management.enabled to true.
Without this configuration, the Flink runtime cannot schedule the slots with your specified resource requirement and the job will fail with an exception. Specify Resource Requirement for Slot Sharing Group # Fine-grained resource requirements are defined on slot sharing groups. A slot sharing group is a hint that tells the JobManager operators/tasks in it CAN be put into the same slot.
For specifying the resource requirement, you need to:
Define the slot sharing group and the operators it contains.
Specify the resource of the slot sharing group.
There are two approaches to define the slot sharing group and the operators it contains:
You can define a slot sharing group only by its name and attach it to an operator through the slotSharingGroup(String name).
You can construct a SlotSharingGroup instance, which contains the name and an optional resource profile of the slot sharing group. The SlotSharingGroup can be attached to an operator through slotSharingGroup(SlotSharingGroup ssg).
You can specify the resource profile for your slot sharing groups:
If you set the slot sharing group through slotSharingGroup(SlotSharingGroup ssg), you can specify the resource profile in constructing the SlotSharingGroup instance.
If you only set the name of slot sharing group with slotSharingGroup(String name). You can construct a SlotSharingGroup instance with the same name along with the resource profile and register the resource of them with StreamExecutionEnvironment#registerSlotSharingGroup(SlotSharingGroup ssg).
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SlotSharingGroup ssgA = SlotSharingGroup.newBuilder(\u0026#34;a\u0026#34;) .setCpuCores(1.0) .setTaskHeapMemoryMB(100) .build(); SlotSharingGroup ssgB = SlotSharingGroup.newBuilder(\u0026#34;b\u0026#34;) .setCpuCores(0.5) .setTaskHeapMemoryMB(100) .build(); someStream.filter(...).slotSharingGroup(\u0026#34;a\u0026#34;) // Set the slot sharing group with name “a” .map(...).slotSharingGroup(ssgB); // Directly set the slot sharing group with name and resource. env.registerSlotSharingGroup(ssgA); // Then register the resource of group “a” Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val ssgA = SlotSharingGroup.newBuilder(\u0026#34;a\u0026#34;) .setCpuCores(1.0) .setTaskHeapMemoryMB(100) .build() val ssgB = SlotSharingGroup.newBuilder(\u0026#34;b\u0026#34;) .setCpuCores(0.5) .setTaskHeapMemoryMB(100) .build() someStream.filter(...).slotSharingGroup(\u0026#34;a\u0026#34;) // Set the slot sharing group with name “a” .map(...).slotSharingGroup(ssgB) // Directly set the slot sharing group with name and resource. env.registerSlotSharingGroup(ssgA) // Then register the resource of group “a” Python env = StreamExecutionEnvironment.get_execution_environment() ssg_a = SlotSharingGroup.builder(\u0026#39;a\u0026#39;) \\ .set_cpu_cores(1.0) \\ .set_task_heap_memory_mb(100) \\ .build() ssg_b = SlotSharingGroup.builder(\u0026#39;b\u0026#39;) \\ .set_cpu_cores(0.5) \\ .set_task_heap_memory_mb(100) \\ .build() some_stream.filter(...).slot_sharing_group(\u0026#39;a\u0026#39;) # Set the slot sharing group with name \u0026#34;a\u0026#34; .map(...).slot_sharing_group(ssg_b) # Directly set the slot sharing group with name and resource. env.register_slot_sharing_group(ssg_a) # Then register the resource of group \u0026#34;a\u0026#34; Note: Each slot sharing group can only attach to one specified resource, any conflict will fail the compiling of your job. In constructing the SlotSharingGroup, you can set the following resource components for the slot sharing group:
CPU Cores. Defines how many CPU cores are needed. Required to be explicitly configured with positive value. Task Heap Memory. Defines how much task heap memory is needed. Required to be explicitly configured with positive value. Task Off-Heap Memory. Defines how much task off-heap memory is needed, can be 0. Managed Memory. Defines how much task managed memory is needed, can be 0. External Resources. Defines the external resources needed, can be empty. Java // Directly build a slot sharing group with specific resource SlotSharingGroup ssgWithResource = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;) .setCpuCores(1.0) // required .setTaskHeapMemoryMB(100) // required .setTaskOffHeapMemoryMB(50) .setManagedMemory(MemorySize.ofMebiBytes(200)) .setExternalResource(\u0026#34;gpu\u0026#34;, 1.0) .build(); // Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment SlotSharingGroup ssgWithName = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;).build(); env.registerSlotSharingGroup(ssgWithResource); Scala // Directly build a slot sharing group with specific resource val ssgWithResource = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;) .setCpuCores(1.0) // required .setTaskHeapMemoryMB(100) // required .setTaskOffHeapMemoryMB(50) .setManagedMemory(MemorySize.ofMebiBytes(200)) .setExternalResource(\u0026#34;gpu\u0026#34;, 1.0) .build() // Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment val ssgWithName = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;).build() env.registerSlotSharingGroup(ssgWithResource) Python # Directly build a slot sharing group with specific resource ssg_with_resource = SlotSharingGroup.builder(\u0026#39;ssg\u0026#39;) \\ .set_cpu_cores(1.0) \\ .set_task_heap_memory_mb(100) \\ .set_task_off_heap_memory_mb(50) \\ .set_managed_memory(MemorySize.of_mebi_bytes(200)) \\ .set_external_resource(\u0026#39;gpu\u0026#39;, 1.0) \\ .build() # Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment ssg_with_name = SlotSharingGroup.builder(\u0026#39;ssg\u0026#39;).build() env.register_slot_sharing_group(ssg_with_resource) Note: You can construct a SlotSharingGroup with or without specifying its resource profile. With specifying the resource profile, you need to explicitly set the CPU cores and Task Heap Memory with a positive value, other components are optional. Limitations # Since fine-grained resource management is a new, experimental feature, not all features supported by the default scheduler are also available with it. The Flink community is working on addressing these limitations.
No support for the Elastic Scaling. The elastic scaling only supports slot requests without specified-resource at the moment.
No support for task manager redundancy. The slotmanager.redundant-taskmanager-num is used to start redundant TaskManagers to speed up job recovery. This config option will not take effect in fine-grained resource management at the moment.
No support for evenly spread out slot strategy. This strategy tries to spread out the slots evenly across all available TaskManagers. The strategy is not supported in the first version of fine-grained resource management and cluster.evenly-spread-out-slots will not take effect in it at the moment.
Limited integration with Flink’s Web UI. Slots in fine-grained resource management can have different resource specs. The web UI only shows the slot number without its details at the moment.
Limited integration with batch jobs. At the moment, fine-grained resource management requires batch workloads to be executed with types of all edges being BLOCKING. To do that, you need to configure fine-grained.shuffle-mode.all-blocking to true. Notice that this may affect the performance. See FLINK-20865 for more details.
Hybrid resource requirements are not recommended. It is not recommended to specify the resource requirements only for some parts of the job and leave the requirements for the rest unspecified. Currently, the unspecified requirement can be fulfilled with slots of any resource. The actual resource acquired by it can be inconsistent across different job executions or failover.
Slot allocation result might not be optimal. As the slot requirements contain multiple dimensions of resources, the slot allocation is indeed a multi-dimensional packing problem, which is NP-hard. The default resource allocation strategy might not achieve optimal slot allocation and can lead to resource fragments or resource allocation failure in some scenarios.
Notice # Setting the slot sharing group may change the performance. Setting chain-able operators to different slot sharing groups may break operator chains, and thus change the performance.
Slot sharing group will not restrict the scheduling of operators. The slot sharing group only hints the scheduler that the grouped operators CAN be deployed into a shared slot. There\u0026rsquo;s no guarantee that the scheduler always deploys the grouped operator together. In cases grouped operators are deployed into separate slots, the slot resources will be derived from the specified group requirement.
Deep Dive # How it improves resource efficiency # In this section, we deep dive into how fine-grained resource management improves resource efficiency, which can help you to understand whether it can benefit your jobs.
Previously, Flink adopted a coarse-grained resource management approach, where tasks are deployed into predefined, usually identical slots without the notion of how many resources each slot contains. For many jobs, using coarse-grained resource management and simply putting all tasks into one slot sharing group works well enough in terms of resource utilization.
For many streaming jobs that all tasks have the same parallelism, each slot will contain an entire pipeline. Ideally, all pipelines should use roughly the same resources, which can be satisfied easily by tuning the resources of the identical slots.
Resource consumption of tasks varies over time. When consumption of a task decreases, the extra resources can be used by another task whose consumption is increasing. This, known as the peak shaving and valley filling effect, reduces the overall resource needed.
However, there are cases where coarse-grained resource management does not work well.
Tasks may have different parallelisms. Sometimes, such different parallelisms cannot be avoided. E.g., the parallelism of source/sink/lookup tasks might be constrained by the partitions and IO load of the external upstream/downstream system. In such cases, slots with fewer tasks would need fewer resources than those with the entire pipeline of tasks.
Sometimes the resource needed for the entire pipeline might be too much to be put into a single slot/TaskManager. In such cases, the pipeline needs to be split into multiple SSGs, which may not always have the same resource requirement.
For batch jobs, not all the tasks can be executed at the same time. Thus, the instantaneous resource requirement of the pipeline changes over time.
Trying to execute all tasks with identical slots can result in non-optimal resource utilization. The resource of the identical slots has to be able to fulfill the highest resource requirement, which will be wasteful for other requirements. When expensive external resources like GPU are involved, such waste can become even harder to afford. The fine-grained resource management leverages slots of different resources to improve resource utilization in such scenarios.
Resource Allocation Strategy # In this section, we talk about the slot partitioning mechanism in Flink runtime and the resource allocation strategy, including how the Flink runtime selects a TaskManager to cut slots and allocates TaskManagers on Native Kubernetes and YARN. Note that the resource allocation strategy is pluggable in Flink runtime and here we introduce its default implementation in the first step of fine-grained resource management. In the future, there might be various strategies that users can select for different scenarios.
As described in How it works section, Flink will cut an exactly matched slot out of the TaskManager for slot requests with specified resources. The internal process is shown above. The TaskManager will be launched with total resources but no predefined slots. When a slot request with 0.25 Core and 1GB memory arrives, Flink will select a TaskManager with enough free resources and create a new slot with the requested resources. If a slot is freed, it will return its resources to the available resources of the TaskManager.
In the current resource allocation strategy, Flink will traverse all the registered TaskManagers and select the first one who has enough free resources to fulfill the slot request. When there is no TaskManager that has enough free resources, Flink will try to allocate a new TaskManager when deploying on Native Kubernetes or YARN. In the current strategy, Flink will allocate identical TaskManagers according to user’s configuration. As the resource spec of TaskManagers is pre-defined:
There might be resource fragments in the cluster. E.g. if there are two slot requests with 3 GB heap memory while the total heap memory of TaskManager is 4 GB, Flink will start two TaskManagers and there will be 1 GB heap memory wasted in each TaskManager. In the future, there might be a resource allocation strategy that can allocate heterogeneous TaskManagers according to the slot requests of the job and thus mitigate the resource fragment.
You need to make sure the resource components configured for the slot sharing group are no larger than the total resources of the TaskManager. Otherwise, your job will fail with an exception.
Back to top
`}),e.add({id:150,href:"/flink/flink-docs-master/docs/connectors/datastream/firehose/",title:"Firehose",section:"DataStream Connectors",content:` Amazon Kinesis Data Firehose Sink # The Firehose sink writes to Amazon Kinesis Data Firehose.
Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to setup a Kinesis Data Firehose delivery stream.
To use the connector, add the following Maven dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-aws-kinesis-firehose\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the AWS Kinesis Firehose connector in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. The KinesisFirehoseSink uses AWS v2 SDK for Java to write data from a Flink stream into a Firehose delivery stream.
Java Properties sinkProperties = new Properties(); // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;eu-west-1\u0026#34;); // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); KinesisFirehoseSink\u0026lt;String\u0026gt; kdfSink = KinesisFirehoseSink.\u0026lt;String\u0026gt;builder() .setFirehoseClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setDeliveryStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(4 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1000 * 1024) // Optional .build(); flinkStream.sinkTo(kdfSink); Scala Properties sinkProperties = new Properties() // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;eu-west-1\u0026#34;) // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) val kdfSink = KinesisFirehoseSink.\u0026lt;String\u0026gt;builder() .setFirehoseClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setDeliveryStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(4 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1000 * 1024) // Optional .build() flinkStream.sinkTo(kdfSink) Python sink_properties = { # Required \u0026#39;aws.region\u0026#39;: \u0026#39;eu-west-1\u0026#39;, # Optional, provide via alternative routes e.g. environment variables \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39; } kdf_sink = KinesisFirehoseSink.builder() \\ .set_firehose_client_properties(sink_properties) \\ # Required .set_serialization_schema(SimpleStringSchema()) \\ # Required .set_delivery_stream_name(\u0026#39;your-stream-name\u0026#39;) \\ # Required .set_fail_on_error(False) \\ # Optional .set_max_batch_size(500) \\ # Optional .set_max_in_flight_requests(50) \\ # Optional .set_max_buffered_requests(10000) \\ # Optional .set_max_batch_size_in_bytes(5 * 1024 * 1024) \\ # Optional .set_max_time_in_buffer_ms(5000) \\ # Optional .set_max_record_size_in_bytes(1 * 1024 * 1024) \\ # Optional .build() Configurations # Flink\u0026rsquo;s Firehose sink is created by using the static builder KinesisFirehoseSink.\u0026lt;InputType\u0026gt;builder().
setFirehoseClientProperties(Properties sinkProperties) Required. Supplies credentials, region and other parameters to the Firehose client. setSerializationSchema(SerializationSchema serializationSchema) Required. Supplies a serialization schema to the Sink. This schema is used to serialize elements before sending to Firehose. setDeliveryStreamName(String deliveryStreamName) Required. Name of the delivery stream to sink to. setFailOnError(boolean failOnError) Optional. Default: false. Whether failed requests to write records to Firehose are treated as fatal exceptions in the sink. setMaxBatchSize(int maxBatchSize) Optional. Default: 500. Maximum size of a batch to write to Firehose. setMaxInFlightRequests(int maxInFlightRequests) Optional. Default: 50. The maximum number of in flight requests allowed before the sink applies backpressure. setMaxBufferedRequests(int maxBufferedRequests) Optional. Default: 10_000. The maximum number of records that may be buffered in the sink before backpressure is applied. setMaxBatchSizeInBytes(int maxBatchSizeInBytes) Optional. Default: 4 * 1024 * 1024. The maximum size (in bytes) a batch may become. All batches sent will be smaller than or equal to this size. setMaxTimeInBufferMS(int maxTimeInBufferMS) Optional. Default: 5000. The maximum time a record may stay in the sink before being flushed. setMaxRecordSizeInBytes(int maxRecordSizeInBytes) Optional. Default: 1000 * 1024. The maximum record size that the sink will accept, records larger than this will be automatically rejected. build() Constructs and returns the Firehose sink. Using Custom Firehose Endpoints # It is sometimes desirable to have Flink operate as a consumer or producer against a Firehose VPC endpoint or a non-AWS Firehose endpoint such as Localstack; this is especially useful when performing functional testing of a Flink application. The AWS endpoint that would normally be inferred by the AWS region set in the Flink configuration must be overridden via a configuration property.
To override the AWS endpoint, set the AWSConfigConstants.AWS_ENDPOINT and AWSConfigConstants.AWS_REGION properties. The region will be used to sign the endpoint URL.
Java Properties producerConfig = new Properties(); producerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4566\u0026#34;); Scala val producerConfig = new Properties() producerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4566\u0026#34;) Python producer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4566\u0026#39; } Back to top
`}),e.add({id:151,href:"/flink/flink-docs-master/docs/connectors/table/firehose/",title:"Firehose",section:"Table API Connectors",content:` Amazon Kinesis Data Firehose SQL Connector # Sink: Streaming Append Mode The Kinesis Data Firehose connector allows for writing data into Amazon Kinesis Data Firehose (KDF).
Dependencies # In order to use the AWS Kinesis Firehose connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis-firehose\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a Kinesis Data Firehose table # Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to set up a Kinesis Data Firehose delivery stream. The following example shows how to create a table backed by a Kinesis Data Firehose delivery stream with minimum required options:
CREATE TABLE FirehoseTable ( \`user_id\` BIGINT, \`item_id\` BIGINT, \`category_id\` BIGINT, \`behavior\` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;firehose\u0026#39;, \u0026#39;delivery-stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Connector Options # Option Required Default Type Description Common Options connector required (none) String Specify what connector to use. For Kinesis Data Firehose use 'firehose'. delivery-stream required (none) String Name of the Kinesis Data Firehose delivery stream backing this table. format required (none) String The format used to deserialize and serialize Kinesis Data Firehose records. See Data Type Mapping for details. aws.region required (none) String The AWS region where the delivery stream is defined. This option is required for KinesisFirehoseSink creation. aws.endpoint optional (none) String The AWS endpoint for Amazon Kinesis Data Firehose. aws.trust.all.certificates optional false Boolean If true accepts all SSL certificates. Authentication Options aws.credentials.provider optional AUTO String A credentials provider to use when authenticating against the Kinesis endpoint. See Authentication for details. aws.credentials.basic.accesskeyid optional (none) String The AWS access key ID to use when setting credentials provider type to BASIC. aws.credentials.basic.secretkey optional (none) String The AWS secret key to use when setting credentials provider type to BASIC. aws.credentials.profile.path optional (none) String Optional configuration for profile path if credential provider type is set to be PROFILE. aws.credentials.profile.name optional (none) String Optional configuration for profile name if credential provider type is set to be PROFILE. aws.credentials.role.arn optional (none) String The role ARN to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.sessionName optional (none) String The role session name to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.externalId optional (none) String The external ID to use when credential provider type is set to ASSUME_ROLE. aws.credentials.role.provider optional (none) String The credentials provider that provides credentials for assuming the role when credential provider type is set to ASSUME_ROLE. Roles can be nested, so this value can again be set to ASSUME_ROLE aws.credentials.webIdentityToken.file optional (none) String The absolute path to the web identity token file that should be used if provider type is set to WEB_IDENTITY_TOKEN. Sink Options sink.http-client.max-concurrency optional 10000 Integer Maximum number of allowed concurrent requests by FirehoseAsyncClient to be delivered to delivery stream. sink.http-client.read-timeout optional 360000 Integer Maximum amount of time in ms for requests to be sent by FirehoseAsyncClient to delivery stream before failure. sink.http-client.protocol.version optional HTTP2 String Http version used by FirehoseAsyncClient. sink.batch.max-size optional 500 Integer Maximum batch size of elements to be passed to FirehoseAsyncClient to be written downstream to delivery stream. sink.requests.max-inflight optional 16 Integer Request threshold for uncompleted requests by FirehoseAsyncClientbefore blocking new write requests. sink.requests.max-buffered optional 10000 String request buffer threshold by FirehoseAsyncClient before blocking new write requests. sink.flush-buffer.size optional 5242880 Long Threshold value in bytes for writer buffer in FirehoseAsyncClient before flushing. sink.flush-buffer.timeout optional 5000 Long Threshold time in ms for an element to be in a buffer of FirehoseAsyncClient before flushing. sink.fail-on-error optional false Boolean Flag used for retrying failed requests. If set any request failure will not be retried and will fail the job. Authorization # Make sure to create an appropriate IAM policy to allow reading writing to the Kinesis Data Firehose delivery stream.
Authentication # Depending on your deployment you would choose a different Credentials Provider to allow access to Kinesis Data Firehose. By default, the AUTO Credentials Provider is used. If the access key ID and secret key are set in the deployment configuration, this results in using the BASIC provider.
A specific AWSCredentialsProvider can be optionally set using the aws.credentials.provider setting. Supported values are:
AUTO - Use the default AWS Credentials Provider chain that searches for credentials in the following order: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE, and EC2/ECS credentials provider. BASIC - Use access key ID and secret key supplied as configuration. ENV_VAR - Use AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY environment variables. SYS_PROP - Use Java system properties aws.accessKeyId and aws.secretKey. PROFILE - Use an AWS credentials profile to create the AWS credentials. ASSUME_ROLE - Create AWS credentials by assuming a role. The credentials for assuming the role must be supplied. WEB_IDENTITY_TOKEN - Create AWS credentials by assuming a role using Web Identity Token. Data Type Mapping # Kinesis Data Firehose stores records as Base64-encoded binary data objects, so it doesn\u0026rsquo;t have a notion of internal record structure. Instead, Kinesis Data Firehose records are deserialized and serialized by formats, e.g. \u0026lsquo;avro\u0026rsquo;, \u0026lsquo;csv\u0026rsquo;, or \u0026lsquo;json\u0026rsquo;. To determine the data type of the messages in your Kinesis Data Firehose backed tables, pick a suitable Flink format with the format keyword. Please refer to the Formats pages for more details.
Notice # The current implementation for the Kinesis Data Firehose SQL connector only supports Kinesis Data Firehose backed sinks and doesn\u0026rsquo;t provide an implementation for source queries. Queries similar to:
SELECT * FROM FirehoseTable; should result in an error similar to
Connector firehose can only be used as a sink. It cannot be used as a source. Back to top
`}),e.add({id:152,href:"/flink/flink-docs-master/docs/dev/python/table/udfs/python_udfs/",title:"General User-defined Functions",section:"User Defined Functions",content:` General User-defined Functions # User-defined functions are important features, because they significantly extend the expressiveness of Python Table API programs.
NOTE: Python UDF execution requires Python version (3.6, 3.7, 3.8 or 3.9) with PyFlink installed. It\u0026rsquo;s required on both the client side and the cluster side.
Scalar Functions # It supports to use Python scalar functions in Python Table API programs. In order to define a Python scalar function, one can extend the base class ScalarFunction in pyflink.table.udf and implement an evaluation method. The behavior of a Python scalar function is defined by the evaluation method which is named eval. The evaluation method can support variable arguments, such as eval(*args).
The following example shows how to define your own Python hash code function, register it in the TableEnvironment, and call it in a query. Note that you can configure your scalar function via a constructor before it is registered:
from pyflink.table.expressions import call, col from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.udf import ScalarFunction, udf class HashCode(ScalarFunction): def __init__(self): self.factor = 12 def eval(self, s): return hash(s) * self.factor settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) hash_code = udf(HashCode(), result_type=DataTypes.BIGINT()) # use the Python function in Python Table API my_table.select(col(\u0026#34;string\u0026#34;), col(\u0026#34;bigint\u0026#34;), hash_code(col(\u0026#34;bigint\u0026#34;)), call(hash_code, col(\u0026#34;bigint\u0026#34;))) # use the Python function in SQL API table_env.create_temporary_function(\u0026#34;hash_code\u0026#34;, udf(HashCode(), result_type=DataTypes.BIGINT())) table_env.sql_query(\u0026#34;SELECT string, bigint, hash_code(bigint) FROM MyTable\u0026#34;) It also supports to use Java/Scala scalar functions in Python Table API programs.
\u0026#39;\u0026#39;\u0026#39; Java code: // The Java class must have a public no-argument constructor and can be founded in current Java classloader. public class HashCode extends ScalarFunction { private int factor = 12; public int eval(String s) { return s.hashCode() * factor; } } \u0026#39;\u0026#39;\u0026#39; from pyflink.table.expressions import call, col from pyflink.table import TableEnvironment, EnvironmentSettings settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) # register the Java function table_env.create_java_temporary_function(\u0026#34;hash_code\u0026#34;, \u0026#34;my.java.function.HashCode\u0026#34;) # use the Java function in Python Table API my_table.select(call(\u0026#39;hash_code\u0026#39;, col(\u0026#34;string\u0026#34;))) # use the Java function in SQL API table_env.sql_query(\u0026#34;SELECT string, bigint, hash_code(string) FROM MyTable\u0026#34;) There are many ways to define a Python scalar function besides extending the base class ScalarFunction. The following examples show the different ways to define a Python scalar function which takes two columns of bigint as the input parameters and returns the sum of them as the result.
# option 1: extending the base class \`ScalarFunction\` class Add(ScalarFunction): def eval(self, i, j): return i + j add = udf(Add(), result_type=DataTypes.BIGINT()) # option 2: Python function @udf(result_type=DataTypes.BIGINT()) def add(i, j): return i + j # option 3: lambda function add = udf(lambda i, j: i + j, result_type=DataTypes.BIGINT()) # option 4: callable function class CallableAdd(object): def __call__(self, i, j): return i + j add = udf(CallableAdd(), result_type=DataTypes.BIGINT()) # option 5: partial function def partial_add(i, j, k): return i + j + k add = udf(functools.partial(partial_add, k=1), result_type=DataTypes.BIGINT()) # register the Python function table_env.create_temporary_function(\u0026#34;add\u0026#34;, add) # use the function in Python Table API my_table.select(call(\u0026#39;add\u0026#39;, col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;))) # You can also use the Python function in Python Table API directly my_table.select(add(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;))) Table Functions # Similar to a Python user-defined scalar function, a user-defined table function takes zero, one, or multiple scalar values as input parameters. However in contrast to a scalar function, it can return an arbitrary number of rows as output instead of a single value. The return type of a Python UDTF could be of types Iterable, Iterator or generator.
The following example shows how to define your own Python multi emit function, register it in the TableEnvironment, and call it in a query.
from pyflink.table.expressions import col from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.udf import TableFunction, udtf class Split(TableFunction): def eval(self, string): for s in string.split(\u0026#34; \u0026#34;): yield s, len(s) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) my_table = ... # type: Table, table schema: [a: String] # register the Python Table Function split = udtf(Split(), result_types=[DataTypes.STRING(), DataTypes.INT()]) # use the Python Table Function in Python Table API my_table.join_lateral(split(col(\u0026#34;a\u0026#34;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)) my_table.left_outer_join_lateral(split(col(\u0026#34;a\u0026#34;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)) # use the Python Table function in SQL API table_env.create_temporary_function(\u0026#34;split\u0026#34;, udtf(Split(), result_types=[DataTypes.STRING(), DataTypes.INT()])) table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)\u0026#34;) table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE\u0026#34;) It also supports to use Java/Scala table functions in Python Table API programs.
\u0026#39;\u0026#39;\u0026#39; Java code: // The generic type \u0026#34;Tuple2\u0026lt;String, Integer\u0026gt;\u0026#34; determines the schema of the returned table as (String, Integer). // The java class must have a public no-argument constructor and can be founded in current java classloader. public class Split extends TableFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private String separator = \u0026#34; \u0026#34;; public void eval(String str) { for (String s : str.split(separator)) { // use collect(...) to emit a row collect(new Tuple2\u0026lt;String, Integer\u0026gt;(s, s.length())); } } } \u0026#39;\u0026#39;\u0026#39; from pyflink.table.expressions import call, col from pyflink.table import TableEnvironment, EnvironmentSettings env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) my_table = ... # type: Table, table schema: [a: String] # Register the java function. table_env.create_java_temporary_function(\u0026#34;split\u0026#34;, \u0026#34;my.java.function.Split\u0026#34;) # Use the table function in the Python Table API. \u0026#34;alias\u0026#34; specifies the field names of the table. my_table.join_lateral(call(\u0026#39;split\u0026#39;, col(\u0026#39;a\u0026#39;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;word\u0026#39;), col(\u0026#39;length\u0026#39;)) my_table.left_outer_join_lateral(call(\u0026#39;split\u0026#39;, col(\u0026#39;a\u0026#39;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;word\u0026#39;), col(\u0026#39;length\u0026#39;)) # Register the python function. # Use the table function in SQL with LATERAL and TABLE keywords. # CROSS JOIN a table function (equivalent to \u0026#34;join\u0026#34; in Table API). table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)\u0026#34;) # LEFT JOIN a table function (equivalent to \u0026#34;left_outer_join\u0026#34; in Table API). table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE\u0026#34;) Like Python scalar functions, you can use the above five ways to define Python TableFunctions.
Note The only difference is that the return type of Python Table Functions needs to be an iterable, iterator or generator.
# option 1: generator function @udtf(result_types=DataTypes.BIGINT()) def generator_func(x): yield 1 yield 2 # option 2: return iterator @udtf(result_types=DataTypes.BIGINT()) def iterator_func(x): return range(5) # option 3: return iterable @udtf(result_types=DataTypes.BIGINT()) def iterable_func(x): result = [1, 2, 3] return result Aggregate Functions # A user-defined aggregate function (UDAGG) maps scalar values of multiple rows to a new scalar value.
NOTE: Currently the general user-defined aggregate function is only supported in the GroupBy aggregation and Group Window Aggregation in streaming mode. For batch mode, it\u0026rsquo;s currently not supported and it is recommended to use the Vectorized Aggregate Functions.
The behavior of an aggregate function is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.
For each set of rows that need to be aggregated, the runtime will create an empty accumulator by calling create_accumulator(). Subsequently, the accumulate(...) method of the aggregate function will be called for each input row to update the accumulator. Currently after each row has been processed, the get_value(...) method of the aggregate function will be called to compute the aggregated result.
The following example illustrates the aggregation process:
In the above example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the highest price of all beverages in the table, i.e., perform a max() aggregation.
In order to define an aggregate function, one has to extend the base class AggregateFunction in pyflink.table and implement the evaluation method named accumulate(...). The result type and accumulator type of the aggregate function can be specified by one of the following two approaches:
Implement the method named get_result_type() and get_accumulator_type(). Wrap the function instance with the decorator udaf in pyflink.table.udf and specify the parameters result_type and accumulator_type. The following example shows how to define your own aggregate function and call it in a query.
from pyflink.common import Row from pyflink.table import AggregateFunction, DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import call from pyflink.table.udf import udaf from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble class WeightedAvg(AggregateFunction): def create_accumulator(self): # Row(sum, count) return Row(0, 0) def get_value(self, accumulator): if accumulator[1] == 0: return None else: return accumulator[0] / accumulator[1] def accumulate(self, accumulator, value, weight): accumulator[0] += value * weight accumulator[1] += weight def retract(self, accumulator, value, weight): accumulator[0] -= value * weight accumulator[1] -= weight def get_result_type(self): return DataTypes.BIGINT() def get_accumulator_type(self): return DataTypes.ROW([ DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udaf decorator: # weighted_avg = udaf(WeightedAvg(), result_type=DataTypes.BIGINT(), accumulator_type=...) weighted_avg = udaf(WeightedAvg()) t = table_env.from_elements([(1, 2, \u0026#34;Lee\u0026#34;), (3, 4, \u0026#34;Jay\u0026#34;), (5, 6, \u0026#34;Jay\u0026#34;), (7, 8, \u0026#34;Lee\u0026#34;)]).alias(\u0026#34;value\u0026#34;, \u0026#34;count\u0026#34;, \u0026#34;name\u0026#34;) # call function \u0026#34;inline\u0026#34; without registration in Table API result = t.group_by(col(\u0026#34;name\u0026#34;)).select(weighted_avg(col(\u0026#34;value\u0026#34;), col(\u0026#34;count\u0026#34;)).alias(\u0026#34;avg\u0026#34;)).execute() result.print() # register function table_env.create_temporary_function(\u0026#34;weighted_avg\u0026#34;, WeightedAvg()) # call registered function in Table API result = t.group_by(col(\u0026#34;name\u0026#34;)).select(call(\u0026#34;weighted_avg\u0026#34;, col(\u0026#34;value\u0026#34;), col(\u0026#34;count\u0026#34;)).alias(\u0026#34;avg\u0026#34;)).execute() result.print() # register table table_env.create_temporary_view(\u0026#34;source\u0026#34;, t) # call registered function in SQL result = table_env.sql_query( \u0026#34;SELECT weighted_avg(\`value\`, \`count\`) AS avg FROM source GROUP BY name\u0026#34;).execute() result.print() # use the general Python aggregate function in GroupBy Window Aggregation tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) result = t.window(tumble_window) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;name\u0026#39;)) \\ .select(col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, weighted_avg(col(\u0026#39;value\u0026#39;), col(\u0026#39;count\u0026#39;))) \\ .execute() result.print() The accumulate(...) method of our WeightedAvg class takes three input arguments. The first one is the accumulator and the other two are user-defined inputs. In order to calculate a weighted average value, the accumulator needs to store the weighted sum and count of all the data that have already been accumulated. In our example, we use a Row object as the accumulator. Accumulators will be managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of failover to ensure exactly-once semantics.
Mandatory and Optional Methods # The following methods are mandatory for each AggregateFunction:
create_accumulator() accumulate(...) get_value(...) The following methods of AggregateFunction are required depending on the use case:
retract(...) is required when there are operations that could generate retraction messages before the current aggregation operation, e.g. group aggregate, outer join. This method is optional, but it is strongly recommended to be implemented to ensure the UDAF can be used in any use case. merge(...) is required for session window ang hop window aggregations. get_result_type() and get_accumulator_type() is required if the result type and accumulator type would not be specified in the udaf decorator. ListView and MapView # If an accumulator needs to store large amounts of data, pyflink.table.ListView and pyflink.table.MapView could be used instead of list and dict. These two data structures provide the similar functionalities as list and dict, however usually having better performance by leveraging Flink\u0026rsquo;s state backend to eliminate unnecessary state access. You can use them by declaring DataTypes.LIST_VIEW(...) and DataTypes.MAP_VIEW(...) in the accumulator type, e.g.:
from pyflink.table import ListView class ListViewConcatAggregateFunction(AggregateFunction): def get_value(self, accumulator): # the ListView is iterable return accumulator[1].join(accumulator[0]) def create_accumulator(self): return Row(ListView(), \u0026#39;\u0026#39;) def accumulate(self, accumulator, *args): accumulator[1] = args[1] # the ListView support add, clear and iterate operations. accumulator[0].add(args[0]) def get_accumulator_type(self): return DataTypes.ROW([ # declare the first column of the accumulator as a string ListView. DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.LIST_VIEW(DataTypes.STRING())), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.STRING() Currently there are 2 limitations to use the ListView and MapView:
The accumulator must be a Row. The ListView and MapView must be the first level children of the Row accumulator. Please refer to the documentation of the corresponding classes for more information about this advanced feature.
NOTE: For reducing the data transmission cost between Python UDF worker and Java process caused by accessing the data in Flink states(e.g. accumulators and data views), there is a cached layer between the raw state handler and the Python state backend. You can adjust the values of these configuration options to change the behavior of the cache layer for best performance: python.state.cache-size, python.map-state.read-cache-size, python.map-state.write-cache-size, python.map-state.iterate-response-batch-size. For more details please refer to the Python Configuration Documentation.
Table Aggregate Functions # A user-defined table aggregate function (UDTAGG) maps scalar values of multiple rows to zero, one, or multiple rows (or structured types). The returned record may consist of one or more fields. If an output record consists of only a single field, the structured record can be omitted, and a scalar value can be emitted that will be implicitly wrapped into a row by the runtime.
NOTE: Currently the general user-defined table aggregate function is only supported in the GroupBy aggregation in streaming mode.
Similar to an aggregate function, the behavior of a table aggregate is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.
For each set of rows that needs to be aggregated, the runtime will create an empty accumulator by calling create_accumulator(). Subsequently, the accumulate(...) method of the function is called for each input row to update the accumulator. Once all rows have been processed, the emit_value(...) method of the function is called to compute and return the final result.
The following example illustrates the aggregation process:
In the example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the 2 highest prices of all beverages in the table, i.e., perform a TOP2() table aggregation. We need to consider each of the 5 rows. The result is a table with the top 2 values.
In order to define a table aggregate function, one has to extend the base class TableAggregateFunction in pyflink.table and implement one or more evaluation methods named accumulate(...).
The result type and accumulator type of the aggregate function can be specified by one of the following two approaches:
Implement the method named get_result_type() and get_accumulator_type(). Wrap the function instance with the decorator udtaf in pyflink.table.udf and specify the parameters result_type and accumulator_type. The following example shows how to define your own aggregate function and call it in a query.
from pyflink.common import Row from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udtaf, TableAggregateFunction class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row): if row[0] is not None: if accumulator[0] is None or row[0] \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row[0] elif accumulator[1] is None or row[0] \u0026gt; accumulator[1]: accumulator[1] = row[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udtaf decorator: # top2 = udtaf(Top2(), result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]), accumulator_type=DataTypes.ARRAY(DataTypes.BIGINT())) top2 = udtaf(Top2()) t = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # call function \u0026#34;inline\u0026#34; without registration in Table API t.group_by(col(\u0026#39;b\u0026#39;)).flat_aggregate(top2).select(col(\u0026#39;*\u0026#39;)).execute().print() # the result is: # b a # 0 Hi2 5.0 # 1 Hi2 NaN # 2 Hi 7.0 # 3 Hi 3.0 The accumulate(...) method of our Top2 class takes two inputs. The first one is the accumulator and the second one is the user-defined input. In order to calculate a result, the accumulator needs to store the 2 highest values of all the data that has been accumulated. Accumulators are automatically managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of a failure to ensure exactly-once semantics. The result values are emitted together with a ranking index.
Mandatory and Optional Methods # The following methods are mandatory for each TableAggregateFunction:
create_accumulator() accumulate(...) emit_value(...) The following methods of TableAggregateFunction are required depending on the use case:
retract(...) is required when there are operations that could generate retraction messages before the current aggregation operation, e.g. group aggregate, outer join. This method is optional, but it is strongly recommended to be implemented to ensure the UDTAF can be used in any use case. get_result_type() and get_accumulator_type() is required if the result type and accumulator type would not be specified in the udtaf decorator. ListView and MapView # Similar to Aggregation function, we can also use ListView and MapView in Table Aggregate Function.
from pyflink.common import Row from pyflink.table import ListView from pyflink.table.types import DataTypes from pyflink.table.udf import TableAggregateFunction class ListViewConcatTableAggregateFunction(TableAggregateFunction): def emit_value(self, accumulator): result = accumulator[1].join(accumulator[0]) yield Row(result) yield Row(result) def create_accumulator(self): return Row(ListView(), \u0026#39;\u0026#39;) def accumulate(self, accumulator, *args): accumulator[1] = args[1] accumulator[0].add(args[0]) def get_accumulator_type(self): return DataTypes.ROW([ DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.LIST_VIEW(DataTypes.STRING())), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.STRING())]) `}),e.add({id:153,href:"/flink/flink-docs-master/docs/concepts/glossary/",title:"Glossary",section:"Concepts",content:` Glossary # Checkpoint Storage # The location where the State Backend will store its snapshot during a checkpoint (Java Heap of JobManager or Filesystem).
Flink Application Cluster # A Flink Application Cluster is a dedicated Flink Cluster that only executes Flink Jobs from one Flink Application. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Application.
Flink Job Cluster # A Flink Job Cluster is a dedicated Flink Cluster that only executes a single Flink Job. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Job. This deployment mode has been deprecated since Flink 1.15.
Flink Cluster # A distributed system consisting of (typically) one JobManager and one or more Flink TaskManager processes.
Event # An event is a statement about a change of the state of the domain modelled by the application. Events can be input and/or output of a stream or batch processing application. Events are special types of records.
ExecutionGraph # see Physical Graph
Function # Functions are implemented by the user and encapsulate the application logic of a Flink program. Most Functions are wrapped by a corresponding Operator.
Instance # The term instance is used to describe a specific instance of a specific type (usually Operator or Function) during runtime. As Apache Flink is mostly written in Java, this corresponds to the definition of Instance or Object in Java. In the context of Apache Flink, the term parallel instance is also frequently used to emphasize that multiple instances of the same Operator or Function type are running in parallel.
Flink Application # A Flink application is a Java Application that submits one or multiple Flink Jobs from the main() method (or by some other means). Submitting jobs is usually done by calling execute() on an execution environment.
The jobs of an application can either be submitted to a long running Flink Session Cluster, to a dedicated Flink Application Cluster, or to a Flink Job Cluster.
Flink Job # A Flink Job is the runtime representation of a logical graph (also often called dataflow graph) that is created and submitted by calling execute() in a Flink Application.
JobGraph # see Logical Graph
Flink JobManager # The JobManager is the orchestrator of a Flink Cluster. It contains three distinct components: Flink Resource Manager, Flink Dispatcher and one Flink JobMaster per running Flink Job.
Flink JobMaster # JobMasters are one of the components running in the JobManager. A JobMaster is responsible for supervising the execution of the Tasks of a single job.
JobResultStore # The JobResultStore is a Flink component that persists the results of globally terminated (i.e. finished, cancelled or failed) jobs to a filesystem, allowing the results to outlive a finished job. These results are then used by Flink to determine whether jobs should be subject to recovery in highly-available clusters.
Logical Graph # A logical graph is a directed graph where the nodes are Operators and the edges define input/output-relationships of the operators and correspond to data streams or data sets. A logical graph is created by submitting jobs from a Flink Application.
Logical graphs are also often referred to as dataflow graphs.
Managed State # Managed State describes application state which has been registered with the framework. For Managed State, Apache Flink will take care about persistence and rescaling among other things.
Operator # Node of a Logical Graph. An Operator performs a certain operation, which is usually executed by a Function. Sources and Sinks are special Operators for data ingestion and data egress.
Operator Chain # An Operator Chain consists of two or more consecutive Operators without any repartitioning in between. Operators within the same Operator Chain forward records to each other directly without going through serialization or Flink\u0026rsquo;s network stack.
Partition # A partition is an independent subset of the overall data stream or data set. A data stream or data set is divided into partitions by assigning each record to one or more partitions. Partitions of data streams or data sets are consumed by Tasks during runtime. A transformation which changes the way a data stream or data set is partitioned is often called repartitioning.
Physical Graph # A physical graph is the result of translating a Logical Graph for execution in a distributed runtime. The nodes are Tasks and the edges indicate input/output-relationships or partitions of data streams or data sets.
Record # Records are the constituent elements of a data set or data stream. Operators and Functions receive records as input and emit records as output.
(Runtime) Execution Mode # DataStream API programs can be executed in one of two execution modes: BATCH or STREAMING. See Execution Mode for more details.
Flink Session Cluster # A long-running Flink Cluster which accepts multiple Flink Jobs for execution. The lifetime of this Flink Cluster is not bound to the lifetime of any Flink Job. Formerly, a Flink Session Cluster was also known as a Flink Cluster in session mode. Compare to Flink Application Cluster.
State Backend # For stream processing programs, the State Backend of a Flink Job determines how its state is stored on each TaskManager (Java Heap of TaskManager or (embedded) RocksDB).
Sub-Task # A Sub-Task is a Task responsible for processing a partition of the data stream. The term \u0026ldquo;Sub-Task\u0026rdquo; emphasizes that there are multiple parallel Tasks for the same Operator or Operator Chain.
Table Program # A generic term for pipelines declared with Flink\u0026rsquo;s relational APIs (Table API or SQL).
Task # Node of a Physical Graph. A task is the basic unit of work, which is executed by Flink\u0026rsquo;s runtime. Tasks encapsulate exactly one parallel instance of an Operator or Operator Chain.
Flink TaskManager # TaskManagers are the worker processes of a Flink Cluster. Tasks are scheduled to TaskManagers for execution. They communicate with each other to exchange data between subsequent Tasks.
Transformation # A Transformation is applied on one or more data streams or data sets and results in one or more output data streams or data sets. A transformation might change a data stream or data set on a per-record basis, but might also only change its partitioning or perform an aggregation. While Operators and Functions are the \u0026ldquo;physical\u0026rdquo; parts of Flink\u0026rsquo;s API, Transformations are only an API concept. Specifically, most transformations are implemented by certain Operators.
`}),e.add({id:154,href:"/flink/flink-docs-master/docs/libs/gelly/graph_algorithms/",title:"Graph Algorithms",section:"Graphs",content:` Graph Algorithms # The logic blocks with which the Graph API and top-level algorithms are assembled are accessible in Gelly as graph algorithms in the org.apache.flink.graph.asm package. These algorithms provide optimization and tuning through configuration parameters and may provide implicit runtime reuse when processing the same input with a similar configuration.
VertexInDegree # Annotate vertices of a directed graph with the in-degree.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; inDegree = graph .run(new VertexInDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with an in-degree of zero.
setParallelism: override the operator parallelism
VertexOutDegree # Annotate vertices of a directed graph with the out-degree.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; outDegree = graph .run(new VertexOutDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with an out-degree of zero.
setParallelism: override the operator parallelism
VertexDegrees # Annotate vertices of a directed graph with the degree, out-degree, and in-degree.
DataSet\u0026lt;Vertex\u0026lt;K, Tuple2\u0026lt;LongValue, LongValue\u0026gt;\u0026gt;\u0026gt; degrees = graph .run(new VertexDegrees().setIncludeZeroDegreeVertices(true)); Optional configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with out- and in-degree of zero.
setParallelism: override the operator parallelism.
EdgeSourceDegrees # Annotate edges of a directed graph with the degree, out-degree, and in-degree of the source ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, Degrees\u0026gt;\u0026gt;\u0026gt; sourceDegrees = graph .run(new EdgeSourceDegrees()); Optional configuration:
setParallelism: override the operator parallelism. EdgeTargetDegrees # Annotate edges of a directed graph with the degree, out-degree, and in-degree of the target ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, Degrees\u0026gt;\u0026gt;\u0026gt; targetDegrees = graph .run(new EdgeTargetDegrees()); Optional configuration:
setParallelism: override the operator parallelism. EdgeDegreesPair # Annotate edges of a directed graph with the degree, out-degree, and in-degree of both the source and target vertices.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; degree = graph .run(new VertexDegree() .setIncludeZeroDegreeVertices(true) .setReduceOnTargetId(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with a degree of zero
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
EdgeSourceDegree # Annotate edges of an undirected graph with degree of the source ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, LongValue\u0026gt;\u0026gt;\u0026gt; sourceDegree = graph .run(new EdgeSourceDegree() .setReduceOnTargetId(true)); Optional Configuration:
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
EdgeTargetDegree # Annotate edges of an undirected graph with degree of the target ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, LongValue\u0026gt;\u0026gt;\u0026gt; targetDegree = graph .run(new EdgeTargetDegree() .setReduceOnSourceId(true)); Optional configuration:
setParallelism: override the operator parallelism
setReduceOnSourceId: the degree can be counted from either the edge source or target IDs. By default the target IDs are counted. Reducing on source IDs may optimize the algorithm if the input edge list is sorted by source ID.
EdgeDegreePair # Annotate edges of an undirected graph with the degree of both the source and target vertices.
DataSet\u0026lt;Edge\u0026lt;K, Tuple3\u0026lt;EV, LongValue, LongValue\u0026gt;\u0026gt;\u0026gt; pairDegree = graph .run(new EdgeDegreePair().setReduceOnTargetId(true)); Optional configuration:
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
MaximumDegree # Filter an undirected graph by maximum degree.
Graph\u0026lt;K, VV, EV\u0026gt; filteredGraph = graph .run(new MaximumDegree(5000) .setBroadcastHighDegreeVertices(true) .setReduceOnTargetId(true)); Optional configuration:
setBroadcastHighDegreeVertices: join high-degree vertices using a broadcast-hash to reduce data shuffling when removing a relatively small number of high-degree vertices.
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
Simplify # Remove self-loops and duplicate edges from a directed graph.
graph.run(new Simplify()); TranslateGraphIds # Translate vertex and edge IDs using the given TranslateFunction.
graph.run(new TranslateGraphIds(new LongValueToStringValue())); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism TranslateVertexValues # Translate vertex values using the given TranslateFunction.
graph.run(new TranslateVertexValues(new LongValueAddOffset(vertexCount))); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism TranslateEdgeValues # Translate edge values using the given TranslateFunction.
graph.run(new TranslateEdgeValues(new Nullify())); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism Back to top
`}),e.add({id:155,href:"/flink/flink-docs-master/docs/connectors/table/hive/hive_functions/",title:"Hive Functions",section:"Hive",content:` Hive Functions # Use Hive Built-in Functions via HiveModule # The HiveModule provides Hive built-in functions as Flink system (built-in) functions to Flink SQL and Table API users.
For detailed information, please refer to HiveModule.
Java String name = \u0026#34;myhive\u0026#34;; String version = \u0026#34;2.3.4\u0026#34;; tableEnv.loadModue(name, new HiveModule(version)); Scala val name = \u0026#34;myhive\u0026#34; val version = \u0026#34;2.3.4\u0026#34; tableEnv.loadModue(name, new HiveModule(version)); Python from pyflink.table.module import HiveModule name = \u0026#34;myhive\u0026#34; version = \u0026#34;2.3.4\u0026#34; t_env.load_module(name, HiveModule(version)) YAML modules: - name: core type: core - name: myhive type: hive Some Hive built-in functions in older versions have thread safety issues. We recommend users patch their own Hive to fix them. Hive User Defined Functions # Users can use their existing Hive User Defined Functions in Flink.
Supported UDF types include:
UDF GenericUDF GenericUDTF UDAF GenericUDAFResolver2 Upon query planning and execution, Hive\u0026rsquo;s UDF and GenericUDF are automatically translated into Flink\u0026rsquo;s ScalarFunction, Hive\u0026rsquo;s GenericUDTF is automatically translated into Flink\u0026rsquo;s TableFunction, and Hive\u0026rsquo;s UDAF and GenericUDAFResolver2 are translated into Flink\u0026rsquo;s AggregateFunction.
To use a Hive User Defined Function, user have to
set a HiveCatalog backed by Hive Metastore that contains that function as current catalog of the session include a jar that contains that function in Flink\u0026rsquo;s classpath Using Hive User Defined Functions # Assuming we have the following Hive functions registered in Hive Metastore:
/** * Test simple udf. Registered under name \u0026#39;myudf\u0026#39; */ public class TestHiveSimpleUDF extends UDF { public IntWritable evaluate(IntWritable i) { return new IntWritable(i.get()); } public Text evaluate(Text text) { return new Text(text.toString()); } } /** * Test generic udf. Registered under name \u0026#39;mygenericudf\u0026#39; */ public class TestHiveGenericUDF extends GenericUDF { @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { checkArgument(arguments.length == 2); checkArgument(arguments[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) arguments[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); if (arguments[0] instanceof IntObjectInspector || arguments[0] instanceof StringObjectInspector) { return arguments[0]; } else { throw new RuntimeException(\u0026#34;Not support argument: \u0026#34; + arguments[0]); } } @Override public Object evaluate(DeferredObject[] arguments) throws HiveException { return arguments[0].get(); } @Override public String getDisplayString(String[] children) { return \u0026#34;TestHiveGenericUDF\u0026#34;; } } /** * Test split udtf. Registered under name \u0026#39;mygenericudtf\u0026#39; */ public class TestHiveUDTF extends GenericUDTF { @Override public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException { checkArgument(argOIs.length == 2); // TEST for constant arguments checkArgument(argOIs[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) argOIs[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); return ObjectInspectorFactory.getStandardStructObjectInspector( Collections.singletonList(\u0026#34;col1\u0026#34;), Collections.singletonList(PrimitiveObjectInspectorFactory.javaStringObjectInspector)); } @Override public void process(Object[] args) throws HiveException { String str = (String) args[0]; for (String s : str.split(\u0026#34;,\u0026#34;)) { forward(s); forward(s); } } @Override public void close() { } } From Hive CLI, we can see they are registered:
hive\u0026gt; show functions; OK ...... mygenericudf myudf myudtf Then, users can use them in SQL as:
Flink SQL\u0026gt; select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b, s from mysourcetable, lateral table(myudtf(name, 1)) as T(s); `}),e.add({id:156,href:"/flink/flink-docs-master/docs/internals/job_scheduling/",title:"Jobs and Scheduling",section:"Internals",content:` Jobs and Scheduling # This document briefly describes how Flink schedules jobs and how it represents and tracks job status on the JobManager.
Scheduling # Execution resources in Flink are defined through Task Slots. Each TaskManager will have one or more task slots, each of which can run one pipeline of parallel tasks. A pipeline consists of multiple successive tasks, such as the n-th parallel instance of a MapFunction together with the n-th parallel instance of a ReduceFunction. Note that Flink often executes successive tasks concurrently: For Streaming programs, that happens in any case, but also for batch programs, it happens frequently.
The figure below illustrates that. Consider a program with a data source, a MapFunction, and a ReduceFunction. The source and MapFunction are executed with a parallelism of 4, while the ReduceFunction is executed with a parallelism of 3. A pipeline consists of the sequence Source - Map - Reduce. On a cluster with 2 TaskManagers with 3 slots each, the program will be executed as described below.
Internally, Flink defines through SlotSharingGroup and CoLocationGroup which tasks may share a slot (permissive), respectively which tasks must be strictly placed into the same slot.
JobManager Data Structures # During job execution, the JobManager keeps track of distributed tasks, decides when to schedule the next task (or set of tasks), and reacts to finished tasks or execution failures.
The JobManager receives the JobGraph , which is a representation of the data flow consisting of operators ( JobVertex ) and intermediate results ( IntermediateDataSet ). Each operator has properties, like the parallelism and the code that it executes. In addition, the JobGraph has a set of attached libraries, that are necessary to execute the code of the operators.
The JobManager transforms the JobGraph into an ExecutionGraph . The ExecutionGraph is a parallel version of the JobGraph: For each JobVertex, it contains an ExecutionVertex per parallel subtask. An operator with a parallelism of 100 will have one JobVertex and 100 ExecutionVertices. The ExecutionVertex tracks the state of execution of a particular subtask. All ExecutionVertices from one JobVertex are held in an ExecutionJobVertex , which tracks the status of the operator as a whole. Besides the vertices, the ExecutionGraph also contains the IntermediateResult and the IntermediateResultPartition . The former tracks the state of the IntermediateDataSet, the latter the state of each of its partitions.
Each ExecutionGraph has a job status associated with it. This job status indicates the current state of the job execution.
A Flink job is first in the created state, then switches to running and upon completion of all work it switches to finished. In case of failures, a job switches first to failing where it cancels all running tasks. If all job vertices have reached a final state and the job is not restartable, then the job transitions to failed. If the job can be restarted, then it will enter the restarting state. Once the job has been completely restarted, it will reach the created state.
In case that the user cancels the job, it will go into the cancelling state. This also entails the cancellation of all currently running tasks. Once all running tasks have reached a final state, the job transitions to the state cancelled.
Unlike the states finished, canceled and failed which denote a globally terminal state and, thus, trigger the clean up of the job, the suspended state is only locally terminal. Locally terminal means that the execution of the job has been terminated on the respective JobManager but another JobManager of the Flink cluster can retrieve the job from the persistent HA store and restart it. Consequently, a job which reaches the suspended state won\u0026rsquo;t be completely cleaned up.
During the execution of the ExecutionGraph, each parallel task goes through multiple stages, from created to finished or failed. The diagram below illustrates the states and possible transitions between them. A task may be executed multiple times (for example in the course of failure recovery). For that reason, the execution of an ExecutionVertex is tracked in an Execution . Each ExecutionVertex has a current Execution, and prior Executions.
Back to top
`}),e.add({id:157,href:"/flink/flink-docs-master/docs/connectors/datastream/kinesis/",title:"Kinesis",section:"DataStream Connectors",content:` Amazon Kinesis Data Streams Connector # The Kinesis connector provides access to Amazon Kinesis Data Streams.
To use this connector, add one or more of the following dependencies to your project, depending on whether you are reading from and/or writing to Kinesis Data Streams:
KDS Connectivity Maven Dependency Source \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Sink \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-aws-kinesis-streams\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Due to the licensing issue, the flink-connector-kinesis artifact is not deployed to Maven central for the prior versions. Please see the version specific documentation for further information.
In order to use the Kinesis connector in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. Using the Amazon Kinesis Streams Service # Follow the instructions from the Amazon Kinesis Streams Developer Guide to setup Kinesis streams.
Configuring Access to Kinesis with IAM # Make sure to create the appropriate IAM policy to allow reading / writing to / from the Kinesis streams. See examples here.
Depending on your deployment you would choose a different Credentials Provider to allow access to Kinesis. By default, the AUTO Credentials Provider is used. If the access key ID and secret key are set in the configuration, the BASIC provider is used.
A specific Credentials Provider can optionally be set by using the AWSConfigConstants.AWS_CREDENTIALS_PROVIDER setting.
Supported Credential Providers are:
AUTO - Using the default AWS Credentials Provider chain that searches for credentials in the following order: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE and EC2/ECS credentials provider. BASIC - Using access key ID and secret key supplied as configuration. ENV_VAR - Using AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY environment variables. SYS_PROP - Using Java system properties aws.accessKeyId and aws.secretKey. PROFILE - Use AWS credentials profile file to create the AWS credentials. ASSUME_ROLE - Create AWS credentials by assuming a role. The credentials for assuming the role must be supplied. WEB_IDENTITY_TOKEN - Create AWS credentials by assuming a role using Web Identity Token. Kinesis Consumer # The FlinkKinesisConsumer is an exactly-once parallel streaming data source that subscribes to multiple AWS Kinesis streams within the same AWS service region, and can transparently handle resharding of streams while the job is running. Each subtask of the consumer is responsible for fetching data records from multiple Kinesis shards. The number of shards fetched by each subtask will change as shards are closed and created by Kinesis.
Before consuming data from Kinesis streams, make sure that all streams1 are created with the status \u0026ldquo;ACTIVE\u0026rdquo; in the Amazon Kinesis Data Stream console.
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); consumerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) consumerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) val env = StreamExecutionEnvironment.getExecutionEnvironment val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer(\u0026#34;stream-1\u0026#34;, SimpleStringSchema(), consumer_config)) The above is a simple example of using the consumer. Configuration for the consumer is supplied with a java.util.Properties instance, the configuration keys for which can be found in AWSConfigConstants (AWS-specific parameters) and ConsumerConfigConstants (Kinesis consumer parameters). The example demonstrates consuming a single Kinesis stream in the AWS region \u0026ldquo;us-east-1\u0026rdquo;. The AWS credentials are supplied using the basic method in which the AWS access key ID and secret access key are directly supplied in the configuration. Also, data is being consumed from the newest position in the Kinesis stream (the other option will be setting ConsumerConfigConstants.STREAM_INITIAL_POSITION to TRIM_HORIZON, which lets the consumer start reading the Kinesis stream from the earliest record possible).
Other optional configuration keys for the consumer can be found in ConsumerConfigConstants.
Note that the configured parallelism of the Flink Kinesis Consumer source can be completely independent of the total number of shards in the Kinesis streams. When the number of shards is larger than the parallelism of the consumer, then each consumer subtask can subscribe to multiple shards; otherwise if the number of shards is smaller than the parallelism of the consumer, then some consumer subtasks will simply be idle and wait until it gets assigned new shards (i.e., when the streams are resharded to increase the number of shards for higher provisioned Kinesis service throughput).
Also note that the default assignment of shards to subtasks is based on the hashes of the shard and stream names, which will more-or-less balance the shards across the subtasks. However, assuming the default Kinesis shard management is used on the stream (UpdateShardCount with UNIFORM_SCALING), setting UniformShardAssigner as the shard assigner on the consumer will much more evenly distribute shards to subtasks. Assuming the incoming Kinesis records are assigned random Kinesis PartitionKey or ExplicitHashKey values, the result is consistent subtask loading. If neither the default assigner nor the UniformShardAssigner suffice, a custom implementation of KinesisShardAssigner can be set.
The DeserializationSchema # Flink Kinesis Consumer also needs a schema to know how to turn the binary data in a Kinesis Data Stream into Java objects. The KinesisDeserializationSchema allows users to specify such a schema. The T deserialize(byte[] recordValue, String partitionKey, String seqNum, long approxArrivalTimestamp, String stream, String shardId) method gets called for each Kinesis record.
For convenience, Flink provides the following schemas out of the box:
TypeInformationSerializationSchema which creates a schema based on a Flink\u0026rsquo;s TypeInformation. This is useful if the data is both written and read by Flink. This schema is a performant Flink-specific alternative to other generic serialization approaches.
GlueSchemaRegistryJsonDeserializationSchema offers the ability to lookup the writer\u0026rsquo;s schema (schema which was used to write the record) in AWS Glue Schema Registry. Using this, deserialization schema record will be read with the schema retrieved from AWS Glue Schema Registry and transformed to either com.amazonaws.services.schemaregistry.serializers.json.JsonDataWithSchema that represents generic record with a manually provided schema or a JAVA POJO generated by mbknor-jackson-jsonSchema.
To use this deserialization schema one has to add the following additional dependency:
GlueSchemaRegistryJsonDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-jsonschema-glue-schema-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! AvroDeserializationSchema which reads data serialized with Avro format using a statically provided schema. It can infer the schema from Avro generated classes (AvroDeserializationSchema.forSpecific(...)) or it can work with GenericRecords with a manually provided schema (with AvroDeserializationSchema.forGeneric(...)). This deserialization schema expects that the serialized records DO NOT contain the embedded schema.
You can use AWS Glue Schema Registry to retrieve the writer’s schema. Similarly, the deserialization record will be read with the schema from AWS Glue Schema Registry and transformed (either through GlueSchemaRegistryAvroDeserializationSchema.forGeneric(...) or GlueSchemaRegistryAvroDeserializationSchema.forSpecific(...)). For more information on integrating the AWS Glue Schema Registry with Apache Flink see Use Case: Amazon Kinesis Data Analytics for Apache Flink. To use this deserialization schema one has to add the following additional dependency:
AvroDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! GlueSchemaRegistryAvroDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-glue-schema-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Configuring Starting Position # The Flink Kinesis Consumer currently provides the following options to configure where to start reading Kinesis streams, simply by setting ConsumerConfigConstants.STREAM_INITIAL_POSITION to one of the following values in the provided configuration properties (the naming of the options identically follows the namings used by the AWS Kinesis Streams service):
LATEST: read all shards of all streams starting from the latest record. TRIM_HORIZON: read all shards of all streams starting from the earliest record possible (data may be trimmed by Kinesis depending on the retention settings). AT_TIMESTAMP: read all shards of all streams starting from a specified timestamp. The timestamp must also be specified in the configuration properties by providing a value for ConsumerConfigConstants.STREAM_INITIAL_TIMESTAMP, in one of the following date pattern : a non-negative double value representing the number of seconds that has elapsed since the Unix epoch (for example, 1459799926.480). a user defined pattern, which is a valid pattern for SimpleDateFormat provided by ConsumerConfigConstants.STREAM_TIMESTAMP_DATE_FORMAT. If ConsumerConfigConstants.STREAM_TIMESTAMP_DATE_FORMAT is not defined then the default pattern will be yyyy-MM-dd'T'HH:mm:ss.SSSXXX (for example, timestamp value is 2016-04-04 and pattern is yyyy-MM-dd given by user or timestamp value is 2016-04-04T19:58:46.480-00:00 without given a pattern). Fault Tolerance for Exactly-Once User-Defined State Update Semantics # With Flink\u0026rsquo;s checkpointing enabled, the Flink Kinesis Consumer will consume records from shards in Kinesis streams and periodically checkpoint each shard\u0026rsquo;s progress. In case of a job failure, Flink will restore the streaming program to the state of the latest complete checkpoint and re-consume the records from Kinesis shards, starting from the progress that was stored in the checkpoint.
The interval of drawing checkpoints therefore defines how much the program may have to go back at most, in case of a failure.
To use fault tolerant Kinesis Consumers, checkpointing of the topology needs to be enabled at the execution environment:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); // checkpoint every 5000 msecs Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableCheckpointing(5000) // checkpoint every 5000 msecs Python env = StreamExecutionEnvironment.get_execution_environment() env.enable_checkpointing(5000) # checkpoint every 5000 msecs Also note that Flink can only restart the topology if enough processing slots are available to restart the topology. Therefore, if the topology fails due to loss of a TaskManager, there must still be enough slots available afterwards. Flink on YARN supports automatic restart of lost YARN containers.
Using Enhanced Fan-Out # Enhanced Fan-Out (EFO) increases the maximum number of concurrent consumers per Kinesis stream. Without EFO, all concurrent consumers share a single read quota per shard. Using EFO, each consumer gets a distinct dedicated read quota per shard, allowing read throughput to scale with the number of consumers. Using EFO will incur additional cost.
In order to enable EFO two additional configuration parameters are required:
RECORD_PUBLISHER_TYPE: Determines whether to use EFO or POLLING. The default RecordPublisher is POLLING. EFO_CONSUMER_NAME: A name to identify the consumer. For a given Kinesis data stream, each consumer must have a unique name. However, consumer names do not have to be unique across data streams. Reusing a consumer name will result in existing subscriptions being terminated. The code snippet below shows a simple example configurating an EFO consumer.
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) EFO Stream Consumer Registration/Deregistration # In order to use EFO, a stream consumer must be registered against each stream you wish to consume. By default, the FlinkKinesisConsumer will register the stream consumer automatically when the Flink job starts. The stream consumer will be registered using the name provided by the EFO_CONSUMER_NAME configuration. FlinkKinesisConsumer provides three registration strategies:
Registration LAZY (default): Stream consumers are registered when the Flink job starts running. If the stream consumer already exists, it will be reused. This is the preferred strategy for the majority of applications. However, jobs with parallelism greater than 1 will result in tasks competing to register and acquire the stream consumer ARN. For jobs with very large parallelism this can result in an increased start-up time. The describe operation has a limit of 20 transactions per second, this means application startup time will increase by roughly parallelism/20 seconds. EAGER: Stream consumers are registered in the FlinkKinesisConsumer constructor. If the stream consumer already exists, it will be reused. This will result in registration occurring when the job is constructed, either on the Flink Job Manager or client environment submitting the job. Using this strategy results in a single thread registering and retrieving the stream consumer ARN, reducing startup time over LAZY (with large parallelism). However, consider that the client environment will require access to the AWS services. NONE: Stream consumer registration is not performed by FlinkKinesisConsumer. Registration must be performed externally using the AWS CLI or SDK to invoke RegisterStreamConsumer. Stream consumer ARNs should be provided to the job via the consumer configuration. Deregistration LAZY|EAGER (default): Stream consumers are deregistered when the job is shutdown gracefully. In the event that a job terminates within executing the shutdown hooks, stream consumers will remain active. In this situation the stream consumers will be gracefully reused when the application restarts. NONE: Stream consumer deregistration is not performed by FlinkKinesisConsumer. Below is an example configuration to use the EAGER registration strategy:
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.EAGER.name()); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.EAGER.name()); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39;, \u0026#39;flink.stream.efo.registration\u0026#39;: \u0026#39;EAGER\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) Below is an example configuration to use the NONE registration strategy:
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.NONE.name()); consumerConfig.put(ConsumerConfigConstants.efoConsumerArn(\u0026#34;stream-name\u0026#34;), \u0026#34;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.NONE.name()); consumerConfig.put(ConsumerConfigConstants.efoConsumerArn(\u0026#34;stream-name\u0026#34;), \u0026#34;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#34;); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39;, \u0026#39;flink.stream.efo.consumerarn.stream-name\u0026#39;: \u0026#39;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) Event Time for Consumed Records # If streaming topologies choose to use the event time notion for record timestamps, an approximate arrival timestamp will be used by default. This timestamp is attached to records by Kinesis once they were successfully received and stored by streams. Note that this timestamp is typically referred to as a Kinesis server-side timestamp, and there are no guarantees about the accuracy or order correctness (i.e., the timestamps may not always be ascending).
Users can choose to override this default with a custom timestamp, as described here, or use one from the predefined ones. After doing so, it can be passed to the consumer in the following way:
Java FlinkKinesisConsumer\u0026lt;String\u0026gt; consumer = new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), kinesisConsumerConfig); consumer.setPeriodicWatermarkAssigner(new CustomAssignerWithPeriodicWatermarks()); DataStream\u0026lt;String\u0026gt; stream = env .addSource(consumer) .print(); Scala val consumer = new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), kinesisConsumerConfig); consumer.setPeriodicWatermarkAssigner(new CustomAssignerWithPeriodicWatermarks()); val stream = env .addSource(consumer) .print(); Python consumer = FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config) stream = env.add_source(consumer).print() Internally, an instance of the assigner is executed per shard / consumer thread (see threading model below). When an assigner is specified, for each record read from Kinesis, the extractTimestamp(T element, long previousElementTimestamp) is called to assign a timestamp to the record and getCurrentWatermark() to determine the new watermark for the shard. The watermark of the consumer subtask is then determined as the minimum watermark of all its shards and emitted periodically. The per shard watermark is essential to deal with varying consumption speed between shards, that otherwise could lead to issues with downstream logic that relies on the watermark, such as incorrect late data dropping.
By default, the watermark is going to stall if shards do not deliver new records. The property ConsumerConfigConstants.SHARD_IDLE_INTERVAL_MILLIS can be used to avoid this potential issue through a timeout that will allow the watermark to progress despite of idle shards.
Event Time Alignment for Shard Consumers # The Flink Kinesis Consumer optionally supports synchronization between parallel consumer subtasks (and their threads) to avoid the event time skew related problems described in Event time synchronization across sources.
To enable synchronization, set the watermark tracker on the consumer:
Java JobManagerWatermarkTracker watermarkTracker = new JobManagerWatermarkTracker(\u0026#34;myKinesisSource\u0026#34;); consumer.setWatermarkTracker(watermarkTracker); Python watermark_tracker = WatermarkTracker.job_manager_watermark_tracker(\u0026#34;myKinesisSource\u0026#34;) consumer.set_watermark_tracker(watermark_tracker) The JobManagerWatermarkTracker will use a global aggregate to synchronize the per subtask watermarks. Each subtask uses a per shard queue to control the rate at which records are emitted downstream based on how far ahead of the global watermark the next record in the queue is.
The \u0026ldquo;emit ahead\u0026rdquo; limit is configured via ConsumerConfigConstants.WATERMARK_LOOKAHEAD_MILLIS. Smaller values reduce the skew but also the throughput. Larger values will allow the subtask to proceed further before waiting for the global watermark to advance.
Another variable in the throughput equation is how frequently the watermark is propagated by the tracker. The interval can be configured via ConsumerConfigConstants.WATERMARK_SYNC_MILLIS. Smaller values reduce emitter waits and come at the cost of increased communication with the job manager.
Since records accumulate in the queues when skew occurs, increased memory consumption needs to be expected. How much depends on the average record size. With larger sizes, it may be necessary to adjust the emitter queue capacity via ConsumerConfigConstants.WATERMARK_SYNC_QUEUE_CAPACITY.
Threading Model # The Flink Kinesis Consumer uses multiple threads for shard discovery and data consumption.
Shard Discovery # For shard discovery, each parallel consumer subtask will have a single thread that constantly queries Kinesis for shard information even if the subtask initially did not have shards to read from when the consumer was started. In other words, if the consumer is run with a parallelism of 10, there will be a total of 10 threads constantly querying Kinesis regardless of the total amount of shards in the subscribed streams.
Polling (default) Record Publisher # For POLLING data consumption, a single thread will be created to consume each discovered shard. Threads will terminate when the shard it is responsible of consuming is closed as a result of stream resharding. In other words, there will always be one thread per open shard.
Enhanced Fan-Out Record Publisher # For EFO data consumption the threading model is the same as POLLING, with additional thread pools to handle asynchronous communication with Kinesis. AWS SDK v2.x KinesisAsyncClient uses additional threads for Netty to handle IO and asynchronous response. Each parallel consumer subtask will have their own instance of the KinesisAsyncClient. In other words, if the consumer is run with a parallelism of 10, there will be a total of 10 KinesisAsyncClient instances. A separate client will be created and subsequently destroyed when registering and deregistering stream consumers.
Internally Used Kinesis APIs # The Flink Kinesis Consumer uses the AWS Java SDK internally to call Kinesis APIs for shard discovery and data consumption. Due to Amazon\u0026rsquo;s service limits for Kinesis Streams on the APIs, the consumer will be competing with other non-Flink consuming applications that the user may be running. Below is a list of APIs called by the consumer with description of how the consumer uses the API, as well as information on how to deal with any errors or warnings that the Flink Kinesis Consumer may have due to these service limits.
Shard Discovery # ListShards: this is constantly called by a single thread in each parallel consumer subtask to discover any new shards as a result of stream resharding. By default, the consumer performs the shard discovery at an interval of 10 seconds, and will retry indefinitely until it gets a result from Kinesis. If this interferes with other non-Flink consuming applications, users can slow down the consumer of calling this API by setting a value for ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS in the supplied configuration properties. This sets the discovery interval to a different value. Note that this setting directly impacts the maximum delay of discovering a new shard and starting to consume it, as shards will not be discovered during the interval. Polling (default) Record Publisher # GetShardIterator: this is called only once when per shard consuming threads are started, and will retry if Kinesis complains that the transaction limit for the API has exceeded, up to a default of 3 attempts. Note that since the rate limit for this API is per shard (not per stream), the consumer itself should not exceed the limit. Usually, if this happens, users can either try to slow down any other non-Flink consuming applications of calling this API, or modify the retry behaviour of this API call in the consumer by setting keys prefixed by ConsumerConfigConstants.SHARD_GETITERATOR_* in the supplied configuration properties.
GetRecords: this is constantly called by per shard consuming threads to fetch records from Kinesis. When a shard has multiple concurrent consumers (when there are any other non-Flink consuming applications running), the per shard rate limit may be exceeded. By default, on each call of this API, the consumer will retry if Kinesis complains that the data size / transaction limit for the API has exceeded, up to a default of 3 attempts. Users can either try to slow down other non-Flink consuming applications, or adjust the throughput of the consumer by setting the ConsumerConfigConstants.SHARD_GETRECORDS_MAX and ConsumerConfigConstants.SHARD_GETRECORDS_INTERVAL_MILLIS keys in the supplied configuration properties. Setting the former adjusts the maximum number of records each consuming thread tries to fetch from shards on each call (default is 10,000), while the latter modifies the sleep interval between each fetch (default is 200). The retry behaviour of the consumer when calling this API can also be modified by using the other keys prefixed by ConsumerConfigConstants.SHARD_GETRECORDS_*.
Enhanced Fan-Out Record Publisher # SubscribeToShard: this is called by per shard consuming threads to obtain shard subscriptions. A shard subscription is typically active for 5 minutes, but subscriptions will be reaquired if any recoverable errors are thrown. Once a subscription is acquired, the consumer will receive a stream of SubscribeToShardEventss. Retry and backoff parameters can be configured using the ConsumerConfigConstants.SUBSCRIBE_TO_SHARD_* keys.
DescribeStreamSummary: this is called once per stream, during stream consumer registration. By default, the LAZY registration strategy will scale the number of calls by the job parallelism. EAGER will invoke this once per stream and NONE will not invoke this API. Retry and backoff parameters can be configured using the ConsumerConfigConstants.STREAM_DESCRIBE_* keys.
DescribeStreamConsumer: this is called during stream consumer registration and deregistration. For each stream this service will be invoked periodically until the stream consumer is reported ACTIVE/not found for registration/deregistration. By default, the LAZY registration strategy will scale the number of calls by the job parallelism. EAGER will call the service once per stream for registration only. NONE will not invoke this service. Retry and backoff parameters can be configured using the ConsumerConfigConstants.DESCRIBE_STREAM_CONSUMER_* keys.
RegisterStreamConsumer: this is called once per stream during stream consumer registration, unless the NONE registration strategy is configured. Retry and backoff parameters can be configured using the ConsumerConfigConstants.REGISTER_STREAM_* keys.
DeregisterStreamConsumer: this is called once per stream during stream consumer deregistration, unless the NONE or EAGER registration strategy is configured. Retry and backoff parameters can be configured using the ConsumerConfigConstants.DEREGISTER_STREAM_* keys.
Kinesis Streams Sink # The Kinesis Streams sink (hereafter \u0026ldquo;Kinesis sink\u0026rdquo;) uses the AWS v2 SDK for Java to write data from a Flink stream into a Kinesis stream.
To write data into a Kinesis stream, make sure the stream is marked as \u0026ldquo;ACTIVE\u0026rdquo; in the Amazon Kinesis Data Stream console.
For the monitoring to work, the user accessing the stream needs access to the CloudWatch service.
Java Properties sinkProperties = new Properties(); // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); KinesisStreamsSink\u0026lt;String\u0026gt; kdsSink = KinesisStreamsSink.\u0026lt;String\u0026gt;builder() .setKinesisClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setPartitionKeyGenerator(element -\u0026gt; String.valueOf(element.hashCode())) // Required .setStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(5 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1 * 1024 * 1024) // Optional .build(); DataStream\u0026lt;String\u0026gt; simpleStringStream = ...; simpleStringStream.sinkTo(kdsSink); Scala val sinkProperties = new Properties() // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) val kdsSink = KinesisStreamsSink.\u0026lt;String\u0026gt;builder() .setKinesisClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setPartitionKeyGenerator(element -\u0026gt; String.valueOf(element.hashCode())) // Required .setStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10000) // Optional .setMaxBatchSizeInBytes(5 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1 * 1024 * 1024) // Optional .build() val simpleStringStream = ... simpleStringStream.sinkTo(kdsSink) Python # Required sink_properties = { # Required \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, # Optional, provide via alternative routes e.g. environment variables \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4567\u0026#39; } kds_sink = KinesisStreamsSink.builder() \\ .set_kinesis_client_properties(sink_properties) \\ # Required .set_serialization_schema(SimpleStringSchema()) \\ # Required .set_partition_key_generator(PartitionKeyGenerator.fixed()) \\ # Required .set_stream_name(\u0026#34;your-stream-name\u0026#34;) \\ # Required .set_fail_on_error(False) \\ # Optional .set_max_batch_size(500) \\ # Optional .set_max_in_flight_requests(50) \\ # Optional .set_max_buffered_requests(10000) \\ # Optional .set_max_batch_size_in_bytes(5 * 1024 * 1024) \\ # Optional .set_max_time_in_buffer_ms(5000) \\ # Optional .set_max_record_size_in_bytes(1 * 1024 * 1024) \\ # Optional .build() simple_string_stream = ... simple_string_stream.sink_to(kds_sink) The above is a simple example of using the Kinesis sink. Begin by creating a java.util.Properties instance with the AWS_REGION, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY configured. You can then construct the sink with the builder. The default values for the optional configurations are shown above. Some of these values have been set as a result of configuration on KDS.
You will always need to specify your serialization schema and logic for generating a partition key from a record.
Some or all of the records in a request may fail to be persisted by Kinesis Data Streams for a number of reasons. If failOnError is on, then a runtime exception will be raised. Otherwise those records will be requeued in the buffer for retry.
The Kinesis Sink provides some metrics through Flink\u0026rsquo;s metrics system to analyze the behavior of the connector. A list of all exposed metrics may be found here.
The sink default maximum record size is 1MB and maximum batch size is 5MB in line with the Kinesis Data Streams maximums. The AWS documentation detailing these maximums may be found here.
Kinesis Sinks and Fault Tolerance # The sink is designed to participate in Flink\u0026rsquo;s checkpointing to provide at-least-once processing guarantees. It does this by completing any in-flight requests while taking a checkpoint. This effectively assures all requests that were triggered before the checkpoint have been successfully delivered to Kinesis Data Streams, before proceeding to process more records.
If Flink needs to restore from a checkpoint (or savepoint), data that has been written since that checkpoint will be written to Kinesis again, leading to duplicates in the stream. Moreover, the sink uses the PutRecords API call internally, which does not guarantee to maintain the order of events.
Backpressure # Backpressure in the sink arises as the sink buffer fills up and writes to the sink begins to exhibit blocking behaviour. More information on the rate restrictions of Kinesis Data Streams may be found at Quotas and Limits.
You generally reduce backpressure by increasing the size of the internal queue:
Java KinesisStreamsSink\u0026lt;String\u0026gt; kdsSink = KinesisStreamsSink.\u0026lt;String\u0026gt;builder() ... .setMaxBufferedRequests(10_000) ... Python kds_sink = KinesisStreamsSink.builder() \\ .set_max_buffered_requests(10000) \\ .build() Kinesis Producer # The old Kinesis sink org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer is deprecated and may be removed with a future release of Flink, please use Kinesis Sink instead. The new sink uses the AWS v2 SDK for Java whereas the old sink uses the Kinesis Producer Library. Because of this, the new Kinesis sink does not support aggregation.
Using Custom Kinesis Endpoints # It is sometimes desirable to have Flink operate as a source or sink against a Kinesis VPC endpoint or a non-AWS Kinesis endpoint such as Kinesalite; this is especially useful when performing functional testing of a Flink application. The AWS endpoint that would normally be inferred by the AWS region set in the Flink configuration must be overridden via a configuration property.
To override the AWS endpoint, set the AWSConfigConstants.AWS_ENDPOINT and AWSConfigConstants.AWS_REGION properties. The region will be used to sign the endpoint URL.
Java Properties config = new Properties(); config.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); config.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); config.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); config.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4567\u0026#34;); Scala val config = new Properties() config.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) config.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) config.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) config.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4567\u0026#34;) Python config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4567\u0026#39; } Back to top
`}),e.add({id:158,href:"/flink/flink-docs-master/docs/connectors/table/kinesis/",title:"Kinesis",section:"Table API Connectors",content:" Amazon Kinesis Data Streams SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode\nThe Kinesis connector allows for reading data from and writing data into Amazon Kinesis Data Streams (KDS).\nDependencies # In order to use the Kinesis connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nMaven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The Kinesis connector is not part of the binary distribution. See how to link with it for cluster execution here.\nHow to create a Kinesis data stream table # Follow the instructions from the Amazon KDS Developer Guide to set up a Kinesis stream. The following example shows how to create a table backed by a Kinesis data stream:\nCREATE TABLE KinesisTable ( `user_id` BIGINT, `item_id` BIGINT, `category_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) ) PARTITIONED BY (user_id, item_id) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kinesis\u0026#39;, \u0026#39;stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;scan.stream.initpos\u0026#39; = \u0026#39;LATEST\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Available Metadata # The following metadata can be exposed as read-only (VIRTUAL) columns in a table definition.\nKey Data Type Description timestamp TIMESTAMP_LTZ(3) NOT NULL The approximate time when the record was inserted into the stream. shard-id VARCHAR(128) NOT NULL The unique identifier of the shard within the stream from which the record was read. sequence-number VARCHAR(128) NOT NULL The unique identifier of the record within its shard. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:\nCREATE TABLE KinesisTable ( `user_id` BIGINT, `item_id` BIGINT, `category_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3), `arrival_time` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; VIRTUAL, `shard_id` VARCHAR(128) NOT NULL METADATA FROM \u0026#39;shard-id\u0026#39; VIRTUAL, `sequence_number` VARCHAR(128) NOT NULL METADATA FROM \u0026#39;sequence-number\u0026#39; VIRTUAL ) PARTITIONED BY (user_id, item_id) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kinesis\u0026#39;, \u0026#39;stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;scan.stream.initpos\u0026#39; = \u0026#39;LATEST\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Connector Options # Option Required Forwarded Default Type Description Common Options connector required no (none) String Specify what connector to use. For Kinesis use 'kinesis'. stream required yes (none) String Name of the Kinesis data stream backing this table. format required no (none) String The format used to deserialize and serialize Kinesis data stream records. See Data Type Mapping for details. aws.region optional no (none) String The AWS region where the stream is defined. Either this or aws.endpoint are required. aws.endpoint optional no (none) String The AWS endpoint for Kinesis (derived from the AWS region setting if not set). Either this or aws.region are required. aws.trust.all.certificates optional false Boolean If true accepts all SSL certificates. Authentication Options aws.credentials.provider optional no AUTO String A credentials provider to use when authenticating against the Kinesis endpoint. See Authentication for details. aws.credentials.basic.accesskeyid optional no (none) String The AWS access key ID to use when setting credentials provider type to BASIC. aws.credentials.basic.secretkey optional no (none) String The AWS secret key to use when setting credentials provider type to BASIC. aws.credentials.profile.path optional no (none) String Optional configuration for profile path if credential provider type is set to be PROFILE. aws.credentials.profile.name optional no (none) String Optional configuration for profile name if credential provider type is set to be PROFILE. aws.credentials.role.arn optional no (none) String The role ARN to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.sessionName optional no (none) String The role session name to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.externalId optional no (none) String The external ID to use when credential provider type is set to ASSUME_ROLE. aws.credentials.role.provider optional no (none) String The credentials provider that provides credentials for assuming the role when credential provider type is set to ASSUME_ROLE. Roles can be nested, so this value can again be set to ASSUME_ROLE aws.credentials.webIdentityToken.file optional no (none) String The absolute path to the web identity token file that should be used if provider type is set to WEB_IDENTITY_TOKEN. Source Options scan.stream.initpos optional no LATEST String Initial position to be used when reading from the table. See Start Reading Position for details. scan.stream.initpos-timestamp optional no (none) String The initial timestamp to start reading Kinesis stream from (when scan.stream.initpos is AT_TIMESTAMP). See Start Reading Position for details. scan.stream.initpos-timestamp-format optional no yyyy-MM-dd'T'HH:mm:ss.SSSXXX String The date format of initial timestamp to start reading Kinesis stream from (when scan.stream.initpos is AT_TIMESTAMP). See Start Reading Position for details. scan.stream.recordpublisher optional no POLLING String The RecordPublisher type to use for sources. See Enhanced Fan-Out for details. scan.stream.efo.consumername optional no (none) String The name of the EFO consumer to register with KDS. See Enhanced Fan-Out for details. scan.stream.efo.registration optional no LAZY String Determine how and when consumer de-/registration is performed (LAZY|EAGER|NONE). See Enhanced Fan-Out for details. scan.stream.efo.consumerarn optional no (none) String The prefix of consumer ARN for a given stream. See Enhanced Fan-Out for details. scan.stream.efo.http-client.max-concurrency optional no 10000 Integer Maximum number of allowed concurrent requests for the EFO client. See Enhanced Fan-Out for details. scan.stream.describe.maxretries optional no 50 Integer The maximum number of describeStream attempts if we get a recoverable exception. scan.stream.describe.backoff.base optional no 2000 Long The base backoff time (in milliseconds) between each describeStream attempt (for consuming from DynamoDB streams). scan.stream.describe.backoff.max optional no 5000 Long The maximum backoff time (in milliseconds) between each describeStream attempt (for consuming from DynamoDB streams). scan.stream.describe.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each describeStream attempt (for consuming from DynamoDB streams). scan.list.shards.maxretries optional no 10 Integer The maximum number of listShards attempts if we get a recoverable exception. scan.list.shards.backoff.base optional no 1000 Long The base backoff time (in milliseconds) between each listShards attempt. scan.list.shards.backoff.max optional no 5000 Long The maximum backoff time (in milliseconds) between each listShards attempt. scan.list.shards.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each listShards attempt. scan.stream.describestreamconsumer.maxretries optional no 50 Integer The maximum number of describeStreamConsumer attempts if we get a recoverable exception. scan.stream.describestreamconsumer.backoff.base optional no 2000 Long The base backoff time (in milliseconds) between each describeStreamConsumer attempt. scan.stream.describestreamconsumer.backoff.max optional no 5000 Long The maximum backoff time (in milliseconds) between each describeStreamConsumer attempt. scan.stream.describestreamconsumer.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each describeStreamConsumer attempt. scan.stream.registerstreamconsumer.maxretries optional no 10 Integer The maximum number of registerStream attempts if we get a recoverable exception. scan.stream.registerstreamconsumer.timeout optional no 60 Integer The maximum time in seconds to wait for a stream consumer to become active before giving up. scan.stream.registerstreamconsumer.backoff.base optional no 500 Long The base backoff time (in milliseconds) between each registerStream attempt. scan.stream.registerstreamconsumer.backoff.max optional no 2000 Long The maximum backoff time (in milliseconds) between each registerStream attempt. scan.stream.registerstreamconsumer.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each registerStream attempt. scan.stream.deregisterstreamconsumer.maxretries optional no 10 Integer The maximum number of deregisterStream attempts if we get a recoverable exception. scan.stream.deregisterstreamconsumer.timeout optional no 60 Integer The maximum time in seconds to wait for a stream consumer to deregister before giving up. scan.stream.deregisterstreamconsumer.backoff.base optional no 500 Long The base backoff time (in milliseconds) between each deregisterStream attempt. scan.stream.deregisterstreamconsumer.backoff.max optional no 2000 Long The maximum backoff time (in milliseconds) between each deregisterStream attempt. scan.stream.deregisterstreamconsumer.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each deregisterStream attempt. scan.shard.subscribetoshard.maxretries optional no 10 Integer The maximum number of subscribeToShard attempts if we get a recoverable exception. scan.shard.subscribetoshard.backoff.base optional no 1000 Long The base backoff time (in milliseconds) between each subscribeToShard attempt. scan.shard.subscribetoshard.backoff.max optional no 2000 Long The maximum backoff time (in milliseconds) between each subscribeToShard attempt. scan.shard.subscribetoshard.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each subscribeToShard attempt. scan.shard.getrecords.maxrecordcount optional no 10000 Integer The maximum number of records to try to get each time we fetch records from a AWS Kinesis shard. scan.shard.getrecords.maxretries optional no 3 Integer The maximum number of getRecords attempts if we get a recoverable exception. scan.shard.getrecords.backoff.base optional no 300 Long The base backoff time (in milliseconds) between getRecords attempts if we get a ProvisionedThroughputExceededException. scan.shard.getrecords.backoff.max optional no 1000 Long The maximum backoff time (in milliseconds) between getRecords attempts if we get a ProvisionedThroughputExceededException. scan.shard.getrecords.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each getRecords attempt. scan.shard.getrecords.intervalmillis optional no 200 Long The interval (in milliseconds) between each getRecords request to a AWS Kinesis shard in milliseconds. scan.shard.getiterator.maxretries optional no 3 Integer The maximum number of getShardIterator attempts if we get ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.base optional no 300 Long The base backoff time (in milliseconds) between getShardIterator attempts if we get a ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.max optional no 1000 Long The maximum backoff time (in milliseconds) between getShardIterator attempts if we get a ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.expconst optional no 1.5 Double The power constant for exponential backoff between each getShardIterator attempt. scan.shard.discovery.intervalmillis optional no 10000 Integer The interval between each attempt to discover new shards. scan.shard.adaptivereads optional no false Boolean The config to turn on adaptive reads from a shard. See the AdaptivePollingRecordPublisher documentation for details. scan.shard.idle.interval optional no -1 Long The interval (in milliseconds) after which to consider a shard idle for purposes of watermark generation. A positive value will allow the watermark to progress even when some shards don't receive new records. scan.watermark.sync.interval optional no 30000 Long The interval (in milliseconds) for periodically synchronizing the shared watermark state. scan.watermark.lookahead.millis optional no 0 Long The maximum delta (in milliseconds) allowed for the reader to advance ahead of the shared global watermark. scan.watermark.sync.queue.capacity optional no 100 Integer The maximum number of records that will be buffered before suspending consumption of a shard. Sink Options sink.partitioner optional yes random or row-based String Optional output partitioning from Flink's partitions into Kinesis shards. See Sink Partitioning for details. sink.partitioner-field-delimiter optional yes | String Optional field delimiter for a fields-based partitioner derived from a PARTITION BY clause. See Sink Partitioning for details. sink.producer.* optional no (none) Deprecated options previously used by the legacy connector. Options with equivalant alternatives in KinesisStreamsSink are matched to their respective properties. Unsupported options are logged out to user as warnings. sink.http-client.max-concurrency optional 10000 Integer Maximum number of allowed concurrent requests by KinesisAsyncClient. sink.http-client.read-timeout optional 360000 Integer Maximum amount of time in ms for requests to be sent by KinesisAsyncClient. sink.http-client.protocol.version optional HTTP2 String Http version used by Kinesis Client. sink.batch.max-size optional 500 Integer Maximum batch size of elements to be passed to KinesisAsyncClient to be written downstream. sink.requests.max-inflight optional 16 Integer Request threshold for uncompleted requests by KinesisAsyncClientbefore blocking new write requests and applying backpressure. sink.requests.max-buffered optional 10000 String Request buffer threshold for buffered requests by KinesisAsyncClient before blocking new write requests and applying backpressure. sink.flush-buffer.size optional 5242880 Long Threshold value in bytes for writer buffer in KinesisAsyncClient before flushing. sink.flush-buffer.timeout optional 5000 Long Threshold time in milliseconds for an element to be in a buffer ofKinesisAsyncClient before flushing. Features # Authorization # Make sure to create an appropriate IAM policy to allow reading from / writing to the Kinesis data streams.\nAuthentication # Depending on your deployment you would choose a different Credentials Provider to allow access to Kinesis. By default, the AUTO Credentials Provider is used. If the access key ID and secret key are set in the deployment configuration, this results in using the BASIC provider.\nA specific AWSCredentialsProvider can be optionally set using the aws.credentials.provider setting. Supported values are:\nAUTO - Use the default AWS Credentials Provider chain that searches for credentials in the following order: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE, and EC2/ECS credentials provider. BASIC - Use access key ID and secret key supplied as configuration. ENV_VAR - Use AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY environment variables. SYS_PROP - Use Java system properties aws.accessKeyId and aws.secretKey. PROFILE - Use an AWS credentials profile to create the AWS credentials. ASSUME_ROLE - Create AWS credentials by assuming a role. The credentials for assuming the role must be supplied. WEB_IDENTITY_TOKEN - Create AWS credentials by assuming a role using Web Identity Token. Start Reading Position # You can configure table sources to start reading a table-backing Kinesis data stream from a specific position through the scan.stream.initpos option. Available values are:\nLATEST: read shards starting from the latest record. TRIM_HORIZON: read shards starting from the earliest record possible (data may be trimmed by Kinesis depending on the current retention settings of the backing stream). AT_TIMESTAMP: read shards starting from a specified timestamp. The timestamp value should be specified through the scan.stream.initpos-timestamp in one of the following formats: A non-negative double value representing the number of seconds that has elapsed since the Unix epoch (for example, 1459799926.480). A value conforming to a user-defined SimpleDateFormat specified at scan.stream.initpos-timestamp-format. If a user does not define a format, the default pattern will be yyyy-MM-dd'T'HH:mm:ss.SSSXXX. For example, timestamp value is 2016-04-04 and user-defined format is yyyy-MM-dd, or timestamp value is 2016-04-04T19:58:46.480-00:00 and a user-defined format is not provided. Sink Partitioning # Kinesis data streams consist of one or more shards, and the sink.partitioner option allows you to control how records written into a multi-shard Kinesis-backed table will be partitioned between its shards. Valid values are:\nfixed: Kinesis PartitionKey values derived from the Flink subtask index, so each Flink partition ends up in at most one Kinesis partition (assuming that no re-sharding takes place at runtime). random: Kinesis PartitionKey values are assigned randomly. This is the default value for tables not defined with a PARTITION BY clause. Custom FixedKinesisPartitioner subclass: e.g. 'org.mycompany.MyPartitioner'. Records written into tables defining a PARTITION BY clause will always be partitioned based on a concatenated projection of the PARTITION BY fields. In this case, the sink.partitioner field cannot be used to modify this behavior (attempting to do this results in a configuration error). You can, however, use the sink.partitioner-field-delimiter option to set the delimiter of field values in the concatenated PartitionKey string (an empty string is also a valid delimiter). Enhanced Fan-Out # Enhanced Fan-Out (EFO) increases the maximum number of concurrent consumers per Kinesis data stream. Without EFO, all concurrent Kinesis consumers share a single read quota per shard. Using EFO, each consumer gets a distinct dedicated read quota per shard, allowing read throughput to scale with the number of consumers.\nNote Using EFO will incur additional cost.\nYou can enable and configure EFO with the following properties:\nscan.stream.recordpublisher: Determines whether to use EFO or POLLING. scan.stream.efo.consumername: A name to identify the consumer when the above value is EFO. scan.stream.efo.registration: Strategy for (de-)registration of EFO consumers with the name given by the scan.stream.efo.consumername value. Valid strategies are: LAZY (default): Stream consumers are registered when the Flink job starts running. If the stream consumer already exists, it will be reused. This is the preferred strategy for the majority of applications. However, jobs with parallelism greater than 1 will result in tasks competing to register and acquire the stream consumer ARN. For jobs with very large parallelism this can result in an increased start-up time. The describe operation has a limit of 20 transactions per second, this means application startup time will increase by roughly parallelism/20 seconds. EAGER: Stream consumers are registered in the FlinkKinesisConsumer constructor. If the stream consumer already exists, it will be reused. This will result in registration occurring when the job is constructed, either on the Flink Job Manager or client environment submitting the job. Using this strategy results in a single thread registering and retrieving the stream consumer ARN, reducing startup time over LAZY (with large parallelism). However, consider that the client environment will require access to the AWS services. NONE: Stream consumer registration is not performed by FlinkKinesisConsumer. Registration must be performed externally using the AWS CLI or SDK to invoke RegisterStreamConsumer. Stream consumer ARNs should be provided to the job via the consumer configuration. scan.stream.efo.consumerarn.\u0026lt;stream-name\u0026gt;: ARNs identifying externally registered ARN-consumers (substitute \u0026lt;stream-name\u0026gt; with the name of your stream in the parameter name). Use this if you choose to use NONE as a scan.stream.efo.registration strategy. Note For a given Kinesis data stream, each EFO consumer must have a unique name. However, consumer names do not have to be unique across data streams. Reusing a consumer name will result in existing subscriptions being terminated.\nNote With the LAZY strategy, stream consumers are de-registered when the job is shutdown gracefully. In the event that a job terminates within executing the shutdown hooks, stream consumers will remain active. In this situation the stream consumers will be gracefully reused when the application restarts. With the NONE and EAGER strategies, stream consumer de-registration is not performed by FlinkKinesisConsumer.\nData Type Mapping # Kinesis stores records as Base64-encoded binary data objects, so it doesn\u0026rsquo;t have a notion of internal record structure. Instead, Kinesis records are deserialized and serialized by formats, e.g. \u0026lsquo;avro\u0026rsquo;, \u0026lsquo;csv\u0026rsquo;, or \u0026lsquo;json\u0026rsquo;. To determine the data type of the messages in your Kinesis-backed tables, pick a suitable Flink format with the format keyword. Please refer to the Formats pages for more details.\nUpdates in 1.15 # Kinesis table API connector sink data stream depends on FlinkKinesisProducer till 1.14, with the introduction of KinesisStreamsSink in 1.15 kinesis table API sink connector has been migrated to the new KinesisStreamsSink. Authentication options have been migrated identically while sink configuration options are now compatible with KinesisStreamsSink.\nOptions configuring FlinkKinesisProducer are now deprecated with fallback support for common configuration options with KinesisStreamsSink.\nKinesisStreamsSink uses KinesisAsyncClient to send records to kinesis, which doesn\u0026rsquo;t support aggregation. In consequence, table options configuring aggregation in the deprecated FlinkKinesisProducer are now deprecated and will be ignored, this includes sink.producer.aggregation-enabled and sink.producer.aggregation-count.\nNote Migrating applications with deprecated options will result in the incompatible deprecated options being ignored and warned to users.\nKinesis table API source connector still depends on FlinkKinesisConsumer with no change in configuration options.\nBack to top\n"}),e.add({id:159,href:"/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/",title:"Kubernetes",section:"Standalone",content:` Kubernetes Setup # Getting Started # This Getting Started guide describes how to deploy a Session cluster on Kubernetes.
Introduction # This page describes deploying a standalone Flink cluster on top of Kubernetes, using Flink\u0026rsquo;s standalone deployment. We generally recommend new users to deploy Flink on Kubernetes using native Kubernetes deployments.
Preparation # This guide expects a Kubernetes environment to be present. You can ensure that your Kubernetes setup is working by running a command like kubectl get nodes, which lists all connected Kubelets.
If you want to run Kubernetes locally, we recommend using MiniKube.
If using MiniKube please make sure to execute minikube ssh 'sudo ip link set docker0 promisc on' before deploying a Flink cluster. Otherwise Flink components are not able to reference themselves through a Kubernetes service. Starting a Kubernetes Cluster (Session Mode) # A Flink Session cluster is executed as a long-running Kubernetes Deployment. You can run multiple Flink jobs on a Session cluster. Each job needs to be submitted to the cluster after the cluster has been deployed.
A Flink Session cluster deployment in Kubernetes has at least three components:
a Deployment which runs a JobManager a Deployment for a pool of TaskManagers a Service exposing the JobManager\u0026rsquo;s REST and UI ports Using the file contents provided in the the common resource definitions, create the following files, and create the respective components with the kubectl command:
# Configuration and service definition \$ kubectl create -f flink-configuration-configmap.yaml \$ kubectl create -f jobmanager-service.yaml # Create the deployments for the cluster \$ kubectl create -f jobmanager-session-deployment.yaml \$ kubectl create -f taskmanager-session-deployment.yaml Next, we set up a port forward to access the Flink UI and submit jobs:
Run kubectl port-forward \${flink-jobmanager-pod} 8081:8081 to forward your jobmanager\u0026rsquo;s web ui port to local 8081. Navigate to http://localhost:8081 in your browser. Moreover, you could use the following command below to submit jobs to the cluster: \$ ./bin/flink run -m localhost:8081 ./examples/streaming/TopSpeedWindowing.jar You can tear down the cluster using the following commands:
\$ kubectl delete -f jobmanager-service.yaml \$ kubectl delete -f flink-configuration-configmap.yaml \$ kubectl delete -f taskmanager-session-deployment.yaml \` kubectl delete -f jobmanager-session-deployment.yaml Back to top
Deployment Modes # Application Mode # For high-level intuition behind the application mode, please refer to the deployment mode overview. A Flink Application cluster is a dedicated cluster which runs a single application, which needs to be available at deployment time.
A basic Flink Application cluster deployment in Kubernetes has three components:
an Application which runs a JobManager a Deployment for a pool of TaskManagers a Service exposing the JobManager\u0026rsquo;s REST and UI ports Check the Application cluster specific resource definitions and adjust them accordingly:
The args attribute in the jobmanager-job.yaml has to specify the main class of the user job. See also how to specify the JobManager arguments to understand how to pass other args to the Flink image in the jobmanager-job.yaml.
The job artifacts should be available from the job-artifacts-volume in the resource definition examples. The definition examples mount the volume as a local directory of the host assuming that you create the components in a minikube cluster. If you do not use a minikube cluster, you can use any other type of volume, available in your Kubernetes cluster, to supply the job artifacts. Alternatively, you can build a custom image which already contains the artifacts instead.
After creating the common cluster components, use the Application cluster specific resource definitions to launch the cluster with the kubectl command:
\$ kubectl create -f jobmanager-job.yaml \$ kubectl create -f taskmanager-job-deployment.yaml To terminate the single application cluster, these components can be deleted along with the common ones with the kubectl command:
\$ kubectl delete -f taskmanager-job-deployment.yaml \$ kubectl delete -f jobmanager-job.yaml Session Mode # For high-level intuition behind the session mode, please refer to the deployment mode overview. Deployment of a Session cluster is explained in the Getting Started guide at the top of this page.
Back to top
Flink on Standalone Kubernetes Reference # Configuration # All configuration options are listed on the configuration page. Configuration options can be added to the flink-conf.yaml section of the flink-configuration-configmap.yaml config map.
Accessing Flink in Kubernetes # You can then access the Flink UI and submit jobs via different ways:
kubectl proxy:
Run kubectl proxy in a terminal. Navigate to http://localhost:8001/api/v1/namespaces/default/services/flink-jobmanager:webui/proxy in your browser. kubectl port-forward:
Run kubectl port-forward \${flink-jobmanager-pod} 8081:8081 to forward your jobmanager\u0026rsquo;s web ui port to local 8081. Navigate to http://localhost:8081 in your browser. Moreover, you can use the following command below to submit jobs to the cluster: \$ ./bin/flink run -m localhost:8081 ./examples/streaming/TopSpeedWindowing.jar Create a NodePort service on the rest service of jobmanager:
Run kubectl create -f jobmanager-rest-service.yaml to create the NodePort service on jobmanager. The example of jobmanager-rest-service.yaml can be found in appendix. Run kubectl get svc flink-jobmanager-rest to know the node-port of this service and navigate to http://\u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt; in your browser. If you use minikube, you can get its public ip by running minikube ip. Similarly to the port-forward solution, you can also use the following command below to submit jobs to the cluster: \$ ./bin/flink run -m \u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt; ./examples/streaming/TopSpeedWindowing.jar Debugging and Log Access # Many common errors are easy to detect by checking Flink\u0026rsquo;s log files. If you have access to Flink\u0026rsquo;s web user interface, you can access the JobManager and TaskManager logs from there.
If there are problems starting Flink, you can also use Kubernetes utilities to access the logs. Use kubectl get pods to see all running pods. For the quickstart example from above, you should see three pods:
\$ kubectl get pods NAME READY STATUS RESTARTS AGE flink-jobmanager-589967dcfc-m49xv 1/1 Running 3 3m32s flink-taskmanager-64847444ff-7rdl4 1/1 Running 3 3m28s flink-taskmanager-64847444ff-nnd6m 1/1 Running 3 3m28s You can now access the logs by running kubectl logs flink-jobmanager-589967dcfc-m49xv
High-Availability with Standalone Kubernetes # For high availability on Kubernetes, you can use the existing high availability services.
Kubernetes High-Availability Services # Session Mode and Application Mode clusters support using the Kubernetes high availability service. You need to add the following Flink config options to flink-configuration-configmap.yaml.
Note The filesystem which corresponds to the scheme of your configured HA storage directory must be available to the runtime. Refer to custom Flink image and enable plugins for more information.
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ ... kubernetes.cluster-id: \u0026lt;cluster-id\u0026gt; high-availability: kubernetes high-availability.storageDir: hdfs:///flink/recovery restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 10 ... Moreover, you have to start the JobManager and TaskManager pods with a service account which has the permissions to create, edit, delete ConfigMaps. See how to configure service accounts for pods for more information.
When High-Availability is enabled, Flink will use its own HA-services for service discovery. Therefore, JobManager pods should be started with their IP address instead of a Kubernetes service as its jobmanager.rpc.address. Refer to the appendix for full configuration.
Standby JobManagers # Usually, it is enough to only start a single JobManager pod, because Kubernetes will restart it once the pod crashes. If you want to achieve faster recovery, configure the replicas in jobmanager-session-deployment-ha.yaml or parallelism in jobmanager-application-ha.yaml to a value greater than 1 to start standby JobManagers.
Enabling Queryable State # You can access the queryable state of TaskManager if you create a NodePort service for it:
Run kubectl create -f taskmanager-query-state-service.yaml to create the NodePort service for the taskmanager pod. The example of taskmanager-query-state-service.yaml can be found in appendix. Run kubectl get svc flink-taskmanager-query-state to get the \u0026lt;node-port\u0026gt; of this service. Then you can create the QueryableStateClient(\u0026lt;public-node-ip\u0026gt;, \u0026lt;node-port\u0026gt; to submit state queries. Using Standalone Kubernetes with Reactive Mode # Reactive Mode allows to run Flink in a mode, where the Application Cluster is always adjusting the job parallelism to the available resources. In combination with Kubernetes, the replica count of the TaskManager deployment determines the available resources. Increasing the replica count will scale up the job, reducing it will trigger a scale down. This can also be done automatically by using a Horizontal Pod Autoscaler.
To use Reactive Mode on Kubernetes, follow the same steps as for deploying a job using an Application Cluster. But instead of flink-configuration-configmap.yaml use this config map: flink-reactive-mode-configuration-configmap.yaml. It contains the scheduler-mode: reactive setting for Flink.
Once you have deployed the Application Cluster, you can scale your job up or down by changing the replica count in the flink-taskmanager deployment.
Enabling Local Recovery Across Pod Restarts # In order to speed up recoveries in case of pod failures, you can leverage Flink\u0026rsquo;s working directory feature together with local recovery. If the working directory is configured to reside on a persistent volume that gets remounted to a restarted TaskManager pod, then Flink is able to recover state locally. With the StatefulSet, Kubernetes gives you the exact tool you need to map a pod to a persistent volume.
Deploying TaskManagers as a StatefulSet, allows you to configure a volume claim template that is used to mount persistent volumes to the TaskManagers. Additionally, you need to configure a deterministic taskmanager.resource-id. A suitable value is the pod name, that you expose using environment variables. For an example StatefulSet configuration take a look at the appendix.
Back to top
Appendix # Common cluster resource definitions # flink-configuration-configmap.yaml
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 queryable-state.proxy.ports: 6125 jobmanager.memory.process.size: 1600m taskmanager.memory.process.size: 1728m parallelism.default: 2 log4j-console.properties: |+ # This affects logging for both user code and Flink rootLogger.level = INFO rootLogger.appenderRef.console.ref = ConsoleAppender rootLogger.appenderRef.rolling.ref = RollingFileAppender # Uncomment this if you want to _only_ change Flink\u0026#39;s logging #logger.flink.name = org.apache.flink #logger.flink.level = INFO # The following lines keep the log level of common libraries/connectors on # log level INFO. The root logger does not override this. You have to manually # change the log levels here. logger.akka.name = akka logger.akka.level = INFO logger.kafka.name= org.apache.kafka logger.kafka.level = INFO logger.hadoop.name = org.apache.hadoop logger.hadoop.level = INFO logger.zookeeper.name = org.apache.zookeeper logger.zookeeper.level = INFO # Log all infos to the console appender.console.name = ConsoleAppender appender.console.type = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # Log all infos in the given rolling file appender.rolling.name = RollingFileAppender appender.rolling.type = RollingFile appender.rolling.append = false appender.rolling.fileName = \${sys:log.file} appender.rolling.filePattern = \${sys:log.file}.%i appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size=100MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 10 # Suppress the irrelevant (wrong) warnings from the Netty channel handler logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline logger.netty.level = OFF flink-reactive-mode-configuration-configmap.yaml
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 queryable-state.proxy.ports: 6125 jobmanager.memory.process.size: 1600m taskmanager.memory.process.size: 1728m parallelism.default: 2 scheduler-mode: reactive execution.checkpointing.interval: 10s log4j-console.properties: |+ # This affects logging for both user code and Flink rootLogger.level = INFO rootLogger.appenderRef.console.ref = ConsoleAppender rootLogger.appenderRef.rolling.ref = RollingFileAppender # Uncomment this if you want to _only_ change Flink\u0026#39;s logging #logger.flink.name = org.apache.flink #logger.flink.level = INFO # The following lines keep the log level of common libraries/connectors on # log level INFO. The root logger does not override this. You have to manually # change the log levels here. logger.akka.name = akka logger.akka.level = INFO logger.kafka.name= org.apache.kafka logger.kafka.level = INFO logger.hadoop.name = org.apache.hadoop logger.hadoop.level = INFO logger.zookeeper.name = org.apache.zookeeper logger.zookeeper.level = INFO # Log all infos to the console appender.console.name = ConsoleAppender appender.console.type = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # Log all infos in the given rolling file appender.rolling.name = RollingFileAppender appender.rolling.type = RollingFile appender.rolling.append = false appender.rolling.fileName = \${sys:log.file} appender.rolling.filePattern = \${sys:log.file}.%i appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size=100MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 10 # Suppress the irrelevant (wrong) warnings from the Netty channel handler logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline logger.netty.level = OFF jobmanager-service.yaml Optional service, which is only necessary for non-HA mode.
apiVersion: v1 kind: Service metadata: name: flink-jobmanager spec: type: ClusterIP ports: - name: rpc port: 6123 - name: blob-server port: 6124 - name: webui port: 8081 selector: app: flink component: jobmanager jobmanager-rest-service.yaml. Optional service, that exposes the jobmanager rest port as public Kubernetes node\u0026rsquo;s port.
apiVersion: v1 kind: Service metadata: name: flink-jobmanager-rest spec: type: NodePort ports: - name: rest port: 8081 targetPort: 8081 nodePort: 30081 selector: app: flink component: jobmanager taskmanager-query-state-service.yaml. Optional service, that exposes the TaskManager port to access the queryable state as a public Kubernetes node\u0026rsquo;s port.
apiVersion: v1 kind: Service metadata: name: flink-taskmanager-query-state spec: type: NodePort ports: - name: query-state port: 6125 targetPort: 6125 nodePort: 30025 selector: app: flink component: taskmanager Session cluster resource definitions # jobmanager-session-deployment-non-ha.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: apache/flink:latest args: [\u0026#34;jobmanager\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties jobmanager-session-deployment-ha.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 # Set the value to greater than 1 to start standby JobManagers selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: apache/flink:latest env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP # The following args overwrite the value of jobmanager.rpc.address configured in the configuration config map to POD_IP. args: [\u0026#34;jobmanager\u0026#34;, \u0026#34;\$(POD_IP)\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary serviceAccountName: flink-service-account # Service account which has the permissions to create, edit, delete ConfigMaps volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties taskmanager-session-deployment.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: apache/flink:latest args: [\u0026#34;taskmanager\u0026#34;] ports: - containerPort: 6122 name: rpc - containerPort: 6125 name: query-state livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties Application cluster resource definitions # jobmanager-application-non-ha.yaml
apiVersion: batch/v1 kind: Job metadata: name: flink-jobmanager spec: template: metadata: labels: app: flink component: jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: apache/flink:latest env: args: [\u0026#34;standalone-job\u0026#34;, \u0026#34;--job-classname\u0026#34;, \u0026#34;com.job.ClassName\u0026#34;, \u0026lt;optional arguments\u0026gt;, \u0026lt;job arguments\u0026gt;] # optional arguments: [\u0026#34;--job-id\u0026#34;, \u0026#34;\u0026lt;job id\u0026gt;\u0026#34;, \u0026#34;--fromSavepoint\u0026#34;, \u0026#34;/path/to/savepoint\u0026#34;, \u0026#34;--allowNonRestoredState\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts jobmanager-application-ha.yaml
apiVersion: batch/v1 kind: Job metadata: name: flink-jobmanager spec: parallelism: 1 # Set the value to greater than 1 to start standby JobManagers template: metadata: labels: app: flink component: jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: apache/flink:latest env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP # The following args overwrite the value of jobmanager.rpc.address configured in the configuration config map to POD_IP. args: [\u0026#34;standalone-job\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;\$(POD_IP)\u0026#34;, \u0026#34;--job-classname\u0026#34;, \u0026#34;com.job.ClassName\u0026#34;, \u0026lt;optional arguments\u0026gt;, \u0026lt;job arguments\u0026gt;] # optional arguments: [\u0026#34;--job-id\u0026#34;, \u0026#34;\u0026lt;job id\u0026gt;\u0026#34;, \u0026#34;--fromSavepoint\u0026#34;, \u0026#34;/path/to/savepoint\u0026#34;, \u0026#34;--allowNonRestoredState\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary serviceAccountName: flink-service-account # Service account which has the permissions to create, edit, delete ConfigMaps volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts taskmanager-job-deployment.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: apache/flink:latest env: args: [\u0026#34;taskmanager\u0026#34;] ports: - containerPort: 6122 name: rpc - containerPort: 6125 name: query-state livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts Local Recovery Enabled TaskManager StatefulSet # apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 state.backend.local-recovery: true process.taskmanager.working-dir: /pv --- apiVersion: v1 kind: Service metadata: name: taskmanager-hl spec: clusterIP: None selector: app: flink component: taskmanager --- apiVersion: apps/v1 kind: StatefulSet metadata: name: flink-taskmanager spec: serviceName: taskmanager-hl replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: securityContext: runAsUser: 9999 fsGroup: 9999 containers: - name: taskmanager image: apache/flink:latest env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: [\u0026#34;taskmanager\u0026#34;, \u0026#34;-Dtaskmanager.resource-id=\$(POD_NAME)\u0026#34;] ports: - containerPort: 6122 name: rpc - containerPort: 6125 name: query-state - containerPort: 6121 name: metrics livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ - name: pv mountPath: /pv volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties volumeClaimTemplates: - metadata: name: pv spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 50Gi Back to top
`}),e.add({id:160,href:"/flink/flink-docs-master/docs/libs/",title:"Libraries",section:"Docs",content:" "}),e.add({id:161,href:"/flink/flink-docs-master/docs/deployment/memory/mem_tuning/",title:"Memory Tuning Guide",section:"Memory Configuration",content:` Memory tuning guide # In addition to the main memory setup guide, this section explains how to set up memory depending on the use case and which options are important for each case.
Configure memory for standalone deployment # It is recommended to configure total Flink memory (taskmanager.memory.flink.size or jobmanager.memory.flink.size) or its components for standalone deployment where you want to declare how much memory is given to Flink itself. Additionally, you can adjust JVM metaspace if it causes problems.
The total Process memory is not relevant because JVM overhead is not controlled by Flink or the deployment environment, only physical resources of the executing machine matter in this case.
Configure memory for containers # It is recommended to configure total process memory (taskmanager.memory.process.size or jobmanager.memory.process.size) for the containerized deployments (Kubernetes or Yarn). It declares how much memory in total should be assigned to the Flink JVM process and corresponds to the size of the requested container.
Note If you configure the total Flink memory Flink will implicitly add JVM memory components to derive the total process memory and request a container with the memory of that derived size.
Warning: If Flink or user code allocates unmanaged off-heap (native) memory beyond the container size the job can fail because the deployment environment can kill the offending containers. See also description of container memory exceeded failure.
Configure memory for state backends # This is only relevant for TaskManagers.
When deploying a Flink streaming application, the type of state backend used will dictate the optimal memory configurations of your cluster.
HashMap state backend # When running a stateless job or using the HashMapStateBackend), set managed memory to zero. This will ensure that the maximum amount of heap memory is allocated for user code on the JVM.
RocksDB state backend # The EmbeddedRocksDBStateBackend uses native memory. By default, RocksDB is set up to limit native memory allocation to the size of the managed memory. Therefore, it is important to reserve enough managed memory for your state. If you disable the default RocksDB memory control, TaskManagers can be killed in containerized deployments if RocksDB allocates memory above the limit of the requested container size (the total process memory). See also how to tune RocksDB memory and state.backend.rocksdb.memory.managed.
Configure memory for batch jobs # This is only relevant for TaskManagers.
Flink\u0026rsquo;s batch operators leverage managed memory to run more efficiently. In doing so, some operations can be performed directly on raw data without having to be deserialized into Java objects. This means that managed memory configurations have practical effects on the performance of your applications. Flink will attempt to allocate and use as much managed memory as configured for batch jobs but not go beyond its limits. This prevents OutOfMemoryError\u0026rsquo;s because Flink knows precisely how much memory it has to leverage. If the managed memory is not sufficient, Flink will gracefully spill to disk.
`}),e.add({id:162,href:"/flink/flink-docs-master/docs/deployment/filesystems/plugins/",title:"Plugins",section:"File Systems",content:` Plugins # Plugins facilitate a strict separation of code through restricted classloaders. Plugins cannot access classes from other plugins or from Flink that have not been specifically whitelisted. This strict isolation allows plugins to contain conflicting versions of the same library without the need to relocate classes or to converge to common versions. Currently, file systems and metric reporters are pluggable but in the future, connectors, formats, and even user code should also be pluggable.
Isolation and plugin structure # Plugins reside in their own folders and can consist of several jars. The names of the plugin folders are arbitrary.
flink-dist ├── conf ├── lib ... └── plugins ├── s3 │ ├── aws-credential-provider.jar │ └── flink-s3-fs-hadoop.jar └── azure └── flink-azure-fs-hadoop.jar Each plugin is loaded through its own classloader and completely isolated from any other plugin. Hence, the flink-s3-fs-hadoop and flink-azure-fs-hadoop can depend on different conflicting library versions. There is no need to relocate any class during the creation of fat jars (shading).
Plugins may access certain whitelisted packages from Flink\u0026rsquo;s lib/ folder. In particular, all necessary service provider interfaces (SPI) are loaded through the system classloader, so that no two versions of org.apache.flink.core.fs.FileSystem exist at any given time, even if users accidentally bundle it in their fat jar. This singleton class requirement is strictly necessary so that the Flink runtime has an entry point into the plugin. Service classes are discovered through the java.util.ServiceLoader, so make sure to retain the service definitions in META-INF/services during shading.
Note Currently, more Flink core classes are still accessible from plugins as we flesh out the SPI system.
Furthermore, the most common logger frameworks are whitelisted, such that logging is uniformly possible across Flink core, plugins, and user code.
File Systems # All file systems are pluggable. That means they can and should be used as plugins. To use a pluggable file system, copy the corresponding JAR file from the opt directory to a directory under plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/s3-fs-hadoop cp ./opt/flink-s3-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/s3-fs-hadoop/ The s3 file systems (flink-s3-fs-presto and flink-s3-fs-hadoop) can only be used as plugins as we already removed the relocations. Placing them in libs/ will result in system failures. Because of the strict isolation, file systems do not have access to credential providers in lib/ anymore. Please add any needed providers to the respective plugin folder. Metric Reporters # All metric reporters that Flink provides can be used as plugins. See the metrics documentation for more details.
Back to top
`}),e.add({id:163,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/queryable_state/",title:"Queryable State",section:"State \u0026 Fault Tolerance",content:` Queryable State # The client APIs for queryable state are currently in an evolving state and there are no guarantees made about stability of the provided interfaces. It is likely that there will be breaking API changes on the client side in the upcoming Flink versions. In a nutshell, this feature exposes Flink\u0026rsquo;s managed keyed (partitioned) state (see Working with State) to the outside world and allows the user to query a job\u0026rsquo;s state from outside Flink. For some scenarios, queryable state eliminates the need for distributed operations/transactions with external systems such as key-value stores which are often the bottleneck in practice. In addition, this feature may be particularly useful for debugging purposes.
When querying a state object, that object is accessed from a concurrent thread without any synchronization or copying. This is a design choice, as any of the above would lead to increased job latency, which we wanted to avoid. Since any state backend using Java heap space, e.g. HashMapStateBackend, does not work with copies when retrieving values but instead directly references the stored values, read-modify-write patterns are unsafe and may cause the queryable state server to fail due to concurrent modifications. The EmbeddedRocksDBStateBackend is safe from these issues. Architecture # Before showing how to use the Queryable State, it is useful to briefly describe the entities that compose it. The Queryable State feature consists of three main entities:
the QueryableStateClient, which (potentially) runs outside the Flink cluster and submits the user queries, the QueryableStateClientProxy, which runs on each TaskManager (i.e. inside the Flink cluster) and is responsible for receiving the client\u0026rsquo;s queries, fetching the requested state from the responsible Task Manager on his behalf, and returning it to the client, and the QueryableStateServer which runs on each TaskManager and is responsible for serving the locally stored state. The client connects to one of the proxies and sends a request for the state associated with a specific key, k. As stated in Working with State, keyed state is organized in Key Groups, and each TaskManager is assigned a number of these key groups. To discover which TaskManager is responsible for the key group holding k, the proxy will ask the JobManager. Based on the answer, the proxy will then query the QueryableStateServer running on that TaskManager for the state associated with k, and forward the response back to the client.
Activating Queryable State # To enable queryable state on your Flink cluster, you need to do the following:
copy the flink-queryable-state-runtime-1.16-SNAPSHOT.jar from the opt/ folder of your Flink distribution, to the lib/ folder. set the property queryable-state.enable to true. See the Configuration documentation for details and additional parameters. To verify that your cluster is running with queryable state enabled, check the logs of any task manager for the line: \u0026quot;Started the Queryable State Proxy Server @ ...\u0026quot;.
Making State Queryable # Now that you have activated queryable state on your cluster, it is time to see how to use it. In order for a state to be visible to the outside world, it needs to be explicitly made queryable by using:
either a QueryableStateStream, a convenience object which acts as a sink and offers its incoming values as queryable state, or the stateDescriptor.setQueryable(String queryableStateName) method, which makes the keyed state represented by the state descriptor, queryable. The following sections explain the use of these two approaches.
Queryable State Stream # Calling .asQueryableState(stateName, stateDescriptor) on a KeyedStream returns a QueryableStateStream which offers its values as queryable state. Depending on the type of state, there are the following variants of the asQueryableState() method:
// ValueState QueryableStateStream asQueryableState( String queryableStateName, ValueStateDescriptor stateDescriptor) // Shortcut for explicit ValueStateDescriptor variant QueryableStateStream asQueryableState(String queryableStateName) // ReducingState QueryableStateStream asQueryableState( String queryableStateName, ReducingStateDescriptor stateDescriptor) Note: There is no queryable ListState sink as it would result in an ever-growing list which may not be cleaned up and thus will eventually consume too much memory. The returned QueryableStateStream can be seen as a sink and cannot be further transformed. Internally, a QueryableStateStream gets translated to an operator which uses all incoming records to update the queryable state instance. The updating logic is implied by the type of the StateDescriptor provided in the asQueryableState call. In a program like the following, all records of the keyed stream will be used to update the state instance via the ValueState.update(value):
stream.keyBy(value -\u0026gt; value.f0).asQueryableState(\u0026#34;query-name\u0026#34;); This acts like the Scala API\u0026rsquo;s flatMapWithState.
Managed Keyed State # Managed keyed state of an operator (see Using Managed Keyed State) can be made queryable by making the appropriate state descriptor queryable via StateDescriptor.setQueryable(String queryableStateName), as in the example below:
ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information descriptor.setQueryable(\u0026#34;query-name\u0026#34;); // queryable state name Note: The queryableStateName parameter may be chosen arbitrarily and is only used for queries. It does not have to be identical to the state\u0026rsquo;s own name. This variant has no limitations as to which type of state can be made queryable. This means that this can be used for any ValueState, ReduceState, ListState, MapState, and AggregatingState.
Querying State # So far, you have set up your cluster to run with queryable state and you have declared (some of) your state as queryable. Now it is time to see how to query this state.
For this you can use the QueryableStateClient helper class. This is available in the flink-queryable-state-client jar which must be explicitly included as a dependency in the pom.xml of your project along with flink-core, as shown below:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-queryable-state-client-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; For more on this, you can check how to set up a Flink program.
The QueryableStateClient will submit your query to the internal proxy, which will then process your query and return the final result. The only requirement to initialize the client is to provide a valid TaskManager hostname (remember that there is a queryable state proxy running on each task manager) and the port where the proxy listens. More on how to configure the proxy and state server port(s) in the Configuration Section.
QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); With the client ready, to query a state of type V, associated with a key of type K, you can use the method:
CompletableFuture\u0026lt;S\u0026gt; getKvState( JobID jobId, String queryableStateName, K key, TypeInformation\u0026lt;K\u0026gt; keyTypeInfo, StateDescriptor\u0026lt;S, V\u0026gt; stateDescriptor) The above returns a CompletableFuture eventually holding the state value for the queryable state instance identified by queryableStateName of the job with ID jobID. The key is the key whose state you are interested in and the keyTypeInfo will tell Flink how to serialize/deserialize it. Finally, the stateDescriptor contains the necessary information about the requested state, namely its type (Value, Reduce, etc) and the necessary information on how to serialize/deserialize it.
The careful reader will notice that the returned future contains a value of type S, i.e. a State object containing the actual value. This can be any of the state types supported by Flink: ValueState, ReduceState, ListState, MapState, and AggregatingState.
Note: These state objects do not allow modifications to the contained state. You can use them to get the actual value of the state, e.g. using valueState.get(), or iterate over the contained \u0026lt;K, V\u0026gt; entries, e.g. using the mapState.entries(), but you cannot modify them. As an example, calling the add() method on a returned list state will throw an UnsupportedOperationException. Note: The client is asynchronous and can be shared by multiple threads. It needs to be shutdown via QueryableStateClient.shutdown() when unused in order to free resources. Example # The following example extends the CountWindowAverage example (see Using Managed Keyed State) by making it queryable and shows how to query this value:
public class CountWindowAverage extends RichFlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { private transient ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; sum; // a tuple containing the count and the sum @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; input, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) throws Exception { Tuple2\u0026lt;Long, Long\u0026gt; currentSum = sum.value(); currentSum.f0 += 1; currentSum.f1 += input.f1; sum.update(currentSum); if (currentSum.f0 \u0026gt;= 2) { out.collect(new Tuple2\u0026lt;\u0026gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information descriptor.setQueryable(\u0026#34;query-name\u0026#34;); sum = getRuntimeContext().getState(descriptor); } } Once used in a job, you can retrieve the job ID and then query any key\u0026rsquo;s current state from this operator:
QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); // the state descriptor of the state to be fetched. ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); CompletableFuture\u0026lt;ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; resultFuture = client.getKvState(jobId, \u0026#34;query-name\u0026#34;, key, BasicTypeInfo.LONG_TYPE_INFO, descriptor); // now handle the returned value resultFuture.thenAccept(response -\u0026gt; { try { Tuple2\u0026lt;Long, Long\u0026gt; res = response.get(); } catch (Exception e) { e.printStackTrace(); } }); Configuration # The following configuration parameters influence the behaviour of the queryable state server and client. They are defined in QueryableStateOptions.
State Server # queryable-state.server.ports: the server port range of the queryable state server. This is useful to avoid port clashes if more than 1 task managers run on the same machine. The specified range can be: a port: \u0026ldquo;9123\u0026rdquo;, a range of ports: \u0026ldquo;50100-50200\u0026rdquo;, or a list of ranges and or points: \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;. The default port is 9067. queryable-state.server.network-threads: number of network (event loop) threads receiving incoming requests for the state server (0 =\u0026gt; #slots) queryable-state.server.query-threads: number of threads handling/serving incoming requests for the state server (0 =\u0026gt; #slots). Proxy # queryable-state.proxy.ports: the server port range of the queryable state proxy. This is useful to avoid port clashes if more than 1 task managers run on the same machine. The specified range can be: a port: \u0026ldquo;9123\u0026rdquo;, a range of ports: \u0026ldquo;50100-50200\u0026rdquo;, or a list of ranges and or points: \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;. The default port is 9069. queryable-state.proxy.network-threads: number of network (event loop) threads receiving incoming requests for the client proxy (0 =\u0026gt; #slots) queryable-state.proxy.query-threads: number of threads handling/serving incoming requests for the client proxy (0 =\u0026gt; #slots). Limitations # The queryable state life-cycle is bound to the life-cycle of the job, e.g. tasks register queryable state on startup and unregister it on disposal. In future versions, it is desirable to decouple this in order to allow queries after a task finishes, and to speed up recovery via state replication. Notifications about available KvState happen via a simple tell. In the future this should be improved to be more robust with asks and acknowledgements. The server and client keep track of statistics for queries. These are currently disabled by default as they would not be exposed anywhere. As soon as there is better support to publish these numbers via the Metrics system, we should enable the stats. Back to top
`}),e.add({id:164,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/select-distinct/",title:"SELECT DISTINCT",section:"Queries",content:` SELECT DISTINCT # Batch Streaming
If SELECT DISTINCT is specified, all duplicate rows are removed from the result set (one row is kept from each group of duplicates).
SELECT DISTINCT id FROM Orders For streaming queries, the required state for computing the query result might grow infinitely. State size depends on number of distinct rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details
Back to top
`}),e.add({id:165,href:"/flink/flink-docs-master/docs/deployment/speculative_execution/",title:"Speculative Execution",section:"Deployment",content:` Speculative Execution # This page describes the background of speculative execution, how to use it, and how to check the effectiveness of it.
Background # Speculative execution is a mechanism to mitigate job slowness which is caused by problematic nodes. A problematic node may have hardware problems, accident I/O busy, or high CPU load. These problems may make the hosted tasks run much slower than tasks on other nodes, and affect the overall execution time of a batch job.
In such cases, speculative execution will start new attempts of the slow task on nodes that are not detected as problematic. The new attempts process the same input data and produces the same data as the old one. The old attempt will not be affected and will keep running. The first finished attempt will be admitted, its output will be seen and consumed by the downstream tasks, and the remaining attempts will be canceled.
To achieve this, Flink uses the slow task detector to detect slow tasks. The nodes that the slow tasks locate in will be identified as problematic nodes and get blocked via the blocklist mechanism. The scheduler will create new attempts for the slow tasks and deploy them on nodes that are not blocked.
Usage # This section describes how to use speculative execution, including how to enable it, how to tune it, and how to develop/improve custom sources to work with speculative execution.
Note: Flink does not support speculative execution of sinks yet and will support it in follow-up releases. Note: Flink does not support speculative execution of DataSet jobs because DataSet will be deprecated in near future. DataStream API is now the recommended low level API to develop Flink batch jobs. Enable Speculative Execution # To enable speculative execution, you need to set the following configuration options:
jobmanager.scheduler: AdaptiveBatch Because only Adaptive Batch Scheduler supports speculative execution. jobmanager.adaptive-batch-scheduler.speculative.enabled: true Tuning Configuration # To make speculative execution work better for different jobs, you can tune below configuration options of the scheduler:
jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration You can also tune below configuration options of the slow task detector:
slow-task-detector.check-interval slow-task-detector.execution-time.baseline-lower-bound slow-task-detector.execution-time.baseline-multiplier slow-task-detector.execution-time.baseline-ratio Enable Sources for Speculative Execution # If your job uses a custom Source , and the source uses custom SourceEvent , you need to change the SplitEnumerator of that source to implement SupportsHandleExecutionAttemptSourceEvent interface.
public interface SupportsHandleExecutionAttemptSourceEvent { void handleSourceEvent(int subtaskId, int attemptNumber, SourceEvent sourceEvent); } This means the SplitEnumerator should be aware of the attempt which sends the event. Otherwise, exceptions will happen when the job manager receives a source event from the tasks and lead to job failures.
No extra change is required for other sources to work with speculative execution, including SourceFunction sources , InputFormat sources , and new sources . All the source connectors offered by Apache Flink can work with speculative execution.
Checking the Effectiveness of Speculative Execution # After enabling speculative execution, when there are slow tasks that trigger speculative execution, the web UI will show the speculative attempts on the SubTasks tab of vertices on the job page. The web UI also shows the blocked taskmanagers on the Flink cluster Overview and Task Managers pages.
You can also check these metrics to see the effectiveness of speculative execution.
Back to top
`}),e.add({id:166,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/",title:"State \u0026 Fault Tolerance",section:"DataStream API",content:""}),e.add({id:167,href:"/flink/flink-docs-master/docs/learn-flink/streaming_analytics/",title:"Streaming Analytics",section:"Learn Flink",content:` Streaming Analytics # Event Time and Watermarks # Introduction # Flink explicitly supports three different notions of time:
event time: the time when an event occurred, as recorded by the device producing (or storing) the event
ingestion time: a timestamp recorded by Flink at the moment it ingests the event
processing time: the time when a specific operator in your pipeline is processing the event
For reproducible results, e.g., when computing the maximum price a stock reached during the first hour of trading on a given day, you should use event time. In this way the result won\u0026rsquo;t depend on when the calculation is performed. This kind of real-time application is sometimes performed using processing time, but then the results are determined by the events that happen to be processed during that hour, rather than the events that occurred then. Computing analytics based on processing time causes inconsistencies, and makes it difficult to re-analyze historic data or test new implementations.
Working with Event Time # If you want to use event time, you will also need to supply a Timestamp Extractor and Watermark Generator that Flink will use to track the progress of event time. This will be covered in the section below on Working with Watermarks, but first we should explain what watermarks are.
Watermarks # Let\u0026rsquo;s work through a simple example that will show why watermarks are needed, and how they work.
In this example you have a stream of timestamped events that arrive somewhat out of order, as shown below. The numbers shown are timestamps that indicate when these events actually occurred. The first event to arrive happened at time 4, and it is followed by an event that happened earlier, at time 2, and so on:
··· 23 19 22 24 21 14 17 13 12 15 9 11 7 2 4 → Now imagine that you are trying create a stream sorter. This is meant to be an application that processes each event from a stream as it arrives, and emits a new stream containing the same events, but ordered by their timestamps.
Some observations:
(1) The first element your stream sorter sees is the 4, but you can\u0026rsquo;t just immediately release it as the first element of the sorted stream. It may have arrived out of order, and an earlier event might yet arrive. In fact, you have the benefit of some god-like knowledge of this stream\u0026rsquo;s future, and you can see that your stream sorter should wait at least until the 2 arrives before producing any results.
Some buffering, and some delay, is necessary.
(2) If you do this wrong, you could end up waiting forever. First the sorter saw an event from time 4, and then an event from time 2. Will an event with a timestamp less than 2 ever arrive? Maybe. Maybe not. You could wait forever and never see a 1.
Eventually you have to be courageous and emit the 2 as the start of the sorted stream.
(3) What you need then is some sort of policy that defines when, for any given timestamped event, to stop waiting for the arrival of earlier events.
This is precisely what watermarks do — they define when to stop waiting for earlier events.
Event time processing in Flink depends on watermark generators that insert special timestamped elements into the stream, called watermarks. A watermark for time t is an assertion that the stream is (probably) now complete up through time t.
When should this stream sorter stop waiting, and push out the 2 to start the sorted stream? When a watermark arrives with a timestamp of 2, or greater.
(4) You might imagine different policies for deciding how to generate watermarks.
Each event arrives after some delay, and these delays vary, so some events are delayed more than others. One simple approach is to assume that these delays are bounded by some maximum delay. Flink refers to this strategy as bounded-out-of-orderness watermarking. It is easy to imagine more complex approaches to watermarking, but for most applications a fixed delay works well enough.
Latency vs. Completeness # Another way to think about watermarks is that they give you, the developer of a streaming application, control over the tradeoff between latency and completeness. Unlike in batch processing, where one has the luxury of being able to have complete knowledge of the input before producing any results, with streaming you must eventually stop waiting to see more of the input, and produce some sort of result.
You can either configure your watermarking aggressively, with a short bounded delay, and thereby take the risk of producing results with rather incomplete knowledge of the input \u0026ndash; i.e., a possibly wrong result, produced quickly. Or you can wait longer, and produce results that take advantage of having more complete knowledge of the input stream(s).
It is also possible to implement hybrid solutions that produce initial results quickly, and then supply updates to those results as additional (late) data is processed. This is a good approach for some applications.
Lateness # Lateness is defined relative to the watermarks. A Watermark(t) asserts that the stream is complete up through time t; any event following this watermark whose timestamp is ≤ t is late.
Working with Watermarks # In order to perform event-time-based event processing, Flink needs to know the time associated with each event, and it also needs the stream to include watermarks.
The Taxi data sources used in the hands-on exercises take care of these details for you. But in your own applications you will have to take care of this yourself, which is usually done by implementing a class that extracts the timestamps from the events, and generates watermarks on demand. The easiest way to do this is by using a WatermarkStrategy:
DataStream\u0026lt;Event\u0026gt; stream = ...; WatermarkStrategy\u0026lt;Event\u0026gt; strategy = WatermarkStrategy .\u0026lt;Event\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -\u0026gt; event.timestamp); DataStream\u0026lt;Event\u0026gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(strategy); Back to top
Windows # Flink features very expressive window semantics.
In this section you will learn:
how windows are used to compute aggregates on unbounded streams, which types of windows Flink supports, and how to implement a DataStream program with a windowed aggregation Introduction # It is natural when doing stream processing to want to compute aggregated analytics on bounded subsets of the streams in order to answer questions like these:
number of page views per minute number of sessions per user per week maximum temperature per sensor per minute Computing windowed analytics with Flink depends on two principal abstractions: Window Assigners that assign events to windows (creating new window objects as necessary), and Window Functions that are applied to the events assigned to a window.
Flink\u0026rsquo;s windowing API also has notions of Triggers, which determine when to call the window function, and Evictors, which can remove elements collected in a window.
In its basic form, you apply windowing to a keyed stream like this:
stream .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;); You can also use windowing with non-keyed streams, but keep in mind that in this case, the processing will not be done in parallel:
stream .windowAll(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;); Window Assigners # Flink has several built-in types of window assigners, which are illustrated below:
Some examples of what these window assigners might be used for, and how to specify them:
Tumbling time windows page views per minute TumblingEventTimeWindows.of(Time.minutes(1)) Sliding time windows page views per minute computed every 10 seconds SlidingEventTimeWindows.of(Time.minutes(1), Time.seconds(10)) Session windows page views per session, where sessions are defined by a gap of at least 30 minutes between sessions EventTimeSessionWindows.withGap(Time.minutes(30)) Durations can be specified using one of Time.milliseconds(n), Time.seconds(n), Time.minutes(n), Time.hours(n), and Time.days(n).
The time-based window assigners (including session windows) come in both event time and processing time flavors. There are significant tradeoffs between these two types of time windows. With processing time windowing you have to accept these limitations:
can not correctly process historic data, can not correctly handle out-of-order data, results will be non-deterministic, but with the advantage of lower latency.
When working with count-based windows, keep in mind that these windows will not fire until a batch is complete. There\u0026rsquo;s no option to time-out and process a partial window, though you could implement that behavior yourself with a custom Trigger.
A global window assigner assigns every event (with the same key) to the same global window. This is only useful if you are going to do your own custom windowing, with a custom Trigger. In many cases where this might seem useful you will be better off using a ProcessFunction as described in another section.
Window Functions # You have three basic options for how to process the contents of your windows:
as a batch, using a ProcessWindowFunction that will be passed an Iterable with the window\u0026rsquo;s contents; incrementally, with a ReduceFunction or an AggregateFunction that is called as each event is assigned to the window; or with a combination of the two, wherein the pre-aggregated results of a ReduceFunction or an AggregateFunction are supplied to a ProcessWindowFunction when the window is triggered. Here are examples of approaches 1 and 3. Each implementation finds the peak value from each sensor in 1 minute event time windows, and producing a stream of Tuples containing (key, end-of-window-timestamp, max_value).
ProcessWindowFunction Example # DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .process(new MyWastefulMax()); public static class MyWastefulMax extends ProcessWindowFunction\u0026lt; SensorReading, // input type Tuple3\u0026lt;String, Long, Integer\u0026gt;, // output type String, // key type TimeWindow\u0026gt; { // window type @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; events, Collector\u0026lt;Tuple3\u0026lt;String, Long, Integer\u0026gt;\u0026gt; out) { int max = 0; for (SensorReading event : events) { max = Math.max(event.value, max); } out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } A couple of things to note in this implementation:
All of the events assigned to the window have to be buffered in keyed Flink state until the window is triggered. This is potentially quite expensive. Our ProcessWindowFunction is being passed a Context object which contains information about the window. Its interface looks like this: public abstract class Context implements java.io.Serializable { public abstract W window(); public abstract long currentProcessingTime(); public abstract long currentWatermark(); public abstract KeyedStateStore windowState(); public abstract KeyedStateStore globalState(); } windowState and globalState are places where you can store per-key, per-window, or global per-key information for all windows of that key. This might be useful, for example, if you want to record something about the current window and use that when processing a subsequent window.
Incremental Aggregation Example # DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .reduce(new MyReducingMax(), new MyWindowFunction()); private static class MyReducingMax implements ReduceFunction\u0026lt;SensorReading\u0026gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() \u0026gt; r2.value() ? r1 : r2; } } private static class MyWindowFunction extends ProcessWindowFunction\u0026lt; SensorReading, Tuple3\u0026lt;String, Long, SensorReading\u0026gt;, String, TimeWindow\u0026gt; { @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; maxReading, Collector\u0026lt;Tuple3\u0026lt;String, Long, SensorReading\u0026gt;\u0026gt; out) { SensorReading max = maxReading.iterator().next(); out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } Notice that the Iterable\u0026lt;SensorReading\u0026gt; will contain exactly one reading \u0026ndash; the pre-aggregated maximum computed by MyReducingMax.
Late Events # By default, when using event time windows, late events are dropped. There are two optional parts of the window API that give you more control over this.
You can arrange for the events that would be dropped to be collected to an alternate output stream instead, using a mechanism called Side Outputs. Here is an example of what that might look like:
OutputTag\u0026lt;Event\u0026gt; lateTag = new OutputTag\u0026lt;Event\u0026gt;(\u0026#34;late\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Event\u0026gt; result = stream .keyBy(...) .window(...) .sideOutputLateData(lateTag) .process(...); DataStream\u0026lt;Event\u0026gt; lateStream = result.getSideOutput(lateTag); You can also specify an interval of allowed lateness during which the late events will continue to be assigned to the appropriate window(s) (whose state will have been retained). By default each late event will cause the window function to be called again (sometimes called a late firing).
By default the allowed lateness is 0. In other words, elements behind the watermark are dropped (or sent to the side output).
For example:
stream .keyBy(...) .window(...) .allowedLateness(Time.seconds(10)) .process(...); When the allowed lateness is greater than zero, only those events that are so late that they would be dropped are sent to the side output (if it has been configured).
Surprises # Some aspects of Flink\u0026rsquo;s windowing API may not behave in the way you would expect. Based on frequently asked questions on the flink-user mailing list and elsewhere, here are some facts about windows that may surprise you.
Sliding Windows Make Copies # Sliding window assigners can create lots of window objects, and will copy each event into every relevant window. For example, if you have sliding windows every 15 minutes that are 24-hours in length, each event will be copied into 4 * 24 = 96 windows.
Time Windows are Aligned to the Epoch # Just because you are using hour-long processing-time windows and start your application running at 12:05 does not mean that the first window will close at 1:05. The first window will be 55 minutes long and close at 1:00.
Note, however, that the tumbling and sliding window assigners take an optional offset parameter that can be used to change the alignment of the windows. See Tumbling Windows and Sliding Windows for details.
Windows Can Follow Windows # For example, it works to do this:
stream .keyBy(t -\u0026gt; t.key) .window(\u0026lt;window assigner\u0026gt;) .reduce(\u0026lt;reduce function\u0026gt;) .windowAll(\u0026lt;same window assigner\u0026gt;) .reduce(\u0026lt;same reduce function\u0026gt;); You might expect Flink\u0026rsquo;s runtime to be smart enough to do this parallel pre-aggregation for you (provided you are using a ReduceFunction or AggregateFunction), but it\u0026rsquo;s not.
The reason why this works is that the events produced by a time window are assigned timestamps based on the time at the end of the window. So, for example, all of the events produced by an hour-long window will have timestamps marking the end of an hour. Any subsequent window consuming those events should have a duration that is the same as, or a multiple of, the previous window.
No Results for Empty TimeWindows # Windows are only created when events are assigned to them. So if there are no events in a given time frame, no results will be reported.
Late Events Can Cause Late Merges # Session windows are based on an abstraction of windows that can merge. Each element is initially assigned to a new window, after which windows are merged whenever the gap between them is small enough. In this way, a late event can bridge the gap separating two previously separate sessions, producing a late merge.
Back to top
Hands-on # The hands-on exercise that goes with this section is the Hourly Tips Exercise .
Further Reading # Timely Stream Processing Windows Back to top
`}),e.add({id:168,href:"/flink/flink-docs-master/docs/dev/datastream/user_defined_functions/",title:"User-Defined Functions",section:"DataStream API",content:` User-Defined Functions # Most operations require a user-defined function. This section lists different ways of how they can be specified. We also cover Accumulators, which can be used to gain insights into your Flink application.
Java Implementing an interface # The most basic way is to implement one of the provided interfaces:
class MyMapFunction implements MapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } data.map(new MyMapFunction()); Anonymous classes # You can pass a function as an anonymous class:
data.map(new MapFunction\u0026lt;String, Integer\u0026gt; () { public Integer map(String value) { return Integer.parseInt(value); } }); Java 8 Lambdas # Flink also supports Java 8 Lambdas in the Java API.
data.filter(s -\u0026gt; s.startsWith(\u0026#34;http://\u0026#34;)); data.reduce((i1,i2) -\u0026gt; i1 + i2); Rich functions # All transformations that require a user-defined function can instead take as argument a rich function. For example, instead of
class MyMapFunction implements MapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } you can write
class MyMapFunction extends RichMapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } and pass the function as usual to a map transformation:
data.map(new MyMapFunction()); Rich functions can also be defined as an anonymous class:
data.map (new RichMapFunction\u0026lt;String, Integer\u0026gt;() { public Integer map(String value) { return Integer.parseInt(value); } }); Scala Lambda Functions # As already seen in previous examples all operations accept lambda functions for describing the operation:
val data: DataSet[String] = // [...] data.filter { _.startsWith(\u0026#34;http://\u0026#34;) } val data: DataSet[Int] = // [...] data.reduce { (i1,i2) =\u0026gt; i1 + i2 } // or data.reduce { _ + _ } Rich functions # All transformations that take as argument a lambda function can instead take as argument a rich function. For example, instead of
data.map { x =\u0026gt; x.toInt } you can write
class MyMapFunction extends RichMapFunction[String, Int] { def map(in: String): Int = in.toInt } and pass the function to a map transformation:
data.map(new MyMapFunction()) Rich functions can also be defined as an anonymous class:
data.map (new RichMapFunction[String, Int] { def map(in: String): Int = in.toInt }) Rich functions provide, in addition to the user-defined function (map, reduce, etc), four methods: open, close, getRuntimeContext, and setRuntimeContext. These are useful for parameterizing the function (see Passing Parameters to Functions), creating and finalizing local state, accessing broadcast variables (see Broadcast Variables), and for accessing runtime information such as accumulators and counters (see Accumulators and Counters), and information on iterations (see Iterations).
Back to top
Accumulators \u0026amp; Counters # Accumulators are simple constructs with an add operation and a final accumulated result, which is available after the job ended.
The most straightforward accumulator is a counter: You can increment it using the Accumulator.add(V value) method. At the end of the job Flink will sum up (merge) all partial results and send the result to the client. Accumulators are useful during debugging or if you quickly want to find out more about your data.
Flink currently has the following built-in accumulators. Each of them implements the Accumulator interface.
IntCounter , LongCounter and DoubleCounter : See below for an example using a counter. Histogram : A histogram implementation for a discrete number of bins. Internally it is just a map from Integer to Integer. You can use this to compute distributions of values, e.g. the distribution of words-per-line for a word count program. How to use accumulators:
First you have to create an accumulator object (here a counter) in the user-defined transformation function where you want to use it.
private IntCounter numLines = new IntCounter(); Second you have to register the accumulator object, typically in the open() method of the rich function. Here you also define the name.
getRuntimeContext().addAccumulator(\u0026#34;num-lines\u0026#34;, this.numLines); You can now use the accumulator anywhere in the operator function, including in the open() and close() methods.
this.numLines.add(1); The overall result will be stored in the JobExecutionResult object which is returned from the execute() method of the execution environment (currently this only works if the execution waits for the completion of the job).
myJobExecutionResult.getAccumulatorResult(\u0026#34;num-lines\u0026#34;); All accumulators share a single namespace per job. Thus you can use the same accumulator in different operator functions of your job. Flink will internally merge all accumulators with the same name.
A note on accumulators and iterations: Currently the result of accumulators is only available after the overall job has ended. We plan to also make the result of the previous iteration available in the next iteration. You can use Aggregators to compute per-iteration statistics and base the termination of iterations on such statistics.
Custom accumulators:
To implement your own accumulator you simply have to write your implementation of the Accumulator interface. Feel free to create a pull request if you think your custom accumulator should be shipped with Flink.
You have the choice to implement either Accumulator or SimpleAccumulator .
Accumulator\u0026lt;V,R\u0026gt; is most flexible: It defines a type V for the value to add, and a result type R for the final result. E.g. for a histogram, V is a number and R is a histogram. SimpleAccumulator is for the cases where both types are the same, e.g. for counters.
Back to top
`}),e.add({id:169,href:"/flink/flink-docs-master/docs/deployment/resource-providers/yarn/",title:"YARN",section:"Resource Providers",content:` Apache Hadoop YARN # Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on YARN.
Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN\u0026rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.
Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.8.5. YARN environments are provided most conveniently through services such as Amazon EMR, Google Cloud DataProc or products like Cloudera. Manually setting up a YARN environment locally or on a cluster is not recommended for following through this Getting Started tutorial.
Make sure your YARN cluster is ready for accepting Flink applications by running yarn top. It should show no error messages. Download a recent Flink distribution from the download page and unpack it. Important Make sure that the HADOOP_CLASSPATH environment variable is set up (it can be checked by running echo \$HADOOP_CLASSPATH). If not, set it up using export HADOOP_CLASSPATH=\`hadoop classpath\` Starting a Flink Session on YARN # Once you\u0026rsquo;ve made sure that the HADOOP_CLASSPATH environment variable is set, you can launch a Flink on YARN session, and submit an example job:
# we assume to be in the root directory of # the unzipped Flink distribution # (0) export HADOOP_CLASSPATH export HADOOP_CLASSPATH=\`hadoop classpath\` # (1) Start YARN Session ./bin/yarn-session.sh --detached # (2) You can now access the Flink Web Interface through the # URL printed in the last lines of the command output, or through # the YARN ResourceManager web UI. # (3) Submit example job ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar # (4) Stop YARN session (replace the application id based # on the output of the yarn-session.sh command) echo \u0026#34;stop\u0026#34; | ./bin/yarn-session.sh -id application_XXXXX_XXX Congratulations! You have successfully run a Flink application by deploying Flink on YARN.
Back to top
Deployment Modes Supported by Flink on YARN # For production use, we recommend deploying Flink Applications in Application Mode as it provides a better isolation between applications.
Application Mode # For high-level intuition behind the application mode, please refer to the deployment mode overview. Application Mode will launch a Flink cluster on YARN, where the main() method of the application jar gets executed on the JobManager in YARN. The cluster will shut down as soon as the application has finished. You can manually stop the cluster using yarn application -kill \u0026lt;ApplicationId\u0026gt; or by cancelling the Flink job.
./bin/flink run-application -t yarn-application ./examples/streaming/TopSpeedWindowing.jar Once an Application Mode cluster is deployed, you can interact with it for operations like cancelling or taking a savepoint.
# List running job on the cluster ./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY # Cancel running job ./bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY \u0026lt;jobId\u0026gt; Note that cancelling your job on an Application Cluster will stop the cluster.
To unlock the full potential of the application mode, consider using it with the yarn.provided.lib.dirs configuration option and pre-upload your application jar to a location accessible by all nodes in your cluster. In this case, the command could look like:
./bin/flink run-application -t yarn-application \\ -Dyarn.provided.lib.dirs=\u0026#34;hdfs://myhdfs/my-remote-flink-dist-dir\u0026#34; \\ hdfs://myhdfs/jars/my-application.jar The above will allow the job submission to be extra lightweight as the needed Flink jars and the application jar are going to be picked up by the specified remote locations rather than be shipped to the cluster by the client.
Session Mode # For high-level intuition behind the session mode, please refer to the deployment mode overview. We describe deployment with the Session Mode in the Getting Started guide at the top of the page.
The Session Mode has two operation modes:
attached mode (default): The yarn-session.sh client submits the Flink cluster to YARN, but the client keeps running, tracking the state of the cluster. If the cluster fails, the client will show the error. If the client gets terminated, it will signal the cluster to shut down as well. detached mode (-d or --detached): The yarn-session.sh client submits the Flink cluster to YARN, then the client returns. Another invocation of the client, or YARN tools is needed to stop the Flink cluster. The session mode will create a hidden YARN properties file in /tmp/.yarn-properties-\u0026lt;username\u0026gt;, which will be picked up for cluster discovery by the command line interface when submitting a job.
You can also manually specify the target YARN cluster in the command line interface when submitting a Flink job. Here\u0026rsquo;s an example:
./bin/flink run -t yarn-session \\ -Dyarn.application.id=application_XXXX_YY \\ ./examples/streaming/TopSpeedWindowing.jar You can re-attach to a YARN session using the following command:
./bin/yarn-session.sh -id application_XXXX_YY Besides passing configuration via the conf/flink-conf.yaml file, you can also pass any configuration at submission time to the ./bin/yarn-session.sh client using -Dkey=value arguments.
The YARN session client also has a few \u0026ldquo;shortcut arguments\u0026rdquo; for commonly used settings. They can be listed with ./bin/yarn-session.sh -h.
Back to top
Per-Job Mode (deprecated) # Per-job mode is only supported by YARN and has been deprecated in Flink 1.15. It will be dropped in FLINK-26000. Please consider application mode to launch a dedicated cluster per-job on YARN. For high-level intuition behind the per-job mode, please refer to the deployment mode overview. The Per-job Cluster mode will launch a Flink cluster on YARN, then run the provided application jar locally and finally submit the JobGraph to the JobManager on YARN. If you pass the --detached argument, the client will stop once the submission is accepted.
The YARN cluster will stop once the job has stopped.
./bin/flink run -t yarn-per-job --detached ./examples/streaming/TopSpeedWindowing.jar Once a Per-Job Cluster is deployed, you can interact with it for operations like cancelling or taking a savepoint.
# List running job on the cluster ./bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY # Cancel running job ./bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY \u0026lt;jobId\u0026gt; Note that cancelling your job on an Per-Job Cluster will stop the cluster.
Flink on YARN Reference # Configuring Flink on YARN # The YARN-specific configurations are listed on the configuration page.
The following configuration parameters are managed by Flink on YARN, as they might get overwritten by the framework at runtime:
jobmanager.rpc.address (dynamically set to the address of the JobManager container by Flink on YARN) io.tmp.dirs (If not set, Flink sets the temporary directories defined by YARN) high-availability.cluster-id (automatically generated ID to distinguish multiple clusters in the HA service) If you need to pass additional Hadoop configuration files to Flink, you can do so via the HADOOP_CONF_DIR environment variable, which accepts a directory name containing Hadoop configuration files. By default, all required Hadoop configuration files are loaded from the classpath via the HADOOP_CLASSPATH environment variable.
Resource Allocation Behavior # A JobManager running on YARN will request additional TaskManagers, if it can not run all submitted jobs with the existing resources. In particular when running in Session Mode, the JobManager will, if needed, allocate additional TaskManagers as additional jobs are submitted. Unused TaskManagers are freed up again after a timeout.
The memory configurations for JobManager and TaskManager processes will be respected by the YARN implementation. The number of reported VCores is by default equal to the number of configured slots per TaskManager. The yarn.containers.vcores allows overwriting the number of vcores with a custom value. In order for this parameter to work you should enable CPU scheduling in your YARN cluster.
Failed containers (including the JobManager) are replaced by YARN. The maximum number of JobManager container restarts is configured via yarn.application-attempts (default 1). The YARN Application will fail once all attempts are exhausted.
High-Availability on YARN # High-Availability on YARN is achieved through a combination of YARN and a high availability service.
Once a HA service is configured, it will persist JobManager metadata and perform leader elections.
YARN is taking care of restarting failed JobManagers. The maximum number of JobManager restarts is defined through two configuration parameters. First Flink\u0026rsquo;s yarn.application-attempts configuration will default 2. This value is limited by YARN\u0026rsquo;s yarn.resourcemanager.am.max-attempts, which also defaults to 2.
Note that Flink is managing the high-availability.cluster-id configuration parameter when deploying on YARN. Flink sets it per default to the YARN application id. You should not overwrite this parameter when deploying an HA cluster on YARN. The cluster ID is used to distinguish multiple HA clusters in the HA backend (for example Zookeeper). Overwriting this configuration parameter can lead to multiple YARN clusters affecting each other.
Container Shutdown Behaviour # YARN 2.3.0 \u0026lt; version \u0026lt; 2.4.0. All containers are restarted if the application master fails. YARN 2.4.0 \u0026lt; version \u0026lt; 2.6.0. TaskManager containers are kept alive across application master failures. This has the advantage that the startup time is faster and that the user does not have to wait for obtaining the container resources again. YARN 2.6.0 \u0026lt;= version: Sets the attempt failure validity interval to the Flinks\u0026rsquo; Akka timeout value. The attempt failure validity interval says that an application is only killed after the system has seen the maximum number of application attempts during one interval. This avoids that a long lasting job will deplete it\u0026rsquo;s application attempts. Hadoop YARN 2.4.0 has a major bug (fixed in 2.5.0) preventing container restarts from a restarted Application Master/Job Manager container. See FLINK-4142 for details. We recommend using at least Hadoop 2.5.0 for high availability setups on YARN. Supported Hadoop versions. # Flink on YARN is compiled against Hadoop 2.8.5, and all Hadoop versions \u0026gt;= 2.8.5 are supported, including Hadoop 3.x.
For providing Flink with the required Hadoop dependencies, we recommend setting the HADOOP_CLASSPATH environment variable already introduced in the Getting Started / Preparation section.
If that is not possible, the dependencies can also be put into the lib/ folder of Flink.
Flink also offers pre-bundled Hadoop fat jars for placing them in the lib/ folder, on the Downloads / Additional Components section of the website. These pre-bundled fat jars are shaded to avoid dependency conflicts with common libraries. The Flink community is not testing the YARN integration against these pre-bundled jars.
Running Flink on YARN behind Firewalls # Some YARN clusters use firewalls for controlling the network traffic between the cluster and the rest of the network. In those setups, Flink jobs can only be submitted to a YARN session from within the cluster\u0026rsquo;s network (behind the firewall). If this is not feasible for production use, Flink allows to configure a port range for its REST endpoint, used for the client-cluster communication. With this range configured, users can also submit jobs to Flink crossing the firewall.
The configuration parameter for specifying the REST endpoint port is rest.bind-port. This configuration option accepts single ports (for example: \u0026ldquo;50010\u0026rdquo;), ranges (\u0026ldquo;50000-50025\u0026rdquo;), or a combination of both.
User jars \u0026amp; Classpath # Session Mode
When deploying Flink with Session Mode on Yarn, only the JAR file specified in startup command will be recognized as user-jars and included into user classpath.
PerJob Mode \u0026amp; Application Mode
When deploying Flink with PerJob/Application Mode on Yarn, the JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder will be recognized as user-jars. By default Flink will include the user-jars into the system classpath. This behavior can be controlled with the yarn.classpath.include-user-jar parameter.
When setting this to DISABLED Flink will include the jar in the user classpath instead.
The user-jars position in the classpath can be controlled by setting the parameter to one of the following:
ORDER: (default) Adds the jar to the system classpath based on the lexicographic order. FIRST: Adds the jar to the beginning of the system classpath. LAST: Adds the jar to the end of the system classpath. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:170,href:"/flink/flink-docs-master/docs/dev/table/sql/alter/",title:"ALTER Statements",section:"SQL",content:` ALTER Statements # ALTER statements are used to modified a registered table/view/function definition in the Catalog.
Flink SQL supports the following ALTER statements for now:
ALTER TABLE ALTER VIEW ALTER DATABASE ALTER FUNCTION Run an ALTER statement # Java ALTER statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful ALTER operation, otherwise will throw an exception.
The following examples show how to run an ALTER statement in TableEnvironment.
Scala ALTER statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful ALTER operation, otherwise will throw an exception.
The following examples show how to run an ALTER statement in TableEnvironment.
Python ALTER statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful ALTER operation, otherwise will throw an exception.
The following examples show how to run an ALTER statement in TableEnvironment.
SQL CLI ALTER statements can be executed in SQL CLI.
The following examples show how to run an ALTER statement in SQL CLI.
Java EnvironmentSettings settings = EnvironmentSettings.newInstance()... TableEnvironment tableEnv = TableEnvironment.create(settings); // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // a string array: [\u0026#34;Orders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // rename \u0026#34;Orders\u0026#34; to \u0026#34;NewOrders\u0026#34; tableEnv.executeSql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;); // a string array: [\u0026#34;NewOrders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // a string array: [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // rename \u0026#34;Orders\u0026#34; to \u0026#34;NewOrders\u0026#34; tableEnv.executeSql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;) // a string array: [\u0026#34;NewOrders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() Python table_env = TableEnvironment.create(...) # a string array: [\u0026#34;Orders\u0026#34;] tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() # rename \u0026#34;Orders\u0026#34; to \u0026#34;NewOrders\u0026#34; table_env.execute_sql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;) # a string array: [\u0026#34;NewOrders\u0026#34;] tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; Orders Flink SQL\u0026gt; ALTER TABLE Orders RENAME TO NewOrders; [INFO] Table has been removed. Flink SQL\u0026gt; SHOW TABLES; NewOrders ALTER TABLE # Rename Table ALTER TABLE [catalog_name.][db_name.]table_name RENAME TO new_table_name Rename the given table name to another new table name.
Set or Alter Table Properties ALTER TABLE [catalog_name.][db_name.]table_name SET (key1=val1, key2=val2, ...) Set one or more properties in the specified table. If a particular property is already set in the table, override the old value with the new one.
ALTER VIEW # ALTER VIEW [catalog_name.][db_name.]view_name RENAME TO new_view_name Renames a given view to a new name within the same catalog and database.
ALTER VIEW [catalog_name.][db_name.]view_name AS new_query_expression Changes the underlying query defining the given view to a new query.
ALTER DATABASE # ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...) Set one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.
ALTER FUNCTION # ALTER [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF EXISTS] [catalog_name.][db_name.]function_name AS identifier [LANGUAGE JAVA|SCALA|PYTHON] Alter a catalog function with the new identifier and optional language tag. If a function doesn\u0026rsquo;t exist in the catalog, an exception is thrown.
If the language tag is JAVA/SCALA, the identifier is the full classpath of the UDF. For the implementation of Java/Scala UDF, please refer to User-defined Functions for more details.
If the language tag is PYTHON, the identifier is the fully qualified name of the UDF, e.g. pyflink.table.tests.test_udf.add. For the implementation of Python UDF, please refer to Python UDFs for more details.
TEMPORARY
Alter temporary catalog function that has catalog and database namespaces and overrides catalog functions.
TEMPORARY SYSTEM
Alter temporary system function that has no namespace and overrides built-in functions
IF EXISTS
If the function doesn\u0026rsquo;t exist, nothing happens.
LANGUAGE JAVA|SCALA|PYTHON
Language tag to instruct flink runtime how to execute the function. Currently only JAVA, SCALA and PYTHON are supported, the default language for a function is JAVA.
`}),e.add({id:171,href:"/flink/flink-docs-master/docs/connectors/table/formats/canal/",title:"Canal",section:"Formats",content:` Canal Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Canal is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into other systems. Canal provides a unified format schema for changelog and supports to serialize messages using JSON and protobuf (protobuf is the default format for Canal).
Flink supports to interpret Canal JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as
synchronizing incremental data from databases to other systems auditing logs real-time materialized views on databases temporal join changing history of a database table and so on. Flink also supports to encode the INSERT/UPDATE/DELETE messages in Flink SQL as Canal JSON messages, and emit to storage like Kafka. However, currently Flink can\u0026rsquo;t combine UPDATE_BEFORE and UPDATE_AFTER into a single UPDATE message. Therefore, Flink encodes UPDATE_BEFORE and UPDATE_AFTER as DELETE and INSERT Canal messages.
Note: Support for interpreting Canal protobuf messages is on the roadmap.
Dependencies # In order to use the Canal format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in Note: please refer to Canal documentation about how to deploy Canal to synchronize changelog to message queues.
How to use Canal format # Canal provides a unified format for changelog, here is a simple example for an update operation captured from a MySQL products table:
{ \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;111\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;5.18\u0026#34; } ], \u0026#34;database\u0026#34;: \u0026#34;inventory\u0026#34;, \u0026#34;es\u0026#34;: 1589373560000, \u0026#34;id\u0026#34;: 9, \u0026#34;isDdl\u0026#34;: false, \u0026#34;mysqlType\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;INTEGER\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VARCHAR(255)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;VARCHAR(512)\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;FLOAT\u0026#34; }, \u0026#34;old\u0026#34;: [ { \u0026#34;weight\u0026#34;: \u0026#34;5.15\u0026#34; } ], \u0026#34;pkNames\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;sql\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sqlType\u0026#34;: { \u0026#34;id\u0026#34;: 4, \u0026#34;name\u0026#34;: 12, \u0026#34;description\u0026#34;: 12, \u0026#34;weight\u0026#34;: 7 }, \u0026#34;table\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;ts\u0026#34;: 1589373560798, \u0026#34;type\u0026#34;: \u0026#34;UPDATE\u0026#34; } Note: please refer to Canal documentation about the meaning of each fields.
The MySQL products table has 4 columns (id, name, description and weight). The above JSON message is an update change event on the products table where the weight value of the row with id = 111 is changed from 5.18 to 5.15. Assuming the messages have been synchronized to Kafka topic products_binlog, then we can use the following DDL to consume this topic and interpret the change events.
CREATE TABLE topic_products ( -- schema is totally the same to the MySQL \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;canal-json\u0026#39; -- using canal-json as the format ) After registering the topic as a Flink table, you can consume the Canal messages as a changelog source.
-- a real-time materialized view on the MySQL \u0026#34;products\u0026#34; -- which calculates the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of MySQL \u0026#34;products\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format. Key Data Type Description database STRING NULL The originating database. Corresponds to the database field in the Canal record if available. table STRING NULL The originating database table. Corresponds to the table field in the Canal record if available. sql-type MAP\u0026lt;STRING, INT\u0026gt; NULL Map of various sql types. Corresponds to the sqlType field in the Canal record if available. pk-names ARRAY\u0026lt;STRING\u0026gt; NULL Array of primary key names. Corresponds to the pkNames field in the Canal record if available. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts field in the Canal record. The following example shows how to access Canal metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_database STRING METADATA FROM \u0026#39;value.database\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, origin_sql_type MAP\u0026lt;STRING, INT\u0026gt; METADATA FROM \u0026#39;value.sql-type\u0026#39; VIRTUAL, origin_pk_names ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.pk-names\u0026#39; VIRTUAL, origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;canal-json\u0026#39; ); Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'canal-json'. canal-json.ignore-parse-errors optional false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. canal-json.timestamp-format.standard optional 'SQL' String Specify the input and output timestamp format. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input timestamp in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g '2020-12-30 12:13:14.123' and output timestamp in the same format. Option 'ISO-8601' will parse input timestamp in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g '2020-12-30T12:13:14.123' and output timestamp in the same format. canal-json.map-null-key.mode optional 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map value with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by canal-json.map-null-key.literal option. canal-json.map-null-key.literal optional 'null' String Specify string literal to replace null key when 'canal-json.map-null-key.mode' is LITERAL. canal-json.encode.decimal-as-plain-number optional false Boolean Encode all decimals as plain numbers instead of possible scientific notations. By default, decimals may be written using scientific notation. For example, 0.000000027 is encoded as 2.7E-8 by default, and will be written as 0.000000027 if set this option to true. canal-json.database.include optional (none) String An optional regular expression to only read the specific databases changelog rows by regular matching the "database" meta field in the Canal record. The pattern string is compatible with Java's Pattern. canal-json.table.include optional (none) String An optional regular expression to only read the specific tables changelog rows by regular matching the "table" meta field in the Canal record. The pattern string is compatible with Java's Pattern. Caveats # Duplicate change events # Under normal operating scenarios, the Canal application delivers every change event exactly-once. Flink works pretty well when consuming Canal produced events in this situation. However, Canal application works in at-least-once delivery if any failover happens. That means, in the abnormal situations, Canal may deliver duplicate change events to message queues and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions. Thus, it is recommended to set job configuration table.exec.source.cdc-events-duplicate to true and define PRIMARY KEY on the source in this situation. Framework will generate an additional stateful operator, and use the primary key to deduplicate the change events and produce a normalized changelog stream.
Data Type Mapping # Currently, the Canal format uses JSON format for serialization and deserialization. Please refer to JSON format documentation for more details about the data type mapping.
`}),e.add({id:172,href:"/flink/flink-docs-master/docs/connectors/",title:"Connectors",section:"Docs",content:""}),e.add({id:173,href:"/flink/flink-docs-master/docs/learn-flink/event_driven/",title:"Event-driven Applications",section:"Learn Flink",content:` Event-driven Applications # Process Functions # Introduction # A ProcessFunction combines event processing with timers and state, making it a powerful building block for stream processing applications. This is the basis for creating event-driven applications with Flink. It is very similar to a RichFlatMapFunction, but with the addition of timers.
Example # If you\u0026rsquo;ve done the hands-on exercise in the Streaming Analytics training, you will recall that it uses a TumblingEventTimeWindow to compute the sum of the tips for each driver during each hour, like this:
// compute the sum of the tips per hour for each driver DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .window(TumblingEventTimeWindows.of(Time.hours(1))) .process(new AddTips()); It is reasonably straightforward, and educational, to do the same thing with a KeyedProcessFunction. Let us begin by replacing the code above with this:
// compute the sum of the tips per hour for each driver DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); In this code snippet a KeyedProcessFunction called PseudoWindow is being applied to a keyed stream, the result of which is a DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; (the same kind of stream produced by the implementation that uses Flink\u0026rsquo;s built-in time windows).
The overall outline of PseudoWindow has this shape:
// Compute the sum of the tips for each driver in hour-long windows. // The keys are driverIds. public static class PseudoWindow extends KeyedProcessFunction\u0026lt;Long, TaxiFare, Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; { private final long durationMsec; public PseudoWindow(Time duration) { this.durationMsec = duration.toMilliseconds(); } @Override // Called once during initialization. public void open(Configuration conf) { . . . } @Override // Called as each fare arrives to be processed. public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } @Override // Called when the current watermark indicates that a window is now complete. public void onTimer(long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } } Things to be aware of:
There are several types of ProcessFunctions \u0026ndash; this is a KeyedProcessFunction, but there are also CoProcessFunctions, BroadcastProcessFunctions, etc.
A KeyedProcessFunction is a kind of RichFunction. Being a RichFunction, it has access to the open and getRuntimeContext methods needed for working with managed keyed state.
There are two callbacks to implement: processElement and onTimer. processElement is called with each incoming event; onTimer is called when timers fire. These can be either event time or processing time timers. Both processElement and onTimer are provided with a context object that can be used to interact with a TimerService (among other things). Both callbacks are also passed a Collector that can be used to emit results.
The open() method # // Keyed, managed state, with an entry for each window, keyed by the window\u0026#39;s end time. // There is a separate MapState object for each driver. private transient MapState\u0026lt;Long, Float\u0026gt; sumOfTips; @Override public void open(Configuration conf) { MapStateDescriptor\u0026lt;Long, Float\u0026gt; sumDesc = new MapStateDescriptor\u0026lt;\u0026gt;(\u0026#34;sumOfTips\u0026#34;, Long.class, Float.class); sumOfTips = getRuntimeContext().getMapState(sumDesc); } Because the fare events can arrive out of order, it will sometimes be necessary to process events for one hour before having finished computing the results for the previous hour. In fact, if the watermarking delay is much longer than the window length, then there may be many windows open simultaneously, rather than just two. This implementation supports this by using a MapState that maps the timestamp for the end of each window to the sum of the tips for that window.
The processElement() method # public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long eventTime = fare.getEventTime(); TimerService timerService = ctx.timerService(); if (eventTime \u0026lt;= timerService.currentWatermark()) { // This event is late; its window has already been triggered. } else { // Round up eventTime to the end of the window containing this event. long endOfWindow = (eventTime - (eventTime % durationMsec) + durationMsec - 1); // Schedule a callback for when the window has been completed. timerService.registerEventTimeTimer(endOfWindow); // Add this fare\u0026#39;s tip to the running total for that window. Float sum = sumOfTips.get(endOfWindow); if (sum == null) { sum = 0.0F; } sum += fare.tip; sumOfTips.put(endOfWindow, sum); } } Things to consider:
What happens with late events? Events that are behind the watermark (i.e., late) are being dropped. If you want to do something better than this, consider using a side output, which is explained in the next section.
This example uses a MapState where the keys are timestamps, and sets a Timer for that same timestamp. This is a common pattern; it makes it easy and efficient to lookup relevant information when the timer fires.
The onTimer() method # public void onTimer( long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long driverId = context.getCurrentKey(); // Look up the result for the hour that just ended. Float sumOfTips = this.sumOfTips.get(timestamp); Tuple3\u0026lt;Long, Long, Float\u0026gt; result = Tuple3.of(driverId, timestamp, sumOfTips); out.collect(result); this.sumOfTips.remove(timestamp); } Observations:
The OnTimerContext context passed in to onTimer can be used to determine the current key.
Our pseudo-windows are being triggered when the current watermark reaches the end of each hour, at which point onTimer is called. This onTimer method removes the related entry from sumOfTips, which has the effect of making it impossible to accommodate late events. This is the equivalent of setting the allowedLateness to zero when working with Flink\u0026rsquo;s time windows.
Performance Considerations # Flink provides MapState and ListState types that are optimized for RocksDB. Where possible, these should be used instead of a ValueState object holding some sort of collection. The RocksDB state backend can append to ListState without going through (de)serialization, and for MapState, each key/value pair is a separate RocksDB object, so MapState can be efficiently accessed and updated.
Back to top
Side Outputs # Introduction # There are several good reasons to want to have more than one output stream from a Flink operator, such as reporting:
exceptions malformed events late events operational alerts, such as timed-out connections to external services Side outputs are a convenient way to do this. Beyond error reporting, side outputs are also a good way to implement an n-way split of a stream.
Example # You are now in a position to do something with the late events that were ignored in the previous section.
A side output channel is associated with an OutputTag\u0026lt;T\u0026gt;. These tags have generic types that correspond to the type of the side output\u0026rsquo;s DataStream, and they have names.
private static final OutputTag\u0026lt;TaxiFare\u0026gt; lateFares = new OutputTag\u0026lt;TaxiFare\u0026gt;(\u0026#34;lateFares\u0026#34;) {}; Shown above is a static OutputTag\u0026lt;TaxiFare\u0026gt; that can be referenced both when emitting late events in the processElement method of the PseudoWindow:
if (eventTime \u0026lt;= timerService.currentWatermark()) { // This event is late; its window has already been triggered. ctx.output(lateFares, fare); } else { . . . } and when accessing the stream from this side output in the main method of the job:
// compute the sum of the tips per hour for each driver SingleOutputStreamOperator hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); hourlyTips.getSideOutput(lateFares).print(); Alternatively, you can use two OutputTags with the same name to refer to the same side output, but if you do, they must have the same type.
Back to top
Closing Remarks # In this example you have seen how a ProcessFunction can be used to reimplement a straightforward time window. Of course, if Flink\u0026rsquo;s built-in windowing API meets your needs, by all means, go ahead and use it. But if you find yourself considering doing something contorted with Flink\u0026rsquo;s windows, don\u0026rsquo;t be afraid to roll your own.
Also, ProcessFunctions are useful for many other use cases beyond computing analytics. The hands-on exercise below provides an example of something completely different.
Another common use case for ProcessFunctions is for expiring stale state. If you think back to the Rides and Fares Exercise , where a RichCoFlatMapFunction is used to compute a simple join, the sample solution assumes that the TaxiRides and TaxiFares are perfectly matched, one-to-one for each rideId. If an event is lost, the other event for the same rideId will be held in state forever. This could instead be implemented as a KeyedCoProcessFunction, and a timer could be used to detect and clear any stale state.
Back to top
Hands-on # The hands-on exercise that goes with this section is the Long Ride Alerts Exercise .
Back to top
Further Reading # ProcessFunction Side Outputs Back to top
`}),e.add({id:174,href:"/flink/flink-docs-master/docs/deployment/filesystems/",title:"File Systems",section:"Deployment",content:""}),e.add({id:175,href:"/flink/flink-docs-master/docs/connectors/datastream/filesystem/",title:"FileSystem",section:"DataStream Connectors",content:` FileSystem # This connector provides a unified Source and Sink for BATCH and STREAMING that reads or writes (partitioned) files to file systems supported by the Flink FileSystem abstraction. This filesystem connector provides the same guarantees for both BATCH and STREAMING and is designed to provide exactly-once semantics for STREAMING execution.
The connector supports reading and writing a set of files from any (distributed) file system (e.g. POSIX, S3, HDFS) with a format (e.g., Avro, CSV, Parquet), and produces a stream or records.
File Source # The File Source is based on the Source API, a unified data source that reads files - both in batch and in streaming mode. It is divided into the following two parts: SplitEnumerator and SourceReader.
SplitEnumerator is responsible for discovering and identifying the files to read and assigns them to the SourceReader. SourceReader requests the files it needs to process and reads the file from the filesystem. You will need to combine the File Source with a format, which allows you to parse CSV, decode AVRO, or read Parquet columnar files.
Bounded and Unbounded Streams # A bounded File Source lists all files (via SplitEnumerator - a recursive directory list with filtered-out hidden files) and reads them all.
An unbounded File Source is created when configuring the enumerator for periodic file discovery. In this case, the SplitEnumerator will enumerate like the bounded case but, after a certain interval, repeats the enumeration. For any repeated enumeration, the SplitEnumerator filters out previously detected files and only sends new ones to the SourceReader.
Usage # You can start building a File Source via one of the following API calls:
Java // reads the contents of a file from a file stream. FileSource.forRecordStreamFormat(StreamFormat,Path...); // reads batches of records from a file at a time FileSource.forBulkFileFormat(BulkFormat,Path...); Python # reads the contents of a file from a file stream. FileSource.for_record_stream_format(stream_format, *path) # reads batches of records from a file at a time FileSource.for_bulk_file_format(bulk_format, *path) This creates a FileSource.FileSourceBuilder on which you can configure all the properties of the File Source.
For the bounded/batch case, the File Source processes all files under the given path(s). For the continuous/streaming case, the source periodically checks the paths for new files and will start reading those.
When you start creating a File Source (via the FileSource.FileSourceBuilder created through one of the above-mentioned methods), the source is in bounded/batch mode by default. You can call AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration) to put the source into continuous streaming mode.
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(...) .monitorContinuously(Duration.ofMillis(5)) .build(); Python source = FileSource.for_record_stream_format(...) \\ .monitor_continously(Duration.of_millis(5)) \\ .build() Format Types # The reading of each file happens through file readers defined by file formats. These define the parsing logic for the contents of the file. There are multiple classes that the source supports. The interfaces are a tradeoff between simplicity of implementation and flexibility/efficiency.
A StreamFormat reads the contents of a file from a file stream. It is the simplest format to implement, and provides many features out-of-the-box (like checkpointing logic) but is limited in the optimizations it can apply (such as object reuse, batching, etc.).
A BulkFormat reads batches of records from a file at a time. It is the most \u0026ldquo;low level\u0026rdquo; format to implement, but offers the greatest flexibility to optimize the implementation.
TextLine Format # A StreamFormat reader formats text lines from a file. The reader uses Java\u0026rsquo;s built-in InputStreamReader to decode the byte stream using various supported charset encodings. This format does not support optimized recovery from checkpoints. On recovery, it will re-read and discard the number of lines that were processed before the last checkpoint. This is due to the fact that the offsets of lines in the file cannot be tracked through the charset decoders with their internal buffering of stream input and charset decoder state.
SimpleStreamFormat Abstract Class # This is a simple version of StreamFormat for formats that are not splittable. Custom reads of Array or File can be done by implementing SimpleStreamFormat:
Java private static final class ArrayReaderFormat extends SimpleStreamFormat\u0026lt;byte[]\u0026gt; { private static final long serialVersionUID = 1L; @Override public Reader\u0026lt;byte[]\u0026gt; createReader(Configuration config, FSDataInputStream stream) throws IOException { return new ArrayReader(stream); } @Override public TypeInformation\u0026lt;byte[]\u0026gt; getProducedType() { return PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO; } } final FileSource\u0026lt;byte[]\u0026gt; source = FileSource.forRecordStreamFormat(new ArrayReaderFormat(), path).build(); An example of a SimpleStreamFormat is CsvReaderFormat. It can be initialized like this:
CsvReaderFormat\u0026lt;SomePojo\u0026gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class); FileSource\u0026lt;SomePojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); The schema for CSV parsing, in this case, is automatically derived based on the fields of the SomePojo class using the Jackson library. (Note: you might need to add @JsonPropertyOrder({field1, field2, ...}) annotation to your class definition with the fields order exactly matching those of the CSV file columns).
If you need more fine-grained control over the CSV schema or the parsing options, use the more low-level forSchema static factory method of CsvReaderFormat:
CsvReaderFormat\u0026lt;T\u0026gt; forSchema(Supplier\u0026lt;CsvMapper\u0026gt; mapperFactory, Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator, TypeInformation\u0026lt;T\u0026gt; typeInformation) Bulk Format # The BulkFormat reads and decodes batches of records at a time. Examples of bulk formats are formats like ORC or Parquet. The outer BulkFormat class acts mainly as a configuration holder and factory for the reader. The actual reading is done by the BulkFormat.Reader, which is created in the BulkFormat#createReader(Configuration, FileSourceSplit) method. If a bulk reader is created based on a checkpoint during checkpointed streaming execution, then the reader is re-created in the BulkFormat#restoreReader(Configuration, FileSourceSplit) method.
A SimpleStreamFormat can be turned into a BulkFormat by wrapping it in a StreamFormatAdapter:
BulkFormat\u0026lt;SomePojo, FileSourceSplit\u0026gt; bulkFormat = new StreamFormatAdapter\u0026lt;\u0026gt;(CsvReaderFormat.forPojo(SomePojo.class)); Customizing File Enumeration # Java /** * A FileEnumerator implementation for hive source, which generates splits based on * HiveTablePartition. */ public class HiveSourceFileEnumerator implements FileEnumerator { // reference constructor public HiveSourceFileEnumerator(...) { ... } /*** * Generates all file splits for the relevant files under the given paths. The {@code * minDesiredSplits} is an optional hint indicating how many splits would be necessary to * exploit parallelism properly. */ @Override public Collection\u0026lt;FileSourceSplit\u0026gt; enumerateSplits(Path[] paths, int minDesiredSplits) throws IOException { // createInputSplits:splitting files into fragmented collections return new ArrayList\u0026lt;\u0026gt;(createInputSplits(...)); } ... /*** * A factory to create HiveSourceFileEnumerator. */ public static class Provider implements FileEnumerator.Provider { ... @Override public FileEnumerator create() { return new HiveSourceFileEnumerator(...); } } } // use the customizing file enumeration new HiveSource\u0026lt;\u0026gt;( ..., new HiveSourceFileEnumerator.Provider( partitions != null ? partitions : Collections.emptyList(), new JobConfWrapper(jobConf)), ...); Current Limitations # Watermarking does not work very well for large backlogs of files. This is because watermarks eagerly advance within a file, and the next file might contain data later than the watermark.
For Unbounded File Sources, the enumerator currently remembers paths of all already processed files, which is a state that can, in some cases, grow rather large. There are plans to add a compressed form of tracking already processed files in the future (for example, by keeping modification timestamps below boundaries).
Behind the Scenes # If you are interested in how File Source works through the new data source API design, you may want to read this part as a reference. For details about the new data source API, check out the documentation on data sources and FLIP-27 for more descriptive discussions. File Sink # The file sink writes incoming data into buckets. Given that the incoming streams can be unbounded, data in each bucket is organized into part files of finite size. The bucketing behaviour is fully configurable with a default time-based bucketing where we start writing a new bucket every hour. This means that each resulting bucket will contain files with records received during 1 hour intervals from the stream.
Data within the bucket directories is split into part files. Each bucket will contain at least one part file for each subtask of the sink that has received data for that bucket. Additional part files will be created according to the configurable rolling policy. For Row-encoded Formats (see File Formats) the default policy rolls part files based on size, a timeout that specifies the maximum duration for which a file can be open, and a maximum inactivity timeout after which the file is closed. For Bulk-encoded Formats we roll on every checkpoint and the user can specify additional conditions based on size or time.
IMPORTANT: Checkpointing needs to be enabled when using the FileSink in STREAMING mode. Part files can only be finalized on successful checkpoints. If checkpointing is disabled, part files will forever stay in the in-progress or the pending state, and cannot be safely read by downstream systems. Format Types # The FileSink supports both row-wise and bulk encoding formats, such as Apache Parquet. These two variants come with their respective builders that can be created with the following static methods:
Row-encoded sink: FileSink.forRowFormat(basePath, rowEncoder) Bulk-encoded sink: FileSink.forBulkFormat(basePath, bulkWriterFactory) When creating either a row or a bulk encoded sink we have to specify the base path where the buckets will be stored and the encoding logic for our data.
Please check out the JavaDoc for FileSink for all the configuration options and more documentation about the implementation of the different data formats.
Row-encoded Formats # Row-encoded formats need to specify an Encoder that is used for serializing individual rows to the OutputStream of the in-progress part files.
In addition to the bucket assigner, the RowFormatBuilder allows the user to specify:
Custom RollingPolicy : Rolling policy to override the DefaultRollingPolicy bucketCheckInterval (default = 1 min) : Interval for checking time based rolling policies Basic usage for writing String elements thus looks like this:
Java import org.apache.flink.api.common.serialization.SimpleStringEncoder; import org.apache.flink.core.fs.Path; import org.apache.flink.configuration.MemorySize; import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy; import java.time.Duration; DataStream\u0026lt;String\u0026gt; input = ...; final FileSink\u0026lt;String\u0026gt; sink = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder\u0026lt;String\u0026gt;(\u0026#34;UTF-8\u0026#34;)) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(Duration.ofMinutes(15)) .withInactivityInterval(Duration.ofMinutes(5)) .withMaxPartSize(MemorySize.ofMebiBytes(1024)) .build()) .build(); input.sinkTo(sink); Scala import org.apache.flink.api.common.serialization.SimpleStringEncoder import org.apache.flink.core.fs.Path import org.apache.flink.configuration.MemorySize import org.apache.flink.connector.file.sink.FileSink import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy import java.time.Duration val input: DataStream[String] = ... val sink: FileSink[String] = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder[String](\u0026#34;UTF-8\u0026#34;)) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(Duration.ofMinutes(15)) .withInactivityInterval(Duration.ofMinutes(5)) .withMaxPartSize(MemorySize.ofMebiBytes(1024)) .build()) .build() input.sinkTo(sink) Python data_stream = ... sink = FileSink \\ .for_row_format(OUTPUT_PATH, Encoder.simple_string_encoder(\u0026#34;UTF-8\u0026#34;)) \\ .with_rolling_policy(RollingPolicy.default_rolling_policy( part_size=1024 ** 3, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)) \\ .build() data_stream.sink_to(sink) This example creates a simple sink that assigns records to the default one hour time buckets. It also specifies a rolling policy that rolls the in-progress part file on any of the following 3 conditions:
It contains at least 15 minutes worth of data It hasn\u0026rsquo;t received new records for the last 5 minutes The file size has reached 1 GB (after writing the last record) Bulk-encoded Formats # Bulk-encoded sinks are created similarly to the row-encoded ones, but instead of specifying an Encoder, we have to specify a BulkWriter.Factory . The BulkWriter logic defines how new elements are added and flushed, and how a batch of records is finalized for further encoding purposes.
Flink comes with five built-in BulkWriter factories:
ParquetWriterFactory AvroWriterFactory SequenceFileWriterFactory CompressWriterFactory OrcBulkWriterFactory Important Bulk Formats can only have a rolling policy that extends the CheckpointRollingPolicy. The latter rolls on every checkpoint. A policy can roll additionally based on size or processing time. Parquet format # Flink contains built in convenience methods for creating Parquet writer factories for Avro data. These methods and their associated documentation can be found in the AvroParquetWriters class.
For writing to other Parquet compatible data formats, users need to create the ParquetWriterFactory with a custom implementation of the ParquetBuilder interface.
To use the Parquet bulk encoder in your application you need to add the following dependency:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-parquet_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the Parquet format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. A FileSink that writes Avro data to Parquet format can be created like this:
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.avro.AvroParquetWriters; import org.apache.avro.Schema; Schema schema = ...; DataStream\u0026lt;GenericRecord\u0026gt; input = ...; final FileSink\u0026lt;GenericRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, AvroParquetWriters.forGenericRecord(schema)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.avro.AvroParquetWriters import org.apache.avro.Schema val schema: Schema = ... val input: DataStream[GenericRecord] = ... val sink: FileSink[GenericRecord] = FileSink .forBulkFormat(outputBasePath, AvroParquetWriters.forGenericRecord(schema)) .build() input.sinkTo(sink) Python schema = AvroSchema.parse_string(JSON_SCHEMA) # The element could be vanilla Python data structure matching the schema, # which is annotated with default Types.PICKLED_BYTE_ARRAY() data_stream = ... avro_type_info = GenericRecordAvroTypeInfo(schema) sink = FileSink \\ .for_bulk_format(OUTPUT_BASE_PATH, AvroParquetWriters.for_generic_record(schema)) \\ .build() # A map to indicate its Avro type info is necessary for serialization data_stream.map(lambda e: e, output_type=avro_type_info).sink_to(sink) Similarly, a FileSink that writes Protobuf data to Parquet format can be created like this:
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters; // ProtoRecord is a generated protobuf Message class. DataStream\u0026lt;ProtoRecord\u0026gt; input = ...; final FileSink\u0026lt;ProtoRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, ParquetProtoWriters.forType(ProtoRecord.class)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters // ProtoRecord is a generated protobuf Message class. val input: DataStream[ProtoRecord] = ... val sink: FileSink[ProtoRecord] = FileSink .forBulkFormat(outputBasePath, ParquetProtoWriters.forType(classOf[ProtoRecord])) .build() input.sinkTo(sink) For PyFlink users, ParquetBulkWriters could be used to create a BulkWriterFactory that writes Rows into Parquet files.
row_type = DataTypes.ROW([ DataTypes.FIELD(\u0026#39;string\u0026#39;, DataTypes.STRING()), DataTypes.FIELD(\u0026#39;int_array\u0026#39;, DataTypes.ARRAY(DataTypes.INT())) ]) sink = FileSink.for_bulk_format( OUTPUT_DIR, ParquetBulkWriters.for_row_type( row_type, hadoop_config=Configuration(), utc_timestamp=True, ) ).build() ds.sink_to(sink) Avro format # Flink also provides built-in support for writing data into Avro files. A list of convenience methods to create Avro writer factories and their associated documentation can be found in the AvroWriters class.
To use the Avro writers in your application you need to add the following dependency:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the Avro format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. A FileSink that writes data to Avro files can be created like this:
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.avro.AvroWriters; import org.apache.avro.Schema; Schema schema = ...; DataStream\u0026lt;GenericRecord\u0026gt; input = ...; final FileSink\u0026lt;GenericRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, AvroWriters.forGenericRecord(schema)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.avro.AvroWriters import org.apache.avro.Schema val schema: Schema = ... val input: DataStream[GenericRecord] = ... val sink: FileSink[GenericRecord] = FileSink .forBulkFormat(outputBasePath, AvroWriters.forGenericRecord(schema)) .build() input.sinkTo(sink) Python schema = AvroSchema.parse_string(JSON_SCHEMA) # The element could be vanilla Python data structure matching the schema, # which is annotated with default Types.PICKLED_BYTE_ARRAY() data_stream = ... avro_type_info = GenericRecordAvroTypeInfo(schema) sink = FileSink \\ .for_bulk_format(OUTPUT_BASE_PATH, AvroBulkWriters.for_generic_record(schema)) \\ .build() # A map to indicate its Avro type info is necessary for serialization data_stream.map(lambda e: e, output_type=avro_type_info).sink_to(sink) For creating customized Avro writers, e.g. enabling compression, users need to create the AvroWriterFactory with a custom implementation of the AvroBuilder interface:
Java AvroWriterFactory\u0026lt;?\u0026gt; factory = new AvroWriterFactory\u0026lt;\u0026gt;((AvroBuilder\u0026lt;Address\u0026gt;) out -\u0026gt; { Schema schema = ReflectData.get().getSchema(Address.class); DatumWriter\u0026lt;Address\u0026gt; datumWriter = new ReflectDatumWriter\u0026lt;\u0026gt;(schema); DataFileWriter\u0026lt;Address\u0026gt; dataFileWriter = new DataFileWriter\u0026lt;\u0026gt;(datumWriter); dataFileWriter.setCodec(CodecFactory.snappyCodec()); dataFileWriter.create(schema, out); return dataFileWriter; }); DataStream\u0026lt;Address\u0026gt; stream = ... stream.sinkTo(FileSink.forBulkFormat( outputBasePath, factory).build()); Scala val factory = new AvroWriterFactory[Address](new AvroBuilder[Address]() { override def createWriter(out: OutputStream): DataFileWriter[Address] = { val schema = ReflectData.get.getSchema(classOf[Address]) val datumWriter = new ReflectDatumWriter[Address](schema) val dataFileWriter = new DataFileWriter[Address](datumWriter) dataFileWriter.setCodec(CodecFactory.snappyCodec) dataFileWriter.create(schema, out) dataFileWriter } }) val stream: DataStream[Address] = ... stream.sinkTo(FileSink.forBulkFormat( outputBasePath, factory).build()); ORC Format # To enable the data to be bulk encoded in ORC format, Flink offers OrcBulkWriterFactory which takes a concrete implementation of Vectorizer.
Like any other columnar format that encodes data in bulk fashion, Flink\u0026rsquo;s OrcBulkWriter writes the input elements in batches. It uses ORC\u0026rsquo;s VectorizedRowBatch to achieve this.
Since the input element has to be transformed to a VectorizedRowBatch, users have to extend the abstract Vectorizer class and override the vectorize(T element, VectorizedRowBatch batch) method. As you can see, the method provides an instance of VectorizedRowBatch to be used directly by the users so users just have to write the logic to transform the input element to ColumnVectors and set them in the provided VectorizedRowBatch instance.
For example, if the input element is of type Person which looks like:
Java class Person { private final String name; private final int age; ... } Then a child implementation to convert the element of type Person and set them in the VectorizedRowBatch can be like:
Java import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector; import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector; import java.io.IOException; import java.io.Serializable; import java.nio.charset.StandardCharsets; public class PersonVectorizer extends Vectorizer\u0026lt;Person\u0026gt; implements Serializable {	public PersonVectorizer(String schema) { super(schema); } @Override public void vectorize(Person element, VectorizedRowBatch batch) throws IOException { BytesColumnVector nameColVector = (BytesColumnVector) batch.cols[0]; LongColumnVector ageColVector = (LongColumnVector) batch.cols[1]; int row = batch.size++; nameColVector.setVal(row, element.getName().getBytes(StandardCharsets.UTF_8)); ageColVector.vector[row] = element.getAge(); } } Scala import java.nio.charset.StandardCharsets import org.apache.hadoop.hive.ql.exec.vector.{BytesColumnVector, LongColumnVector} class PersonVectorizer(schema: String) extends Vectorizer[Person](schema) { override def vectorize(element: Person, batch: VectorizedRowBatch): Unit = { val nameColVector = batch.cols(0).asInstanceOf[BytesColumnVector] val ageColVector = batch.cols(1).asInstanceOf[LongColumnVector] nameColVector.setVal(batch.size + 1, element.getName.getBytes(StandardCharsets.UTF_8)) ageColVector.vector(batch.size + 1) = element.getAge } } To use the ORC bulk encoder in an application, users need to add the following dependency:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-orc_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! And then a FileSink that writes data in ORC format can be created like this:
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.orc.writer.OrcBulkWriterFactory; String schema = \u0026#34;struct\u0026lt;_col0:string,_col1:int\u0026gt;\u0026#34;; DataStream\u0026lt;Person\u0026gt; input = ...; final OrcBulkWriterFactory\u0026lt;Person\u0026gt; writerFactory = new OrcBulkWriterFactory\u0026lt;\u0026gt;(new PersonVectorizer(schema)); final FileSink\u0026lt;Person\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, writerFactory) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.orc.writer.OrcBulkWriterFactory val schema: String = \u0026#34;struct\u0026lt;_col0:string,_col1:int\u0026gt;\u0026#34; val input: DataStream[Person] = ... val writerFactory = new OrcBulkWriterFactory(new PersonVectorizer(schema)); val sink: FileSink[Person] = FileSink .forBulkFormat(outputBasePath, writerFactory) .build() input.sinkTo(sink) OrcBulkWriterFactory can also take Hadoop Configuration and Properties so that a custom Hadoop configuration and ORC writer properties can be provided.
Java String schema = ...; Configuration conf = ...; Properties writerProperties = new Properties(); writerProperties.setProperty(\u0026#34;orc.compress\u0026#34;, \u0026#34;LZ4\u0026#34;); // Other ORC supported properties can also be set similarly. final OrcBulkWriterFactory\u0026lt;Person\u0026gt; writerFactory = new OrcBulkWriterFactory\u0026lt;\u0026gt;( new PersonVectorizer(schema), writerProperties, conf); Scala val schema: String = ... val conf: Configuration = ... val writerProperties: Properties = new Properties() writerProperties.setProperty(\u0026#34;orc.compress\u0026#34;, \u0026#34;LZ4\u0026#34;) // Other ORC supported properties can also be set similarly. val writerFactory = new OrcBulkWriterFactory( new PersonVectorizer(schema), writerProperties, conf) The complete list of ORC writer properties can be found here.
Users who want to add user metadata to the ORC files can do so by calling addUserMetadata(...) inside the overriding vectorize(...) method.
Java public class PersonVectorizer extends Vectorizer\u0026lt;Person\u0026gt; implements Serializable {	@Override public void vectorize(Person element, VectorizedRowBatch batch) throws IOException { ... String metadataKey = ...; ByteBuffer metadataValue = ...; this.addUserMetadata(metadataKey, metadataValue); } } Scala class PersonVectorizer(schema: String) extends Vectorizer[Person](schema) { override def vectorize(element: Person, batch: VectorizedRowBatch): Unit = { ... val metadataKey: String = ... val metadataValue: ByteBuffer = ... addUserMetadata(metadataKey, metadataValue) } } For PyFlink users, OrcBulkWriters could be used to create BulkWriterFactory to write records to files in Orc format.
In order to use the ORC format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. row_type = DataTypes.ROW([ DataTypes.FIELD(\u0026#39;name\u0026#39;, DataTypes.STRING()), DataTypes.FIELD(\u0026#39;age\u0026#39;, DataTypes.INT()), ]) sink = FileSink.for_bulk_format( OUTPUT_DIR, OrcBulkWriters.for_row_type( row_type=row_type, writer_properties=Configuration(), hadoop_config=Configuration(), ) ).build() ds.sink_to(sink) Hadoop SequenceFile format # To use the SequenceFile bulk encoder in your application you need to add the following dependency:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-sequence-file\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! A simple SequenceFile writer can be created like this:
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.configuration.GlobalConfiguration; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.SequenceFile; import org.apache.hadoop.io.Text; DataStream\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = ...; Configuration hadoopConf = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration()); final FileSink\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; sink = FileSink .forBulkFormat( outputBasePath, new SequenceFileWriterFactory\u0026lt;\u0026gt;(hadoopConf, LongWritable.class, Text.class)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.configuration.GlobalConfiguration import org.apache.hadoop.conf.Configuration import org.apache.hadoop.io.LongWritable import org.apache.hadoop.io.SequenceFile import org.apache.hadoop.io.Text; val input: DataStream[(LongWritable, Text)] = ... val hadoopConf: Configuration = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration()) val sink: FileSink[(LongWritable, Text)] = FileSink .forBulkFormat( outputBasePath, new SequenceFileWriterFactory(hadoopConf, LongWritable.class, Text.class)) .build() input.sinkTo(sink) The SequenceFileWriterFactory supports additional constructor parameters to specify compression settings.
Bucket Assignment # The bucketing logic defines how the data will be structured into subdirectories inside the base output directory.
Both row and bulk formats (see File Formats) use the DateTimeBucketAssigner as the default assigner. By default the DateTimeBucketAssigner creates hourly buckets based on the system default timezone with the following format: yyyy-MM-dd--HH. Both the date format (i.e. bucket size) and timezone can be configured manually.
We can specify a custom BucketAssigner by calling .withBucketAssigner(assigner) on the format builders.
Flink comes with two built-in BucketAssigners:
DateTimeBucketAssigner : Default time based assigner BasePathBucketAssigner : Assigner that stores all part files in the base path (single global bucket) Note: PyFlink only supports DateTimeBucketAssigner and BasePathBucketAssigner. Rolling Policy # The RollingPolicy defines when a given in-progress part file will be closed and moved to the pending and later to finished state. Part files in the \u0026ldquo;finished\u0026rdquo; state are the ones that are ready for viewing and are guaranteed to contain valid data that will not be reverted in case of failure. In STREAMING mode, the Rolling Policy in combination with the checkpointing interval (pending files become finished on the next checkpoint) control how quickly part files become available for downstream readers and also the size and number of these parts. In BATCH mode, part-files become visible at the end of the job but the rolling policy can control their maximum size.
Flink comes with two built-in RollingPolicies:
DefaultRollingPolicy OnCheckpointRollingPolicy Note: PyFlink only supports DefaultRollingPolicy and OnCheckpointRollingPolicy. Part file lifecycle # In order to use the output of the FileSink in downstream systems, we need to understand the naming and lifecycle of the output files produced.
Part files can be in one of three states:
In-progress : The part file that is currently being written to is in-progress Pending : Closed (due to the specified rolling policy) in-progress files that are waiting to be committed Finished : On successful checkpoints (STREAMING) or at the end of input (BATCH) pending files transition to \u0026ldquo;Finished\u0026rdquo; Only finished files are safe to read by downstream systems as those are guaranteed to not be modified later.
Each writer subtask will have a single in-progress part file at any given time for every active bucket, but there can be several pending and finished files.
Part file example
To better understand the lifecycle of these files let\u0026rsquo;s look at a simple example with 2 sink subtasks:
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 └── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575 When the part file part-81fc4980-a6af-41c8-9937-9939408a734b-0 is rolled (let\u0026rsquo;s say it becomes too large), it becomes pending but it is not renamed. The sink then opens a new part file: part-81fc4980-a6af-41c8-9937-9939408a734b-1:
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 As part-81fc4980-a6af-41c8-9937-9939408a734b-0 is now pending completion, after the next successful checkpoint, it is finalized:
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 New buckets are created as dictated by the bucketing policy, and this doesn\u0026rsquo;t affect currently in-progress files:
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 └── 2019-08-25--13 └── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.2b475fec-1482-4dea-9946-eb4353b475f1 Old buckets can still receive new records as the bucketing policy is evaluated on a per-record basis.
Part file configuration # Finished files can be distinguished from the in-progress ones by their naming scheme only.
By default, the file naming strategy is as follows:
In-progress / Pending: part-\u0026lt;uid\u0026gt;-\u0026lt;partFileIndex\u0026gt;.inprogress.uid Finished: part-\u0026lt;uid\u0026gt;-\u0026lt;partFileIndex\u0026gt; where uid is a random id assigned to a subtask of the sink when the subtask is instantiated. This uid is not fault-tolerant so it is regenerated when the subtask recovers from a failure. Flink allows the user to specify a prefix and/or a suffix for his/her part files. This can be done using an OutputFileConfig. For example for a prefix \u0026ldquo;prefix\u0026rdquo; and a suffix \u0026ldquo;.ext\u0026rdquo; the sink will create the following files:
└── 2019-08-25--12 ├── prefix-4005733d-a830-4323-8291-8866de98b582-0.ext ├── prefix-4005733d-a830-4323-8291-8866de98b582-1.ext.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── prefix-81fc4980-a6af-41c8-9937-9939408a734b-0.ext └── prefix-81fc4980-a6af-41c8-9937-9939408a734b-1.ext.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 The user can specify an OutputFileConfig in the following way:
Java OutputFileConfig config = OutputFileConfig .builder() .withPartPrefix(\u0026#34;prefix\u0026#34;) .withPartSuffix(\u0026#34;.ext\u0026#34;) .build(); FileSink\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; sink = FileSink .forRowFormat((new Path(outputPath), new SimpleStringEncoder\u0026lt;\u0026gt;(\u0026#34;UTF-8\u0026#34;)) .withBucketAssigner(new KeyBucketAssigner()) .withRollingPolicy(OnCheckpointRollingPolicy.build()) .withOutputFileConfig(config) .build(); Scala val config = OutputFileConfig .builder() .withPartPrefix(\u0026#34;prefix\u0026#34;) .withPartSuffix(\u0026#34;.ext\u0026#34;) .build() val sink = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder[String](\u0026#34;UTF-8\u0026#34;)) .withBucketAssigner(new KeyBucketAssigner()) .withRollingPolicy(OnCheckpointRollingPolicy.build()) .withOutputFileConfig(config) .build() Python config = OutputFileConfig \\ .builder() \\ .with_part_prefix(\u0026#34;prefix\u0026#34;) \\ .with_part_suffix(\u0026#34;.ext\u0026#34;) \\ .build() sink = FileSink \\ .for_row_format(OUTPUT_PATH, Encoder.simple_string_encoder(\u0026#34;UTF-8\u0026#34;)) \\ .with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()) \\ .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \\ .with_output_file_config(config) \\ .build() Compaction # Since version 1.15 FileSink supports compaction of the pending files, which allows the application to have smaller checkpoint interval without generating a lot of small files, especially when using the bulk encoded formats that have to rolling on taking checkpoints.
Compaction could be enabled with
Java FileSink\u0026lt;Integer\u0026gt; fileSink= FileSink.forRowFormat(new Path(path),new SimpleStringEncoder\u0026lt;Integer\u0026gt;()) .enableCompact( FileCompactStrategy.Builder.newBuilder() .setSizeThreshold(1024) .enableCompactionOnCheckpoint(5) .build(), new RecordWiseFileCompactor\u0026lt;\u0026gt;( new DecoderBasedReader.Factory\u0026lt;\u0026gt;(SimpleStringDecoder::new))) .build(); Scala val fileSink: FileSink[Integer] = FileSink.forRowFormat(new Path(path), new SimpleStringEncoder[Integer]()) .enableCompact( FileCompactStrategy.Builder.newBuilder() .setSizeThreshold(1024) .enableCompactionOnCheckpoint(5) .build(), new RecordWiseFileCompactor( new DecoderBasedReader.Factory(() =\u0026gt; new SimpleStringDecoder))) .build() Python file_sink = FileSink \\ .for_row_format(PATH, Encoder.simple_string_encoder()) \\ .enable_compact( FileCompactStrategy.builder() .set_size_threshold(1024) .enable_compaction_on_checkpoint(5) .build(), FileCompactor.concat_file_compactor()) \\ .build() Once enabled, the compaction happens between the files become pending and get committed. The pending files will be first committed to temporary files whose path starts with .. Then these files will be compacted according to the strategy by the compactor specified by the users, and the new compacted pending files will be generated. Then these pending files will be emitted to the committer to be committed to the formal files. After that, the source files will be removed.
When enabling compaction, you need to specify the FileCompactStrategy and the FileCompactor .
The FileCompactStrategy specifies when and which files get compacted. Currently, there are two parallel conditions: the target file size and the number of checkpoints get passed. Once the total size of the cached files has reached the size threshold or the number of checkpoints since the last compaction has reached the specified number, the cached files will be scheduled to compact.
The FileCompactor specifies how to compact the give list of Path and write the result file. It could be classified into two types according to how to write the file:
OutputStreamBasedFileCompactor : The users can write the compacted results into an output stream. This is useful when the users don\u0026rsquo;t want to or can\u0026rsquo;t read records from the input files. An example is the ConcatFileCompactor that concats the list of files directly. RecordWiseFileCompactor : The compactor can read records one-by-one from the input files and write into the result file similar to the FileWriter. An example is the RecordWiseFileCompactor that reads records from the source files and then writes them with the CompactingFileWriter. Users need to specify how to read records from the source files. Important Note 1 Once the compaction is enabled, you must explicitly call disableCompact when building the FileSink if you want to disable compaction.
Important Note 2 When the compaction is enabled, the written files need to wait for longer time before they get visible.
Note: PyFlink only supports ConcatFileCompactor and IdenticalFileCompactor. Important Considerations # General # Important Note 1: When using Hadoop \u0026lt; 2.7, please use the OnCheckpointRollingPolicy which rolls part files on every checkpoint. The reason is that if part files \u0026ldquo;traverse\u0026rdquo; the checkpoint interval, then, upon recovery from a failure the FileSink may use the truncate() method of the filesystem to discard uncommitted data from the in-progress file. This method is not supported by pre-2.7 Hadoop versions and Flink will throw an exception.
Important Note 2: Given that Flink sinks and UDFs in general do not differentiate between normal job termination (e.g. finite input stream) and termination due to failure, upon normal termination of a job, the last in-progress files will not be transitioned to the \u0026ldquo;finished\u0026rdquo; state.
Important Note 3: Flink and the FileSink never overwrites committed data. Given this, when trying to restore from an old checkpoint/savepoint which assumes an in-progress file which was committed by subsequent successful checkpoints, the FileSink will refuse to resume and will throw an exception as it cannot locate the in-progress file.
Important Note 4: Currently, the FileSink only supports four filesystems: HDFS, S3, OSS, and Local. Flink will throw an exception when using an unsupported filesystem at runtime.
BATCH-specific # Important Note 1: Although the Writer is executed with the user-specified parallelism, the Committer is executed with parallelism equal to 1.
Important Note 2: Pending files are committed, i.e. transition to Finished state, after the whole input has been processed.
Important Note 3: When High-Availability is activated, if a JobManager failure happens while the Committers are committing, then we may have duplicates. This is going to be fixed in
future Flink versions (see progress in FLIP-147).
S3-specific # Important Note 1: For S3, the FileSink supports only the Hadoop-based FileSystem implementation, not the implementation based on Presto. In case your job uses the FileSink to write to S3 but you want to use the Presto-based one for checkpointing, it is advised to use explicitly \u0026ldquo;s3a://\u0026rdquo; (for Hadoop) as the scheme for the target path of the sink and \u0026ldquo;s3p://\u0026rdquo; for checkpointing (for Presto). Using \u0026ldquo;s3://\u0026rdquo; for both the sink and checkpointing may lead to unpredictable behavior, as both implementations \u0026ldquo;listen\u0026rdquo; to that scheme.
Important Note 2: To guarantee exactly-once semantics while being efficient, the FileSink uses the Multi-part Upload feature of S3 (MPU from now on). This feature allows to upload files in independent chunks (thus the \u0026ldquo;multi-part\u0026rdquo;) which can be combined into the original file when all the parts of the MPU are successfully uploaded. For inactive MPUs, S3 supports a bucket lifecycle rule that the user can use to abort multipart uploads that don\u0026rsquo;t complete within a specified number of days after being initiated. This implies that if you set this rule aggressively and take a savepoint with some part-files being not fully uploaded, their associated MPUs may time-out before the job is restarted. This will result in your job not being able to restore from that savepoint as the pending part-files are no longer there and Flink will fail with an exception as it tries to fetch them and fails.
OSS-specific # Important Note: To guarantee exactly-once semantics while being efficient, the FileSink also uses the Multi-part Upload feature of OSS(similar with S3).
Back to top
`}),e.add({id:176,href:"/flink/flink-docs-master/docs/try-flink/flink-operations-playground/",title:"Flink Operations Playground",section:"Try Flink",content:` Flink Operations Playground # There are many ways to deploy and operate Apache Flink in various environments. Regardless of this variety, the fundamental building blocks of a Flink Cluster remain the same, and similar operational principles apply.
In this playground, you will learn how to manage and run Flink Jobs. You will see how to deploy and monitor an application, experience how Flink recovers from Job failure, and perform everyday operational tasks like upgrades and rescaling.
Attention: The Apache Flink Docker images used for this playground are only available for released versions of Apache Flink.
Since you are currently looking at the latest SNAPSHOT version of the documentation, all version references below will not work. Please switch the documentation to the latest released version via the release picker which you find on the left side below the menu.
Anatomy of this Playground # This playground consists of a long living Flink Session Cluster and a Kafka Cluster.
A Flink Cluster always consists of a JobManager and one or more Flink TaskManagers. The JobManager is responsible for handling Job submissions, the supervision of Jobs as well as resource management. The Flink TaskManagers are the worker processes and are responsible for the execution of the actual Tasks which make up a Flink Job. In this playground you will start with a single TaskManager, but scale out to more TaskManagers later. Additionally, this playground comes with a dedicated client container, which we use to submit the Flink Job initially and to perform various operational tasks later on. The client container is not needed by the Flink Cluster itself but only included for ease of use.
The Kafka Cluster consists of a Zookeeper server and a Kafka Broker.
When the playground is started a Flink Job called Flink Event Count will be submitted to the JobManager. Additionally, two Kafka Topics input and output are created.
The Job consumes ClickEvents from the input topic, each with a timestamp and a page. The events are then keyed by page and counted in 15 second windows. The results are written to the output topic.
There are six different pages and we generate 1000 click events per page and 15 seconds. Hence, the output of the Flink job should show 1000 views per page and window.
Back to top
Starting the Playground # The playground environment is set up in just a few steps. We will walk you through the necessary commands and show how to validate that everything is running correctly.
We assume that you have Docker (1.12+) and docker-compose (2.1+) installed on your machine.
The required configuration files are available in the flink-playgrounds repository. First checkout the code and build the docker image:
git clone https://github.com/apache/flink-playgrounds.git cd flink-playgrounds/operations-playground docker-compose build Then before starting the playground, create the checkpoint and savepoint directories on the Docker host machine (these volumes are mounted by the jobmanager and taskmanager, as specified in docker-compose.yaml):
mkdir -p /tmp/flink-checkpoints-directory mkdir -p /tmp/flink-savepoints-directory Then start the playground:
docker-compose up -d Afterwards, you can inspect the running Docker containers with the following command:
docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------------------------------------------- operations-playground_clickevent-generator_1 /docker-entrypoint.sh java ... Up 6123/tcp, 8081/tcp operations-playground_client_1 /docker-entrypoint.sh flin ... Exit 0 operations-playground_jobmanager_1 /docker-entrypoint.sh jobm ... Up 6123/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp operations-playground_kafka_1 start-kafka.sh Up 0.0.0.0:9094-\u0026gt;9094/tcp operations-playground_taskmanager_1 /docker-entrypoint.sh task ... Up 6123/tcp, 8081/tcp operations-playground_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 2181/tcp, 22/tcp, 2888/tcp, 3888/tcp This indicates that the client container has successfully submitted the Flink Job (Exit 0) and all cluster components as well as the data generator are running (Up).
You can stop the playground environment by calling:
docker-compose down -v Entering the Playground # There are many things you can try and check out in this playground. In the following two sections we will show you how to interact with the Flink Cluster and demonstrate some of Flink\u0026rsquo;s key features.
Flink WebUI # The most natural starting point to observe your Flink Cluster is the WebUI exposed under http://localhost:8081. If everything went well, you\u0026rsquo;ll see that the cluster initially consists of one TaskManager and executes a Job called Click Event Count.
The Flink WebUI contains a lot of useful and interesting information about your Flink Cluster and its Jobs (JobGraph, Metrics, Checkpointing Statistics, TaskManager Status,\u0026hellip;).
Logs # JobManager
The JobManager logs can be tailed via docker-compose.
docker-compose logs -f jobmanager After the initial startup you should mainly see log messages for every checkpoint completion.
TaskManager
The TaskManager log can be tailed in the same way.
docker-compose logs -f taskmanager After the initial startup you should mainly see log messages for every checkpoint completion.
Flink CLI # The Flink CLI can be used from within the client container. For example, to print the help message of the Flink CLI you can run
docker-compose run --no-deps client flink --help Flink REST API # The Flink REST API is exposed via localhost:8081 on the host or via jobmanager:8081 from the client container, e.g. to list all currently running jobs, you can run:
curl localhost:8081/jobs Note: If the curl command is not available on your machine, you can run it from the client container (similar to the Flink CLI):
docker-compose run --no-deps client curl jobmanager:8081/jobs Kafka Topics # You can look at the records that are written to the Kafka Topics by running
//input topic (1000 records/s) docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic input //output topic (24 records/min) docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Back to top
Time to Play! # Now that you learned how to interact with Flink and the Docker containers, let\u0026rsquo;s have a look at some common operational tasks that you can try out on our playground. All of these tasks are independent of each other, i.e. you can perform them in any order. Most tasks can be executed via the CLI and the REST API.
Listing Running Jobs # CLI Command
docker-compose run --no-deps client flink list Expected Output
Waiting for response... ------------------ Running/Restarting Jobs ------------------- 16.07.2019 16:37:55 : \u0026lt;job-id\u0026gt; : Click Event Count (RUNNING) -------------------------------------------------------------- No scheduled jobs. REST API Request
curl localhost:8081/jobs Expected Response (pretty-printed)
{ \u0026#34;jobs\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34; } ] } The JobID is assigned to a Job upon submission and is needed to perform actions on the Job via the CLI or REST API.
Observing Failure \u0026amp; Recovery # Flink provides exactly-once processing guarantees under (partial) failure. In this playground you can observe and - to some extent - verify this behavior.
Step 1: Observing the Output # As described above, the events in this playground are generated such that each window contains exactly one thousand records. So, in order to verify that Flink successfully recovers from a TaskManager failure without data loss or duplication you can tail the output topic and check that - after recovery - all windows are present and the count is correct.
For this, start reading from the output topic and leave this command running until after recovery (Step 3).
docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Step 2: Introducing a Fault # In order to simulate a partial failure you can kill a TaskManager. In a production setup, this could correspond to a loss of the TaskManager process, the TaskManager machine or simply a transient exception being thrown from the framework or user code (e.g. due to the temporary unavailability of an external resource).
docker-compose kill taskmanager After a few seconds, the JobManager will notice the loss of the TaskManager, cancel the affected Job, and immediately resubmit it for recovery. When the Job gets restarted, its tasks remain in the SCHEDULED state, which is indicated by the purple colored squares (see screenshot below).
Note: Even though the tasks of the job are in SCHEDULED state and not RUNNING yet, the overall status of a Job is shown as RUNNING. At this point, the tasks of the Job cannot move from the SCHEDULED state to RUNNING because there are no resources (TaskSlots provided by TaskManagers) to the run the tasks. Until a new TaskManager becomes available, the Job will go through a cycle of cancellations and resubmissions.
In the meantime, the data generator keeps pushing ClickEvents into the input topic. This is similar to a real production setup where data is produced while the Job to process it is down.
Step 3: Recovery # Once you restart the TaskManager, it reconnects to the JobManager.
docker-compose up -d taskmanager When the JobManager is notified about the new TaskManager, it schedules the tasks of the recovering Job to the newly available TaskSlots. Upon restart, the tasks recover their state from the last successful checkpoint that was taken before the failure and switch to the RUNNING state.
The Job will quickly process the full backlog of input events (accumulated during the outage) from Kafka and produce output at a much higher rate (\u0026gt; 24 records/minute) until it reaches the head of the stream. In the output you will see that all keys (pages) are present for all time windows and that every count is exactly one thousand. Since we are using the FlinkKafkaProducer in its \u0026ldquo;at-least-once\u0026rdquo; mode, there is a chance that you will see some duplicate output records.
Note: Most production setups rely on a resource manager (Kubernetes, Yarn) to automatically restart failed processes. Upgrading \u0026amp; Rescaling a Job # Upgrading a Flink Job always involves two steps: First, the Flink Job is gracefully stopped with a Savepoint. A Savepoint is a consistent snapshot of the complete application state at a well-defined, globally consistent point in time (similar to a checkpoint). Second, the upgraded Flink Job is started from the Savepoint. In this context \u0026ldquo;upgrade\u0026rdquo; can mean different things including the following:
An upgrade to the configuration (incl. the parallelism of the Job) An upgrade to the topology of the Job (added/removed Operators) An upgrade to the user-defined functions of the Job Before starting with the upgrade you might want to start tailing the output topic, in order to observe that no data is lost or corrupted in the course the upgrade.
docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Step 1: Stopping the Job # To gracefully stop the Job, you need to use the \u0026ldquo;stop\u0026rdquo; command of either the CLI or the REST API. For this you will need the JobID of the Job, which you can obtain by listing all running Jobs or from the WebUI. With the JobID you can proceed to stopping the Job:
CLI Command
docker-compose run --no-deps client flink stop \u0026lt;job-id\u0026gt; Expected Output
Suspending job \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; with a savepoint. Savepoint completed. Path: file:\u0026lt;savepoint-path\u0026gt; The Savepoint has been stored to the state.savepoints.dir configured in the flink-conf.yaml, which is mounted under /tmp/flink-savepoints-directory/ on your local machine. You will need the path to this Savepoint in the next step.
REST API Request
# triggering stop curl -X POST localhost:8081/jobs/\u0026lt;job-id\u0026gt;/stop -d \u0026#39;{\u0026#34;drain\u0026#34;: false}\u0026#39; Expected Response (pretty-printed)
{ \u0026#34;request-id\u0026#34;: \u0026#34;\u0026lt;trigger-id\u0026gt;\u0026#34; } Request
# check status of stop action and retrieve savepoint path curl localhost:8081/jobs/\u0026lt;job-id\u0026gt;/savepoints/\u0026lt;trigger-id\u0026gt; Expected Response (pretty-printed)
{ \u0026#34;status\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;COMPLETED\u0026#34; }, \u0026#34;operation\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34; } } Step 2a: Restart Job without Changes # You can now restart the upgraded Job from this Savepoint. For simplicity, you can start by restarting it without any changes.
CLI Command
docker-compose run --no-deps client flink run -s \u0026lt;savepoint-path\u0026gt; \\ -d /opt/ClickCountJob.jar \\ --bootstrap.servers kafka:9092 --checkpointing --event-time Expected Output
Job has been submitted with JobID \u0026lt;job-id\u0026gt; REST API Request
# Uploading the JAR from the Client container docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\ -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload Expected Response (pretty-printed)
{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Request
# Submitting the Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\ -d \u0026#39;{\u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; Expected Response (pretty-printed)
{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } Once the Job is RUNNING again, you will see in the output Topic that records are produced at a higher rate while the Job is processing the backlog accumulated during the outage. Additionally, you will see that no data was lost during the upgrade: all windows are present with a count of exactly one thousand.
Step 2b: Restart Job with a Different Parallelism (Rescaling) # Alternatively, you could also rescale the Job from this Savepoint by passing a different parallelism during resubmission.
CLI Command
docker-compose run --no-deps client flink run -p 3 -s \u0026lt;savepoint-path\u0026gt; \\ -d /opt/ClickCountJob.jar \\ --bootstrap.servers kafka:9092 --checkpointing --event-time Expected Output
Starting execution of program Job has been submitted with JobID \u0026lt;job-id\u0026gt; REST API Request
# Uploading the JAR from the Client container docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\ -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload Expected Response (pretty-printed)
{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Request
# Submitting the Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\ -d \u0026#39;{\u0026#34;parallelism\u0026#34;: 3, \u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; Expected Response (pretty-printed
{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } Now, the Job has been resubmitted, but it will not start as there are not enough TaskSlots to execute it with the increased parallelism (2 available, 3 needed). With
docker-compose scale taskmanager=2 you can add a second TaskManager with two TaskSlots to the Flink Cluster, which will automatically register with the JobManager. Shortly after adding the TaskManager the Job should start running again.
Once the Job is \u0026ldquo;RUNNING\u0026rdquo; again, you will see in the output Topic that no data was lost during rescaling: all windows are present with a count of exactly one thousand.
Querying the Metrics of a Job # The JobManager exposes system and user metrics via its REST API.
The endpoint depends on the scope of these metrics. Metrics scoped to a Job can be listed via jobs/\u0026lt;job-id\u0026gt;/metrics. The actual value of a metric can be queried via the get query parameter.
Request
curl \u0026#34;localhost:8081/jobs/\u0026lt;jod-id\u0026gt;/metrics?get=lastCheckpointSize\u0026#34; Expected Response (pretty-printed; no placeholders)
[ { \u0026#34;id\u0026#34;: \u0026#34;lastCheckpointSize\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;9378\u0026#34; } ] The REST API can not only be used to query metrics, but you can also retrieve detailed information about the status of a running Job.
Request
# find the vertex-id of the vertex of interest curl localhost:8081/jobs/\u0026lt;jod-id\u0026gt; Expected Response (pretty-printed)
{ \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;isStoppable\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066026, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374793, \u0026#34;now\u0026#34;: 1564467440819, \u0026#34;timestamps\u0026#34;: { \u0026#34;CREATED\u0026#34;: 1564467066026, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;SUSPENDED\u0026#34;: 0, \u0026#34;FAILING\u0026#34;: 0, \u0026#34;CANCELLING\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 1564467066126, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;RESTARTING\u0026#34;: 0 }, \u0026#34;vertices\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066423, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374396, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 0, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 5033461, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 0, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 166351, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066469, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374350, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 5085332, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 316, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 166305, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 6, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066476, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374343, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 20668, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 0, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 6, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 0, \u0026#34;write-records-complete\u0026#34;: true } } ], \u0026#34;status-counts\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 4, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;plan\u0026#34;: { \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STREAMING\u0026#34;, \u0026#34;nodes\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;FORWARD\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;HASH\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;optimizer_properties\u0026#34;: {} } ] } } Please consult the REST API reference for a complete list of possible queries including how to query metrics of different scopes (e.g. TaskManager metrics);
Back to top
Variants # You might have noticed that the Click Event Count application was always started with --checkpointing and --event-time program arguments. By omitting these in the command of the client container in the docker-compose.yaml, you can change the behavior of the Job.
--checkpointing enables checkpoint, which is Flink\u0026rsquo;s fault-tolerance mechanism. If you run without it and go through failure and recovery, you should will see that data is actually lost.
--event-time enables event time semantics for your Job. When disabled, the Job will assign events to windows based on the wall-clock time instead of the timestamp of the ClickEvent. Consequently, the number of events per window will not be exactly one thousand anymore.
The Click Event Count application also has another option, turned off by default, that you can enable to explore the behavior of this job under backpressure. You can add this option in the command of the client container in docker-compose.yaml.
--backpressure adds an additional operator into the middle of the job that causes severe backpressure during even-numbered minutes (e.g., during 10:12, but not during 10:13). This can be observed by inspecting various network metrics such as outputQueueLength and outPoolUsage, and/or by using the backpressure monitoring available in the WebUI. `}),e.add({id:177,href:"/flink/flink-docs-master/docs/libs/gelly/graph_generators/",title:"Graph Generators",section:"Graphs",content:` Graph Generators # Gelly provides a collection of scalable graph generators. Each generator is
parallelizable, in order to create large datasets scale-free, generating the same graph regardless of parallelism thrifty, using as few operators as possible Graph generators are configured using the builder pattern. The parallelism of generator operators can be set explicitly by calling setParallelism(parallelism). Lowering the parallelism will reduce the allocation of memory and network buffers.
Graph-specific configuration must be called first, then configuration common to all generators, and lastly the call to generate(). The following example configures a grid graph with two dimensions, configures the parallelism, and generates the graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); boolean wrapEndpoints = false; int parallelism = 4; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new GridGraph(env) .addDimension(2, wrapEndpoints) .addDimension(4, wrapEndpoints) .setParallelism(parallelism) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.GridGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment wrapEndpoints = false val parallelism = 4 val graph = new GridGraph(env.getJavaEnv).addDimension(2, wrapEndpoints).addDimension(4, wrapEndpoints).setParallelism(parallelism).generate() Circulant Graph # A circulant graph is an oriented graph configured with one or more contiguous ranges of offsets. Edges connect integer vertex IDs whose difference equals a configured offset. The circulant graph with no offsets is the empty graph and the graph with the maximum range is the complete graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CirculantGraph(env, vertexCount) .addRange(1, 2) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CirculantGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CirculantGraph(env.getJavaEnv, vertexCount).addRange(1, 2).generate() Complete Graph # An undirected graph connecting every distinct pair of vertices.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CompleteGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CompleteGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CompleteGraph(env.getJavaEnv, vertexCount).generate() Cycle Graph # An undirected graph where the set of edges form a single cycle by connecting each vertex to two adjacent vertices in a chained loop.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CycleGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CycleGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CycleGraph(env.getJavaEnv, vertexCount).generate() Echo Graph # An echo graph is a circulant graph with n vertices defined by the width of a single range of offsets centered at n/2. A vertex is connected to \u0026lsquo;far\u0026rsquo; vertices, which connect to \u0026rsquo;near\u0026rsquo; vertices, which connect to \u0026lsquo;far\u0026rsquo; vertices, \u0026hellip;.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; long vertexDegree = 2; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new EchoGraph(env, vertexCount, vertexDegree) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.EchoGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val vertexDegree = 2 val graph = new EchoGraph(env.getJavaEnv, vertexCount, vertexDegree).generate() Empty Graph # A graph containing no edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new EmptyGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.EmptyGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new EmptyGraph(env.getJavaEnv, vertexCount).generate() Grid Graph # An undirected graph connecting vertices in a regular tiling in one or more dimensions. Each dimension is configured separately. When the dimension size is at least three the endpoints are optionally connected by setting wrapEndpoints. Changing the following example to addDimension(4, true) would connect 0 to 3 and 4 to 7.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); boolean wrapEndpoints = false; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new GridGraph(env) .addDimension(2, wrapEndpoints) .addDimension(4, wrapEndpoints) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.GridGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val wrapEndpoints = false val graph = new GridGraph(env.getJavaEnv).addDimension(2, wrapEndpoints).addDimension(4, wrapEndpoints).generate() Hypercube Graph # An undirected graph where edges form an n-dimensional hypercube. Each vertex in a hypercube connects to one other vertex in each dimension.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long dimensions = 3; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new HypercubeGraph(env, dimensions) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.HypercubeGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dimensions = 3 val graph = new HypercubeGraph(env.getJavaEnv, dimensions).generate() Path Graph # An undirected graph where the set of edges form a single path by connecting two endpoint vertices with degree 1 and all midpoint vertices with degree 2. A path graph can be formed by removing a single edge from a cycle graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new PathGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.PathGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new PathGraph(env.getJavaEnv, vertexCount).generate() RMat Graph # A directed power-law multigraph generated using the Recursive Matrix (R-Mat) model.
RMat is a stochastic generator configured with a source of randomness implementing the RandomGenerableFactory interface. Provided implementations are JDKRandomGeneratorFactory and MersenneTwisterFactory. These generate an initial sequence of random values which are then used as seeds for generating the edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); RandomGenerableFactory\u0026lt;JDKRandomGenerator\u0026gt; rnd = new JDKRandomGeneratorFactory(); int vertexCount = 1 \u0026lt;\u0026lt; scale; int edgeCount = edgeFactor * vertexCount; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new RMatGraph\u0026lt;\u0026gt;(env, rnd, vertexCount, edgeCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.RMatGraph val env = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 1 \u0026lt;\u0026lt; scale val edgeCount = edgeFactor * vertexCount val graph = new RMatGraph(env.getJavaEnv, rnd, vertexCount, edgeCount).generate() The default RMat constants can be overridden as shown in the following example. The constants define the interdependence of bits from each generated edge\u0026rsquo;s source and target labels. The RMat noise can be enabled and progressively perturbs the constants while generating each edge.
The RMat generator can be configured to produce a simple graph by removing self-loops and duplicate edges. Symmetrization is performed either by a \u0026ldquo;clip-and-flip\u0026rdquo; throwing away the half matrix above the diagonal or a full \u0026ldquo;flip\u0026rdquo; preserving and mirroring all edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); RandomGenerableFactory\u0026lt;JDKRandomGenerator\u0026gt; rnd = new JDKRandomGeneratorFactory(); int vertexCount = 1 \u0026lt;\u0026lt; scale; int edgeCount = edgeFactor * vertexCount; boolean clipAndFlip = false; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new RMatGraph\u0026lt;\u0026gt;(env, rnd, vertexCount, edgeCount) .setConstants(0.57f, 0.19f, 0.19f) .setNoise(true, 0.10f) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.RMatGraph val env = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 1 \u0026lt;\u0026lt; scale val edgeCount = edgeFactor * vertexCount clipAndFlip = false val graph = new RMatGraph(env.getJavaEnv, rnd, vertexCount, edgeCount).setConstants(0.57f, 0.19f, 0.19f).setNoise(true, 0.10f).generate() Singleton Edge Graph # An undirected graph containing isolated two-paths where every vertex has degree 1.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexPairCount = 4; // note: configured with the number of vertex pairs Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new SingletonEdgeGraph(env, vertexPairCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.SingletonEdgeGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexPairCount = 4 // note: configured with the number of vertex pairs val graph = new SingletonEdgeGraph(env.getJavaEnv, vertexPairCount).generate() Star Graph # An undirected graph containing a single central vertex connected to all other leaf vertices.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 6; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new StarGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.StarGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 6 val graph = new StarGraph(env.getJavaEnv, vertexCount).generate() Back to top
`}),e.add({id:178,href:"/flink/flink-docs-master/docs/deployment/ha/",title:"High Availability",section:"Deployment",content:" "}),e.add({id:179,href:"/flink/flink-docs-master/docs/connectors/table/jdbc/",title:"JDBC",section:"Table API Connectors",content:` JDBC SQL Connector # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append \u0026amp; Upsert Mode
The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases.
The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn\u0026rsquo;t support to consume UPDATE/DELETE messages.
Dependencies # In order to use the JDBC connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-jdbc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The JDBC connector is not part of the binary distribution. See how to link with it for cluster execution here.
A driver dependency is also required to connect to a specified database. Here are drivers currently supported:
Driver Group Id Artifact Id JAR MySQL mysql mysql-connector-java Download Oracle com.oracle.database.jdbc ojdbc8 Download PostgreSQL org.postgresql postgresql Download Derby org.apache.derby derby Download JDBC connector and drivers are not part of Flink\u0026rsquo;s binary distribution. See how to link with them for cluster execution here.
How to create a JDBC table # The JDBC table can be defined as following:
-- register a MySQL table \u0026#39;users\u0026#39; in Flink SQL CREATE TABLE MyUserTable ( id BIGINT, name STRING, age INT, status BOOLEAN, PRIMARY KEY (id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;users\u0026#39; ); -- write data into the JDBC table from the other table \u0026#34;T\u0026#34; INSERT INTO MyUserTable SELECT id, name, age, status FROM T; -- scan data from the JDBC table SELECT id, name, age, status FROM MyUserTable; -- temporal join the JDBC table as a dimension table SELECT * FROM myTopic LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = MyUserTable.id; Connector Options # Option Required Forwarded Default Type Description connector required no (none) String Specify what connector to use, here should be 'jdbc'. url required yes (none) String The JDBC database url. table-name required yes (none) String The name of JDBC table to connect. driver optional yes (none) String The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL. username optional yes (none) String The JDBC user name. 'username' and 'password' must both be specified if any of them is specified. password optional yes (none) String The JDBC password. connection.max-retry-timeout optional yes 60s Duration Maximum timeout between retries. The timeout should be in second granularity and shouldn't be smaller than 1 second. scan.partition.column optional no (none) String The column name used for partitioning the input. See the following Partitioned Scan section for more details. scan.partition.num optional no (none) Integer The number of partitions. scan.partition.lower-bound optional no (none) Integer The smallest value of the first partition. scan.partition.upper-bound optional no (none) Integer The largest value of the last partition. scan.fetch-size optional yes 0 Integer The number of rows that should be fetched from the database when reading per round trip. If the value specified is zero, then the hint is ignored. scan.auto-commit optional yes true Boolean Sets the auto-commit flag on the JDBC driver, which determines whether each statement is committed in a transaction automatically. Some JDBC drivers, specifically Postgres, may require this to be set to false in order to stream results. lookup.cache.max-rows optional yes (none) Integer The max number of rows of lookup cache, over this value, the oldest rows will be expired. Lookup cache is disabled by default. See the following Lookup Cache section for more details. lookup.cache.ttl optional yes (none) Duration The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired. Lookup cache is disabled by default. See the following Lookup Cache section for more details. lookup.cache.caching-missing-key optional yes true Boolean Flag to cache missing key, true by default lookup.max-retries optional yes 3 Integer The max retry times if lookup database failed. sink.buffer-flush.max-rows optional yes 100 Integer The max size of buffered records before flush. Can be set to zero to disable it. sink.buffer-flush.interval optional yes 1s Duration The flush interval mills, over this time, asynchronous threads will flush data. Can be set to '0' to disable it. Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions. sink.max-retries optional yes 3 Integer The max retry times if writing records to database failed. sink.parallelism optional no (none) Integer Defines the parallelism of the JDBC sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator. Features # Key handling # Flink uses the primary key that was defined in DDL when writing data to external databases. The connector operates in upsert mode if the primary key was defined, otherwise, the connector operates in append mode.
In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way. To guarantee the output result is as expected, it\u0026rsquo;s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table. In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database.
See CREATE TABLE DDL for more details about PRIMARY KEY syntax.
Partitioned Scan # To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table.
All the following scan partition options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple tasks. The scan.partition.column must be a numeric, date, or timestamp column from the table in question. Notice that scan.partition.lower-bound and scan.partition.upper-bound are used to decide the partition stride and filter the rows in table. If it is a batch job, it also doable to get the max and min value first before submitting the flink job.
scan.partition.column: The column name used for partitioning the input. scan.partition.num: The number of partitions. scan.partition.lower-bound: The smallest value of the first partition. scan.partition.upper-bound: The largest value of the last partition. Lookup Cache # JDBC connector can be used in temporal join as a lookup source (aka. dimension table). Currently, only sync lookup mode is supported.
By default, lookup cache is not enabled. You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl.
The lookup cache is used to improve performance of temporal join the JDBC connector. By default, lookup cache is not enabled, so all the requests are sent to external database. When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache. Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned. The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl. The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database. So this is a balance between throughput and correctness.
By default, flink will cache the empty query result for a Primary key, you can toggle the behaviour by setting lookup.cache.caching-missing-key to false.
Idempotent Writes # JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL. Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence.
If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery. The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed.
Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable.
As there is no standard syntax for upsert, the following table describes the database-specific DML that is used.
Database Upsert Grammar MySQL INSERT .. ON DUPLICATE KEY UPDATE .. Oracle MERGE INTO .. USING (..) ON (..) WHEN MATCHED THEN UPDATE SET (..) WHEN NOT MATCHED THEN INSERT (..) VALUES (..) PostgreSQL INSERT .. ON CONFLICT .. DO UPDATE SET .. JDBC Catalog # The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol.
Currently, there are two JDBC catalog implementations, Postgres Catalog and MySQL Catalog. They support the following catalog methods. Other methods are currently not supported.
// The supported methods by Postgres \u0026amp; MySQL Catalog. databaseExists(String databaseName); listDatabases(); getDatabase(String databaseName); listTables(String databaseName); getTable(ObjectPath tablePath); tableExists(ObjectPath tablePath); Other Catalog methods are currently not supported.
Usage of JDBC Catalog # The section mainly describes how to create and use a Postgres Catalog or MySQL Catalog. Please refer to Dependencies section for how to setup a JDBC connector and the corresponding driver.
The JDBC catalog supports the following options:
name: required, name of the catalog. default-database: required, default database to connect to. username: required, username of Postgres/MySQL account. password: required, password of the account. base-url: required (should not contain the database name) for Postgres Catalog this should be \u0026quot;jdbc:postgresql://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;\u0026quot; for MySQL Catalog this should be \u0026quot;jdbc:mysql://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;\u0026quot; SQL CREATE CATALOG my_catalog WITH( \u0026#39;type\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;default-database\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;username\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;base-url\u0026#39; = \u0026#39;...\u0026#39; ); USE CATALOG my_catalog; Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name = \u0026#34;my_catalog\u0026#34;; String defaultDatabase = \u0026#34;mydb\u0026#34;; String username = \u0026#34;...\u0026#34;; String password = \u0026#34;...\u0026#34;; String baseUrl = \u0026#34;...\u0026#34; JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog(\u0026#34;my_catalog\u0026#34;, catalog); // set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;my_catalog\u0026#34;); Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;my_catalog\u0026#34; val defaultDatabase = \u0026#34;mydb\u0026#34; val username = \u0026#34;...\u0026#34; val password = \u0026#34;...\u0026#34; val baseUrl = \u0026#34;...\u0026#34; val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl) tableEnv.registerCatalog(\u0026#34;my_catalog\u0026#34;, catalog) // set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;my_catalog\u0026#34;) Python from pyflink.table.catalog import JdbcCatalog environment_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(environment_settings) name = \u0026#34;my_catalog\u0026#34; default_database = \u0026#34;mydb\u0026#34; username = \u0026#34;...\u0026#34; password = \u0026#34;...\u0026#34; base_url = \u0026#34;...\u0026#34; catalog = JdbcCatalog(name, default_database, username, password, base_url) t_env.register_catalog(\u0026#34;my_catalog\u0026#34;, catalog) # set the JdbcCatalog as the current catalog of the session t_env.use_catalog(\u0026#34;my_catalog\u0026#34;) YAML execution: ... current-catalog: my_catalog # set the target JdbcCatalog as the current catalog of the session current-database: mydb catalogs: - name: my_catalog type: jdbc default-database: mydb username: ... password: ... base-url: ... JDBC Catalog for PostgreSQL # PostgreSQL Metaspace Mapping # PostgreSQL has an additional namespace as schema besides database. A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named \u0026ldquo;public\u0026rdquo;, each schema can have multiple tables. In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name. The schema_name is optional and defaults to \u0026ldquo;public\u0026rdquo;.
Therefore, the metaspace mapping between Flink Catalog and Postgres is as following:
Flink Catalog Metaspace Structure Postgres Metaspace Structure catalog name (defined in Flink only) N/A database name database name table name [schema_name.]table_name The full path of Postgres table in Flink should be \u0026quot;\u0026lt;catalog\u0026gt;.\u0026lt;db\u0026gt;.\`\u0026lt;schema.table\u0026gt;\`\u0026quot; if schema is specified, note the \u0026lt;schema.table\u0026gt; should be escaped.
Here are some examples to access Postgres tables:
-- scan table \u0026#39;test_table\u0026#39; of \u0026#39;public\u0026#39; schema (i.e. the default schema), the schema name can be omitted SELECT * FROM mypg.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- scan table \u0026#39;test_table2\u0026#39; of \u0026#39;custom_schema\u0026#39; schema, -- the custom schema can not be omitted and must be escaped with table. SELECT * FROM mypg.mydb.\`custom_schema.test_table2\` SELECT * FROM mydb.\`custom_schema.test_table2\`; SELECT * FROM \`custom_schema.test_table2\`; JDBC Catalog for MySQL # MySQL Metaspace Mapping # The databases in a MySQL instance are at the same mapping level as the databases under the catalog registered with MySQL Catalog. A MySQL instance can have multiple databases, each database can have multiple tables. In Flink, when querying tables registered by MySQL catalog, users can use either database.table_name or just table_name. The default value is the default database specified when MySQL Catalog was created.
Therefore, the metaspace mapping between Flink Catalog and MySQL Catalog is as following:
Flink Catalog Metaspace Structure MySQL Metaspace Structure catalog name (defined in Flink only) N/A database name database name table name table_name The full path of MySQL table in Flink should be \u0026quot;\`\u0026lt;catalog\u0026gt;\`.\`\u0026lt;db\u0026gt;\`.\`\u0026lt;table\u0026gt;\`\u0026quot;.
Here are some examples to access MySQL tables:
-- scan table \u0026#39;test_table\u0026#39;, the default database is \u0026#39;mydb\u0026#39;. SELECT * FROM mysql_catalog.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- scan table \u0026#39;test_table\u0026#39; with the given database. SELECT * FROM mysql_catalog.given_database.test_table2; SELECT * FROM given_database.test_table2; Data Type Mapping # Flink supports connect to several databases which uses dialect like MySQL, Oracle, PostgreSQL, Derby. The Derby dialect usually used for testing purpose. The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily.
MySQL type Oracle type PostgreSQL type Flink SQL type TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT
INT2
SMALLSERIAL
SERIAL2 SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INTEGER
SERIAL INT BIGINT
INT UNSIGNED BIGINT
BIGSERIAL BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT BIGINT FLOAT BINARY_FLOAT REAL
FLOAT4 FLOAT DOUBLE
DOUBLE PRECISION BINARY_DOUBLE FLOAT8
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) SMALLINT
FLOAT(s)
DOUBLE PRECISION
REAL
NUMBER(p, s) NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN
TINYINT(1) BOOLEAN BOOLEAN DATE DATE DATE DATE TIME [(p)] DATE TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] DATETIME [(p)] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n)
VARCHAR(n)
TEXT CHAR(n)
VARCHAR(n)
CLOB CHAR(n)
CHARACTER(n)
VARCHAR(n)
CHARACTER VARYING(n)
TEXT STRING BINARY
VARBINARY
BLOB RAW(s)
BLOB BYTEA BYTES ARRAY ARRAY Back to top
`}),e.add({id:180,href:"/flink/flink-docs-master/docs/ops/metrics/",title:"Metrics",section:"Operations",content:` Metrics # Flink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from any user function that extends RichFunction by calling getRuntimeContext().getMetricGroup(). This method returns a MetricGroup object on which you can create and register new metrics.
Metric types # Flink supports Counters, Gauges, Histograms and Meters.
Counter # A Counter is used to count something. The current value can be in- or decremented using inc()/inc(long n) or dec()/dec(long n). You can create and register a Counter by calling counter(String name) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient Counter counter; @Override public void open(Configuration config) { this.counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCounter\u0026#34;); } @Override public String map(String value) throws Exception { this.counter.inc(); return value; } } Scala class MyMapper extends RichMapFunction[String,String] { @transient private var counter: Counter = _ override def open(parameters: Configuration): Unit = { counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCounter\u0026#34;) } override def map(value: String): String = { counter.inc() value } } Python class MyMapper(MapFunction): def __init__(self): self.counter = None def open(self, runtime_context: RuntimeContext): self.counter = runtime_context \\ .get_metrics_group() \\ .counter(\u0026#34;my_counter\u0026#34;) def map(self, value: str): self.counter.inc() return value Alternatively you can also use your own Counter implementation:
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient Counter counter; @Override public void open(Configuration config) { this.counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCustomCounter\u0026#34;, new CustomCounter()); } @Override public String map(String value) throws Exception { this.counter.inc(); return value; } } Scala class MyMapper extends RichMapFunction[String,String] { @transient private var counter: Counter = _ override def open(parameters: Configuration): Unit = { counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCustomCounter\u0026#34;, new CustomCounter()) } override def map(value: String): String = { counter.inc() value } } Python Still not supported in Python API. Gauge # A Gauge provides a value of any type on demand. In order to use a Gauge you must first create a class that implements the org.apache.flink.metrics.Gauge interface. There is no restriction for the type of the returned value. You can register a gauge by calling gauge(String name, Gauge gauge) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient int valueToExpose = 0; @Override public void open(Configuration config) { getRuntimeContext() .getMetricGroup() .gauge(\u0026#34;MyGauge\u0026#34;, new Gauge\u0026lt;Integer\u0026gt;() { @Override public Integer getValue() { return valueToExpose; } }); } @Override public String map(String value) throws Exception { valueToExpose++; return value; } } Scala new class MyMapper extends RichMapFunction[String,String] { @transient private var valueToExpose = 0 override def open(parameters: Configuration): Unit = { getRuntimeContext() .getMetricGroup() .gauge[Int, ScalaGauge[Int]](\u0026#34;MyGauge\u0026#34;, ScalaGauge[Int]( () =\u0026gt; valueToExpose ) ) } override def map(value: String): String = { valueToExpose += 1 value } } Python class MyMapper(MapFunction): def __init__(self): self.value_to_expose = 0 def open(self, runtime_context: RuntimeContext): runtime_context \\ .get_metrics_group() \\ .gauge(\u0026#34;my_gauge\u0026#34;, lambda: self.value_to_expose) def map(self, value: str): self.value_to_expose += 1 return value Note that reporters will turn the exposed object into a String, which means that a meaningful toString() implementation is required.
Histogram # A Histogram measures the distribution of long values. You can register one by calling histogram(String name, Histogram histogram) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Histogram histogram; @Override public void open(Configuration config) { this.histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new MyHistogram()); } @Override public Long map(Long value) throws Exception { this.histogram.update(value); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var histogram: Histogram = _ override def open(parameters: Configuration): Unit = { histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new MyHistogram()) } override def map(value: Long): Long = { histogram.update(value) value } } Python Still not supported in Python API. Flink does not provide a default implementation for Histogram, but offers a Wrapper that allows usage of Codahale/DropWizard histograms. To use this wrapper add the following dependency in your pom.xml:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-metrics-dropwizard\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; You can then register a Codahale/DropWizard histogram like this:
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Histogram histogram; @Override public void open(Configuration config) { com.codahale.metrics.Histogram dropwizardHistogram = new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500)); this.histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new DropwizardHistogramWrapper(dropwizardHistogram)); } @Override public Long map(Long value) throws Exception { this.histogram.update(value); return value; } } Scala class MyMapper extends RichMapFunction[Long, Long] { @transient private var histogram: Histogram = _ override def open(config: Configuration): Unit = { com.codahale.metrics.Histogram dropwizardHistogram = new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500)) histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new DropwizardHistogramWrapper(dropwizardHistogram)) } override def map(value: Long): Long = { histogram.update(value) value } } Python Still not supported in Python API. Meter # A Meter measures an average throughput. An occurrence of an event can be registered with the markEvent() method. Occurrence of multiple events at the same time can be registered with markEvent(long n) method. You can register a meter by calling meter(String name, Meter meter) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Meter meter; @Override public void open(Configuration config) { this.meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new MyMeter()); } @Override public Long map(Long value) throws Exception { this.meter.markEvent(); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var meter: Meter = _ override def open(config: Configuration): Unit = { meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new MyMeter()) } override def map(value: Long): Long = { meter.markEvent() value } } Python class MyMapperMeter(MapFunction): def __init__(self): self.meter = None def open(self, runtime_context: RuntimeContext): # an average rate of events per second over 120s, default is 60s. self.meter = runtime_context .get_metrics_group() .meter(\u0026#34;my_meter\u0026#34;, time_span_in_seconds=120) def map(self, value: str): self.meter.mark_event() return value Flink offers a Wrapper that allows usage of Codahale/DropWizard meters. To use this wrapper add the following dependency in your pom.xml:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-metrics-dropwizard\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; You can then register a Codahale/DropWizard meter like this:
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Meter meter; @Override public void open(Configuration config) { com.codahale.metrics.Meter dropwizardMeter = new com.codahale.metrics.Meter(); this.meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new DropwizardMeterWrapper(dropwizardMeter)); } @Override public Long map(Long value) throws Exception { this.meter.markEvent(); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var meter: Meter = _ override def open(config: Configuration): Unit = { val dropwizardMeter: com.codahale.metrics.Meter = new com.codahale.metrics.Meter() meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new DropwizardMeterWrapper(dropwizardMeter)) } override def map(value: Long): Long = { meter.markEvent() value } } Python Still not supported in Python API. Scope # Every metric is assigned an identifier and a set of key-value pairs under which the metric will be reported.
The identifier is based on 3 components: a user-defined name when registering the metric, an optional user-defined scope and a system-provided scope. For example, if A.B is the system scope, C.D the user scope and E the name, then the identifier for the metric will be A.B.C.D.E.
You can configure which delimiter to use for the identifier (default: .) by setting the metrics.scope.delimiter key in conf/flink-conf.yaml.
User Scope # You can define a user scope by calling MetricGroup#addGroup(String name), MetricGroup#addGroup(int name) or MetricGroup#addGroup(String key, String value). These methods affect what MetricGroup#getMetricIdentifier and MetricGroup#getScopeComponents return.
Java counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetrics\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); Scala counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetrics\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) Python counter = runtime_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) counter = runtime_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) System Scope # The system scope contains context information about the metric, for example in which task it was registered or what job that task belongs to.
Which context information should be included can be configured by setting the following keys in conf/flink-conf.yaml. Each of these keys expect a format string that may contain constants (e.g. \u0026ldquo;taskmanager\u0026rdquo;) and variables (e.g. \u0026ldquo;\u0026lt;task_id\u0026gt;\u0026rdquo;) which will be replaced at runtime.
metrics.scope.jm Default: \u0026lt;host\u0026gt;.jobmanager Applied to all metrics that were scoped to a job manager. metrics.scope.jm.job Default: \u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt; Applied to all metrics that were scoped to a job manager and job. metrics.scope.tm Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt; Applied to all metrics that were scoped to a task manager. metrics.scope.tm.job Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt; Applied to all metrics that were scoped to a task manager and job. metrics.scope.task Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;subtask_index\u0026gt; Applied to all metrics that were scoped to a task. metrics.scope.operator Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; Applied to all metrics that were scoped to an operator. There are no restrictions on the number or order of variables. Variables are case sensitive.
The default scope for operator metrics will result in an identifier akin to localhost.taskmanager.1234.MyJob.MyOperator.0.MyMetric
If you also want to include the task name but omit the task manager information you can specify the following format:
metrics.scope.operator: \u0026lt;host\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;
This could create the identifier localhost.MyJob.MySource_-\u0026gt;_MyOperator.MyOperator.0.MyMetric.
Note that for this format string an identifier clash can occur should the same job be run multiple times concurrently, which can lead to inconsistent metric data. As such it is advised to either use format strings that provide a certain degree of uniqueness by including IDs (e.g \u0026lt;job_id\u0026gt;) or by assigning unique names to jobs and operators.
List of all Variables # JobManager: \u0026lt;host\u0026gt; TaskManager: \u0026lt;host\u0026gt;, \u0026lt;tm_id\u0026gt; Job: \u0026lt;job_id\u0026gt;, \u0026lt;job_name\u0026gt; Task: \u0026lt;task_id\u0026gt;, \u0026lt;task_name\u0026gt;, \u0026lt;task_attempt_id\u0026gt;, \u0026lt;task_attempt_num\u0026gt;, \u0026lt;subtask_index\u0026gt; Operator: \u0026lt;operator_id\u0026gt;,\u0026lt;operator_name\u0026gt;, \u0026lt;subtask_index\u0026gt; Important: For the Batch API, \u0026lt;operator_id\u0026gt; is always equal to \u0026lt;task_id\u0026gt;.
User Variables # You can define a user variable by calling MetricGroup#addGroup(String key, String value). This method affects what MetricGroup#getMetricIdentifier, MetricGroup#getScopeComponents and MetricGroup#getAllVariables() returns.
Important: User variables cannot be used in scope formats.
Java counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); Scala counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) Reporter # For information on how to set up Flink\u0026rsquo;s metric reporters please take a look at the metric reporters documentation.
System metrics # By default Flink gathers several metrics that provide deep insights on the current state. This section is a reference of all these metrics.
The tables below generally feature 5 columns:
The \u0026ldquo;Scope\u0026rdquo; column describes which scope format is used to generate the system scope. For example, if the cell contains \u0026ldquo;Operator\u0026rdquo; then the scope format for \u0026ldquo;metrics.scope.operator\u0026rdquo; is used. If the cell contains multiple values, separated by a slash, then the metrics are reported multiple times for different entities, like for both job- and taskmanagers.
The (optional)\u0026ldquo;Infix\u0026rdquo; column describes which infix is appended to the system scope.
The \u0026ldquo;Metrics\u0026rdquo; column lists the names of all metrics that are registered for the given scope and infix.
The \u0026ldquo;Description\u0026rdquo; column provides information as to what a given metric is measuring.
The \u0026ldquo;Type\u0026rdquo; column describes which metric type is used for the measurement.
Note that all dots in the infix/metric name columns are still subject to the \u0026ldquo;metrics.delimiter\u0026rdquo; setting.
Thus, in order to infer the metric identifier:
Take the scope-format based on the \u0026ldquo;Scope\u0026rdquo; column Append the value in the \u0026ldquo;Infix\u0026rdquo; column if present, and account for the \u0026ldquo;metrics.delimiter\u0026rdquo; setting Append metric name. CPU # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.CPU Load The recent CPU usage of the JVM. Gauge Time The CPU time used by the JVM. Gauge Memory # The memory-related metrics require Oracle\u0026rsquo;s memory management (also included in OpenJDK\u0026rsquo;s Hotspot implementation) to be in place. Some metrics might not be exposed when using other JVM implementations (e.g. IBM\u0026rsquo;s J9).
Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.Memory Heap.Used The amount of heap memory currently used (in bytes). Gauge Heap.Committed The amount of heap memory guaranteed to be available to the JVM (in bytes). Gauge Heap.Max The maximum amount of heap memory that can be used for memory management (in bytes). This value might not be necessarily equal to the maximum value specified through -Xmx or the equivalent Flink configuration parameter. Some GC algorithms allocate heap memory that won't be available to the user code and, therefore, not being exposed through the heap metrics. Gauge NonHeap.Used The amount of non-heap memory currently used (in bytes). Gauge NonHeap.Committed The amount of non-heap memory guaranteed to be available to the JVM (in bytes). Gauge NonHeap.Max The maximum amount of non-heap memory that can be used for memory management (in bytes). Gauge Metaspace.Used The amount of memory currently used in the Metaspace memory pool (in bytes). Gauge Metaspace.Committed The amount of memory guaranteed to be available to the JVM in the Metaspace memory pool (in bytes). Gauge Metaspace.Max The maximum amount of memory that can be used in the Metaspace memory pool (in bytes). Gauge Direct.Count The number of buffers in the direct buffer pool. Gauge Direct.MemoryUsed The amount of memory used by the JVM for the direct buffer pool (in bytes). Gauge Direct.TotalCapacity The total capacity of all buffers in the direct buffer pool (in bytes). Gauge Mapped.Count The number of buffers in the mapped buffer pool. Gauge Mapped.MemoryUsed The amount of memory used by the JVM for the mapped buffer pool (in bytes). Gauge Mapped.TotalCapacity The number of buffers in the mapped buffer pool (in bytes). Gauge Status.Flink.Memory Managed.Used The amount of managed memory currently used. Gauge Managed.Total The total amount of managed memory. Gauge Threads # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.Threads Count The total number of live threads. Gauge GarbageCollection # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.GarbageCollector \u0026lt;GarbageCollector\u0026gt;.Count The total number of collections that have occurred. Gauge \u0026lt;GarbageCollector\u0026gt;.Time The total time spent performing garbage collection. Gauge ClassLoader # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.ClassLoader ClassesLoaded The total number of classes loaded since the start of the JVM. Gauge ClassesUnloaded The total number of classes unloaded since the start of the JVM. Gauge Network # Deprecated: use Default shuffle service metrics Scope Infix Metrics Description Type TaskManager Status.Network AvailableMemorySegments The number of unused memory segments. Gauge TotalMemorySegments The number of allocated memory segments. Gauge Task buffers inputQueueLength The number of queued input buffers. (ignores LocalInputChannels which are using blocking subpartitions) Gauge outputQueueLength The number of queued output buffers. Gauge inPoolUsage An estimate of the input buffers usage. (ignores LocalInputChannels) Gauge inputFloatingBuffersUsage An estimate of the floating input buffers usage. (ignores LocalInputChannels) Gauge inputExclusiveBuffersUsage An estimate of the exclusive input buffers usage. (ignores LocalInputChannels) Gauge outPoolUsage An estimate of the output buffers usage. The pool usage can be \u003e 100% if overdraft buffers are being used. Gauge Network.\u0026lt;Input|Output\u0026gt;.\u0026lt;gate|partition\u0026gt;
(only available if taskmanager.network.detailed-metrics config option is set) totalQueueLen Total number of queued buffers in all input/output channels. Gauge minQueueLen Minimum number of queued buffers in all input/output channels. Gauge maxQueueLen Maximum number of queued buffers in all input/output channels. Gauge avgQueueLen Average number of queued buffers in all input/output channels. Gauge Default shuffle service # Metrics related to data exchange between task executors using netty network communication.
Scope Infix Metrics Description Type TaskManager Status.Shuffle.Netty AvailableMemorySegments The number of unused memory segments. Gauge UsedMemorySegments The number of used memory segments. Gauge TotalMemorySegments The number of allocated memory segments. Gauge AvailableMemory The amount of unused memory in bytes. Gauge UsedMemory The amount of used memory in bytes. Gauge TotalMemory The amount of allocated memory in bytes. Gauge RequestedMemoryUsage Experimental: The usage of the network memory. Shows (as percentage) the total amount of requested memory from all of the subtasks. It can exceed 100% as not all requested memory is required for subtask to make progress. However if usage exceeds 100% throughput can suffer greatly and please consider increasing available network memory, or decreasing configured size of network buffer pools. Gauge Task Shuffle.Netty.Input.Buffers inputQueueLength The number of queued input buffers. Gauge inputQueueSize The real size of queued input buffers in bytes. The size for local input channels is always \`0\` since the local channel takes records directly from the output queue. Gauge inPoolUsage An estimate of the input buffers usage. (ignores LocalInputChannels) Gauge inputFloatingBuffersUsage An estimate of the floating input buffers usage. (ignores LocalInputChannels) Gauge inputExclusiveBuffersUsage An estimate of the exclusive input buffers usage. (ignores LocalInputChannels) Gauge Shuffle.Netty.Output.Buffers outputQueueLength The number of queued output buffers. Gauge outputQueueSize The real size of queued output buffers in bytes. Gauge outPoolUsage An estimate of the output buffers usage. The pool usage can be \u003e 100% if overdraft buffers are being used. Gauge Shuffle.Netty.\u0026lt;Input|Output\u0026gt;.\u0026lt;gate|partition\u0026gt;
(only available if taskmanager.network.detailed-metrics config option is set) totalQueueLen Total number of queued buffers in all input/output channels. Gauge minQueueLen Minimum number of queued buffers in all input/output channels. Gauge maxQueueLen Maximum number of queued buffers in all input/output channels. Gauge avgQueueLen Average number of queued buffers in all input/output channels. Gauge Shuffle.Netty.Input numBytesInLocal The total number of bytes this task has read from a local source. Counter numBytesInLocalPerSecond The number of bytes this task reads from a local source per second. Meter numBytesInRemote The total number of bytes this task has read from a remote source. Counter numBytesInRemotePerSecond The number of bytes this task reads from a remote source per second. Meter numBuffersInLocal The total number of network buffers this task has read from a local source. Counter numBuffersInLocalPerSecond The number of network buffers this task reads from a local source per second. Meter numBuffersInRemote The total number of network buffers this task has read from a remote source. Counter numBuffersInRemotePerSecond The number of network buffers this task reads from a remote source per second. Meter Cluster # Scope Metrics Description Type JobManager numRegisteredTaskManagers The number of registered taskmanagers. Gauge numPendingTaskManagers (only applicable to Native Kubernetes / YARN) The number of outstanding taskmanagers that Flink has requested. Gauge numRunningJobs The number of running jobs. Gauge taskSlotsAvailable The number of available task slots. Gauge taskSlotsTotal The total number of task slots. Gauge Availability # The metrics in this table are available for each of the following job states: INITIALIZING, CREATED, RUNNING, RESTARTING, CANCELLING, FAILING. Whether these metrics are reported depends on the metrics.job.status.enable setting.
Evolving The semantics of these metrics may change in later releases.
Scope Metrics Description Type Job (only available on JobManager) \u0026lt;jobStatus\u0026gt;State For a given state, return 1 if the job is currently in that state, otherwise return 0. Gauge \u0026lt;jobStatus\u0026gt;Time For a given state, if the job is currently in that state, return the time (in milliseconds) since the job transitioned into that state, otherwise return 0. Gauge \u0026lt;jobStatus\u0026gt;TimeTotal For a given state, return how much time (in milliseconds) the job has spent in that state in total. Gauge Experimental
While the job is in the RUNNING state the metrics in this table provide additional details on what the job is currently doing. Whether these metrics are reported depends on the metrics.job.status.enable setting.
Scope Metrics Description Type Job (only available on JobManager) deployingState Return 1 if the job is currently deploying* tasks, otherwise return 0. Gauge deployingTime Return the time (in milliseconds) since the job has started deploying* tasks, otherwise return 0. Gauge deployingTimeTotal Return how much time (in milliseconds) the job has spent deploying* tasks in total. Gauge *A job is considered to be deploying tasks when:
for streaming jobs, any task is in the DEPLOYING state for batch jobs, if at least 1 task is in the DEPLOYING state, and there are no INITIALIZING/RUNNING tasks Scope Metrics Description Type Job (only available on JobManager) uptime Attention: deprecated, use runningTime. Gauge downtime Attention: deprecated, use restartingTime, cancellingTime failingTime. Gauge fullRestarts Attention: deprecated, use numRestarts. Gauge numRestarts The total number of restarts since this job was submitted, including full restarts and fine-grained restarts. Gauge Checkpointing # Note that for failed checkpoints, metrics are updated on a best efforts basis and may be not accurate.
Scope Metrics Description Type Job (only available on JobManager) lastCheckpointDuration The time it took to complete the last checkpoint (in milliseconds). Gauge lastCheckpointSize The checkpointed size of the last checkpoint (in bytes), this metric could be different from lastCheckpointFullSize if incremental checkpoint or changelog is enabled. Gauge lastCheckpointFullSize The full size of the last checkpoint (in bytes). Gauge lastCheckpointExternalPath The path where the last external checkpoint was stored. Gauge lastCheckpointRestoreTimestamp Timestamp when the last checkpoint was restored at the coordinator (in milliseconds). Gauge numberOfInProgressCheckpoints The number of in progress checkpoints. Gauge numberOfCompletedCheckpoints The number of successfully completed checkpoints. Gauge numberOfFailedCheckpoints The number of failed checkpoints. Gauge totalNumberOfCheckpoints The number of total checkpoints (in progress, completed, failed). Gauge Task checkpointAlignmentTime The time in nanoseconds that the last barrier alignment took to complete, or how long the current alignment has taken so far (in nanoseconds). This is the time between receiving first and the last checkpoint barrier. You can find more information in the [Monitoring State and Checkpoints section](//localhost/flink/flink-docs-master/docs/ops/state/large_state_tuning/#monitoring-state-and-checkpoints) Gauge checkpointStartDelayNanos The time in nanoseconds that elapsed between the creation of the last checkpoint and the time when the checkpointing process has started by this Task. This delay shows how long it takes for the first checkpoint barrier to reach the task. A high value indicates back-pressure. If only a specific task has a long start delay, the most likely reason is data skew. Gauge State Access Latency # Scope Metrics Description Type Task/Operator stateClearLatency The latency of clear operation for state Histogram valueStateGetLatency The latency of Get operation for value state Histogram valueStateUpdateLatency The latency of update operation for value state Histogram listStateGetLatency The latency of get operation for list state Histogram listStateAddLatency The latency of add operation for list state Histogram listStateAddAllLatency The latency of addAll operation for list state Histogram listStateUpdateLatency The latency of update operation for list state Histogram listStateMergeNamespacesLatency The latency of merge namespace operation for list state Histogram mapStateGetLatency The latency of get operation for map state Histogram mapStatePutLatency The latency of put operation for map state Histogram mapStatePutAllLatency The latency of putAll operation for map state Histogram mapStateRemoveLatency The latency of remove operation for map state Histogram mapStateContainsLatency The latency of contains operation for map state Histogram mapStateEntriesInitLatency The init latency of entries operation for map state Histogram mapStateKeysInitLatency The init latency of keys operation for map state Histogram mapStateValuesInitLatency The init latency of values operation for map state Histogram mapStateIteratorInitLatency The init latency of iterator operation for map state Histogram mapStateIsEmptyLatency The latency of isEmpty operation for map state Histogram mapStateIteratorHasNextLatency The latency of iterator#hasNext operation for map state Histogram mapStateIteratorNextLatency The latency of iterator#next operation for map state Histogram mapStateIteratorRemoveLatency The latency of iterator#remove operation for map state Histogram aggregatingStateGetLatency The latency of get operation for aggregating state Histogram aggregatingStateAddLatency The latency of add operation for aggregating state Histogram aggregatingStateMergeNamespacesLatency The latency of merge namespace operation for aggregating state Histogram reducingStateGetLatency The latency of get operation for reducing state Histogram reducingStateAddLatency The latency of add operation for reducing state Histogram reducingStateMergeNamespacesLatency The latency of merge namespace operation for reducing state Histogram RocksDB # Certain RocksDB native metrics are available but disabled by default, you can find full documentation here
State Changelog # Note that the metrics are only available via reporters.
Scope Metrics Description Type Job (only available on TaskManager) numberOfUploadRequests Total number of upload requests made Counter numberOfUploadFailures Total number of failed upload requests (request may be retried after the failure) Counter attemptsPerUpload The number of attempts per upload Histogram totalAttemptsPerUpload The total count distributions of attempts for per upload Histogram uploadBatchSizes The number of upload tasks (coming from one or more writers, i.e. backends/tasks) that were grouped together and form a single upload resulting in a single file Histogram uploadLatenciesNanos The latency distributions of uploads Histogram uploadSizes The size distributions of uploads Histogram uploadQueueSize Current size of upload queue. Queue items can be packed together and form a single upload. Gauge Task/Operator startedMaterialization The number of started materializations. Counter completedMaterialization The number of successfully completed materializations. Counter failedMaterialization The number of failed materializations. Counter lastFullSizeOfMaterialization The full size of the materialization part of the last reported checkpoint (in bytes). Gauge lastIncSizeOfMaterialization The incremental size of the materialization part of the last reported checkpoint (in bytes). Gauge lastFullSizeOfNonMaterialization The full size of the non-materialization part of the last reported checkpoint (in bytes). Gauge lastIncSizeOfNonMaterialization The incremental size of the non-materialization part of the last reported checkpoint (in bytes). Gauge IO # Scope Metrics Description Type Job (only available on TaskManager) [\u0026lt;source_id\u0026gt;.[\u0026lt;source_subtask_index\u0026gt;.]]\u0026lt;operator_id\u0026gt;.\u0026lt;operator_subtask_index\u0026gt;.latency The latency distributions from a given source (subtask) to an operator subtask (in milliseconds), depending on the latency granularity. Histogram Task numBytesInLocal Attention: deprecated, use Default shuffle service metrics. Counter numBytesInLocalPerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBytesInRemote Attention: deprecated, use Default shuffle service metrics. Counter numBytesInRemotePerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBuffersInLocal Attention: deprecated, use Default shuffle service metrics. Counter numBuffersInLocalPerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBuffersInRemote Attention: deprecated, use Default shuffle service metrics. Counter numBuffersInRemotePerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBytesOut The total number of bytes this task has emitted. Counter numBytesOutPerSecond The number of bytes this task emits per second. Meter numBuffersOut The total number of network buffers this task has emitted. Counter numBuffersOutPerSecond The number of network buffers this task emits per second. Meter isBackPressured Whether the task is back-pressured. Gauge idleTimeMsPerSecond The time (in milliseconds) this task is idle (has no data to process) per second. Idle time excludes back pressured time, so if the task is back pressured it is not idle. Meter busyTimeMsPerSecond The time (in milliseconds) this task is busy (neither idle nor back pressured) per second. Can be NaN, if the value could not be calculated. Gauge backPressuredTimeMsPerSecond The time (in milliseconds) this task is back pressured (soft or hard) per second. It's a sum of softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond. Gauge softBackPressuredTimeMsPerSecond The time (in milliseconds) this task is softly back pressured per second. Softly back pressured task will be still responsive and capable of for example triggering unaligned checkpoints. Gauge hardBackPressuredTimeMsPerSecond The time (in milliseconds) this task is back pressured in a hard way per second. During hard back pressured task is completely blocked and unresponsive preventing for example unaligned checkpoints from triggering. Gauge maxSoftBackPressuredTimeMs Maximum recorded duration of a single consecutive period of the task being softly back pressured in the last sampling period. Please check softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond for more information. Gauge maxHardBackPressuredTimeMs Maximum recorded duration of a single consecutive period of the task being in the hard back pressure state in the last sampling period. Please check softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond for more information. Gauge mailboxMailsPerSecond The number of actions processed from the task's mailbox per second which includes all actions, e.g., checkpointing, timer, or cancellation actions. Meter mailboxLatencyMs The latency is the time that actions spend waiting in the task's mailbox before being processed. The metric is a statistic of the latency in milliseconds that is measured approximately once every second and includes the last 60 measurements. Histogram mailboxQueueSize The number of actions in the task's mailbox that are waiting to be processed. Gauge Task (only if buffer debloating is enabled and in non-source tasks) estimatedTimeToConsumeBuffersMs The estimated time (in milliseconds) by the buffer debloater to consume all of the buffered data in the network exchange preceding this task. This value is calculated by approximated amount of the in-flight data and calculated throughput. Gauge debloatedBufferSize The desired buffer size (in bytes) calculated by the buffer debloater. Buffer debloater is trying to reduce buffer size when the amount of in-flight data (after taking into account current throughput) exceeds the configured target value. Gauge Task/Operator numRecordsIn The total number of records this operator/task has received. Counter numRecordsInPerSecond The number of records this operator/task receives per second. Meter numRecordsOut The total number of records this operator/task has emitted. Counter numRecordsOutPerSecond The number of records this operator/task sends per second. Meter numLateRecordsDropped The number of records this operator/task has dropped due to arriving late. Counter currentInputWatermark The last watermark this operator/tasks has received (in milliseconds). Note: For operators/tasks with 2 inputs this is the minimum of the last received watermarks.
Gauge Operator currentInputNWatermark The last watermark this operator has received in its N'th input (in milliseconds), with index N starting from 1. For example currentInput1Watermark, currentInput2Watermark, ... Note: Only for operators with 2 or more inputs.
Gauge currentOutputWatermark The last watermark this operator has emitted (in milliseconds). Gauge watermarkAlignmentDrift The current drift from the minimal watermark emitted by all sources/tasks/splits that belong to the same watermark group. Note: Available only when watermark alignment is enabled and the first common watermark is announced. You can configure the update interval in the WatermarkStrategy.
Gauge numSplitsProcessed The total number of InputSplits this data source has processed (if the operator is a data source). Gauge Connectors # Kafka Connectors # Please refer to Kafka monitoring.
Kinesis Source # Scope Metrics User Variables Description Type Operator millisBehindLatest stream, shardId The number of milliseconds the consumer is behind the head of the stream, indicating how far behind current time the consumer is, for each Kinesis shard. A particular shard's metric can be specified by stream name and shard id. A value of 0 indicates record processing is caught up, and there are no new records to process at this moment. A value of -1 indicates that there is no reported value for the metric, yet. Gauge Operator sleepTimeMillis stream, shardId The number of milliseconds the consumer spends sleeping before fetching records from Kinesis. A particular shard's metric can be specified by stream name and shard id. Gauge Operator maxNumberOfRecordsPerFetch stream, shardId The maximum number of records requested by the consumer in a single getRecords call to Kinesis. If ConsumerConfigConstants.SHARD_USE_ADAPTIVE_READS is set to true, this value is adaptively calculated to maximize the 2 Mbps read limits from Kinesis. Gauge Operator numberOfAggregatedRecordsPerFetch stream, shardId The number of aggregated Kinesis records fetched by the consumer in a single getRecords call to Kinesis. Gauge Operator numberOfDeggregatedRecordsPerFetch stream, shardId The number of deaggregated Kinesis records fetched by the consumer in a single getRecords call to Kinesis. Gauge Operator averageRecordSizeBytes stream, shardId The average size of a Kinesis record in bytes, fetched by the consumer in a single getRecords call. Gauge Operator runLoopTimeNanos stream, shardId The actual time taken, in nanoseconds, by the consumer in the run loop. Gauge Operator loopFrequencyHz stream, shardId The number of calls to getRecords in one second. Gauge Operator bytesRequestedPerFetch stream, shardId The bytes requested (2 Mbps / loopFrequencyHz) in a single call to getRecords. Gauge Kinesis Sink # Scope Metrics Description Type Operator numRecordsOutErrors (deprecated, please use numRecordsSendErrors) Number of rejected record writes. Counter Operator numRecordsSendErrors Number of rejected record writes. Counter Operator CurrentSendTime Number of ms taken for 1 round trip of the last request batch. Gauge HBase Connectors # Scope Metrics User Variables Description Type Operator lookupCacheHitRate n/a Cache hit ratio for lookup. Gauge System resources # System resources reporting is disabled by default. When metrics.system-resource is enabled additional metrics listed below will be available on Job- and TaskManager. System resources metrics are updated periodically and they present average values for a configured interval (metrics.system-resource-probing-interval).
System resources reporting requires an optional dependency to be present on the classpath (for example placed in Flink\u0026rsquo;s lib directory):
com.github.oshi:oshi-core:6.1.5 (licensed under MIT license) Including it\u0026rsquo;s transitive dependencies:
net.java.dev.jna:jna-platform:jar:5.10.0 net.java.dev.jna:jna:jar:5.10.0 Failures in this regard will be reported as warning messages like NoClassDefFoundError logged by SystemResourcesMetricsInitializer during the startup.
System CPU # Scope Infix Metrics Description Job-/TaskManager System.CPU Usage Overall % of CPU usage on the machine. Idle % of CPU Idle time on the machine. Sys % of System CPU time on the machine. User % of User CPU time on the machine. IOWait % of IOWait CPU time on the machine. Irq % of Irq CPU time on the machine. SoftIrq % of SoftIrq CPU time on the machine. Nice % of Nice CPU time on the machine. Steal % of Steal CPU time on the machine. Load1min Average CPU load over 1 minute Load5min Average CPU load over 5 minute Load15min Average CPU load over 15 minute UsageCPU* % of CPU usage per each processor System memory # Scope Infix Metrics Description Job-/TaskManager System.Memory Available Available memory in bytes Total Total memory in bytes System.Swap Used Used swap bytes Total Total swap in bytes System network # Scope Infix Metrics Description Job-/TaskManager System.Network.INTERFACE_NAME ReceiveRate Average receive rate in bytes per second SendRate Average send rate in bytes per second Speculative Execution # Metrics below can be used to measure the effectiveness of speculative execution.
Scope Metrics Description Type Job (only available on JobManager) numSlowExecutionVertices Number of slow execution vertices at the moment. Gauge numEffectiveSpeculativeExecutions Number of effective speculative execution attempts, i.e. speculative execution attempts which finish earlier than their corresponding original attempts. Counter End-to-End latency tracking # Flink allows to track the latency of records travelling through the system. This feature is disabled by default. To enable the latency tracking you must set the latencyTrackingInterval to a positive number in either the Flink configuration or ExecutionConfig.
At the latencyTrackingInterval, the sources will periodically emit a special record, called a LatencyMarker. The marker contains a timestamp from the time when the record has been emitted at the sources. Latency markers can not overtake regular user records, thus if records are queuing up in front of an operator, it will add to the latency tracked by the marker.
Note that the latency markers are not accounting for the time user records spend in operators as they are bypassing them. In particular the markers are not accounting for the time records spend for example in window buffers. Only if operators are not able to accept new records, thus they are queuing up, the latency measured using the markers will reflect that.
The LatencyMarkers are used to derive a distribution of the latency between the sources of the topology and each downstream operator. These distributions are reported as histogram metrics. The granularity of these distributions can be controlled in the Flink configuration. For the highest granularity subtask Flink will derive the latency distribution between every source subtask and every downstream subtask, which results in quadratic (in the terms of the parallelism) number of histograms.
Currently, Flink assumes that the clocks of all machines in the cluster are in sync. We recommend setting up an automated clock synchronisation service (like NTP) to avoid false latency results.
Warning Enabling latency metrics can significantly impact the performance of the cluster (in particular for subtask granularity). It is highly recommended to only use them for debugging purposes.
State access latency tracking # Flink also allows to track the keyed state access latency for standard Flink state-backends or customized state backends which extending from AbstractStateBackend. This feature is disabled by default. To enable this feature you must set the state.backend.latency-track.keyed-state-enabled to true in the Flink configuration.
Once tracking keyed state access latency is enabled, Flink will sample the state access latency every N access, in which N is defined by state.backend.latency-track.sample-interval. This configuration has a default value of 100. A smaller value will get more accurate results but have a higher performance impact since it is sampled more frequently.
As the type of this latency metrics is histogram, state.backend.latency-track.history-size will control the maximum number of recorded values in history, which has the default value of 128. A larger value of this configuration will require more memory, but will provide a more accurate result.
Warning Enabling state-access-latency metrics may impact the performance. It is recommended to only use them for debugging purposes.
REST API integration # Metrics can be queried through the Monitoring REST API.
Below is a list of available endpoints, with a sample JSON response. All endpoints are of the sample form http://hostname:8081/jobmanager/metrics, below we list only the path part of the URLs.
Values in angle brackets are variables, for example http://hostname:8081/jobs/\u0026lt;jobid\u0026gt;/metrics will have to be requested for example as http://hostname:8081/jobs/7684be6004e4e955c2a558a9bc463f65/metrics.
Request metrics for a specific entity:
/jobmanager/metrics /taskmanagers/\u0026lt;taskmanagerid\u0026gt;/metrics /jobs/\u0026lt;jobid\u0026gt;/metrics /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtaskindex\u0026gt; Request metrics aggregated across all entities of the respective type:
/taskmanagers/metrics /jobs/metrics /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/metrics Request metrics aggregated over a subset of all entities of the respective type:
/taskmanagers/metrics?taskmanagers=A,B,C /jobs/metrics?jobs=D,E,F /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/metrics?subtask=1,2,3 Warning Metric names can contain special characters that you need to be escape when querying metrics. For example, \u0026ldquo;a_+_b\u0026rdquo; would be escaped to \u0026ldquo;a_%2B_b\u0026rdquo;.
List of characters that should be escaped:
Character Escape Sequence # %23 \$ %24 \u0026 %26 + %2B / %2F ; %3B = %3D ? %3F @ %40 Request a list of available metrics:
GET /jobmanager/metrics
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34; } ] Request the values for specific (unaggregated) metrics:
GET taskmanagers/ABCDE/metrics?get=metric1,metric2
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;34\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; } ] Request aggregated values for specific metrics:
GET /taskmanagers/metrics?get=metric1,metric2
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;min\u0026#34;: 1, \u0026#34;max\u0026#34;: 34, \u0026#34;avg\u0026#34;: 15, \u0026#34;sum\u0026#34;: 45 }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;min\u0026#34;: 2, \u0026#34;max\u0026#34;: 14, \u0026#34;avg\u0026#34;: 7, \u0026#34;sum\u0026#34;: 16 } ] Request specific aggregated values for specific metrics:
GET /taskmanagers/metrics?get=metric1,metric2\u0026amp;agg=min,max
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;min\u0026#34;: 1, \u0026#34;max\u0026#34;: 34 }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;min\u0026#34;: 2, \u0026#34;max\u0026#34;: 14 } ] Dashboard integration # Metrics that were gathered for each task or operator can also be visualized in the Dashboard. On the main page for a job, select the Metrics tab. After selecting one of the tasks in the top graph you can select metrics to display using the Add Metric drop-down menu.
Task metrics are listed as \u0026lt;subtask_index\u0026gt;.\u0026lt;metric_name\u0026gt;. Operator metrics are listed as \u0026lt;subtask_index\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;metric_name\u0026gt;. Each metric will be visualized as a separate graph, with the x-axis representing time and the y-axis the measured value. All graphs are automatically updated every 10 seconds, and continue to do so when navigating to another page.
There is no limit as to the number of visualized metrics; however only numeric metrics can be visualized.
Back to top
`}),e.add({id:181,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state_backends/",title:"State Backends",section:"State \u0026 Fault Tolerance",content:` State Backends # Flink provides different state backends that specify how and where state is stored.
State can be located on Java’s heap or off-heap. Depending on your state backend, Flink can also manage the state for the application, meaning Flink deals with the memory management (possibly spilling to disk if necessary) to allow applications to hold very large state. By default, the configuration file flink-conf.yaml determines the state backend for all Flink jobs.
However, the default state backend can be overridden on a per-job basis, as shown below.
For more information about the available state backends, their advantages, limitations, and configuration parameters see the corresponding section in Deployment \u0026amp; Operations.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(...); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(...) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(...) Back to top
`}),e.add({id:182,href:"/flink/flink-docs-master/docs/internals/task_lifecycle/",title:"Task Lifecycle",section:"Internals",content:` Task Lifecycle # A task in Flink is the basic unit of execution. It is the place where each parallel instance of an operator is executed. As an example, an operator with a parallelism of 5 will have each of its instances executed by a separate task.
The StreamTask is the base for all different task sub-types in Flink\u0026rsquo;s streaming engine. This document goes through the different phases in the lifecycle of the StreamTask and describes the main methods representing each of these phases.
Operator Lifecycle in a nutshell # Because the task is the entity that executes a parallel instance of an operator, its lifecycle is tightly integrated with that of an operator. So, we will briefly mention the basic methods representing the lifecycle of an operator before diving into those of the StreamTask itself. The list is presented below in the order that each of the methods is called. Given that an operator can have a user-defined function (UDF), below each of the operator methods we also present (indented) the methods in the lifecycle of the UDF that it calls. These methods are available if your operator extends the AbstractUdfStreamOperator, which is the basic class for all operators that execute UDFs.
// initialization phase OPERATOR::setup UDF::setRuntimeContext OPERATOR::initializeState OPERATOR::open UDF::open // processing phase (called on every element/watermark) OPERATOR::processElement UDF::run OPERATOR::processWatermark // checkpointing phase (called asynchronously on every checkpoint) OPERATOR::snapshotState // notify the operator about the end of processing records OPERATOR::finish // termination phase OPERATOR::close UDF::close In a nutshell, the setup() is called to initialize some operator-specific machinery, such as its RuntimeContext and its metric collection data-structures. After this, the initializeState() gives an operator its initial state, and the open() method executes any operator-specific initialization, such as opening the user-defined function in the case of the AbstractUdfStreamOperator.
The initializeState() contains both the logic for initializing the state of the operator during its initial execution (e.g. register any keyed state), and also the logic to retrieve its state from a checkpoint after a failure. More about this on the rest of this page. Now that everything is set, the operator is ready to process incoming data. Incoming elements can be one of the following: input elements, watermark, and checkpoint barriers. Each one of them has a special element for handling it. Elements are processed by the processElement() method, watermarks by the processWatermark(), and checkpoint barriers trigger a checkpoint which invokes (asynchronously) the snapshotState() method, which we describe below. For each incoming element, depending on its type one of the aforementioned methods is called. Note that the processElement() is also the place where the UDF\u0026rsquo;s logic is invoked, e.g. the map() method of your MapFunction.
Finally, in the case of a normal, fault-free termination of the operator (e.g. if the stream is finite and its end is reached), the finish() method is called to perform any final bookkeeping action required by the operator\u0026rsquo;s logic (e.g. flush any buffered data, or emit data to mark end of processing), and the close() is called after that to free any resources held by the operator (e.g. open network connections, io streams, or native memory held by the operator\u0026rsquo;s data).
In the case of a termination due to a failure or due to manual cancellation, the execution jumps directly to the close() and skips any intermediate phases between the phase the operator was in when the failure happened and the close().
Checkpoints: The snapshotState() method of the operator is called asynchronously to the rest of the methods described above whenever a checkpoint barrier is received. Checkpoints are performed during the processing phase, i.e. after the operator is opened and before it is closed. The responsibility of this method is to store the current state of the operator to the specified state backend from where it will be retrieved when the job resumes execution after a failure. Below we include a brief description of Flink\u0026rsquo;s checkpointing mechanism, and for a more detailed discussion on the principles around checkpointing in Flink please read the corresponding documentation: Data Streaming Fault Tolerance.
Task Lifecycle # Following that brief introduction on the operator\u0026rsquo;s main phases, this section describes in more detail how a task calls the respective methods during its execution on a cluster. The sequence of the phases described here is mainly included in the invoke() method of the StreamTask class. The remainder of this document is split into two subsections, one describing the phases during a regular, fault-free execution of a task (see Normal Execution), and (a shorter) one describing the different sequence followed in case the task is cancelled (see Interrupted Execution), either manually, or due some other reason, e.g. an exception thrown during execution.
Normal Execution # The steps a task goes through when executed until completion without being interrupted are illustrated below:
TASK::setInitialState TASK::invoke create basic utils (config, etc) and load the chain of operators setup-operators task-specific-init initialize-operator-states open-operators run finish-operators wait for the final checkponit completed (if enabled) close-operators task-specific-cleanup common-cleanup As shown above, after recovering the task configuration and initializing some important runtime parameters, the very first step for the task is to retrieve its initial, task-wide state. This is done in the setInitialState(), and it is particularly important in two cases:
when the task is recovering from a failure and restarts from the last successful checkpoint when resuming from a savepoint. If it is the first time the task is executed, the initial task state is empty.
After recovering any initial state, the task goes into its invoke() method. There, it first initializes the operators involved in the local computation by calling the setup() method of each one of them and then performs its task-specific initialization by calling the local init() method. By task-specific, we mean that depending on the type of the task (SourceTask, OneInputStreamTask or TwoInputStreamTask, etc), this step may differ, but in any case, here is where the necessary task-wide resources are acquired. As an example, the OneInputStreamTask, which represents a task that expects to have a single input stream, initializes the connection(s) to the location(s) of the different partitions of the input stream that are relevant to the local task.
Having acquired the necessary resources, it is time for the different operators and user-defined functions to acquire their individual state from the task-wide state retrieved above. This is done in the initializeState() method, which calls the initializeState() of each individual operator. This method should be overridden by every stateful operator and should contain the state initialization logic, both for the first time a job is executed, and also for the case when the task recovers from a failure or when using a savepoint.
Now that all operators in the task have been initialized, the open() method of each individual operator is called by the openAllOperators() method of the StreamTask. This method performs all the operational initialization, such as registering any retrieved timers with the timer service. A single task may be executing multiple operators with one consuming the output of its predecessor. In this case, the open() method is called from the last operator, i.e. the one whose output is also the output of the task itself, to the first. This is done so that when the first operator starts processing the task\u0026rsquo;s input, all downstream operators are ready to receive its output.
Consecutive operators in a task are opened from the last to the first. Now the task can resume execution and operators can start processing fresh input data. This is the place where the task-specific run() method is called. This method will run until either there is no more input data (finite stream), or the task is cancelled (manually or not). Here is where the operator specific processElement() and processWatermark() methods are called.
In the case of running till completion, i.e. there is no more input data to process, after exiting from the run() method, the task enters its shutdown process. Initially, the timer service stops registering any new timers (e.g. from fired timers that are being executed), clears all not-yet-started timers, and awaits the completion of currently executing timers. Then the finishAllOperators() notifies the operators involved in the computation by calling the finish() method of each operator. Then, any buffered output data is flushed so that they can be processed by the downstream tasks. Then if final checkpoint is enabled, the task would wait for the final checkpoint completed to ensure operators using two-phase committing have committed all the records. Finally the task tries to clear all the resources held by the operators by calling the close() method of each one. When opening the different operators, we mentioned that the order is from the last to the first. Closing happens in the opposite manner, from first to last.
Consecutive operators in a task are closed from the first to the last. Finally, when all operators have been closed and all their resources freed, the task shuts down its timer service, performs its task-specific cleanup, e.g. cleans all its internal buffers, and then performs its generic task clean up which consists of closing all its output channels and cleaning any output buffers.
Checkpoints: Previously we saw that during initializeState(), and in case of recovering from a failure, the task and all its operators and functions retrieve the state that was persisted to stable storage during the last successful checkpoint before the failure. Checkpoints in Flink are performed periodically based on a user-specified interval, and are performed by a different thread than that of the main task thread. That\u0026rsquo;s why they are not included in the main phases of the task lifecycle. In a nutshell, special elements called CheckpointBarriers are injected periodically by the source tasks of a job in the stream of input data, and travel with the actual data from source to sink. A source task injects these barriers after it is in running mode, and assuming that the CheckpointCoordinator is also running. Whenever a task receives such a barrier, it schedules a task to be performed by the checkpoint thread, which calls the snapshotState() of the operators in the task. Input data can still be received by the task while the checkpoint is being performed, but the data is buffered and only processed and emitted downstream after the checkpoint is successfully completed.
Interrupted Execution # In the previous sections we described the lifecycle of a task that runs till completion. In case the task is cancelled at any point, then the normal execution is interrupted and the only operations performed from that point on are the timer service shutdown, the task-specific cleanup, the closing of the operators, and the general task cleanup, as described above.
Back to top
`}),e.add({id:183,href:"/flink/flink-docs-master/docs/dev/configuration/testing/",title:"Test Dependencies",section:"Project Configuration",content:` Dependencies for Testing # Flink provides utilities for testing your job that you can add as dependencies.
DataStream API Testing # You need to add the following dependencies if you want to develop tests for a job built with the DataStream API:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. testCompile "org.apache.flink:flink-test-utils:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. Among the various test utilities, this module provides MiniCluster, a lightweight configurable Flink cluster runnable in a JUnit test that can directly execute jobs.
For more information on how to use these utilities, check out the section on DataStream API testing
Table API Testing # If you want to test the Table API \u0026amp; SQL programs locally within your IDE, you can add the following dependency, in addition to the aforementioned flink-test-utils:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. testCompile "org.apache.flink:flink-table-test-utils:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. This will automatically bring in the query planner and the runtime, required respectively to plan and execute the queries.
The module flink-table-test-utils has been introduced in Flink 1.15 and is considered experimental. `}),e.add({id:184,href:"/flink/flink-docs-master/docs/deployment/memory/mem_trouble/",title:"Troubleshooting",section:"Memory Configuration",content:` Troubleshooting # IllegalConfigurationException # If you see an IllegalConfigurationException thrown from TaskExecutorProcessUtils or JobManagerProcessUtils, it usually indicates that there is either an invalid configuration value (e.g. negative memory size, fraction that is greater than 1, etc.) or configuration conflicts. Check the documentation chapters or configuration options related to the memory components mentioned in the exception message.
OutOfMemoryError: Java heap space # The exception usually indicates that the JVM Heap is too small. You can try to increase the JVM Heap size by increasing total memory. You can also directly increase task heap memory for TaskManagers or JVM Heap memory for JobManagers.
You can also increase the framework heap memory for TaskManagers, but you should only change this option if you are sure the Flink framework itself needs more memory. OutOfMemoryError: Direct buffer memory # The exception usually indicates that the JVM direct memory limit is too small or that there is a direct memory leak. Check whether user code or other external dependencies use the JVM direct memory and that it is properly accounted for. You can try to increase its limit by adjusting direct off-heap memory. See also how to configure off-heap memory for TaskManagers, JobManagers and the JVM arguments which Flink sets.
OutOfMemoryError: Metaspace # The exception usually indicates that JVM metaspace limit is configured too small. You can try to increase the JVM metaspace option for TaskManagers or JobManagers.
IOException: Insufficient number of network buffers # This is only relevant for TaskManagers.
The exception usually indicates that the size of the configured network memory is not big enough. You can try to increase the network memory by adjusting the following options:
taskmanager.memory.network.min taskmanager.memory.network.max taskmanager.memory.network.fraction Container Memory Exceeded # If a Flink container tries to allocate memory beyond its requested size (Yarn or Kubernetes), this usually indicates that Flink has not reserved enough native memory. You can observe this either by using an external monitoring system or from the error messages when a container gets killed by the deployment environment.
If you encounter this problem in the JobManager process, you can also enable the JVM Direct Memory limit by setting the jobmanager.memory.enable-jvm-direct-memory-limit option to exclude possible JVM Direct Memory leak.
If RocksDBStateBackend is used：
and memory controlling is disabled: You can try to increase the TaskManager\u0026rsquo;s managed memory. and memory controlling is enabled and non-heap memory increases during savepoint or full checkpoints: This may happen due to the glibc memory allocator (see glibc bug). You can try to add the environment variable MALLOC_ARENA_MAX=1 for TaskManagers. Alternatively, you can increase the JVM Overhead.
See also how to configure memory for containers.
`}),e.add({id:185,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/window-tvf/",title:"Windowing TVF",section:"Queries",content:` Windowing table-valued functions (Windowing TVFs) # Batch Streaming
Windows are at the heart of processing infinite streams. Windows split the stream into “buckets” of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink SQL and how the programmer can benefit to the maximum from its offered functionality.
Apache Flink provides several window table-valued functions (TVF) to divide the elements of your table into windows, including:
Tumble Windows Hop Windows Cumulate Windows Session Windows (will be supported soon) Note that each element can logically belong to more than one window, depending on the windowing table-valued function you use. For example, HOP windowing creates overlapping windows wherein a single element can be assigned to multiple windows.
Windowing TVFs are Flink defined Polymorphic Table Functions (abbreviated PTF). PTF is part of the SQL 2016 standard, a special table-function, but can have a table as a parameter. PTF is a powerful feature to change the shape of a table. Because PTFs are used semantically like tables, their invocation occurs in a FROM clause of a SELECT statement.
Windowing TVFs is a replacement of legacy Grouped Window Functions. Windowing TVFs is more SQL standard compliant and more powerful to support complex window-based computations, e.g. Window TopN, Window Join. However, Grouped Window Functions can only support Window Aggregation.
See more how to apply further computations based on windowing TVF:
Window Aggregation Window TopN Window Join Window Deduplication Window Functions # Apache Flink provides 3 built-in windowing TVFs: TUMBLE, HOP and CUMULATE. The return value of windowing TVF is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. In streaming mode, the \u0026ldquo;window_time\u0026rdquo; field is a time attributes of the window. In batch mode, the \u0026ldquo;window_time\u0026rdquo; field is an attribute of type TIMESTAMP or TIMESTAMP_LTZ based on input time field type. The \u0026ldquo;window_time\u0026rdquo; field can be used in subsequent time-based operations, e.g. another windowing TVF, or interval joins, over aggregations. The value of window_time always equal to window_end - 1ms.
TUMBLE # The TUMBLE function assigns each element to a window of specified window size. Tumbling windows have a fixed size and do not overlap. For example, suppose you specify a tumbling window with a size of 5 minutes. In that case, Flink will evaluate the current window, and a new window started every five minutes, as illustrated by the following figure.
The TUMBLE function assigns a window for each row of a relation based on a time attribute field. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of TUMBLE is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after window TVF.
TUMBLE function takes three required parameters, one optional parameter:
TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ]) data: is a table parameter that can be any relation with a time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to tumbling windows. size: is a duration specifying the width of the tumbling windows. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+ | bidtime | price | item | +------------------+-------+------+ | 2020-04-15 08:05 | 4.00 | C | | 2020-04-15 08:07 | 2.00 | A | | 2020-04-15 08:09 | 5.00 | D | | 2020-04-15 08:11 | 3.00 | B | | 2020-04-15 08:13 | 1.00 | E | | 2020-04-15 08:17 | 6.00 | F | +------------------+-------+------+ Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the tumbling windowed table Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
HOP # The HOP function assigns elements to windows of fixed length. Like a TUMBLE windowing function, the size of the windows is configured by the window size parameter. An additional window slide parameter controls how frequently a hopping window is started. Hence, hopping windows can be overlapping if the slide is smaller than the window size. In this case, elements are assigned to multiple windows. Hopping windows are also known as \u0026ldquo;sliding windows\u0026rdquo;.
For example, you could have windows of size 10 minutes that slides by 5 minutes. With this, you get every 5 minutes a window that contains the events that arrived during the last 10 minutes, as depicted by the following figure.
The HOP function assigns windows that cover rows within the interval of size and shifting every slide based on a time attribute field. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of HOP is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after windowing TVF.
HOP takes four required parameters, one optional parameter:
HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ]) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to hopping windows. slide: is a duration specifying the duration between the start of sequential hopping windows size: is a duration specifying the width of the hopping windows. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
\u0026gt; SELECT * FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first \u0026gt; SELECT * FROM TABLE( HOP( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SLIDE =\u0026gt; INTERVAL \u0026#39;5\u0026#39; MINUTES, SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:15 | 2020-04-15 08:25 | 2020-04-15 08:24:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the hopping windowed table \u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | | 2020-04-15 08:15 | 2020-04-15 08:25 | 6.00 | +------------------+------------------+-------+ CUMULATE # Cumulating windows are very useful in some scenarios, such as tumbling windows with early firing in a fixed window interval. For example, a daily dashboard draws cumulative UVs from 00:00 to every minute, the UV at 10:00 represents the total number of UV from 00:00 to 10:00. This can be easily and efficiently implemented by CUMULATE windowing.
The CUMULATE function assigns elements to windows that cover rows within an initial interval of step size and expand to one more step size (keep window start fixed) every step until the max window size. You can think CUMULATE function as applying TUMBLE windowing with max window size first, and split each tumbling windows into several windows with same window start and window ends of step-size difference. So cumulating windows do overlap and don\u0026rsquo;t have a fixed size.
For example, you could have a cumulating window for 1 hour step and 1 day max size, and you will get windows: [00:00, 01:00), [00:00, 02:00), [00:00, 03:00), \u0026hellip;, [00:00, 24:00) for every day.
The CUMULATE functions assigns windows based on a time attribute column. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of CUMULATE is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after window TVF.
CUMULATE takes four required parameters, one optional parameter:
CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to cumulating windows. step: is a duration specifying the increased window size between the end of sequential cumulating windows. size: is a duration specifying the max width of the cumulating windows. size must be an integral multiple of step. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
\u0026gt; SELECT * FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first \u0026gt; SELECT * FROM TABLE( CUMULATE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), STEP =\u0026gt; INTERVAL \u0026#39;2\u0026#39; MINUTES, SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:06 | 2020-04-15 08:05:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:12 | 2020-04-15 08:11:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the cumulating windowed table \u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:06 | 4.00 | | 2020-04-15 08:00 | 2020-04-15 08:08 | 6.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:12 | 3.00 | | 2020-04-15 08:10 | 2020-04-15 08:14 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:16 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Window Offset # Offset is an optional parameter which could be used to change the window assignment. It could be positive duration and negative duration. Default values for window offset is 0. The same record maybe assigned to the different window if set different offset value. For example, which window would be assigned to for a record with timestamp 2021-06-30 00:00:04 for a Tumble window with 10 MINUTE as size?
If offset value is -16 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset value is -6 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset is -4 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). If offset is 0, the record assigns to window [2021-06-30 00:00:00, 2021-06-30 00:10:00). If offset is 4 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset is 6 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). If offset is 16 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). We could find that, some windows offset parameters may have same effect on the assignment of windows. In the above case, -16 MINUTE, -6 MINUTE and 4 MINUTE have same effect for a Tumble window with 10 MINUTE as size. Note: The effect of window offset is just for updating window assignment, it has no effect on Watermark.
We show an example to describe how to use offset in Tumble window in the following SQL.
-- NOTE: Currently Flink doesn\u0026#39;t support evaluating individual window table-valued function, -- window table-valued function should be used with aggregate operation, -- this example is just used for explaining the syntax and the data produced by table-valued function. Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES, INTERVAL \u0026#39;1\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES, OFFSET =\u0026gt; INTERVAL \u0026#39;1\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the tumbling windowed table Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES, INTERVAL \u0026#39;1\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:01 | 2020-04-15 08:11 | 11.00 | | 2020-04-15 08:11 | 2020-04-15 08:21 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Back to top
`}),e.add({id:186,href:"/flink/flink-docs-master/docs/libs/gelly/bipartite_graph/",title:"Bipartite Graph",section:"Graphs",content:` Bipartite Graph # Bipartite Graph currently only supported in Gelly Java API. Bipartite Graph # A bipartite graph (also called a two-mode graph) is a type of graph where vertices are separated into two disjoint sets. These sets are usually called top and bottom vertices. An edge in this graph can only connect vertices from opposite sets (i.e. bottom vertex to top vertex) and cannot connect two vertices in the same set.
These graphs have wide application in practice and can be a more natural choice for particular domains. For example to represent authorship of scientific papers top vertices can represent scientific papers while bottom nodes will represent authors. Naturally an edge between a top and a bottom nodes would represent an authorship of a particular scientific paper. Another common example for applications of bipartite graphs is relationships between actors and movies. In this case an edge represents that a particular actor played in a movie.
Bipartite graphs are used instead of regular graphs (one-mode) for the following practical reasons:
They preserve more information about a connection between vertices. For example instead of a single link between two researchers in a graph that represents that they authored a paper together a bipartite graph preserves the information about what papers they authored Bipartite graphs can encode the same information more compactly than one-mode graphs Graph Representation # A BipartiteGraph is represented by:
A DataSet of top nodes A DataSet of bottom nodes A DataSet of edges between top and bottom nodes As in the Graph class nodes are represented by the Vertex type and the same rules apply to its types and values.
The graph edges are represented by the BipartiteEdge type. A BipartiteEdge is defined by a top ID (the ID of the top Vertex), a bottom ID (the ID of the bottom Vertex) and an optional value. The main difference between the Edge and BipartiteEdge is that IDs of nodes it links can be of different types. Edges with no value have a NullValue value type.
Java BipartiteEdge\u0026lt;Long, String, Double\u0026gt; e = new BipartiteEdge\u0026lt;Long, String, Double\u0026gt;(1L, \u0026#34;id1\u0026#34;, 0.5); Double weight = e.getValue(); // weight = 0.5 Scala // Scala API is not yet supported Back to top
Graph Creation # You can create a BipartiteGraph in the following ways:
from a DataSet of top vertices, a DataSet of bottom vertices and a DataSet of edges: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; topVertices = ...; DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; bottomVertices = ...; DataSet\u0026lt;Edge\u0026lt;String, String, Double\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;String, String, Long, Long, Double\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); Scala // Scala API is not yet supported Graph Transformations # Projection: Projection is a common operation for bipartite graphs that converts a bipartite graph into a regular graph. There are two types of projections: top and bottom projections. Top projection preserves only top nodes in the result graph and creates a link between them in a new graph only if there is an intermediate bottom node both top nodes connect to in the original graph. Bottom projection is the opposite to top projection, i.e. only preserves bottom nodes and connects a pair of nodes if they are connected in the original graph. Gelly supports two sub-types of projections: simple projections and full projections. The only difference between them is what data is associated with edges in the result graph.
In the case of a simple projection each node in the result graph contains a pair of values of bipartite edges that connect nodes in the original graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Vertices (1, \u0026#34;top1\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; topVertices = ...; // Vertices (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; bottomVertices = ...; // Edge that connect vertex 2 to vertex 1 and vertex 4 to vertex 1: // (1, 2, \u0026#34;1-2-edge\u0026#34;); (1, 4, \u0026#34;1-4-edge\u0026#34;) DataSet\u0026lt;Edge\u0026lt;Long, Long, String\u0026gt;\u0026gt; edges = ...; BipartiteGraph\u0026lt;Long, Long, String, String, String\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); // Result graph with two vertices: // (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) // // and one edge that contains ids of bottom edges and a tuple with // values of intermediate edges in the original bipartite graph: // (2, 4, (\u0026#34;1-2-edge\u0026#34;, \u0026#34;1-4-edge\u0026#34;)) Graph\u0026lt;Long, String, Tuple2\u0026lt;String, String\u0026gt;\u0026gt; graph bipartiteGraph.projectionBottomSimple(); Scala // Scala API is not yet supported Full projection preserves all the information about the connection between two vertices and stores it in Projection instances. This includes value and id of an intermediate vertex, source and target vertex values and source and target edge values:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Vertices (1, \u0026#34;top1\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; topVertices = ...; // Vertices (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; bottomVertices = ...; // Edge that connect vertex 2 to vertex 1 and vertex 4 to vertex 1: // (1, 2, \u0026#34;1-2-edge\u0026#34;); (1, 4, \u0026#34;1-4-edge\u0026#34;) DataSet\u0026lt;Edge\u0026lt;Long, Long, String\u0026gt;\u0026gt; edges = ...; BipartiteGraph\u0026lt;Long, Long, String, String, String\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); // Result graph with two vertices: // (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) // and one edge that contains ids of bottom edges and a tuple that // contains id and value of the intermediate edge, values of connected vertices // and values of intermediate edges in the original bipartite graph: // (2, 4, (1, \u0026#34;top1\u0026#34;, \u0026#34;bottom2\u0026#34;, \u0026#34;bottom4\u0026#34;, \u0026#34;1-2-edge\u0026#34;, \u0026#34;1-4-edge\u0026#34;)) Graph\u0026lt;String, String, Projection\u0026lt;Long, String, String, String\u0026gt;\u0026gt; graph bipartiteGraph.projectionBottomFull(); Scala // Scala API is not yet supported Back to top
`}),e.add({id:187,href:"/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/",title:"Data Types \u0026 Serialization",section:"State \u0026 Fault Tolerance",content:""}),e.add({id:188,href:"/flink/flink-docs-master/docs/deployment/",title:"Deployment",section:"Docs",content:" "}),e.add({id:189,href:"/flink/flink-docs-master/docs/connectors/table/elasticsearch/",title:"Elasticsearch",section:"Table API Connectors",content:` Elasticsearch SQL Connector # Sink: Batch Sink: Streaming Append \u0026amp; Upsert Mode
The Elasticsearch connector allows for writing into an index of the Elasticsearch engine. This document describes how to setup the Elasticsearch Connector to run SQL queries against Elasticsearch.
The connector can operate in upsert mode for exchanging UPDATE/DELETE messages with the external system using the primary key defined on the DDL.
If no primary key is defined on the DDL, the connector can only operate in append mode for exchanging INSERT only messages with external system.
Dependencies # In order to use the Elasticsearch connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Elasticsearch version Maven dependency SQL Client JAR 6.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch6\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 7.x and later versions \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch7\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The Elasticsearch connector is not part of the binary distribution. See how to link with it for cluster execution here.
How to create an Elasticsearch table # The example below shows how to create an Elasticsearch sink table:
CREATE TABLE myUserTable ( user_id STRING, user_name STRING, uv BIGINT, pv BIGINT, PRIMARY KEY (user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;elasticsearch-7\u0026#39;, \u0026#39;hosts\u0026#39; = \u0026#39;http://localhost:9200\u0026#39;, \u0026#39;index\u0026#39; = \u0026#39;users\u0026#39; ); Connector Options # Option Required Forwarded Default Type Description connector required no (none) String Specify what connector to use, valid values are: elasticsearch-6: connect to Elasticsearch 6.x cluster. elasticsearch-7: connect to Elasticsearch 7.x cluster. hosts required yes (none) String One or more Elasticsearch hosts to connect to, e.g. 'http://host_name:9092;http://host_name:9093'. index required yes (none) String Elasticsearch index for every record. Can be a static index (e.g. 'myIndex') or a dynamic index (e.g. 'index-{log_ts|yyyy-MM-dd}'). See the following Dynamic Index section for more details. document-type required in 6.x yes in 6.x (none) String Elasticsearch document type. Not necessary anymore in elasticsearch-7. document-id.key-delimiter optional yes _ String Delimiter for composite keys ("_" by default), e.g., "\$" would result in IDs "KEY1\$KEY2\$KEY3". username optional yes (none) String Username used to connect to Elasticsearch instance. Please notice that Elasticsearch doesn't pre-bundled security feature, but you can enable it by following the guideline to secure an Elasticsearch cluster. password optional yes (none) String Password used to connect to Elasticsearch instance. If username is configured, this option must be configured with non-empty string as well. failure-handler optional yes fail String Failure handling strategy in case a request to Elasticsearch fails. Valid strategies are: fail: throws an exception if a request fails and thus causes a job failure. ignore: ignores failures and drops the request. retry-rejected: re-adds requests that have failed due to queue capacity saturation. custom class name: for failure handling with a ActionRequestFailureHandler subclass. sink.flush-on-checkpoint optional true Boolean Flush on checkpoint or not. When disabled, a sink will not wait for all pending action requests to be acknowledged by Elasticsearch on checkpoints. Thus, a sink does NOT provide any strong guarantees for at-least-once delivery of action requests. sink.bulk-flush.max-actions optional yes 1000 Integer Maximum number of buffered actions per bulk request. Can be set to '0' to disable it. sink.bulk-flush.max-size optional yes 2mb MemorySize Maximum size in memory of buffered actions per bulk request. Must be in MB granularity. Can be set to '0' to disable it. sink.bulk-flush.interval optional yes 1s Duration The interval to flush buffered actions. Can be set to '0' to disable it. Note, both 'sink.bulk-flush.max-size' and 'sink.bulk-flush.max-actions' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions. sink.bulk-flush.backoff.strategy optional yes DISABLED String Specify how to perform retries if any flush actions failed due to a temporary request error. Valid strategies are: DISABLED: no retry performed, i.e. fail after the first request error. CONSTANT: wait for backoff delay between retries. EXPONENTIAL: initially wait for backoff delay and increase exponentially between retries. sink.bulk-flush.backoff.max-retries optional yes (none) Integer Maximum number of backoff retries. sink.bulk-flush.backoff.delay optional yes (none) Duration Delay between each backoff attempt. For CONSTANT backoff, this is simply the delay between each retry. For EXPONENTIAL backoff, this is the initial base delay. connection.path-prefix optional yes (none) String Prefix string to be added to every REST communication, e.g., '/v1'. format optional no json String Elasticsearch connector supports to specify a format. The format must produce a valid json document. By default uses built-in 'json' format. Please refer to JSON Format page for more details. Features # Key Handling # The Elasticsearch sink can work in either upsert mode or append mode, depending on whether a primary key is defined. If a primary key is defined, the Elasticsearch sink works in upsert mode which can consume queries containing UPDATE/DELETE messages. If a primary key is not defined, the Elasticsearch sink works in append mode which can only consume queries containing INSERT only messages.
In the Elasticsearch connector, the primary key is used to calculate the Elasticsearch document id, which is a string of up to 512 bytes. It cannot have whitespaces. The Elasticsearch connector generates a document ID string for every row by concatenating all primary key fields in the order defined in the DDL using a key delimiter specified by document-id.key-delimiter. Certain types are not allowed as a primary key field as they do not have a good string representation, e.g. BYTES, ROW, ARRAY, MAP, etc. If no primary key is specified, Elasticsearch will generate a document id automatically.
See CREATE TABLE DDL for more details about the PRIMARY KEY syntax.
Dynamic Index # The Elasticsearch sink supports both static index and dynamic index.
If you want to have a static index, the index option value should be a plain string, e.g. 'myusers', all the records will be consistently written into \u0026ldquo;myusers\u0026rdquo; index.
If you want to have a dynamic index, you can use {field_name} to reference a field value in the record to dynamically generate a target index. You can also use '{field_name|date_format_string}' to convert a field value of TIMESTAMP/DATE/TIME type into the format specified by the date_format_string. The date_format_string is compatible with Java\u0026rsquo;s DateTimeFormatter. For example, if the option value is 'myusers-{log_ts|yyyy-MM-dd}', then a record with log_ts field value 2020-03-27 12:25:55 will be written into \u0026ldquo;myusers-2020-03-27\u0026rdquo; index.
You can also use '{now()|date_format_string}' to convert the current system time to the format specified by date_format_string. The corresponding time type of now() is TIMESTAMP_WITH_LTZ. When formatting the system time as a string, the time zone configured in the session through table.local-time-zone will be used. You can use NOW(), now(), CURRENT_TIMESTAMP, current_timestamp.
NOTE: When using the dynamic index generated by the current system time, for changelog stream, there is no guarantee that the records with the same primary key can generate the same index name. Therefore, the dynamic index based on the system time can only support append only stream.
Data Type Mapping # Elasticsearch stores document in a JSON string. So the data type mapping is between Flink data type and JSON data type. Flink uses built-in 'json' format for Elasticsearch connector. Please refer to JSON Format page for more type mapping details.
Back to top
`}),e.add({id:190,href:"/flink/flink-docs-master/docs/learn-flink/fault_tolerance/",title:"Fault Tolerance",section:"Learn Flink",content:` Fault Tolerance via State Snapshots # State Backends # The keyed state managed by Flink is a sort of sharded, key/value store, and the working copy of each item of keyed state is kept somewhere local to the taskmanager responsible for that key. Operator state is also local to the machine(s) that need(s) it.
This state that Flink manages is stored in a state backend. Two implementations of state backends are available \u0026ndash; one based on RocksDB, an embedded key/value store that keeps its working state on disk, and another heap-based state backend that keeps its working state in memory, on the Java heap.
Name Working State Snapshotting EmbeddedRocksDBStateBackend Local disk (tmp dir) Full / Incremental Supports state larger than available memory Rule of thumb: 10x slower than heap-based backends HashMapStateBackend JVM Heap Full Fast, requires large heap Subject to GC When working with state kept in a heap-based state backend, accesses and updates involve reading and writing objects on the heap. But for objects kept in the EmbeddedRocksDBStateBackend, accesses and updates involve serialization and deserialization, and so are much more expensive. But the amount of state you can have with RocksDB is limited only by the size of the local disk. Note also that only the EmbeddedRocksDBStateBackend is able to do incremental snapshotting, which is a significant benefit for applications with large amounts of slowly changing state.
Both of these state backends are able to do asynchronous snapshotting, meaning that they can take a snapshot without impeding the ongoing stream processing.
Checkpoint Storage # Flink periodically takes persistent snapshots of all the state in every operator and copies these snapshots somewhere more durable, such as a distributed file system. In the event of the failure, Flink can restore the complete state of your application and resume processing as though nothing had gone wrong.
The location where these snapshots are stored is defined via the jobs checkpoint storage. Two implementations of checkpoint storage are available - one that persists its state snapshots to a distributed file system, and another that users the JobManager\u0026rsquo;s heap.
Name State Backup FileSystemCheckpointStorage Distributed file system Supports very large state size Highly durable Recommended for production deployments JobManagerCheckpointStorage JobManager JVM Heap Good for testing and experimentation with small state (locally) Back to top
State Snapshots # Definitions # Snapshot \u0026ndash; a generic term referring to a global, consistent image of the state of a Flink job. A snapshot includes a pointer into each of the data sources (e.g., an offset into a file or Kafka partition), as well as a copy of the state from each of the job\u0026rsquo;s stateful operators that resulted from having processed all of the events up to those positions in the sources. Checkpoint \u0026ndash; a snapshot taken automatically by Flink for the purpose of being able to recover from faults. Checkpoints can be incremental, and are optimized for being restored quickly. Externalized Checkpoint \u0026ndash; normally checkpoints are not intended to be manipulated by users. Flink retains only the n-most-recent checkpoints (n being configurable) while a job is running, and deletes them when a job is cancelled. But you can configure them to be retained instead, in which case you can manually resume from them. Savepoint \u0026ndash; a snapshot triggered manually by a user (or an API call) for some operational purpose, such as a stateful redeploy/upgrade/rescaling operation. Savepoints are always complete, and are optimized for operational flexibility. How does State Snapshotting Work? # Flink uses a variant of the Chandy-Lamport algorithm known as asynchronous barrier snapshotting.
When a task manager is instructed by the checkpoint coordinator (part of the job manager) to begin a checkpoint, it has all of the sources record their offsets and insert numbered checkpoint barriers into their streams. These barriers flow through the job graph, indicating the part of the stream before and after each checkpoint.
Checkpoint n will contain the state of each operator that resulted from having consumed every event before checkpoint barrier n, and none of the events after it.
As each operator in the job graph receives one of these barriers, it records its state. Operators with two input streams (such as a CoProcessFunction) perform barrier alignment so that the snapshot will reflect the state resulting from consuming events from both input streams up to (but not past) both barriers.
Flink\u0026rsquo;s state backends use a copy-on-write mechanism to allow stream processing to continue unimpeded while older versions of the state are being asynchronously snapshotted. Only when the snapshots have been durably persisted will these older versions of the state be garbage collected.
Exactly Once Guarantees # When things go wrong in a stream processing application, it is possible to have either lost, or duplicated results. With Flink, depending on the choices you make for your application and the cluster you run it on, any of these outcomes is possible:
Flink makes no effort to recover from failures (at most once) Nothing is lost, but you may experience duplicated results (at least once) Nothing is lost or duplicated (exactly once) Given that Flink recovers from faults by rewinding and replaying the source data streams, when the ideal situation is described as exactly once this does not mean that every event will be processed exactly once. Instead, it means that every event will affect the state being managed by Flink exactly once.
Barrier alignment is only needed for providing exactly once guarantees. If you don\u0026rsquo;t need this, you can gain some performance by configuring Flink to use CheckpointingMode.AT_LEAST_ONCE, which has the effect of disabling barrier alignment.
Exactly Once End-to-end # To achieve exactly once end-to-end, so that every event from the sources affects the sinks exactly once, the following must be true:
your sources must be replayable, and your sinks must be transactional (or idempotent) Back to top
Hands-on # The Flink Operations Playground includes a section on Observing Failure \u0026amp; Recovery.
Back to top
Further Reading # Stateful Stream Processing State Backends Fault Tolerance Guarantees of Data Sources and Sinks Enabling and Configuring Checkpointing Checkpoints Savepoints Tuning Checkpoints and Large State Monitoring Checkpointing Task Failure Recovery Back to top
`}),e.add({id:191,href:"/flink/flink-docs-master/docs/dev/table/sql/insert/",title:"INSERT Statement",section:"SQL",content:` INSERT Statement # INSERT statements are used to add rows to a table.
Run an INSERT statement # Java Single INSERT statement can be executed through the executeSql() method of the TableEnvironment. The executeSql() method for INSERT statement will submit a Flink job immediately, and return a TableResult instance which associates the submitted job. Multiple INSERT statements can be executed through the addInsertSql() method of the StatementSet which can be created by the TableEnvironment.createStatementSet() method. The addInsertSql() method is a lazy execution, they will be executed only when StatementSet.execute() is invoked.
The following examples show how to run a single INSERT statement in TableEnvironment, run multiple INSERT statements in StatementSet.
Scala Single INSERT statement can be executed through the executeSql() method of the TableEnvironment. The executeSql() method for INSERT statement will submit a Flink job immediately, and return a TableResult instance which associates the submitted job. Multiple INSERT statements can be executed through the addInsertSql() method of the StatementSet which can be created by the TableEnvironment.createStatementSet() method. The addInsertSql() method is a lazy execution, they will be executed only when StatementSet.execute() is invoked.
The following examples show how to run a single INSERT statement in TableEnvironment, run multiple INSERT statements in StatementSet.
Python Single INSERT statement can be executed through the execute_sql() method of the TableEnvironment. The execute_sql() method for INSERT statement will submit a Flink job immediately, and return a TableResult instance which associates the submitted job. Multiple INSERT statements can be executed through the add_insert_sql() method of the StatementSet which can be created by the TableEnvironment.create_statement_set() method. The add_insert_sql() method is a lazy execution, they will be executed only when StatementSet.execute() is invoked.
The following examples show how to run a single INSERT statement in TableEnvironment, run multiple INSERT statements in StatementSet.
SQL CLI Single INSERT statement can be executed in SQL CLI.
The following examples show how to run a single INSERT statement in SQL CLI.
Java TableEnvironment tEnv = TableEnvironment.create(...); // register a source table named \u0026#34;Orders\u0026#34; and a sink table named \u0026#34;RubberOrders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product VARCHAR, amount INT) WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;); // run a single INSERT query on the registered source table and emit the result to registered sink table TableResult tableResult1 = tEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // get job status through TableResult System.out.println(tableResult1.getJobClient().get().getJobStatus()); //---------------------------------------------------------------------------- // register another sink table named \u0026#34;GlassOrders\u0026#34; for multiple INSERT queries tEnv.executeSql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;); // run multiple INSERT queries on the registered source table and emit the result to registered sink tables StatementSet stmtSet = tEnv.createStatementSet(); // only single INSERT query can be accepted by \`addInsertSql\` method stmtSet.addInsertSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); stmtSet.addInsertSql( \u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;); // execute all statements together TableResult tableResult2 = stmtSet.execute(); // get job status through TableResult System.out.println(tableResult2.getJobClient().get().getJobStatus()); Scala val tEnv = TableEnvironment.create(...) // register a source table named \u0026#34;Orders\u0026#34; and a sink table named \u0026#34;RubberOrders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) // run a single INSERT query on the registered source table and emit the result to registered sink table val tableResult1 = tEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // get job status through TableResult println(tableResult1.getJobClient().get().getJobStatus()) //---------------------------------------------------------------------------- // register another sink table named \u0026#34;GlassOrders\u0026#34; for multiple INSERT queries tEnv.executeSql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;) // run multiple INSERT queries on the registered source table and emit the result to registered sink tables val stmtSet = tEnv.createStatementSet() // only single INSERT query can be accepted by \`addInsertSql\` method stmtSet.addInsertSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) stmtSet.addInsertSql( \u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;) // execute all statements together val tableResult2 = stmtSet.execute() // get job status through TableResult println(tableResult2.getJobClient().get().getJobStatus()) Python table_env = TableEnvironment.create(...) # register a source table named \u0026#34;Orders\u0026#34; and a sink table named \u0026#34;RubberOrders\u0026#34; table_env.execute_sql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) table_env.execute_sql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) # run a single INSERT query on the registered source table and emit the result to registered sink table table_result1 = table_env \\ .execute_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) # get job status through TableResult print(table_result1get_job_client().get_job_status()) #---------------------------------------------------------------------------- # register another sink table named \u0026#34;GlassOrders\u0026#34; for multiple INSERT queries table_env.execute_sql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;) # run multiple INSERT queries on the registered source table and emit the result to registered sink tables stmt_set = table_env.create_statement_set() # only single INSERT query can be accepted by \`add_insert_sql\` method stmt_set \\ .add_insert_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) stmt_set \\ .add_insert_sql(\u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;) # execute all statements together table_result2 = stmt_set.execute() # get job status through TableResult print(table_result2.get_job_client().get_job_status()) SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; Orders RubberOrders Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... [INFO] Table update statement has been successfully submitted to the cluster: Back to top
Insert from select queries # Query Results can be inserted into tables by using the insert clause.
Syntax # [EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name [PARTITION part_spec] [column_list] select_statement part_spec: (part_col_name1=val1 [, part_col_name2=val2, ...]) column_list: (col_name1 [, column_name2, ...]) OVERWRITE
INSERT OVERWRITE will overwrite any existing data in the table or partition. Otherwise, new data is appended.
PARTITION
PARTITION clause should contain static partition columns of this inserting.
COLUMN LIST
Given a table T(a INT, b INT, c INT), Flink supports INSERT INTO T(c, b) SELECT x, y FROM S. The expectation is that \u0026lsquo;x\u0026rsquo; is written to column \u0026lsquo;c\u0026rsquo; and \u0026lsquo;y\u0026rsquo; is written to column \u0026lsquo;b\u0026rsquo; and \u0026lsquo;a\u0026rsquo; is set to NULL (assuming column \u0026lsquo;a\u0026rsquo; is nullable).
Examples # -- Creates a partitioned table CREATE TABLE country_page_view (user STRING, cnt INT, date STRING, country STRING) PARTITIONED BY (date, country) WITH (...) -- Appends rows into the static partition (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- Key word EXECUTE can be added at the beginning of Insert to indicate explicitly that we are going to execute the statement, -- it is equivalent to Statement without the key word. EXECUTE INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- Appends rows into partition (date, country), where date is static partition with value \u0026#39;2019-8-30\u0026#39;, -- country is dynamic partition whose value is dynamic determined by each row. INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;) SELECT user, cnt, country FROM page_view_source; -- Overwrites rows into static partition (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERT OVERWRITE country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- Overwrites rows into partition (date, country), where date is static partition with value \u0026#39;2019-8-30\u0026#39;, -- country is dynamic partition whose value is dynamic determined by each row. INSERT OVERWRITE country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;) SELECT user, cnt, country FROM page_view_source; -- Appends rows into the static partition (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) -- the column cnt is set to NULL INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) (user) SELECT user FROM page_view_source; Insert values into tables # The INSERT\u0026hellip;VALUES statement can be used to insert data into tables directly from SQL.
Syntax # [EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name VALUES values_row [, values_row ...] values_row: (val1 [, val2, ...]) OVERWRITE
INSERT OVERWRITE will overwrite any existing data in the table. Otherwise, new data is appended.
Examples # CREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...); INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); Insert into multiple tables # The STATEMENT SET can be used to insert data into multiple tables in a statement.
Syntax # EXECUTE STATEMENT SET BEGIN insert_statement; ... insert_statement; END; insert_statement: \u0026lt;insert_from_select\u0026gt;|\u0026lt;insert_from_values\u0026gt; Examples # CREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...); EXECUTE STATEMENT SET BEGIN INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); END; Back to top
`}),e.add({id:192,href:"/flink/flink-docs-master/docs/connectors/table/formats/maxwell/",title:"Maxwell",section:"Formats",content:` Maxwell Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Maxwell is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into Kafka, Kinesis and other streaming connectors. Maxwell provides a unified format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Maxwell JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as
synchronizing incremental data from databases to other systems auditing logs real-time materialized views on databases temporal join changing history of a database table and so on. Flink also supports to encode the INSERT/UPDATE/DELETE messages in Flink SQL as Maxwell JSON messages, and emit to external systems like Kafka. However, currently Flink can\u0026rsquo;t combine UPDATE_BEFORE and UPDATE_AFTER into a single UPDATE message. Therefore, Flink encodes UPDATE_BEFORE and UDPATE_AFTER as DELETE and INSERT Maxwell messages.
Dependencies # In order to use the Maxwell format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in Note: please refer to Maxwell documentation about how to synchronize changelog to Kafka topics with Maxwell JSON.
How to use Maxwell format # Maxwell provides a unified format for changelog, here is a simple example for an update operation captured from a MySQL products table in JSON format:
{ \u0026#34;database\u0026#34;:\u0026#34;test\u0026#34;, \u0026#34;table\u0026#34;:\u0026#34;e\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;insert\u0026#34;, \u0026#34;ts\u0026#34;:1477053217, \u0026#34;xid\u0026#34;:23396, \u0026#34;commit\u0026#34;:true, \u0026#34;position\u0026#34;:\u0026#34;master.000006:800911\u0026#34;, \u0026#34;server_id\u0026#34;:23042, \u0026#34;thread_id\u0026#34;:108, \u0026#34;primary_key\u0026#34;: [1, \u0026#34;2016-10-21 05:33:37.523000\u0026#34;], \u0026#34;primary_key_columns\u0026#34;: [\u0026#34;id\u0026#34;, \u0026#34;c\u0026#34;], \u0026#34;data\u0026#34;:{ \u0026#34;id\u0026#34;:111, \u0026#34;name\u0026#34;:\u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;:\u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;:5.15 }, \u0026#34;old\u0026#34;:{ \u0026#34;weight\u0026#34;:5.18, } } Note: please refer to Maxwell documentation about the meaning of each fields.
The MySQL products table has 4 columns (id, name, description and weight). The above JSON message is an update change event on the products table where the weight value of the row with id = 111 is changed from 5.18 to 5.15. Assuming this messages is synchronized to Kafka topic products_binlog, then we can use the following DDL to consume this topic and interpret the change events.
CREATE TABLE topic_products ( -- schema is totally the same to the MySQL \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;maxwell-json\u0026#39; ) After registering the topic as a Flink table, then you can consume the Maxwell messages as a changelog source.
-- a real-time materialized view on the MySQL \u0026#34;products\u0026#34; -- which calculate the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of MySQL \u0026#34;products\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format. Key Data Type Description database STRING NULL The originating database. Corresponds to the database field in the Maxwell record if available. table STRING NULL The originating database table. Corresponds to the table field in the Maxwell record if available. primary-key-columns ARRAY\u0026lt;STRING\u0026gt; NULL Array of primary key names. Corresponds to the primary_key_columns field in the Maxwell record if available. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts field in the Maxwell record. The following example shows how to access Maxwell metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_database STRING METADATA FROM \u0026#39;value.database\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, origin_primary_key_columns ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.primary-key-columns\u0026#39; VIRTUAL, origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;maxwell-json\u0026#39; ); Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'maxwell-json'. maxwell-json.ignore-parse-errors optional false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. maxwell-json.timestamp-format.standard optional 'SQL' String Specify the input and output timestamp format. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input timestamp in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g '2020-12-30 12:13:14.123' and output timestamp in the same format. Option 'ISO-8601'will parse input timestamp in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g '2020-12-30T12:13:14.123' and output timestamp in the same format. maxwell-json.map-null-key.mode optional 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by maxwell-json.map-null-key.literal option. maxwell-json.map-null-key.literal optional 'null' String Specify string literal to replace null key when 'maxwell-json.map-null-key.mode' is LITERAL. maxwell-json.encode.decimal-as-plain-number optional false Boolean Encode all decimals as plain numbers instead of possible scientific notations. By default, decimals may be written using scientific notation. For example, 0.000000027 is encoded as 2.7E-8 by default, and will be written as 0.000000027 if set this option to true. Caveats # Duplicate change events # The Maxwell application allows to deliver every change event exactly-once. Flink works pretty well when consuming Maxwell produced events in this situation. If Maxwell application works in at-least-once delivery, it may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions. Thus, it is recommended to set job configuration table.exec.source.cdc-events-duplicate to true and define PRIMARY KEY on the source in this situation. Framework will generate an additional stateful operator, and use the primary key to deduplicate the change events and produce a normalized changelog stream.
Data Type Mapping # Currently, the Maxwell format uses JSON for serialization and deserialization. Please refer to JSON Format documentation for more details about the data type mapping.
`}),e.add({id:193,href:"/flink/flink-docs-master/docs/deployment/metric_reporters/",title:"Metric Reporters",section:"Deployment",content:` Metric Reporters # Flink allows reporting metrics to external systems. For more information about Flink\u0026rsquo;s metric system go to the metric system documentation.
Metrics can be exposed to an external system by configuring one or several reporters in conf/flink-conf.yaml. These reporters will be instantiated on each job and task manager when they are started.
Below is a list of parameters that are generally applicable to all reporters. All properties are configured by setting metrics.reporter.\u0026lt;reporter_name\u0026gt;.\u0026lt;property\u0026gt; in the configuration. Reporters may additionally offer implementation-specific parameters, which are documented in the respective reporter\u0026rsquo;s section.
Key Default Type Description factory.class (none) String The reporter factory class to use for the reporter named \u0026lt;name\u0026gt;. interval 10 s Duration The reporter interval to use for the reporter named \u0026lt;name\u0026gt;. Only applicable to push-based reporters. scope.delimiter "." String The delimiter used to assemble the metric identifier for the reporter named \u0026lt;name\u0026gt;. scope.variables.additional Map The map of additional variables that should be included for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. scope.variables.excludes "." String The set of variables that should be excluded for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. filter.includes "*:*:*" List\u0026lt;String\u0026gt; The metrics that should be included for the reporter named \u0026lt;name\u0026gt;. Filters are specified as a list, with each filter following this format:
\u0026lt;scope\u0026gt;[:\u0026lt;name\u0026gt;[,\u0026lt;name\u0026gt;][:\u0026lt;type\u0026gt;[,\u0026lt;type\u0026gt;]]]
A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.
scope: Filters based on the logical scope.
Specified as a pattern where * matches any sequence of characters and . separates scope components.
For example:
"jobmanager.job" matches any job-related metrics on the JobManager,
"*.job" matches all job-related metrics and
"*.job.*" matches all metrics below the job-level (i.e., task/operator metrics etc.).
name: Filters based on the metric name.
Specified as a comma-separate list of patterns where * matches any sequence of characters.
For example, "*Records*,*Bytes*" matches any metrics where the name contains "Records" or "Bytes".
type: Filters based on the metric type. Specified as a comma-separated list of metric types: [counter, meter, gauge, histogram]Examples:"*:numRecords*" Matches metrics like numRecordsIn."*.job.task.operator:numRecords*" Matches metrics like numRecordsIn on the operator level."*.job.task.operator:numRecords*:meter" Matches meter metrics like numRecordsInPerSecond on the operator level."*:numRecords*,numBytes*:counter,meter" Matches all counter/meter metrics like or numRecordsInPerSecond. filter.excludes List\u0026lt;String\u0026gt; The metrics that should be excluded for the reporter named \u0026lt;name\u0026gt;. The format is identical to filter.includes
\u0026lt;parameter\u0026gt; (none) String Configures the parameter \u0026lt;parameter\u0026gt; for the reporter named \u0026lt;name\u0026gt;. All reporter configurations must contain the factory.class property. Some reporters (referred to as Scheduled) allow specifying a reporting interval.
Example reporter configuration that specifies multiple reporters:
metrics.reporters: my_jmx_reporter,my_other_reporter metrics.reporter.my_jmx_reporter.factory.class: org.apache.flink.metrics.jmx.JMXReporterFactory metrics.reporter.my_jmx_reporter.port: 9020-9040 metrics.reporter.my_jmx_reporter.scope.variables.excludes: job_id;task_attempt_num metrics.reporter.my_jmx_reporter.scope.variables.additional: cluster_name:my_test_cluster,tag_name:tag_value metrics.reporter.my_other_reporter.factory.class: org.apache.flink.metrics.graphite.GraphiteReporterFactory metrics.reporter.my_other_reporter.host: 192.168.1.1 metrics.reporter.my_other_reporter.port: 10000 Important: The jar containing the reporter must be accessible when Flink is started. Reporters are loaded as plugins. All reporters documented on this page are available by default.
You can write your own Reporter by implementing the org.apache.flink.metrics.reporter.MetricReporter interface. If the Reporter should send out reports regularly you have to implement the Scheduled interface as well. By additionally implementing a MetricReporterFactory your reporter can also be loaded as a plugin.
Identifiers vs. tags # There are generally 2 formats for how reporters export metrics.
Identifier-based reporters assemble a flat string containing all scope information and the metric name. An example could be job.MyJobName.numRestarts.
Tag-based reporters on the other hand define a generic class of metrics consisting of a logical scope and metric name (e.g., job.numRestarts), and report a particular instance of said metric as a set of key-value pairs, so called \u0026ldquo;tags\u0026rdquo; or \u0026ldquo;variables\u0026rdquo; (e.g., \u0026ldquo;jobName=MyJobName\u0026rdquo;).
Push vs. Pull # Metrics are exported either via pushes or pulls.
Push-based reporters usually implement the Scheduled interface and periodically send a summary of current metrics to an external system.
Pull-based reporters are queried from an external system instead.
Reporters # The following sections list the supported reporters.
JMX # (org.apache.flink.metrics.jmx.JMXReporter) # Type: pull/tags
Parameters:
port - (optional) the port on which JMX listens for connections. In order to be able to run several instances of the reporter on one host (e.g. when one TaskManager is colocated with the JobManager) it is advisable to use a port range like 9250-9260. When a range is specified the actual port is shown in the relevant job or task manager log. If this setting is set Flink will start an extra JMX connector for the given port/range. Metrics are always available on the default local JMX interface. Example configuration:
metrics.reporter.jmx.factory.class: org.apache.flink.metrics.jmx.JMXReporterFactory metrics.reporter.jmx.port: 8789 Metrics exposed through JMX are identified by a domain and a list of key-properties, which together form the object name.
The domain always begins with org.apache.flink followed by a generalized metric identifier. In contrast to the usual identifier it is not affected by scope-formats, does not contain any variables and is constant across jobs. An example for such a domain would be org.apache.flink.job.task.numBytesOut.
The key-property list contains the values for all variables, regardless of configured scope formats, that are associated with a given metric. An example for such a list would be host=localhost,job_name=MyJob,task_name=MyTask.
The domain thus identifies a metric class, while the key-property list identifies one (or multiple) instances of that metric.
Graphite # (org.apache.flink.metrics.graphite.GraphiteReporter) # Type: push/identifier
Parameters:
host - the Graphite server host port - the Graphite server port protocol - protocol to use (TCP/UDP) Example configuration:
metrics.reporter.grph.factory.class: org.apache.flink.metrics.graphite.GraphiteReporterFactory metrics.reporter.grph.host: localhost metrics.reporter.grph.port: 2003 metrics.reporter.grph.protocol: TCP metrics.reporter.grph.interval: 60 SECONDS InfluxDB # (org.apache.flink.metrics.influxdb.InfluxdbReporter) # Type: push/tags
Parameters:
Key Default Type Description connectTimeout 10000 Integer (optional) the InfluxDB connect timeout for metrics consistency ONE Enum
(optional) the InfluxDB consistency level for metrics
Possible values:"ALL""ANY""ONE""QUORUM" db (none) String the InfluxDB database to store metrics host (none) String the InfluxDB server host password (none) String (optional) InfluxDB username's password used for authentication port 8086 Integer the InfluxDB server port retentionPolicy (none) String (optional) the InfluxDB retention policy for metrics scheme http Enum
the InfluxDB schema
Possible values:"http""https" username (none) String (optional) InfluxDB username used for authentication writeTimeout 10000 Integer (optional) the InfluxDB write timeout for metrics Example configuration:
metrics.reporter.influxdb.factory.class: org.apache.flink.metrics.influxdb.InfluxdbReporterFactory metrics.reporter.influxdb.scheme: http metrics.reporter.influxdb.host: localhost metrics.reporter.influxdb.port: 8086 metrics.reporter.influxdb.db: flink metrics.reporter.influxdb.username: flink-metrics metrics.reporter.influxdb.password: qwerty metrics.reporter.influxdb.retentionPolicy: one_hour metrics.reporter.influxdb.consistency: ANY metrics.reporter.influxdb.connectTimeout: 60000 metrics.reporter.influxdb.writeTimeout: 60000 metrics.reporter.influxdb.interval: 60 SECONDS The reporter would send metrics using http protocol to the InfluxDB server with the specified retention policy (or the default policy specified on the server). All Flink metrics variables (see List of all Variables) are exported as InfluxDB tags.
Prometheus # (org.apache.flink.metrics.prometheus.PrometheusReporter) # Type: pull/tags
Parameters:
port - (optional) the port the Prometheus exporter listens on, defaults to 9249. In order to be able to run several instances of the reporter on one host (e.g. when one TaskManager is colocated with the JobManager) it is advisable to use a port range like 9250-9260. filterLabelValueCharacters - (optional) Specifies whether to filter label value characters. If enabled, all characters not matching [a-zA-Z0-9:_] will be removed, otherwise no characters will be removed. Before disabling this option please ensure that your label values meet the Prometheus requirements. Example configuration:
metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory Flink metric types are mapped to Prometheus metric types as follows:
Flink Prometheus Note Counter Gauge Prometheus counters cannot be decremented. Gauge Gauge Only numbers and booleans are supported. Histogram Summary Quantiles .5, .75, .95, .98, .99 and .999 Meter Gauge The gauge exports the meter\u0026rsquo;s rate. All Flink metrics variables (see List of all Variables) are exported to Prometheus as labels.
PrometheusPushGateway # (org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter) # Type: push/tags
Parameters:
Key Default Type Description deleteOnShutdown true Boolean Specifies whether to delete metrics from the PushGateway on shutdown. Flink will try its best to delete the metrics but this is not guaranteed. See here for more details. filterLabelValueCharacters true Boolean Specifies whether to filter label value characters. If enabled, all characters not matching [a-zA-Z0-9:_] will be removed, otherwise no characters will be removed. Before disabling this option please ensure that your label values meet the Prometheus requirements. groupingKey (none) String Specifies the grouping key which is the group and global labels of all metrics. The label name and value are separated by '=', and labels are separated by ';', e.g., k1=v1;k2=v2. Please ensure that your grouping key meets the Prometheus requirements. hostUrl (none) String The PushGateway server host URL including scheme, host name, and port. jobName (none) String The job name under which metrics will be pushed randomJobNameSuffix true Boolean Specifies whether a random suffix should be appended to the job name. Example configuration:
metrics.reporter.promgateway.factory.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterFactory metrics.reporter.promgateway.hostUrl: http://localhost:9091 metrics.reporter.promgateway.jobName: myJob metrics.reporter.promgateway.randomJobNameSuffix: true metrics.reporter.promgateway.deleteOnShutdown: false metrics.reporter.promgateway.groupingKey: k1=v1;k2=v2 metrics.reporter.promgateway.interval: 60 SECONDS The PrometheusPushGatewayReporter pushes metrics to a Pushgateway, which can be scraped by Prometheus.
Please see the Prometheus documentation for use-cases.
StatsD # (org.apache.flink.metrics.statsd.StatsDReporter) # Type: push/identifier
Parameters:
host - the StatsD server host port - the StatsD server port Example configuration:
metrics.reporter.stsd.factory.class: org.apache.flink.metrics.statsd.StatsDReporterFactory metrics.reporter.stsd.host: localhost metrics.reporter.stsd.port: 8125 metrics.reporter.stsd.interval: 60 SECONDS Datadog # (org.apache.flink.metrics.datadog.DatadogHttpReporter) # Type: push/tags
Note any variables in Flink metrics, such as \u0026lt;host\u0026gt;, \u0026lt;job_name\u0026gt;, \u0026lt;tm_id\u0026gt;, \u0026lt;subtask_index\u0026gt;, \u0026lt;task_name\u0026gt;, and \u0026lt;operator_name\u0026gt;, will be sent to Datadog as tags. Tags will look like host:localhost and job_name:myjobname.
Note For legacy reasons the reporter uses both the metric identifier and tags. This redundancy can be avoided by enabling useLogicalIdentifier.
Note Histograms are exposed as a series of gauges following the naming convention of Datadog histograms (\u0026lt;metric_name\u0026gt;.\u0026lt;aggregation\u0026gt;). The min aggregation is reported by default, whereas sum is not available. In contrast to Datadog-provided Histograms the reported aggregations are not computed for a specific reporting interval.
Parameters:
apikey - the Datadog API key proxyHost - (optional) The proxy host to use when sending to Datadog. proxyPort - (optional) The proxy port to use when sending to Datadog, defaults to 8080. dataCenter - (optional) The data center (EU/US) to connect to, defaults to US. maxMetricsPerRequest - (optional) The maximum number of metrics to include in each request, defaults to 2000. useLogicalIdentifier -\u0026gt; (optional) Whether the reporter uses a logical metric identifier, defaults to false. Example configuration:
metrics.reporter.dghttp.factory.class: org.apache.flink.metrics.datadog.DatadogHttpReporterFactory metrics.reporter.dghttp.apikey: xxx metrics.reporter.dghttp.proxyHost: my.web.proxy.com metrics.reporter.dghttp.proxyPort: 8080 metrics.reporter.dghttp.dataCenter: US metrics.reporter.dghttp.maxMetricsPerRequest: 2000 metrics.reporter.dghttp.interval: 60 SECONDS metrics.reporter.dghttp.useLogicalIdentifier: true Slf4j # (org.apache.flink.metrics.slf4j.Slf4jReporter) # Type: push/identifier
Example configuration:
metrics.reporter.slf4j.factory.class: org.apache.flink.metrics.slf4j.Slf4jReporterFactory metrics.reporter.slf4j.interval: 60 SECONDS Back to top
`}),e.add({id:194,href:"/flink/flink-docs-master/docs/deployment/memory/mem_migration/",title:"Migration Guide",section:"Memory Configuration",content:` Migration Guide # The memory setup has changed a lot with the 1.10 release for TaskManagers and with the 1.11 release for JobManagers. Many configuration options were removed or their semantics changed. This guide will help you to migrate the TaskManager memory configuration from Flink \u0026lt;= 1.9 to \u0026gt;= 1.10 and the JobManager memory configuration from Flink \u0026lt;= 1.10 to \u0026gt;= 1.11.
It is important to review this guide because the legacy and new memory configuration can result in different sizes of memory components. If you try to reuse your Flink configuration from older versions before 1.10 for TaskManagers or before 1.11 for JobManagers, it can result in changes to the behavior, performance or even configuration failures of your application. Note Before version 1.10 for TaskManagers and before 1.11 for JobManagers, Flink did not require that memory related options are set at all as they all had default values. The new memory configuration requires that at least one subset of the following options is configured explicitly, otherwise the configuration will fail:
for TaskManager: for JobManager: taskmanager.memory.flink.size jobmanager.memory.flink.size taskmanager.memory.process.size jobmanager.memory.process.size taskmanager.memory.task.heap.size and taskmanager.memory.managed.size jobmanager.memory.heap.size The default flink-conf.yaml shipped with Flink sets taskmanager.memory.process.size (since 1.10) and jobmanager.memory.process.size (since 1.11) to make the default memory configuration consistent.
This spreadsheet can also help to evaluate and compare the results of the legacy and new memory computations.
Migrate Task Manager Memory Configuration # Changes in Configuration Options # This chapter shortly lists all changes to Flink\u0026rsquo;s memory configuration options introduced with the 1.10 release. It also references other chapters for more details about migrating to the new configuration options.
The following options are completely removed. If they are still used, they will be ignored.
Removed option Note taskmanager.memory.fraction Check the description of the new option taskmanager.memory.managed.fraction. The new option has different semantics and the value of the deprecated option usually has to be adjusted. See also how to migrate managed memory. taskmanager.memory.off-heap On-heap managed memory is no longer supported. See also how to migrate managed memory. taskmanager.memory.preallocate Pre-allocation is no longer supported and managed memory is always allocated lazily. See also how to migrate managed memory. The following options are deprecated but if they are still used they will be interpreted as new options for backwards compatibility:
Deprecated option Interpreted as taskmanager.heap.size taskmanager.memory.flink.size for standalone deployment taskmanager.memory.process.size for containerized deployments See also how to migrate total memory. taskmanager.memory.size taskmanager.memory.managed.size, see also how to migrate managed memory. taskmanager.network.memory.min taskmanager.memory.network.min taskmanager.network.memory.max taskmanager.memory.network.max taskmanager.network.memory.fraction taskmanager.memory.network.fraction Although, the network memory configuration has not changed too much it is recommended to verify its configuration. It can change if other memory components have new sizes, e.g. the total memory which the network can be a fraction of. See also new detailed memory model.
The container cut-off configuration options, containerized.heap-cutoff-ratio and containerized.heap-cutoff-min, have no effect anymore for TaskManagers. See also how to migrate container cut-off.
Total Memory (Previously Heap Memory) # The previous options which were responsible for the total memory used by Flink are taskmanager.heap.size or taskmanager.heap.mb. Despite their naming, they included not only JVM Heap but also other off-heap memory components. The options have been deprecated.
If you use the mentioned legacy options without specifying the corresponding new options, they will be directly translated into the following new options:
Total Flink memory (taskmanager.memory.flink.size) for standalone deployments Total process memory (taskmanager.memory.process.size) for containerized deployments (Yarn) It is also recommended using these new options instead of the legacy ones as they might be completely removed in the following releases.
See also how to configure total memory now.
JVM Heap Memory # JVM Heap memory previously consisted of the managed memory (if configured to be on-heap) and the rest which included any other usages of heap memory. This rest was the remaining part of the total memory, see also how to migrate managed memory.
Now, if only total Flink memory or total process memory is configured, then the JVM Heap is the rest of what is left after subtracting all other components from the total memory, see also how to configure total memory.
Additionally, you can now have more direct control over the JVM Heap assigned to the operator tasks (taskmanager.memory.task.heap.size), see also Task (Operator) Heap Memory. The JVM Heap memory is also used by the heap state backends (MemoryStateBackend or FsStateBackend) if it is chosen for streaming jobs.
A part of the JVM Heap is now always reserved for the Flink framework (taskmanager.memory.framework.heap.size). See also Framework memory.
Managed Memory # See also how to configure managed memory now.
Explicit Size # The previous option to configure managed memory size (taskmanager.memory.size) was renamed to taskmanager.memory.managed.size and deprecated. It is recommended to use the new option because the legacy one can be removed in future releases.
Fraction # If not set explicitly, the managed memory could be previously specified as a fraction (taskmanager.memory.fraction) of the total memory minus network memory and container cut-off (only for Yarn deployments). This option has been completely removed and will have no effect if still used. Please, use the new option taskmanager.memory.managed.fraction instead. This new option will set the managed memory to the specified fraction of the total Flink memory if its size is not set explicitly by taskmanager.memory.managed.size.
RocksDB state # If the RocksDBStateBackend is chosen for a streaming job, its native memory consumption should now be accounted for in managed memory. The RocksDB memory allocation is limited by the managed memory size. This should prevent the killing of containers on Yarn. You can disable the RocksDB memory control by setting state.backend.rocksdb.memory.managed to false. See also how to migrate container cut-off.
Other changes # Additionally, the following changes have been made:
The managed memory is always off-heap now. The configuration option taskmanager.memory.off-heap is removed and will have no effect anymore. The managed memory now uses native memory which is not direct memory. It means that the managed memory is no longer accounted for in the JVM direct memory limit. The managed memory is always lazily allocated now. The configuration option taskmanager.memory.preallocate is removed and will have no effect anymore. Migrate Job Manager Memory Configuration # Previously, there were options responsible for setting the JVM Heap size of the JobManager:
jobmanager.heap.size jobmanager.heap.mb Despite their naming, they represented the JVM Heap only for standalone deployments. For the containerized deployments (Kubernetes and Yarn), they also included other off-heap memory consumption. The size of JVM Heap was additionally reduced by the container cut-off which has been completely removed after 1.11.
The mentioned legacy options have been deprecated. If they are used without specifying the corresponding new options, they will be directly translated into the following new options:
JVM Heap (jobmanager.memory.heap.size) for standalone deployments Total process memory (jobmanager.memory.process.size) for containerized deployments (Kubernetes and Yarn) It is also recommended using these new options instead of the legacy ones as they might be completely removed in the following releases.
Now, if only the total Flink memory or total process memory is configured, then the JVM Heap is also derived as the rest of what is left after subtracting all other components from the total memory, see also how to configure total memory. Additionally, you can now have more direct control over the JVM Heap by adjusting the jobmanager.memory.heap.size option.
Flink JVM process memory limits # Since 1.10 release, Flink sets the JVM Metaspace and JVM Direct Memory limits for the TaskManager process by adding the corresponding JVM arguments. Since 1.11 release, Flink also sets the JVM Metaspace limit for the JobManager process. You can enable the JVM Direct Memory limit for JobManager process if you set the jobmanager.memory.enable-jvm-direct-memory-limit option. See also JVM parameters.
Flink sets the mentioned JVM memory limits to simplify debugging of the corresponding memory leaks and avoid the container out-of-memory errors. See also the troubleshooting guide for details about the JVM Metaspace and JVM Direct Memory OutOfMemoryErrors.
Container Cut-Off Memory # For containerized deployments, you could previously specify a cut-off memory. This memory could accommodate for unaccounted memory allocations. Dependencies which were not directly controlled by Flink were the main source of those allocations, e.g. RocksDB, JVM internals, etc. This is no longer available, and the related configuration options (containerized.heap-cutoff-ratio and containerized.heap-cutoff-min) will have no effect anymore. The new memory model introduced more specific memory components to address these concerns.
for TaskManagers # In streaming jobs which use RocksDBStateBackend, the RocksDB native memory consumption should be accounted for as a part of the managed memory now. The RocksDB memory allocation is also limited by the configured size of the managed memory. See also migrating managed memory and how to configure managed memory now.
The other direct or native off-heap memory consumers can now be addressed by the following new configuration options:
Task off-heap memory (taskmanager.memory.task.off-heap.size) Framework off-heap memory (taskmanager.memory.framework.off-heap.size) JVM metaspace (taskmanager.memory.jvm-metaspace.size) JVM overhead for JobManagers # The direct or native off-heap memory consumers can now be addressed by the following new configuration options:
Off-heap memory (jobmanager.memory.off-heap.size) JVM metaspace (jobmanager.memory.jvm-metaspace.size) JVM overhead Default Configuration in flink-conf.yaml # This section describes the changes of the default flink-conf.yaml shipped with Flink.
The total memory for TaskManagers (taskmanager.heap.size) is replaced by taskmanager.memory.process.size in the default flink-conf.yaml. The value increased from 1024MB to 1728MB.
The total memory for JobManagers (jobmanager.heap.size) is replaced by jobmanager.memory.process.size in the default flink-conf.yaml. The value increased from 1024MB to 1600MB.
See also how to configure total memory now.
Warning: If you use the new default flink-conf.yaml it can result in different sizes of memory components and can lead to performance changes. `}),e.add({id:195,href:"/flink/flink-docs-master/docs/dev/datastream/operators/",title:"Operators",section:"DataStream API",content:""}),e.add({id:196,href:"/flink/flink-docs-master/docs/deployment/repls/python_shell/",title:"Python REPL",section:"REPLs",content:` Python REPL # Flink comes with an integrated interactive Python Shell. It can be used in a local setup as well as in a cluster setup. See the standalone resource provider page for more information about how to setup a local Flink. You can also build a local setup from source.
Note The Python Shell will run the command “python”. Please refer to the Python Table API installation guide on how to set up the Python execution environments.
To use the shell with an integrated Flink cluster, you can simply install PyFlink with PyPi and execute the shell directly:
# install PyFlink \$ python -m pip install apache-flink # execute the shell \$ pyflink-shell.sh local To run the shell on a cluster, please see the Setup section below.
Usage # The shell only supports Table API currently. The Table Environments are automatically prebound after startup. Use \u0026ldquo;bt_env\u0026rdquo; and \u0026ldquo;st_env\u0026rdquo; to access BatchTableEnvironment and StreamTableEnvironment respectively.
Table API # The example below is a simple program in the Python shell: stream \u0026gt;\u0026gt;\u0026gt; import tempfile \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; import shutil \u0026gt;\u0026gt;\u0026gt; sink_path = tempfile.gettempdir() + \u0026#39;/streaming.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; if os.path.exists(sink_path): ... if os.path.isfile(sink_path): ... os.remove(sink_path) ... else: ... shutil.rmtree(sink_path) \u0026gt;\u0026gt;\u0026gt; s_env.set_parallelism(1) \u0026gt;\u0026gt;\u0026gt; t = st_env.from_elements([(1, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;), (2, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; st_env.create_temporary_table(\u0026#34;stream_sink\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) ... .schema(Schema.new_builder() ... .column(\u0026#34;a\u0026#34;, DataTypes.BIGINT()) ... .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) ... .column(\u0026#34;c\u0026#34;, DataTypes.STRING()) ... .build()) ... .option(\u0026#34;path\u0026#34;, path) ... .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) ... .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) ... .build()) ... .build()) \u0026gt;\u0026gt;\u0026gt; t.select(\u0026#34;a + 1, b, c\u0026#34;)\\ ... .execute_insert(\u0026#34;stream_sink\u0026#34;).wait() \u0026gt;\u0026gt;\u0026gt; # If the job runs in local mode, you can exec following code in Python shell to see the result: \u0026gt;\u0026gt;\u0026gt; with open(os.path.join(sink_path, os.listdir(sink_path)[0]), \u0026#39;r\u0026#39;) as f: ... print(f.read()) batch \u0026gt;\u0026gt;\u0026gt; import tempfile \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; import shutil \u0026gt;\u0026gt;\u0026gt; sink_path = tempfile.gettempdir() + \u0026#39;/batch.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; if os.path.exists(sink_path): ... if os.path.isfile(sink_path): ... os.remove(sink_path) ... else: ... shutil.rmtree(sink_path) \u0026gt;\u0026gt;\u0026gt; b_env.set_parallelism(1) \u0026gt;\u0026gt;\u0026gt; t = bt_env.from_elements([(1, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;), (2, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; st_env.create_temporary_table(\u0026#34;batch_sink\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) ... .schema(Schema.new_builder() ... .column(\u0026#34;a\u0026#34;, DataTypes.BIGINT()) ... .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) ... .column(\u0026#34;c\u0026#34;, DataTypes.STRING()) ... .build()) ... .option(\u0026#34;path\u0026#34;, path) ... .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) ... .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) ... .build()) ... .build()) \u0026gt;\u0026gt;\u0026gt; t.select(\u0026#34;a + 1, b, c\u0026#34;)\\ ... .execute_insert(\u0026#34;batch_sink\u0026#34;).wait() \u0026gt;\u0026gt;\u0026gt; # If the job runs in local mode, you can exec following code in Python shell to see the result: \u0026gt;\u0026gt;\u0026gt; with open(os.path.join(sink_path, os.listdir(sink_path)[0]), \u0026#39;r\u0026#39;) as f: ... print(f.read()) Setup # To get an overview of what options the Python Shell provides, please use
pyflink-shell.sh --help Local # To use the shell with an integrated Flink cluster just execute:
pyflink-shell.sh local Remote # To use it with a running cluster, please start the Python shell with the keyword remote and supply the host and port of the JobManager with:
pyflink-shell.sh remote \u0026lt;hostname\u0026gt; \u0026lt;portnumber\u0026gt; Yarn Python Shell cluster # The shell can deploy a Flink cluster to YARN, which is used exclusively by the shell. The shell deploys a new Flink cluster on YARN and connects the cluster. You can also specify options for YARN cluster such as memory for JobManager, name of YARN application, etc.
For example, to start a Yarn cluster for the Python Shell with two TaskManagers use the following:
pyflink-shell.sh yarn -n 2 For all other options, see the full reference at the bottom.
Yarn Session # If you have previously deployed a Flink cluster using the Flink Yarn Session, the Python shell can connect with it using the following command:
pyflink-shell.sh yarn Full Reference # Flink Python Shell Usage: pyflink-shell.sh [local|remote|yarn] [options] \u0026lt;args\u0026gt;... Command: local [options] Starts Flink Python shell with a local Flink cluster usage: -h,--help Show the help message with descriptions of all options. Command: remote [options] \u0026lt;host\u0026gt; \u0026lt;port\u0026gt; Starts Flink Python shell connecting to a remote cluster \u0026lt;host\u0026gt; Remote host name as string \u0026lt;port\u0026gt; Remote port as integer usage: -h,--help Show the help message with descriptions of all options. Command: yarn [options] Starts Flink Python shell connecting to a yarn cluster usage: -h,--help Show the help message with descriptions of all options. -jm,--jobManagerMemory \u0026lt;arg\u0026gt; Memory for JobManager Container with optional unit (default: MB) -nm,--name \u0026lt;arg\u0026gt; Set a custom name for the application on YARN -qu,--queue \u0026lt;arg\u0026gt; Specify YARN queue. -s,--slots \u0026lt;arg\u0026gt; Number of slots per TaskManager -tm,--taskManagerMemory \u0026lt;arg\u0026gt; Memory per TaskManager Container with optional unit (default: MB) -h | --help Prints this usage text Back to top
`}),e.add({id:197,href:"/flink/flink-docs-master/docs/connectors/datastream/rabbitmq/",title:"RabbitMQ",section:"DataStream Connectors",content:` RabbitMQ Connector # License of the RabbitMQ Connector # Flink\u0026rsquo;s RabbitMQ connector defines a Maven dependency on the \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo;, is triple-licensed under the Mozilla Public License 1.1 (\u0026ldquo;MPL\u0026rdquo;), the GNU General Public License version 2 (\u0026ldquo;GPL\u0026rdquo;) and the Apache License version 2 (\u0026ldquo;ASL\u0026rdquo;).
Flink itself neither reuses source code from the \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo; nor packages binaries from the \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo;.
Users that create and publish derivative work based on Flink\u0026rsquo;s RabbitMQ connector (thereby re-distributing the \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo;) must be aware that this may be subject to conditions declared in the Mozilla Public License 1.1 (\u0026ldquo;MPL\u0026rdquo;), the GNU General Public License version 2 (\u0026ldquo;GPL\u0026rdquo;) and the Apache License version 2 (\u0026ldquo;ASL\u0026rdquo;).
RabbitMQ Connector # This connector provides access to data streams from RabbitMQ. To use this connector, add the following dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-rabbitmq\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the RabbitMQ connector in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. Note that the streaming connectors are currently not part of the binary distribution. See linking with them for cluster execution here.
Installing RabbitMQ # Follow the instructions from the RabbitMQ download page. After the installation the server automatically starts, and the application connecting to RabbitMQ can be launched.
RabbitMQ Source # This connector provides a RMQSource class to consume messages from a RabbitMQ queue. This source provides three different levels of guarantees, depending on how it is configured with Flink:
Exactly-once: In order to achieve exactly-once guarantees with the RabbitMQ source, the following is required - Enable checkpointing: With checkpointing enabled, messages are only acknowledged (hence, removed from the RabbitMQ queue) when checkpoints are completed. Use correlation ids: Correlation ids are a RabbitMQ application feature. You have to set it in the message properties when injecting messages into RabbitMQ. The correlation id is used by the source to deduplicate any messages that have been reprocessed when restoring from a checkpoint. Non-parallel source: The source must be non-parallel (parallelism set to 1) in order to achieve exactly-once. This limitation is mainly due to RabbitMQ\u0026rsquo;s approach to dispatching messages from a single queue to multiple consumers. At-least-once: When checkpointing is enabled, but correlation ids are not used or the source is parallel, the source only provides at-least-once guarantees.
No guarantee: If checkpointing isn\u0026rsquo;t enabled, the source does not have any strong delivery guarantees. Under this setting, instead of collaborating with Flink\u0026rsquo;s checkpointing, messages will be automatically acknowledged once the source receives and processes them.
Below is a code example for setting up an exactly-once RabbitMQ source. Inline comments explain which parts of the configuration can be ignored for more relaxed guarantees.
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // checkpointing is required for exactly-once or at-least-once guarantees env.enableCheckpointing(...); final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build(); final DataStream\u0026lt;String\u0026gt; stream = env .addSource(new RMQSource\u0026lt;String\u0026gt;( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to consume true, // use correlation ids; can be false if only at-least-once is required new SimpleStringSchema())) // deserialization schema to turn messages into Java objects .setParallelism(1); // non-parallel source is only required for exactly-once Scala val env = StreamExecutionEnvironment.getExecutionEnvironment // checkpointing is required for exactly-once or at-least-once guarantees env.enableCheckpointing(...) val connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build val stream = env .addSource(new RMQSource[String]( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to consume true, // use correlation ids; can be false if only at-least-once is required new SimpleStringSchema)) // deserialization schema to turn messages into Java objects .setParallelism(1) // non-parallel source is only required for exactly-once Python env = StreamExecutionEnvironment.get_execution_environment() # checkpointing is required for exactly-once or at-least-once guarantees env.enable_checkpointing(...) connection_config = RMQConnectionConfig.Builder() \\ .set_host(\u0026#34;localhost\u0026#34;) \\ .set_port(5000) \\ ... .build() stream = env \\ .add_source(RMQSource( connection_config, \u0026#34;queueName\u0026#34;, True, SimpleStringSchema(), )) \\ .set_parallelism(1) Quality of Service (QoS) / Consumer Prefetch # The RabbitMQ Source provides a simple way to set the basicQos on the source\u0026rsquo;s channel through the RMQConnectionConfig. Since there is one connection/ channel per-parallel source, this prefetch count will effectively be multiplied by the source\u0026rsquo;s parallelism for how many total unacknowledged messages can be sent to the job at one time. If more complex configuration is required, RMQSource#setupChannel(Connection) can be overridden and manually configured.
Java final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30_000) ... .build(); Scala val connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30000) ... .build Python connection_config = RMQConnectionConfig.Builder() \\ .set_prefetch_count(30000) \\ ... .build() The prefetch count is unset by default, meaning the RabbitMQ server will send unlimited messages. In production, it is best to set this value. For high volume queues and checkpointing enabled, some tuning may be required to reduce wasted cycles, as messages are only acknowledged on checkpoints if enabled.
More about QoS and prefetch can be found here and more about the options available in AMQP 0-9-1 here.
RabbitMQ Sink # This connector provides a RMQSink class for sending messages to a RabbitMQ queue. Below is a code example for setting up a RabbitMQ sink.
Java final DataStream\u0026lt;String\u0026gt; stream = ... final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build(); stream.addSink(new RMQSink\u0026lt;String\u0026gt;( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to send messages to new SimpleStringSchema())); // serialization schema to turn Java objects to messages Scala val stream: DataStream[String] = ... val connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build stream.addSink(new RMQSink[String]( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to send messages to new SimpleStringSchema)) // serialization schema to turn Java objects to messages Python stream = ... connection_config = RMQConnectionConfig.Builder() \\ .set_host(\u0026#34;localhost\u0026#34;) \\ .set_port(5000) \\ ... .build() stream.add_sink(RMQSink( connection_config, # config for the RabbitMQ connection \u0026#39;queueName\u0026#39;, # name of the RabbitMQ queue to send messages to SimpleStringSchema())) # serialization schema to turn Java objects to messages More about RabbitMQ can be found here.
Back to top
`}),e.add({id:198,href:"/flink/flink-docs-master/docs/ops/rest_api/",title:"REST API",section:"Operations",content:` REST API # Flink has a monitoring API that can be used to query status and statistics of running jobs, as well as recent completed jobs. This monitoring API is used by Flink\u0026rsquo;s own dashboard, but is designed to be used also by custom monitoring tools.
The monitoring API is a REST-ful API that accepts HTTP requests and responds with JSON data.
Overview # The monitoring API is backed by a web server that runs as part of the JobManager. By default, this server listens at port 8081, which can be configured in flink-conf.yaml via rest.port. Note that the monitoring API web server and the web dashboard web server are currently the same and thus run together at the same port. They respond to different HTTP URLs, though.
In the case of multiple JobManagers (for high availability), each JobManager will run its own instance of the monitoring API, which offers information about completed and running job while that JobManager was elected the cluster leader.
Developing # The REST API backend is in the flink-runtime project. The core class is org.apache.flink.runtime.webmonitor.WebMonitorEndpoint, which sets up the server and the request routing.
We use Netty and the Netty Router library to handle REST requests and translate URLs. This choice was made because this combination has lightweight dependencies, and the performance of Netty HTTP is very good.
To add new requests, one needs to
add a new MessageHeaders class which serves as an interface for the new request, add a new AbstractRestHandler class which handles the request according to the added MessageHeaders class, add the handler to org.apache.flink.runtime.webmonitor.WebMonitorEndpoint#initializeHandlers(). A good example is the org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler that uses the org.apache.flink.runtime.rest.messages.JobExceptionsHeaders.
API # The REST API is versioned, with specific versions being queryable by prefixing the url with the version prefix. Prefixes are always of the form v[version_number]. For example, to access version 1 of /foo/bar one would query /v1/foo/bar.
If no version is specified Flink will default to the oldest version supporting the request.
Querying unsupported/non-existing versions will return a 404 error.
There exist several async operations among these APIs, e.g. trigger savepoint, rescale a job. They would return a triggerid to identify the operation you just POST and then you need to use that triggerid to query for the status of the operation.
For (stop-with-)savepoint operations you can control this triggerId by setting it in the body of the request that triggers the operation. This allow you to safely* retry such operations without triggering multiple savepoints.
The retry is only safe until the async operation store duration has elapsed. JobManager # OpenAPI specification
The OpenAPI specification is still experimental. API reference # v1 /cluster Verb: DELETE Response code: 200 OK Shuts down the cluster Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /config Verb: GET Response code: 200 OK Returns the configuration of the WebUI. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:DashboardConfiguration", "properties" : { "features" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:DashboardConfiguration:Features", "properties" : { "web-cancel" : { "type" : "boolean" }, "web-history" : { "type" : "boolean" }, "web-submit" : { "type" : "boolean" } } }, "flink-revision" : { "type" : "string" }, "flink-version" : { "type" : "string" }, "refresh-interval" : { "type" : "integer" }, "timezone-name" : { "type" : "string" }, "timezone-offset" : { "type" : "integer" } } } /datasets Verb: GET Response code: 200 OK Returns all cluster data sets. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:dataset:ClusterDataSetListResponseBody", "properties" : { "dataSets" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:dataset:ClusterDataSetEntry", "properties" : { "id" : { "type" : "string" }, "isComplete" : { "type" : "boolean" } } } } } } /datasets/delete/:triggerid Verb: GET Response code: 200 OK Returns the status for the delete operation of a cluster data set. Path parameters triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /datasets/:datasetid Verb: DELETE Response code: 202 Accepted Triggers the deletion of a cluster data set. This async operation would return a 'triggerid' for further query identifier. Path parameters datasetid - 32-character hexadecimal string value that identifies a cluster data set. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jars Verb: GET Response code: 200 OK Returns a list of all jars previously uploaded via '/jars/upload'. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo", "properties" : { "address" : { "type" : "string" }, "files" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo:JarFileInfo", "properties" : { "entry" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo:JarEntryInfo", "properties" : { "description" : { "type" : "string" }, "name" : { "type" : "string" } } } }, "id" : { "type" : "string" }, "name" : { "type" : "string" }, "uploaded" : { "type" : "integer" } } } } } } /jars/upload Verb: POST Response code: 200 OK Uploads a jar to the cluster. The jar must be sent as multi-part data. Make sure that the "Content-Type" header is set to "application/x-java-archive", as some http libraries do not add the header by default. Using 'curl' you can upload a jar via 'curl -X POST -H "Expect:" -F "jarfile=@path/to/flink-job.jar" http://hostname:port/jars/upload'. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarUploadResponseBody", "properties" : { "filename" : { "type" : "string" }, "status" : { "type" : "string", "enum" : [ "success" ] } } } /jars/:jarid Verb: DELETE Response code: 200 OK Deletes a jar previously uploaded via '/jars/upload'. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /jars/:jarid/plan Verb: POST Response code: 200 OK Returns the dataflow plan of a job contained in a jar previously uploaded via '/jars/upload'. Program arguments can be passed both via the JSON request (recommended) or query parameters. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Query parameters program-args (optional): Deprecated, please use 'programArg' instead. String value that specifies the arguments for the program or plan programArg (optional): Comma-separated list of program arguments. entry-class (optional): String value that specifies the fully qualified name of the entry point class. Overrides the class defined in the jar file manifest. parallelism (optional): Positive integer value that specifies the desired parallelism for the job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarPlanRequestBody", "properties" : { "entryClass" : { "type" : "string" }, "jobId" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "programArgs" : { "type" : "string" }, "programArgsList" : { "type" : "array", "items" : { "type" : "string" } } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobPlanInfo", "properties" : { "plan" : { "type" : "any" } } } /jars/:jarid/run Verb: POST Response code: 200 OK Submits a job by running a jar previously uploaded via '/jars/upload'. Program arguments can be passed both via the JSON request (recommended) or query parameters. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Query parameters allowNonRestoredState (optional): Boolean value that specifies whether the job submission should be rejected if the savepoint contains state that cannot be mapped back to the job. savepointPath (optional): String value that specifies the path of the savepoint to restore the job from. program-args (optional): Deprecated, please use 'programArg' instead. String value that specifies the arguments for the program or plan programArg (optional): Comma-separated list of program arguments. entry-class (optional): String value that specifies the fully qualified name of the entry point class. Overrides the class defined in the jar file manifest. parallelism (optional): Positive integer value that specifies the desired parallelism for the job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarRunRequestBody", "properties" : { "allowNonRestoredState" : { "type" : "boolean" }, "entryClass" : { "type" : "string" }, "jobId" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "programArgs" : { "type" : "string" }, "programArgsList" : { "type" : "array", "items" : { "type" : "string" } }, "restoreMode" : { "type" : "string", "enum" : [ "CLAIM", "NO_CLAIM", "LEGACY" ] }, "savepointPath" : { "type" : "string" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarRunResponseBody", "properties" : { "jobid" : { "type" : "any" } } } /jobmanager/config Verb: GET Response code: 200 OK Returns the cluster configuration. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ConfigurationInfoEntry", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } } /jobmanager/environment Verb: GET Response code: 200 OK Returns the jobmanager environment. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo", "properties" : { "classpath" : { "type" : "array", "items" : { "type" : "string" } }, "environment" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:EnvironmentVariableItem", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } }, "jvm" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:JVMInfo", "properties" : { "arch" : { "type" : "string" }, "options" : { "type" : "array", "items" : { "type" : "string" } }, "version" : { "type" : "string" } } } } } /jobmanager/logs Verb: GET Response code: 200 OK Returns the list of log files on the JobManager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogListInfo", "properties" : { "logs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogInfo", "properties" : { "mtime" : { "type" : "integer" }, "name" : { "type" : "string" }, "size" : { "type" : "integer" } } } } } } /jobmanager/metrics Verb: GET Response code: 200 OK Provides access to job manager metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobmanager/thread-dump Verb: GET Response code: 200 OK Returns the thread dump of the JobManager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo", "properties" : { "threadInfos" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo:ThreadInfo", "properties" : { "stringifiedThreadInfo" : { "type" : "string" }, "threadName" : { "type" : "string" } } } } } } /jobs Verb: GET Response code: 200 OK Returns an overview over all jobs and their current state. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobIdsWithStatusOverview", "properties" : { "jobs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobIdsWithStatusOverview:JobIdWithStatus", "properties" : { "id" : { "type" : "any" }, "status" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] } } } } } } /jobs Verb: POST Response code: 202 Accepted Submits a job. This call is primarily intended to be used by the Flink client. This call expects a multipart/form-data request that consists of file uploads for the serialized JobGraph, jars and distributed cache artifacts and an attribute named "request" for the JSON payload. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitRequestBody", "properties" : { "jobArtifactFileNames" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitRequestBody:DistributedCacheFile", "properties" : { "entryName" : { "type" : "string" }, "fileName" : { "type" : "string" } } } }, "jobGraphFileName" : { "type" : "string" }, "jobJarFileNames" : { "type" : "array", "items" : { "type" : "string" } } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitResponseBody", "properties" : { "jobUrl" : { "type" : "string" } } } /jobs/metrics Verb: GET Response code: 200 OK Provides access to aggregated job metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". jobs (optional): Comma-separated list of 32-character hexadecimal strings to select specific jobs. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/overview Verb: GET Response code: 200 OK Returns an overview over all jobs. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:MultipleJobsDetails", "properties" : { "jobs" : { "type" : "array", "items" : { "type" : "any" } } } } /jobs/:jobid Verb: GET Response code: 200 OK Returns details of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobDetailsInfo", "properties" : { "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "isStoppable" : { "type" : "boolean" }, "jid" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "plan" : { "type" : "string" }, "start-time" : { "type" : "integer" }, "state" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] }, "status-counts" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "timestamps" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "vertices" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobDetailsInfo:JobVertexDetailsInfo", "properties" : { "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "id" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "name" : { "type" : "string" }, "parallelism" : { "type" : "integer" }, "start-time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } } } } /jobs/:jobid Verb: PATCH Response code: 202 Accepted Terminates a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters mode (optional): String value that specifies the termination mode. The only supported value is: "cancel". Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /jobs/:jobid/accumulators Verb: GET Response code: 200 OK Returns the accumulators for all tasks of a job, aggregated across the respective subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters includeSerializedValue (optional): Boolean value that specifies whether serialized user task accumulators should be included in the response. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobAccumulatorsInfo", "properties" : { "job-accumulators" : { "type" : "array", "items" : { "type" : "any" } }, "serialized-user-task-accumulators" : { "type" : "object", "additionalProperties" : { "type" : "any" } }, "user-task-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobAccumulatorsInfo:UserTaskAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/checkpoints Verb: GET Response code: 200 OK Returns checkpointing statistics for a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics", "properties" : { "counts" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:Counts", "properties" : { "completed" : { "type" : "integer" }, "failed" : { "type" : "integer" }, "in_progress" : { "type" : "integer" }, "restored" : { "type" : "integer" }, "total" : { "type" : "integer" } } }, "history" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics" } }, "trigger_timestamp" : { "type" : "integer" } } } }, "latest" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:LatestCheckpoints", "properties" : { "completed" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:CompletedCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "discarded" : { "type" : "boolean" }, "end_to_end_duration" : { "type" : "integer" }, "external_path" : { "type" : "string" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] } } } }, "trigger_timestamp" : { "type" : "integer" } } }, "failed" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:FailedCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "failure_message" : { "type" : "string" }, "failure_timestamp" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics" } }, "trigger_timestamp" : { "type" : "integer" } } }, "restored" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:RestoredCheckpointStatistics", "properties" : { "external_path" : { "type" : "string" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "restore_timestamp" : { "type" : "integer" } } }, "savepoint" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:CompletedCheckpointStatistics" } } }, "summary" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:Summary", "properties" : { "alignment_buffered" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "checkpointed_size" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto", "properties" : { "avg" : { "type" : "integer" }, "max" : { "type" : "integer" }, "min" : { "type" : "integer" }, "p50" : { "type" : "number" }, "p90" : { "type" : "number" }, "p95" : { "type" : "number" }, "p99" : { "type" : "number" }, "p999" : { "type" : "number" } } }, "end_to_end_duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "persisted_data" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "processed_data" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "state_size" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } } } } /jobs/:jobid/checkpoints/config Verb: GET Response code: 200 OK Returns the checkpointing configuration. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointConfigInfo", "properties" : { "aligned_checkpoint_timeout" : { "type" : "integer" }, "changelog_periodic_materialization_interval" : { "type" : "integer" }, "changelog_storage" : { "type" : "string" }, "checkpoint_storage" : { "type" : "string" }, "checkpoints_after_tasks_finish" : { "type" : "boolean" }, "externalization" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointConfigInfo:ExternalizedCheckpointInfo", "properties" : { "delete_on_cancellation" : { "type" : "boolean" }, "enabled" : { "type" : "boolean" } } }, "interval" : { "type" : "integer" }, "max_concurrent" : { "type" : "integer" }, "min_pause" : { "type" : "integer" }, "mode" : { "type" : "any" }, "state_backend" : { "type" : "string" }, "state_changelog_enabled" : { "type" : "boolean" }, "timeout" : { "type" : "integer" }, "tolerable_failed_checkpoints" : { "type" : "integer" }, "unaligned_checkpoints" : { "type" : "boolean" } } } /jobs/:jobid/checkpoints/details/:checkpointid Verb: GET Response code: 200 OK Returns details for a checkpoint. Path parameters jobid - 32-character hexadecimal string value that identifies a job. checkpointid - Long value that identifies a checkpoint. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] } } } }, "trigger_timestamp" : { "type" : "integer" } } } /jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid Verb: GET Response code: 200 OK Returns checkpoint statistics for a task and its subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. checkpointid - Long value that identifies a checkpoint. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:SubtaskCheckpointStatistics", "properties" : { "index" : { "type" : "integer" }, "status" : { "type" : "string" } } } }, "summary" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:Summary", "properties" : { "alignment" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:CheckpointAlignment", "properties" : { "buffered" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "persisted" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "processed" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } }, "checkpoint_duration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:CheckpointDuration", "properties" : { "async" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "sync" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } }, "checkpointed_size" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto", "properties" : { "avg" : { "type" : "integer" }, "max" : { "type" : "integer" }, "min" : { "type" : "integer" }, "p50" : { "type" : "number" }, "p90" : { "type" : "number" }, "p95" : { "type" : "number" }, "p99" : { "type" : "number" }, "p999" : { "type" : "number" } } }, "end_to_end_duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "start_delay" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "state_size" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } } } } /jobs/:jobid/config Verb: GET Response code: 200 OK Returns the configuration of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/exceptions Verb: GET Response code: 200 OK Returns the most recent exceptions that have been handled by Flink for this job. The 'exceptionHistory.truncated' flag defines whether exceptions were filtered out through the GET parameter. The backend collects only a specific amount of most recent exceptions per job. This can be configured through web.exception-history-size in the Flink configuration. The following first-level members are deprecated: 'root-exception', 'timestamp', 'all-exceptions', and 'truncated'. Use the data provided through 'exceptionHistory', instead. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters maxExceptions (optional): Comma-separated list of integer values that specifies the upper limit of exceptions to return. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory", "properties" : { "all-exceptions" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfo:ExecutionExceptionInfo", "properties" : { "exception" : { "type" : "string" }, "location" : { "type" : "string" }, "task" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "exceptionHistory" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:JobExceptionHistory", "properties" : { "entries" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:RootExceptionInfo", "properties" : { "concurrentExceptions" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:ExceptionInfo", "properties" : { "exceptionName" : { "type" : "string" }, "location" : { "type" : "string" }, "stacktrace" : { "type" : "string" }, "taskName" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "exceptionName" : { "type" : "string" }, "location" : { "type" : "string" }, "stacktrace" : { "type" : "string" }, "taskName" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "truncated" : { "type" : "boolean" } } }, "root-exception" : { "type" : "string" }, "timestamp" : { "type" : "integer" }, "truncated" : { "type" : "boolean" } } } /jobs/:jobid/execution-result Verb: GET Response code: 200 OK Returns the result of a job execution. Gives access to the execution time of the job and to all accumulators created by this job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobExecutionResultResponseBody", "properties" : { "job-execution-result" : { "type" : "any" }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "required" : true, "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/jobmanager/config Verb: GET Response code: 200 OK Returns the jobmanager's configuration of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ConfigurationInfoEntry", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } } /jobs/:jobid/jobmanager/environment Verb: GET Response code: 200 OK Returns the jobmanager's environment variables of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo", "properties" : { "classpath" : { "type" : "array", "items" : { "type" : "string" } }, "environment" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:EnvironmentVariableItem", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } }, "jvm" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:JVMInfo", "properties" : { "arch" : { "type" : "string" }, "options" : { "type" : "array", "items" : { "type" : "string" } }, "version" : { "type" : "string" } } } } } /jobs/:jobid/jobmanager/log-url Verb: GET Response code: 200 OK Returns the log url of jobmanager of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogUrlResponse", "properties" : { "url" : { "type" : "string" } } } /jobs/:jobid/metrics Verb: GET Response code: 200 OK Provides access to job metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/plan Verb: GET Response code: 200 OK Returns the dataflow plan of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobPlanInfo", "properties" : { "plan" : { "type" : "any" } } } /jobs/:jobid/rescaling Verb: PATCH Response code: 200 OK Triggers the rescaling of a job. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters parallelism (mandatory): Positive integer value that specifies the desired parallelism. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/rescaling/:triggerid Verb: GET Response code: 200 OK Returns the status of a rescaling operation. Path parameters jobid - 32-character hexadecimal string value that identifies a job. triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/savepoints Verb: POST Response code: 202 Accepted Triggers a savepoint, and optionally cancels the job afterwards. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointTriggerRequestBody", "properties" : { "cancel-job" : { "type" : "boolean" }, "formatType" : { "type" : "string", "enum" : [ "CANONICAL", "NATIVE" ] }, "target-directory" : { "type" : "string" }, "triggerId" : { "type" : "any" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/savepoints/:triggerid Verb: GET Response code: 200 OK Returns the status of a savepoint operation. Path parameters jobid - 32-character hexadecimal string value that identifies a job. triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointInfo", "properties" : { "failure-cause" : { "type" : "any" }, "location" : { "type" : "string" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/status Verb: GET Response code: 200 OK Returns the current status of a job execution. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobStatusInfo", "properties" : { "status" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] } } } /jobs/:jobid/stop Verb: POST Response code: 202 Accepted Stops a job with a savepoint. Optionally, it can also emit a MAX_WATERMARK before taking the savepoint to flush out any state waiting for timers to fire. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:stop:StopWithSavepointRequestBody", "properties" : { "drain" : { "type" : "boolean" }, "formatType" : { "type" : "string", "enum" : [ "CANONICAL", "NATIVE" ] }, "targetDirectory" : { "type" : "string" }, "triggerId" : { "type" : "any" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/taskmanagers/:taskmanagerid/log-url Verb: GET Response code: 200 OK Returns the log url of jobmanager of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogUrlResponse", "properties" : { "url" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid Verb: GET Response code: 200 OK Returns details for a task, with a summary for each of its subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexDetailsInfo", "properties" : { "aggregated" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:AggregatedTaskDetailsInfo", "properties" : { "metrics" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } }, "id" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "parallelism" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/accumulators Verb: GET Response code: 200 OK Returns user-defined accumulators of a task, aggregated across all subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexAccumulatorsInfo", "properties" : { "id" : { "type" : "string" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/backpressure Verb: GET Response code: 200 OK Returns back-pressure information for a job, and may initiate back-pressure sampling if necessary. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexBackPressureInfo", "properties" : { "backpressure-level" : { "type" : "string", "enum" : [ "ok", "low", "high" ] }, "end-timestamp" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "deprecated", "ok" ] }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexBackPressureInfo:SubtaskBackPressureInfo", "properties" : { "backpressure-level" : { "type" : "string", "enum" : [ "ok", "low", "high" ] }, "busyRatio" : { "type" : "number" }, "idleRatio" : { "type" : "number" }, "ratio" : { "type" : "number" }, "subtask" : { "type" : "integer" } } } } } } /jobs/:jobid/vertices/:vertexid/flamegraph Verb: GET Response code: 200 OK Returns flame graph information for a vertex, and may initiate flame graph sampling if necessary. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters type (optional): String value that specifies the Flame Graph type. Supported options are: "[FULL, ON_CPU, OFF_CPU]". Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph", "properties" : { "data" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph:Node", "properties" : { "children" : { "type" : "array", "items" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph:Node" } }, "name" : { "type" : "string" }, "value" : { "type" : "integer" } } }, "endTimestamp" : { "type" : "integer" } } } /jobs/:jobid/vertices/:vertexid/metrics Verb: GET Response code: 200 OK Provides access to task metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasks/accumulators Verb: GET Response code: 200 OK Returns all user-defined accumulators for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtasksAllAccumulatorsInfo", "properties" : { "id" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtasksAllAccumulatorsInfo:SubtaskAccumulatorsInfo", "properties" : { "attempt" : { "type" : "integer" }, "host" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } } } } /jobs/:jobid/vertices/:vertexid/subtasks/metrics Verb: GET Response code: 200 OK Provides access to aggregated subtask metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". subtasks (optional): Comma-separated list of integer ranges (e.g. "1,3,5-9") to select specific subtasks. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex Verb: GET Response code: 200 OK Returns details of the current or latest execution attempt of a subtask. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt Verb: GET Response code: 200 OK Returns details of an execution attempt of a subtask. Multiple execution attempts happen in case of failure/recovery. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. attempt - Positive integer value that identifies an execution attempt. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators Verb: GET Response code: 200 OK Returns the accumulators of an execution attempt of a subtask. Multiple execution attempts happen in case of failure/recovery. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. attempt - Positive integer value that identifies an execution attempt. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptAccumulatorsInfo", "properties" : { "attempt" : { "type" : "integer" }, "id" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics Verb: GET Response code: 200 OK Provides access to subtask metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasktimes Verb: GET Response code: 200 OK Returns time-related information for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:SubtasksTimesInfo", "properties" : { "id" : { "type" : "string" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:SubtasksTimesInfo:SubtaskTimeInfo", "properties" : { "duration" : { "type" : "integer" }, "host" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "timestamps" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } } } } /jobs/:jobid/vertices/:vertexid/taskmanagers Verb: GET Response code: 200 OK Returns task information aggregated by task manager. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexTaskManagersInfo", "properties" : { "id" : { "type" : "any" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "taskmanagers" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexTaskManagersInfo:TaskManagersInfo", "properties" : { "aggregated" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:AggregatedTaskDetailsInfo", "properties" : { "metrics" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-counts" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "taskmanager-id" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/watermarks Verb: GET Response code: 200 OK Returns the watermarks for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /overview Verb: GET Response code: 200 OK Returns an overview over the Flink cluster. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:legacy:messages:ClusterOverviewWithVersion", "properties" : { "flink-commit" : { "type" : "string" }, "flink-version" : { "type" : "string" }, "jobs-cancelled" : { "type" : "integer" }, "jobs-failed" : { "type" : "integer" }, "jobs-finished" : { "type" : "integer" }, "jobs-running" : { "type" : "integer" }, "slots-available" : { "type" : "integer" }, "slots-total" : { "type" : "integer" }, "taskmanagers" : { "type" : "integer" } } } /savepoint-disposal Verb: POST Response code: 200 OK Triggers the desposal of a savepoint. This async operation would return a 'triggerid' for further query identifier. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointDisposalRequest", "properties" : { "savepoint-path" : { "type" : "string" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /savepoint-disposal/:triggerid Verb: GET Response code: 200 OK Returns the status of a savepoint disposal operation. Path parameters triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /taskmanagers Verb: GET Response code: 200 OK Returns an overview over all task managers. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagersInfo", "properties" : { "taskmanagers" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerInfo", "properties" : { "dataPort" : { "type" : "integer" }, "freeResource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" }, "freeSlots" : { "type" : "integer" }, "hardware" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:instance:HardwareDescription", "properties" : { "cpuCores" : { "type" : "integer" }, "freeMemory" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "physicalMemory" : { "type" : "integer" } } }, "id" : { "type" : "any" }, "jmxPort" : { "type" : "integer" }, "memoryConfiguration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:taskexecutor:TaskExecutorMemoryConfiguration", "properties" : { "frameworkHeap" : { "type" : "integer" }, "frameworkOffHeap" : { "type" : "integer" }, "jvmMetaspace" : { "type" : "integer" }, "jvmOverhead" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeap" : { "type" : "integer" }, "taskOffHeap" : { "type" : "integer" }, "totalFlinkMemory" : { "type" : "integer" }, "totalProcessMemory" : { "type" : "integer" } } }, "path" : { "type" : "string" }, "slotsNumber" : { "type" : "integer" }, "timeSinceLastHeartbeat" : { "type" : "integer" }, "totalResource" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo", "properties" : { "cpuCores" : { "type" : "number" }, "extendedResources" : { "type" : "object", "additionalProperties" : { "type" : "number" } }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeapMemory" : { "type" : "integer" }, "taskOffHeapMemory" : { "type" : "integer" } } } } } } } } /taskmanagers/metrics Verb: GET Response code: 200 OK Provides access to aggregated task manager metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". taskmanagers (optional): Comma-separated list of 32-character hexadecimal strings to select specific task managers. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /taskmanagers/:taskmanagerid Verb: GET Response code: 200 OK Returns details for a task manager. "metrics.memorySegmentsAvailable" and "metrics.memorySegmentsTotal" are deprecated. Please use "metrics.nettyShuffleMemorySegmentsAvailable" and "metrics.nettyShuffleMemorySegmentsTotal" instead. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerDetailsInfo", "properties" : { "allocatedSlots" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:SlotInfo", "properties" : { "jobId" : { "type" : "any" }, "resource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" } } } }, "dataPort" : { "type" : "integer" }, "freeResource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" }, "freeSlots" : { "type" : "integer" }, "hardware" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:instance:HardwareDescription", "properties" : { "cpuCores" : { "type" : "integer" }, "freeMemory" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "physicalMemory" : { "type" : "integer" } } }, "id" : { "type" : "any" }, "jmxPort" : { "type" : "integer" }, "memoryConfiguration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:taskexecutor:TaskExecutorMemoryConfiguration", "properties" : { "frameworkHeap" : { "type" : "integer" }, "frameworkOffHeap" : { "type" : "integer" }, "jvmMetaspace" : { "type" : "integer" }, "jvmOverhead" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeap" : { "type" : "integer" }, "taskOffHeap" : { "type" : "integer" }, "totalFlinkMemory" : { "type" : "integer" }, "totalProcessMemory" : { "type" : "integer" } } }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerMetricsInfo", "properties" : { "directCount" : { "type" : "integer" }, "directMax" : { "type" : "integer" }, "directUsed" : { "type" : "integer" }, "garbageCollectors" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerMetricsInfo:GarbageCollectorInfo", "properties" : { "count" : { "type" : "integer" }, "name" : { "type" : "string" }, "time" : { "type" : "integer" } } } }, "heapCommitted" : { "type" : "integer" }, "heapMax" : { "type" : "integer" }, "heapUsed" : { "type" : "integer" }, "mappedCount" : { "type" : "integer" }, "mappedMax" : { "type" : "integer" }, "mappedUsed" : { "type" : "integer" }, "memorySegmentsAvailable" : { "type" : "integer" }, "memorySegmentsTotal" : { "type" : "integer" }, "nettyShuffleMemoryAvailable" : { "type" : "integer" }, "nettyShuffleMemorySegmentsAvailable" : { "type" : "integer" }, "nettyShuffleMemorySegmentsTotal" : { "type" : "integer" }, "nettyShuffleMemorySegmentsUsed" : { "type" : "integer" }, "nettyShuffleMemoryTotal" : { "type" : "integer" }, "nettyShuffleMemoryUsed" : { "type" : "integer" }, "nonHeapCommitted" : { "type" : "integer" }, "nonHeapMax" : { "type" : "integer" }, "nonHeapUsed" : { "type" : "integer" } } }, "path" : { "type" : "string" }, "slotsNumber" : { "type" : "integer" }, "timeSinceLastHeartbeat" : { "type" : "integer" }, "totalResource" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo", "properties" : { "cpuCores" : { "type" : "number" }, "extendedResources" : { "type" : "object", "additionalProperties" : { "type" : "number" } }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeapMemory" : { "type" : "integer" }, "taskOffHeapMemory" : { "type" : "integer" } } } } } /taskmanagers/:taskmanagerid/logs Verb: GET Response code: 200 OK Returns the list of log files on a TaskManager. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogListInfo", "properties" : { "logs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogInfo", "properties" : { "mtime" : { "type" : "integer" }, "name" : { "type" : "string" }, "size" : { "type" : "integer" } } } } } } /taskmanagers/:taskmanagerid/metrics Verb: GET Response code: 200 OK Provides access to task manager metrics. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /taskmanagers/:taskmanagerid/thread-dump Verb: GET Response code: 200 OK Returns the thread dump of the requested TaskManager. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo", "properties" : { "threadInfos" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo:ThreadInfo", "properties" : { "stringifiedThreadInfo" : { "type" : "string" }, "threadName" : { "type" : "string" } } } } } } `}),e.add({id:199,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/",title:"Window Aggregation",section:"Queries",content:` Window Aggregation # Window TVF Aggregation # Batch Streaming
Window aggregations are defined in the GROUP BY clause contains \u0026ldquo;window_start\u0026rdquo; and \u0026ldquo;window_end\u0026rdquo; columns of the relation applied Windowing TVF. Just like queries with regular GROUP BY clauses, queries with a group by window aggregation will compute a single result row per group.
SELECT ... FROM \u0026lt;windowed_table\u0026gt; -- relation applied windowing TVF GROUP BY window_start, window_end, ... Unlike other aggregations on continuous tables, window aggregation do not emit intermediate results but only a final result, the total aggregation at the end of the window. Moreover, window aggregations purge all intermediate state when no longer needed.
Windowing TVFs # Flink supports TUMBLE, HOP and CUMULATE types of window aggregations. In streaming mode, the time attribute field of a window table-valued function must be on either event or processing time attributes. See Windowing TVF for more windowing functions information. In batch mode, the time attribute field of a window table-valued function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ.
Here are some examples for TUMBLE, HOP and CUMULATE window aggregations.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | | supplier_id | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+-------------+ | bidtime | price | item | supplier_id | +------------------+-------+------+-------------+ | 2020-04-15 08:05 | 4.00 | C | supplier1 | | 2020-04-15 08:07 | 2.00 | A | supplier1 | | 2020-04-15 08:09 | 5.00 | D | supplier2 | | 2020-04-15 08:11 | 3.00 | B | supplier2 | | 2020-04-15 08:13 | 1.00 | E | supplier1 | | 2020-04-15 08:17 | 6.00 | F | supplier2 | +------------------+-------+------+-------------+ -- tumbling window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ -- hopping window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | | 2020-04-15 08:15 | 2020-04-15 08:25 | 6.00 | +------------------+------------------+-------+ -- cumulative window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:06 | 4.00 | | 2020-04-15 08:00 | 2020-04-15 08:08 | 6.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:12 | 3.00 | | 2020-04-15 08:10 | 2020-04-15 08:14 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:16 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
GROUPING SETS # Window aggregations also support GROUPING SETS syntax. Grouping sets allow for more complex grouping operations than those describable by a standard GROUP BY. Rows are grouped separately by each specified grouping set and aggregates are computed for each group just as for simple GROUP BY clauses.
Window aggregations with GROUPING SETS require both the window_start and window_end columns have to be in the GROUP BY clause, but not in the GROUPING SETS clause.
Flink SQL\u0026gt; SELECT window_start, window_end, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, GROUPING SETS ((supplier_id), ()); +------------------+------------------+-------------+-------+ | window_start | window_end | supplier_id | price | +------------------+------------------+-------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | (NULL) | 11.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier2 | 5.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier1 | 6.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | (NULL) | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier2 | 9.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier1 | 1.00 | +------------------+------------------+-------------+-------+ Each sublist of GROUPING SETS may specify zero or more columns or expressions and is interpreted the same way as though used directly in the GROUP BY clause. An empty grouping set means that all rows are aggregated down to a single group, which is output even if no input rows were present.
References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.
ROLLUP # ROLLUP is a shorthand notation for specifying a common type of grouping set. It represents the given list of expressions and all prefixes of the list, including the empty list.
Window aggregations with ROLLUP requires both the window_start and window_end columns have to be in the GROUP BY clause, but not in the ROLLUP clause.
For example, the following query is equivalent to the one above.
SELECT window_start, window_end, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, ROLLUP (supplier_id); CUBE # CUBE is a shorthand notation for specifying a common type of grouping set. It represents the given list and all of its possible subsets - the power set.
Window aggregations with CUBE requires both the window_start and window_end columns have to be in the GROUP BY clause, but not in the CUBE clause.
For example, the following two queries are equivalent.
SELECT window_start, window_end, item, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, CUBE (supplier_id, item); SELECT window_start, window_end, item, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, GROUPING SETS ( (supplier_id, item), (supplier_id ), ( item), ( ) ) Selecting Group Window Start and End Timestamps # The start and end timestamps of group windows can be selected with the grouped window_start and window_end columns.
Cascading Window Aggregation # The window_start and window_end columns are regular timestamp columns, not time attributes. Thus they can\u0026rsquo;t be used as time attributes in subsequent time-based operations. In order to propagate time attributes, you need to additionally add window_time column into GROUP BY clause. The window_time is the third column produced by Windowing TVFs which is a time attribute of the assigned window. Adding window_time into GROUP BY clause makes window_time also to be group key that can be selected. Then following queries can use this column for subsequent time-based operations, such as cascading window aggregations and Window TopN.
The following shows a cascading window aggregation where the first window aggregation propagates the time attribute for the second window aggregation.
-- tumbling 5 minutes for each supplier_id CREATE VIEW window1 AS -- Note: The window start and window end fields of inner Window TVF are optional in the select clause. However, if they appear in the clause, they need to be aliased to prevent name conflicting with the window start and window end of the outer Window TVF. SELECT window_start as window_5mintumble_start, window_end as window_5mintumble_end, window_time as rowtime, SUM(price) as partial_price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES)) GROUP BY supplier_id, window_start, window_end, window_time; -- tumbling 10 minutes on the first window SELECT window_start, window_end, SUM(partial_price) as total_price FROM TABLE( TUMBLE(TABLE window1, DESCRIPTOR(rowtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; Group Window Aggregation # Batch Streaming
Warning: Group Window Aggregation is deprecated. It\u0026rsquo;s encouraged to use Window TVF Aggregation which is more powerful and effective.
Compared to Group Window Aggregation, Window TVF Aggregation have many advantages, including:
Have all performance optimizations mentioned in Performance Tuning. Support standard GROUPING SETS syntax. Can apply Window TopN after window aggregation result. and so on. Group Window Aggregations are defined in the GROUP BY clause of a SQL query. Just like queries with regular GROUP BY clauses, queries with a GROUP BY clause that includes a group window function compute a single result row per group. The following group windows functions are supported for SQL on batch and streaming tables.
Group Window Functions # Group Window Function Description TUMBLE(time_attr, interval) Defines a tumbling time window. A tumbling time window assigns rows to non-overlapping, continuous windows with a fixed duration (interval). For example, a tumbling window of 5 minutes groups rows in 5 minutes intervals. Tumbling windows can be defined on event-time (stream + batch) or processing-time (stream). HOP(time_attr, interval, interval) Defines a hopping time window (called sliding window in the Table API). A hopping time window has a fixed duration (second interval parameter) and hops by a specified hop interval (first interval parameter). If the hop interval is smaller than the window size, hopping windows are overlapping. Thus, rows can be assigned to multiple windows. For example, a hopping window of 15 minutes size and 5 minute hop interval assigns each row to 3 different windows of 15 minute size, which are evaluated in an interval of 5 minutes. Hopping windows can be defined on event-time (stream + batch) or processing-time (stream). SESSION(time_attr, interval) Defines a session time window. Session time windows do not have a fixed duration but their bounds are defined by a time interval of inactivity, i.e., a session window is closed if no event appears for a defined gap period. For example a session window with a 30 minute gap starts when a row is observed after 30 minutes inactivity (otherwise the row would be added to an existing window) and is closed if no row is added within 30 minutes. Session windows can work on event-time (stream + batch) or processing-time (stream). Time Attributes # In streaming mode, the time_attr argument of the group window function must refer to a valid time attribute that specifies the processing time or event time of rows. See the documentation of time attributes to learn how to define time attributes.
In batch mode, the time_attr argument of the group window function must be an attribute of type TIMESTAMP.
Selecting Group Window Start and End Timestamps # The start and end timestamps of group windows as well as time attributes can be selected with the following auxiliary functions:
Auxiliary Function Description TUMBLE_START(time_attr, interval) HOP_START(time_attr, interval, interval) SESSION_START(time_attr, interval) Returns the timestamp of the inclusive lower bound of the corresponding tumbling, hopping, or session window.
TUMBLE_END(time_attr, interval) HOP_END(time_attr, interval, interval) SESSION_END(time_attr, interval) Returns the timestamp of the exclusive upper bound of the corresponding tumbling, hopping, or session window.
Note: The exclusive upper bound timestamp cannot be used as a rowtime attribute in subsequent time-based operations, such as interval joins and group window or over window aggregations.
TUMBLE_ROWTIME(time_attr, interval) HOP_ROWTIME(time_attr, interval, interval) SESSION_ROWTIME(time_attr, interval) Returns the timestamp of the inclusive upper bound of the corresponding tumbling, hopping, or session window.
The resulting attribute is a rowtime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
TUMBLE_PROCTIME(time_attr, interval) HOP_PROCTIME(time_attr, interval, interval) SESSION_PROCTIME(time_attr, interval) Returns a proctime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
Note: Auxiliary functions must be called with exactly same arguments as the group window function in the GROUP BY clause.
The following examples show how to specify SQL queries with group windows on streaming tables.
CREATE TABLE Orders ( user BIGINT, product STRING, amount INT, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;1\u0026#39; MINUTE ) WITH (...); SELECT user, TUMBLE_START(order_time, INTERVAL \u0026#39;1\u0026#39; DAY) AS wStart, SUM(amount) FROM Orders GROUP BY TUMBLE(order_time, INTERVAL \u0026#39;1\u0026#39; DAY), user Back to top
`}),e.add({id:200,href:"/flink/flink-docs-master/docs/dev/table/sql/analyze/",title:"ANALYZE Statements",section:"SQL",content:" ANALYZE Statements # ANALYZE statements are used to collect statistics for existing tables and store the result to catalog. Only ANALYZE TABLE statements are supported now, and need to be triggered manually instead of automatically.\nAttention Currently, ANALYZE TABLE only supports in batch mode. Only existing table is supported, and an exception will be thrown if the table is a view or table not exists.\nRun an ANALYZE TABLE statement # Java ANALYZE TABLE statements can be executed with the executeSql() method of the TableEnvironment.\nThe following examples show how to run a ANALYZE TABLE statement in TableEnvironment.\nScala ANALYZE TABLE statements can be executed with the executeSql() method of the TableEnvironment.\nThe following examples show how to run a ANALYZE TABLE statement in TableEnvironment.\nPython ANALYZE TABLE statements can be executed with the execute_sql() method of the TableEnvironment.\nThe following examples show how to run a ANALYZE TABLE statement in TableEnvironment.\nSQL CLI ANALYZE TABLE statements can be executed in SQL CLI.\nThe following examples show how to run a ANALYZE TABLE statement in SQL CLI.\nJava TableEnvironment tableEnv = TableEnvironment.create(...); // register a non-partition table named \u0026#34;Store\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); // register a partition table named \u0026#34;Orders\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); // Non-partition table, collect row count. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); // Non-partition table, collect row count and statistics for all columns. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Non-partition table, collect row count and statistics for column `location`. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); // Suppose table \u0026#34;Orders\u0026#34; has 4 partitions with specs: // Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition table, collect row count for Partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count for Partition1 and Partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count for all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count and statistics for all columns on partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for all columns on partition1 and partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for all columns on all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for column `amount` on partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); // Partition table, collect row count and statistics for `amount` and `product` on partition1 and partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); // Partition table, collect row count and statistics for column `amount` and `product` on all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); Scala val tableEnv = TableEnvironment.create(...) // register a non-partition table named \u0026#34;Store\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); // register a partition table named \u0026#34;Orders\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); // Non-partition table, collect row count. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); // Non-partition table, collect row count and statistics for all columns. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Non-partition table, collect row count and statistics for column `location`. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); // Suppose table \u0026#34;Orders\u0026#34; has 4 partitions with specs: // Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition table, collect row count for Partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count for Partition1 and Partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count for all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); // Partition table, collect row count and statistics for all columns on partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for all columns on partition1 and partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for all columns on all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // Partition table, collect row count and statistics for column `amount` on partition1. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); // Partition table, collect row count and statistics for `amount` and `product` on partition1 and partition2. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); // Partition table, collect row count and statistics for column `amount` and `product` on all partitions. tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); Python table_env = TableEnvironment.create(...) # register a non-partition table named \u0026#34;Store\u0026#34; table_env.execute_sql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); # register a partition table named \u0026#34;Orders\u0026#34; table_env.execute_sql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); # Non-partition table, collect row count. table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); # Non-partition table, collect row count and statistics for all columns. table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # Non-partition table, collect row count and statistics for column `location`. table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); # Suppose table \u0026#34;Orders\u0026#34; has 4 partitions with specs: # Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) # Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) # Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) # Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) # Partition table, collect row count for Partition1. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); # Partition table, collect row count for Partition1 and Partition2. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); # Partition table, collect row count for all partitions. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); # Partition table, collect row count and statistics for all columns on partition1. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # Partition table, collect row count and statistics for all columns on partition1 and partition2. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # Partition table, collect row count and statistics for all columns on all partitions. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # Partition table, collect row count and statistics for column `amount` on partition1. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); # Partition table, collect row count and statistics for `amount` and `product` on partition1 and partition2. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); # Partition table, collect row count and statistics for column `amount` and `product` on all partitions. table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); SQL CLI Flink SQL\u0026gt; CREATE TABLE Store ( \u0026gt; `id` BIGINT NOT NULl, \u0026gt; `location` VARCHAR(32), \u0026gt; `owner` VARCHAR(32) \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE Orders ( \u0026gt; `id` BIGINT NOT NULl, \u0026gt; `product` VARCHAR(32), \u0026gt; `amount` INT, \u0026gt; `sold_year` BIGINT, \u0026gt; `sold_month` BIGINT, \u0026gt; `sold_day` BIGINT \u0026gt; ) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product; [INFO] Execute statement succeed. Syntax # ANALYZE TABLE [catalog_name.][db_name.]table_name PARTITION(partcol1[=val1] [, partcol2[=val2], ...]) COMPUTE STATISTICS [FOR COLUMNS col1 [, col2, ...] | FOR ALL COLUMNS] PARTITION(partcol1[=val1] [, partcol2[=val2], \u0026hellip;]) is required for the partition table\nIf no partition is specified, the statistics will be gathered for all partitions If a certain partition is specified, the statistics will be gathered only for specific partition If the table is non-partition table , while a partition is specified, an exception will be thrown If a certain partition is specified, but the partition does not exist, an exception will be thrown FOR COLUMNS col1 [, col2, \u0026hellip;] or FOR ALL COLUMNS are optional\nIf no column is specified, only the table level statistics will be gathered If a column does not exist, or column is not a physical column, an exception will be thrown. If a column or any column is specified, the column level statistics will be gathered the column level statistics include: ndv: the number of distinct values nullCount: the number of nulls avgLen: the average length of column values maxLen: the max length of column values minValue: the min value of column values maxValue: the max value of column values valueCount: the value count only for boolean type the supported types and its corresponding column level statistics are as following sheet lists(\u0026ldquo;Y\u0026rdquo; means support, \u0026ldquo;N\u0026rdquo; means unsupported): Types ndv nullCount avgLen maxLen maxValue minValue valueCount BOOLEAN N Y N N N N Y TINYINT Y Y N N Y Y N SMALLINT Y Y N N Y Y N INTEGER Y Y N N Y Y N FLOAT Y Y N N Y Y N DATE Y Y N N Y Y N TIME_WITHOUT_TIME_ZONE Y Y N N Y Y N BIGINT Y Y N N Y Y N DOUBLE Y Y N N Y Y N DECIMAL Y Y N N Y Y N TIMESTAMP_WITH_LOCAL_TIME_ZONE Y Y N N Y Y N TIMESTAMP_WITHOUT_TIME_ZONE Y Y N N Y Y N CHAR Y Y Y Y N N N VARCHAR Y Y Y Y N N N other types N Y N N N N N NOTE: For the fix length types (like BOOLEAN, INTEGER, DOUBLE etc.), we need not collect the avgLen and maxLen from the original records.\nBack to top\n"}),e.add({id:201,href:"/flink/flink-docs-master/docs/ops/batch/",title:"Batch",section:"Operations",content:""}),e.add({id:202,href:"/flink/flink-docs-master/docs/ops/state/checkpoints/",title:"Checkpoints",section:"State \u0026 Fault Tolerance",content:` Checkpoints # Overview # Checkpoints make state in Flink fault tolerant by allowing state and the corresponding stream positions to be recovered, thereby giving the application the same semantics as a failure-free execution.
See Checkpointing for how to enable and configure checkpoints for your program.
To understand the differences between checkpoints and savepoints see checkpoints vs. savepoints.
Checkpoint Storage # When checkpointing is enabled, managed state is persisted to ensure consistent recovery in case of failures. Where the state is persisted during checkpointing depends on the chosen Checkpoint Storage.
Available Checkpoint Storage Options # Out of the box, Flink bundles these checkpoint storage types:
JobManagerCheckpointStorage FileSystemCheckpointStorage If a checkpoint directory is configured FileSystemCheckpointStorage will be used, otherwise the system will use the JobManagerCheckpointStorage. The JobManagerCheckpointStorage # The JobManagerCheckpointStorage stores checkpoint snapshots in the JobManager\u0026rsquo;s heap.
It can be configured to fail the checkpoint if it goes over a certain size to avoid OutOfMemoryError\u0026rsquo;s on the JobManager. To set this feature, users can instantiate a JobManagerCheckpointStorage with the corresponding max size:
new JobManagerCheckpointStorage(MAX_MEM_STATE_SIZE); Limitations of the JobManagerCheckpointStorage:
The size of each individual state is by default limited to 5 MB. This value can be increased in the constructor of the JobManagerCheckpointStorage. Irrespective of the configured maximal state size, the state cannot be larger than the Akka frame size (see Configuration). The aggregate state must fit into the JobManager memory. The JobManagerCheckpointStorage is encouraged for:
Local development and debugging Jobs that use very little state, such as jobs that consist only of record-at-a-time functions (Map, FlatMap, Filter, \u0026hellip;). The Kafka Consumer requires very little state. The FileSystemCheckpointStorage # The FileSystemCheckpointStorage is configured with a file system URL (type, address, path), such as \u0026ldquo;hdfs://namenode:40010/flink/checkpoints\u0026rdquo; or \u0026ldquo;file:///data/flink/checkpoints\u0026rdquo;.
Upon checkpointing, it writes state snapshots into files in the configured file system and directory. Minimal metadata is stored in the JobManager\u0026rsquo;s memory (or, in high-availability mode, in the metadata checkpoint).
If a checkpoint directory is specified, FileSystemCheckpointStorage will be used to persist checkpoint snapshots.
The FileSystemCheckpointStorage is encouraged for:
All high-availability setups. It is also recommended to set managed memory to zero. This will ensure that the maximum amount of memory is allocated for user code on the JVM.
Retained Checkpoints # Checkpoints are by default not retained and are only used to resume a job from failures. They are deleted when a program is cancelled. You can, however, configure periodic checkpoints to be retained. Depending on the configuration these retained checkpoints are not automatically cleaned up when the job fails or is canceled. This way, you will have a checkpoint around to resume from if your job fails.
CheckpointConfig config = env.getCheckpointConfig(); config.setExternalizedCheckpointCleanup(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); The ExternalizedCheckpointCleanup mode configures what happens with checkpoints when you cancel the job:
ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION: Retain the checkpoint when the job is cancelled. Note that you have to manually clean up the checkpoint state after cancellation in this case.
ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: Delete the checkpoint when the job is cancelled. The checkpoint state will only be available if the job fails.
Directory Structure # Similarly to savepoints, a checkpoint consists of a meta data file and, depending on the state backend, some additional data files. The meta data file and data files are stored in the directory that is configured via state.checkpoints.dir in the configuration files, and also can be specified for per job in the code.
The current checkpoint directory layout (introduced by FLINK-8531) is as follows:
/user-defined-checkpoint-dir /{job-id} | + --shared/ + --taskowned/ + --chk-1/ + --chk-2/ + --chk-3/ ... The SHARED directory is for state that is possibly part of multiple checkpoints, TASKOWNED is for state that must never be dropped by the JobManager, and EXCLUSIVE is for state that belongs to one checkpoint only.
The checkpoint directory is not part of a public API and can be changed in the future release. Configure globally via configuration files # state.checkpoints.dir: hdfs:///checkpoints/ Configure for per job on the checkpoint configuration # env.getCheckpointConfig().setCheckpointStorage(\u0026#34;hdfs:///checkpoints-data/\u0026#34;); Configure with checkpoint storage instance # Alternatively, checkpoint storage can be set by specifying the desired checkpoint storage instance which allows for setting low level configurations such as write buffer sizes.
env.getCheckpointConfig().setCheckpointStorage( new FileSystemCheckpointStorage(\u0026#34;hdfs:///checkpoints-data/\u0026#34;, FILE_SIZE_THESHOLD)); Resuming from a retained checkpoint # A job may be resumed from a checkpoint just as from a savepoint by using the checkpoint\u0026rsquo;s meta data file instead (see the savepoint restore guide). Note that if the meta data file is not self-contained, the jobmanager needs to have access to the data files it refers to (see Directory Structure above).
\$ bin/flink run -s :checkpointMetaDataPath [:runArgs] Back to top
`}),e.add({id:203,href:"/flink/flink-docs-master/docs/ops/debugging/",title:"Debugging",section:"Operations",content:""}),e.add({id:204,href:"/flink/flink-docs-master/docs/dev/table/sql/describe/",title:"DESCRIBE Statements",section:"SQL",content:" DESCRIBE Statements # DESCRIBE statements are used to describe the schema of a table or a view.\nRun a DESCRIBE statement # Java DESCRIBE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns the schema of given table for a successful DESCRIBE operation, otherwise will throw an exception.\nThe following examples show how to run a DESCRIBE statement in TableEnvironment.\nScala DESCRIBE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns the schema of given table for a successful DESCRIBE operation, otherwise will throw an exception.\nThe following examples show how to run a DESCRIBE statement in TableEnvironment.\nPython DESCRIBE statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns the schema of given table for a successful DESCRIBE operation, otherwise will throw an exception.\nThe following examples show how to run a DESCRIBE statement in TableEnvironment.\nSQL CLI DESCRIBE statements can be executed in SQL CLI.\nThe following examples show how to run a DESCRIBE statement in SQL CLI.\nJava TableEnvironment tableEnv = TableEnvironment.create(...); // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `user` BIGINT NOT NULl,\u0026#34; + \u0026#34; product VARCHAR(32),\u0026#34; + \u0026#34; amount INT,\u0026#34; + \u0026#34; ts TIMESTAMP(3),\u0026#34; + \u0026#34; ptime AS PROCTIME(),\u0026#34; + \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; + \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; + \u0026#34;) with (...)\u0026#34;); // print the schema tableEnv.executeSql(\u0026#34;DESCRIBE Orders\u0026#34;).print(); // print the schema tableEnv.executeSql(\u0026#34;DESC Orders\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // register a table named \u0026#34;Orders\u0026#34; tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `user` BIGINT NOT NULl,\u0026#34; + \u0026#34; product VARCHAR(32),\u0026#34; + \u0026#34; amount INT,\u0026#34; + \u0026#34; ts TIMESTAMP(3),\u0026#34; + \u0026#34; ptime AS PROCTIME(),\u0026#34; + \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; + \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; + \u0026#34;) with (...)\u0026#34;) // print the schema tableEnv.executeSql(\u0026#34;DESCRIBE Orders\u0026#34;).print() // print the schema tableEnv.executeSql(\u0026#34;DESC Orders\u0026#34;).print() Python table_env = TableEnvironment.create(...) # register a table named \u0026#34;Orders\u0026#34; table_env.execute_sql( \\ \u0026#34;CREATE TABLE Orders (\u0026#34; \u0026#34; `user` BIGINT NOT NULl,\u0026#34; \u0026#34; product VARCHAR(32),\u0026#34; \u0026#34; amount INT,\u0026#34; \u0026#34; ts TIMESTAMP(3),\u0026#34; \u0026#34; ptime AS PROCTIME(),\u0026#34; \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; \u0026#34;) with (...)\u0026#34;); # print the schema table_env.execute_sql(\u0026#34;DESCRIBE Orders\u0026#34;).print() # print the schema table_env.execute_sql(\u0026#34;DESC Orders\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders ( \u0026gt; `user` BIGINT NOT NULl, \u0026gt; product VARCHAR(32), \u0026gt; amount INT, \u0026gt; ts TIMESTAMP(3), \u0026gt; ptime AS PROCTIME(), \u0026gt; PRIMARY KEY(`user`) NOT ENFORCED, \u0026gt; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; DESCRIBE Orders; Flink SQL\u0026gt; DESC Orders; The result of the above example is: Java +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set Scala +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set Python +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set SQL CLI root |-- user: BIGINT NOT NULL |-- product: VARCHAR(32) |-- amount: INT |-- ts: TIMESTAMP(3) *ROWTIME* |-- ptime: TIMESTAMP(3) NOT NULL *PROCTIME* AS PROCTIME() |-- WATERMARK FOR ts AS `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND |-- CONSTRAINT PK_3599338 PRIMARY KEY (user) Back to top\nSyntax # { DESCRIBE | DESC } [catalog_name.][db_name.]table_name "}),e.add({id:205,href:"/flink/flink-docs-master/docs/connectors/table/filesystem/",title:"FileSystem",section:"Table API Connectors",content:` FileSystem SQL Connector # This connector provides access to partitioned files in filesystems supported by the Flink FileSystem abstraction.
The file system connector itself is included in Flink and does not require an additional dependency. The corresponding jar can be found in the Flink distribution inside the /lib directory. A corresponding format needs to be specified for reading and writing rows from and to a file system.
The file system connector allows for reading and writing from a local or distributed filesystem. A filesystem table can be defined as:
CREATE TABLE MyUserTable ( column_name1 INT, column_name2 STRING, ... part_name1 INT, part_name2 STRING ) PARTITIONED BY (part_name1, part_name2) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, -- required: specify the connector \u0026#39;path\u0026#39; = \u0026#39;file:///path/to/whatever\u0026#39;, -- required: path to a directory \u0026#39;format\u0026#39; = \u0026#39;...\u0026#39;, -- required: file system connector requires to specify a format, -- Please refer to Table Formats -- section for more details \u0026#39;partition.default-name\u0026#39; = \u0026#39;...\u0026#39;, -- optional: default partition name in case the dynamic partition -- column value is null/empty string -- optional: the option to enable shuffle data by dynamic partition fields in sink phase, this can greatly -- reduce the number of file for filesystem sink but may lead data skew, the default value is false. \u0026#39;sink.shuffle-by-partition.enable\u0026#39; = \u0026#39;...\u0026#39;, ... ) Make sure to include Flink File System specific dependencies. The behaviour of file system connector is much different from previous legacy filesystem connector: the path parameter is specified for a directory not for a file and you can\u0026rsquo;t get a human-readable file in the path that you declare. Partition Files # Flink\u0026rsquo;s file system partition support uses the standard hive format. However, it does not require partitions to be pre-registered with a table catalog. Partitions are discovered and inferred based on directory structure. For example, a table partitioned based on the directory below would be inferred to contain datetime and hour partitions.
path └── datetime=2019-08-25 └── hour=11 ├── part-0.parquet ├── part-1.parquet └── hour=12 ├── part-0.parquet └── datetime=2019-08-26 └── hour=6 ├── part-0.parquet The file system table supports both partition inserting and overwrite inserting. See INSERT Statement. When you insert overwrite to a partitioned table, only the corresponding partition will be overwritten, not the entire table.
File Formats # The file system connector supports multiple formats:
CSV: RFC-4180. Uncompressed. JSON: Note JSON format for file system connector is not a typical JSON file but uncompressed newline delimited JSON. Avro: Apache Avro. Support compression by configuring avro.codec. Parquet: Apache Parquet. Compatible with Hive. Orc: Apache Orc. Compatible with Hive. Debezium-JSON: debezium-json. Canal-JSON: canal-json. Raw: raw. Source # The file system connector can be used to read single files or entire directories into a single table.
When using a directory as the source path, there is no defined order of ingestion for the files inside the directory.
Directory watching # By default, the file system connector is bounded, that is it will scan the configured path once and then close itself.
You can enable continuous directory watching by configuring the option source.monitor-interval:
Key Default Type Description source.monitor-interval (none) Duration The interval in which the source checks for new files. The interval must be greater than 0. Each file is uniquely identified by its path, and will be processed once, as soon as it's discovered. The set of files already processed is kept in state during the whole lifecycle of the source, so it's persisted in checkpoints and savepoints together with the source state. Shorter intervals mean that files are discovered more quickly, but also imply more frequent listing or directory traversal of the file system / object store. If this config option is not set, the provided path will be scanned once, hence the source will be bounded. Available Metadata # The following connector metadata can be accessed as metadata columns in a table definition. All the metadata are read only.
Key Data Type Description file.path STRING NOT NULL Full path of the input file. file.name STRING NOT NULL Name of the file, that is the farthest element from the root of the filepath. file.size BIGINT NOT NULL Byte count of the file. file.modification-time TIMESTAMP_LTZ(3) NOT NULL Modification time of the file. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:
CREATE TABLE MyUserTableWithFilepath ( column_name1 INT, column_name2 STRING, \`file.path\` STRING NOT NULL METADATA ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;file:///path/to/whatever\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) Streaming Sink # The file system connector supports streaming writes, based on Flink\u0026rsquo;s FileSystem, to write records to file. Row-encoded Formats are CSV and JSON. Bulk-encoded Formats are Parquet, ORC and Avro.
You can write SQL directly, insert the stream data into the non-partitioned table. If it is a partitioned table, you can configure partition related operations. See Partition Commit for details.
Rolling Policy # Data within the partition directories are split into part files. Each partition will contain at least one part file for each subtask of the sink that has received data for that partition. The in-progress part file will be closed and additional part file will be created according to the configurable rolling policy. The policy rolls part files based on size, a timeout that specifies the maximum duration for which a file can be open.
Option Required Forwarded Default Type Description sink.rolling-policy.file-size optional yes 128MB MemorySize The maximum part file size before rolling. sink.rolling-policy.rollover-interval optional yes 30 min Duration The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files). The frequency at which this is checked is controlled by the 'sink.rolling-policy.check-interval' option. sink.rolling-policy.check-interval optional yes 1 min Duration The interval for checking time based rolling policies. This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'. NOTE: For bulk formats (parquet, orc, avro), the rolling policy in combination with the checkpoint interval(pending files become finished on the next checkpoint) control the size and number of these parts.
NOTE: For row formats (csv, json), you can set the parameter sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval in the connector properties and parameter execution.checkpointing.interval in flink-conf.yaml together if you don\u0026rsquo;t want to wait a long period before observe the data exists in file system. For other formats (avro, orc), you can just set parameter execution.checkpointing.interval in flink-conf.yaml.
File Compaction # The file sink supports file compactions, which allows applications to have smaller checkpoint intervals without generating a large number of files.
Option Required Forwarded Default Type Description auto-compaction optional no false Boolean Whether to enable automatic compaction in streaming sink or not. The data will be written to temporary files. After the checkpoint is completed, the temporary files generated by a checkpoint will be compacted. The temporary files are invisible before compaction. compaction.file-size optional yes (none) MemorySize The compaction target file size, the default value is the rolling file size. If enabled, file compaction will merge multiple small files into larger files based on the target file size. When running file compaction in production, please be aware that:
Only files in a single checkpoint are compacted, that is, at least the same number of files as the number of checkpoints is generated. The file before merging is invisible, so the visibility of the file may be: checkpoint interval + compaction time. If the compaction takes too long, it will backpressure the job and extend the time period of checkpoint. Partition Commit # After writing a partition, it is often necessary to notify downstream applications. For example, add the partition to a Hive metastore or writing a _SUCCESS file in the directory. The file system sink contains a partition commit feature that allows configuring custom policies. Commit actions are based on a combination of triggers and policies.
Trigger: The timing of the commit of the partition can be determined by the watermark with the time extracted from the partition, or by processing time. Policy: How to commit a partition, built-in policies support for the commit of success files and metastore, you can also implement your own policies, such as triggering hive\u0026rsquo;s analysis to generate statistics, or merging small files, etc. NOTE: Partition Commit only works in dynamic partition inserting.
Partition commit trigger # To define when to commit a partition, providing partition commit trigger:
Option Required Forwarded Default Type Description sink.partition-commit.trigger optional yes process-time String Trigger type for partition commit: 'process-time': based on the time of the machine, it neither requires partition time extraction nor watermark generation. Commit partition once the 'current system time' passes 'partition creation system time' plus 'delay'. 'partition-time': based on the time that extracted from partition values, it requires watermark generation. Commit partition once the 'watermark' passes 'time extracted from partition values' plus 'delay'. sink.partition-commit.delay optional yes 0 s Duration The partition will not commit until the delay time. If it is a daily partition, should be '1 d', if it is a hourly partition, should be '1 h'. sink.partition-commit.watermark-time-zone optional yes UTC String The time zone to parse the long watermark value to TIMESTAMP value, the parsed watermark timestamp is used to compare with partition time to decide the partition should commit or not. This option is only take effect when \`sink.partition-commit.trigger\` is set to 'partition-time'. If this option is not configured correctly, e.g. source rowtime is defined on TIMESTAMP_LTZ column, but this config is not configured, then users may see the partition committed after a few hours. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is the session time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'. There are two types of trigger:
The first is partition processing time. It neither requires partition time extraction nor watermark generation. The trigger of partition commit according to partition creation time and current system time. This trigger is more universal, but not so precise. For example, data delay or failover will lead to premature partition commit. The second is the trigger of partition commit according to the time that extracted from partition values and watermark. This requires that your job has watermark generation, and the partition is divided according to time, such as hourly partition or daily partition. If you want to let downstream see the partition as soon as possible, no matter whether its data is complete or not:
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;process-time\u0026rsquo; (Default value) \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;0s\u0026rsquo; (Default value) Once there is data in the partition, it will immediately commit. Note: the partition may be committed multiple times. If you want to let downstream see the partition only when its data is complete, and your job has watermark generation, and you can extract the time from partition values:
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;partition-time\u0026rsquo; \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;1h\u0026rsquo; (\u0026lsquo;1h\u0026rsquo; if your partition is hourly partition, depends on your partition type) This is the most accurate way to commit partition, and it will try to ensure that the committed partitions are as data complete as possible. If you want to let downstream see the partition only when its data is complete, but there is no watermark, or the time cannot be extracted from partition values:
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;process-time\u0026rsquo; (Default value) \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;1h\u0026rsquo; (\u0026lsquo;1h\u0026rsquo; if your partition is hourly partition, depends on your partition type) Try to commit partition accurately, but data delay or failover will lead to premature partition commit. Late data processing: The record will be written into its partition when a record is supposed to be written into a partition that has already been committed, and then the committing of this partition will be triggered again.
Partition Time Extractor # Time extractors define extracting time from partition values.
Option Required Forwarded Default Type Description partition.time-extractor.kind optional no default String Time extractor to extract time from partition values. Support default and custom. For default, can configure timestamp pattern\\formatter. For custom, should configure extractor class. partition.time-extractor.class optional no (none) String The extractor class for implement PartitionTimeExtractor interface. partition.time-extractor.timestamp-pattern optional no (none) String The 'default' construction way allows users to use partition fields to get a legal timestamp pattern. Default support 'yyyy-MM-dd hh:mm:ss' from first field. If timestamp should be extracted from a single partition field 'dt', can configure: '\$dt'. If timestamp should be extracted from multiple partition fields, say 'year', 'month', 'day' and 'hour', can configure: '\$year-\$month-\$day \$hour:00:00'. If timestamp should be extracted from two partition fields 'dt' and 'hour', can configure: '\$dt \$hour:00:00'. partition.time-extractor.timestamp-formatter optional no yyyy-MM-dd\u0026nbsp;HH:mm:ss String The formatter that formats the partition timestamp string value to timestamp, the partition timestamp string value is expressed by 'partition.time-extractor.timestamp-pattern'. For example, the partition timestamp is extracted from multiple partition fields, say 'year', 'month' and 'day', you can configure 'partition.time-extractor.timestamp-pattern' to '\$year\$month\$day', and configure \`partition.time-extractor.timestamp-formatter\` to 'yyyyMMdd'. By default the formatter is 'yyyy-MM-dd HH:mm:ss'. The timestamp-formatter is compatible with Java's DateTimeFormatter The default extractor is based on a timestamp pattern composed of your partition fields. You can also specify an implementation for fully custom partition extraction based on the PartitionTimeExtractor interface.
public class HourPartTimeExtractor implements PartitionTimeExtractor { @Override public LocalDateTime extract(List\u0026lt;String\u0026gt; keys, List\u0026lt;String\u0026gt; values) { String dt = values.get(0); String hour = values.get(1); return Timestamp.valueOf(dt + \u0026#34; \u0026#34; + hour + \u0026#34;:00:00\u0026#34;).toLocalDateTime(); } } Partition Commit Policy # The partition commit policy defines what action is taken when partitions are committed.
The first is metastore, only hive table supports metastore policy, file system manages partitions through directory structure. The second is the success file, which will write an empty file in the directory corresponding to the partition. Option Required Forwarded Default Type Description sink.partition-commit.policy.kind optional yes (none) String Policy to commit a partition is to notify the downstream application that the partition has finished writing, the partition is ready to be read. metastore: add partition to metastore. Only hive table supports metastore policy, file system manages partitions through directory structure. success-file: add '_success' file to directory. Both can be configured at the same time: 'metastore,success-file'. custom: use policy class to create a commit policy. Support to configure multiple policies: 'metastore,success-file'. sink.partition-commit.policy.class optional yes (none) String The partition commit policy class for implement PartitionCommitPolicy interface. Only work in custom commit policy. sink.partition-commit.success-file.name optional yes _SUCCESS String The file name for success-file partition commit policy, default is '_SUCCESS'. You can extend the implementation of commit policy, The custom commit policy implementation like:
public class AnalysisCommitPolicy implements PartitionCommitPolicy { private HiveShell hiveShell; @Override public void commit(Context context) throws Exception { if (hiveShell == null) { hiveShell = createHiveShell(context.catalogName()); } hiveShell.execute(String.format( \u0026#34;ALTER TABLE %s ADD IF NOT EXISTS PARTITION (%s = \u0026#39;%s\u0026#39;) location \u0026#39;%s\u0026#39;\u0026#34;, context.tableName(), context.partitionKeys().get(0), context.partitionValues().get(0), context.partitionPath())); hiveShell.execute(String.format( \u0026#34;ANALYZE TABLE %s PARTITION (%s = \u0026#39;%s\u0026#39;) COMPUTE STATISTICS FOR COLUMNS\u0026#34;, context.tableName(), context.partitionKeys().get(0), context.partitionValues().get(0))); } } Sink Parallelism # The parallelism of writing files into external file system (including Hive) can be configured by the corresponding table option, which is supported both in streaming mode and in batch mode. By default, the parallelism is configured to being the same as the parallelism of its last upstream chained operator. When the parallelism which is different from the parallelism of the upstream parallelism is configured, the operator of writing files and the operator compacting files (if used) will apply the parallelism.
Option Required Forwarded Default Type Description sink.parallelism optional no (none) Integer Parallelism of writing files into external file system. The value should greater than zero otherwise exception will be thrown. NOTE: Currently, Configuring sink parallelism is supported if and only if the changelog mode of upstream is INSERT-ONLY. Otherwise, exception will be thrown.
Full Example # The below examples show how the file system connector can be used to write a streaming query to write data from Kafka into a file system and runs a batch query to read that data back out.
CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, \`hour\` STRING ) PARTITIONED BY (dt, \`hour\`) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;format\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;success-file\u0026#39; ); -- streaming sql, insert into file system table INSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(log_ts, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM fs_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and \`hour\`=\u0026#39;12\u0026#39;; If the watermark is defined on TIMESTAMP_LTZ column and used partition-time to commit, the sink.partition-commit.watermark-time-zone is required to set to the session time zone, otherwise the partition committed may happen after a few hours.
CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, ts BIGINT, -- time in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND -- Define watermark on TIMESTAMP_LTZ column ) WITH (...); CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, \`hour\` STRING ) PARTITIONED BY (dt, \`hour\`) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;format\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hour:00:00\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.watermark-time-zone\u0026#39;=\u0026#39;Asia/Shanghai\u0026#39;, -- Assume user configured time zone is \u0026#39;Asia/Shanghai\u0026#39; \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;success-file\u0026#39; ); -- streaming sql, insert into file system table INSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(ts_ltz, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(ts_ltz, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM fs_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and \`hour\`=\u0026#39;12\u0026#39;; Back to top
`}),e.add({id:206,href:"/flink/flink-docs-master/docs/connectors/datastream/pubsub/",title:"Google Cloud PubSub",section:"DataStream Connectors",content:` Google Cloud PubSub # This connector provides a Source and Sink that can read from and write to Google Cloud PubSub. To use this connector, add the following dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-gcp-pubsub\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note: This connector has been added to Flink recently. It has not received widespread testing yet. Note that the streaming connectors are currently not part of the binary distribution. See here for information about how to package the program with the libraries for cluster execution.
Consuming or Producing PubSubMessages # The connector provides a connectors for receiving and sending messages from and to Google PubSub. Google PubSub has an at-least-once guarantee and as such the connector delivers the same guarantees.
PubSub SourceFunction # The class PubSubSource has a builder to create PubSubsources: PubSubSource.newBuilder(...)
There are several optional methods to alter how the PubSubSource is created, the bare minimum is to provide a Google project, Pubsub subscription and a way to deserialize the PubSubMessages.
Example:
Java StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment(); DeserializationSchema\u0026lt;SomeObject\u0026gt; deserializer = (...); SourceFunction\u0026lt;SomeObject\u0026gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializer) .withProjectName(\u0026#34;project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .build(); streamExecEnv.addSource(pubsubSource); Currently the source functions pulls messages from PubSub, push endpoints are not supported.
PubSub Sink # The class PubSubSink has a builder to create PubSubSinks. PubSubSink.newBuilder(...)
This builder works in a similar way to the PubSubSource.
Example:
DataStream\u0026lt;SomeObject\u0026gt; dataStream = (...); SerializationSchema\u0026lt;SomeObject\u0026gt; serializationSchema = (...); SinkFunction\u0026lt;SomeObject\u0026gt; pubsubSink = PubSubSink.newBuilder() .withSerializationSchema(serializationSchema) .withProjectName(\u0026#34;project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .build() dataStream.addSink(pubsubSink); Google Credentials # Google uses Credentials to authenticate and authorize applications so that they can use Google Cloud Platform resources (such as PubSub).
Both builders allow you to provide these credentials but by default the connectors will look for an environment variable: GOOGLE_APPLICATION_CREDENTIALS which should point to a file containing the credentials.
If you want to provide Credentials manually, for instance if you read the Credentials yourself from an external system, you can use PubSubSource.newBuilder(...).withCredentials(...).
Integration testing # When running integration tests you might not want to connect to PubSub directly but use a docker container to read and write to. (See: PubSub testing locally)
The following example shows how you would create a source to read messages from the emulator and send them back:
String hostAndPort = \u0026#34;localhost:1234\u0026#34;; DeserializationSchema\u0026lt;SomeObject\u0026gt; deserializationSchema = (...); SourceFunction\u0026lt;SomeObject\u0026gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializationSchema) .withProjectName(\u0026#34;my-fake-project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .withPubSubSubscriberFactory(new PubSubSubscriberFactoryForEmulator(hostAndPort, \u0026#34;my-fake-project\u0026#34;, \u0026#34;subscription\u0026#34;, 10, Duration.ofSeconds(15), 100)) .build(); SerializationSchema\u0026lt;SomeObject\u0026gt; serializationSchema = (...); SinkFunction\u0026lt;SomeObject\u0026gt; pubsubSink = PubSubSink.newBuilder() .withSerializationSchema(serializationSchema) .withProjectName(\u0026#34;my-fake-project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .withHostAndPortForEmulator(hostAndPort) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(pubsubSource) .addSink(pubsubSink); At least once guarantee # SourceFunction # There are several reasons why a message might be send multiple times, such as failure scenarios on Google PubSub\u0026rsquo;s side.
Another reason is when the acknowledgement deadline has passed. This is the time between receiving the message and acknowledging the message. The PubSubSource will only acknowledge a message on successful checkpoints to guarantee at-least-once. This does mean if the time between successful checkpoints is larger than the acknowledgment deadline of your subscription messages will most likely be processed multiple times.
For this reason it\u0026rsquo;s recommended to have a (much) lower checkpoint interval than acknowledgement deadline.
See PubSub for details on how to increase the acknowledgment deadline of your subscription.
Note: The metric PubSubMessagesProcessedNotAcked shows how many messages are waiting for the next checkpoint before they will be acknowledged.
SinkFunction # The sink function buffers messages that are to be send to PubSub for a short amount of time for performance reasons. Before each checkpoint this buffer is flushed and the checkpoint will not succeed unless the messages have been delivered to PubSub.
Back to top
`}),e.add({id:207,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/group-agg/",title:"Group Aggregation",section:"Queries",content:` Group Aggregation # Batch Streaming
Like most data systems, Apache Flink supports aggregate functions; both built-in and user-defined. User-defined functions must be registered in a catalog before use.
An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT, SUM, AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.
SELECT COUNT(*) FROM Orders For streaming queries, it is important to understand that Flink runs continuous queries that never terminate. Instead, they update their result table according to the updates on its input tables. For the above query, Flink will output an updated count each time a new row is inserted into the Orders table.
Apache Flink supports the standard GROUP BY clause for aggregating data.
SELECT COUNT(*) FROM Orders GROUP BY order_id For streaming queries, the required state for computing the query result might grow infinitely. State size depends on the number of groups and the number and type of aggregation functions. For example MIN/MAX are heavy on state size while COUNT is cheap. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
Apache Flink provides a set of performance tuning ways for Group Aggregation, see more Performance Tuning.
DISTINCT Aggregation # Distinct aggregates remove duplicate values before applying an aggregation function. The following example counts the number of distinct order_ids instead of the total number of rows in the Orders table.
SELECT COUNT(DISTINCT order_id) FROM Orders For streaming queries, the required state for computing the query result might grow infinitely. State size is mostly depends on the number of distinct rows and the time that a group is maintained, short lived group by windows are not a problem. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
GROUPING SETS # Grouping sets allow for more complex grouping operations than those describable by a standard GROUP BY. Rows are grouped separately by each specified grouping set and aggregates are computed for each group just as for simple GROUP BY clauses.
SELECT supplier_id, rating, COUNT(*) AS total FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY GROUPING SETS ((supplier_id, rating), (supplier_id), ()) Results:
+-------------+--------+-------+ | supplier_id | rating | total | +-------------+--------+-------+ | supplier1 | 4 | 1 | | supplier1 | (NULL) | 2 | | (NULL) | (NULL) | 4 | | supplier1 | 3 | 1 | | supplier2 | 3 | 1 | | supplier2 | (NULL) | 2 | | supplier2 | 4 | 1 | +-------------+--------+-------+ Each sublist of GROUPING SETS may specify zero or more columns or expressions and is interpreted the same way as though it was used directly in the GROUP BY clause. An empty grouping set means that all rows are aggregated down to a single group, which is output even if no input rows were present.
References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.
For streaming queries, the required state for computing the query result might grow infinitely. State size depends on number of group sets and type of aggregation functions. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
ROLLUP # ROLLUP is a shorthand notation for specifying a common type of grouping set. It represents the given list of expressions and all prefixes of the list, including the empty list.
For example, the following query is equivalent to the one above.
SELECT supplier_id, rating, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY ROLLUP (supplier_id, rating) CUBE # CUBE is a shorthand notation for specifying a common type of grouping set. It represents the given list and all of its possible subsets - the power set.
For example, the following two queries are equivalent.
SELECT supplier_id, rating, product_id, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY CUBE (supplier_id, rating, product_id) SELECT supplier_id, rating, product_id, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY GROUPING SET ( ( supplier_id, product_id, rating ), ( supplier_id, product_id ), ( supplier_id, rating ), ( supplier_id ), ( product_id, rating ), ( product_id ), ( rating ), ( ) ) HAVING # HAVING eliminates group rows that do not satisfy the condition. HAVING is different from WHERE: WHERE filters individual rows before the GROUP BY while HAVING filters group rows created by GROUP BY. Each column referenced in condition must unambiguously reference a grouping column unless it appears within an aggregate function.
SELECT SUM(amount) FROM Orders GROUP BY users HAVING SUM(amount) \u0026gt; 50 The presence of HAVING turns a query into a grouped query even if there is no GROUP BY clause. It is the same as what happens when the query contains aggregate functions but no GROUP BY clause. The query considers all selected rows to form a single group, and the SELECT list and HAVING clause can only reference table columns from within aggregate functions. Such a query will emit a single row if the HAVING condition is true, zero rows if it is not true.
Back to top
`}),e.add({id:208,href:"/flink/flink-docs-master/docs/dev/dataset/hadoop_map_reduce/",title:"Hadoop MapReduce compatibility with Flink",section:"DataSet API (Legacy)",content:` Flink and Map Reduce compatibility # Flink is compatible with Apache Hadoop MapReduce interfaces and therefore allows reusing code that was implemented for Hadoop MapReduce.
You can:
use Hadoop\u0026rsquo;s Writable data types in Flink programs. use any Hadoop InputFormat as a DataSource. use any Hadoop OutputFormat as a DataSink. use a Hadoop Mapper as FlatMapFunction. use a Hadoop Reducer as GroupReduceFunction. This document shows how to use existing Hadoop MapReduce code with Flink. Please refer to the Connecting to other systems guide for reading from Hadoop supported file systems.
Project Configuration # Support for Hadoop is contained in the flink-hadoop-compatibility Maven module.
Add the following dependency to your pom.xml to use hadoop
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Using Hadoop Mappers and Reducers # Hadoop Mappers are semantically equivalent to Flink\u0026rsquo;s FlatMapFunctions and Hadoop Reducers are equivalent to Flink\u0026rsquo;s GroupReduceFunctions. Flink provides wrappers for implementations of Hadoop MapReduce\u0026rsquo;s Mapper and Reducer interfaces, i.e., you can reuse your Hadoop Mappers and Reducers in regular Flink programs. At the moment, only the Mapper and Reduce interfaces of Hadoop\u0026rsquo;s mapred API (org.apache.hadoop.mapred) are supported.
The wrappers take a DataSet\u0026lt;Tuple2\u0026lt;KEYIN,VALUEIN\u0026gt;\u0026gt; as input and produce a DataSet\u0026lt;Tuple2\u0026lt;KEYOUT,VALUEOUT\u0026gt;\u0026gt; as output where KEYIN and KEYOUT are the keys and VALUEIN and VALUEOUT are the values of the Hadoop key-value pairs that are processed by the Hadoop functions. For Reducers, Flink offers a wrapper for a GroupReduceFunction with (HadoopReduceCombineFunction) and without a Combiner (HadoopReduceFunction). The wrappers accept an optional JobConf object to configure the Hadoop Mapper or Reducer.
Flink\u0026rsquo;s function wrappers are
org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction, org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction, and org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction. and can be used as regular Flink FlatMapFunctions or GroupReduceFunctions.
The following example shows how to use Hadoop Mapper and Reducer functions.
// Obtain data to process somehow. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = [...]; DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); Please note: The Reducer wrapper works on groups as defined by Flink\u0026rsquo;s groupBy() operation. It does not consider any custom partitioners, sort or grouping comparators you might have set in the JobConf.
Complete Hadoop WordCount Example # The following example shows a complete WordCount implementation using Hadoop data types, Input- and OutputFormats, and Mapper and Reducer implementations.
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Set up the Hadoop TextInputFormat. Job job = Job.getInstance(); HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt; hadoopIF = new HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt;( new TextInputFormat(), LongWritable.class, Text.class, job ); TextInputFormat.addInputPath(job, new Path(inputPath)); // Read data using the Hadoop TextInputFormat. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = env.createInput(hadoopIF); DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt; hadoopOF = new HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt;( new TextOutputFormat\u0026lt;Text, LongWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. result.output(hadoopOF); // Execute Program env.execute(\u0026#34;Hadoop WordCount\u0026#34;); Back to top
`}),e.add({id:209,href:"/flink/flink-docs-master/docs/connectors/datastream/hybridsource/",title:"Hybrid Source",section:"DataStream Connectors",content:` Hybrid Source # HybridSource is a source that contains a list of concrete sources. It solves the problem of sequentially reading input from heterogeneous sources to produce a single input stream.
For example, a bootstrap use case may need to read several days worth of bounded input from S3 before continuing with the latest unbounded input from Kafka. HybridSource switches from FileSource to KafkaSource when the bounded file input finishes without interrupting the application.
Prior to HybridSource, it was necessary to create a topology with multiple sources and define a switching mechanism in user land, which leads to operational complexity and inefficiency.
With HybridSource the multiple sources appear as a single source in the Flink job graph and from DataStream API perspective.
For more background see FLIP-150
To use the connector, add the flink-connector-base dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-base\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! (Typically comes as transitive dependency with concrete sources.)
Start position for next source # To arrange multiple sources in a HybridSource, all sources except the last one need to be bounded. Therefore, the sources typically need to be assigned a start and end position. The last source may be bounded in which case the HybridSource is bounded and unbounded otherwise. Details depend on the specific source and the external storage systems.
Here we cover the most basic and then a more complex scenario, following the File/Kafka example.
Fixed start position at graph construction time # Example: Read till pre-determined switch time from files and then continue reading from Kafka. Each source covers an upfront known range and therefore the contained sources can be created upfront as if they were used directly:
Java long switchTimestamp = ...; // derive from file input paths FileSource\u0026lt;String\u0026gt; fileSource = FileSource.forRecordStreamFormat(new TextLineInputFormat(), Path.fromLocalFile(testDir)).build(); KafkaSource\u0026lt;String\u0026gt; kafkaSource = KafkaSource.\u0026lt;String\u0026gt;builder() .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + 1)) .build(); HybridSource\u0026lt;String\u0026gt; hybridSource = HybridSource.builder(fileSource) .addSource(kafkaSource) .build(); Python switch_timestamp = ... # derive from file input paths file_source = FileSource \\ .for_record_stream_format(StreamFormat.text_line_format(), test_dir) \\ .build() kafka_source = KafkaSource \\ .builder() \\ .set_bootstrap_servers(\u0026#39;localhost:9092\u0026#39;) \\ .set_group_id(\u0026#39;MY_GROUP\u0026#39;) \\ .set_topics(\u0026#39;quickstart-events\u0026#39;) \\ .set_value_only_deserializer(SimpleStringSchema()) \\ .set_starting_offsets(KafkaOffsetsInitializer.timestamp(switch_timestamp)) \\ .build() hybrid_source = HybridSource.builder(file_source).add_source(kafka_source).build() Dynamic start position at switch time # Example: File source reads a very large backlog, taking potentially longer than retention available for next source. Switch needs to occur at \u0026ldquo;current time - X\u0026rdquo;. This requires the start time for the next source to be set at switch time. Here we require transfer of end position from the previous file enumerator for deferred construction of KafkaSource by implementing SourceFactory.
Note that enumerators need to support getting the end timestamp. This may currently require a source customization. Adding support for dynamic end position to FileSource is tracked in FLINK-23633.
Java FileSource\u0026lt;String\u0026gt; fileSource = CustomFileSource.readTillOneDayFromLatest(); HybridSource\u0026lt;String\u0026gt; hybridSource = HybridSource.\u0026lt;String, CustomFileSplitEnumerator\u0026gt;builder(fileSource) .addSource( switchContext -\u0026gt; { CustomFileSplitEnumerator previousEnumerator = switchContext.getPreviousEnumerator(); // how to get timestamp depends on specific enumerator long switchTimestamp = previousEnumerator.getEndTimestamp(); KafkaSource\u0026lt;String\u0026gt; kafkaSource = KafkaSource.\u0026lt;String\u0026gt;builder() .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + 1)) .build(); return kafkaSource; }, Boundedness.CONTINUOUS_UNBOUNDED) .build(); Python Still not supported in Python API. `}),e.add({id:210,href:"/flink/flink-docs-master/docs/connectors/table/formats/ogg/",title:"Ogg",section:"Formats",content:` Ogg Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Oracle GoldenGate (a.k.a ogg) is a managed service providing a real-time data mesh platform, which uses replication to keep data highly available, and enabling real-time analysis. Customers can design, execute, and monitor their data replication and stream data processing solutions without the need to allocate or manage compute environments. Ogg provides a format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Ogg JSON as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as
synchronizing incremental data from databases to other systems auditing logs real-time materialized views on databases temporal join changing history of a database table and so on. Flink also supports to encode the INSERT/UPDATE/DELETE messages in Flink SQL as Ogg JSON, and emit to external systems like Kafka. However, currently Flink can\u0026rsquo;t combine UPDATE_BEFORE and UPDATE_AFTER into a single UPDATE message. Therefore, Flink encodes UPDATE_BEFORE and UPDATE_AFTER as DELETE and INSERT Ogg messages.
Dependencies # Ogg Json # In order to use the Ogg the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in Note: please refer to Ogg Kafka Handler documentation about how to set up an Ogg Kafka handler to synchronize changelog to Kafka topics.
How to use Ogg format # Ogg provides a unified format for changelog, here is a simple example for an update operation captured from an Oracle PRODUCTS table in JSON format:
{ \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;op_type\u0026#34;: \u0026#34;U\u0026#34;, \u0026#34;op_ts\u0026#34;: \u0026#34;2020-05-13 15:40:06.000000\u0026#34;, \u0026#34;current_ts\u0026#34;: \u0026#34;2020-05-13 15:40:07.000000\u0026#34;, \u0026#34;primary_keys\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;pos\u0026#34;: \u0026#34;00000000000000000000143\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;PRODUCTS\u0026#34; } Note: please refer to Debezium documentation about the meaning of each field.
The Oracle PRODUCTS table has 4 columns (id, name, description and weight). The above JSON message is an update change event on the PRODUCTS table where the weight value of the row with id = 111 is changed from 5.18 to 5.15. Assuming this messages is synchronized to Kafka topic products_ogg, then we can use the following DDL to consume this topic and interpret the change events.
CREATE TABLE topic_products ( -- schema is totally the same to the Oracle \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_ogg\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;ogg-json\u0026#39; ) After registering the topic as a Flink table, then you can consume the Ogg messages as a changelog source.
-- a real-time materialized view on the Oracle \u0026#34;PRODUCTS\u0026#34; -- which calculate the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of Oracle \u0026#34;PRODUCTS\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Attention Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format.
Key Data Type Description table STRING NULL Contains fully qualified table name. The format of the fully qualified table name is: CATALOG NAME.SCHEMA NAME.TABLE NAME primary-keys ARRAY\u0026lt;STRING\u0026gt; NULL An array variable holding the column names of the primary keys of the source table. The primary-keys field is only include in the JSON output if the includePrimaryKeys configuration property is set to true. ingestion-timestamp TIMESTAMP_LTZ(6) NULL The timestamp at which the connector processed the event. Corresponds to the current_ts field in the Ogg record. event-timestamp TIMESTAMP_LTZ(6) NULL The timestamp at which the source system created the event. Corresponds to the op_ts field in the Ogg record. The following example shows how to access Ogg metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, event_time TIMESTAMP(3) METADATA FROM \u0026#39;value.event-timestamp\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, primary_keys ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.primary-keys\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;ogg-json\u0026#39; ); Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'ogg-json'. ogg-json.ignore-parse-errors optional false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. ogg-json.timestamp-format.standard optional 'SQL' String Specify the input and output timestamp format. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input timestamp in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g '2020-12-30 12:13:14.123' and output timestamp in the same format. Option 'ISO-8601'will parse input timestamp in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g '2020-12-30T12:13:14.123' and output timestamp in the same format. ogg-json.map-null-key.mode optional 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by ogg-json.map-null-key.literal option. ogg-json.map-null-key.literal optional 'null' String Specify string literal to replace null key when 'ogg-json.map-null-key.mode' is LITERAL. Data Type Mapping # Currently, the Ogg format uses JSON format for serialization and deserialization. Please refer to JSON Format documentation for more details about the data type mapping.
`}),e.add({id:211,href:"/flink/flink-docs-master/docs/ops/",title:"Operations",section:"Docs",content:""}),e.add({id:212,href:"/flink/flink-docs-master/docs/deployment/security/",title:"Security",section:"Deployment",content:""}),e.add({id:213,href:"/flink/flink-docs-master/docs/ops/state/checkpointing_under_backpressure/",title:"Checkpointing under backpressure",section:"State \u0026 Fault Tolerance",content:` Checkpointing under backpressure # Normally aligned checkpointing time is dominated by the synchronous and asynchronous parts of the checkpointing process. However, when a Flink job is running under heavy backpressure, the dominant factor in the end-to-end time of a checkpoint can be the time to propagate checkpoint barriers to all operators/subtasks. This is explained in the overview of the checkpointing process). and can be observed by high alignment time and start delay metrics. When this happens and becomes an issue, there are three ways to address the problem:
Remove the backpressure source by optimizing the Flink job, by adjusting Flink or JVM configurations, or by scaling up. Reduce the amount of buffered in-flight data in the Flink job. Enable unaligned checkpoints. These options are not mutually exclusive and can be combined together. This document focuses on the latter two options.
Buffer debloating # Flink 1.14 introduced a new tool to automatically control the amount of buffered in-flight data between Flink operators/subtasks. The buffer debloating mechanism can be enabled by setting the property taskmanager.network.memory.buffer-debloat.enabled to true.
This feature works with both aligned and unaligned checkpoints and can improve checkpointing times in both cases, but the effect of the debloating is most visible with aligned checkpoints. When using buffer debloating with unaligned checkpoints, the added benefit will be smaller checkpoint sizes and quicker recovery times (there will be less in-flight data to persist and recover).
For more information on how the buffer debloating feature works and how to configure it, please refer to the network memory tuning guide. Keep in mind that you can also manually reduce the amount of buffered in-flight data which is also described in the aforementioned tuning guide.
Unaligned checkpoints # Starting with Flink 1.11, checkpoints can be unaligned. Unaligned checkpoints contain in-flight data (i.e., data stored in buffers) as part of the checkpoint state, allowing checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.
You should use unaligned checkpoints if your checkpointing durations are very high due to backpressure. Then, checkpointing time becomes mostly independent of the end-to-end latency. Be aware unaligned checkpointing adds to I/O to the state storage, so you shouldn\u0026rsquo;t use it when the I/O to the state storage is actually the bottleneck during checkpointing.
In order to enable unaligned checkpoints you can:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // enables the unaligned checkpoints env.getCheckpointConfig().enableUnalignedCheckpoints(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() // enables the unaligned checkpoints env.getCheckpointConfig.enableUnalignedCheckpoints() Python env = StreamExecutionEnvironment.get_execution_environment() # enables the unaligned checkpoints env.get_checkpoint_config().enable_unaligned_checkpoints() or in the flink-conf.yml configuration file:
execution.checkpointing.unaligned: true Aligned checkpoint timeout # After enabling unaligned checkpoints, you can also specify the aligned checkpoint timeout programmatically:
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getCheckpointConfig().setAlignedCheckpointTimeout(Duration.ofSeconds(30)); or in the flink-conf.yml configuration file:
execution.checkpointing.aligned-checkpoint-timeout: 30 s When activated, each checkpoint will still begin as an aligned checkpoint, but when the global checkpoint duration exceeds the aligned-checkpoint-timeout, if the aligned checkpoint has not completed, then the checkpoint will proceed as an unaligned checkpoint.
Limitations # Concurrent checkpoints # Flink currently does not support concurrent unaligned checkpoints. However, due to the more predictable and shorter checkpointing times, concurrent checkpoints might not be needed at all. However, savepoints can also not happen concurrently to unaligned checkpoints, so they will take slightly longer.
Interplay with watermarks # Unaligned checkpoints break with an implicit guarantee in respect to watermarks during recovery. Currently, Flink generates the watermark as the first step of recovery instead of storing the latest watermark in the operators to ease rescaling. In unaligned checkpoints, that means on recovery, Flink generates watermarks after it restores in-flight data. If your pipeline uses an operator that applies the latest watermark on each record will produce different results than for aligned checkpoints. If your operator depends on the latest watermark being always available, the workaround is to store the watermark in the operator state. In that case, watermarks should be stored per key group in a union state to support rescaling.
Interplay with long-running record processing # Despite that unaligned checkpoints barriers are able to overtake all other records in the queue. The handling of this barrier still can be delayed if the current record takes a lot of time to be processed. This situation can occur when firing many timers all at once, for example in windowed operations. Second problematic scenario might occur when system is being blocked waiting for more than one network buffer availability when processing a single input record. Flink can not interrupt processing of a single input record, and unaligned checkpoints have to wait for the currently processed record to be fully processed. This can cause problems in two scenarios. Either as a result of serialisation of a large record that doesn\u0026rsquo;t fit into single network buffer or in a flatMap operation, that produces many output records for one input record. In such scenarios back pressure can block unaligned checkpoints until all the network buffers required to process the single input record are available. It also can happen in any other situation when the processing of the single record takes a while. As result, the time of the checkpoint can be higher than expected or it can vary.
Certain data distribution patterns are not checkpointed # There are types of connections with properties that are impossible to keep with channel data stored in checkpoints. To preserve these characteristics and ensure no state corruption or unexpected behaviour, unaligned checkpoints are disabled for such connections. All other exchanges still perform unaligned checkpoints.
Pointwise connections
We currently do not have any hard guarantees on pointwise connections regarding data orderliness. However, since data was structured implicitly in the same way as any preceding source or keyby, some users relied on this behaviour to divide compute-intensive tasks into smaller chunks while depending on orderliness guarantees.
As long as the parallelism does not change, unaligned checkpoints (UC) retain these properties. With the addition of rescaling of UC that has changed.
Consider a job
If we want to rescale from parallelism p = 2 to p = 3, suddenly the records inside the keyby channels need to be divided into three channels according to the key groups. That is easily possible by using the key group ranges of the operators and a way to determine the key(group) of the record ( independent of the actual approach). For the forward channels, we lack the key context entirely. No record in the forward channel has any key group assigned; it\u0026rsquo;s also impossible to calculate it as there is no guarantee that the key is still present.
Broadcast connections
Broadcast connections bring another problem to the table. There are no guarantees that records are consumed at the same rate in all channels. This can result in some tasks applying state changes corresponding to a specific broadcasted event while others don\u0026rsquo;t, as depicted in the figure.
Broadcast partitioning is often used to implement a broadcast state which should be equal across all operators. Flink implements the broadcast state by checkpointing only a single copy of the state from subtask 0 of the stateful operator. Upon restore, we send that copy to all of the operators. Therefore it might happen that an operator will get the state with changes applied for a record that it will soon consume from its checkpointed channels.
Troubleshooting # Corrupted in-flight data # Actions described below are a last resort as they will lead to data loss. In case of the in-flight data corrupted or by another reason when the job should be restored without the in-flight data, it is possible to use recover-without-channel-state.checkpoint-id property. This property requires to specify a checkpoint id for which in-flight data will be ignored. Do not set this property, unless a corruption inside the persisted in-flight data has lead to an otherwise unrecoverable situation. The property can be applied only after the job will be redeployed which means this operation makes sense only if externalized checkpoint is enabled.
`}),e.add({id:214,href:"/flink/flink-docs-master/docs/dev/table/sql/explain/",title:"EXPLAIN Statements",section:"SQL",content:" EXPLAIN Statements # EXPLAIN statements are used to explain the logical and optimized query plans of a query or an INSERT statement.\nRun an EXPLAIN statement # Java EXPLAIN statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns explain result for a successful EXPLAIN operation, otherwise will throw an exception.\nThe following examples show how to run an EXPLAIN statement in TableEnvironment.\nScala EXPLAIN statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns explain result for a successful EXPLAIN operation, otherwise will throw an exception.\nThe following examples show how to run an EXPLAIN statement in TableEnvironment.\nPython EXPLAIN statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns explain result for a successful EXPLAIN operation, otherwise will throw an exception.\nThe following examples show how to run an EXPLAIN statement in TableEnvironment.\nSQL CLI EXPLAIN statements can be executed in SQL CLI.\nThe following examples show how to run an EXPLAIN statement in SQL CLI.\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // register a table named \u0026#34;Orders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;); // explain SELECT statement through TableEnvironment.explainSql() String explanation = tEnv.explainSql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); System.out.println(explanation); // explain SELECT statement through TableEnvironment.executeSql() TableResult tableResult = tEnv.executeSql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); tableResult.print(); TableResult tableResult2 = tEnv.executeSql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); tableResult2.print(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // register a table named \u0026#34;Orders\u0026#34; tEnv.executeSql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) // explain SELECT statement through TableEnvironment.explainSql() val explanation = tEnv.explainSql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) println(explanation) // explain SELECT statement through TableEnvironment.executeSql() val tableResult = tEnv.executeSql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) tableResult.print() val tableResult2 = tEnv.executeSql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) tableResult2.print() Python table_env = StreamTableEnvironment.create(...) t_env.execute_sql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) t_env.execute_sql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) # explain SELECT statement through TableEnvironment.explain_sql() explanation1 = t_env.explain_sql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) print(explanation1) # explain SELECT statement through TableEnvironment.execute_sql() table_result = t_env.execute_sql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) table_result.print() table_result2 = t_env.execute_sql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) table_result2.print() SQL CLI Flink SQL\u0026gt; CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;); [INFO] Table has been created. Flink SQL\u0026gt; EXPLAIN PLAN FOR SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026gt; UNION ALL \u0026gt; SELECT `count`, word FROM MyTable2; Flink SQL\u0026gt; EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN SELECT `count`, word FROM MyTable1 \u0026gt; WHERE word LIKE \u0026#39;F%\u0026#39; \u0026gt; UNION ALL \u0026gt; SELECT `count`, word FROM MyTable2; The EXPLAIN result is:\nEXPLAIN PLAN == Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalProject(count=[$0], word=[$1]) : +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]]) +- LogicalProject(count=[$0], word=[$1]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) EXPLAIN PLAN WITH DETAILS == Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalProject(count=[$0], word=[$1]) : +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]]) +- LogicalProject(count=[$0], word=[$1]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word], changelogMode=[I]): rowcount = 1.05E8, cumulative cost = {3.1E8 rows, 3.05E8 cpu, 4.0E9 io, 0.0 network, 0.0 memory} :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)], changelogMode=[I]): rowcount = 5000000.0, cumulative cost = {1.05E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word], changelogMode=[I]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word], changelogMode=[I]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Physical Execution Plan == { \u0026#34;nodes\u0026#34; : [ { \u0026#34;id\u0026#34; : 37, \u0026#34;type\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Data Source\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1 }, { \u0026#34;id\u0026#34; : 38, \u0026#34;type\u0026#34; : \u0026#34;Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Operator\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1, \u0026#34;predecessors\u0026#34; : [ { \u0026#34;id\u0026#34; : 37, \u0026#34;ship_strategy\u0026#34; : \u0026#34;FORWARD\u0026#34;, \u0026#34;side\u0026#34; : \u0026#34;second\u0026#34; } ] }, { \u0026#34;id\u0026#34; : 39, \u0026#34;type\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Data Source\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1 } ] Back to top\nExplainDetails # Print the plan for the statement with specified ExplainDetails. ESTIMATED_COST: generates cost information on physical node estimated by optimizer, e.g. TableSourceScan(..., cumulative cost ={1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}) CHANGELOG_MODE:generates changelog mode for every physical rel node. e.g. GroupAggregate(..., changelogMode=[I,UA,D]) JSON_EXECUTION_PLAN: generates the execution plan in json format of the program. Syntax # EXPLAIN [([ExplainDetail[, ExplainDetail]*]) | PLAN FOR] \u0026lt;query_statement_or_insert_statement_or_statement_set\u0026gt; statement_set: EXECUTE STATEMENT SET BEGIN insert_statement; ... insert_statement; END; For query syntax, please refer to Queries page. For insert syntax, please refer to INSERT page.\n"}),e.add({id:215,href:"/flink/flink-docs-master/docs/flinkdev/",title:"Flink Development",section:"Docs",content:""}),e.add({id:216,href:"/flink/flink-docs-master/docs/connectors/table/hbase/",title:"HBase",section:"Table API Connectors",content:` HBase SQL Connector # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Upsert Mode
The HBase connector allows for reading from and writing to an HBase cluster. This document describes how to setup the HBase Connector to run SQL queries against HBase.
HBase always works in upsert mode for exchange changelog messages with the external system using a primary key defined on the DDL. The primary key must be defined on the HBase rowkey field (rowkey field must be declared). If the PRIMARY KEY clause is not declared, the HBase connector will take rowkey as the primary key by default.
Dependencies # In order to use the HBase connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
HBase version Maven dependency SQL Client JAR 1.4.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-hbase-1.4\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 2.2.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-hbase-2.2\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. The HBase connector is not part of the binary distribution. See how to link with it for cluster execution here.
How to use HBase table # All the column families in HBase table must be declared as ROW type, the field name maps to the column family name, and the nested field names map to the column qualifier names. There is no need to declare all the families and qualifiers in the schema, users can declare what’s used in the query. Except the ROW type fields, the single atomic type field (e.g. STRING, BIGINT) will be recognized as HBase rowkey. The rowkey field can be arbitrary name, but should be quoted using backticks if it is a reserved keyword.
-- register the HBase table \u0026#39;mytable\u0026#39; in Flink SQL CREATE TABLE hTable ( rowkey INT, family1 ROW\u0026lt;q1 INT\u0026gt;, family2 ROW\u0026lt;q2 STRING, q3 BIGINT\u0026gt;, family3 ROW\u0026lt;q4 DOUBLE, q5 BOOLEAN, q6 STRING\u0026gt;, PRIMARY KEY (rowkey) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;hbase-1.4\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;mytable\u0026#39;, \u0026#39;zookeeper.quorum\u0026#39; = \u0026#39;localhost:2181\u0026#39; ); -- use ROW(...) construction function construct column families and write data into the HBase table. -- assuming the schema of \u0026#34;T\u0026#34; is [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6] INSERT INTO hTable SELECT rowkey, ROW(f1q1), ROW(f2q2, f2q3), ROW(f3q4, f3q5, f3q6) FROM T; -- scan data from the HBase table SELECT rowkey, family1, family3.q4, family3.q6 FROM hTable; -- temporal join the HBase table as a dimension table SELECT * FROM myTopic LEFT JOIN hTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = hTable.rowkey; Connector Options # Option Required Forwarded Default Type Description connector required no (none) String Specify what connector to use, valid values are: hbase-1.4: connect to HBase 1.4.x cluster hbase-2.2: connect to HBase 2.2.x cluster table-name required yes (none) String The name of HBase table to connect. By default, the table is in 'default' namespace. To assign the table a specified namespace you need to use 'namespace:table'. zookeeper.quorum required yes (none) String The HBase Zookeeper quorum. zookeeper.znode.parent optional yes /hbase String The root dir in Zookeeper for HBase cluster. null-string-literal optional yes null String Representation for null values for string fields. HBase source and sink encodes/decodes empty bytes as null values for all types except string type. sink.buffer-flush.max-size optional yes 2mb MemorySize Writing option, maximum size in memory of buffered rows for each writing request. This can improve performance for writing data to HBase database, but may increase the latency. Can be set to '0' to disable it. sink.buffer-flush.max-rows optional yes 1000 Integer Writing option, maximum number of rows to buffer for each writing request. This can improve performance for writing data to HBase database, but may increase the latency. Can be set to '0' to disable it. sink.buffer-flush.interval optional yes 1s Duration Writing option, the interval to flush any buffered rows. This can improve performance for writing data to HBase database, but may increase the latency. Can be set to '0' to disable it. Note, both 'sink.buffer-flush.max-size' and 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions. sink.parallelism optional no (none) Integer Defines the parallelism of the HBase sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator. lookup.async optional no false Boolean Whether async lookup are enabled. If true, the lookup will be async. Note, async only supports hbase-2.2 connector. lookup.cache.max-rows optional yes -1 Long The max number of rows of lookup cache, over this value, the oldest rows will be expired. Note, "lookup.cache.max-rows" and "lookup.cache.ttl" options must all be specified if any of them is specified. Lookup cache is disabled by default. lookup.cache.ttl optional yes 0 s Duration The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired. Note, "cache.max-rows" and "cache.ttl" options must all be specified if any of them is specified.Lookup cache is disabled by default. lookup.max-retries optional yes 3 Integer The max retry times if lookup database failed. properties.* optional no (none) String This can set and pass arbitrary HBase configurations. Suffix names must match the configuration key defined in HBase Configuration documentation. Flink will remove the "properties." key prefix and pass the transformed key and values to the underlying HBaseClient. For example, you can add a kerberos authentication parameter 'properties.hbase.security.authentication' = 'kerberos'. Data Type Mapping # HBase stores all data as byte arrays. The data needs to be serialized and deserialized during read and write operation
When serializing and de-serializing, Flink HBase connector uses utility class org.apache.hadoop.hbase.util.Bytes provided by HBase (Hadoop) to convert Flink Data Types to and from byte arrays.
Flink HBase connector encodes null values to empty bytes, and decode empty bytes to null values for all data types except string type. For string type, the null literal is determined by null-string-literal option.
The data type mappings are as follows:
Flink SQL type HBase conversion CHAR / VARCHAR / STRING byte[] toBytes(String s) String toString(byte[] b) BOOLEAN byte[] toBytes(boolean b) boolean toBoolean(byte[] b) BINARY / VARBINARY Returns byte[] as is. DECIMAL byte[] toBytes(BigDecimal v) BigDecimal toBigDecimal(byte[] b) TINYINT new byte[] { val } bytes[0] // returns first and only byte from bytes SMALLINT byte[] toBytes(short val) short toShort(byte[] bytes) INT byte[] toBytes(int val) int toInt(byte[] bytes) BIGINT byte[] toBytes(long val) long toLong(byte[] bytes) FLOAT byte[] toBytes(float val) float toFloat(byte[] bytes) DOUBLE byte[] toBytes(double val) double toDouble(byte[] bytes) DATE Stores the number of days since epoch as int value. TIME Stores the number of milliseconds of the day as int value. TIMESTAMP Stores the milliseconds since epoch as long value. ARRAY Not supported MAP / MULTISET Not supported ROW Not supported Back to top
`}),e.add({id:217,href:"/flink/flink-docs-master/docs/dev/dataset/local_execution/",title:"Local Execution",section:"DataSet API (Legacy)",content:` Local Execution # Flink can run on a single machine, even in a single Java Virtual Machine. This allows users to test and debug Flink programs locally. This section gives an overview of the local execution mechanisms.
The local environments and executors allow you to run Flink programs in a local Java Virtual Machine, or with within any JVM as part of existing programs. Most examples can be launched locally by simply hitting the \u0026ldquo;Run\u0026rdquo; button of your IDE.
There are two different kinds of local execution supported in Flink. The LocalExecutionEnvironment is starting the full Flink runtime, including a JobManager and a TaskManager. These include memory management and all the internal algorithms that are executed in the cluster mode.
The CollectionEnvironment is executing the Flink program on Java collections. This mode will not start the full Flink runtime, so the execution is very low-overhead and lightweight. For example a DataSet.map()-transformation will be executed by applying the map() function to all elements in a Java list.
Debugging # If you are running Flink programs locally, you can also debug your program like any other Java program. You can either use System.out.println() to write out some internal variables or you can use the debugger. It is possible to set breakpoints within map(), reduce() and all the other methods. Please also refer to the debugging section in the Java API documentation for a guide to testing and local debugging utilities in the Java API.
Maven Dependency # If you are developing your program in a Maven project, you have to add the flink-clients module using this dependency:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Local Environment # The LocalEnvironment is a handle to local execution for Flink programs. Use it to run a program within a local JVM - standalone or embedded in other programs.
The local environment is instantiated via the method ExecutionEnvironment.createLocalEnvironment(). By default, it will use as many local threads for execution as your machine has CPU cores (hardware contexts). You can alternatively specify the desired parallelism. The local environment can be configured to log to the console using enableLogging()/disableLogging().
In most cases, calling ExecutionEnvironment.getExecutionEnvironment() is the even better way to go. That method returns a LocalEnvironment when the program is started locally (outside the command line interface), and it returns a pre-configured environment for cluster execution, when the program is invoked by the command line interface.
public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;file:///path/to/result\u0026#34;); JobExecutionResult res = env.execute(); } The JobExecutionResult object, which is returned after the execution finished, contains the program runtime and the accumulator results.
The LocalEnvironment allows also to pass custom configuration values to Flink.
Configuration conf = new Configuration(); conf.setFloat(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY, 0.5f); final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(conf); Note: The local execution environments do not start any web frontend to monitor the execution.
Collection Environment # The execution on Java Collections using the CollectionEnvironment is a low-overhead approach for executing Flink programs. Typical use-cases for this mode are automated tests, debugging and code re-use.
Users can use algorithms implemented for batch processing also for cases that are more interactive. A slightly changed variant of a Flink program could be used in a Java Application Server for processing incoming requests.
Skeleton for Collection-based execution
public static void main(String[] args) throws Exception { // initialize a new Collection-based execution environment final ExecutionEnvironment env = new CollectionEnvironment(); DataSet\u0026lt;User\u0026gt; users = env.fromCollection( /* get elements from a Java Collection */); /* Data Set transformations ... */ // retrieve the resulting Tuple2 elements into a ArrayList. Collection\u0026lt;...\u0026gt; result = new ArrayList\u0026lt;...\u0026gt;(); resultDataSet.output(new LocalCollectionOutputFormat\u0026lt;...\u0026gt;(result)); // kick off execution. env.execute(); // Do some work with the resulting ArrayList (=Collection). for(... t : result) { System.err.println(\u0026#34;Result = \u0026#34;+t); } } The flink-examples-batch module contains a full example, called CollectionExecutionExample.
Please note that the execution of the collection-based Flink programs is only possible on small data, which fits into the JVM heap. The execution on collections is not multi-threaded, only one thread is used.
Back to top
`}),e.add({id:218,href:"/flink/flink-docs-master/docs/ops/monitoring/",title:"Monitoring",section:"Operations",content:""}),e.add({id:219,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/over-agg/",title:"Over Aggregation",section:"Queries",content:` Over Aggregation # Batch Streaming
OVER aggregates compute an aggregated value for every input row over a range of ordered rows. In contrast to GROUP BY aggregates, OVER aggregates do not reduce the number of result rows to a single row for every group. Instead OVER aggregates produce an aggregated value for every input row.
The following query computes for every order the sum of amounts of all orders for the same product that were received within one hour before the current order.
SELECT order_id, order_time, amount, SUM(amount) OVER ( PARTITION BY product ORDER BY order_time RANGE BETWEEN INTERVAL \u0026#39;1\u0026#39; HOUR PRECEDING AND CURRENT ROW ) AS one_hour_prod_amount_sum FROM Orders The syntax for an OVER window is summarized below.
SELECT agg_func(agg_col) OVER ( [PARTITION BY col1[, col2, ...]] ORDER BY time_col range_definition), ... FROM ... You can define multiple OVER window aggregates in a SELECT clause. However, for streaming queries, the OVER windows for all aggregates must be identical due to current limitation.
ORDER BY # OVER windows are defined on an ordered sequence of rows. Since tables do not have an inherent order, the ORDER BY clause is mandatory. For streaming queries, Flink currently only supports OVER windows that are defined with an ascending time attributes order. Additional orderings are not supported.
PARTITION BY # OVER windows can be defined on a partitioned table. In presence of a PARTITION BY clause, the aggregate is computed for each input row only over the rows of its partition.
Range Definitions # The range definition specifies how many rows are included in the aggregate. The range is defined with a BETWEEN clause that defines a lower and an upper boundary. All rows between these boundaries are included in the aggregate. Flink only supports CURRENT ROW as the upper boundary.
There are two options to define the range, ROWS intervals and RANGE intervals.
RANGE intervals # A RANGE interval is defined on the values of the ORDER BY column, which is in case of Flink always a time attribute. The following RANGE interval defines that all rows with a time attribute of at most 30 minutes less than the current row are included in the aggregate.
RANGE BETWEEN INTERVAL \u0026#39;30\u0026#39; MINUTE PRECEDING AND CURRENT ROW ROW intervals # A ROWS interval is a count-based interval. It defines exactly how many rows are included in the aggregate. The following ROWS interval defines that the 10 rows preceding the current row and the current row (so 11 rows in total) are included in the aggregate.
ROWS BETWEEN 10 PRECEDING AND CURRENT ROW WINDOW The WINDOW clause can be used to define an OVER window outside of the SELECT clause. It can make queries more readable and also allows us to reuse the window definition for multiple aggregates.
SELECT order_id, order_time, amount, SUM(amount) OVER w AS sum_amount, AVG(amount) OVER w AS avg_amount FROM Orders WINDOW w AS ( PARTITION BY product ORDER BY order_time RANGE BETWEEN INTERVAL \u0026#39;1\u0026#39; HOUR PRECEDING AND CURRENT ROW) Back to top
`}),e.add({id:220,href:"/flink/flink-docs-master/docs/connectors/table/formats/parquet/",title:"Parquet",section:"Formats",content:` Parquet Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Parquet format allows to read and write Parquet data.
Dependencies # In order to use the Parquet format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-parquet\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a table with Parquet format # Here is an example to create a table using Filesystem connector and Parquet format.
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3), dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/tmp/user_behavior\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;parquet\u0026#39; ) Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'parquet'. parquet.utc-timezone optional false Boolean Use UTC timezone or local timezone to the conversion between epoch time and LocalDateTime. Hive 0.x/1.x/2.x use local timezone. But Hive 3.x use UTC timezone. Parquet format also supports configuration from ParquetOutputFormat. For example, you can configure parquet.compression=GZIP to enable gzip compression.
Data Type Mapping # Currently, Parquet format type mapping is compatible with Apache Hive, but different with Apache Spark:
Timestamp: mapping timestamp type to int96 whatever the precision is. Decimal: mapping decimal type to fixed length byte array according to the precision. The following table lists the type mapping from Flink type to Parquet type.
Flink Data Type Parquet type Parquet logical type CHAR / VARCHAR / STRING BINARY UTF8 BOOLEAN BOOLEAN BINARY / VARBINARY BINARY DECIMAL FIXED_LEN_BYTE_ARRAY DECIMAL TINYINT INT32 INT_8 SMALLINT INT32 INT_16 INT INT32 BIGINT INT64 FLOAT FLOAT DOUBLE DOUBLE DATE INT32 DATE TIME INT32 TIME_MILLIS TIMESTAMP INT96 ARRAY LIST MAP MAP ROW STRUCT Composite data type: Array, Map and Row are currently only supported when writing, not reading. `}),e.add({id:221,href:"/flink/flink-docs-master/docs/connectors/datastream/pulsar/",title:"Pulsar",section:"DataStream Connectors",content:` Apache Pulsar Connector # Flink provides an Apache Pulsar connector for reading and writing data from and to Pulsar topics with exactly-once guarantees.
Dependency # You can use the connector with the Pulsar 2.8.1 or higher. Because the Pulsar connector supports Pulsar transactions, it is recommended to use the Pulsar 2.9.2 or higher. Details on Pulsar compatibility can be found in PIP-72.
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-pulsar\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! In order to use the Pulsar connector in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases. See Python dependency management for more details on how to use JARs in PyFlink. Flink\u0026rsquo;s streaming connectors are not part of the binary distribution. See how to link with them for cluster execution here.
Pulsar Source # This part describes the Pulsar source based on the new data source API. Usage # The Pulsar source provides a builder class for constructing a PulsarSource instance. The code snippet below builds a PulsarSource instance. It consumes messages from the earliest cursor of the topic \u0026ldquo;persistent://public/default/my-topic\u0026rdquo; in Exclusive subscription type (my-subscription) and deserializes the raw payload of the messages as strings.
Java PulsarSource\u0026lt;String\u0026gt; source = PulsarSource.builder() .setServiceUrl(serviceUrl) .setAdminUrl(adminUrl) .setStartCursor(StartCursor.earliest()) .setTopics(\u0026#34;my-topic\u0026#34;) .setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema())) .setSubscriptionName(\u0026#34;my-subscription\u0026#34;) .setSubscriptionType(SubscriptionType.Exclusive) .build(); env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Pulsar Source\u0026#34;); Python pulsar_source = PulsarSource.builder() \\ .set_service_url(\u0026#39;pulsar://localhost:6650\u0026#39;) \\ .set_admin_url(\u0026#39;http://localhost:8080\u0026#39;) \\ .set_start_cursor(StartCursor.earliest()) \\ .set_topics(\u0026#34;my-topic\u0026#34;) \\ .set_deserialization_schema( PulsarDeserializationSchema.flink_schema(SimpleStringSchema())) \\ .set_subscription_name(\u0026#39;my-subscription\u0026#39;) \\ .set_subscription_type(SubscriptionType.Exclusive) \\ .build() env.from_source(source=pulsar_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;pulsar source\u0026#34;) The following properties are required for building a PulsarSource:
Pulsar service URL, configured by setServiceUrl(String) Pulsar service HTTP URL (also known as admin URL), configured by setAdminUrl(String) Pulsar subscription name, configured by setSubscriptionName(String) Topics / partitions to subscribe, see the following topic-partition subscription for more details. Deserializer to parse Pulsar messages, see the following deserializer for more details. It is recommended to set the consumer name in Pulsar Source by setConsumerName(String). This sets a unique name for the Flink connector in the Pulsar statistic dashboard. You can use it to monitor the performance of your Flink connector and applications.
Topic-partition Subscription # Pulsar source provide two ways of topic-partition subscription:
Topic list, subscribing messages from all partitions in a list of topics. For example: Java PulsarSource.builder().setTopics(\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;); // Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; PulsarSource.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;); Python PulsarSource.builder().set_topics([\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;]) # Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; PulsarSource.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;]) Topic pattern, subscribing messages from all topics whose name matches the provided regular expression. For example: Java PulsarSource.builder().setTopicPattern(\u0026#34;topic-*\u0026#34;); Python PulsarSource.builder().set_topic_pattern(\u0026#34;topic-*\u0026#34;) Flexible Topic Naming # Since Pulsar 2.0, all topic names internally are in a form of {persistent|non-persistent}://tenant/namespace/topic. Now, for partitioned topics, you can use short names in many cases (for the sake of simplicity). The flexible naming system stems from the fact that there is now a default topic type, tenant, and namespace in a Pulsar cluster.
Topic property Default topic type persistent tenant public namespace default This table lists a mapping relationship between your input topic name and the translated topic name:
Input topic name Translated topic name my-topic persistent://public/default/my-topic my-tenant/my-namespace/my-topic persistent://my-tenant/my-namespace/my-topic For non-persistent topics, you need to specify the entire topic name, as the default-based rules do not apply for non-partitioned topics. Thus, you cannot use a short name like non-persistent://my-topic and need to use non-persistent://public/default/my-topic instead. Subscribing Pulsar Topic Partition # Internally, Pulsar divides a partitioned topic as a set of non-partitioned topics according to the partition size.
For example, if a simple-string topic with 3 partitions is created under the sample tenant with the flink namespace. The topics on Pulsar would be:
Topic name Partitioned persistent://sample/flink/simple-string Y persistent://sample/flink/simple-string-partition-0 N persistent://sample/flink/simple-string-partition-1 N persistent://sample/flink/simple-string-partition-2 N You can directly consume messages from the topic partitions by using the non-partitioned topic names above. For example, use PulsarSource.builder().setTopics(\u0026quot;sample/flink/simple-string-partition-1\u0026quot;, \u0026quot;sample/flink/simple-string-partition-2\u0026quot;) would consume the partitions 1 and 2 of the sample/flink/simple-string topic.
Setting Topic Patterns # The Pulsar source extracts the topic type (persistent or non-persistent) from the provided topic pattern. For example, you can use the PulsarSource.builder().setTopicPattern(\u0026quot;non-persistent://my-topic*\u0026quot;) to specify a non-persistent topic. By default, a persistent topic is created if you do not specify the topic type in the regular expression.
You can use setTopicPattern(\u0026quot;topic-*\u0026quot;, RegexSubscriptionMode.AllTopics) to consume both persistent and non-persistent topics based on the topic pattern. The Pulsar source would filter the available topics by the RegexSubscriptionMode.
Deserializer # A deserializer (PulsarDeserializationSchema) is for decoding Pulsar messages from bytes. You can configure the deserializer using setDeserializationSchema(PulsarDeserializationSchema). The PulsarDeserializationSchema defines how to deserialize a Pulsar Message\u0026lt;byte[]\u0026gt;.
If only the raw payload of a message (message data in bytes) is needed, you can use the predefined PulsarDeserializationSchema. Pulsar connector provides three implementation methods.
Decode the message by using Pulsar\u0026rsquo;s Schema.
// Primitive types PulsarDeserializationSchema.pulsarSchema(Schema); // Struct types (JSON, Protobuf, Avro, etc.) PulsarDeserializationSchema.pulsarSchema(Schema, Class); // KeyValue type PulsarDeserializationSchema.pulsarSchema(Schema, Class, Class); Decode the message by using Flink\u0026rsquo;s DeserializationSchema Java PulsarDeserializationSchema.flinkSchema(DeserializationSchema); Python PulsarDeserializationSchema.flink_schema(DeserializationSchema) Decode the message by using Flink\u0026rsquo;s TypeInformation Java PulsarDeserializationSchema.flinkTypeInfo(TypeInformation, ExecutionConfig); Python PulsarDeserializationSchema.flink_type_info(TypeInformation) Pulsar Message\u0026lt;byte[]\u0026gt; contains some extra properties, such as message key, message publish time, message time, and application-defined key/value pairs etc. These properties could be defined in the Message\u0026lt;byte[]\u0026gt; interface.
If you want to deserialize the Pulsar message by these properties, you need to implement PulsarDeserializationSchema. Ensure that the TypeInformation from the PulsarDeserializationSchema.getProducedType() is correct. Flink uses this TypeInformation to pass the messages to downstream operators.
Pulsar Subscriptions # A Pulsar subscription is a named configuration rule that determines how messages are delivered to Flink readers. The subscription name is required for consuming messages. Pulsar connector supports four subscription types:
Exclusive Shared Failover Key_Shared There is no difference between Exclusive and Failover in the Pulsar connector. When a Flink reader crashes, all (non-acknowledged and subsequent) messages are redelivered to the available Flink readers.
By default, if no subscription type is defined, Pulsar source uses the Shared subscription type.
Java // Shared subscription with name \u0026#34;my-shared\u0026#34; PulsarSource.builder().setSubscriptionName(\u0026#34;my-shared\u0026#34;); // Exclusive subscription with name \u0026#34;my-exclusive\u0026#34; PulsarSource.builder().setSubscriptionName(\u0026#34;my-exclusive\u0026#34;).setSubscriptionType(SubscriptionType.Exclusive); Python # Shared subscription with name \u0026#34;my-shared\u0026#34; PulsarSource.builder().set_subscription_name(\u0026#34;my-shared\u0026#34;) # Exclusive subscription with name \u0026#34;my-exclusive\u0026#34; PulsarSource.builder().set_subscription_name(\u0026#34;my-exclusive\u0026#34;).set_subscription_type(SubscriptionType.Exclusive) Ensure that you provide a RangeGenerator implementation if you want to use the Key_Shared subscription type on the Pulsar connector. The RangeGenerator generates a set of key hash ranges so that a respective reader subtask only dispatches messages where the hash of the message key is contained in the specified range.
The Pulsar connector uses UniformRangeGenerator that divides the range by the Flink source parallelism if no RangeGenerator is provided in the Key_Shared subscription type.
Starting Position # The Pulsar source is able to consume messages starting from different positions by setting the setStartCursor(StartCursor) option. Built-in start cursors include:
Start from the earliest available message in the topic. Java StartCursor.earliest(); Python StartCursor.earliest() Start from the latest available message in the topic. Java StartCursor.latest(); Python StartCursor.latest() Start from a specified message between the earliest and the latest. The Pulsar connector consumes from the latest available message if the message ID does not exist.
The start message is included in consuming result. Java StartCursor.fromMessageId(MessageId); Python StartCursor.from_message_id(message_id) Start from a specified message between the earliest and the latest. The Pulsar connector consumes from the latest available message if the message ID doesn\u0026rsquo;t exist.
Include or exclude the start message by using the second boolean parameter. Java StartCursor.fromMessageId(MessageId, boolean); Python StartCursor.from_message_id(message_id, boolean) Start from the specified message publish time by Message\u0026lt;byte[]\u0026gt;.getPublishTime(). This method is deprecated because the name is totally wrong which may cause confuse. You can use StartCursor.fromPublishTime(long) instead.
Java StartCursor.fromMessageTime(long); Python StartCursor.from_message_time(int) Start from the specified message publish time by Message\u0026lt;byte[]\u0026gt;.getPublishTime(). Java StartCursor.fromPublishTime(long); Python StartCursor.from_publish_time(int) Each Pulsar message belongs to an ordered sequence on its topic. The sequence ID (MessageId) of the message is ordered in that sequence. The MessageId contains some extra information (the ledger, entry, partition) about how the message is stored, you can create a MessageId by using DefaultImplementation.newMessageId(long ledgerId, long entryId, int partitionIndex). Boundedness # The Pulsar source supports streaming and batch execution mode. By default, the PulsarSource is configured for unbounded data.
For unbounded data the Pulsar source never stops until a Flink job is stopped or failed. You can use the setUnboundedStopCursor(StopCursor) to set the Pulsar source to stop at a specific stop position.
You can use setBoundedStopCursor(StopCursor) to specify a stop position for bounded data.
Built-in stop cursors include:
The Pulsar source never stops consuming messages. Java StopCursor.never(); Python StopCursor.never() Stop at the latest available message when the Pulsar source starts consuming messages. Java StopCursor.latest(); Python StopCursor.latest() Stop when the connector meets a given message, or stop at a message which is produced after this given message. Java StopCursor.atMessageId(MessageId); Python StopCursor.at_message_id(message_id) Stop but include the given message in the consuming result. Java StopCursor.afterMessageId(MessageId); Python StopCursor.after_message_id(message_id) Stop at the specified event time by Message\u0026lt;byte[]\u0026gt;.getEventTime(). The message with the given event time won\u0026rsquo;t be included in the consuming result. Java StopCursor.atEventTime(long); Python StopCursor.at_event_time(int) Stop after the specified event time by Message\u0026lt;byte[]\u0026gt;.getEventTime(). The message with the given event time will be included in the consuming result. Java StopCursor.afterEventTime(long); Python StopCursor.after_event_time(int) Stop at the specified publish time by Message\u0026lt;byte[]\u0026gt;.getPublishTime(). The message with the given publish time won\u0026rsquo;t be included in the consuming result. Java StopCursor.atPublishTime(long); Python StopCursor.at_publish_time(int) Stop after the specified publish time by Message\u0026lt;byte[]\u0026gt;.getPublishTime(). The message with the given publish time will be included in the consuming result. Java StopCursor.afterPublishTime(long); Python StopCursor.after_publish_time(int) Source Configurable Options # In addition to configuration options described above, you can set arbitrary options for PulsarClient, PulsarAdmin, Pulsar Consumer and PulsarSource by using setConfig(ConfigOption\u0026lt;T\u0026gt;, T), setConfig(Configuration) and setConfig(Properties).
PulsarClient Options # The Pulsar connector uses the client API to create the Consumer instance. The Pulsar connector extracts most parts of Pulsar\u0026rsquo;s ClientConfigurationData, which is required for creating a PulsarClient, as Flink configuration options in PulsarOptions.
Key Default Type Description pulsar.client.authParamMap (none) Map Parameters for the authentication plugin. pulsar.client.authParams (none) String Parameters for the authentication plugin.
Example:
key1:val1,key2:val2 pulsar.client.authPluginClassName (none) String Name of the authentication plugin. pulsar.client.concurrentLookupRequest 5000 Integer The number of concurrent lookup requests allowed to send on each broker connection to prevent overload on the broker. It should be configured with a higher value only in case of it requires to produce or subscribe on thousands of topic using a created PulsarClient pulsar.client.connectionTimeoutMs 10000 Integer Duration (in ms) of waiting for a connection to a broker to be established.
If the duration passes without a response from a broker, the connection attempt is dropped. pulsar.client.connectionsPerBroker 1 Integer The maximum number of connections that the client library will open to a single broker.
By default, the connection pool will use a single connection for all the producers and consumers. Increasing this parameter may improve throughput when using many producers over a high latency connection. pulsar.client.enableBusyWait false Boolean Option to enable busy-wait settings.
This option will enable spin-waiting on executors and IO threads in order to reduce latency during context switches. The spinning will consume 100% CPU even when the broker is not doing any work. It is recommended to reduce the number of IO threads and BookKeeper client threads to only have fewer CPU cores busy. pulsar.client.enableTransaction false Boolean If transaction is enabled, start the transactionCoordinatorClient with PulsarClient. pulsar.client.initialBackoffIntervalNanos 100000000 Long Default duration (in nanoseconds) for a backoff interval. pulsar.client.keepAliveIntervalSeconds 30 Integer Interval (in seconds) for keeping connection between the Pulsar client and broker alive. pulsar.client.listenerName (none) String Configure the listenerName that the broker will return the corresponding advertisedListener. pulsar.client.maxBackoffIntervalNanos 60000000000 Long The maximum duration (in nanoseconds) for a backoff interval. pulsar.client.maxLookupRedirects 20 Integer The maximum number of times a lookup-request redirections to a broker. pulsar.client.maxLookupRequest 50000 Integer The maximum number of lookup requests allowed on each broker connection to prevent overload on the broker. It should be greater than pulsar.client.concurrentLookupRequest. Requests that inside pulsar.client.concurrentLookupRequest are already sent to broker, and requests beyond pulsar.client.concurrentLookupRequest and under maxLookupRequests will wait in each client cnx. pulsar.client.maxNumberOfRejectedRequestPerConnection 50 Integer The maximum number of rejected requests of a broker in a certain period (30s) after the current connection is closed and the client creates a new connection to connect to a different broker. pulsar.client.memoryLimitBytes 67108864 Long The limit (in bytes) on the amount of direct memory that will be allocated by this client instance.
Note: at this moment this is only limiting the memory for producers. Setting this to 0 will disable the limit. pulsar.client.numIoThreads 1 Integer The number of threads used for handling connections to brokers. pulsar.client.numListenerThreads 1 Integer The number of threads used for handling message listeners. The listener thread pool is shared across all the consumers and readers that are using a listener model to get messages. For a given consumer, the listener is always invoked from the same thread to ensure ordering. pulsar.client.operationTimeoutMs 30000 Integer Operation timeout (in ms). Operations such as creating producers, subscribing or unsubscribing topics are retried during this interval. If the operation is not completed during this interval, the operation will be marked as failed. pulsar.client.proxyProtocol SNI Enum
Protocol type to determine the type of proxy routing when a client connects to the proxy using pulsar.client.proxyServiceUrl.
Possible values:"SNI" pulsar.client.proxyServiceUrl (none) String Proxy-service URL when a client connects to the broker via the proxy. The client can choose the type of proxy-routing. pulsar.client.requestTimeoutMs 60000 Integer Maximum duration (in ms) for completing a request. This config option is not supported before Pulsar 2.8.1 pulsar.client.serviceUrl (none) String Service URL provider for Pulsar service.
To connect to Pulsar using client libraries, you need to specify a Pulsar protocol URL.
You can assign Pulsar protocol URLs to specific clusters and use the Pulsar scheme.
This is an example of localhost: pulsar://localhost:6650.If you have multiple brokers, the URL is as: pulsar://localhost:6550,localhost:6651,localhost:6652A URL for a production Pulsar cluster is as: pulsar://pulsar.us-west.example.com:6650If you use TLS authentication, the URL is as pulsar+ssl://pulsar.us-west.example.com:6651 pulsar.client.sslProvider (none) String The name of the security provider used for SSL connections. The default value is the default security provider of the JVM. pulsar.client.statsIntervalSeconds 60 Long Interval between each stats info.
Stats is activated with positive statsIntervalSet statsIntervalSeconds to 1 second at least. pulsar.client.tlsAllowInsecureConnection false Boolean Whether the Pulsar client accepts untrusted TLS certificate from the broker. pulsar.client.tlsCiphers List\u0026lt;String\u0026gt; A list of cipher suites. This is a named combination of authentication, encryption, MAC and the key exchange algorithm used to negotiate the security settings for a network connection using the TLS or SSL network protocol. By default all the available cipher suites are supported. pulsar.client.tlsHostnameVerificationEnable false Boolean Whether to enable TLS hostname verification. It allows to validate hostname verification when a client connects to the broker over TLS. It validates incoming x509 certificate and matches provided hostname (CN/SAN) with the expected broker's host name. It follows RFC 2818, 3.1. Server Identity hostname verification. pulsar.client.tlsProtocols List\u0026lt;String\u0026gt; The SSL protocol used to generate the SSLContext. By default, it is set TLS, which is fine for most cases. Allowed values in recent JVMs are TLS, TLSv1.3, TLSv1.2 and TLSv1.1. pulsar.client.tlsTrustCertsFilePath (none) String Path to the trusted TLS certificate file. pulsar.client.tlsTrustStorePassword (none) String The store password for the key store file. pulsar.client.tlsTrustStorePath (none) String The location of the trust store file. pulsar.client.tlsTrustStoreType "JKS" String The file format of the trust store file. pulsar.client.useKeyStoreTls false Boolean If TLS is enabled, whether use the KeyStore type as the TLS configuration parameter. If it is set to false, it means to use the default pem type configuration. pulsar.client.useTcpNoDelay true Boolean Whether to use the TCP no-delay flag on the connection to disable Nagle algorithm.
No-delay features ensures that packets are sent out on the network as soon as possible, and it is critical to achieve low latency publishes. On the other hand, sending out a huge number of small packets might limit the overall throughput. Therefore, if latency is not a concern, it is recommended to set this option to false.
By default, it is set to true. PulsarAdmin Options # The admin API is used for querying topic metadata and for discovering the desired topics when the Pulsar connector uses topic-pattern subscription. It shares most part of the configuration options with the client API. The configuration options listed here are only used in the admin API. They are also defined in PulsarOptions.
Key Default Type Description pulsar.admin.adminUrl (none) String The Pulsar service HTTP URL for the admin endpoint. For example, http://my-broker.example.com:8080, or https://my-broker.example.com:8443 for TLS. pulsar.admin.autoCertRefreshTime 300000 Integer The auto cert refresh time (in ms) if Pulsar admin supports TLS authentication. pulsar.admin.connectTimeout 60000 Integer The connection time out (in ms) for the PulsarAdmin client. pulsar.admin.readTimeout 60000 Integer The server response read timeout (in ms) for the PulsarAdmin client for any request. pulsar.admin.requestTimeout 300000 Integer The server request timeout (in ms) for the PulsarAdmin client for any request. Pulsar Consumer Options # In general, Pulsar provides the Reader API and Consumer API for consuming messages in different scenarios. The Pulsar connector uses the Consumer API. It extracts most parts of Pulsar\u0026rsquo;s ConsumerConfigurationData as Flink configuration options in PulsarSourceOptions.
Key Default Type Description pulsar.consumer.ackReceiptEnabled false Boolean Acknowledgement will return a receipt but this does not mean that the message will not be resent after getting the receipt. pulsar.consumer.ackTimeoutMillis 0 Long The timeout (in ms) for unacknowledged messages, truncated to the nearest millisecond. The timeout needs to be greater than 1 second.
By default, the acknowledge timeout is disabled and that means that messages delivered to a consumer will not be re-delivered unless the consumer crashes.
When acknowledgement timeout being enabled, if a message is not acknowledged within the specified timeout it will be re-delivered to the consumer (possibly to a different consumer in case of a shared subscription). pulsar.consumer.acknowledgementsGroupTimeMicros 100000 Long Group a consumer acknowledgment for a specified time (in μs). By default, a consumer uses 100μs grouping time to send out acknowledgments to a broker. If the group time is set to 0, acknowledgments are sent out immediately. A longer ack group time is more efficient at the expense of a slight increase in message re-deliveries after a failure. pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull false Boolean Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once a consumer reaches this threshold, it drops the outstanding unchunked-messages by silently acknowledging if pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull is true. Otherwise, it marks them for redelivery. pulsar.consumer.autoUpdatePartitionsIntervalSeconds 60 Integer The interval (in seconds) of updating partitions. This only works if autoUpdatePartitions is enabled. pulsar.consumer.consumerName (none) String The consumer name is informative and it can be used to identify a particular consumer instance from the topic stats. pulsar.consumer.cryptoFailureAction FAIL Enum
The consumer should take action when it receives a message that can not be decrypted.FAIL: this is the default option to fail messages until crypto succeeds.DISCARD: silently acknowledge but do not deliver messages to an application.CONSUME: deliver encrypted messages to applications. It is the application's responsibility to decrypt the message.
Fail to decompress the messages.
If messages contain batch messages, a client is not be able to retrieve individual messages in batch.
The delivered encrypted message contains EncryptionContext which contains encryption and compression information in. You can use an application to decrypt the consumed message payload.
Possible values:"FAIL""DISCARD""CONSUME" pulsar.consumer.deadLetterPolicy.deadLetterTopic (none) String Name of the dead topic where the failed messages are sent. pulsar.consumer.deadLetterPolicy.maxRedeliverCount 0 Integer The maximum number of times that a message are redelivered before being sent to the dead letter queue. pulsar.consumer.deadLetterPolicy.retryLetterTopic (none) String Name of the retry topic where the failed messages are sent. pulsar.consumer.expireTimeOfIncompleteChunkedMessageMillis 60000 Long If a producer fails to publish all the chunks of a message, the consumer can expire incomplete chunks if the consumer cannot receive all chunks in expire times (default 1 hour, in ms). pulsar.consumer.maxPendingChunkedMessage 10 Integer The consumer buffers chunk messages into memory until it receives all the chunks of the original message. While consuming chunk-messages, chunks from the same message might not be contiguous in the stream and they might be mixed with other messages' chunks. So, consumer has to maintain multiple buffers to manage chunks coming from different messages. This mainly happens when multiple publishers are publishing messages on the topic concurrently or publishers failed to publish all chunks of the messages.
For example, there are M1-C1, M2-C1, M1-C2, M2-C2 messages.Messages M1-C1 and M1-C2 belong to the M1 original message while M2-C1 and M2-C2 belong to the M2 message.
Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once, a consumer reaches this threshold, it drops the outstanding unchunked messages by silently acknowledging or asking the broker to redeliver messages later by marking it unacknowledged. This behavior can be controlled by the pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull option. pulsar.consumer.maxTotalReceiverQueueSizeAcrossPartitions 50000 Integer The maximum total receiver queue size across partitions.
This setting reduces the receiver queue size for individual partitions if the total receiver queue size exceeds this value. pulsar.consumer.negativeAckRedeliveryDelayMicros 60000000 Long Delay (in μs) to wait before redelivering messages that failed to be processed.
When an application uses Consumer.negativeAcknowledge(Message), failed messages are redelivered after a fixed timeout. pulsar.consumer.poolMessages false Boolean Enable pooling of messages and the underlying data buffers. pulsar.consumer.priorityLevel 0 Integer Priority level for a consumer to which a broker gives more priorities while dispatching messages in the shared subscription type.
The broker follows descending priorities. For example, 0=max-priority, 1, 2,...
In shared subscription mode, the broker first dispatches messages to the consumers on the highest priority level if they have permits. Otherwise, the broker considers consumers on the next priority level.
Example 1
If a subscription has consumer A with priorityLevel 0 and consumer B with priorityLevel 1, then the broker only dispatches messages to consumer A until it runs out permits and then starts dispatching messages to consumer B.
Example 2
Consumer Priority, Level, Permits C1, 0, 2 C2, 0, 1 C3, 0, 1 C4, 1, 2 C5, 1, 1 The order in which a broker dispatches messages to consumers is: C1, C2, C3, C1, C4, C5, C4. pulsar.consumer.properties Map A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification. pulsar.consumer.readCompacted false Boolean If enabling readCompacted, a consumer reads messages from a compacted topic rather than reading a full message backlog of a topic.
A consumer only sees the latest value for each key in the compacted topic, up until reaching the point in the topic message when compacting backlog. Beyond that point, send messages as normal.
Only enabling readCompacted on subscriptions to persistent topics, which have a single active consumer (like failure or exclusive subscriptions).
Attempting to enable it on subscriptions to non-persistent topics or on shared subscriptions leads to a subscription call throwing a PulsarClientException. pulsar.consumer.receiverQueueSize 1000 Integer Size of a consumer's receiver queue.
For example, the number of messages accumulated by a consumer before an application calls Receive.
A value higher than the default value increases consumer throughput, though at the expense of more memory utilization. pulsar.consumer.replicateSubscriptionState false Boolean If replicateSubscriptionState is enabled, a subscription state is replicated to geo-replicated clusters. pulsar.consumer.retryEnable false Boolean If enabled, the consumer will automatically retry messages. pulsar.consumer.subscriptionMode Durable Enum
Select the subscription mode to be used when subscribing to the topic.Durable: Make the subscription to be backed by a durable cursor that will retain messages and persist the current position.NonDurable: Lightweight subscription mode that doesn't have a durable cursor associated
Possible values:"Durable""NonDurable" pulsar.consumer.subscriptionName (none) String Specify the subscription name for this consumer. This argument is required when constructing the consumer. pulsar.consumer.subscriptionType Shared Enum
Subscription type.
Four subscription types are available:ExclusiveFailoverSharedKey_Shared
Possible values:"Exclusive""Shared""Failover""Key_Shared" pulsar.consumer.tickDurationMillis 1000 Long Granularity (in ms) of the ack-timeout redelivery.
A greater (for example, 1 hour) tickDurationMillis reduces the memory overhead to track messages. PulsarSource Options # The configuration options below are mainly used for customizing the performance and message acknowledgement behavior. You can ignore them if you do not have any performance issues.
Key Default Type Description pulsar.source.autoCommitCursorInterval 5000 Long This option is used only when the user disables the checkpoint and uses Exclusive or Failover subscription. We would automatically commit the cursor using the given period (in ms). pulsar.source.enableAutoAcknowledgeMessage false Boolean Flink commits the consuming position with pulsar transactions on checkpoint. However, if you have disabled the Flink checkpoint or disabled transaction for your Pulsar cluster, ensure that you have set this option to true.
The source would use pulsar client's internal mechanism and commit cursor in two ways.For Key_Shared and Shared subscription, the cursor would be committed once the message is consumed.For Exclusive and Failover subscription, the cursor would be committed in a given interval. pulsar.source.maxFetchRecords 100 Integer The maximum number of records to fetch to wait when polling. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchTime. pulsar.source.maxFetchTime 10000 Long The maximum time (in ms) to wait when fetching records. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchRecords. pulsar.source.partitionDiscoveryIntervalMs 30000 Long The interval (in ms) for the Pulsar source to discover the new partitions. A non-positive value disables the partition discovery. pulsar.source.transactionTimeoutMillis 10800000 Long This option is used in Shared or Key_Shared subscription. You should configure this option when you do not enable the pulsar.source.enableAutoAcknowledgeMessage option.
The value (in ms) should be greater than the checkpoint interval. pulsar.source.verifyInitialOffsets WARN_ON_MISMATCH Enum
Upon (re)starting the source, check whether the expected message can be read. If failure is enabled, the application fails. Otherwise, it logs a warning. A possible solution is to adjust the retention settings in Pulsar or ignoring the check result.
Possible values:"FAIL_ON_MISMATCH": Fail the consuming from Pulsar when we don't find the related cursor."WARN_ON_MISMATCH": Print a warn message and start consuming from the valid offset. Dynamic Partition Discovery # To handle scenarios like topic scaling-out or topic creation without restarting the Flink job, the Pulsar source periodically discover new partitions under a provided topic-partition subscription pattern. To enable partition discovery, you can set a non-negative value for the PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS option:
Java // discover new partitions per 10 seconds PulsarSource.builder() .setConfig(PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS, 10000); Python # discover new partitions per 10 seconds PulsarSource.builder() .set_config(\u0026#34;pulsar.source.partitionDiscoveryIntervalMs\u0026#34;, 10000) Partition discovery is enabled by default. The Pulsar connector queries the topic metadata every 30 seconds. To disable partition discovery, you need to set a negative partition discovery interval. Partition discovery is disabled for bounded data even if you set this option with a non-negative value. Event Time and Watermarks # By default, the message uses the timestamp embedded in Pulsar Message\u0026lt;byte[]\u0026gt; as the event time. You can define your own WatermarkStrategy to extract the event time from the message, and emit the watermark downstream:
Java env.fromSource(pulsarSource, new CustomWatermarkStrategy(), \u0026#34;Pulsar Source With Custom Watermark Strategy\u0026#34;); Python env.from_source(pulsar_source, CustomWatermarkStrategy(), \u0026#34;Pulsar Source With Custom Watermark Strategy\u0026#34;) This documentation describes details about how to define a WatermarkStrategy.
Message Acknowledgement # When a subscription is created, Pulsar retains all messages, even if the consumer is disconnected. The retained messages are discarded only when the connector acknowledges that all these messages are processed successfully. The Pulsar connector supports four subscription types, which makes the acknowledgement behaviors vary among different subscriptions.
Acknowledgement on Exclusive and Failover Subscription Types # Exclusive and Failover subscription types support cumulative acknowledgment. In these subscription types, Flink only needs to acknowledge the latest successfully consumed message. All the message before the given message are marked with a consumed status.
The Pulsar source acknowledges the current consuming message when checkpoints are completed, to ensure the consistency between Flink\u0026rsquo;s checkpoint state and committed position on the Pulsar brokers.
If checkpointing is disabled, Pulsar source periodically acknowledges messages. You can use the PulsarSourceOptions.PULSAR_AUTO_COMMIT_CURSOR_INTERVAL option to set the acknowledgement period.
Pulsar source does NOT rely on committed positions for fault tolerance. Acknowledging messages is only for exposing the progress of consumers and monitoring on these two subscription types.
Acknowledgement on Shared and Key_Shared Subscription Types # In Shared and Key_Shared subscription types, messages are acknowledged one by one. You can acknowledge a message in a transaction and commit it to Pulsar.
You should enable transaction in the Pulsar borker.conf file when using these two subscription types in connector:
transactionCoordinatorEnabled=true The default timeout for Pulsar transactions is 3 hours. Make sure that that timeout is greater than checkpoint interval + maximum recovery time. A shorter checkpoint interval indicates a better consuming performance. You can use the PulsarSourceOptions.PULSAR_TRANSACTION_TIMEOUT_MILLIS option to change the transaction timeout.
If checkpointing is disabled or you can not enable the transaction on Pulsar broker, you should set PulsarSourceOptions.PULSAR_ENABLE_AUTO_ACKNOWLEDGE_MESSAGE to true. The message is immediately acknowledged after consuming. No consistency guarantees can be made in this scenario.
All acknowledgements in a transaction are recorded in the Pulsar broker side. Pulsar Sink # The Pulsar Sink supports writing records into one or more Pulsar topics or a specified list of Pulsar partitions.
This part describes the Pulsar sink based on the new data sink API.
If you still want to use the legacy SinkFunction or on Flink 1.14 or previous releases, just use the StreamNative\u0026rsquo;s pulsar-flink.
Usage # The Pulsar Sink uses a builder class to construct the PulsarSink instance. This example writes a String record to a Pulsar topic with at-least-once delivery guarantee.
Java DataStream\u0026lt;String\u0026gt; stream = ... PulsarSink\u0026lt;String\u0026gt; sink = PulsarSink.builder() .setServiceUrl(serviceUrl) .setAdminUrl(adminUrl) .setTopics(\u0026#34;topic1\u0026#34;) .setSerializationSchema(PulsarSerializationSchema.flinkSchema(new SimpleStringSchema())) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build(); stream.sinkTo(sink); Python stream = ... pulsar_sink = PulsarSink.builder() \\ .set_service_url(\u0026#39;pulsar://localhost:6650\u0026#39;) \\ .set_admin_url(\u0026#39;http://localhost:8080\u0026#39;) \\ .set_topics(\u0026#34;topic1\u0026#34;) \\ .set_serialization_schema(PulsarSerializationSchema.flink_schema(SimpleStringSchema())) \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .build() stream.sink_to(pulsar_sink) The following properties are required for building PulsarSink:
Pulsar service url, configured by setServiceUrl(String) Pulsar service http url (aka. admin url), configured by setAdminUrl(String) Topics / partitions to write, see writing targets for more details. Serializer to generate Pulsar messages, see serializer for more details. It is recommended to set the producer name in Pulsar Source by setProducerName(String). This sets a unique name for the Flink connector in the Pulsar statistic dashboard. You can use it to monitor the performance of your Flink connector and applications.
Producing to topics # Defining the topics for producing is similar to the topic-partition subscription in the Pulsar source. We support a mix-in style of topic setting. You can provide a list of topics, partitions, or both of them.
Java // Topic \u0026#34;some-topic1\u0026#34; and \u0026#34;some-topic2\u0026#34; PulsarSink.builder().setTopics(\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;) // Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; PulsarSink.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;) // Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; and topic \u0026#34;some-topic2\u0026#34; PulsarSink.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;, \u0026#34;some-topic2\u0026#34;) Python # Topic \u0026#34;some-topic1\u0026#34; and \u0026#34;some-topic2\u0026#34; PulsarSink.builder().set_topics([\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;]) # Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; PulsarSink.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;]) # Partition 0 and 2 of topic \u0026#34;topic-a\u0026#34; and topic \u0026#34;some-topic2\u0026#34; PulsarSink.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;, \u0026#34;some-topic2\u0026#34;]) The topics you provide support auto partition discovery. We query the topic metadata from the Pulsar in a fixed interval. You can use the PulsarSinkOptions.PULSAR_TOPIC_METADATA_REFRESH_INTERVAL option to change the discovery interval option.
Configuring writing targets can be replaced by using a custom [TopicRouter] message routing. Configuring partitions on the Pulsar connector is explained in the flexible topic naming section.
If you build the Pulsar sink based on both the topic and its corresponding partitions, Pulsar sink merges them and only uses the topic.
For example, when using the PulsarSink.builder().setTopics(\u0026quot;some-topic1\u0026quot;, \u0026quot;some-topic1-partition-0\u0026quot;) option to build the Pulsar sink, this is simplified to PulsarSink.builder().setTopics(\u0026quot;some-topic1\u0026quot;).
Serializer # A serializer (PulsarSerializationSchema) is required for serializing the record instance into bytes. Similar to PulsarSource, Pulsar sink supports both Flink\u0026rsquo;s SerializationSchema and Pulsar\u0026rsquo;s Schema. Pulsar\u0026rsquo;s Schema.AUTO_PRODUCE_BYTES() is not supported in the Pulsar sink.
If you do not need the message key and other message properties in Pulsar\u0026rsquo;s Message interface, you can use the predefined PulsarSerializationSchema. The Pulsar sink provides two implementation methods.
Encode the message by using Pulsar\u0026rsquo;s Schema.
// Primitive types PulsarSerializationSchema.pulsarSchema(Schema) // Struct types (JSON, Protobuf, Avro, etc.) PulsarSerializationSchema.pulsarSchema(Schema, Class) // KeyValue type PulsarSerializationSchema.pulsarSchema(Schema, Class, Class) Encode the message by using Flink\u0026rsquo;s SerializationSchema
Java PulsarSerializationSchema.flinkSchema(SerializationSchema) Python PulsarSerializationSchema.flink_schema(SimpleStringSchema()) Schema evolution can be enabled by users using PulsarSerializationSchema.pulsarSchema() and PulsarSinkBuilder.enableSchemaEvolution(). This means that any broker schema validation is in place.
Schema\u0026lt;SomePojo\u0026gt; schema = Schema.AVRO(SomePojo.class); PulsarSerializationSchema\u0026lt;SomePojo\u0026gt; pulsarSchema = PulsarSerializationSchema.pulsarSchema(schema, SomePojo.class); PulsarSink\u0026lt;String\u0026gt; sink = PulsarSink.builder() ... .setSerializationSchema(pulsarSchema) .enableSchemaEvolution() .build(); If you use Pulsar schema without enabling schema evolution, the target topic will have a Schema.BYTES schema. Consumers will need to handle the deserialization (if needed) themselves.
For example, if you set PulsarSerializationSchema.pulsarSchema(Schema.STRING) without enabling schema evolution, the schema stored in Pulsar topics is Schema.BYTES.
Message Routing # Routing in Pulsar Sink is operated on the partition level. For a list of partitioned topics, the routing algorithm first collects all partitions from different topics, and then calculates routing within all the partitions. By default Pulsar Sink supports two router implementation.
KeyHashTopicRouter: use the hashcode of the message\u0026rsquo;s key to decide the topic partition that messages are sent to.
The message key is provided by PulsarSerializationSchema.key(IN, PulsarSinkContext) You need to implement this interface and extract the message key when you want to send the message with the same key to the same topic partition.
If you do not provide the message key. A topic partition is randomly chosen from the topic list.
The message key can be hashed in two ways: MessageKeyHash.JAVA_HASH and MessageKeyHash.MURMUR3_32_HASH. You can use the PulsarSinkOptions.PULSAR_MESSAGE_KEY_HASH option to choose the hash method.
RoundRobinRouter: Round-robin among all the partitions.
All messages are sent to the first partition, and switch to the next partition after sending a fixed number of messages. The batch size can be customized by the PulsarSinkOptions.PULSAR_BATCHING_MAX_MESSAGES option.
Let’s assume there are ten messages and two topics. Topic A has two partitions while topic B has three partitions. The batch size is set to five messages. In this case, topic A has 5 messages per partition which topic B does not receive any messages.
You can configure custom routers by using the TopicRouter interface. If you implement a TopicRouter, ensure that it is serializable. And you can return partitions which are not available in the pre-discovered partition list.
Thus, you do not need to specify topics using the PulsarSinkBuilder.setTopics option when you implement the custom topic router.
@PublicEvolving public interface TopicRouter\u0026lt;IN\u0026gt; extends Serializable { String route(IN in, List\u0026lt;String\u0026gt; partitions, PulsarSinkContext context); default void open(SinkConfiguration sinkConfiguration) { // Nothing to do by default. } } Internally, a Pulsar partition is implemented as a topic. The Pulsar client provides APIs to hide this implementation detail and handles routing under the hood automatically. Pulsar Sink uses a lower client API to implement its own routing layer to support multiple topics routing.
For details, see partitioned topics.
Delivery Guarantee # PulsarSink supports three delivery guarantee semantics.
NONE: Data loss can happen even when the pipeline is running. Basically, we use a fire-and-forget strategy to send records to Pulsar topics in this mode. It means that this mode has the highest throughput. AT_LEAST_ONCE: No data loss happens, but data duplication can happen after a restart from checkpoint. EXACTLY_ONCE: No data loss happens. Each record is sent to the Pulsar broker only once. Pulsar Sink uses Pulsar transaction and two-phase commit (2PC) to ensure records are sent only once even after pipeline restarts. Delayed message delivery # Delayed message delivery enables you to delay the possibility to consume a message. With delayed message enabled, the Pulsar sink sends a message to the Pulsar topic immediately, but the message is delivered to a consumer once the specified delay is over.
Delayed message delivery only works in the Shared subscription type. In Exclusive and Failover subscription types, the delayed message is dispatched immediately.
You can configure the MessageDelayer to define when to send the message to the consumer. The default option is to never delay the message dispatching. You can use the MessageDelayer.fixed(Duration) option to Configure delaying all messages in a fixed duration. You can also implement the MessageDelayer interface to dispatch messages at different time.
The dispatch time should be calculated by the PulsarSinkContext.processTime(). Sink Configurable Options # You can set options for PulsarClient, PulsarAdmin, Pulsar Producer and PulsarSink by using setConfig(ConfigOption\u0026lt;T\u0026gt;, T), setConfig(Configuration) and setConfig(Properties).
PulsarClient and PulsarAdmin Options # For details, refer to PulsarAdmin options.
Pulsar Producer Options # The Pulsar connector uses the Producer API to send messages. It extracts most parts of Pulsar\u0026rsquo;s ProducerConfigurationData as Flink configuration options in PulsarSinkOptions.
Key Default Type Description pulsar.producer.batchingEnabled true Boolean Enable batch send ability, it was enabled by default. pulsar.producer.batchingMaxBytes 131072 Integer The maximum size of messages permitted in a batch. Keep the maximum consistent as previous versions. pulsar.producer.batchingMaxMessages 1000 Integer The maximum number of messages permitted in a batch. pulsar.producer.batchingMaxPublishDelayMicros 1000 Long Batching time period of sending messages. pulsar.producer.batchingPartitionSwitchFrequencyByPublishDelay 10 Integer The maximum wait time for switching topic partitions. pulsar.producer.chunkingEnabled false Boolean pulsar.producer.compressionType NONE Enum
Message data compression type used by a producer.Available options:https://github.com/lz4/lz4https://zlib.net/https://facebook.github.io/zstd/https://google.github.io/snappy/
Possible values:"NONE""LZ4""ZLIB""ZSTD""SNAPPY" pulsar.producer.initialSequenceId (none) Long The sequence id for avoiding the duplication, it's used when Pulsar doesn't have transaction. pulsar.producer.producerName (none) String A producer name which would be displayed in the Pulsar's dashboard. If no producer name was provided, we would use a Pulsar generated name instead. pulsar.producer.properties Map A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification. pulsar.producer.sendTimeoutMs 30000 Long Message send timeout in ms.If a message is not acknowledged by a server before the sendTimeout expires, an error occurs. PulsarSink Options # The configuration options below are mainly used for customizing the performance and message sending behavior. You can just leave them alone if you do not have any performance issues.
Key Default Type Description pulsar.sink.deliveryGuarantee none Enum
Optional delivery guarantee when committing.
Possible values:"exactly-once": Records are only delivered exactly-once also under failover scenarios. To build a complete exactly-once pipeline is required that the source and sink support exactly-once and are properly configured."at-least-once": Records are ensured to be delivered but it may happen that the same record is delivered multiple times. Usually, this guarantee is faster than the exactly-once delivery."none": Records are delivered on a best effort basis. It is often the fastest way to process records but it may happen that records are lost or duplicated. pulsar.sink.enableSchemaEvolution false Boolean If you enable this option and use PulsarSerializationSchema.pulsarSchema(), we would consume and deserialize the message by using Pulsar's Schema. pulsar.sink.maxPendingMessages 1000 Integer The maximum number of pending messages in one sink parallelism. pulsar.sink.maxRecommitTimes 5 Integer The allowed transaction recommit times if we meet some retryable exception. This is used in Pulsar Transaction. pulsar.sink.messageKeyHash murmur-3-32-hash Enum
The hash policy for routing message by calculating the hash code of message key.
Possible values:"java-hash": This hash would use String.hashCode() to calculate the message key string's hash code."murmur-3-32-hash": This hash would calculate message key's hash code by using Murmur3 algorithm. pulsar.sink.topicMetadataRefreshInterval 1800000 Long Auto update the topic metadata in a fixed interval (in ms). The default value is 30 minutes. pulsar.sink.transactionTimeoutMillis 10800000 Long This option is used when the user require the DeliveryGuarantee.EXACTLY_ONCE semantic.We would use transaction for making sure the message could be write only once. Sink Metrics # This table lists supported metrics. The first 6 metrics are standard Pulsar Sink metrics as described in FLIP-33: Standardize Connector Metrics
Scope Metrics User Variables Description Type Operator numBytesOut n/a The total number of output bytes since the sink starts. Count towards the numBytesOut in TaskIOMetricsGroup. Counter numBytesOutPerSecond n/a The output bytes per second Meter numRecordsOut n/a The total number of output records since the sink starts. Counter numRecordsOutPerSecond n/a The output records per second Meter numRecordsOutErrors n/a The total number of records failed to send Counter currentSendTime n/a The time it takes to send the last record, from enqueue the message in client buffer to its ack. Gauge PulsarSink.numAcksReceived n/a The number of acks received for sent messages. Counter PulsarSink.sendLatencyMax n/a The maximum send latency in the last refresh interval across all producers. Gauge PulsarSink.producer."ProducerName".sendLatency50Pct ProducerName The 50th percentile of send latency in the last refresh interval for a specific producer. Gauge PulsarSink.producer."ProducerName".sendLatency75Pct ProducerName The 75th percentile of send latency in the last refresh interval for a specific producer. Gauge PulsarSink.producer."ProducerName".sendLatency95Pct ProducerName The 95th percentile of send latency in the last refresh interval for a specific producer. Gauge PulsarSink.producer."ProducerName".sendLatency99Pct ProducerName The 99th percentile of send latency in the last refresh interval for a specific producer. Gauge PulsarSink.producer."ProducerName".sendLatency999Pct ProducerName The 99.9th percentile of send latency in the last refresh interval for a specific producer. Gauge numBytesOut, numRecordsOut, numRecordsOutErrors are retrieved from Pulsar client metrics.
currentSendTime tracks the time from when the producer calls sendAync() to the time when the message is acknowledged by the broker. This metric is not available in NONE delivery guarantee.
The Pulsar producer refreshes its stats every 60 seconds by default. The PulsarSink retrieves the Pulsar producer stats every 500ms. That means that numRecordsOut, numBytesOut, numAcksReceived, and numRecordsOutErrors are updated every 60 seconds. To increase the metrics refresh frequency, you can change the Pulsar producer stats refresh interval to a smaller value (minimum 1 second), as shown below.
Java builder.setConfig(PulsarOptions.PULSAR_STATS_INTERVAL_SECONDS, 1L); Python builder.set_config(\u0026#34;pulsar.client.statsIntervalSeconds\u0026#34;, \u0026#34;1\u0026#34;) numBytesOutRate and numRecordsOutRate are calculated based on the numBytesOut and numRecordsOUt counter respectively. Flink internally uses a fixed 60 seconds window to calculate the rates.
Brief Design Rationale # Pulsar sink follow the Sink API defined in FLIP-191.
Stateless SinkWriter # In EXACTLY_ONCE mode, the Pulsar sink does not store transaction information in a checkpoint. That means that new transactions will be created after a restart. Therefore, any message in previous pending transactions is either aborted or timed out (They are never visible to the downstream Pulsar consumer). The Pulsar team is working to optimize the needed resources by unfinished pending transactions.
Pulsar Schema Evolution # Pulsar Schema Evolution allows you to reuse the same Flink job after certain \u0026ldquo;allowed\u0026rdquo; data model changes, like adding or deleting a field in a AVRO-based Pojo class. Please note that you can specify Pulsar schema validation rules and define an auto schema update. For details, refer to Pulsar Schema Evolution.
Upgrading to the Latest Connector Version # The generic upgrade steps are outlined in upgrading jobs and Flink versions guide. The Pulsar connector does not store any state on the Flink side. The Pulsar connector pushes and stores all the states on the Pulsar side. For Pulsar, you additionally need to know these limitations:
Do not upgrade the Pulsar connector and Pulsar broker version at the same time. Always use a newer Pulsar client with Pulsar connector to consume messages from Pulsar. Troubleshooting # If you have a problem with Pulsar when using Flink, keep in mind that Flink only wraps PulsarClient or PulsarAdmin and your problem might be independent of Flink and sometimes can be solved by upgrading Pulsar brokers, reconfiguring Pulsar brokers or reconfiguring Pulsar connector in Flink.
Known Issues # This section describes some known issues about the Pulsar connectors.
Unstable on Java 11 # Pulsar connector has some known issues on Java 11. It is recommended to run Pulsar connector on Java 8.
No TransactionCoordinatorNotFound, but automatic reconnect # Pulsar transactions are still in active development and are not stable. Pulsar 2.9.2 introduces a break change in transactions. If you use Pulsar 2.9.2 or higher with an older Pulsar client, you might get a TransactionCoordinatorNotFound exception.
You can use the latest pulsar-client-all release to resolve this issue.
Back to top
`}),e.add({id:222,href:"/flink/flink-docs-master/docs/deployment/repls/",title:"REPLs",section:"Deployment",content:""}),e.add({id:223,href:"/flink/flink-docs-master/docs/ops/state/savepoints/",title:"Savepoints",section:"State \u0026 Fault Tolerance",content:` Savepoints # What is a Savepoint? # A Savepoint is a consistent image of the execution state of a streaming job, created via Flink\u0026rsquo;s checkpointing mechanism. You can use Savepoints to stop-and-resume, fork, or update your Flink jobs. Savepoints consist of two parts: a directory with (typically large) binary files on stable storage (e.g. HDFS, S3, \u0026hellip;) and a (relatively small) meta data file. The files on stable storage represent the net data of the job\u0026rsquo;s execution state image. The meta data file of a Savepoint contains (primarily) pointers to all files on stable storage that are part of the Savepoint, in form of relative paths.
In order to allow upgrades between programs and Flink versions, it is important to check out the following section about assigning IDs to your operators. To make proper use of savepoints, it\u0026rsquo;s important to understand the differences between checkpoints and savepoints which is described in checkpoints vs. savepoints.
Assigning Operator IDs # It is highly recommended that you specify operator IDs via the uid(String) method. These IDs are used to scope the state of each operator.
DataStream\u0026lt;String\u0026gt; stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(\u0026#34;source-id\u0026#34;) // ID for the source operator .shuffle() // Stateful mapper with ID .map(new StatefulMapper()) .uid(\u0026#34;mapper-id\u0026#34;) // ID for the mapper // Stateless printing sink .print(); // Auto-generated ID If you do not specify the IDs manually they will be generated automatically. You can automatically restore from the savepoint as long as these IDs do not change. The generated IDs depend on the structure of your program and are sensitive to program changes. Therefore, it is highly recommended assigning these IDs manually.
Savepoint State # You can think of a savepoint as holding a map of Operator ID -\u0026gt; State for each stateful operator:
Operator ID | State ------------+------------------------ source-id | State of StatefulSource mapper-id | State of StatefulMapper In the above example, the print sink is stateless and hence not part of the savepoint state. By default, we try to map each entry of the savepoint back to the new program.
Operations # You can use the command line client to trigger savepoints, cancel a job with a savepoint, resume from savepoints, and dispose savepoints.
It is also possible to resume from savepoints using the webui.
Triggering Savepoints # When triggering a savepoint, a new savepoint directory is created where the data as well as the meta data will be stored. The location of this directory can be controlled by configuring a default target directory or by specifying a custom target directory with the trigger commands (see the :targetDirectory argument).
Attention: The target directory has to be a location accessible by both the JobManager(s) and TaskManager(s) e.g. a location on a distributed file-system or Object Store. For example with a FsStateBackend or RocksDBStateBackend:
# Savepoint target directory /savepoints/ # Savepoint directory /savepoints/savepoint-:shortjobid-:savepointid/ # Savepoint file contains the checkpoint meta data /savepoints/savepoint-:shortjobid-:savepointid/_metadata # Savepoint state /savepoints/savepoint-:shortjobid-:savepointid/... Savepoints can generally be moved by moving (or copying) the entire savepoint directory to a different location, and Flink will be able to restore from the moved savepoint.
There are two exceptions:
if entropy injection is activated: In that case the savepoint directory will not contain all savepoint data files, because the injected path entropy spreads the files over many directories. Lacking a common savepoint root directory, the savepoints will contain absolute path references, which prevent moving the directory.
The job contains task-owned state, such as GenericWriteAhreadLog sink.
Unlike savepoints, checkpoints cannot generally be moved to a different location, because checkpoints may include some absolute path references.
If you use statebackend: jobmanager, metadata and savepoint state will be stored in the _metadata file, so don\u0026rsquo;t be confused by the absence of additional data files.
Starting from Flink 1.15 intermediate savepoints (savepoints other than created with stop-with-savepoint) are not used for recovery and do not commit any side effects.
This has to be taken into consideration, especially when running multiple jobs in the same checkpointing timeline. It is possible in that solution that if the original job (after taking a savepoint) fails, then it will fall back to a checkpoint prior to the savepoint. However, if we now resume a job from the savepoint, then we might commit transactions that might’ve never happened because of falling back to a checkpoint before the savepoint (assuming non-determinism).
If one wants to be safe in those scenarios, we advise dropping the state of transactional sinks, by changing sinks uids.
It should not require any additional steps if there is just a single job running in the same checkpointing timeline, which means that you stop the original job before running a new job from the savepoint.
Savepoint format # You can choose between two binary formats of a savepoint:
canonical format - a format that has been unified across all state backends, which lets you take a savepoint with one state backend and then restore it using another. This is the most stable format, that is targeted at maintaining the most compatibility with previous versions, schemas, modifications etc.
native format - the downside of the canonical format is that often it is slow to take and restore from. Native format creates a snapshot in the format specific for the used state backend (e.g. SST files for RocksDB).
The possibility to trigger a savepoint in the native format was introduced in Flink 1.15. Up until then savepoints were created in the canonical format. Trigger a Savepoint # \$ bin/flink savepoint :jobId [:targetDirectory] This will trigger a savepoint for the job with ID :jobId, and returns the path of the created savepoint. You need this path to restore and dispose savepoints. You can also pass a type in which the savepoint should be taken. By default the savepoint will be taken in canonical format.
\$ bin/flink savepoint --type [native/canonical] :jobId [:targetDirectory] Trigger a Savepoint with YARN # \$ bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId This will trigger a savepoint for the job with ID :jobId and YARN application ID :yarnAppId, and returns the path of the created savepoint.
Stopping a Job with Savepoint # \$ bin/flink stop --type [native/canonical] --savepointPath [:targetDirectory] :jobId This will atomically trigger a savepoint for the job with ID :jobid and stop the job. Furthermore, you can specify a target file system directory to store the savepoint in. The directory needs to be accessible by the JobManager(s) and TaskManager(s). You can also pass a type in which the savepoint should be taken. By default the savepoint will be taken in canonical format.
Resuming from Savepoints # \$ bin/flink run -s :savepointPath [:runArgs] This submits a job and specifies a savepoint to resume from. You may give a path to either the savepoint\u0026rsquo;s directory or the _metadata file.
Allowing Non-Restored State # By default, the resume operation will try to map all state of the savepoint back to the program you are restoring with. If you dropped an operator, you can allow to skip state that cannot be mapped to the new program via --allowNonRestoredState (short: -n) option:
Restore mode # The Restore Mode determines who takes ownership of the files that make up a Savepoint or externalized checkpoints after restoring it. Both savepoints and externalized checkpoints behave similarly in this context. Here, they are just called \u0026ldquo;snapshots\u0026rdquo; unless explicitely noted otherwise.
As mentioned, the restore mode determines who takes over ownership of the files of the snapshots that we are restoring from. Snapshots can be owned either by a user or Flink itself. If a snapshot is owned by a user, Flink will not delete its files, moreover, Flink can not depend on the existence of the files from such a snapshot, as it might be deleted outside of Flink\u0026rsquo;s control.
Each restore mode serves a specific purposes. Still, we believe the default NO_CLAIM mode is a good tradeoff in most situations, as it provides clear ownership with a small price for the first checkpoint after the restore.
You can pass the restore mode as:
\$ bin/flink run -s :savepointPath -restoreMode :mode -n [:runArgs] NO_CLAIM (default)
In the NO_CLAIM mode Flink will not assume ownership of the snapshot. It will leave the files in user\u0026rsquo;s control and never delete any of the files. In this mode you can start multiple jobs from the same snapshot.
In order to make sure Flink does not depend on any of the files from that snapshot, it will force the first (successful) checkpoint to be a full checkpoint as opposed to an incremental one. This only makes a difference for state.backend: rocksdb, because all other state backends always take full checkpoints.
Once the first full checkpoint completes, all subsequent checkpoints will be taken as usual/configured. Consequently, once a checkpoint succeeds you can manually delete the original snapshot. You can not do this earlier, because without any completed checkpoints Flink will - upon failure - try to recover from the initial snapshot.
CLAIM
The other available mode is the CLAIM mode. In this mode Flink claims ownership of the snapshot and essentially treats it like a checkpoint: its controls the lifecycle and might delete it if it is not needed for recovery anymore. Hence, it is not safe to manually delete the snapshot or to start two jobs from the same snapshot. Flink keeps around a configured number of checkpoints.
Attention:
Retained checkpoints are stored in a path like \u0026lt;checkpoint_dir\u0026gt;/\u0026lt;job_id\u0026gt;/chk-\u0026lt;x\u0026gt;. Flink does not take ownership of the \u0026lt;checkpoint_dir\u0026gt;/\u0026lt;job_id\u0026gt; directory, but only the chk-\u0026lt;x\u0026gt;. The directory of the old job will not be deleted by Flink
Native format supports incremental RocksDB savepoints. For those savepoints Flink puts all SST files inside the savepoints directory. This means such savepoints are self-contained and relocatable. Please note that, when restored in CLAIM mode, subsequent checkpoints might reuse some SST files, which might delay the deletion the savepoints directory.
LEGACY
The legacy mode is how Flink worked until 1.15. In this mode Flink will never delete the initial checkpoint. At the same time, it is not clear if a user can ever delete it as well. The problem here, is that Flink might immediately build an incremental checkpoint on top of the restored one. Therefore, subsequent checkpoints depend on the restored checkpoint. Overall, the ownership is not well-defined.
Disposing Savepoints # \$ bin/flink savepoint -d :savepointPath This disposes the savepoint stored in :savepointPath.
Note that it is possible to also manually delete a savepoint via regular file system operations without affecting other savepoints or checkpoints (recall that each savepoint is self-contained).
Configuration # You can configure a default savepoint target directory via the state.savepoints.dir key or StreamExecutionEnvironment. When triggering savepoints, this directory will be used to store the savepoint. You can overwrite the default by specifying a custom target directory with the trigger commands (see the :targetDirectory argument).
flink-conf.yaml # Default savepoint target directory state.savepoints.dir: hdfs:///flink/savepoints Java env.setDefaultSavepointDir(\u0026#34;hdfs:///flink/savepoints\u0026#34;); Scala env.setDefaultSavepointDir(\u0026#34;hdfs:///flink/savepoints\u0026#34;) If you neither configure a default nor specify a custom target directory, triggering the savepoint will fail.
The target directory has to be a location accessible by both the JobManager(s) and TaskManager(s) e.g. a location on a distributed file-system. F.A.Q # Should I assign IDs to all operators in my job? # As a rule of thumb, yes. Strictly speaking, it is sufficient to only assign IDs via the uid method to the stateful operators in your job. The savepoint only contains state for these operators and stateless operator are not part of the savepoint.
In practice, it is recommended to assign it to all operators, because some of Flink\u0026rsquo;s built-in operators like the Window operator are also stateful and it is not obvious which built-in operators are actually stateful and which are not. If you are absolutely certain that an operator is stateless, you can skip the uid method.
What happens if I add a new operator that requires state to my job? # When you add a new operator to your job, it will be initialized without any state. Savepoints contain the state of each stateful operator. Stateless operators are simply not part of the savepoint. The new operator behaves similar to a stateless operator.
What happens if I delete an operator that has state from my job? # By default, a savepoint restore will try to match all state back to the restored job. If you restore from a savepoint that contains state for an operator that has been deleted, this will therefore fail.
You can allow non restored state by setting the --allowNonRestoredState (short: -n) with the run command:
\$ bin/flink run -s :savepointPath -n [:runArgs] What happens if I reorder stateful operators in my job? # If you assigned IDs to these operators, they will be restored as usual.
If you did not assign IDs, the auto generated IDs of the stateful operators will most likely change after the reordering. This would result in you not being able to restore from a previous savepoint.
What happens if I add or delete or reorder operators that have no state in my job? # If you assigned IDs to your stateful operators, the stateless operators will not influence the savepoint restore.
If you did not assign IDs, the auto generated IDs of the stateful operators will most likely change after the reordering. This would result in you not being able to restore from a previous savepoint.
What happens when I change the parallelism of my program when restoring? # You can simply restore the program from a savepoint and specify a new parallelism.
Can I move the Savepoint files on stable storage? # The quick answer to this question is currently \u0026ldquo;yes\u0026rdquo;. Savepoints are self-contained and relocatable. You can move the file and restore from any location.
Back to top
`}),e.add({id:224,href:"/flink/flink-docs-master/docs/deployment/advanced/",title:"Advanced",section:"Deployment",content:""}),e.add({id:225,href:"/flink/flink-docs-master/docs/dev/configuration/advanced/",title:"Advanced Configuration",section:"Project Configuration",content:` Advanced Configuration Topics # Anatomy of the Flink distribution # Flink itself consists of a set of classes and dependencies that form the core of Flink\u0026rsquo;s runtime and must be present when a Flink application is started. The classes and dependencies needed to run the system handle areas such as coordination, networking, checkpointing, failover, APIs, operators (such as windowing), resource management, etc.
These core classes and dependencies are packaged in the flink-dist.jar, which is available in the /lib folder in the downloaded distribution and is part of the basic Flink container images. You can think of these dependencies as similar to Java\u0026rsquo;s core library, which contains classes like String and List.
In order to keep the core dependencies as small as possible and avoid dependency clashes, the Flink Core Dependencies do not contain any connectors or libraries (i.e. CEP, SQL, ML) in order to avoid having an excessive default number of classes and dependencies in the classpath.
The /lib directory of the Flink distribution additionally contains various JARs including commonly used modules, such as all the required modules to execute Table jobs and a set of connector and formats. These are loaded by default and can be removed from the classpath just by removing them from the /lib folder.
Flink also ships additional optional dependencies under the /opt folder, which can be enabled by moving the JARs in the /lib folder.
For more information about classloading, refer to the section on Classloading in Flink.
Scala Versions # Different Scala versions are not binary compatible with one another. All Flink dependencies that (transitively) depend on Scala are suffixed with the Scala version that they are built for (i.e. flink-streaming-scala_2.12).
If you are only using Flink\u0026rsquo;s Java APIs, you can use any Scala version. If you are using Flink\u0026rsquo;s Scala APIs, you need to pick the Scala version that matches the application\u0026rsquo;s Scala version.
Please refer to the build guide for details on how to build Flink for a specific Scala version.
Scala versions after 2.12.8 are not binary compatible with previous 2.12.x versions. This prevents the Flink project from upgrading its 2.12.x builds beyond 2.12.8. You can build Flink locally for later Scala versions by following the build guide. For this to work, you will need to add -Djapicmp.skip to skip binary compatibility checks when building.
See the Scala 2.12.8 release notes for more details. The relevant section states:
The second fix is not binary compatible: the 2.12.8 compiler omits certain methods that are generated by earlier 2.12 compilers. However, we believe that these methods are never used and existing compiled code will continue to work. See the pull request description for more details.
Anatomy of Table Dependencies # The Flink distribution contains by default the required JARs to execute Flink SQL Jobs (found in the /lib folder), in particular:
flink-table-api-java-uber-1.16-SNAPSHOT.jar → contains all the Java APIs flink-table-runtime-1.16-SNAPSHOT.jar → contains the table runtime flink-table-planner-loader-1.16-SNAPSHOT.jar → contains the query planner Previously, these JARs were all packaged into flink-table.jar. Since Flink 1.15, this has now been split into three JARs in order to allow users to swap the flink-table-planner-loader-1.16-SNAPSHOT.jar with flink-table-planner_2.12-1.16-SNAPSHOT.jar. While Table Java API artifacts are built into the distribution, Table Scala API artifacts are not included by default. When using formats and connectors with the Flink Scala API, you need to either download and include these JARs in the distribution /lib folder manually (recommended), or package them as dependencies in the uber/fat JAR of your Flink SQL Jobs.
For more details, check out how to connect to external systems.
Table Planner and Table Planner Loader # Starting from Flink 1.15, the distribution contains two planners:
flink-table-planner_2.12-1.16-SNAPSHOT.jar, in /opt, contains the query planner flink-table-planner-loader-1.16-SNAPSHOT.jar, loaded by default in /lib, contains the query planner hidden behind an isolated classpath (you won\u0026rsquo;t be able to address any io.apache.flink.table.planner directly) The two planner JARs contain the same code, but they are packaged differently. In the first case, you must use the same Scala version of the JAR. In second case, you do not need to make considerations about Scala, since it is hidden inside the JAR.
By default,flink-table-planner-loader is used by the distribution. If you need to access and use the internals of the query planner, you can swap the JARs (copying and pasting flink-table-planner_2.12.jar in the distribution /lib folder). Be aware that you will be constrained to using the Scala version of the Flink distribution that you are using.
The two planners cannot co-exist at the same time in the classpath. If you load both of them in /lib your Table Jobs will fail. In the upcoming Flink versions, we will stop shipping the flink-table-planner_2.12 artifact in the Flink distribution. We strongly suggest migrating your jobs and your custom connectors/formats to work with the API modules, without relying on planner internals. If you need some functionality from the planner, which is currently not exposed through the API modules, please open a ticket in order to discuss it with the community. Hadoop Dependencies # General rule: It should not be necessary to add Hadoop dependencies directly to your application. The only exception is when you use existing Hadoop input/output formats with Flink\u0026rsquo;s Hadoop compatibility wrappers.
If you want to use Flink with Hadoop, you need to have a Flink setup that includes the Hadoop dependencies, rather than adding Hadoop as an application dependency. In other words, Hadoop must be a dependency of the Flink system itself and not of the user code that contains the application. Flink will use the Hadoop dependencies specified by the HADOOP_CLASSPATH environment variable, which can be set like this:
export HADOOP_CLASSPATH=\`hadoop classpath\` There are two main reasons for this design:
Some Hadoop interactions happen in Flink\u0026rsquo;s core, possibly before the user application is started. These include setting up HDFS for checkpoints, authenticating via Hadoop\u0026rsquo;s Kerberos tokens, or deploying on YARN.
Flink\u0026rsquo;s inverted classloading approach hides many transitive dependencies from the core dependencies. This applies not only to Flink\u0026rsquo;s own core dependencies, but also to Hadoop\u0026rsquo;s dependencies when present in the setup. This way, applications can use different versions of the same dependencies without running into dependency conflicts. This is very useful when dependency trees become very large.
If you need Hadoop dependencies during developing or testing inside the IDE (i.e. for HDFS access), you should configure these dependencies similar to the scope of the dependencies (i.e. to test or to provided).
`}),e.add({id:226,href:"/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/",title:"Checkpoints vs. Savepoints",section:"State \u0026 Fault Tolerance",content:` Checkpoints vs. Savepoints # Overview # Conceptually, Flink\u0026rsquo;s savepoints are different from checkpoints in a way that\u0026rsquo;s analogous to how backups are different from recovery logs in traditional database systems.
The primary purpose of checkpoints is to provide a recovery mechanism in case of unexpected job failures. A checkpoint\u0026rsquo;s lifecycle is managed by Flink, i.e. a checkpoint is created, owned, and released by Flink - without user interaction. Because checkpoints are being triggered often, and are relied upon for failure recovery, the two main design goals for the checkpoint implementation are i) being as lightweight to create and ii) being as fast to restore from as possible. Optimizations towards those goals can exploit certain properties, e.g., that the job code doesn\u0026rsquo;t change between the execution attempts.
Checkpoints are automatically deleted if the application is terminated by the user (except if checkpoints are explicitly configured to be retained). Checkpoints are stored in state backend-specific (native) data format (may be incremental depending on the specific backend). Although savepoints are created internally with the same mechanisms as checkpoints, they are conceptually different and can be a bit more expensive to produce and restore from. Their design focuses more on portability and operational flexibility, especially with respect to changes to the job. The use case for savepoints is for planned, manual operations. For example, this could be an update of your Flink version, changing your job graph, and so on.
Savepoints are created, owned and deleted solely by the user. That means, Flink does not delete savepoints neither after job termination nor after restore. Savepoints are stored in a state backend independent (canonical) format (Note: Since Flink 1.15, savepoints can be also stored in the backend-specific native format which is faster to create and restore but comes with some limitations. Capabilities and limitations # The following table gives an overview of capabilities and limitations for the various types of savepoints and checkpoints.
✓ - Flink fully support this type of the snapshot x - Flink doesn\u0026rsquo;t support this type of the snapshot ! - While these operations currently work, Flink doesn\u0026rsquo;t officially guarantee support for them, so there is a certain level of risk associated with them Operation Canonical Savepoint Native Savepoint Aligned Checkpoint Unaligned Checkpoint State backend change ✓ x x x State Processor API (writing) ✓ x x x State Processor API (reading) ✓ ! ! x Self-contained and relocatable ✓ ✓ x x Schema evolution ✓ ! ! ! Arbitrary job upgrade ✓ ✓ ✓ x Non-arbitrary job upgrade ✓ ✓ ✓ ✓ Flink minor version upgrade ✓ ✓ ✓ x Flink bug/patch version upgrade ✓ ✓ ✓ ✓ Rescaling ✓ ✓ ✓ ✓ State backend change - configuring a different State Backend than was used when taking the snapshot. State Processor API (writing) - the ability to create a new snapshot of this type via the State Processor API. State Processor API (reading) - the ability to read states from an existing snapshot of this type via the State Processor API. Self-contained and relocatable - the one snapshot folder contains everything it needs for recovery and it doesn\u0026rsquo;t depend on other snapshots which means it can be easily moved to another place if needed. Schema evolution - the state data type can be changed if it uses a serializer that supports schema evolution (e.g., POJOs and Avro types) Arbitrary job upgrade - the snapshot can be restored even if the partitioning types(rescale, rebalance, map, etc.) or in-flight record types for the existing operators have changed. Non-arbitrary job upgrade - restoring the snapshot is possible with updated operators if the job graph topology and in-flight record types remain unchanged. Flink minor version upgrade - restoring a snapshot taken with an older minor version of Flink (1.x → 1.y). Flink bug/patch version upgrade - restoring a snapshot taken with an older patch version of Flink (1.14.x → 1.14.y). Rescaling - restoring the snapshot with a different parallelism than was used during the snapshot creation. Back to top
`}),e.add({id:227,href:"/flink/flink-docs-master/docs/dev/dataset/",title:"DataSet API (Legacy)",section:"Application Development",content:" "}),e.add({id:228,href:"/flink/flink-docs-master/docs/internals/",title:"Internals",section:"Docs",content:""}),e.add({id:229,href:"/flink/flink-docs-master/docs/connectors/datastream/jdbc/",title:"JDBC",section:"DataStream Connectors",content:` JDBC Connector # This connector provides a sink that writes data to a JDBC database.
To use it, add the following dependency to your project (along with your JDBC driver):
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-jdbc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here. A driver dependency is also required to connect to a specified database. Please consult your database documentation on how to add the corresponding driver.
JdbcSink.sink # The JDBC sink provides at-least-once guarantee. Effectively though, exactly-once can be achieved by crafting upsert SQL statements or idempotent SQL updates. Configuration goes as follow (see also JdbcSink javadoc ).
Java JdbcSink.sink( sqlDmlStatement, // mandatory jdbcStatementBuilder, // mandatory jdbcExecutionOptions, // optional jdbcConnectionOptions // mandatory ); Python JdbcSink.sink( sql_dml_statement, # mandatory type_info, # mandatory jdbc_connection_options, # mandatory jdbc_execution_options # optional ) SQL DML statement and JDBC statement builder # The sink builds one JDBC prepared statement from a user-provider SQL string, e.g.:
INSERT INTO some_table field1, field2 values (?, ?) It then repeatedly calls a user-provided function to update that prepared statement with each value of the stream, e.g.:
(preparedStatement, someRecord) -\u0026gt; { ... update here the preparedStatement with values from someRecord ... } JDBC execution options # The SQL DML statements are executed in batches, which can optionally be configured with the following instance (see also JdbcExecutionOptions javadoc )
Java JdbcExecutionOptions.builder() .withBatchIntervalMs(200) // optional: default = 0, meaning no time-based execution is done .withBatchSize(1000) // optional: default = 5000 values .withMaxRetries(5) // optional: default = 3 .build(); Python JdbcExecutionOptions.builder() \\ .with_batch_interval_ms(2000) \\ .with_batch_size(100) \\ .with_max_retries(5) \\ .build() A JDBC batch is executed as soon as one of the following conditions is true:
the configured batch interval time is elapsed the maximum batch size is reached a Flink checkpoint has started JDBC connection parameters # The connection to the database is configured with a JdbcConnectionOptions instance. Please see JdbcConnectionOptions javadoc for details
Full example # Java public class JdbcSinkExample { static class Book { public Book(Long id, String title, String authors, Integer year) { this.id = id; this.title = title; this.authors = authors; this.year = year; } final Long id; final String title; final String authors; final Integer year; } public static void main(String[] args) throws Exception { var env = StreamExecutionEnvironment.getExecutionEnvironment(); env.fromElements( new Book(101L, \u0026#34;Stream Processing with Apache Flink\u0026#34;, \u0026#34;Fabian Hueske, Vasiliki Kalavri\u0026#34;, 2019), new Book(102L, \u0026#34;Streaming Systems\u0026#34;, \u0026#34;Tyler Akidau, Slava Chernyak, Reuven Lax\u0026#34;, 2018), new Book(103L, \u0026#34;Designing Data-Intensive Applications\u0026#34;, \u0026#34;Martin Kleppmann\u0026#34;, 2017), new Book(104L, \u0026#34;Kafka: The Definitive Guide\u0026#34;, \u0026#34;Gwen Shapira, Neha Narkhede, Todd Palino\u0026#34;, 2017) ).addSink( JdbcSink.sink( \u0026#34;insert into books (id, title, authors, year) values (?, ?, ?, ?)\u0026#34;, (statement, book) -\u0026gt; { statement.setLong(1, book.id); statement.setString(2, book.title); statement.setString(3, book.authors); statement.setInt(4, book.year); }, JdbcExecutionOptions.builder() .withBatchSize(1000) .withBatchIntervalMs(200) .withMaxRetries(5) .build(), new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withUrl(\u0026#34;jdbc:postgresql://dbhost:5432/postgresdb\u0026#34;) .withDriverName(\u0026#34;org.postgresql.Driver\u0026#34;) .withUsername(\u0026#34;someUser\u0026#34;) .withPassword(\u0026#34;somePassword\u0026#34;) .build() )); env.execute(); } } Python env = StreamExecutionEnvironment.get_execution_environment() type_info = Types.ROW([Types.INT(), Types.STRING(), Types.STRING(), Types.INT()]) env.from_collection( [(101, \u0026#34;Stream Processing with Apache Flink\u0026#34;, \u0026#34;Fabian Hueske, Vasiliki Kalavri\u0026#34;, 2019), (102, \u0026#34;Streaming Systems\u0026#34;, \u0026#34;Tyler Akidau, Slava Chernyak, Reuven Lax\u0026#34;, 2018), (103, \u0026#34;Designing Data-Intensive Applications\u0026#34;, \u0026#34;Martin Kleppmann\u0026#34;, 2017), (104, \u0026#34;Kafka: The Definitive Guide\u0026#34;, \u0026#34;Gwen Shapira, Neha Narkhede, Todd Palino\u0026#34;, 2017) ], type_info=type_info) \\ .add_sink( JdbcSink.sink( \u0026#34;insert into books (id, title, authors, year) values (?, ?, ?, ?)\u0026#34;, type_info, JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .with_url(\u0026#39;jdbc:postgresql://dbhost:5432/postgresdb\u0026#39;) .with_driver_name(\u0026#39;org.postgresql.Driver\u0026#39;) .with_user_name(\u0026#39;someUser\u0026#39;) .with_password(\u0026#39;somePassword\u0026#39;) .build(), JdbcExecutionOptions.builder() .with_batch_interval_ms(1000) .with_batch_size(200) .with_max_retries(5) .build() )) env.execute() JdbcSink.exactlyOnceSink # Since 1.13, Flink JDBC sink supports exactly-once mode. The implementation relies on the JDBC driver support of XA standard. Most drivers support XA if the database also supports XA (so the driver is usually the same).
To use it, create a sink using exactlyOnceSink() method as above and additionally provide:
exactly-once options execution options XA DataSource Supplier For example:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .fromElements(...) .addSink(JdbcSink.exactlyOnceSink( \u0026#34;insert into books (id, title, author, price, qty) values (?,?,?,?,?)\u0026#34;, (ps, t) -\u0026gt; { ps.setInt(1, t.id); ps.setString(2, t.title); ps.setString(3, t.author); ps.setDouble(4, t.price); ps.setInt(5, t.qty); }, JdbcExecutionOptions.builder() .withMaxRetries(0) .build(), JdbcExactlyOnceOptions.defaults(), () -\u0026gt; { // create a driver-specific XA DataSource // The following example is for derby EmbeddedXADataSource ds = new EmbeddedXADataSource(); ds.setDatabaseName(\u0026#34;my_db\u0026#34;); return ds; }); env.execute(); Python Still not supported in Python API. NOTE: Some databases only allow a single XA transaction per connection (e.g. PostgreSQL, MySQL). In such cases, please use the following API to construct JdbcExactlyOnceOptions:
Java JdbcExactlyOnceOptions.builder() .withTransactionPerConnection(true) .build(); Python Still not supported in Python API. This will make Flink use a separate connection for every XA transaction. This may require adjusting connection limits. For PostgreSQL and MySQL, this can be done by increasing max_connections.
Furthermore, XA needs to be enabled and/or configured in some databases. For PostgreSQL, you should set max_prepared_transactions to some value greater than zero. For MySQL v8+, you should grant XA_RECOVER_ADMIN to Flink DB user.
ATTENTION: Currently, JdbcSink.exactlyOnceSink can ensure exactly once semantics with JdbcExecutionOptions.maxRetries == 0; otherwise, duplicated results maybe produced.
XADataSource examples # PostgreSQL XADataSource example: Java PGXADataSource xaDataSource = new org.postgresql.xa.PGXADataSource(); xaDataSource.setUrl(\u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;); xaDataSource.setUser(username); xaDataSource.setPassword(password); Python Still not supported in Python API. MySQL XADataSource example: Java MysqlXADataSource xaDataSource = new com.mysql.cj.jdbc.MysqlXADataSource(); xaDataSource.setUrl(\u0026#34;jdbc:mysql://localhost:3306/\u0026#34;); xaDataSource.setUser(username); xaDataSource.setPassword(password); Python Still not supported in Python API. Oracle XADataSource example: Java OracleXADataSource xaDataSource = new oracle.jdbc.xa.OracleXADataSource(); xaDataSource.setURL(\u0026#34;jdbc:oracle:oci8:@\u0026#34;); xaDataSource.setUser(\u0026#34;scott\u0026#34;); xaDataSource.setPassword(\u0026#34;tiger\u0026#34;); Python Still not supported in Python API. Please also take Oracle connection pooling into account.
Please refer to the JdbcXaSinkFunction documentation for more details.
`}),e.add({id:230,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/joins/",title:"Joins",section:"Queries",content:` Joins # Batch Streaming
Flink SQL supports complex and flexible join operations over dynamic tables. There are several different types of joins to account for the wide variety of semantics queries may require.
By default, the order of joins is not optimized. Tables are joined in the order in which they are specified in the FROM clause. You can tweak the performance of your join queries, by listing the tables with the lowest update frequency first and the tables with the highest update frequency last. Make sure to specify tables in an order that does not yield a cross join (Cartesian product), which are not supported and would cause a query to fail.
Regular Joins # Regular joins are the most generic type of join in which any new record, or changes to either side of the join, are visible and affect the entirety of the join result. For example, if there is a new record on the left side, it will be joined with all the previous and future records on the right side when the product id equals.
SELECT * FROM Orders INNER JOIN Product ON Orders.productId = Product.id For streaming queries, the grammar of regular joins is the most flexible and allow for any kind of updating (insert, update, delete) input table. However, this operation has important operational implications: it requires to keep both sides of the join input in Flink state forever. Thus, the required state for computing the query result might grow infinitely depending on the number of distinct input rows of all input tables and intermediate join results. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. INNER Equi-JOIN # Returns a simple Cartesian product restricted by the join condition. Currently, only equi-joins are supported, i.e., joins that have at least one conjunctive condition with an equality predicate. Arbitrary cross or theta joins are not supported.
SELECT * FROM Orders INNER JOIN Product ON Orders.product_id = Product.id OUTER Equi-JOIN # Returns all rows in the qualified Cartesian product (i.e., all combined rows that pass its join condition), plus one copy of each row in an outer table for which the join condition did not match with any row of the other table. Flink supports LEFT, RIGHT, and FULL outer joins. Currently, only equi-joins are supported, i.e., joins with at least one conjunctive condition with an equality predicate. Arbitrary cross or theta joins are not supported.
SELECT * FROM Orders LEFT JOIN Product ON Orders.product_id = Product.id SELECT * FROM Orders RIGHT JOIN Product ON Orders.product_id = Product.id SELECT * FROM Orders FULL OUTER JOIN Product ON Orders.product_id = Product.id Interval Joins # Returns a simple Cartesian product restricted by the join condition and a time constraint. An interval join requires at least one equi-join predicate and a join condition that bounds the time on both sides. Two appropriate range predicates can define such a condition (\u0026lt;, \u0026lt;=, \u0026gt;=, \u0026gt;), a BETWEEN predicate, or a single equality predicate that compares time attributes of the same type (i.e., processing time or event time) of both input tables.
For example, this query will join all orders with their corresponding shipments if the order was shipped four hours after the order was received.
SELECT * FROM Orders o, Shipments s WHERE o.id = s.order_id AND o.order_time BETWEEN s.ship_time - INTERVAL \u0026#39;4\u0026#39; HOUR AND s.ship_time The following predicates are examples of valid interval join conditions:
ltime = rtime ltime \u0026gt;= rtime AND ltime \u0026lt; rtime + INTERVAL '10' MINUTE ltime BETWEEN rtime - INTERVAL '10' SECOND AND rtime + INTERVAL '5' SECOND For streaming queries, compared to the regular join, interval join only supports append-only tables with time attributes. Since time attributes are quasi-monotonic increasing, Flink can remove old values from its state without affecting the correctness of the result.
Temporal Joins # A Temporal table is a table that evolves over time - otherwise known in Flink as a dynamic table. Rows in a temporal table are associated with one or more temporal periods and all Flink tables are temporal(dynamic). The temporal table contains one or more versioned table snapshots, it can be a changing history table which tracks the changes(e.g. database changelog, contains all snapshots) or a changing dimensioned table which materializes the changes(e.g. database table which contains the latest snapshot).
Event Time Temporal Join # Event Time temporal joins allow joining against a versioned table. This means a table can be enriched with changing metadata and retrieve its value at a certain point in time.
Temporal joins take an arbitrary table (left input/probe site) and correlate each row to the corresponding row\u0026rsquo;s relevant version in the versioned table (right input/build side). Flink uses the SQL syntax of FOR SYSTEM_TIME AS OF to perform this operation from the SQL:2011 standard. The syntax of a temporal join is as follows;
SELECT [column_list] FROM table1 [AS \u0026lt;alias1\u0026gt;] [LEFT] JOIN table2 FOR SYSTEM_TIME AS OF table1.{ proctime | rowtime } [AS \u0026lt;alias2\u0026gt;] ON table1.column-name1 = table2.column-name1 With an event-time attribute (i.e., a rowtime attribute), it is possible to retrieve the value of a key as it was at some point in the past. This allows for joining the two tables at a common point in time. The versioned table will store all versions - identified by time - since the last watermark.
For example, suppose we have a table of orders, each with prices in different currencies. To properly normalize this table to a single currency, such as USD, each order needs to be joined with the proper currency conversion rate from the point-in-time when the order was placed.
-- Create a table of orders. This is a standard -- append-only dynamic table. CREATE TABLE orders ( order_id STRING, price DECIMAL(32,2), currency STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time ) WITH (/* ... */); -- Define a versioned table of currency rates. -- This could be from a change-data-capture -- such as Debezium, a compacted Kafka topic, or any other -- way of defining a versioned table. CREATE TABLE currency_rates ( currency STRING, conversion_rate DECIMAL(32, 2), update_time TIMESTAMP(3) METADATA FROM \`values.source.timestamp\` VIRTUAL, WATERMARK FOR update_time AS update_time, PRIMARY KEY(currency) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39;, /* ... */ ); SELECT order_id, price, currency, conversion_rate, order_time FROM orders LEFT JOIN currency_rates FOR SYSTEM_TIME AS OF orders.order_time ON orders.currency = currency_rates.currency; order_id price currency conversion_rate order_time ======== ===== ======== =============== ========= o_001 11.11 EUR 1.14 12:00:00 o_002 12.51 EUR 1.10 12:06:00 Note: The event-time temporal join is triggered by a watermark from the left and right sides; please ensure both sides of the join have set watermark correctly.
Note: The event-time temporal join requires the primary key contained in the equivalence condition of the temporal join condition, e.g., The primary key currency_rates.currency of table currency_rates to be constrained in the condition orders.currency = currency_rates.currency.
In contrast to regular joins, the previous temporal table results will not be affected despite the changes on the build side. Compared to interval joins, temporal table joins do not define a time window within which the records will be joined. Records from the probe side are always joined with the build side\u0026rsquo;s version at the time specified by the time attribute. Thus, rows on the build side might be arbitrarily old. As time passes, no longer needed versions of the record (for the given primary key) will be removed from the state.
Processing Time Temporal Join # A processing time temporal table join uses a processing-time attribute to correlate rows to the latest version of a key in an external versioned table.
By definition, with a processing-time attribute, the join will always return the most up-to-date value for a given key. One can think of a lookup table as a simple HashMap\u0026lt;K, V\u0026gt; that stores all the records from the build side. The power of this join is it allows Flink to work directly against external systems when it is not feasible to materialize the table as a dynamic table within Flink.
The following processing-time temporal table join example shows an append-only table orders that should be joined with the table LatestRates. LatestRates is a dimension table (e.g. HBase table) that is materialized with the latest rate. At time 10:15, 10:30, 10:52, the content of LatestRates looks as follows:
10:15\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 114 Yen 1 10:30\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 114 Yen 1 10:52\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 116 \u0026lt;==== changed from 114 to 116 Yen 1 The content of LastestRates at times 10:15 and 10:30 are equal. The Euro rate has changed from 114 to 116 at 10:52.
Orders is an append-only table representing payments for the given amount and the given currency. For example, at 10:15 there was an order for an amount of 2 Euro.
SELECT * FROM Orders; amount currency ====== ========= 2 Euro \u0026lt;== arrived at time 10:15 1 US Dollar \u0026lt;== arrived at time 10:30 2 Euro \u0026lt;== arrived at time 10:52 Given these tables, we would like to calculate all Orders converted to a common currency.
amount currency rate amount*rate ====== ========= ======= ============ 2 Euro 114 228 \u0026lt;== arrived at time 10:15 1 US Dollar 102 102 \u0026lt;== arrived at time 10:30 2 Euro 116 232 \u0026lt;== arrived at time 10:52 Currently, the FOR SYSTEM_TIME AS OF syntax used in temporal join with latest version of any view/table is not support yet, you can use temporal table function syntax as following:
SELECT o_amount, r_rate FROM Orders, LATERAL TABLE (Rates(o_proctime)) WHERE r_currency = o_currency Note The reason why the FOR SYSTEM_TIME AS OF syntax used in temporal join with latest version of any table/view is not support is only the semantic consideration, because the join processing for left stream doesn\u0026rsquo;t wait for the complete snapshot of temporal table, this may mislead users in production environment. The processing-time temporal join by temporal table function also exists same semantic problem, but it has been alive for a long time, thus we support it from the perspective of compatibility.
The result is not deterministic for processing-time. The processing-time temporal join is most often used to enrich the stream with an external table (i.e., dimension table).
In contrast to regular joins, the previous temporal table results will not be affected despite the changes on the build side. Compared to interval joins, temporal table joins do not define a time window within which the records join, i.e., old rows are not stored in state.
Temporal Table Function Join # The syntax to join a table with a temporal table function is the same as in Join with Table Function.
Note: Currently only inner join and left outer join with temporal tables are supported.
Assuming Rates is a temporal table function, the join can be expressed in SQL as follows:
SELECT o_amount, r_rate FROM Orders, LATERAL TABLE (Rates(o_proctime)) WHERE r_currency = o_currency The main difference between above Temporal Table DDL and Temporal Table Function are:
The temporal table DDL can be defined in SQL but temporal table function can not; Both temporal table DDL and temporal table function support temporal join versioned table, but only temporal table function can temporal join the latest version of any table/view. Lookup Join # A lookup join is typically used to enrich a table with data that is queried from an external system. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.
The lookup join uses the above Processing Time Temporal Join syntax with the right table to be backed by a lookup source connector.
The following example shows the syntax to specify a lookup join.
-- Customers is backed by the JDBC connector and can be used for lookup joins CREATE TEMPORARY TABLE Customers ( id INT, name STRING, country STRING, zip STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysqlhost:3306/customerdb\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;customers\u0026#39; ); -- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN Customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; In the example above, the Orders table is enriched with data from the Customers table which resides in a MySQL database. The FOR SYSTEM_TIME AS OF clause with the subsequent processing time attribute ensures that each row of the Orders table is joined with those Customers rows that match the join predicate at the point in time when the Orders row is processed by the join operator. It also prevents that the join result is updated when a joined Customer row is updated in the future. The lookup join also requires a mandatory equality join predicate, in the example above o.customer_id = c.id.
Array Expansion # Returns a new row for each element in the given array. Unnesting WITH ORDINALITY is not yet supported.
SELECT order_id, tag FROM Orders CROSS JOIN UNNEST(tags) AS t (tag) Table Function # Joins a table with the results of a table function. Each row of the left (outer) table is joined with all rows produced by the corresponding call of the table function. User-defined table functions must be registered before use.
INNER JOIN # The row of the left (outer) table is dropped, if its table function call returns an empty result.
SELECT order_id, res FROM Orders, LATERAL TABLE(table_func(order_id)) t(res) LEFT OUTER JOIN # If a table function call returns an empty result, the corresponding outer row is preserved, and the result padded with null values. Currently, a left outer join against a lateral table requires a TRUE literal in the ON clause.
SELECT order_id, res FROM Orders LEFT OUTER JOIN LATERAL TABLE(table_func(order_id)) t(res) ON TRUE Back to top
`}),e.add({id:231,href:"/flink/flink-docs-master/docs/connectors/table/formats/orc/",title:"Orc",section:"Formats",content:` Orc Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Orc format allows to read and write Orc data.
Dependencies # In order to use the ORC format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-orc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a table with Orc format # Here is an example to create a table using Filesystem connector and Orc format.
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3), dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/tmp/user_behavior\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;orc\u0026#39; ) Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'orc'. Orc format also supports table properties from Table properties. For example, you can configure orc.compress=SNAPPY to enable snappy compression.
Data Type Mapping # Orc format type mapping is compatible with Apache Hive. The following table lists the type mapping from Flink type to Orc type.
Flink Data Type Orc physical type Orc logical type CHAR bytes CHAR VARCHAR bytes VARCHAR STRING bytes STRING BOOLEAN long BOOLEAN BYTES bytes BINARY DECIMAL decimal DECIMAL TINYINT long BYTE SMALLINT long SHORT INT long INT BIGINT long LONG FLOAT double FLOAT DOUBLE double DOUBLE DATE long DATE TIMESTAMP timestamp TIMESTAMP ARRAY - LIST MAP - MAP ROW - STRUCT `}),e.add({id:232,href:"/flink/flink-docs-master/docs/ops/upgrading/",title:"Upgrading Applications and Flink Versions",section:"Operations",content:` Upgrading Applications and Flink Versions # Flink DataStream programs are typically designed to run for long periods of time such as weeks, months, or even years. As with all long-running services, Flink streaming applications need to be maintained, which includes fixing bugs, implementing improvements, or migrating an application to a Flink cluster of a later version.
This document describes how to update a Flink streaming application and how to migrate a running streaming application to a different Flink cluster.
Restarting Streaming Applications # The line of action for upgrading a streaming application or migrating an application to a different cluster is based on Flink\u0026rsquo;s Savepoint feature. A savepoint is a consistent snapshot of the state of an application at a specific point in time.
There are two ways of taking a savepoint from a running streaming application.
Taking a savepoint and continue processing. \u0026gt; ./bin/flink savepoint \u0026lt;jobID\u0026gt; [pathToSavepoint] It is recommended to periodically take savepoints in order to be able to restart an application from a previous point in time.
Taking a savepoint and stopping the application as a single action. \u0026gt; ./bin/flink cancel -s [pathToSavepoint] \u0026lt;jobID\u0026gt; This means that the application is canceled immediately after the savepoint completed, i.e., no other checkpoints are taken after the savepoint.
Given a savepoint taken from an application, the same or a compatible application (see Application State Compatibility section below) can be started from that savepoint. Starting an application from a savepoint means that the state of its operators is initialized with the operator state persisted in the savepoint. This is done by starting an application using a savepoint.
\u0026gt; ./bin/flink run -d -s [pathToSavepoint] ~/application.jar The operators of the started application are initialized with the operator state of the original application (i.e., the application the savepoint was taken from) at the time when the savepoint was taken. The started application continues processing from exactly this point on.
Note: Even though Flink consistently restores the state of an application, it cannot revert writes to external systems. This can be an issue if you resume from a savepoint that was taken without stopping the application. In this case, the application has probably emitted data after the savepoint was taken. The restarted application might (depending on whether you changed the application logic or not) emit the same data again. The exact effect of this behavior can be very different depending on the SinkFunction and storage system. Data that is emitted twice might be OK in case of idempotent writes to a key-value store like Cassandra but problematic in case of appends to a durable log such as Kafka. In any case, you should carefully check and test the behavior of a restarted application.
Application State Compatibility # When upgrading an application in order to fix a bug or to improve the application, usually the goal is to replace the application logic of the running application while preserving its state. We do this by starting the upgraded application from a savepoint which was taken from the original application. However, this does only work if both applications are state compatible, meaning that the operators of upgraded application are able to initialize their state with the state of the operators of original application.
In this section, we discuss how applications can be modified to remain state compatible.
DataStream API # Matching Operator State # When an application is restarted from a savepoint, Flink matches the operator state stored in the savepoint to stateful operators of the started application. The matching is done based on operator IDs, which are also stored in the savepoint. Each operator has a default ID that is derived from the operator\u0026rsquo;s position in the application\u0026rsquo;s operator topology. Hence, an unmodified application can always be restarted from one of its own savepoints. However, the default IDs of operators are likely to change if an application is modified. Therefore, modified applications can only be started from a savepoint if the operator IDs have been explicitly specified. Assigning IDs to operators is very simple and done using the uid(String) method as follows:
val mappedEvents: DataStream[(Int, Long)] = events .map(new MyStatefulMapFunc()).uid(\u0026#34;mapper-1\u0026#34;) Note: Since the operator IDs stored in a savepoint and IDs of operators in the application to start must be equal, it is highly recommended to assign unique IDs to all operators of an application that might be upgraded in the future. This advice applies to all operators, i.e., operators with and without explicitly declared operator state, because some operators have internal state that is not visible to the user. Upgrading an application without assigned operator IDs is significantly more difficult and may only be possible via a low-level workaround using the setUidHash() method.
Important: As of 1.3.x this also applies to operators that are part of a chain.
By default all state stored in a savepoint must be matched to the operators of a starting application. However, users can explicitly agree to skip (and thereby discard) state that cannot be matched to an operator when starting a application from a savepoint. Stateful operators for which no state is found in the savepoint are initialized with their default state. Users may enforce best practices by calling ExecutionConfig#disableAutoGeneratedUIDs which will fail the job submission if any operator does not contain a custom unique ID.
Stateful Operators and User Functions # When upgrading an application, user functions and operators can be freely modified with one restriction. It is not possible to change the data type of the state of an operator. This is important because, state from a savepoint can (currently) not be converted into a different data type before it is loaded into an operator. Hence, changing the data type of operator state when upgrading an application breaks application state consistency and prevents the upgraded application from being restarted from the savepoint.
Operator state can be either user-defined or internal.
User-defined operator state: In functions with user-defined operator state the type of the state is explicitly defined by the user. Although it is not possible to change the data type of operator state, a workaround to overcome this limitation can be to define a second state with a different data type and to implement logic to migrate the state from the original state into the new state. This approach requires a good migration strategy and a solid understanding of the behavior of key-partitioned state.
Internal operator state: Operators such as window or join operators hold internal operator state which is not exposed to the user. For these operators the data type of the internal state depends on the input or output type of the operator. Consequently, changing the respective input or output type breaks application state consistency and prevents an upgrade. The following table lists operators with internal state and shows how the state data type relates to their input and output types. For operators which are applied on a keyed stream, the key type (KEY) is always part of the state data type as well.
Operator Data Type of Internal Operator State ReduceFunction[IOT] IOT (Input and output type) [, KEY] WindowFunction[IT, OT, KEY, WINDOW] IT (Input type), KEY AllWindowFunction[IT, OT, WINDOW] IT (Input type) JoinFunction[IT1, IT2, OT] IT1, IT2 (Type of 1. and 2. input), KEY CoGroupFunction[IT1, IT2, OT] IT1, IT2 (Type of 1. and 2. input), KEY Built-in Aggregations (sum, min, max, minBy, maxBy) Input Type [, KEY] Application Topology # Besides changing the logic of one or more existing operators, applications can be upgraded by changing the topology of the application, i.e., by adding or removing operators, changing the parallelism of an operator, or modifying the operator chaining behavior.
When upgrading an application by changing its topology, a few things need to be considered in order to preserve application state consistency.
Adding or removing a stateless operator: This is no problem unless one of the cases below applies. Adding a stateful operator: The state of the operator will be initialized with the default state unless it takes over the state of another operator. Removing a stateful operator: The state of the removed operator is lost unless another operator takes it over. When starting the upgraded application, you have to explicitly agree to discard the state. Changing of input and output types of operators: When adding a new operator before or behind an operator with internal state, you have to ensure that the input or output type of the stateful operator is not modified to preserve the data type of the internal operator state (see above for details). Changing operator chaining: Operators can be chained together for improved performance. When restoring from a savepoint taken since 1.3.x it is possible to modify chains while preserving state consistency. It is possible a break the chain such that a stateful operator is moved out of the chain. It is also possible to append or inject a new or existing stateful operator into a chain, or to modify the operator order within a chain. However, when upgrading a savepoint to 1.3.x it is paramount that the topology did not change in regards to chaining. All operators that are part of a chain should be assigned an ID as described in the Matching Operator State section above. Table API \u0026amp; SQL # Due to the declarative nature of Table API \u0026amp; SQL programs, the underlying operator topology and state representation are mostly determined and optimized by the table planner.
Be aware that any change to both the query and the Flink version could lead to state incompatibility. Every new major-minor Flink version (e.g. 1.12 to 1.13) might introduce new optimizer rules or more specialized runtime operators that change the execution plan. However, the community tries to keep patch versions state-compatible (e.g. 1.13.1 to 1.13.2).
See the table state management section for more information.
Upgrading the Flink Framework Version # This section describes the general way of upgrading Flink across versions and migrating your jobs between the versions.
In a nutshell, this procedure consists of 2 fundamental steps:
Take a savepoint in the previous, old Flink version for the jobs you want to migrate. Resume your jobs under the new Flink version from the previously taken savepoints. Besides those two fundamental steps, some additional steps can be required that depend on the way you want to change the Flink version. In this guide we differentiate two approaches to upgrade across Flink versions: in-place upgrade and shadow copy upgrade.
For in-place update, after taking savepoints, you need to:
Stop/cancel all running jobs. Shutdown the cluster that runs the old Flink version. Upgrade Flink to the newer version on the cluster. Restart the cluster under the new version. For shadow copy, you need to:
Before resuming from the savepoint, setup a new installation of the new Flink version besides your old Flink installation. Resume from the savepoints with the new Flink installation. If everything runs ok, stop and shutdown the old Flink cluster. In the following, we will first present the preconditions for successful job migration and then go into more detail about the steps that we outlined before.
Preconditions # Before starting the migration, please check that the jobs you are trying to migrate are following the best practices for savepoints.
In particular, we advise you to check that explicit uids were set for operators in your job.
This is a soft precondition, and restore should still work in case you forgot about assigning uids. If you run into a case where this is not working, you can manually add the generated legacy vertex ids from previous Flink versions to your job using the setUidHash(String hash) call. For each operator (in operator chains: only the head operator) you must assign the 32 character hex string representing the hash that you can see in the web ui or logs for the operator.
Besides operator uids, there are currently two hard preconditions for job migration that will make migration fail:
We do not support migration for state in RocksDB that was checkpointed using semi-asynchronous mode. In case your old job was using this mode, you can still change your job to use fully-asynchronous mode before taking the savepoint that is used as the basis for the migration.
Another important precondition is that all the savepoint data must be accessible from the new installation under the same (absolute) path. This also includes access to any additional files that are referenced from inside the savepoint file (the output from state backend snapshots), including, but not limited to additional referenced savepoints from modifications with the State Processor API.
STEP 1: Stop the existing job with a savepoint # The first major step in version migration is taking a savepoint and stopping your job running on the old Flink version.
You can do this with the command:
\$ bin/flink stop [--savepointPath :savepointPath] :jobId For more details, please read the savepoint documentation.
STEP 2: Update your cluster to the new Flink version. # In this step, we update the framework version of the cluster. What this basically means is replacing the content of the Flink installation with the new version. This step can depend on how you are running Flink in your cluster (e.g. standalone, \u0026hellip;).
If you are unfamiliar with installing Flink in your cluster, please read the deployment and cluster setup documentation.
STEP 3: Resume the job under the new Flink version from savepoint. # As the last step of job migration, you resume from the savepoint taken above on the updated cluster. You can do this with the command:
\$ bin/flink run -s :savepointPath [:runArgs] For more details, please take a look at the savepoint documentation.
Compatibility Table # Savepoints are compatible across Flink versions as indicated by the table below:
Created with \\ Resumed with 1.1.x 1.2.x 1.3.x 1.4.x 1.5.x 1.6.x 1.7.x 1.8.x 1.9.x 1.10.x 1.11.x 1.12.x 1.13.x 1.14.x 1.15.x Limitations 1.1.x O O O The maximum parallelism of a job that was migrated from Flink 1.1.x to 1.2.x+ is currently fixed as the parallelism of the job. This means that the parallelism can not be increased after migration. This limitation might be removed in a future bugfix release. 1.2.x O O O O O O O O O O O O O O When migrating from Flink 1.2.x to Flink 1.3.x+, changing parallelism at the same time is not supported. Users have to first take a savepoint after migrating to Flink 1.3.x+, and then change parallelism. Savepoints created for CEP applications cannot be restored in 1.4.x+. Savepoints from Flink 1.2 that contain a Scala TraversableSerializer are not compatible with Flink 1.8 anymore because of an update in this serializer. You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8. 1.3.x O O O O O O O O O O O O O Migrating from Flink 1.3.0 to Flink 1.4.[0,1] will fail if the savepoint contains Scala case classes. Users have to directly migrate to 1.4.2+ instead. 1.4.x O O O O O O O O O O O O 1.5.x O O O O O O O O O O O There is a known issue with resuming broadcast state created with 1.5.x in versions 1.6.x up to 1.6.2, and 1.7.0: FLINK-11087. Users upgrading to 1.6.x or 1.7.x series need to directly migrate to minor versions higher than 1.6.2 and 1.7.0, respectively. 1.6.x O O O O O O O O O O 1.7.x O O O O O O O O O 1.8.x O O O O O O O O 1.9.x O O O O O O O 1.10.x O O O O O O 1.11.x O O O O O 1.12.x O O O O 1.13.x O O O Don't upgrade from 1.12.x to 1.13.x with an unaligned checkpoint. Please use a savepoint for migrating. 1.14.x O O 1.15.x O For Table API: 1.15.0 and 1.15.1 generated non-deterministic UIDs for operators that make it difficult/impossible to restore state or upgrade to next patch version. A new table.exec.uid.generation config option (with correct default behavior) disables setting a UID for new pipelines from non-compiled plans. Existing pipelines can set table.exec.uid.generation=ALWAYS if the 1.15.0/1 behavior was acceptable due to a stable environment. See FLINK-28861 for more information. Back to top
`}),e.add({id:233,href:"/flink/flink-docs-master/docs/dev/table/sql/use/",title:"USE Statements",section:"SQL",content:` USE Statements # USE statements are used to set the current database or catalog, or change the resolution order and enabled status of module.
Run a USE statement # Java USE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful USE operation, otherwise will throw an exception.
The following examples show how to run a USE statement in TableEnvironment.
Scala USE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful USE operation, otherwise will throw an exception.
The following examples show how to run a USE statement in TableEnvironment.
Python USE statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful USE operation, otherwise will throw an exception.
The following examples show how to run a USE statement in TableEnvironment.
SQL CLI USE statements can be executed in SQL CLI.
The following examples show how to run a USE statement in SQL CLI.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // create a catalog tEnv.executeSql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print(); // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // | cat1 | // +-----------------+ // change default catalog tEnv.executeSql(\u0026#34;USE CATALOG cat1\u0026#34;); tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // databases are empty // +---------------+ // | database name | // +---------------+ // +---------------+ // create a database tEnv.executeSql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // +---------------+ // | database name | // +---------------+ // | db1 | // +---------------+ // change default database tEnv.executeSql(\u0026#34;USE db1\u0026#34;); // change module resolution order and enabled status tEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;); tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // create a catalog tEnv.executeSql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // | cat1 | // +-----------------+ // change default catalog tEnv.executeSql(\u0026#34;USE CATALOG cat1\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // databases are empty // +---------------+ // | database name | // +---------------+ // +---------------+ // create a database tEnv.executeSql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +---------------+ // | database name | // +---------------+ // | db1 | // +---------------+ // change default database tEnv.executeSql(\u0026#34;USE db1\u0026#34;) // change module resolution order and enabled status tEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;) tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ Python table_env = StreamTableEnvironment.create(...) # create a catalog table_env.execute_sql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW CATALOGS\u0026#34;).print() # +-----------------+ # | catalog name | # +-----------------+ # | default_catalog | # | cat1 | # +-----------------+ # change default catalog table_env.execute_sql(\u0026#34;USE CATALOG cat1\u0026#34;) table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # databases are empty # +---------------+ # | database name | # +---------------+ # +---------------+ # create a database table_env.execute_sql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # +---------------+ # | database name | # +---------------+ # | db1 | # +---------------+ # change default database table_env.execute_sql(\u0026#34;USE db1\u0026#34;) # change module resolution order and enabled status table_env.execute_sql(\u0026#34;USE MODULES hive\u0026#34;) table_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ SQL CLI Flink SQL\u0026gt; CREATE CATALOG cat1 WITH (...); [INFO] Catalog has been created. Flink SQL\u0026gt; SHOW CATALOGS; default_catalog cat1 Flink SQL\u0026gt; USE CATALOG cat1; Flink SQL\u0026gt; SHOW DATABASES; Flink SQL\u0026gt; CREATE DATABASE db1 WITH (...); [INFO] Database has been created. Flink SQL\u0026gt; SHOW DATABASES; db1 Flink SQL\u0026gt; USE db1; Flink SQL\u0026gt; USE MODULES hive; [INFO] Use modules succeeded! Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+-------+ | module name | used | +-------------+-------+ | hive | true | | core | false | +-------------+-------+ 2 rows in set Back to top
USE CATALOG # USE CATALOG catalog_name Set the current catalog. All subsequent commands that do not explicitly specify a catalog will use this one. If the provided catalog does not exist, an exception is thrown. The default current catalog is default_catalog.
USE MODULES # USE MODULES module_name1[, module_name2, ...] Set the enabled modules with declared order. All subsequent commands will resolve metadata(functions/user-defined types/rules, etc.) within enabled modules and follow resolution order. A module is used by default when it is loaded. Loaded modules will become disabled if not used by USE MODULES statement. The default loaded and enabled module is core.
USE # USE [catalog_name.]database_name Set the current database. All subsequent commands that do not explicitly specify a database will use this one. If the provided database does not exist, an exception is thrown. The default current database is default_database.
`}),e.add({id:234,href:"/flink/flink-docs-master/docs/dev/python/table/udfs/vectorized_python_udfs/",title:"Vectorized User-defined Functions",section:"User Defined Functions",content:` Vectorized User-defined Functions # Vectorized Python user-defined functions are functions which are executed by transferring a batch of elements between JVM and Python VM in Arrow columnar format. The performance of vectorized Python user-defined functions are usually much higher than non-vectorized Python user-defined functions as the serialization/deserialization overhead and invocation overhead are much reduced. Besides, users could leverage the popular Python libraries such as Pandas, Numpy, etc for the vectorized Python user-defined functions implementation. These Python libraries are highly optimized and provide high-performance data structures and functions. It shares the similar way as the non-vectorized user-defined functions on how to define vectorized user-defined functions. Users only need to add an extra parameter func_type=\u0026quot;pandas\u0026quot; in the decorator udf or udaf to mark it as a vectorized user-defined function.
NOTE: Python UDF execution requires Python version (3.6, 3.7, 3.8 or 3.9) with PyFlink installed. It\u0026rsquo;s required on both the client side and the cluster side.
Vectorized Scalar Functions # Vectorized Python scalar functions take pandas.Series as the inputs and return a pandas.Series of the same length as the output. Internally, Flink will split the input elements into batches, convert a batch of input elements into Pandas.Series and then call user-defined vectorized Python scalar functions for each batch of input elements. Please refer to the config option python.fn-execution.arrow.batch.size for more details on how to configure the batch size.
Vectorized Python scalar function could be used in any places where non-vectorized Python scalar functions could be used.
The following example shows how to define your own vectorized Python scalar function which computes the sum of two columns, and use it in a query:
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udf @udf(result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) def add(i, j): return i + j settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) # use the vectorized Python scalar function in Python Table API my_table.select(add(col(\u0026#34;bigint\u0026#34;), col(\u0026#34;bigint\u0026#34;))) # use the vectorized Python scalar function in SQL API table_env.create_temporary_function(\u0026#34;add\u0026#34;, add) table_env.sql_query(\u0026#34;SELECT add(bigint, bigint) FROM MyTable\u0026#34;) Vectorized Aggregate Functions # Vectorized Python aggregate functions takes one or more pandas.Series as the inputs and return one scalar value as output.
Note The return type does not support RowType and MapType for the time being.
Vectorized Python aggregate function could be used in GroupBy Aggregation(Batch), GroupBy Window Aggregation(Batch and Stream) and Over Window Aggregation(Batch and Stream bounded over window). For more details on the usage of Aggregations, you can refer to the relevant documentation.
Note Pandas UDAF does not support partial aggregation. Besides, all the data for a group or window will be loaded into memory at the same time during execution and so you must make sure that the data of a group or window could fit into the memory.
The following example shows how to define your own vectorized Python aggregate function which computes mean, and use it in GroupBy Aggregation, GroupBy Window Aggregation and Over Window Aggregation:
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col, lit from pyflink.table.udf import udaf from pyflink.table.window import Tumble @udaf(result_type=DataTypes.FLOAT(), func_type=\u0026#34;pandas\u0026#34;) def mean_udaf(v): return v.mean() settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) my_table = ... # type: Table, table schema: [a: String, b: BigInt, c: BigInt] # use the vectorized Python aggregate function in GroupBy Aggregation my_table.group_by(col(\u0026#39;a\u0026#39;)).select(col(\u0026#39;a\u0026#39;), mean_udaf(col(\u0026#39;b\u0026#39;))) # use the vectorized Python aggregate function in GroupBy Window Aggregation tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) my_table.window(tumble_window) \\ .group_by(col(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, mean_udaf(col(\u0026#39;b\u0026#39;))) # use the vectorized Python aggregate function in Over Window Aggregation table_env.create_temporary_function(\u0026#34;mean_udaf\u0026#34;, mean_udaf) table_env.sql_query(\u0026#34;\u0026#34;\u0026#34; SELECT a, mean_udaf(b) over (PARTITION BY a ORDER BY rowtime ROWS BETWEEN UNBOUNDED preceding AND UNBOUNDED FOLLOWING) FROM MyTable\u0026#34;\u0026#34;\u0026#34;) There are many ways to define a vectorized Python aggregate functions. The following examples show the different ways to define a vectorized Python aggregate function which takes two columns of bigint as the inputs and returns the sum of the maximum of them as the result.
from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf # option 1: extending the base class \`AggregateFunction\` class MaxAdd(AggregateFunction): def open(self, function_context): mg = function_context.get_metric_group() self.counter = mg.add_group(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;).counter(\u0026#34;my_counter\u0026#34;) self.counter_sum = 0 def get_value(self, accumulator): # counter self.counter.inc(10) self.counter_sum += 10 return accumulator[0] def create_accumulator(self): return [] def accumulate(self, accumulator, *args): result = 0 for arg in args: result += arg.max() accumulator.append(result) max_add = udaf(MaxAdd(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # option 2: Python function @udaf(result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) def max_add(i, j): return i.max() + j.max() # option 3: lambda function max_add = udaf(lambda i, j: i.max() + j.max(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # option 4: callable function class CallableMaxAdd(object): def __call__(self, i, j): return i.max() + j.max() max_add = udaf(CallableMaxAdd(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # option 5: partial function def partial_max_add(i, j, k): return i.max() + j.max() + k max_add = udaf(functools.partial(partial_max_add, k=1), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) `}),e.add({id:235,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/window-join/",title:"Window JOIN",section:"Queries",content:` Window Join # Batch Streaming
A window join adds the dimension of time into the join criteria themselves. In doing so, the window join joins the elements of two streams that share a common key and are in the same window. The semantic of window join is same to the DataStream window join
For streaming queries, unlike other joins on continuous tables, window join does not emit intermediate results but only emits final results at the end of the window. Moreover, window join purge all intermediate state when no longer needed.
Usually, Window Join is used with Windowing TVF. Besides, Window Join could follow after other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Currently, Window Join requires the join on condition contains window starts equality of input tables and window ends equality of input tables.
Window Join supports INNER/LEFT/RIGHT/FULL OUTER/ANTI/SEMI JOIN.
INNER/LEFT/RIGHT/FULL OUTER # The following shows the syntax of the INNER/LEFT/RIGHT/FULL OUTER Window Join statement.
SELECT ... FROM L [LEFT|RIGHT|FULL OUTER] JOIN R -- L and R are relations applied windowing TVF ON L.window_start = R.window_start AND L.window_end = R.window_end AND ... The syntax of INNER/LEFT/RIGHT/FULL OUTER WINDOW JOIN are very similar with each other, we only give an example for FULL OUTER JOIN here. When performing a window join, all elements with a common key and a common tumbling window are joined together. We only give an example for a Window Join which works on a Tumble Window TVF. By scoping the region of time for the join into fixed five-minute intervals, we chopped our datasets into two distinct windows of time: [12:00, 12:05) and [12:05, 12:10). The L2 and R2 rows could not join together because they fell into separate windows.
Flink SQL\u0026gt; desc LeftTable; +----------+------------------------+------+-----+--------+----------------------------------+ | name | type | null | key | extras | watermark | +----------+------------------------+------+-----+--------+----------------------------------+ | row_time | TIMESTAMP(3) *ROWTIME* | true | | | \`row_time\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | num | INT | true | | | | | id | STRING | true | | | | +----------+------------------------+------+-----+--------+----------------------------------+ Flink SQL\u0026gt; SELECT * FROM LeftTable; +------------------+-----+----+ | row_time | num | id | +------------------+-----+----+ | 2020-04-15 12:02 | 1 | L1 | | 2020-04-15 12:06 | 2 | L2 | | 2020-04-15 12:03 | 3 | L3 | +------------------+-----+----+ Flink SQL\u0026gt; desc RightTable; +----------+------------------------+------+-----+--------+----------------------------------+ | name | type | null | key | extras | watermark | +----------+------------------------+------+-----+--------+----------------------------------+ | row_time | TIMESTAMP(3) *ROWTIME* | true | | | \`row_time\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | num | INT | true | | | | | id | STRING | true | | | | +----------+------------------------+------+-----+--------+----------------------------------+ Flink SQL\u0026gt; SELECT * FROM RightTable; +------------------+-----+----+ | row_time | num | id | +------------------+-----+----+ | 2020-04-15 12:01 | 2 | R2 | | 2020-04-15 12:04 | 3 | R3 | | 2020-04-15 12:05 | 4 | R4 | +------------------+-----+----+ Flink SQL\u0026gt; SELECT L.num as L_Num, L.id as L_Id, R.num as R_Num, R.id as R_Id, COALESCE(L.window_start, R.window_start) as window_start, COALESCE(L.window_end, R.window_end) as window_end FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L FULL JOIN ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R ON L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end; +-------+------+-------+------+------------------+------------------+ | L_Num | L_Id | R_Num | R_Id | window_start | window_end | +-------+------+-------+------+------------------+------------------+ | 1 | L1 | null | null | 2020-04-15 12:00 | 2020-04-15 12:05 | | null | null | 2 | R2 | 2020-04-15 12:00 | 2020-04-15 12:05 | | 3 | L3 | 3 | R3 | 2020-04-15 12:00 | 2020-04-15 12:05 | | 2 | L2 | null | null | 2020-04-15 12:05 | 2020-04-15 12:10 | | null | null | 4 | R4 | 2020-04-15 12:05 | 2020-04-15 12:10 | +-------+------+-------+------+------------------+------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
SEMI # Semi Window Joins returns a row from one left record if there is at least one matching row on the right side within the common window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE L.num IN ( SELECT num FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:03 | 3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE EXISTS ( SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:03 | 3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
ANTI # Anti Window Joins are the obverse of the Inner Window Join: they contain all of the unjoined rows within each common window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE L.num NOT IN ( SELECT num FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:02 | 1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | | 2020-04-15 12:06 | 2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE NOT EXISTS ( SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:02 | 1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | | 2020-04-15 12:06 | 2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Limitation on Join clause # Currently, The window join requires the join on condition contains window starts equality of input tables and window ends equality of input tables. In the future, we can also simplify the join on clause to only include the window start equality if the windowing TVF is TUMBLE or HOP.
Limitation on windowing TVFs of inputs # Currently, the windowing TVFs must be the same of left and right inputs. This can be extended in the future, for example, tumbling windows join sliding windows with the same window size.
Limitation on Window Join which follows after Windowing TVFs directly # Currently, if Window Join follows after Windowing TVF, the Windowing TVF has to be with Tumble Windows, Hop Windows or Cumulate Windows instead of Session windows.
Back to top
`}),e.add({id:236,href:"/flink/flink-docs-master/docs/dev/datastream/sources/",title:"Data Sources",section:"DataStream API",content:` Data Sources # This page describes Flink\u0026rsquo;s Data Source API and the concepts and architecture behind it. Read this, if you are interested in how data sources in Flink work, or if you want to implement a new Data Source.
If you are looking for pre-defined source connectors, please check the Connector Docs.
Data Source Concepts # Core Components
A Data Source has three core components: Splits, the SplitEnumerator, and the SourceReader.
A Split is a portion of data consumed by the source, like a file or a log partition. Splits are the granularity by which the source distributes the work and parallelizes reading data.
The SourceReader requests Splits and processes them, for example by reading the file or log partition represented by the Split. The SourceReaders run in parallel on the Task Managers in the SourceOperators and produce the parallel stream of events/records.
The SplitEnumerator generates the Splits and assigns them to the SourceReaders. It runs as a single instance on the Job Manager and is responsible for maintaining the backlog of pending Splits and assigning them to the readers in a balanced manner.
The Source class is the API entry point that ties the above three components together.
Unified Across Streaming and Batch
The Data Source API supports both unbounded streaming sources and bounded batch sources, in a unified way.
The difference between both cases is minimal: In the bounded/batch case, the enumerator generates a fixed set of splits, and each split is necessarily finite. In the unbounded streaming case, one of the two is not true (splits are not finite, or the enumerator keeps generating new splits).
Examples # Here are some simplified conceptual examples to illustrate how the data source components interact, in streaming and batch cases.
Note that this does not accurately describe how the Kafka and File source implementations work; parts are simplified, for illustrative purposes.
Bounded File Source
The source has the URI/Path of a directory to read, and a Format that defines how to parse the files.
A Split is a file, or a region of a file (if the data format supports splitting the file). The SplitEnumerator lists all files under the given directory path. It assigns Splits to the next reader that requests a Split. Once all Splits are assigned, it responds to requests with NoMoreSplits. The SourceReader requests a Split and reads the assigned Split (file or file region) and parses it using the given Format. If it does not get another Split, but a NoMoreSplits message, it finishes. Unbounded Streaming File Source
This source works the same way as described above, except that the SplitEnumerator never responds with NoMoreSplits and periodically lists the contents under the given URI/Path to check for new files. Once it finds new files, it generates new Splits for them and can assign them to the available SourceReaders.
Unbounded Streaming Kafka Source
The source has a Kafka Topic (or list of Topics or Topic regex) and a Deserializer to parse the records.
A Split is a Kafka Topic Partition. The SplitEnumerator connects to the brokers to list all topic partitions involved in the subscribed topics. The enumerator can optionally repeat this operation to discover newly added topics/partitions. The SourceReader reads the assigned Splits (Topic Partitions) using the KafkaConsumer and deserializes the records using the provided Deserializer. The splits (Topic Partitions) do not have an end, so the reader never reaches the end of the data. Bounded Kafka Source
Same as above, except that each Split (Topic Partition) has a defined end offset. Once the SourceReader reaches the end offset for a Split, it finishes that Split. Once all assigned Splits are finished, the SourceReader finishes.
The Data Source API # This section describes the major interfaces of the new Source API introduced in FLIP-27, and provides tips to the developers on the Source development.
Source # The Source API is a factory style interface to create the following components.
Split Enumerator Source Reader Split Serializer Enumerator Checkpoint Serializer In addition to that, the Source provides the boundedness attribute of the source, so that Flink can choose the appropriate mode to run the Flink jobs.
The Source implementations should be serializable as the Source instances are serialized and uploaded to the Flink cluster at runtime.
SplitEnumerator # The SplitEnumerator is expected to be the \u0026ldquo;brain\u0026rdquo; of the Source. Typical implementations of the SplitEnumerator do the following:
SourceReader registration handling SourceReader failure handling The addSplitsBack() method will be invoked when a SourceReader fails. The SplitEnumerator should take back the split assignments that have not been acknowledged by the failed SourceReader. SourceEvent handling SourceEvents are custom events sent between SplitEnumerator and SourceReader. The implementation can leverage this mechanism to perform sophisticated coordination. Split discovery and assignment The SplitEnumerator can assign splits to the SourceReaders in response to various events, including discovery of new splits, new SourceReader registration, SourceReader failure, etc. A SplitEnumerator can accomplish the above work with the help of the SplitEnumeratorContext which is provided to the Source on creation or restore of the SplitEnumerator. The SplitEnumeratorContext allows a SplitEnumerator to retrieve necessary information of the readers and perform coordination actions. The Source implementation is expected to pass the SplitEnumeratorContext to the SplitEnumerator instance.
While a SplitEnumerator implementation can work well in a reactive way by only taking coordination actions when its method is invoked, some SplitEnumerator implementations might want to take actions actively. For example, a SplitEnumerator may want to periodically run split discovery and assign the new splits to the SourceReaders. Such implementations may find that the callAsync() method in the SplitEnumeratorContext is handy. The code snippet below shows how the SplitEnumerator implementation can achieve that without maintaining its own threads.
Java class MySplitEnumerator implements SplitEnumerator\u0026lt;MySplit, MyCheckpoint\u0026gt; { private final long DISCOVER_INTERVAL = 60_000L; private final SplitEnumeratorContext\u0026lt;MySplit\u0026gt; enumContext ; /** The Source creates instances of SplitEnumerator and provides the context. */ MySplitEnumerator(SplitEnumeratorContext\u0026lt;MySplit\u0026gt; enumContext) { this.enumContext = enumContext; } /** * A method to discover the splits. */ private List\u0026lt;MySplit\u0026gt; discoverSplits() {...} @Override public void start() { ... enumContext.callAsync(this::discoverSplits, (splits, thrown) -\u0026gt; { Map\u0026lt;Integer, List\u0026lt;MySplit\u0026gt;\u0026gt; assignments = new HashMap\u0026lt;\u0026gt;(); int parallelism = enumContext.currentParallelism(); for (MySplit split : splits) { int owner = split.splitId().hashCode() % parallelism; assignments.computeIfAbsent(owner, s -\u0026gt; new ArrayList\u0026lt;\u0026gt;()).add(split); } enumContext.assignSplits(new SplitsAssignment\u0026lt;\u0026gt;(assignments)); }, 0L, DISCOVER_INTERVAL); ... } ... } Python Still not supported in Python API. SourceReader # The SourceReader is a component running in the Task Managers to consume the records from the Splits.
The SourceReader exposes a pull-based consumption interface. A Flink task keeps calling pollNext(ReaderOutput) in a loop to poll records from the SourceReader. The return value of the pollNext(ReaderOutput) method indicates the status of the source reader.
MORE_AVAILABLE - The SourceReader has more records available immediately. NOTHING_AVAILABLE - The SourceReader does not have more records available at this point, but may have more records in the future. END_OF_INPUT - The SourceReader has exhausted all the records and reached the end of data. This means the SourceReader can be closed. In the interest of performance, a ReaderOutput is provided to the pollNext(ReaderOutput) method, so a SourceReader can emit multiple records in a single call of pollNext() if it has to. For example, sometimes the external system works at the granularity of blocks. A block may contain multiple records but the source can only checkpoint at the block boundaries. In this case the SourceReader can emit all the records in one block at a time to the ReaderOutput. However, the SourceReader implementation should avoid emitting multiple records in a single pollNext(ReaderOutput) invocation unless necessary. This is because the task thread that is polling from the SourceReader works in an event-loop and cannot block.
All the state of a SourceReader should be maintained inside the SourceSplits which are returned at the snapshotState() invocation. Doing this allows the SourceSplits to be reassigned to other SourceReaders when needed.
A SourceReaderContext is provided to the Source upon a SourceReader creation. It is expected that the Source will pass the context to the SourceReader instance. The SourceReader can send SourceEvent to its SplitEnumerator through the SourceReaderContext. A typical design pattern of the Source is letting the SourceReaders report their local information to the SplitEnumerator who has a global view to make decisions.
The SourceReader API is a low level API that allows users to deal with the splits manually and have their own threading model to fetch and handover the records. To facilitate the SourceReader implementation, Flink has provided a SourceReaderBase class which significantly reduces the amount the work needed to write a SourceReader. It is highly recommended for the connector developers to take advantage of the SourceReaderBase instead of writing the SourceReaders from scratch. For more details please check the Split Reader API section.
Use the Source # In order to create a DataStream from a Source, one needs to pass the Source to a StreamExecutionEnvironment. For example,
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Source mySource = new MySource(...); DataStream\u0026lt;Integer\u0026gt; stream = env.fromSource( mySource, WatermarkStrategy.noWatermarks(), \u0026#34;MySourceName\u0026#34;); ... Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val mySource = new MySource(...) val stream = env.fromSource( mySource, WatermarkStrategy.noWatermarks(), \u0026#34;MySourceName\u0026#34;) ... Python env = StreamExecutionEnvironment.get_execution_environment() my_source = ... env.from_source( my_source, WatermarkStrategy.no_watermarks(), \u0026#34;my_source_name\u0026#34;) The Split Reader API # The core SourceReader API is fully asynchronous and requires implementations to manually manage reading splits asynchronously. However, in practice, most sources perform blocking operations, like blocking poll() calls on clients (for example the KafkaConsumer), or blocking I/O operations on distributed file systems (HDFS, S3, \u0026hellip;). To make this compatible with the asynchronous Source API, these blocking (synchronous) operations need to happen in separate threads, which hand over the data to the asynchronous part of the reader.
The SplitReader is the high-level API for simple synchronous reading/polling-based source implementations, like file reading, Kafka, etc.
The core is the SourceReaderBase class, which takes a SplitReader and creates fetcher threads running the SplitReader, supporting different consumption threading models.
SplitReader # The SplitReader API only has three methods:
A blocking fetch method to return a RecordsWithSplitIds A non-blocking method to handle split changes. A non-blocking wake up method to wake up the blocking fetch operation. The SplitReader only focuses on reading the records from the external system, therefore is much simpler compared with SourceReader. Please check the Java doc of the class for more details.
SourceReaderBase # It is quite common that a SourceReader implementation does the following:
Have a pool of threads fetching from splits of the external system in a blocking way. Handle the synchronization between the internal fetching threads and other methods invocations such as pollNext(ReaderOutput). Maintain the per split watermark for watermark alignment. Maintain the state of each split for checkpoint. In order to reduce the work of writing a new SourceReader, Flink provides a SourceReaderBase class to serve as a base implementation of the SourceReader. SourceReaderBase has all the above work done out of the box. To write a new SourceReader, one can just let the SourceReader implementation inherit from the SourceReaderBase, fill in a few methods and implement a high level SplitReader .
SplitFetcherManager # The SourceReaderBase supports a few threading models out of the box, depending on the behavior of the SplitFetcherManager it works with. The SplitFetcherManager helps create and maintain a pool of SplitFetchers each fetching with a SplitReader. It also determines how to assign splits to each split fetcher.
As an example, as illustrated below, a SplitFetcherManager may have a fixed number of threads, each fetching from some splits assigned to the SourceReader.
The following code snippet implements this threading model.
Java /** * A SplitFetcherManager that has a fixed size of split fetchers and assign splits * to the split fetchers based on the hash code of split IDs. */ public class FixedSizeSplitFetcherManager\u0026lt;E, SplitT extends SourceSplit\u0026gt; extends SplitFetcherManager\u0026lt;E, SplitT\u0026gt; { private final int numFetchers; public FixedSizeSplitFetcherManager( int numFetchers, FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitReaderSupplier) { super(elementsQueue, splitReaderSupplier); this.numFetchers = numFetchers; // Create numFetchers split fetchers. for (int i = 0; i \u0026lt; numFetchers; i++) { startFetcher(createSplitFetcher()); } } @Override public void addSplits(List\u0026lt;SplitT\u0026gt; splitsToAdd) { // Group splits by their owner fetchers. Map\u0026lt;Integer, List\u0026lt;SplitT\u0026gt;\u0026gt; splitsByFetcherIndex = new HashMap\u0026lt;\u0026gt;(); splitsToAdd.forEach(split -\u0026gt; { int ownerFetcherIndex = split.hashCode() % numFetchers; splitsByFetcherIndex .computeIfAbsent(ownerFetcherIndex, s -\u0026gt; new ArrayList\u0026lt;\u0026gt;()) .add(split); }); // Assign the splits to their owner fetcher. splitsByFetcherIndex.forEach((fetcherIndex, splitsForFetcher) -\u0026gt; { fetchers.get(fetcherIndex).addSplits(splitsForFetcher); }); } } Python Still not supported in Python API. And a SourceReader using this threading model can be created like following:
Java public class FixedFetcherSizeSourceReader\u0026lt;E, T, SplitT extends SourceSplit, SplitStateT\u0026gt; extends SourceReaderBase\u0026lt;E, T, SplitT, SplitStateT\u0026gt; { public FixedFetcherSizeSourceReader( FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitFetcherSupplier, RecordEmitter\u0026lt;E, T, SplitStateT\u0026gt; recordEmitter, Configuration config, SourceReaderContext context) { super( elementsQueue, new FixedSizeSplitFetcherManager\u0026lt;\u0026gt;( config.getInteger(SourceConfig.NUM_FETCHERS), elementsQueue, splitFetcherSupplier), recordEmitter, config, context); } @Override protected void onSplitFinished(Map\u0026lt;String, SplitStateT\u0026gt; finishedSplitIds) { // Do something in the callback for the finished splits. } @Override protected SplitStateT initializedState(SplitT split) { ... } @Override protected SplitT toSplitType(String splitId, SplitStateT splitState) { ... } } Python Still not supported in Python API. The SourceReader implementations can also implement their own threading model easily on top of the SplitFetcherManager and SourceReaderBase.
Event Time and Watermarks # Event Time assignment and Watermark Generation happen as part of the data sources. The event streams leaving the Source Readers have event timestamps and (during streaming execution) contain watermarks. See Timely Stream Processing for an introduction to Event Time and Watermarks.
Applications based on the legacy SourceFunction typically generate timestamps and watermarks in a separate later step via stream.assignTimestampsAndWatermarks(WatermarkStrategy). This function should not be used with the new sources, because timestamps will be already assigned, and it will override the previous split-aware watermarks. API # The WatermarkStrategy is passed to the Source during creation in the DataStream API and creates both the TimestampAssigner and WatermarkGenerator .
Java environment.fromSource( Source\u0026lt;OUT, ?, ?\u0026gt; source, WatermarkStrategy\u0026lt;OUT\u0026gt; timestampsAndWatermarks, String sourceName); Python environment.from_source( source: Source, watermark_strategy: WatermarkStrategy, source_name: str, type_info: TypeInformation = None) The TimestampAssigner and WatermarkGenerator run transparently as part of the ReaderOutput(or SourceOutput) so source implementors do not have to implement any timestamp extraction and watermark generation code.
Event Timestamps # Event timestamps are assigned in two steps:
The SourceReader may attach the source record timestamp to the event, by calling SourceOutput.collect(event, timestamp). This is relevant only for data sources that are record-based and have timestamps, such as Kafka, Kinesis, Pulsar, or Pravega. Sources that are not based on records with timestamps (like files) do not have a source record timestamp. This step is part of the source connector implementation and not parameterized by the application that uses the source.
The TimestampAssigner, which is configured by the application, assigns the final timestamp. The TimestampAssigner sees the original source record timestamp and the event. The assigner can use the source record timestamp or access a field of the event to obtain the final event timestamp.
This two-step approach allows users to reference both timestamps from the source systems and timestamps in the event\u0026rsquo;s data as the event timestamp.
Note: When using a data source without source record timestamps (like files) and selecting the source record timestamp as the final event timestamp, events will get a default timestamp equal to LONG_MIN (=-9,223,372,036,854,775,808).
Watermark Generation # Watermark Generators are only active during streaming execution. Batch execution deactivates Watermark Generators; all related operations described below become effectively no-ops.
The data source API supports running watermark generators individually per split. That allows Flink to observe the event time progress per split individually, which is important to handle event time skew properly and prevent idle partitions from holding back the event time progress of the entire application.
When implementing a source connector using the Split Reader API, this is automatically handled. All implementations based on the Split Reader API have split-aware watermarks out-of-the-box.
For an implementation of the lower level SourceReader API to use split-aware watermark generation, the implementation must output events from different splits to different outputs: the Split-local SourceOutputs. Split-local outputs can be created and released on the main ReaderOutput via the createOutputForSplit(splitId) and releaseOutputForSplit(splitId) methods. Please refer to the JavaDocs of the class and methods for details.
`}),e.add({id:237,href:"/flink/flink-docs-master/docs/internals/filesystems/",title:"File Systems",section:"Internals",content:` File Systems # Flink has its own file system abstraction via the org.apache.flink.core.fs.FileSystem class. This abstraction provides a common set of operations and minimal guarantees across various types of file system implementations.
The FileSystem\u0026rsquo;s set of available operations is quite limited, in order to support a wide range of file systems. For example, appending to or mutating existing files is not supported.
File systems are identified by a file system scheme, such as file://, hdfs://, etc.
Implementations # Flink implements the file systems directly, with the following file system schemes:
file, which represents the machine\u0026rsquo;s local file system. Other file system types are accessed by an implementation that bridges to the suite of file systems supported by Apache Hadoop. The following is an incomplete list of examples:
hdfs: Hadoop Distributed File System s3, s3n, and s3a: Amazon S3 file system gcs: Google Cloud Storage \u0026hellip; Flink loads Hadoop\u0026rsquo;s file systems transparently if it finds the Hadoop File System classes in the class path and finds a valid Hadoop configuration. By default, it looks for the Hadoop configuration in the class path. Alternatively, one can specify a custom location via the configuration entry fs.hdfs.hadoopconf.
Persistence Guarantees # These FileSystem and its FsDataOutputStream instances are used to persistently store data, both for results of applications and for fault tolerance and recovery. It is therefore crucial that the persistence semantics of these streams are well defined.
Definition of Persistence Guarantees # Data written to an output stream is considered persistent, if two requirements are met:
Visibility Requirement: It must be guaranteed that all other processes, machines, virtual machines, containers, etc. that are able to access the file see the data consistently when given the absolute file path. This requirement is similar to the close-to-open semantics defined by POSIX, but restricted to the file itself (by its absolute path).
Durability Requirement: The file system\u0026rsquo;s specific durability/persistence requirements must be met. These are specific to the particular file system. For example the {@link LocalFileSystem} does not provide any durability guarantees for crashes of both hardware and operating system, while replicated distributed file systems (like HDFS) guarantee typically durability in the presence of up n concurrent node failures, where n is the replication factor.
Updates to the file\u0026rsquo;s parent directory (such that the file shows up when listing the directory contents) are not required to be complete for the data in the file stream to be considered persistent. This relaxation is important for file systems where updates to directory contents are only eventually consistent.
The FSDataOutputStream has to guarantee data persistence for the written bytes once the call to FSDataOutputStream.close() returns.
Examples # For fault-tolerant distributed file systems, data is considered persistent once it has been received and acknowledged by the file system, typically by having been replicated to a quorum of machines (durability requirement). In addition the absolute file path must be visible to all other machines that will potentially access the file (visibility requirement).
Whether data has hit non-volatile storage on the storage nodes depends on the specific guarantees of the particular file system.
The metadata updates to the file\u0026rsquo;s parent directory are not required to have reached a consistent state. It is permissible that some machines see the file when listing the parent directory\u0026rsquo;s contents while others do not, as long as access to the file by its absolute path is possible on all nodes.
A local file system must support the POSIX close-to-open semantics. Because the local file system does not have any fault tolerance guarantees, no further requirements exist.
The above implies specifically that data may still be in the OS cache when considered persistent from the local file system\u0026rsquo;s perspective. Crashes that cause the OS cache to lose data are considered fatal to the local machine and are not covered by the local file system\u0026rsquo;s guarantees as defined by Flink.
That means that computed results, checkpoints, and savepoints that are written only to the local filesystem are not guaranteed to be recoverable from the local machine\u0026rsquo;s failure, making local file systems unsuitable for production setups.
Updating File Contents # Many file systems either do not support overwriting contents of existing files at all, or do not support consistent visibility of the updated contents in that case. For that reason, Flink\u0026rsquo;s FileSystem does not support appending to existing files, or seeking within output streams such that previously written data could be changed within the same file.
Overwriting Files # Overwriting files is in general possible. A file is overwritten by deleting it and creating a new file. However, certain filesystems cannot make that change synchronously visible to all parties that have access to the file. For example Amazon S3 guarantees only eventual consistency in the visibility of the file replacement: Some machines may see the old file, some machines may see the new file.
To avoid these consistency issues, the implementations of failure/recovery mechanisms in Flink strictly avoid writing to the same file path more than once.
Thread Safety # Implementations of FileSystem must be thread-safe: The same instance of FileSystem is frequently shared across multiple threads in Flink and must be able to concurrently create input/output streams and list file metadata.
The FSDataOutputStream and FSDataOutputStream implementations are strictly not thread-safe. Instances of the streams should also not be passed between threads in between read or write operations, because there are no guarantees about the visibility of operations across threads (many operations do not create memory fences).
Back to top
`}),e.add({id:238,href:"/flink/flink-docs-master/docs/ops/production_ready/",title:"Production Readiness Checklist",section:"Operations",content:` Production Readiness Checklist # The production readiness checklist provides an overview of configuration options that should be carefully considered before bringing an Apache Flink job into production. While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs.
Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale. There is currently no way to change the maximum parallelism of an operator after a job has started without discarding that operators state. The reason maximum parallelism exists, versus allowing stateful operators to be infinitely scalable, is that it has some impact on your application\u0026rsquo;s performance and state size. Flink has to maintain specific metadata for its ability to rescale state which grows linearly with max parallelism. In general, you should choose max parallelism that is high enough to fit your future needs in scalability, while keeping it low enough to maintain reasonable performance.
Maximum parallelism must fulfill the following conditions: 0 \u0026lt; parallelism \u0026lt;= max parallelism \u0026lt;= 2^15 You can explicitly set maximum parallelism by using setMaxParallelism(int maxparallelism). If no max parallelism is set Flink will decide using a function of the operators parallelism when the job is first started:
128 : for all parallelism \u0026lt;= 128. MIN(nextPowerOfTwo(parallelism + (parallelism / 2)), 2^15) : for all parallelism \u0026gt; 128. Set UUIDs For All Operators # As mentioned in the documentation for savepoints, users should set uids for each operator in their DataStream. Uids are necessary for Flink\u0026rsquo;s mapping of operator states to operators which, in turn, is essential for savepoints. By default, operator uids are generated by traversing the JobGraph and hashing specific operator properties. While this is comfortable from a user perspective, it is also very fragile, as changes to the JobGraph (e.g., exchanging an operator) results in new UUIDs. To establish a stable mapping, we need stable operator uids provided by the user through setUid(String uid).
Choose The Right State Backend # See the description of state backends for choosing the right one for your use case.
Choose The Right Checkpoint Interval # Checkpointing is Flink\u0026rsquo;s primary fault-tolerance mechanism, wherein a snapshot of your job\u0026rsquo;s state persisted periodically to some durable location. In the case of failure, Flink will restart from the most recent checkpoint and resume processing. A jobs checkpoint interval configures how often Flink will take these snapshots. While there is no single correct answer on the perfect checkpoint interval, the community can guide what factors to consider when configuring this parameter.
What is the SLA of your service: Checkpoint interval is best understood as an expression of the jobs service level agreement (SLA). In the worst-case scenario, where a job fails one second before the next checkpoint, how much data can you tolerate reprocessing? A checkpoint interval of 5 minutes implies that Flink will never reprocess more than 5 minutes worth of data after a failure.
How often must your service deliver results: Exactly once sinks, such as Kafka or the FileSink, only make results visible on checkpoint completion. Shorter checkpoint intervals make results available more quickly but may also put additional pressure on these systems. It is important to work with stakeholders to find a delivery time that meet product requirements without putting undue load on your sinks.
How much load can your Task Managers sustain: All of Flinks\u0026rsquo; built-in state backends support asynchronous checkpointing, meaning the snapshot process will not pause data processing. However, it still does require CPU cycles and network bandwidth from your machines. Incremental checkpointing can be a powerful tool to reduce the cost of any given checkpoint.
And most importantly, test and measure your job. Every Flink application is unique, and the best way to find the appropriate checkpoint interval is to see how yours behaves in practice.
Configure JobManager High Availability # The JobManager serves as a central coordinator for each Flink deployment, being responsible for both scheduling and resource management of the cluster. It is a single point of failure within the cluster, and if it crashes, no new jobs can be submitted, and running applications will fail.
Configuring High Availability, in conjunction with Apache Zookeeper or Flinks Kubernetes based service, allows for a swift recovery and is highly recommended for production setups.
Back to top
`}),e.add({id:239,href:"/flink/flink-docs-master/docs/connectors/table/formats/raw/",title:"Raw",section:"Formats",content:` Raw Format # Format: Serialization Schema Format: Deserialization Schema
The Raw format allows to read and write raw (byte based) values as a single column.
Note: this format encodes null values as null of byte[] type. This may have limitation when used in upsert-kafka, because upsert-kafka treats null values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using upsert-kafka connector and the raw format as a value.format if the field can have a null value.
The Raw connector is built-in, no additional dependencies are required.
Example # For example, you may have following raw log data in Kafka and want to read and analyse such data using Flink SQL.
47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \u0026#34;GET /?p=1 HTTP/2.0\u0026#34; 200 5316 \u0026#34;https://domain.com/?p=1\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\u0026#34; \u0026#34;2.75\u0026#34; The following creates a table where it reads from (and can writes to) the underlying Kafka topic as an anonymous string value in UTF-8 encoding by using raw format:
CREATE TABLE nginx_log ( log STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;nginx_log\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;raw\u0026#39; ) Then you can read out the raw data as a pure string, and split it into multiple fields using an user-defined-function for further analysing, e.g. my_split in the example.
SELECT t.hostname, t.datetime, t.url, t.browser, ... FROM( SELECT my_split(log) as t FROM nginx_log ); In contrast, you can also write a single column of STRING type into this Kafka topic as an anonymous string value in UTF-8 encoding.
Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'raw'. raw.charset optional UTF-8 String Specify the charset to encode the text string. raw.endianness optional big-endian String Specify the endianness to encode the bytes of numeric value. Valid values are 'big-endian' and 'little-endian'. See more details of endianness. Data Type Mapping # The table below details the SQL types the format supports, including details of the serializer and deserializer class for encoding and decoding.
Flink SQL type Value CHAR / VARCHAR / STRING A UTF-8 (by default) encoded text string.
The encoding charset can be configured by 'raw.charset'. BINARY / VARBINARY / BYTES The sequence of bytes itself. BOOLEAN A single byte to indicate boolean value, 0 means false, 1 means true. TINYINT A single byte of the signed number value. SMALLINT Two bytes with big-endian (by default) encoding.
The endianness can be configured by 'raw.endianness'. INT Four bytes with big-endian (by default) encoding.
The endianness can be configured by 'raw.endianness'. BIGINT Eight bytes with big-endian (by default) encoding.
The endianness can be configured by 'raw.endianness'. FLOAT Four bytes with IEEE 754 format and big-endian (by default) encoding.
The endianness can be configured by 'raw.endianness'. DOUBLE Eight bytes with IEEE 754 format and big-endian (by default) encoding.
The endianness can be configured by 'raw.endianness'. RAW The sequence of bytes serialized by the underlying TypeSerializer of the RAW type. `}),e.add({id:240,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/set-ops/",title:"Set Operations",section:"Queries",content:` Set Operations # Batch Streaming
UNION # UNION and UNION ALL return the rows that are found in either table. UNION takes only distinct rows while UNION ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; create view t1(s) as values (\u0026#39;c\u0026#39;), (\u0026#39;a\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;c\u0026#39;); Flink SQL\u0026gt; create view t2(s) as values (\u0026#39;d\u0026#39;), (\u0026#39;e\u0026#39;), (\u0026#39;a\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;b\u0026#39;); Flink SQL\u0026gt; (SELECT s FROM t1) UNION (SELECT s FROM t2); +---+ | s| +---+ | c| | a| | b| | d| | e| +---+ Flink SQL\u0026gt; (SELECT s FROM t1) UNION ALL (SELECT s FROM t2); +---+ | c| +---+ | c| | a| | b| | b| | c| | d| | e| | a| | b| | b| +---+ INTERSECT # INTERSECT and INTERSECT ALL return the rows that are found in both tables. INTERSECT takes only distinct rows while INTERSECT ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; (SELECT s FROM t1) INTERSECT (SELECT s FROM t2); +---+ | s| +---+ | a| | b| +---+ Flink SQL\u0026gt; (SELECT s FROM t1) INTERSECT ALL (SELECT s FROM t2); +---+ | s| +---+ | a| | b| | b| +---+ EXCEPT # EXCEPT and EXCEPT ALL return the rows that are found in one table but not the other. EXCEPT takes only distinct rows while EXCEPT ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; (SELECT s FROM t1) EXCEPT (SELECT s FROM t2); +---+ | s | +---+ | c | +---+ Flink SQL\u0026gt; (SELECT s FROM t1) EXCEPT ALL (SELECT s FROM t2); +---+ | s | +---+ | c | | c | +---+ IN # Returns true if an expression exists in a given table sub-query. The sub-query table must consist of one column. This column must have the same data type as the expression.
SELECT user, amount FROM Orders WHERE product IN ( SELECT product FROM NewProducts ) The optimizer rewrites the IN condition into a join and group operation. For streaming queries, the required state for computing the query result might grow infinitely depending on the number of distinct input rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
EXISTS # SELECT user, amount FROM Orders WHERE product EXISTS ( SELECT product FROM NewProducts ) Returns true if the sub-query returns at least one row. Only supported if the operation can be rewritten in a join and group operation.
The optimizer rewrites the EXISTS operation into a join and group operation. For streaming queries, the required state for computing the query result might grow infinitely depending on the number of distinct input rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
Back to top
`}),e.add({id:241,href:"/flink/flink-docs-master/docs/dev/table/sql/show/",title:"SHOW Statements",section:"SQL",content:" SHOW Statements # SHOW statements are used to list objects within their corresponding parent, such as catalogs, databases, tables and views, columns, functions, and modules. See the individual commands for more details and additional options.\nSHOW CREATE statements are used to print a DDL statement with which a given object can be created. The currently \u0026lsquo;SHOW CREATE\u0026rsquo; statement is only available in printing DDL statement of the given table and view.\nFlink SQL supports the following SHOW statements for now:\nSHOW CATALOGS SHOW CURRENT CATALOG SHOW DATABASES SHOW CURRENT DATABASE SHOW TABLES SHOW CREATE TABLE SHOW COLUMNS SHOW VIEWS SHOW CREATE VIEW SHOW FUNCTIONS SHOW MODULES SHOW JARS Run a SHOW statement # Java SHOW statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns objects for a successful SHOW operation, otherwise will throw an exception.\nThe following examples show how to run a SHOW statement in TableEnvironment.\nScala SHOW statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns objects for a successful SHOW operation, otherwise will throw an exception.\nThe following examples show how to run a SHOW statement in TableEnvironment.\nPython SHOW statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns objects for a successful SHOW operation, otherwise will throw an exception.\nThe following examples show how to run a SHOW statement in TableEnvironment.\nSQL CLI SHOW statements can be executed in SQL CLI.\nThe following examples show how to run a SHOW statement in SQL CLI.\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // show catalogs tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print(); // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // +-----------------+ // show current catalog tEnv.executeSql(\u0026#34;SHOW CURRENT CATALOG\u0026#34;).print(); // +----------------------+ // | current catalog name | // +----------------------+ // | default_catalog | // +----------------------+ // show databases tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // +------------------+ // | database name | // +------------------+ // | default_database | // +------------------+ // show current database tEnv.executeSql(\u0026#34;SHOW CURRENT DATABASE\u0026#34;).print(); // +-----------------------+ // | current database name | // +-----------------------+ // | default_database | // +-----------------------+ // create a table tEnv.executeSql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;); // show tables tEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // +------------+ // | table name | // +------------+ // | my_table | // +------------+ // show create table tEnv.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print(); // CREATE TABLE `default_catalog`.`default_db`.`my_table` ( // ... // ) WITH ( // ... // ) // show columns tEnv.executeSql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print(); // +--------+-------+------+-----+--------+-----------+ // | name | type | null | key | extras | watermark | // +--------+-------+------+-----+--------+-----------+ // | field2 | BYTES | true | | | | // +--------+-------+------+-----+--------+-----------+ // create a view tEnv.executeSql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;); // show views tEnv.executeSql(\u0026#34;SHOW VIEWS\u0026#34;).print(); // +-----------+ // | view name | // +-----------+ // | my_view | // +-----------+ // show create view tEnv.executeSql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print(); // CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as // SELECT * // FROM `default_catalog`.`default_database`.`my_table` // show functions tEnv.executeSql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print(); // +---------------+ // | function name | // +---------------+ // | mod | // | sha256 | // | ... | // +---------------+ // create a user defined function tEnv.executeSql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;); // show user defined functions tEnv.executeSql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print(); // +---------------+ // | function name | // +---------------+ // | f1 | // | ... | // +---------------+ // show modules tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ // show full modules tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | core | true | // | hive | false | // +-------------+-------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // show catalogs tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // +-----------------+ // show databases tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +------------------+ // | database name | // +------------------+ // | default_database | // +------------------+ // create a table tEnv.executeSql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;) // show tables tEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // +------------+ // | table name | // +------------+ // | my_table | // +------------+ // show create table tEnv.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print() // CREATE TABLE `default_catalog`.`default_db`.`my_table` ( // ... // ) WITH ( // ... // ) // show columns tEnv.executeSql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print() // +--------+-------+------+-----+--------+-----------+ // | name | type | null | key | extras | watermark | // +--------+-------+------+-----+--------+-----------+ // | field2 | BYTES | true | | | | // +--------+-------+------+-----+--------+-----------+ // create a view tEnv.executeSql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;) // show views tEnv.executeSql(\u0026#34;SHOW VIEWS\u0026#34;).print() // +-----------+ // | view name | // +-----------+ // | my_view | // +-----------+ // show create view tEnv.executeSql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print(); // CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as // SELECT * // FROM `default_catalog`.`default_database`.`my_table` // show functions tEnv.executeSql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print() // +---------------+ // | function name | // +---------------+ // | mod | // | sha256 | // | ... | // +---------------+ // create a user defined function tEnv.executeSql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;) // show user defined functions tEnv.executeSql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print() // +---------------+ // | function name | // +---------------+ // | f1 | // | ... | // +---------------+ // show modules tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ // show full modules tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | core | true | // | hive | false | // +-------------+-------+ Python table_env = StreamTableEnvironment.create(...) # show catalogs table_env.execute_sql(\u0026#34;SHOW CATALOGS\u0026#34;).print() # +-----------------+ # | catalog name | # +-----------------+ # | default_catalog | # +-----------------+ # show databases table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # +------------------+ # | database name | # +------------------+ # | default_database | # +------------------+ # create a table table_env.execute_sql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;) # show tables table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() # +------------+ # | table name | # +------------+ # | my_table | # +------------+ # show create table table_env.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print() # CREATE TABLE `default_catalog`.`default_db`.`my_table` ( # ... # ) WITH ( # ... # ) # show columns table_env.execute_sql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print() # +--------+-------+------+-----+--------+-----------+ # | name | type | null | key | extras | watermark | # +--------+-------+------+-----+--------+-----------+ # | field2 | BYTES | true | | | | # +--------+-------+------+-----+--------+-----------+ # create a view table_env.execute_sql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;) # show views table_env.execute_sql(\u0026#34;SHOW VIEWS\u0026#34;).print() # +-----------+ # | view name | # +-----------+ # | my_view | # +-----------+ # show create view table_env.execute_sql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print() # CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as # SELECT * # FROM `default_catalog`.`default_database`.`my_table` # show functions table_env.execute_sql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print() # +---------------+ # | function name | # +---------------+ # | mod | # | sha256 | # | ... | # +---------------+ # create a user defined function table_env.execute_sql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;) # show user defined functions table_env.execute_sql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print() # +---------------+ # | function name | # +---------------+ # | f1 | # | ... | # +---------------+ # show modules table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ # show full modules table_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | core | true | # | hive | false | # +-------------+-------+ SQL CLI Flink SQL\u0026gt; SHOW CATALOGS; default_catalog Flink SQL\u0026gt; SHOW DATABASES; default_database Flink SQL\u0026gt; CREATE TABLE my_table (...) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; my_table Flink SQL\u0026gt; SHOW CREATE TABLE my_table; CREATE TABLE `default_catalog`.`default_db`.`my_table` ( ... ) WITH ( ... ) Flink SQL\u0026gt; SHOW COLUMNS from MyUserTable LIKE \u0026#39;%f%\u0026#39;; +--------+-------+------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +--------+-------+------+-----+--------+-----------+ | field2 | BYTES | true | | | | +--------+-------+------+-----+--------+-----------+ 1 row in set Flink SQL\u0026gt; CREATE VIEW my_view AS SELECT * from my_table; [INFO] View has been created. Flink SQL\u0026gt; SHOW VIEWS; my_view Flink SQL\u0026gt; SHOW CREATE VIEW my_view; CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as SELECT * FROM `default_catalog`.`default_database`.`my_table` Flink SQL\u0026gt; SHOW FUNCTIONS; mod sha256 ... Flink SQL\u0026gt; CREATE FUNCTION f1 AS ...; [INFO] Function has been created. Flink SQL\u0026gt; SHOW USER FUNCTIONS; f1 ... Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | +-------------+ 1 row in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | +-------------+------+ 1 row in set Flink SQL\u0026gt; SHOW JARS; /path/to/addedJar.jar Back to top\nSHOW CATALOGS # SHOW CATALOGS Show all catalogs.\nSHOW CURRENT CATALOG # SHOW CURRENT CATALOG Show current catalog.\nSHOW DATABASES # SHOW DATABASES Show all databases in the current catalog.\nSHOW CURRENT DATABASE # SHOW CURRENT DATABASE Show current database.\nSHOW TABLES # SHOW TABLES [ ( FROM | IN ) [catalog_name.]database_name ] [ [NOT] LIKE \u0026lt;sql_like_pattern\u0026gt; ] Show all tables for an optionally specified database. If no database is specified then the tables are returned from the current database. Additionally, the output of this statement may be filtered by an optional matching pattern.\nLIKE Show all tables with given table name and optional LIKE clause, whose name is whether similar to the \u0026lt;sql_like_pattern\u0026gt;.\nThe syntax of sql pattern in LIKE clause is the same as that of MySQL dialect.\n% matches any number of characters, even zero characters, \\% matches one % character. _ matches exactly one character, \\_ matches one _ character. SHOW TABLES EXAMPLES # Assumes that the db1 database located in catalog1 catalog has the following tables:\nperson dim the current database in session has the following tables:\nfights orders Shows all tables of the given database. show tables from db1; -- show tables from catalog1.db1; -- show tables in db1; -- show tables in catalog1.db1; +------------+ | table name | +------------+ | dim | | person | +------------+ 2 rows in set Shows all tables of the given database, which are similar to the given sql pattern. show tables from db1 like \u0026#39;%n\u0026#39;; -- show tables from catalog1.db1 like \u0026#39;%n\u0026#39;; -- show tables in db1 like \u0026#39;%n\u0026#39;; -- show tables in catalog1.db1 like \u0026#39;%n\u0026#39;; +------------+ | table name | +------------+ | person | +------------+ 1 row in set Shows all tables of the given database, which are not similar to the given sql pattern. show tables from db1 not like \u0026#39;%n\u0026#39;; -- show tables from catalog1.db1 not like \u0026#39;%n\u0026#39;; -- show tables in db1 not like \u0026#39;%n\u0026#39;; -- show tables in catalog1.db1 not like \u0026#39;%n\u0026#39;; +------------+ | table name | +------------+ | dim | +------------+ 1 row in set Shows all tables of the current database. show tables; +------------+ | table name | +------------+ | items | | orders | +------------+ 2 rows in set SHOW CREATE TABLE # SHOW CREATE TABLE Show create table statement for specified table.\nAttention Currently SHOW CREATE TABLE only supports table that is created by Flink SQL DDL.\nSHOW COLUMNS # SHOW COLUMNS ( FROM | IN ) [[catalog_name.]database.]\u0026lt;table_name\u0026gt; [ [NOT] LIKE \u0026lt;sql_like_pattern\u0026gt;] Show all columns of the table with given table name and optional like clause.\nLIKE Show all columns of the table with given table name and optional LIKE clause, whose name is whether similar to the \u0026lt;sql_like_pattern\u0026gt;.\nThe syntax of sql pattern in LIKE clause is the same as that of MySQL dialect.\nSHOW COLUMNS EXAMPLES # Assumes that the table named orders in the database1 database which is located in the catalog1 catalog has the following structure:\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ Shows all columns of the given table. show columns from orders; -- show columns from database1.orders; -- show columns from catalog1.database1.orders; -- show columns in orders; -- show columns in database1.orders; -- show columns in catalog1.database1.orders; +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ 5 rows in set Shows all columns of the given table, which are similar to the given sql pattern. show columns from orders like \u0026#39;%r\u0026#39;; -- show columns from database1.orders like \u0026#39;%r\u0026#39;; -- show columns from catalog1.database1.orders like \u0026#39;%r\u0026#39;; -- show columns in orders like \u0026#39;%r\u0026#39;; -- show columns in database1.orders like \u0026#39;%r\u0026#39;; -- show columns in catalog1.database1.orders like \u0026#39;%r\u0026#39;; +------+--------+-------+-----------+--------+-----------+ | name | type | null | key | extras | watermark | +------+--------+-------+-----------+--------+-----------+ | user | BIGINT | false | PRI(user) | | | +------+--------+-------+-----------+--------+-----------+ 1 row in set Shows all columns of the given table, which are not similar to the given sql pattern. show columns from orders not like \u0026#39;%_r\u0026#39;; -- show columns from database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns from catalog1.database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns in orders not like \u0026#39;%_r\u0026#39;; -- show columns in database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns in catalog1.database1.orders not like \u0026#39;%_r\u0026#39;; +---------+-----------------------------+-------+-----+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----+---------------+----------------------------+ | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----+---------------+----------------------------+ 4 rows in set SHOW VIEWS # SHOW VIEWS Show all views in the current catalog and the current database.\nSHOW CREATE VIEW # SHOW CREATE VIEW [catalog_name.][db_name.]view_name Show create view statement for specified view.\nSHOW FUNCTIONS # SHOW [USER] FUNCTIONS Show all functions including system functions and user-defined functions in the current catalog and current database.\nUSER Show only user-defined functions in the current catalog and current database.\nSHOW MODULES # SHOW [FULL] MODULES Show all enabled module names with resolution order.\nFULL Show all loaded modules and enabled status with resolution order.\nSHOW JARS # SHOW JARS Show all added jars in the session classloader which are added by ADD JAR statements.\nAttention Currently SHOW JARS only works in the SQL CLI.\nBack to top\n"}),e.add({id:242,href:"/flink/flink-docs-master/docs/dev/table/sql/load/",title:"LOAD Statements",section:"SQL",content:` LOAD Statements # LOAD statements are used to load a built-in or user-defined module.
Run a LOAD statement # Java LOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise, it will throw an exception.
The following examples show how to run a LOAD statement in TableEnvironment.
Scala LOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise, it will throw an exception.
The following examples show how to run a LOAD statement in TableEnvironment.
Python LOAD statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise, it will throw an exception.
The following examples show how to run a LOAD statement in TableEnvironment.
SQL CLI LOAD statements can be executed in SQL CLI.
The following examples show how to run a LOAD statement in SQL CLI.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // load a hive module tEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // load a hive module tEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ Python table_env = StreamTableEnvironment.create(...) # load a hive module table_env.execute_sql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ SQL CLI Flink SQL\u0026gt; LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;); [INFO] Load module succeeded! Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | | hive | +-------------+ Back to top
LOAD MODULE # The following grammar gives an overview of the available syntax:
LOAD MODULE module_name [WITH (\u0026#39;key1\u0026#39; = \u0026#39;val1\u0026#39;, \u0026#39;key2\u0026#39; = \u0026#39;val2\u0026#39;, ...)] module_name is a simple identifier. It is case-sensitive and should be identical to the module type defined in the module factory because it is used to perform module discovery. Properties ('key1' = 'val1', 'key2' = 'val2', ...) is a map that contains a set of key-value pairs (except for the key 'type') and passed to the discovery service to instantiate the corresponding module. `}),e.add({id:243,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/orderby/",title:"ORDER BY clause",section:"Queries",content:` ORDER BY clause # Batch Streaming
The ORDER BY clause causes the result rows to be sorted according to the specified expression(s). If two rows are equal according to the leftmost expression, they are compared according to the next expression and so on. If they are equal according to all specified expressions, they are returned in an implementation-dependent order.
When running in streaming mode, the primary sort order of a table must be ascending on a time attribute. All subsequent orders can be freely chosen. But there is no this limitation in batch mode.
SELECT * FROM Orders ORDER BY order_time, order_id Back to top
`}),e.add({id:244,href:"/flink/flink-docs-master/docs/ops/state/state_backends/",title:"State Backends",section:"State \u0026 Fault Tolerance",content:` State Backends # Programs written in the Data Stream API often hold state in various forms:
Windows gather elements or aggregates until they are triggered Transformation functions may use the key/value state interface to store values Transformation functions may implement the CheckpointedFunction interface to make their local variables fault tolerant See also state section in the streaming API guide.
When checkpointing is activated, such state is persisted upon checkpoints to guard against data loss and recover consistently. How the state is represented internally, and how and where it is persisted upon checkpoints depends on the chosen State Backend.
Available State Backends # Out of the box, Flink bundles these state backends:
HashMapStateBackend EmbeddedRocksDBStateBackend If nothing else is configured, the system will use the HashMapStateBackend.
The HashMapStateBackend # The HashMapStateBackend holds data internally as objects on the Java heap. Key/value state and window operators hold hash tables that store the values, triggers, etc.
The HashMapStateBackend is encouraged for:
Jobs with large state, long windows, large key/value states. All high-availability setups. It is also recommended to set managed memory to zero. This will ensure that the maximum amount of memory is allocated for user code on the JVM.
Unlike EmbeddedRocksDBStateBackend, the HashMapStateBackend stores data as objects on the heap so that it is unsafe to reuse objects.
The EmbeddedRocksDBStateBackend # The EmbeddedRocksDBStateBackend holds in-flight data in a RocksDB database that is (per default) stored in the TaskManager local data directories. Unlike storing java objects in HashMapStateBackend, data is stored as serialized byte arrays, which are mainly defined by the type serializer, resulting in key comparisons being byte-wise instead of using Java\u0026rsquo;s hashCode() and equals() methods.
The EmbeddedRocksDBStateBackend always performs asynchronous snapshots.
Limitations of the EmbeddedRocksDBStateBackend:
As RocksDB\u0026rsquo;s JNI bridge API is based on byte[], the maximum supported size per key and per value is 2^31 bytes each. States that use merge operations in RocksDB (e.g. ListState) can silently accumulate value sizes \u0026gt; 2^31 bytes and will then fail on their next retrieval. This is currently a limitation of RocksDB JNI. The EmbeddedRocksDBStateBackend is encouraged for:
Jobs with very large state, long windows, large key/value states. All high-availability setups. Note that the amount of state that you can keep is only limited by the amount of disk space available. This allows keeping very large state, compared to the HashMapStateBackend that keeps state in memory. This also means, however, that the maximum throughput that can be achieved will be lower with this state backend. All reads/writes from/to this backend have to go through de-/serialization to retrieve/store the state objects, which is also more expensive than always working with the on-heap representation as the heap-based backends are doing. It\u0026rsquo;s safe for EmbeddedRocksDBStateBackend to reuse objects due to the de-/serialization.
Check also recommendations about the task executor memory configuration for the EmbeddedRocksDBStateBackend.
EmbeddedRocksDBStateBackend is currently the only backend that offers incremental checkpoints (see here).
Certain RocksDB native metrics are available but disabled by default, you can find full documentation here
The total memory amount of RocksDB instance(s) per slot can also be bounded, please refer to documentation here for details.
Choose The Right State Backend # When deciding between HashMapStateBackend and RocksDB, it is a choice between performance and scalability. HashMapStateBackend is very fast as each state access and update operates on objects on the Java heap; however, state size is limited by available memory within the cluster. On the other hand, RocksDB can scale based on available disk space and is the only state backend to support incremental snapshots. However, each state access and update requires (de-)serialization and potentially reading from disk which leads to average performance that is an order of magnitude slower than the memory state backends.
In Flink 1.13 we unified the binary format of Flink\u0026rsquo;s savepoints. That means you can take a savepoint and then restore from it using a different state backend. All the state backends produce a common format only starting from version 1.13. Therefore, if you want to switch the state backend you should first upgrade your Flink version then take a savepoint with the new version, and only after that you can restore it with a different state backend. Configuring a State Backend # The default state backend, if you specify nothing, is the jobmanager. If you wish to establish a different default for all jobs on your cluster, you can do so by defining a new default state backend in flink-conf.yaml. The default state backend can be overridden on a per-job basis, as shown below.
Setting the Per-job State Backend # The per-job state backend is set on the StreamExecutionEnvironment of the job, as shown in the example below:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(new HashMapStateBackend()) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) If you want to use the EmbeddedRocksDBStateBackend in your IDE or configure it programmatically in your Flink job, you will have to add the following dependency to your Flink project.
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-statebackend-rocksdb\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Since RocksDB is part of the default Flink distribution, you do not need this dependency if you are not using any RocksDB code in your job and configure the state backend via state.backend and further checkpointing and RocksDB-specific parameters in your flink-conf.yaml. Setting Default State Backend # A default state backend can be configured in the flink-conf.yaml, using the configuration key state.backend.
Possible values for the config entry are hashmap (HashMapStateBackend), rocksdb (EmbeddedRocksDBStateBackend), or the fully qualified class name of the class that implements the state backend factory StateBackendFactory , such as org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendFactory for EmbeddedRocksDBStateBackend.
The state.checkpoints.dir option defines the directory to which all backends write checkpoint data and meta data files. You can find more details about the checkpoint directory structure here.
A sample section in the configuration file could look as follows:
# The backend that will be used to store operator state checkpoints state.backend: hashmap # Directory for storing checkpoints state.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints RocksDB State Backend Details # This section describes the RocksDB state backend in more detail.
Incremental Checkpoints # RocksDB supports Incremental Checkpoints, which can dramatically reduce the checkpointing time in comparison to full checkpoints. Instead of producing a full, self-contained backup of the state backend, incremental checkpoints only record the changes that happened since the latest completed checkpoint.
An incremental checkpoint builds upon (typically multiple) previous checkpoints. Flink leverages RocksDB\u0026rsquo;s internal compaction mechanism in a way that is self-consolidating over time. As a result, the incremental checkpoint history in Flink does not grow indefinitely, and old checkpoints are eventually subsumed and pruned automatically.
Recovery time of incremental checkpoints may be longer or shorter compared to full checkpoints. If your network bandwidth is the bottleneck, it may take a bit longer to restore from an incremental checkpoint, because it implies fetching more data (more deltas). Restoring from an incremental checkpoint is faster, if the bottleneck is your CPU or IOPs, because restoring from an incremental checkpoint means not re-building the local RocksDB tables from Flink\u0026rsquo;s canonical key/value snapshot format (used in savepoints and full checkpoints).
While we encourage the use of incremental checkpoints for large state, you need to enable this feature manually:
Setting a default in your flink-conf.yaml: state.backend.incremental: true will enable incremental checkpoints, unless the application overrides this setting in the code. You can alternatively configure this directly in the code (overrides the config default): EmbeddedRocksDBStateBackend backend = new EmbeddedRocksDBStateBackend(true); Notice that once incremental checkpoont is enabled, the Checkpointed Data Size showed in web UI only represents the delta checkpointed data size of that checkpoint instead of full state size.
Memory Management # Flink aims to control the total process memory consumption to make sure that the Flink TaskManagers have a well-behaved memory footprint. That means staying within the limits enforced by the environment (Docker/Kubernetes, Yarn, etc) to not get killed for consuming too much memory, but also to not under-utilize memory (unnecessary spilling to disk, wasted caching opportunities, reduced performance).
To achieve that, Flink by default configures RocksDB\u0026rsquo;s memory allocation to the amount of managed memory of the TaskManager (or, more precisely, task slot). This should give good out-of-the-box experience for most applications, meaning most applications should not need to tune any of the detailed RocksDB settings. The primary mechanism for improving memory-related performance issues would be to simply increase Flink\u0026rsquo;s managed memory.
Users can choose to deactivate that feature and let RocksDB allocate memory independently per ColumnFamily (one per state per operator). This offers expert users ultimately more fine grained control over RocksDB, but means that users need to take care themselves that the overall memory consumption does not exceed the limits of the environment. See large state tuning for some guideline about large state performance tuning.
Managed Memory for RocksDB
This feature is active by default and can be (de)activated via the state.backend.rocksdb.memory.managed configuration key.
Flink does not directly manage RocksDB\u0026rsquo;s native memory allocations, but configures RocksDB in a certain way to ensure it uses exactly as much memory as Flink has for its managed memory budget. This is done on a per-slot level (managed memory is accounted per slot).
To set the total memory usage of RocksDB instance(s), Flink leverages a shared cache and write buffer manager among all instances in a single slot. The shared cache will place an upper limit on the three components that use the majority of memory in RocksDB: block cache, index and bloom filters, and MemTables.
For advanced tuning, Flink also provides two parameters to control the division of memory between the write path (MemTable) and read path (index \u0026amp; filters, remaining cache). When you see that RocksDB performs badly due to lack of write buffer memory (frequent flushes) or cache misses, you can use these parameters to redistribute the memory.
state.backend.rocksdb.memory.write-buffer-ratio, by default 0.5, which means 50% of the given memory would be used by write buffer manager. state.backend.rocksdb.memory.high-prio-pool-ratio, by default 0.1, which means 10% of the given memory would be set as high priority for index and filters in shared block cache. We strongly suggest not to set this to zero, to prevent index and filters from competing against data blocks for staying in cache and causing performance issues. Moreover, the L0 level filter and index are pinned into the cache by default to mitigate performance problems, more details please refer to the RocksDB-documentation. When the above described mechanism (cache and write buffer manager) is enabled, it will override any customized settings for block caches and write buffers done via PredefinedOptions and RocksDBOptionsFactory. Expert Mode To control memory manually, you can set state.backend.rocksdb.memory.managed to false and configure RocksDB via ColumnFamilyOptions. Alternatively, you can use the above mentioned cache/buffer-manager mechanism, but set the memory size to a fixed amount independent of Flink\u0026rsquo;s managed memory size (state.backend.rocksdb.memory.fixed-per-slot option). Note that in both cases, users need to ensure on their own that enough memory is available outside the JVM for RocksDB. Timers (Heap vs. RocksDB) # Timers are used to schedule actions for later (event-time or processing-time), such as firing a window, or calling back a ProcessFunction.
When selecting the RocksDB State Backend, timers are by default also stored in RocksDB. That is a robust and scalable way that lets applications scale to many timers. However, maintaining timers in RocksDB can have a certain cost, which is why Flink provides the option to store timers on the JVM heap instead, even when RocksDB is used to store other states. Heap-based timers can have a better performance when there is a smaller number of timers.
Set the configuration option state.backend.rocksdb.timer-service.factory to heap (rather than the default, rocksdb) to store timers on heap.
The combination RocksDB state backend with heap-based timers currently does NOT support asynchronous snapshots for the timers state. Other state like keyed state is still snapshotted asynchronously. When using RocksDB state backend with heap-based timers, checkpointing and taking savepoints is expected to fail if there are operators in application that write to raw keyed state. This is only relevant to advanced users who are writing custom stream operators. Enabling RocksDB Native Metrics # You can optionally access RockDB\u0026rsquo;s native metrics through Flink\u0026rsquo;s metrics system, by enabling certain metrics selectively. See configuration docs for details.
Enabling RocksDB\u0026rsquo;s native metrics may have a negative performance impact on your application. Advanced RocksDB Memory Turning # Flink offers sophisticated default memory management for RocksDB that should work for most use-cases. The below mechanisms should mainly be used for expert tuning or trouble shooting. Predefined Per-ColumnFamily Options # With Predefined Options, users can apply some predefined config profiles on each RocksDB Column Family, configuring for example memory use, thread, compaction settings, etc. There is currently one Column Family per each state in each operator.
There are two ways to select predefined options to be applied:
Set the option\u0026rsquo;s name in flink-conf.yaml via state.backend.rocksdb.predefined-options. Set the predefined options programmatically: EmbeddedRocksDBStateBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED_HIGH_MEM). The default value for this option is DEFAULT which translates to PredefinedOptions.DEFAULT.
Predefined options set programmatically would override the ones configured via flink-conf.yaml.
Reading Column Family Options from flink-conf.yaml # RocksDB State Backend picks up all config options defined here. Hence, you can configure low-level Column Family options simply by turning off managed memory for RocksDB and putting the relevant entries in the configuration.
Passing Options Factory to RocksDB # To manually control RocksDB\u0026rsquo;s options, you need to configure an RocksDBOptionsFactory. This mechanism gives you fine-grained control over the settings of the Column Families, for example memory use, thread, compaction settings, etc. There is currently one Column Family per each state in each operator.
There are two ways to pass a RocksDBOptionsFactory to the RocksDB State Backend:
Configure options factory class name in the flink-conf.yaml via state.backend.rocksdb.options-factory.
Set the options factory programmatically, e.g. EmbeddedRocksDBStateBackend.setRocksDBOptions(new MyOptionsFactory());
Options factory which set programmatically would override the one configured via flink-conf.yaml, and options factory has a higher priority over the predefined options if ever configured or set.
RocksDB is a native library that allocates memory directly from the process, and not from the JVM. Any memory you assign to RocksDB will have to be accounted for, typically by decreasing the JVM heap size of the TaskManagers by the same amount. Not doing that may result in YARN/etc terminating the JVM processes for allocating more memory than configured.
Below is an example how to define a custom ConfigurableOptionsFactory (set class name under state.backend.rocksdb.options-factory).
Java public class MyOptionsFactory implements ConfigurableRocksDBOptionsFactory { public static final ConfigOption\u0026lt;Integer\u0026gt; BLOCK_RESTART_INTERVAL = ConfigOptions .key(\u0026#34;my.custom.rocksdb.block.restart-interval\u0026#34;) .intType() .defaultValue(16) .withDescription( \u0026#34; Block restart interval. RocksDB has default block restart interval as 16. \u0026#34;); private int blockRestartInterval = BLOCK_RESTART_INTERVAL.defaultValue(); @Override public DBOptions createDBOptions(DBOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { return currentOptions .setIncreaseParallelism(4) .setUseFsync(false); } @Override public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { return currentOptions.setTableFormatConfig( new BlockBasedTableConfig() .setBlockRestartInterval(blockRestartInterval)); } @Override public RocksDBOptionsFactory configure(ReadableConfig configuration) { this.blockRestartInterval = configuration.get(BLOCK_RESTART_INTERVAL); return this; } } Python Still not supported in Python API. Back to top
Enabling Changelog # This feature is in experimental status. Enabling Changelog may have a negative performance impact on your application (see below). Introduction # Changelog is a feature that aims to decrease checkpointing time and, therefore, end-to-end latency in exactly-once mode.
Most commonly, checkpoint duration is affected by:
Barrier travel time and alignment, addressed by Unaligned checkpoints and Buffer debloating Snapshot creation time (so-called synchronous phase), addressed by asynchronous snapshots (mentioned above) Snapshot upload time (asynchronous phase) Upload time can be decreased by incremental checkpoints. However, most incremental state backends perform some form of compaction periodically, which results in re-uploading the old state in addition to the new changes. In large deployments, the probability of at least one task uploading lots of data tends to be very high in every checkpoint.
With Changelog enabled, Flink uploads state changes continuously and forms a changelog. On checkpoint, only the relevant part of this changelog needs to be uploaded. The configured state backend is snapshotted in the background periodically. Upon successful upload, the changelog is truncated.
As a result, asynchronous phase duration is reduced, as well as synchronous phase - because no data needs to be flushed to disk. In particular, long-tail latency is improved.
However, resource usage is higher:
more files are created on DFS more files can be left undeleted DFS (this will be addressed in the future versions in FLINK-25511 and FLINK-25512) more IO bandwidth is used to upload state changes more CPU used to serialize state changes more memory used by Task Managers to buffer state changes Recovery time is another thing to consider. Depending on the state.backend.changelog.periodic-materialize.interval setting, the changelog can become lengthy and replaying it may take more time. However, recovery time combined with checkpoint duration will likely still be lower than in non-changelog setups, providing lower end-to-end latency even in failover case. However, it\u0026rsquo;s also possible that the effective recovery time will increase, depending on the actual ratio of the aforementioned times.
For more details, see FLIP-158.
Installation # Changelog JARs are included into the standard Flink distribution.
Make sure to add the necessary filesystem plugins.
Configuration # Here is an example configuration in YAML:
state.backend.changelog.enabled: true state.backend.changelog.storage: filesystem # currently, only filesystem and memory (for tests) are supported dstl.dfs.base-path: s3://\u0026lt;bucket-name\u0026gt; # similar to state.checkpoints.dir Please keep the following defaults (see limitations):
execution.checkpointing.max-concurrent-checkpoints: 1 Please refer to the configuration section for other options.
Changelog can also be enabled or disabled per job programmatically: Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableChangelogStateBackend(true); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableChangelogStateBackend(true) Python env = StreamExecutionEnvironment.get_execution_environment() env.enable_changelog_statebackend(true) Monitoring # Available metrics are listed here.
If a task is backpressured by writing state changes, it will be shown as busy (red) in the UI.
Upgrading existing jobs # Enabling Changelog
Resuming from both savepoints and checkpoints is supported:
given an existing non-changelog job take either a savepoint or a checkpoint alter configuration (enable Changelog) resume from the taken snapshot Disabling Changelog
Resuming from both savepoints and checkpoints is supported:
given an existing changelog job take either a savepoint or a checkpoint alter configuration (disable Changelog) resume from the taken snapshot Limitations # At most one concurrent checkpoint As of Flink 1.15, only filesystem changelog implementation is available NO_CLAIM mode not supported Migrating from Legacy Backends # Beginning in Flink 1.13, the community reworked its public state backend classes to help users better understand the separation of local state storage and checkpoint storage. This change does not affect the runtime implementation or characteristics of Flink\u0026rsquo;s state backend or checkpointing process; it is simply to communicate intent better. Users can migrate existing applications to use the new API without losing any state or consistency.
MemoryStateBackend # The legacy MemoryStateBackend is equivalent to using HashMapStateBackend and JobManagerCheckpointStorage.
flink-conf.yaml configuration # state.backend: hashmap # Optional, Flink will automatically default to JobManagerCheckpointStorage # when no checkpoint directory is specified. state.checkpoint-storage: jobmanager Code Configuration # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new HashMapStateBackend) env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) env.get_checkpoint_config().set_checkpoint_storage(JobManagerCheckpointStorage()) FsStateBackend # The legacy FsStateBackend is equivalent to using HashMapStateBackend and FileSystemCheckpointStorage.
flink-conf.yaml configuration # state.backend: hashmap state.checkpoints.dir: file:///checkpoint-dir/ # Optional, Flink will automatically default to FileSystemCheckpointStorage # when a checkpoint directory is specified. state.checkpoint-storage: filesystem Code Configuration # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;); // Advanced FsStateBackend configurations, such as write buffer size // can be set by manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new HashMapStateBackend) env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;) // Advanced FsStateBackend configurations, such as write buffer size // can be set by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) env.get_checkpoint_config().set_checkpoint_storage_dir(\u0026#34;file:///checkpoint-dir\u0026#34;) # Advanced FsStateBackend configurations, such as write buffer size # can be set by manually instantiating a FileSystemCheckpointStorage object. env.get_checkpoint_config().set_checkpoint_storage(FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) RocksDBStateBackend # The legacy RocksDBStateBackend is equivalent to using EmbeddedRocksDBStateBackend and FileSystemCheckpointStorage.
flink-conf.yaml configuration # state.backend: rocksdb state.checkpoints.dir: file:///checkpoint-dir/ # Optional, Flink will automatically default to FileSystemCheckpointStorage # when a checkpoint directory is specified. state.checkpoint-storage: filesystem Code Configuration # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new EmbeddedRocksDBStateBackend()); env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;); // If you manually passed FsStateBackend into the RocksDBStateBackend constructor // to specify advanced checkpointing configurations such as write buffer size, // you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new EmbeddedRocksDBStateBackend) env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;) // If you manually passed FsStateBackend into the RocksDBStateBackend constructor // to specify advanced checkpointing configurations such as write buffer size, // you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(EmbeddedRocksDBStateBackend()) env.get_checkpoint_config().set_checkpoint_storage_dir(\u0026#34;file:///checkpoint-dir\u0026#34;) # If you manually passed FsStateBackend into the RocksDBStateBackend constructor # to specify advanced checkpointing configurations such as write buffer size, # you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.get_checkpoint_config().set_checkpoint_storage(FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) `}),e.add({id:245,href:"/flink/flink-docs-master/docs/dev/dataset/cluster_execution/",title:"Cluster Execution",section:"DataSet API (Legacy)",content:` Cluster Execution # Flink programs can run distributed on clusters of many machines. There are two ways to send a program to a cluster for execution:
Command Line Interface # The command line interface lets you submit packaged programs (JARs) to a cluster (or single machine setup).
Please refer to the Command Line Interface documentation for details.
Remote Environment # The remote environment lets you execute Flink Java programs on a cluster directly. The remote environment points to the cluster on which you want to execute the program.
Maven Dependency # If you are developing your program as a Maven project, you have to add the flink-clients module using this dependency:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Example # The following illustrates the use of the RemoteEnvironment:
public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment .createRemoteEnvironment(\u0026#34;flink-jobmanager\u0026#34;, 8081, \u0026#34;/home/user/udfs.jar\u0026#34;); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;hdfs://path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;hdfs://path/to/result\u0026#34;); env.execute(); } Note that the program contains custom user code and hence requires a JAR file with the classes of the code attached. The constructor of the remote environment takes the path(s) to the JAR file(s).
Back to top
`}),e.add({id:246,href:"/flink/flink-docs-master/docs/connectors/table/datagen/",title:"DataGen",section:"Table API Connectors",content:` DataGen SQL Connector # Scan Source: Bounded Scan Source: UnBounded
The DataGen connector allows for creating tables based on in-memory data generation. This is useful when developing queries locally without access to external systems such as Kafka. Tables can include Computed Column syntax which allows for flexible record generation.
The DataGen connector is built-in, no additional dependencies are required.
Usage # By default, a DataGen table will create an unbounded number of rows with a random value for each column. For variable sized types, char/varchar/binary/varbinary/string/array/map/multiset, the length can be specified. Additionally, a total number of rows can be specified, resulting in a bounded table.
There also exists a sequence generator, where users specify a sequence of start and end values. If any column in a table is a sequence type, the table will be bounded and end with the first sequence completes.
Time types are always the local machines current system time.
CREATE TABLE Orders ( order_number BIGINT, price DECIMAL(32,2), buyer ROW\u0026lt;first_name STRING, last_name STRING\u0026gt;, order_time TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39; ) Often, the data generator connector is used in conjunction with the LIKE clause to mock out physical tables.
CREATE TABLE Orders ( order_number BIGINT, price DECIMAL(32,2), buyer ROW\u0026lt;first_name STRING, last_name STRING\u0026gt;, order_time TIMESTAMP(3) ) WITH (...) -- create a bounded mock table CREATE TEMPORARY TABLE GenOrders WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;number-of-rows\u0026#39; = \u0026#39;10\u0026#39; ) LIKE Orders (EXCLUDING ALL) Types # Type Supported Generators Notes BOOLEAN random CHAR random / sequence VARCHAR random / sequence BINARY random / sequence VARBINARY random / sequence STRING random / sequence DECIMAL random / sequence TINYINT random / sequence SMALLINT random / sequence INT random / sequence BIGINT random / sequence FLOAT random / sequence DOUBLE random / sequence DATE random Always resolves to the current date of the local machine. TIME random Always resolves to the current time of the local machine. TIMESTAMP random Resolves a past timestamp relative to the current timestamp of the local machine. The max past can be specified by the 'max-past' option. TIMESTAMP_LTZ random Resolves a past timestamp relative to the current timestamp of the local machine. The max past can be specified by the 'max-past' option. INTERVAL YEAR TO MONTH random INTERVAL DAY TO MONTH random ROW random Generates a row with random subfields. ARRAY random Generates an array with random entries. MAP random Generates a map with random entries. MULTISET random Generates a multiset with random entries. Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'datagen'. rows-per-second optional 10000 Long Rows per second to control the emit rate. number-of-rows optional (none) Long The total number of rows to emit. By default, the table is unbounded. fields.#.kind optional random String Generator of this '#' field. Can be 'sequence' or 'random'. fields.#.min optional (Minimum value of type) (Type of field) Minimum value of random generator, work for numeric types. fields.#.max optional (Maximum value of type) (Type of field) Maximum value of random generator, work for numeric types. fields.#.max-past optional 0 Duration Maximum past of timestamp random generator, only works for timestamp types. fields.#.length optional 100 Integer Size or length of the collection for generating char/varchar/binary/varbinary/string/array/map/multiset types. fields.#.start optional (none) (Type of field) Start value of sequence generator. fields.#.end optional (none) (Type of field) End value of sequence generator. `}),e.add({id:247,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/limit/",title:"LIMIT clause",section:"Queries",content:` LIMIT clause # Batch LIMIT clause constrains the number of rows returned by the SELECT statement. In general, this clause is used in conjunction with ORDER BY to ensure that the results are deterministic.
The following example selects the first 3 rows in Orders table.
SELECT * FROM Orders ORDER BY orderTime LIMIT 3 Back to top
`}),e.add({id:248,href:"/flink/flink-docs-master/docs/ops/state/large_state_tuning/",title:"Tuning Checkpoints and Large State",section:"State \u0026 Fault Tolerance",content:` Tuning Checkpoints and Large State # This page gives a guide how to configure and tune applications that use large state.
Overview # For Flink applications to run reliably at large scale, two conditions must be fulfilled:
The application needs to be able to take checkpoints reliably
The resources need to be sufficient catch up with the input data streams after a failure
The first sections discuss how to get well performing checkpoints at scale. The last section explains some best practices concerning planning how many resources to use.
Monitoring State and Checkpoints # The easiest way to monitor checkpoint behavior is via the UI\u0026rsquo;s checkpoint section. The documentation for checkpoint monitoring shows how to access the available checkpoint metrics.
The two numbers (both exposed via Task level metrics and in the web interface) that are of particular interest when scaling up checkpoints are:
The time until operators receive their first checkpoint barrier When the time to trigger the checkpoint is constantly very high, it means that the checkpoint barriers need a long time to travel from the source to the operators. That typically indicates that the system is operating under a constant backpressure.
The alignment duration, which is defined as the time between receiving first and the last checkpoint barrier. During unaligned exactly-once checkpoints and at-least-once checkpoints subtasks are processing all of the data from the upstream subtasks without any interruptions. However with aligned exactly-once checkpoints, the channels that have already received a checkpoint barrier are blocked from sending further data until all of the remaining channels catch up and receive theirs checkpoint barriers (alignment time).
Both of those values should ideally be low - higher amounts means that checkpoint barriers traveling through the job graph slowly, due to some back-pressure (not enough resources to process the incoming records). This can also be observed via increased end-to-end latency of processed records. Note that those numbers can be occasionally high in the presence of a transient backpressure, data skew, or network issues.
Unaligned checkpoints can be used to speed up the propagation time of the checkpoint barriers. However please note, that this does not solve the underlying problem that\u0026rsquo;s causing the backpressure in the first place (and end-to-end records latency will remain high).
Tuning Checkpointing # Checkpoints are triggered at regular intervals that applications can configure. When a checkpoint takes longer to complete than the checkpoint interval, the next checkpoint is not triggered before the in-progress checkpoint completes. By default the next checkpoint will then be triggered immediately once the ongoing checkpoint completes.
When checkpoints end up frequently taking longer than the base interval (for example because state grew larger than planned, or the storage where checkpoints are stored is temporarily slow), the system is constantly taking checkpoints (new ones are started immediately once ongoing once finish). That can mean that too many resources are constantly tied up in checkpointing and that the operators make too little progress. This behavior has less impact on streaming applications that use asynchronously checkpointed state, but may still have an impact on overall application performance.
To prevent such a situation, applications can define a minimum duration between checkpoints:
StreamExecutionEnvironment.getCheckpointConfig().setMinPauseBetweenCheckpoints(milliseconds)
This duration is the minimum time interval that must pass between the end of the latest checkpoint and the beginning of the next. The figure below illustrates how this impacts checkpointing.
Note: Applications can be configured (via the CheckpointConfig) to allow multiple checkpoints to be in progress at the same time. For applications with large state in Flink, this often ties up too many resources into the checkpointing. When a savepoint is manually triggered, it may be in process concurrently with an ongoing checkpoint.
Tuning RocksDB # The state storage workhorse of many large scale Flink streaming applications is the RocksDB State Backend. The backend scales well beyond main memory and reliably stores large keyed state.
RocksDB\u0026rsquo;s performance can vary with configuration, this section outlines some best-practices for tuning jobs that use the RocksDB State Backend.
Incremental Checkpoints # When it comes to reducing the time that checkpoints take, activating incremental checkpoints should be one of the first considerations. Incremental checkpoints can dramatically reduce the checkpointing time in comparison to full checkpoints, because incremental checkpoints only record the changes compared to the previous completed checkpoint, instead of producing a full, self-contained backup of the state backend.
See Incremental Checkpoints in RocksDB for more background information.
Timers in RocksDB or on JVM Heap # Timers are stored in RocksDB by default, which is the more robust and scalable choice.
When performance-tuning jobs that have few timers only (no windows, not using timers in ProcessFunction), putting those timers on the heap can increase performance. Use this feature carefully, as heap-based timers may increase checkpointing times and naturally cannot scale beyond memory.
See this section for details on how to configure heap-based timers.
Tuning RocksDB Memory # The performance of the RocksDB State Backend much depends on the amount of memory that it has available. To increase performance, adding memory can help a lot, or adjusting to which functions memory goes.
By default, the RocksDB State Backend uses Flink\u0026rsquo;s managed memory budget for RocksDBs buffers and caches (state.backend.rocksdb.memory.managed: true). Please refer to the RocksDB Memory Management for background on how that mechanism works.
To tune memory-related performance issues, the following steps may be helpful:
The first step to try and increase performance should be to increase the amount of managed memory. This usually improves the situation a lot, without opening up the complexity of tuning low-level RocksDB options.
Especially with large container/process sizes, much of the total memory can typically go to RocksDB, unless the application logic requires a lot of JVM heap itself. The default managed memory fraction (0.4) is conservative and can often be increased when using TaskManagers with multi-GB process sizes.
The number of write buffers in RocksDB depends on the number of states you have in your application (states across all operators in the pipeline). Each state corresponds to one ColumnFamily, which needs its own write buffers. Hence, applications with many states typically need more memory for the same performance.
You can try and compare the performance of RocksDB with managed memory to RocksDB with per-column-family memory by setting state.backend.rocksdb.memory.managed: false. Especially to test against a baseline (assuming no- or gracious container memory limits) or to test for regressions compared to earlier versions of Flink, this can be useful.
Compared to the managed memory setup (constant memory pool), not using managed memory means that RocksDB allocates memory proportional to the number of states in the application (memory footprint changes with application changes). As a rule of thumb, the non-managed mode has (unless ColumnFamily options are applied) an upper bound of roughly \u0026ldquo;140MB * num-states-across-all-tasks * num-slots\u0026rdquo;. Timers count as state as well!
If your application has many states and you see frequent MemTable flushes (write-side bottleneck), but you cannot give more memory you can increase the ratio of memory going to the write buffers (state.backend.rocksdb.memory.write-buffer-ratio). See RocksDB Memory Management for details.
An advanced option (expert mode) to reduce the number of MemTable flushes in setups with many states, is to tune RocksDB\u0026rsquo;s ColumnFamily options (arena block size, max background flush threads, etc.) via a RocksDBOptionsFactory:
public class MyOptionsFactory implements ConfigurableRocksDBOptionsFactory { @Override public DBOptions createDBOptions(DBOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { // increase the max background flush threads when we have many states in one operator, // which means we would have many column families in one DB instance. return currentOptions.setMaxBackgroundFlushes(4); } @Override public ColumnFamilyOptions createColumnOptions( ColumnFamilyOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { // decrease the arena block size from default 8MB to 1MB. return currentOptions.setArenaBlockSize(1024 * 1024); } @Override public OptionsFactory configure(ReadableConfig configuration) { return this; } } Capacity Planning # This section discusses how to decide how many resources should be used for a Flink job to run reliably. The basic rules of thumb for capacity planning are:
Normal operation should have enough capacity to not operate under constant back pressure. See back pressure monitoring for details on how to check whether the application runs under back pressure.
Provision some extra resources on top of the resources needed to run the program back-pressure-free during failure-free time. These resources are needed to \u0026ldquo;catch up\u0026rdquo; with the input data that accumulated during the time the application was recovering. How much that should be depends on how long recovery operations usually take (which depends on the size of the state that needs to be loaded into the new TaskManagers on a failover) and how fast the scenario requires failures to recover.
Important: The base line should to be established with checkpointing activated, because checkpointing ties up some amount of resources (such as network bandwidth).
Temporary back pressure is usually okay, and an essential part of execution flow control during load spikes, during catch-up phases, or when external systems (that are written to in a sink) exhibit temporary slowdown.
Certain operations (like large windows) result in a spiky load for their downstream operators: In the case of windows, the downstream operators may have little to do while the window is being built, and have a load to do when the windows are emitted. The planning for the downstream parallelism needs to take into account how much the windows emit and how fast such a spike needs to be processed.
Important: In order to allow for adding resources later, make sure to set the maximum parallelism of the data stream program to a reasonable number. The maximum parallelism defines how high you can set the programs parallelism when re-scaling the program (via a savepoint).
Flink\u0026rsquo;s internal bookkeeping tracks parallel state in the granularity of max-parallelism-many key groups. Flink\u0026rsquo;s design strives to make it efficient to have a very high value for the maximum parallelism, even if executing the program with a low parallelism.
Compression # Flink offers optional compression (default: off) for all checkpoints and savepoints. Currently, compression always uses the snappy compression algorithm (version 1.1.4) but we are planning to support custom compression algorithms in the future. Compression works on the granularity of key-groups in keyed state, i.e. each key-group can be decompressed individually, which is important for rescaling.
Compression can be activated through the ExecutionConfig:
ExecutionConfig executionConfig = new ExecutionConfig(); executionConfig.setUseSnapshotCompression(true); Note The compression option has no impact on incremental snapshots, because they are using RocksDB\u0026rsquo;s internal format which is always using snappy compression out of the box.
Task-Local Recovery # Motivation # In Flink\u0026rsquo;s checkpointing, each task produces a snapshot of its state that is then written to a distributed store. Each task acknowledges a successful write of the state to the job manager by sending a handle that describes the location of the state in the distributed store. The job manager, in turn, collects the handles from all tasks and bundles them into a checkpoint object.
In case of recovery, the job manager opens the latest checkpoint object and sends the handles back to the corresponding tasks, which can then restore their state from the distributed storage. Using a distributed storage to store state has two important advantages. First, the storage is fault tolerant and second, all state in the distributed store is accessible to all nodes and can be easily redistributed (e.g. for rescaling).
However, using a remote distributed store has also one big disadvantage: all tasks must read their state from a remote location, over the network. In many scenarios, recovery could reschedule failed tasks to the same task manager as in the previous run (of course there are exceptions like machine failures), but we still have to read remote state. This can result in long recovery time for large states, even if there was only a small failure on a single machine.
Approach # Task-local state recovery targets exactly this problem of long recovery time and the main idea is the following: for every checkpoint, each task does not only write task states to the distributed storage, but also keep a secondary copy of the state snapshot in a storage that is local to the task (e.g. on local disk or in memory). Notice that the primary store for snapshots must still be the distributed store, because local storage does not ensure durability under node failures and also does not provide access for other nodes to redistribute state, this functionality still requires the primary copy.
However, for each task that can be rescheduled to the previous location for recovery, we can restore state from the secondary, local copy and avoid the costs of reading the state remotely. Given that many failures are not node failures and node failures typically only affect one or very few nodes at a time, it is very likely that in a recovery most tasks can return to their previous location and find their local state intact. This is what makes local recovery effective in reducing recovery time.
Please note that this can come at some additional costs per checkpoint for creating and storing the secondary local state copy, depending on the chosen state backend and checkpointing strategy. For example, in most cases the implementation will simply duplicate the writes to the distributed store to a local file.
Relationship of primary (distributed store) and secondary (task-local) state snapshots # Task-local state is always considered a secondary copy, the ground truth of the checkpoint state is the primary copy in the distributed store. This has implications for problems with local state during checkpointing and recovery:
For checkpointing, the primary copy must be successful and a failure to produce the secondary, local copy will not fail the checkpoint. A checkpoint will fail if the primary copy could not be created, even if the secondary copy was successfully created.
Only the primary copy is acknowledged and managed by the job manager, secondary copies are owned by task managers and their life cycles can be independent from their primary copies. For example, it is possible to retain a history of the 3 latest checkpoints as primary copies and only keep the task-local state of the latest checkpoint.
For recovery, Flink will always attempt to restore from task-local state first, if a matching secondary copy is available. If any problem occurs during the recovery from the secondary copy, Flink will transparently retry to recover the task from the primary copy. Recovery only fails, if primary and the (optional) secondary copy failed. In this case, depending on the configuration Flink could still fall back to an older checkpoint.
It is possible that the task-local copy contains only parts of the full task state (e.g. exception while writing one local file). In this case, Flink will first try to recover local parts locally, non-local state is restored from the primary copy. Primary state must always be complete and is a superset of the task-local state.
Task-local state can have a different format than the primary state, they are not required to be byte identical. For example, it could be even possible that the task-local state is an in-memory consisting of heap objects, and not stored in any files.
If a task manager is lost, the local state from all its task is lost.
Configuring task-local recovery # Task-local recovery is deactivated by default and can be activated through Flink\u0026rsquo;s configuration with the key state.backend.local-recovery as specified in CheckpointingOptions.LOCAL_RECOVERY. The value for this setting can either be true to enable or false (default) to disable local recovery.
Note that unaligned checkpoints currently do not support task-local recovery.
Details on task-local recovery for different state backends # Limitation: Currently, task-local recovery only covers keyed state backends. Keyed state is typically by far the largest part of the state. In the near future, we will also cover operator state and timers.
The following state backends can support task-local recovery.
HashMapStateBackend: task-local recovery is supported for keyed state. The implementation will duplicate the state to a local file. This can introduce additional write costs and occupy local disk space. In the future, we might also offer an implementation that keeps task-local state in memory.
EmbeddedRocksDBStateBackend: task-local recovery is supported for keyed state. For full checkpoints, state is duplicated to a local file. This can introduce additional write costs and occupy local disk space. For incremental snapshots, the local state is based on RocksDB\u0026rsquo;s native checkpointing mechanism. This mechanism is also used as the first step to create the primary copy, which means that in this case no additional cost is introduced for creating the secondary copy. We simply keep the native checkpoint directory around instead of deleting it after uploading to the distributed store. This local copy can share active files with the working directory of RocksDB (via hard links), so for active files also no additional disk space is consumed for task-local recovery with incremental snapshots. Using hard links also means that the RocksDB directories must be on the same physical device as all the configure local recovery directories that can be used to store local state, or else establishing hard links can fail (see FLINK-10954). Currently, this also prevents using local recovery when RocksDB directories are configured to be located on more than one physical device.
Allocation-preserving scheduling # Task-local recovery assumes allocation-preserving task scheduling under failures, which works as follows. Each task remembers its previous allocation and requests the exact same slot to restart in recovery. If this slot is not available, the task will request a new, fresh slot from the resource manager. This way, if a task manager is no longer available, a task that cannot return to its previous location will not drive other recovering tasks out of their previous slots. Our reasoning is that the previous slot can only disappear when a task manager is no longer available, and in this case some tasks have to request a new slot anyways. With our scheduling strategy we give the maximum number of tasks a chance to recover from their local state and avoid the cascading effect of tasks stealing their previous slots from one another.
Back to top
`}),e.add({id:249,href:"/flink/flink-docs-master/docs/dev/table/sql/unload/",title:"UNLOAD Statements",section:"SQL",content:` UNLOAD Statements # UNLOAD statements are used to unload a built-in or user-defined module.
Run a UNLOAD statement # Java UNLOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise it will throw an exception.
The following examples show how to run a UNLOAD statement in TableEnvironment.
Scala UNLOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise it will throw an exception.
The following examples show how to run a UNLOAD statement in TableEnvironment.
Python UNLOAD statements can be executed with the execute_sql() method of the TableEnvironment. The execute_sql() method returns \u0026lsquo;OK\u0026rsquo; for a successful LOAD operation; otherwise it will throw an exception.
The following examples show how to run a UNLOAD statement in TableEnvironment.
SQL CLI UNLOAD statements can be executed in SQL CLI.
The following examples show how to run a UNLOAD statement in SQL CLI.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // unload a core module tEnv.executeSql(\u0026#34;UNLOAD MODULE core\u0026#34;); tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // Empty set Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // unload a core module tEnv.executeSql(\u0026#34;UNLOAD MODULE core\u0026#34;) tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // Empty set Python table_env = StreamTableEnvironment.create(...) # unload a core module table_env.execute_sql(\u0026#34;UNLOAD MODULE core\u0026#34;) table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # Empty set SQL CLI Flink SQL\u0026gt; UNLOAD MODULE core; [INFO] Unload module succeeded! Flink SQL\u0026gt; SHOW MODULES; Empty set Back to top
UNLOAD MODULE # The following grammar gives an overview of the available syntax:
UNLOAD MODULE module_name `}),e.add({id:250,href:"/flink/flink-docs-master/docs/connectors/table/print/",title:"Print",section:"Table API Connectors",content:` Print SQL Connector # Sink The Print connector allows for writing every row to the standard output or standard error stream.
It is designed for:
Easy test for streaming job. Very useful in production debugging. Four possible format options:
Print Condition1 Condition2 PRINT_IDENTIFIER:taskId\u003e output PRINT_IDENTIFIER provided parallelism \u003e 1 PRINT_IDENTIFIER\u003e output PRINT_IDENTIFIER provided parallelism == 1 taskId\u003e output no PRINT_IDENTIFIER provided parallelism \u003e 1 output no PRINT_IDENTIFIER provided parallelism == 1 The output string format is \u0026ldquo;\$row_kind(f0,f1,f2\u0026hellip;)\u0026rdquo;, row_kind is the short string of RowKind, example is: \u0026ldquo;+I(1,1)\u0026rdquo;.
The Print connector is built-in.
Attention Print sinks print records in runtime tasks, you need to observe the task log.
How to create a Print table # CREATE TABLE print_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ); Alternatively, it may be based on an existing schema using the LIKE Clause.
CREATE TABLE print_table WITH (\u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39;) LIKE source_table (EXCLUDING ALL) Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'print'. print-identifier optional (none) String Message that identify print and is prefixed to the output of the value. standard-error optional false Boolean True, if the format should print to standard error instead of standard out. sink.parallelism optional (none) Integer Defines the parallelism of the Print sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator. `}),e.add({id:251,href:"/flink/flink-docs-master/docs/dev/table/sql/set/",title:"SET Statements",section:"SQL",content:` SET Statements # SET statements are used to modify the configuration or list the configuration.
Run a SET statement # SQL CLI SET statements can be executed in SQL CLI.
The following examples show how to run a SET statement in SQL CLI.
SQL CLI Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Europe/Berlin\u0026#39;; [INFO] Session property has been set. Flink SQL\u0026gt; SET; \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Europe/Berlin\u0026#39; Syntax # SET (\u0026#39;key\u0026#39; = \u0026#39;value\u0026#39;)? If no key and value are specified, it just prints all the properties. Otherwise, set the key with specified value.
Back to top
`}),e.add({id:252,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/topn/",title:"Top-N",section:"Queries",content:` Top-N # Batch Streaming
Top-N queries ask for the N smallest or largest values ordered by columns. Both smallest and largest values sets are considered Top-N queries. Top-N queries are useful in cases where the need is to display only the N bottom-most or the N top- most records from batch/streaming table on a condition. This result set can be used for further analysis.
Flink uses the combination of a OVER window clause and a filter condition to express a Top-N query. With the power of OVER window PARTITION BY clause, Flink also supports per group Top-N. For example, the top five products per category that have the maximum sales in realtime. Top-N queries are supported for SQL on batch and streaming tables.
The following shows the syntax of the Top-N statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]] ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum FROM table_name) WHERE rownum \u0026lt;= N [AND conditions] Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one, according to the ordering of rows within the partition. Currently, we only support ROW_NUMBER as the over window function. In the future, we will support RANK() and DENSE_RANK(). PARTITION BY col1[, col2...]: Specifies the partition columns. Each partition will have a Top-N result. ORDER BY col1 [asc|desc][, col2 [asc|desc]...]: Specifies the ordering columns. The ordering directions can be different on different columns. WHERE rownum \u0026lt;= N: The rownum \u0026lt;= N is required for Flink to recognize this query is a Top-N query. The N represents the N smallest or largest records will be retained. [AND conditions]: It is free to add other conditions in the where clause, but the other conditions can only be combined with rownum \u0026lt;= N using AND conjunction. Note: the above pattern must be followed exactly, otherwise the optimizer won’t be able to translate the query. The TopN query is Result Updating. Flink SQL will sort the input data stream according to the order key, so if the top N records have been changed, the changed ones will be sent as retraction/update records to downstream. It is recommended to use a storage which supports updating as the sink of Top-N query. In addition, if the top N records need to be stored in external storage, the result table should have the same unique key with the Top-N query. The unique keys of Top-N query is the combination of partition columns and rownum column. Top-N query can also derive the unique key of upstream. Take following job as an example, say product_id is the unique key of the ShopSales, then the unique keys of the Top-N query are [category, rownum] and [product_id].
The following examples show how to specify SQL queries with Top-N on streaming tables. This is an example to get \u0026ldquo;the top five products per category that have the maximum sales in realtime\u0026rdquo; we mentioned above.
CREATE TABLE ShopSales ( product_id STRING, category STRING, product_name STRING, sales BIGINT ) WITH (...); SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num FROM ShopSales) WHERE row_num \u0026lt;= 5 No Ranking Output Optimization # As described above, the rownum field will be written into the result table as one field of the unique key, which may lead to a lot of records being written to the result table. For example, when the record (say product-1001) of ranking 9 is updated and its rank is upgraded to 1, all the records from ranking 1 ~ 9 will be output to the result table as update messages. If the result table receives too many data, it will become the bottleneck of the SQL job.
The optimization way is omitting rownum field in the outer SELECT clause of the Top-N query. This is reasonable because the number of the top N records is usually not large, thus the consumers can sort the records themselves quickly. Without rownum field, in the example above, only the changed record (product-1001) needs to be sent to downstream, which can reduce much IO to the result table.
The following example shows how to optimize the above Top-N example in this way:
CREATE TABLE ShopSales ( product_id STRING, category STRING, product_name STRING, sales BIGINT ) WITH (...); -- omit row_num field from the output SELECT product_id, category, product_name, sales FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num FROM ShopSales) WHERE row_num \u0026lt;= 5 Attention in Streaming Mode In order to output the above query to an external storage and have a correct result, the external storage must have the same unique key with the Top-N query. In the above example query, if the product_id is the unique key of the query, then the external table should also has product_id as the unique key.
Back to top
`}),e.add({id:253,href:"/flink/flink-docs-master/docs/connectors/table/blackhole/",title:"BlackHole",section:"Table API Connectors",content:` BlackHole SQL Connector # Sink Sink
The BlackHole connector allows for swallowing all input records. It is designed for:
high performance testing. UDF to output, not substantive sink. Just like /dev/null device on Unix-like operating systems.
The BlackHole connector is built-in.
How to create a BlackHole table # CREATE TABLE blackhole_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39; ); Alternatively, it may be based on an existing schema using the LIKE Clause.
CREATE TABLE blackhole_table WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE source_table (EXCLUDING ALL) Connector Options # Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'blackhole'. `}),e.add({id:254,href:"/flink/flink-docs-master/docs/dev/table/sql/reset/",title:"RESET Statements",section:"SQL",content:` RESET Statements # RESET statements are used to reset the configuration to the default.
Run a RESET statement # SQL CLI RESET statements can be executed in SQL CLI.
The following examples show how to run a RESET statement in SQL CLI.
SQL CLI Flink SQL\u0026gt; RESET \u0026#39;table.planner\u0026#39;; [INFO] Session property has been reset. Flink SQL\u0026gt; RESET; [INFO] All session properties have been set to their default values. Syntax # RESET (\u0026#39;key\u0026#39;)? If no key is specified, it reset all the properties to the default. Otherwise, reset the specified key to the default.
Back to top
`}),e.add({id:255,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/window-topn/",title:"Window Top-N",section:"Queries",content:` Window Top-N # Batch Streaming
Window Top-N is a special Top-N which returns the N smallest or largest values for each window and other partitioned keys.
For streaming queries, unlike regular Top-N on continuous tables, window Top-N does not emit intermediate results but only a final result, the total top N records at the end of the window. Moreover, window Top-N purges all intermediate state when no longer needed. Therefore, window Top-N queries have better performance if users don\u0026rsquo;t need results updated per record. Usually, Window Top-N is used with Windowing TVF directly. Besides, Window Top-N could be used with other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Window Top-N can be defined in the same syntax as regular Top-N, see Top-N documentation for more information. Besides that, Window Top-N requires the PARTITION BY clause contains window_start and window_end columns of the relation applied Windowing TVF or Window Aggregation. Otherwise, the optimizer won’t be able to translate the query.
The following shows the syntax of the Window Top-N statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...] ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum FROM table_name) -- relation applied windowing TVF WHERE rownum \u0026lt;= N [AND conditions] Example # Window Top-N follows after Window Aggregation # The following example shows how to calculate Top 3 suppliers who have the highest sales for every tumbling 10 minutes window.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | | supplier_id | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+-------------+ | bidtime | price | item | supplier_id | +------------------+-------+------+-------------+ | 2020-04-15 08:05 | 4.00 | A | supplier1 | | 2020-04-15 08:06 | 4.00 | C | supplier2 | | 2020-04-15 08:07 | 2.00 | G | supplier1 | | 2020-04-15 08:08 | 2.00 | B | supplier3 | | 2020-04-15 08:09 | 5.00 | D | supplier4 | | 2020-04-15 08:11 | 2.00 | B | supplier3 | | 2020-04-15 08:13 | 1.00 | E | supplier1 | | 2020-04-15 08:15 | 3.00 | H | supplier2 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | +------------------+-------+------+-------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum FROM ( SELECT window_start, window_end, supplier_id, SUM(price) as price, COUNT(*) as cnt FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, supplier_id ) ) WHERE rownum \u0026lt;= 3; +------------------+------------------+-------------+-------+-----+--------+ | window_start | window_end | supplier_id | price | cnt | rownum | +------------------+------------------+-------------+-------+-----+--------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier1 | 6.00 | 2 | 1 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier4 | 5.00 | 1 | 2 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier2 | 4.00 | 1 | 3 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier5 | 6.00 | 1 | 1 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier2 | 3.00 | 1 | 2 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier3 | 2.00 | 1 | 3 | +------------------+------------------+-------------+-------+-----+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Window Top-N follows after Windowing TVF # The following example shows how to calculate Top 3 items which have the highest price for every tumbling 10 minutes window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT bidtime, price, item, supplier_id, window_start, window_end, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) ) WHERE rownum \u0026lt;= 3; +------------------+-------+------+-------------+------------------+------------------+--------+ | bidtime | price | item | supplier_id | window_start | window_end | rownum | +------------------+-------+------+-------------+------------------+------------------+--------+ | 2020-04-15 08:05 | 4.00 | A | supplier1 | 2020-04-15 08:00 | 2020-04-15 08:10 | 2 | | 2020-04-15 08:06 | 4.00 | C | supplier2 | 2020-04-15 08:00 | 2020-04-15 08:10 | 3 | | 2020-04-15 08:09 | 5.00 | D | supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 | 1 | | 2020-04-15 08:11 | 2.00 | B | supplier3 | 2020-04-15 08:10 | 2020-04-15 08:20 | 3 | | 2020-04-15 08:15 | 3.00 | H | supplier2 | 2020-04-15 08:10 | 2020-04-15 08:20 | 2 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 | 1 | +------------------+-------+------+-------------+------------------+------------------+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Currently, Flink only supports Window Top-N follows after Windowing TVF with Tumble Windows, Hop Windows and Cumulate Windows. Window Top-N follows after Windowing TVF with Session windows will be supported in the near future.
Back to top
`}),e.add({id:256,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/deduplication/",title:"Deduplication",section:"Queries",content:` Deduplication # Batch Streaming
Deduplication removes rows that duplicate over a set of columns, keeping only the first one or the last one. In some cases, the upstream ETL jobs are not end-to-end exactly-once; this may result in duplicate records in the sink in case of failover. However, the duplicate records will affect the correctness of downstream analytical jobs - e.g. SUM, COUNT - so deduplication is needed before further analysis.
Flink uses ROW_NUMBER() to remove duplicates, just like the way of Top-N query. In theory, deduplication is a special case of Top-N in which the N is one and order by the processing time or event time.
The following shows the syntax of the Deduplication statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]] ORDER BY time_attr [asc|desc]) AS rownum FROM table_name) WHERE rownum = 1 Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one. PARTITION BY col1[, col2...]: Specifies the partition columns, i.e. the deduplicate key. ORDER BY time_attr [asc|desc]: Specifies the ordering column, it must be a time attribute. Currently Flink supports processing time attribute and event time attribute. Ordering by ASC means keeping the first row, ordering by DESC means keeping the last row. WHERE rownum = 1: The rownum = 1 is required for Flink to recognize this query is deduplication. Note: the above pattern must be followed exactly, otherwise the optimizer won’t be able to translate the query. The following examples show how to specify SQL queries with Deduplication on streaming tables.
CREATE TABLE Orders ( order_id STRING, user STRING, product STRING, num BIGINT, proctime AS PROCTIME() ) WITH (...); -- remove duplicate rows on order_id and keep the first occurrence row, -- because there shouldn\u0026#39;t be two orders with the same order_id. SELECT order_id, user, product, num FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime ASC) AS row_num FROM Orders) WHERE row_num = 1 Back to top
`}),e.add({id:257,href:"/flink/flink-docs-master/docs/connectors/table/hive/",title:"Hive",section:"Table API Connectors",content:""}),e.add({id:258,href:"/flink/flink-docs-master/docs/dev/table/sql/jar/",title:"JAR Statements",section:"SQL",content:` JAR Statements # JAR statements are used to add user jars into the classpath or remove user jars from the classpath or show added jars in the classpath in the runtime.
Flink SQL supports the following JAR statements for now:
ADD JAR REMOVE JAR SHOW JARS Attention JAR statements only work in the SQL CLI.
Run a JAR statement # SQL CLI The following examples show how to run JAR statements in SQL CLI. SQL CLI Flink SQL\u0026gt; ADD JAR \u0026#39;/path/hello.jar\u0026#39;; [INFO] The specified jar is added into session classloader. Flink SQL\u0026gt; SHOW JARS; /path/hello.jar Flink SQL\u0026gt; REMOVE JAR \u0026#39;/path/hello.jar\u0026#39;; [INFO] The specified jar is removed from session classloader. ADD JAR # ADD JAR \u0026#39;\u0026lt;path_to_filename\u0026gt;.jar\u0026#39; Currently it only supports to add the local jar into the session classloader.
REMOVE JAR # REMOVE JAR \u0026#39;\u0026lt;path_to_filename\u0026gt;.jar\u0026#39; Currently it only supports to remove the jar that is added by the ADD JAR statements.
SHOW JARS # SHOW JARS Show all added jars in the session classloader which are added by ADD JAR statements.
Back to top
`}),e.add({id:259,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/window-deduplication/",title:"Window Deduplication",section:"Queries",content:` Window Deduplication # Streaming Window Deduplication is a special Deduplication which removes rows that duplicate over a set of columns, keeping the first one or the last one for each window and partitioned keys.
For streaming queries, unlike regular Deduplicate on continuous tables, Window Deduplication does not emit intermediate results but only a final result at the end of the window. Moreover, window Deduplication purges all intermediate state when no longer needed. Therefore, Window Deduplication queries have better performance if users don\u0026rsquo;t need results updated per record. Usually, Window Deduplication is used with Windowing TVF directly. Besides, Window Deduplication could be used with other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Window Deduplication can be defined in the same syntax as regular Deduplication, see Deduplication documentation for more information. Besides that, Window Deduplication requires the PARTITION BY clause contains window_start and window_end columns of the relation. Otherwise, the optimizer won’t be able to translate the query.
Flink uses ROW_NUMBER() to remove duplicates, just like the way of Window Top-N query. In theory, Window Deduplication is a special case of Window Top-N in which the N is one and order by the processing time or event time.
The following shows the syntax of the Window Deduplication statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...] ORDER BY time_attr [asc|desc]) AS rownum FROM table_name) -- relation applied windowing TVF WHERE (rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2) [AND conditions] Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one. PARTITION BY window_start, window_end [, col_key1...]: Specifies the partition columns which contain window_start, window_end and other partition keys. ORDER BY time_attr [asc|desc]: Specifies the ordering column, it must be a time attribute. Currently Flink supports processing time attribute and event time attribute. Ordering by ASC means keeping the first row, ordering by DESC means keeping the last row. WHERE (rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2): The rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2 is required for the optimizer to recognize the query could be translated to Window Deduplication. Note: the above pattern must be followed exactly, otherwise the optimizer won’t translate the query to Window Deduplication. Example # The following example shows how to keep last record for every 10 minutes tumbling window.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; DESC Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+ | bidtime | price | item | +------------------+-------+------+ | 2020-04-15 08:05 | 4.00 | C | | 2020-04-15 08:07 | 2.00 | A | | 2020-04-15 08:09 | 5.00 | D | | 2020-04-15 08:11 | 3.00 | B | | 2020-04-15 08:13 | 1.00 | E | | 2020-04-15 08:17 | 6.00 | F | +------------------+-------+------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT bidtime, price, item, supplier_id, window_start, window_end, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY bidtime DESC) AS rownum FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) ) WHERE rownum \u0026lt;= 1; +------------------+-------+------+-------------+------------------+------------------+--------+ | bidtime | price | item | supplier_id | window_start | window_end | rownum | +------------------+-------+------+-------------+------------------+------------------+--------+ | 2020-04-15 08:09 | 5.00 | D | supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 | 1 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 | 1 | +------------------+-------+------+-------------+------------------+------------------+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Limitation on Window Deduplication which follows after Windowing TVFs directly # Currently, if Window Deduplication follows after Windowing TVF, the Windowing TVF has to be with Tumble Windows, Hop Windows or Cumulate Windows instead of Session windows. Session windows will be supported in the near future.
Limitation on time attribute of order key # Currently, Window Deduplication requires order key must be event time attribute instead of processing time attribute. Ordering by processing-time would be supported in the near future.
Back to top
`}),e.add({id:260,href:"/flink/flink-docs-master/docs/dev/table/sql/queries/match_recognize/",title:"Pattern Recognition",section:"Queries",content:` Pattern Recognition # Streaming It is a common use case to search for a set of event patterns, especially in case of data streams. Flink comes with a complex event processing (CEP) library which allows for pattern detection in event streams. Furthermore, Flink\u0026rsquo;s SQL API provides a relational way of expressing queries with a large set of built-in functions and rule-based optimizations that can be used out of the box.
In December 2016, the International Organization for Standardization (ISO) released a new version of the SQL standard which includes Row Pattern Recognition in SQL (ISO/IEC TR 19075-5:2016). It allows Flink to consolidate CEP and SQL API using the MATCH_RECOGNIZE clause for complex event processing in SQL.
A MATCH_RECOGNIZE clause enables the following tasks:
Logically partition and order the data that is used with the PARTITION BY and ORDER BY clauses. Define patterns of rows to seek using the PATTERN clause. These patterns use a syntax similar to that of regular expressions. The logical components of the row pattern variables are specified in the DEFINE clause. Define measures, which are expressions usable in other parts of the SQL query, in the MEASURES clause. The following example illustrates the syntax for basic pattern recognition:
SELECT T.aid, T.bid, T.cid FROM MyTable MATCH_RECOGNIZE ( PARTITION BY userid ORDER BY proctime MEASURES A.id AS aid, B.id AS bid, C.id AS cid PATTERN (A B C) DEFINE A AS name = \u0026#39;a\u0026#39;, B AS name = \u0026#39;b\u0026#39;, C AS name = \u0026#39;c\u0026#39; ) AS T This page will explain each keyword in more detail and will illustrate more complex examples.
Flink\u0026rsquo;s implementation of the MATCH_RECOGNIZE clause is a subset of the full standard. Only those features documented in the following sections are supported. Additional features may be supported based on community feedback, please also take a look at the known limitations. Introduction and Examples # Installation Guide # The pattern recognition feature uses the Apache Flink\u0026rsquo;s CEP library internally. In order to be able to use the MATCH_RECOGNIZE clause, the library needs to be added as a dependency to your Maven project.
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-cep\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Alternatively, you can also add the dependency to the cluster classpath (see the dependency section for more information).
If you want to use the MATCH_RECOGNIZE clause in the SQL Client, you don\u0026rsquo;t have to do anything as all the dependencies are included by default.
SQL Semantics # Every MATCH_RECOGNIZE query consists of the following clauses:
PARTITION BY - defines the logical partitioning of the table; similar to a GROUP BY operation. ORDER BY - specifies how the incoming rows should be ordered; this is essential as patterns depend on an order. MEASURES - defines output of the clause; similar to a SELECT clause. ONE ROW PER MATCH - output mode which defines how many rows per match should be produced. AFTER MATCH SKIP - specifies where the next match should start; this is also a way to control how many distinct matches a single event can belong to. PATTERN - allows constructing patterns that will be searched for using a regular expression-like syntax. DEFINE - this section defines the conditions that the pattern variables must satisfy. Attention Currently, the MATCH_RECOGNIZE clause can only be applied to an append table. Furthermore, it always produces an append table as well.
Examples # For our examples, we assume that a table Ticker has been registered. The table contains prices of stocks at a particular point in time.
The table has a following schema:
Ticker |-- symbol: String # symbol of the stock |-- price: Long # price of the stock |-- tax: Long # tax liability of the stock |-- rowtime: TimeIndicatorTypeInfo(rowtime) # point in time when the change to those values happened For simplification, we only consider the incoming data for a single stock ACME. A ticker could look similar to the following table where rows are continuously appended.
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 12 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:01\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:02\u0026#39; 19 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:03\u0026#39; 21 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:04\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:05\u0026#39; 18 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:06\u0026#39; 15 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:07\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:08\u0026#39; 24 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:09\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:10\u0026#39; 19 1 The task is now to find periods of a constantly decreasing price of a single ticker. For this, one could write a query like:
SELECT * FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES START_ROW.rowtime AS start_tstamp, LAST(PRICE_DOWN.rowtime) AS bottom_tstamp, LAST(PRICE_UP.rowtime) AS end_tstamp ONE ROW PER MATCH AFTER MATCH SKIP TO LAST PRICE_UP PATTERN (START_ROW PRICE_DOWN+ PRICE_UP) DEFINE PRICE_DOWN AS (LAST(PRICE_DOWN.price, 1) IS NULL AND PRICE_DOWN.price \u0026lt; START_ROW.price) OR PRICE_DOWN.price \u0026lt; LAST(PRICE_DOWN.price, 1), PRICE_UP AS PRICE_UP.price \u0026gt; LAST(PRICE_DOWN.price, 1) ) MR; The query partitions the Ticker table by the symbol column and orders it by the rowtime time attribute.
The PATTERN clause specifies that we are interested in a pattern with a starting event START_ROW that is followed by one or more PRICE_DOWN events and concluded with a PRICE_UP event. If such a pattern can be found, the next pattern match will be seeked at the last PRICE_UP event as indicated by the AFTER MATCH SKIP TO LAST clause.
The DEFINE clause specifies the conditions that need to be met for a PRICE_DOWN and PRICE_UP event. Although the START_ROW pattern variable is not present it has an implicit condition that is evaluated always as TRUE.
A pattern variable PRICE_DOWN is defined as a row with a price that is smaller than the price of the last row that met the PRICE_DOWN condition. For the initial case or when there is no last row that met the PRICE_DOWN condition, the price of the row should be smaller than the price of the preceding row in the pattern (referenced by START_ROW).
A pattern variable PRICE_UP is defined as a row with a price that is larger than the price of the last row that met the PRICE_DOWN condition.
This query produces a summary row for each period in which the price of a stock was continuously decreasing.
The exact representation of the output rows is defined in the MEASURES part of the query. The number of output rows is defined by the ONE ROW PER MATCH output mode.
symbol start_tstamp bottom_tstamp end_tstamp ========= ================== ================== ================== ACME 01-APR-11 10:00:04 01-APR-11 10:00:07 01-APR-11 10:00:08 The resulting row describes a period of falling prices that started at 01-APR-11 10:00:04 and achieved the lowest price at 01-APR-11 10:00:07 that increased again at 01-APR-11 10:00:08.
Partitioning # It is possible to look for patterns in partitioned data, e.g., trends for a single ticker or a particular user. This can be expressed using the PARTITION BY clause. The clause is similar to using GROUP BY for aggregations.
It is highly advised to partition the incoming data because otherwise the MATCH_RECOGNIZE clause will be translated into a non-parallel operator to ensure global ordering.
Order of Events # Apache Flink allows for searching for patterns based on time; either processing time or event time.
In case of event time, the events are sorted before they are passed to the internal pattern state machine. As a consequence, the produced output will be correct regardless of the order in which rows are appended to the table. Instead, the pattern is evaluated in the order specified by the time contained in each row.
The MATCH_RECOGNIZE clause assumes a time attribute with ascending ordering as the first argument to ORDER BY clause.
For the example Ticker table, a definition like ORDER BY rowtime ASC, price DESC is valid but ORDER BY price, rowtime or ORDER BY rowtime DESC, price ASC is not.
Define \u0026amp; Measures # The DEFINE and MEASURES keywords have similar meanings to the WHERE and SELECT clauses in a simple SQL query.
The MEASURES clause defines what will be included in the output of a matching pattern. It can project columns and define expressions for evaluation. The number of produced rows depends on the output mode setting.
The DEFINE clause specifies conditions that rows have to fulfill in order to be classified to a corresponding pattern variable. If a condition is not defined for a pattern variable, a default condition will be used which evaluates to true for every row.
For a more detailed explanation about expressions that can be used in those clauses, please have a look at the event stream navigation section.
Aggregations # Aggregations can be used in DEFINE and MEASURES clauses. Both built-in and custom user defined functions are supported.
Aggregate functions are applied to each subset of rows mapped to a match. In order to understand how those subsets are evaluated have a look at the event stream navigation section.
The task of the following example is to find the longest period of time for which the average price of a ticker did not go below certain threshold. It shows how expressible MATCH_RECOGNIZE can become with aggregations. This task can be performed with the following query:
SELECT * FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.rowtime) AS start_tstamp, LAST(A.rowtime) AS end_tstamp, AVG(A.price) AS avgPrice ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A+ B) DEFINE A AS AVG(A.price) \u0026lt; 15 ) MR; Given this query and following input values:
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 12 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:01\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:02\u0026#39; 13 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:03\u0026#39; 16 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:04\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:05\u0026#39; 2 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:06\u0026#39; 4 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:07\u0026#39; 10 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:08\u0026#39; 15 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:09\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:10\u0026#39; 25 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:11\u0026#39; 30 1 The query will accumulate events as part of the pattern variable A as long as the average price of them does not exceed 15. For example, such a limit exceeding happens at 01-Apr-11 10:00:04. The following period exceeds the average price of 15 again at 01-Apr-11 10:00:11. Thus the results for said query will be:
symbol start_tstamp end_tstamp avgPrice ========= ================== ================== ============ ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5 ACME 01-APR-11 10:00:05 01-APR-11 10:00:10 13.5 Aggregations can be applied to expressions, but only if they reference a single pattern variable. Thus SUM(A.price * A.tax) is a valid one, but AVG(A.price * B.tax) is not.
DISTINCT aggregations are not supported. Defining a Pattern # The MATCH_RECOGNIZE clause allows users to search for patterns in event streams using a powerful and expressive syntax that is somewhat similar to the widespread regular expression syntax.
Every pattern is constructed from basic building blocks, called pattern variables, to which operators (quantifiers and other modifiers) can be applied. The whole pattern must be enclosed in brackets.
An example pattern could look like:
PATTERN (A B+ C* D) One may use the following operators:
Concatenation - a pattern like (A B) means that the contiguity is strict between A and B. Therefore, there can be no rows that were not mapped to A or B in between. Quantifiers - modify the number of rows that can be mapped to the pattern variable. * — 0 or more rows + — 1 or more rows ? — 0 or 1 rows { n } — exactly n rows (n \u0026gt; 0) { n, } — n or more rows (n ≥ 0) { n, m } — between n and m (inclusive) rows (0 ≤ n ≤ m, 0 \u0026lt; m) { , m } — between 0 and m (inclusive) rows (m \u0026gt; 0) Patterns that can potentially produce an empty match are not supported. Examples of such patterns are PATTERN (A*), PATTERN (A? B*), PATTERN (A{0,} B{0,} C*), etc. Greedy \u0026amp; Reluctant Quantifiers # Each quantifier can be either greedy (default behavior) or reluctant. Greedy quantifiers try to match as many rows as possible while reluctant quantifiers try to match as few as possible.
In order to illustrate the difference, one can view the following example with a query where a greedy quantifier is applied to the B variable:
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES C.price AS lastPrice ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A B* C) DEFINE A AS A.price \u0026gt; 10, B AS B.price \u0026lt; 15, C AS C.price \u0026gt; 12 ) Given we have the following input:
symbol tax price rowtime ======= ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 11 2018-09-17 10:00:03 XYZ 1 12 2018-09-17 10:00:04 XYZ 2 13 2018-09-17 10:00:05 XYZ 1 14 2018-09-17 10:00:06 XYZ 2 16 2018-09-17 10:00:07 The pattern above will produce the following output:
symbol lastPrice ======== =========== XYZ 16 The same query where B* is modified to B*?, which means that B* should be reluctant, will produce:
symbol lastPrice ======== =========== XYZ 13 XYZ 16 The pattern variable B matches only to the row with price 12 instead of swallowing the rows with prices 12, 13, and 14.
It is not possible to use a greedy quantifier for the last variable of a pattern. Thus, a pattern like (A B*) is not allowed. This can be easily worked around by introducing an artificial state (e.g. C) that has a negated condition of B. So you could use a query like:
PATTERN (A B* C) DEFINE A AS condA(), B AS condB(), C AS NOT condB() Attention The optional reluctant quantifier (A?? or A{0,1}?) is not supported right now.
Time constraint # Especially for streaming use cases, it is often required that a pattern finishes within a given period of time. This allows for limiting the overall state size that Flink has to maintain internally, even in case of greedy quantifiers.
Therefore, Flink SQL supports the additional (non-standard SQL) WITHIN clause for defining a time constraint for a pattern. The clause can be defined after the PATTERN clause and takes an interval of millisecond resolution.
If the time between the first and last event of a potential match is longer than the given value, such a match will not be appended to the result table.
Note It is generally encouraged to use the WITHIN clause as it helps Flink with efficient memory management. Underlying state can be pruned once the threshold is reached.
Attention However, the WITHIN clause is not part of the SQL standard. The recommended way of dealing with time constraints might change in the future.
The use of the WITHIN clause is illustrated in the following example query:
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES C.rowtime AS dropTime, A.price - C.price AS dropDiff ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A B* C) WITHIN INTERVAL \u0026#39;1\u0026#39; HOUR DEFINE B AS B.price \u0026gt; A.price - 10, C AS C.price \u0026lt; A.price - 10 ) The query detects a price drop of 10 that happens within an interval of 1 hour.
Let\u0026rsquo;s assume the query is used to analyze the following ticker data:
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 20 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:20:00\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:40:00\u0026#39; 18 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:00:00\u0026#39; 11 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:20:00\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:40:00\u0026#39; 9 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:00:00\u0026#39; 15 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:20:00\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:40:00\u0026#39; 24 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:00:00\u0026#39; 1 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:20:00\u0026#39; 19 1 The query will produce the following results:
symbol dropTime dropDiff ====== ==================== ============= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:00:00\u0026#39; 14 The resulting row represents a price drop from 15 (at 01-Apr-11 12:00:00) to 1 (at 01-Apr-11 13:00:00). The dropDiff column contains the price difference.
Notice that even though prices also drop by higher values, for example, by 11 (between 01-Apr-11 10:00:00 and 01-Apr-11 11:40:00), the time difference between those two events is larger than 1 hour. Thus, they don\u0026rsquo;t produce a match.
Output Mode # The output mode describes how many rows should be emitted for every found match. The SQL standard describes two modes:
ALL ROWS PER MATCH ONE ROW PER MATCH. Currently, the only supported output mode is ONE ROW PER MATCH that will always produce one output summary row for each found match.
The schema of the output row will be a concatenation of [partitioning columns] + [measures columns] in that particular order.
The following example shows the output of a query defined as:
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.price) AS startPrice, LAST(A.price) AS topPrice, B.price AS lastPrice ONE ROW PER MATCH PATTERN (A+ B) DEFINE A AS LAST(A.price, 1) IS NULL OR A.price \u0026gt; LAST(A.price, 1), B AS B.price \u0026lt; LAST(A.price) ) For the following input rows:
symbol tax price rowtime ======== ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 12 2018-09-17 10:00:03 XYZ 1 13 2018-09-17 10:00:04 XYZ 2 11 2018-09-17 10:00:05 The query will produce the following output:
symbol startPrice topPrice lastPrice ======== ============ ========== =========== XYZ 10 13 11 The pattern recognition is partitioned by the symbol column. Even though not explicitly mentioned in the MEASURES clause, the partitioned column is added at the beginning of the result.
Pattern Navigation # The DEFINE and MEASURES clauses allow for navigating within the list of rows that (potentially) match a pattern.
This section discusses this navigation for declaring conditions or producing output results.
Pattern Variable Referencing # A pattern variable reference allows a set of rows mapped to a particular pattern variable in the DEFINE or MEASURES clauses to be referenced.
For example, the expression A.price describes a set of rows mapped so far to A plus the current row if we try to match the current row to A. If an expression in the DEFINE/MEASURES clause requires a single row (e.g. A.price or A.price \u0026gt; 10), it selects the last value belonging to the corresponding set.
If no pattern variable is specified (e.g. SUM(price)), an expression references the default pattern variable * which references all variables in the pattern. In other words, it creates a list of all the rows mapped so far to any variable plus the current row.
Example # For a more thorough example, one can take a look at the following pattern and corresponding conditions:
PATTERN (A B+) DEFINE A AS A.price \u0026gt;= 10, B AS B.price \u0026gt; A.price AND SUM(price) \u0026lt; 100 AND SUM(B.price) \u0026lt; 80 The following table describes how those conditions are evaluated for each incoming event.
The table consists of the following columns:
# - the row identifier that uniquely identifies an incoming row in the lists [A.price]/[B.price]/[price]. price - the price of the incoming row. [A.price]/[B.price]/[price] - describe lists of rows which are used in the DEFINE clause to evaluate conditions. Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to. A.price/B.price/SUM(price)/SUM(B.price) - describes the result after those expressions have been evaluated. # price Classifier [A.price] [B.price] [price] A.price B.price SUM(price) SUM(B.price) #1 10 -\u0026gt; A #1 - - 10 - - - #2 15 -\u0026gt; B #1 #2 #1, #2 10 15 25 15 #3 20 -\u0026gt; B #1 #2, #3 #1, #2, #3 10 20 45 35 #4 31 -\u0026gt; B #1 #2, #3, #4 #1, #2, #3, #4 10 31 76 66 #5 35 #1 #2, #3, #4, #5 #1, #2, #3, #4, #5 10 35 111 101 As can be seen in the table, the first row is mapped to pattern variable A and subsequent rows are mapped to pattern variable B. However, the last row does not fulfill the B condition because the sum over all mapped rows SUM(price) and the sum over all rows in B exceed the specified thresholds.
Logical Offsets # Logical offsets enable navigation within the events that were mapped to a particular pattern variable. This can be expressed with two corresponding functions:
Offset functions Description LAST(variable.field, n) Returns the value of the field from the event that was mapped to the n-th last element of the variable. The counting starts at the last element mapped.
FIRST(variable.field, n) Returns the value of the field from the event that was mapped to the n-th element of the variable. The counting starts at the first element mapped.
Examples # For a more thorough example, one can take a look at the following pattern and corresponding conditions:
PATTERN (A B+) DEFINE A AS A.price \u0026gt;= 10, B AS (LAST(B.price, 1) IS NULL OR B.price \u0026gt; LAST(B.price, 1)) AND (LAST(B.price, 2) IS NULL OR B.price \u0026gt; 2 * LAST(B.price, 2)) The following table describes how those conditions are evaluated for each incoming event.
The table consists of the following columns:
price - the price of the incoming row. Classifier - the classifier of the current row which indicates the pattern variable the row is mapped to. LAST(B.price, 1)/LAST(B.price, 2) - describes the result after those expressions have been evaluated. price Classifier LAST(B.price, 1) LAST(B.price, 2) Comment 10 -\u0026gt; A 15 -\u0026gt; B null null Notice that LAST(B.price, 1) is null because there is still nothing mapped to B. 20 -\u0026gt; B 15 null 31 -\u0026gt; B 20 15 35 31 20 Not mapped because 35 \u0026lt; 2 * 20. It might also make sense to use the default pattern variable with logical offsets.
In this case, an offset considers all the rows mapped so far:
PATTERN (A B? C) DEFINE B AS B.price \u0026lt; 20, C AS LAST(price, 1) \u0026lt; C.price price Classifier LAST(price, 1) Comment 10 -\u0026gt; A 15 -\u0026gt; B 20 -\u0026gt; C 15 LAST(price, 1) is evaluated as the price of the row mapped to the B variable. If the second row did not map to the B variable, we would have the following results:
price Classifier LAST(price, 1) Comment 10 -\u0026gt; A 20 -\u0026gt; C 10 LAST(price, 1) is evaluated as the price of the row mapped to the A variable. It is also possible to use multiple pattern variable references in the first argument of the FIRST/LAST functions. This way, one can write an expression that accesses multiple columns. However, all of them must use the same pattern variable. In other words, the value of the LAST/FIRST function must be computed in a single row.
Thus, it is possible to use LAST(A.price * A.tax), but an expression like LAST(A.price * B.tax) is not allowed.
After Match Strategy # The AFTER MATCH SKIP clause specifies where to start a new matching procedure after a complete match was found.
There are four different strategies:
SKIP PAST LAST ROW - resumes the pattern matching at the next row after the last row of the current match. SKIP TO NEXT ROW - continues searching for a new match starting at the next row after the starting row of the match. SKIP TO LAST variable - resumes the pattern matching at the last row that is mapped to the specified pattern variable. SKIP TO FIRST variable - resumes the pattern matching at the first row that is mapped to the specified pattern variable. This is also a way to specify how many matches a single event can belong to. For example, with the SKIP PAST LAST ROW strategy every event can belong to at most one match.
Examples # In order to better understand the differences between those strategies one can take a look at the following example.
For the following input rows:
symbol tax price rowtime ======== ===== ======= ===================== XYZ 1 7 2018-09-17 10:00:01 XYZ 2 9 2018-09-17 10:00:02 XYZ 1 10 2018-09-17 10:00:03 XYZ 2 5 2018-09-17 10:00:04 XYZ 2 10 2018-09-17 10:00:05 XYZ 2 7 2018-09-17 10:00:06 XYZ 2 14 2018-09-17 10:00:07 We evaluate the following query with different strategies:
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES SUM(A.price) AS sumPrice, FIRST(rowtime) AS startTime, LAST(rowtime) AS endTime ONE ROW PER MATCH [AFTER MATCH STRATEGY] PATTERN (A+ C) DEFINE A AS SUM(A.price) \u0026lt; 30 ) The query returns the sum of the prices of all rows mapped to A and the first and last timestamp of the overall match.
The query will produce different results based on which AFTER MATCH strategy was used:
AFTER MATCH SKIP PAST LAST ROW # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 The first result matched against the rows #1, #2, #3, #4.
The second result matched against the rows #5, #6, #7.
AFTER MATCH SKIP TO NEXT ROW # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 24 2018-09-17 10:00:02 2018-09-17 10:00:05 XYZ 25 2018-09-17 10:00:03 2018-09-17 10:00:06 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:07 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 Again, the first result matched against the rows #1, #2, #3, #4.
Compared to the previous strategy, the next match includes row #2 again for the next matching. Therefore, the second result matched against the rows #2, #3, #4, #5.
The third result matched against the rows #3, #4, #5, #6.
The forth result matched against the rows #4, #5, #6, #7.
The last result matched against the rows #5, #6, #7.
AFTER MATCH SKIP TO LAST A # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 25 2018-09-17 10:00:03 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 Again, the first result matched against the rows #1, #2, #3, #4.
Compared to the previous strategy, the next match includes only row #3 (mapped to A) again for the next matching. Therefore, the second result matched against the rows #3, #4, #5, #6.
The last result matched against the rows #5, #6, #7.
AFTER MATCH SKIP TO FIRST A # This combination will produce a runtime exception because one would always try to start a new match where the last one started. This would produce an infinite loop and, thus, is prohibited.
One has to keep in mind that in case of the SKIP TO FIRST/LAST variable strategy it might be possible that there are no rows mapped to that variable (e.g. for pattern A*). In such cases, a runtime exception will be thrown as the standard requires a valid row to continue the matching.
Time attributes # In order to apply some subsequent queries on top of the MATCH_RECOGNIZE it might be required to use time attributes. To select those there are available two functions:
Function Description MATCH_ROWTIME([rowtime_field]) Returns the timestamp of the last row that was mapped to the given pattern.
The function accepts zero or one operand which is a field reference with rowtime attribute. If there is no operand, the function will return rowtime attribute with TIMESTAMP type. Otherwise, the return type will be same with the operand type.
The resulting attribute is a rowtime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
MATCH_PROCTIME() Returns a proctime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
Controlling Memory Consumption # Memory consumption is an important consideration when writing MATCH_RECOGNIZE queries, as the space of potential matches is built in a breadth-first-like manner. Having that in mind, one must make sure that the pattern can finish. Preferably with a reasonable number of rows mapped to the match as they have to fit into memory.
For example, the pattern must not have a quantifier without an upper limit that accepts every single row. Such a pattern could look like this:
PATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 The query will map every incoming row to the B variable and thus will never finish. This query could be fixed, e.g., by negating the condition for C:
PATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, B as B.price \u0026lt;= 20, C as C.price \u0026gt; 20 Or by using the reluctant quantifier:
PATTERN (A B+? C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 Attention Please note that the MATCH_RECOGNIZE clause does not use a configured state retention time. One may want to use the WITHIN clause for this purpose.
Known Limitations # Flink\u0026rsquo;s implementation of the MATCH_RECOGNIZE clause is an ongoing effort, and some features of the SQL standard are not yet supported.
Unsupported features include:
Pattern expressions: Pattern groups - this means that e.g. quantifiers can not be applied to a subsequence of the pattern. Thus, (A (B C)+) is not a valid pattern. Alterations - patterns like PATTERN((A B | C D) E), which means that either a subsequence A B or C D has to be found before looking for the E row. PERMUTE operator - which is equivalent to all permutations of variables that it was applied to e.g. PATTERN (PERMUTE (A, B, C)) = PATTERN (A B C | A C B | B A C | B C A | C A B | C B A). Anchors - ^, \$, which denote beginning/end of a partition, those do not make sense in the streaming context and will not be supported. Exclusion - PATTERN ({- A -} B) meaning that A will be looked for but will not participate in the output. This works only for the ALL ROWS PER MATCH mode. Reluctant optional quantifier - PATTERN A?? only the greedy optional quantifier is supported. ALL ROWS PER MATCH output mode - which produces an output row for every row that participated in the creation of a found match. This also means: that the only supported semantic for the MEASURES clause is FINAL CLASSIFIER function, which returns the pattern variable that a row was mapped to, is not yet supported. SUBSET - which allows creating logical groups of pattern variables and using those groups in the DEFINE and MEASURES clauses. Physical offsets - PREV/NEXT, which indexes all events seen rather than only those that were mapped to a pattern variable (as in logical offsets case). Extracting time attributes - there is currently no possibility to get a time attribute for subsequent time-based operations. MATCH_RECOGNIZE is supported only for SQL. There is no equivalent in the Table API. Aggregations: distinct aggregations are not supported. Back to top
`}),e.add({id:261,href:"/flink/flink-docs-master/docs/dev/dataset/examples/",title:"Batch Examples",section:"DataSet API (Legacy)",content:` Batch Examples # The following example programs showcase different applications of Flink from simple word counting to graph algorithms. The code samples illustrate the use of Flink\u0026rsquo;s DataSet API.
The full source code of the following and more examples can be found in the flink-examples-batch module of the Flink source repository.
Running an example # In order to run a Flink example, we assume you have a running Flink instance available. The \u0026ldquo;Quickstart\u0026rdquo; and \u0026ldquo;Setup\u0026rdquo; tabs in the navigation describe various ways of starting Flink.
The easiest way is running the ./bin/start-cluster.sh, which by default starts a local cluster with one JobManager and one TaskManager.
Each binary release of Flink contains an examples directory with jar files for each of the examples on this page.
To run the WordCount example, issue the following command:
./bin/flink run ./examples/batch/WordCount.jar The other examples can be started in a similar way.
Note that many examples run without passing any arguments for them, by using build-in data. To run WordCount with real data, you have to pass the path to the data:
./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result Note that non-local file systems require a schema prefix, such as hdfs://.
Word Count # WordCount is the \u0026ldquo;Hello World\u0026rdquo; of Big Data processing systems. It computes the frequency of words in a text collection. The algorithm works in two steps: First, the texts are splits the text to individual words. Second, the words are grouped and counted.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;String\u0026gt; text = env.readTextFile(\u0026#34;/path/to/file\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap(new Tokenizer()) // group by the tuple field \u0026#34;0\u0026#34; and sum up tuple field \u0026#34;1\u0026#34; .groupBy(0) .sum(1); counts.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // User-defined functions public static class Tokenizer implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { // normalize and split the line String[] tokens = value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;); // emit the pairs for (String token : tokens) { if (token.length() \u0026gt; 0) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(token, 1)); } } } } The WordCount example implements the above described algorithm with input parameters: --input \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt;. As test data, any text file will do.
Scala val env = ExecutionEnvironment.getExecutionEnvironment // get input data val text = env.readTextFile(\u0026#34;/path/to/file\u0026#34;) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) The WordCount example implements the above described algorithm with input parameters: --input \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt;. As test data, any text file will do.
Page Rank # The PageRank algorithm computes the \u0026ldquo;importance\u0026rdquo; of pages in a graph defined by links, which point from one pages to another page. It is an iterative graph algorithm, which means that it repeatedly applies the same computation. In each iteration, each page distributes its current rank over all its neighbors, and compute its new rank as a taxed sum of the ranks it received from its neighbors. The PageRank algorithm was popularized by the Google search engine which uses the importance of webpages to rank the results of search queries.
In this simple example, PageRank is implemented with a bulk iteration and a fixed number of iterations.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // read the pages and initial ranks by parsing a CSV file DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; pagesWithRanks = env.readCsvFile(pagesInputPath) .types(Long.class, Double.class); // the links are encoded as an adjacency list: (page-id, Array(neighbor-ids)) DataSet\u0026lt;Tuple2\u0026lt;Long, Long[]\u0026gt;\u0026gt; pageLinkLists = getLinksDataSet(env); // set iterative data set IterativeDataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; iteration = pagesWithRanks.iterate(maxIterations); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; newRanks = iteration // join pages with outgoing edges and distribute rank .join(pageLinkLists).where(0).equalTo(0).flatMap(new JoinVertexWithEdgesMatch()) // collect and sum ranks .groupBy(0).sum(1) // apply dampening factor .map(new Dampener(DAMPENING_FACTOR, numPages)); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; finalPageRanks = iteration.closeWith( newRanks, newRanks.join(iteration).where(0).equalTo(0) // termination condition .filter(new EpsilonFilter())); finalPageRanks.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // User-defined functions public static final class JoinVertexWithEdgesMatch implements FlatJoinFunction\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Long[]\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; { @Override public void join(\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt; page, Tuple2\u0026lt;Long, Long[]\u0026gt; adj, Collector\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; out) { Long[] neighbors = adj.f1; double rank = page.f1; double rankToDistribute = rank / ((double) neigbors.length); for (int i = 0; i \u0026lt; neighbors.length; i++) { out.collect(new Tuple2\u0026lt;Long, Double\u0026gt;(neighbors[i], rankToDistribute)); } } } public static final class Dampener implements MapFunction\u0026lt;Tuple2\u0026lt;Long,Double\u0026gt;, Tuple2\u0026lt;Long,Double\u0026gt;\u0026gt; { private final double dampening, randomJump; public Dampener(double dampening, double numVertices) { this.dampening = dampening; this.randomJump = (1 - dampening) / numVertices; } @Override public Tuple2\u0026lt;Long, Double\u0026gt; map(Tuple2\u0026lt;Long, Double\u0026gt; value) { value.f1 = (value.f1 * dampening) + randomJump; return value; } } public static final class EpsilonFilter implements FilterFunction\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt;\u0026gt; { @Override public boolean filter(Tuple2\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; value) { return Math.abs(value.f0.f1 - value.f1.f1) \u0026gt; EPSILON; } } The PageRank program implements the above example. It requires the following parameters to run: --pages \u0026lt;path\u0026gt; --links \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --numPages \u0026lt;n\u0026gt; --iterations \u0026lt;n\u0026gt;.
Scala // User-defined types case class Link(sourceId: Long, targetId: Long) case class Page(pageId: Long, rank: Double) case class AdjacencyList(sourceId: Long, targetIds: Array[Long]) // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment // read the pages and initial ranks by parsing a CSV file val pages = env.readCsvFile[Page](pagesInputPath) // the links are encoded as an adjacency list: (page-id, Array(neighbor-ids)) val links = env.readCsvFile[Link](linksInputPath) // assign initial ranks to pages val pagesWithRanks = pages.map(p =\u0026gt; Page(p, 1.0 / numPages)) // build adjacency list from link input val adjacencyLists = links // initialize lists .map(e =\u0026gt; AdjacencyList(e.sourceId, Array(e.targetId))) // concatenate lists .groupBy(\u0026#34;sourceId\u0026#34;).reduce { (l1, l2) =\u0026gt; AdjacencyList(l1.sourceId, l1.targetIds ++ l2.targetIds) } // start iteration val finalRanks = pagesWithRanks.iterateWithTermination(maxIterations) { currentRanks =\u0026gt; val newRanks = currentRanks // distribute ranks to target pages .join(adjacencyLists).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;sourceId\u0026#34;) { (page, adjacent, out: Collector[Page]) =\u0026gt; for (targetId \u0026lt;- adjacent.targetIds) { out.collect(Page(targetId, page.rank / adjacent.targetIds.length)) } } // collect ranks and sum them up .groupBy(\u0026#34;pageId\u0026#34;).aggregate(SUM, \u0026#34;rank\u0026#34;) // apply dampening factor .map { p =\u0026gt; Page(p.pageId, (p.rank * DAMPENING_FACTOR) + ((1 - DAMPENING_FACTOR) / numPages)) } // terminate if no rank update was significant val termination = currentRanks.join(newRanks).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;pageId\u0026#34;) { (current, next, out: Collector[Int]) =\u0026gt; // check for significant update if (math.abs(current.rank - next.rank) \u0026gt; EPSILON) out.collect(1) } (newRanks, termination) } val result = finalRanks // emit result result.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) The PageRank program implements the above example. It requires the following parameters to run: --pages \u0026lt;path\u0026gt; --links \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --numPages \u0026lt;n\u0026gt; --iterations \u0026lt;n\u0026gt;.
Input files are plain text files and must be formatted as follows:
Pages represented as an (long) ID separated by new-line characters. For example \u0026quot;1\\n2\\n12\\n42\\n63\\n\u0026quot; gives five pages with IDs 1, 2, 12, 42, and 63. Links are represented as pairs of page IDs which are separated by space characters. Links are separated by new-line characters: For example \u0026quot;1 2\\n2 12\\n1 12\\n42 63\\n\u0026quot; gives four (directed) links (1)-\u0026gt;(2), (2)-\u0026gt;(12), (1)-\u0026gt;(12), and (42)-\u0026gt;(63). For this simple implementation it is required that each page has at least one incoming and one outgoing link (a page can point to itself).
Connected Components # The Connected Components algorithm identifies parts of a larger graph which are connected by assigning all vertices in the same connected part the same component ID. Similar to PageRank, Connected Components is an iterative algorithm. In each step, each vertex propagates its current component ID to all its neighbors. A vertex accepts the component ID from a neighbor, if it is smaller than its own component ID.
This implementation uses a delta iteration: Vertices that have not changed their component ID do not participate in the next step. This yields much better performance, because the later iterations typically deal only with a few outlier vertices.
Java // read vertex and edge data DataSet\u0026lt;Long\u0026gt; vertices = getVertexDataSet(env); DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; edges = getEdgeDataSet(env).flatMap(new UndirectEdge()); // assign the initial component IDs (equal to the vertex ID) DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithInitialId = vertices.map(new DuplicateValue\u0026lt;Long\u0026gt;()); // open a delta iteration DeltaIteration\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; iteration = verticesWithInitialId.iterateDelta(verticesWithInitialId, maxIterations, 0); // apply the step logic: DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; changes = iteration.getWorkset() // join with the edges .join(edges).where(0).equalTo(0).with(new NeighborWithComponentIDJoin()) // select the minimum neighbor component ID .groupBy(0).aggregate(Aggregations.MIN, 1) // update if the component ID of the candidate is smaller .join(iteration.getSolutionSet()).where(0).equalTo(0) .flatMap(new ComponentIdFilter()); // close the delta iteration (delta and new workset are identical) DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; result = iteration.closeWith(changes, changes); // emit result result.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // User-defined functions public static final class DuplicateValue\u0026lt;T\u0026gt; implements MapFunction\u0026lt;T, Tuple2\u0026lt;T, T\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;T, T\u0026gt; map(T vertex) { return new Tuple2\u0026lt;T, T\u0026gt;(vertex, vertex); } } public static final class UndirectEdge implements FlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { Tuple2\u0026lt;Long, Long\u0026gt; invertedEdge = new Tuple2\u0026lt;Long, Long\u0026gt;(); @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; edge, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) { invertedEdge.f0 = edge.f1; invertedEdge.f1 = edge.f0; out.collect(edge); out.collect(invertedEdge); } } public static final class NeighborWithComponentIDJoin implements JoinFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; join(Tuple2\u0026lt;Long, Long\u0026gt; vertexWithComponent, Tuple2\u0026lt;Long, Long\u0026gt; edge) { return new Tuple2\u0026lt;Long, Long\u0026gt;(edge.f1, vertexWithComponent.f1); } } public static final class ComponentIdFilter implements FlatMapFunction\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { @Override public void flatMap(Tuple2\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; value, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) { if (value.f0.f1 \u0026lt; value.f1.f1) { out.collect(value.f0); } } } The ConnectedComponents program implements the above example. It requires the following parameters to run: --vertices \u0026lt;path\u0026gt; --edges \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --iterations \u0026lt;n\u0026gt;.
Scala // set up execution environment val env = ExecutionEnvironment.getExecutionEnvironment // read vertex and edge data // assign the initial components (equal to the vertex id) val vertices = getVerticesDataSet(env).map { id =\u0026gt; (id, id) } // undirected edges by emitting for each input edge the input edges itself and an inverted // version val edges = getEdgesDataSet(env).flatMap { edge =\u0026gt; Seq(edge, (edge._2, edge._1)) } // open a delta iteration val verticesWithComponents = vertices.iterateDelta(vertices, maxIterations, Array(0)) { (s, ws) =\u0026gt; // apply the step logic: join with the edges val allNeighbors = ws.join(edges).where(0).equalTo(0) { (vertex, edge) =\u0026gt; (edge._2, vertex._2) } // select the minimum neighbor val minNeighbors = allNeighbors.groupBy(0).min(1) // update if the component of the candidate is smaller val updatedComponents = minNeighbors.join(s).where(0).equalTo(0) { (newVertex, oldVertex, out: Collector[(Long, Long)]) =\u0026gt; if (newVertex._2 \u0026lt; oldVertex._2) out.collect(newVertex) } // delta and new workset are identical (updatedComponents, updatedComponents) } verticesWithComponents.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) The ConnectedComponents program implements the above example. It requires the following parameters to run: --vertices \u0026lt;path\u0026gt; --edges \u0026lt;path\u0026gt; --output \u0026lt;path\u0026gt; --iterations \u0026lt;n\u0026gt;.
Input files are plain text files and must be formatted as follows:
Vertices represented as IDs and separated by new-line characters. For example \u0026quot;1\\n2\\n12\\n42\\n63\\n\u0026quot; gives five vertices with (1), (2), (12), (42), and (63). Edges are represented as pairs for vertex IDs which are separated by space characters. Edges are separated by new-line characters: For example \u0026quot;1 2\\n2 12\\n1 12\\n42 63\\n\u0026quot; gives four (undirected) links (1)-(2), (2)-(12), (1)-(12), and (42)-(63). Back to top
`}),e.add({id:262,href:"/flink/flink-docs-master/docs/flinkdev/building/",title:"Building Flink from Source",section:"Flink Development",content:` Building Flink from Source # This page covers how to build Flink 1.16-SNAPSHOT from sources.
Build Flink # In order to build Flink you need the source code. Either download the source of a release or clone the git repository.
In addition you need Maven 3 and a JDK (Java Development Kit). Flink requires at least Java 11 to build.
NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies. Maven 3.2.5 creates the libraries properly.
To clone from git, enter:
git clone https://github.com/apache/flink.git The simplest way of building Flink is by running:
mvn clean install -DskipTests This instructs Maven (mvn) to first remove all existing builds (clean) and then create a new Flink binary (install).
To speed up the build you can:
skip tests by using \u0026rsquo; -DskipTests' skip QA plugins and javadoc generation by using the fast Maven profile skip the WebUI compilation by using the skip-webui-build Maven profile use Maven\u0026rsquo;s parallel build feature, e.g., \u0026lsquo;mvn package -T 1C\u0026rsquo; will attempt to build 1 module for each CPU core in parallel. Parallel builds may deadlock due to a bug in the maven-shade-plugin. It is recommended to only use it as a 2 steps process, where you first run mvn validate/test-compile/test in parallel, and then run mvn package/verify/install with a single thread. The build script will be:
mvn clean install -DskipTests -Dfast -Pskip-webui-build -T 1C The fast and skip-webui-build profiles have a significant impact on the build time, particularly on slower storage devices, due to them reading/writing many small files.
Build PyFlink # Prerequisites # Building Flink
If you want to build a PyFlink package that can be used for pip installation, you need to build the Flink project first, as described in Build Flink.
Python version(3.6, 3.7, 3.8 or 3.9) is required
\$ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 Build PyFlink with Cython extension support (optional)
To build PyFlink with Cython extension support, you’ll need a C compiler. It\u0026rsquo;s a little different on how to install the C compiler on different operating systems:
Linux Linux operating systems usually come with GCC pre-installed. Otherwise, you need to install it manually. For example, you can install it with command sudo apt-get install build-essential On Ubuntu or Debian.
Mac OS X To install GCC on Mac OS X, you need to download and install \u0026ldquo;Command Line Tools for Xcode\u0026rdquo;, which is available in Apple’s developer page.
You also need to install the dependencies with following command:
\$ python -m pip install -r flink-python/dev/dev-requirements.txt Installation # Then go to the root directory of flink source code and run this command to build the sdist package and wheel package of apache-flink and apache-flink-libraries:
cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..; The sdist package of apache-flink-libraries will be found under ./flink-python/apache-flink-libraries/dist/. It could be installed as following:
python -m pip install apache-flink-libraries/dist/*.tar.gz The sdist and wheel packages of apache-flink will be found under ./flink-python/dist/. Either of them could be used for installation, such as:
python -m pip install dist/*.whl Dependency Shading # Flink shades away some of the libraries it uses, in order to avoid version clashes with user programs that use different versions of these libraries. Among the shaded libraries are Google Guava, Asm, Apache Curator, Apache HTTP Components, Netty, and others.
The dependency shading mechanism was recently changed in Maven and requires users to build Flink slightly differently, depending on their Maven version:
Maven and 3.2.x It is sufficient to call mvn clean install -DskipTests in the root directory of Flink code base.
Maven 3.3.x The build has to be done in two steps: First in the base directory, then in shaded modules, such as the distribution and the filesystems:
# build overall project mvn clean install -DskipTests # build shaded modules used in dist again, for example: cd flink-filesystems/flink-s3-fs-presto/ mvn clean install -DskipTests # ... and other modules # build dist again to include shaded modules cd flink-dist mvn clean install Note: To check your Maven version, run mvn --version.
Note: We recommend using the latest Maven 3.2.x version for building production-grade Flink distributions, as this is the version the Flink developers are using for the official releases and testing.
Back to top
Scala Versions # Users that purely use the Java APIs and libraries can ignore this section. Flink has APIs, libraries, and runtime modules written in Scala. Users of the Scala API and libraries may have to match the Scala version of Flink with the Scala version of their projects (because Scala is not strictly backwards compatible).
Since version 1.7 Flink builds with Scala version 2.11 (default) and 2.12.
To build Flink against Scala 2.12, issue the following command:
mvn clean install -DskipTests -Dscala-2.12 To build against a specific binary Scala version you can use:
mvn clean install -DskipTests -Dscala-2.12 -Dscala.version=\u0026lt;scala version\u0026gt; Back to top
Encrypted File Systems # If your home directory is encrypted you might encounter a java.io.IOException: File name too long exception. Some encrypted file systems, like encfs used by Ubuntu, do not allow long filenames, which is the cause of this error.
The workaround is to add:
\u0026lt;args\u0026gt; \u0026lt;arg\u0026gt;-Xmax-classfile-name\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;128\u0026lt;/arg\u0026gt; \u0026lt;/args\u0026gt; in the compiler configuration of the pom.xml file of the module causing the error. For example, if the error appears in the flink-yarn module, the above code should be added under the \u0026lt;configuration\u0026gt; tag of scala-maven-plugin. See this issue for more information.
Back to top
`}),e.add({id:263,href:"/flink/flink-docs-master/docs/dev/table/types/",title:"Data Types",section:"Table API \u0026 SQL",content:` Data Types # Flink SQL has a rich set of native data types available to users.
Data Type # A data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of operations.
Flink\u0026rsquo;s data types are similar to the SQL standard\u0026rsquo;s data type terminology but also contain information about the nullability of a value for efficient handling of scalar expressions.
Examples of data types are:
INT INT NOT NULL INTERVAL DAY TO SECOND(3) ROW\u0026lt;myField ARRAY\u0026lt;BOOLEAN\u0026gt;, myOtherField TIMESTAMP(3)\u0026gt; A list of all pre-defined data types can be found below.
Data Types in the Table API # Java/Scala Users of the JVM-based API work with instances of org.apache.flink.table.types.DataType within the Table API or when defining connectors, catalogs, or user-defined functions.
A DataType instance has two responsibilities:
Declaration of a logical type which does not imply a concrete physical representation for transmission or storage but defines the boundaries between JVM-based/Python languages and the table ecosystem. Optional: Giving hints about the physical representation of data to the planner which is useful at the edges to other APIs. For JVM-based languages, all pre-defined data types are available in org.apache.flink.table.api.DataTypes.
Python Users of the Python API work with instances of pyflink.table.types.DataType within the Python Table API or when defining Python user-defined functions.
A DataType instance has such a responsibility:
Declaration of a logical type which does not imply a concrete physical representation for transmission or storage but defines the boundaries between Python languages and the table ecosystem. For Python language, those types are available in pyflink.table.types.DataTypes.
Java It is recommended to add a star import to your table programs for having a fluent API:
import static org.apache.flink.table.api.DataTypes.*; DataType t = INTERVAL(DAY(), SECOND(3)); Scala It is recommended to add a star import to your table programs for having a fluent API:
import org.apache.flink.table.api.DataTypes._ val t: DataType = INTERVAL(DAY(), SECOND(3)) Python from pyflink.table.types import DataTypes t = DataTypes.INTERVAL(DataTypes.DAY(), DataTypes.SECOND(3)) Physical Hints # Physical hints are required at the edges of the table ecosystem where the SQL-based type system ends and programming-specific data types are required. Hints indicate the data format that an implementation expects.
For example, a data source could express that it produces values for logical TIMESTAMPs using a java.sql.Timestamp class instead of using java.time.LocalDateTime which would be the default. With this information, the runtime is able to convert the produced class into its internal data format. In return, a data sink can declare the data format it consumes from the runtime.
Here are some examples of how to declare a bridging conversion class:
Java // tell the runtime to not produce or consume java.time.LocalDateTime instances // but java.sql.Timestamp DataType t = DataTypes.TIMESTAMP(3).bridgedTo(java.sql.Timestamp.class); // tell the runtime to not produce or consume boxed integer arrays // but primitive int arrays DataType t = DataTypes.ARRAY(DataTypes.INT().notNull()).bridgedTo(int[].class); Scala // tell the runtime to not produce or consume java.time.LocalDateTime instances // but java.sql.Timestamp val t: DataType = DataTypes.TIMESTAMP(3).bridgedTo(classOf[java.sql.Timestamp]) // tell the runtime to not produce or consume boxed integer arrays // but primitive int arrays val t: DataType = DataTypes.ARRAY(DataTypes.INT().notNull()).bridgedTo(classOf[Array[Int]]) Attention Please note that physical hints are usually only required if the API is extended. Users of predefined sources/sinks/functions do not need to define such hints. Hints within a table program (e.g. field.cast(TIMESTAMP(3).bridgedTo(Timestamp.class))) are ignored.
List of Data Types # This section lists all pre-defined data types. Java/Scala For the JVM-based Table API those types are also available in org.apache.flink.table.api.DataTypes. Python For the Python Table API, those types are available in pyflink.table.types.DataTypes. The default planner supports the following set of SQL types:
Data Type Remarks for Data Type CHAR VARCHAR STRING BOOLEAN BINARY VARBINARY BYTES DECIMAL Supports fixed precision and scale. TINYINT SMALLINT INTEGER BIGINT FLOAT DOUBLE DATE TIME Supports only a precision of 0. TIMESTAMP TIMESTAMP_LTZ INTERVAL Supports only interval of MONTH and SECOND(3). ARRAY MULTISET MAP ROW RAW Structured types Only exposed in user-defined functions yet. Character Strings # CHAR # Data type of a fixed-length character string.
Declaration
SQL CHAR CHAR(n) Java/Scala DataTypes.CHAR(n) Bridging to JVM Types
Java Type Input Output Remarks java.lang.String X X Default byte[] X X Assumes UTF-8 encoding. org.apache.flink.table.data.StringData X X Internal data structure. Python Not supported. The type can be declared using CHAR(n) where n is the number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
VARCHAR / STRING # Data type of a variable-length character string.
Declaration
SQL VARCHAR VARCHAR(n) STRING Java/Scala DataTypes.VARCHAR(n) DataTypes.STRING() Bridging to JVM Types
Java Type Input Output Remarks java.lang.String X X Default byte[] X X Assumes UTF-8 encoding. org.apache.flink.table.data.StringData X X Internal data structure. Python DataTypes.VARCHAR(n) DataTypes.STRING() Attention The specified maximum number of code points n in DataTypes.VARCHAR(n) must be 2,147,483,647 currently.
The type can be declared using VARCHAR(n) where n is the maximum number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
STRING is a synonym for VARCHAR(2147483647).
Binary Strings # BINARY # Data type of a fixed-length binary string (=a sequence of bytes).
Declaration
SQL BINARY BINARY(n) Java/Scala DataTypes.BINARY(n) Bridging to JVM Types
Java Type Input Output Remarks byte[] X X Default Python Not supported. The type can be declared using BINARY(n) where n is the number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
VARBINARY / BYTES # Data type of a variable-length binary string (=a sequence of bytes).
Declaration
SQL VARBINARY VARBINARY(n) BYTES Java/Scala DataTypes.VARBINARY(n) DataTypes.BYTES() Bridging to JVM Types
Java Type Input Output Remarks byte[] X X Default Python DataTypes.VARBINARY(n) DataTypes.BYTES() Attention The specified maximum number of bytes n in DataTypes.VARBINARY(n) must be 2,147,483,647 currently.
The type can be declared using VARBINARY(n) where n is the maximum number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
BYTES is a synonym for VARBINARY(2147483647).
Exact Numerics # DECIMAL # Data type of a decimal number with fixed precision and scale.
Declaration
SQL DECIMAL DECIMAL(p) DECIMAL(p, s) DEC DEC(p) DEC(p, s) NUMERIC NUMERIC(p) NUMERIC(p, s) Java/Scala DataTypes.DECIMAL(p, s) Bridging to JVM Types
Java Type Input Output Remarks java.math.BigDecimal X X Default org.apache.flink.table.data.DecimalData X X Internal data structure. Python DataTypes.DECIMAL(p, s) Attention The precision and scale specified in DataTypes.DECIMAL(p, s) must be 38 and 18 separately currently.
The type can be declared using DECIMAL(p, s) where p is the number of digits in a number (precision) and s is the number of digits to the right of the decimal point in a number (scale). p must have a value between 1 and 38 (both inclusive). s must have a value between 0 and p (both inclusive). The default value for p is 10. The default value for s is 0.
NUMERIC(p, s) and DEC(p, s) are synonyms for this type.
TINYINT # Data type of a 1-byte signed integer with values from -128 to 127.
Declaration
SQL TINYINT Java/Scala DataTypes.TINYINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Byte X X Default byte X (X) Output only if type is not nullable. Python DataTypes.TINYINT() SMALLINT # Data type of a 2-byte signed integer with values from -32,768 to 32,767.
Declaration
SQL SMALLINT Java/Scala DataTypes.SMALLINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Short X X Default short X (X) Output only if type is not nullable. Python DataTypes.SMALLINT() INT # Data type of a 4-byte signed integer with values from -2,147,483,648 to 2,147,483,647.
Declaration
SQL INT INTEGER Java/Scala DataTypes.INT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Integer X X Default int X (X) Output only if type is not nullable. Python DataTypes.INT() INTEGER is a synonym for this type.
BIGINT # Data type of an 8-byte signed integer with values from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.
Declaration
SQL BIGINT Java/Scala DataTypes.BIGINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Long X X Default long X (X) Output only if type is not nullable. Python DataTypes.BIGINT() Approximate Numerics # FLOAT # Data type of a 4-byte single precision floating point number.
Compared to the SQL standard, the type does not take parameters.
Declaration
SQL FLOAT Java/Scala DataTypes.FLOAT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Float X X Default float X (X) Output only if type is not nullable. Python DataTypes.FLOAT() DOUBLE # Data type of an 8-byte double precision floating point number.
Declaration
SQL DOUBLE DOUBLE PRECISION Java/Scala DataTypes.DOUBLE() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Double X X Default double X (X) Output only if type is not nullable. Python DataTypes.DOUBLE() DOUBLE PRECISION is a synonym for this type.
Date and Time # DATE # Data type of a date consisting of year-month-day with values ranging from 0000-01-01 to 9999-12-31.
Compared to the SQL standard, the range starts at year 0000.
Declaration
SQL DATE Java/Scala DataTypes.DATE() Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalDate X X Default java.sql.Date X X java.lang.Integer X X Describes the number of days since epoch. int X (X) Describes the number of days since epoch.
Output only if type is not nullable. Python DataTypes.DATE() TIME # Data type of a time without time zone consisting of hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 00:00:00.000000000 to 23:59:59.999999999.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.LocalTime. A time with time zone is not provided. Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported. A time with time zone is not provided. Declaration
SQL TIME TIME(p) Java/Scala DataTypes.TIME(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalTime X X Default java.sql.Time X X java.lang.Integer X X Describes the number of milliseconds of the day. int X (X) Describes the number of milliseconds of the day.
Output only if type is not nullable. java.lang.Long X X Describes the number of nanoseconds of the day. long X (X) Describes the number of nanoseconds of the day.
Output only if type is not nullable. Python DataTypes.TIME(p) Attention The precision specified in DataTypes.TIME(p) must be 0 currently.
The type can be declared using TIME(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 0.
TIMESTAMP # Data type of a timestamp without time zone consisting of year-month-day hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 to 9999-12-31 23:59:59.999999999.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.LocalDateTime.
A conversion from and to BIGINT (a JVM long type) is not supported as this would imply a time zone. However, this type is time zone free. For more java.time.Instant-like semantics use TIMESTAMP_LTZ.
Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported.
A conversion from and to BIGINT is not supported as this would imply a time zone. However, this type is time zone free. If you have such a requirement please use TIMESTAMP_LTZ.
Declaration
SQL TIMESTAMP TIMESTAMP(p) TIMESTAMP WITHOUT TIME ZONE TIMESTAMP(p) WITHOUT TIME ZONE Java/Scala DataTypes.TIMESTAMP(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalDateTime X X Default java.sql.Timestamp X X org.apache.flink.table.data.TimestampData X X Internal data structure. Python DataTypes.TIMESTAMP(p) Attention The precision specified in DataTypes.TIMESTAMP(p) must be 3 currently.
The type can be declared using TIMESTAMP(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.
TIMESTAMP(p) WITHOUT TIME ZONE is a synonym for this type.
TIMESTAMP WITH TIME ZONE # Data type of a timestamp with time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.OffsetDateTime. Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported. Compared to TIMESTAMP_LTZ, the time zone offset information is physically stored in every datum. It is used individually for every computation, visualization, or communication to external systems.
Declaration
SQL TIMESTAMP WITH TIME ZONE TIMESTAMP(p) WITH TIME ZONE Java/Scala DataTypes.TIMESTAMP_WITH_TIME_ZONE(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.OffsetDateTime X X Default java.time.ZonedDateTime X Ignores the zone ID. Python Not supported. SQL/Java/Scala The type can be declared using TIMESTAMP(p) WITH TIME ZONE where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6. Python TIMESTAMP_LTZ # Data type of a timestamp with local time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.
SQL/Java/Scala Leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.OffsetDateTime.
Compared to TIMESTAMP WITH TIME ZONE, the time zone offset information is not stored physically in every datum. Instead, the type assumes java.time.Instant semantics in UTC time zone at the edges of the table ecosystem. Every datum is interpreted in the local time zone configured in the current session for computation and visualization.
Python Leap seconds (23:59:60 and 23:59:61) are not supported.
Compared to TIMESTAMP WITH TIME ZONE, the time zone offset information is not stored physically in every datum. Every datum is interpreted in the local time zone configured in the current session for computation and visualization.
This type fills the gap between time zone free and time zone mandatory timestamp types by allowing the interpretation of UTC timestamps according to the configured session time zone.
Declaration
SQL TIMESTAMP_LTZ TIMESTAMP_LTZ(p) TIMESTAMP WITH LOCAL TIME ZONE TIMESTAMP(p) WITH LOCAL TIME ZONE Java/Scala DataTypes.TIMESTAMP_LTZ(p) DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.Instant X X Default java.lang.Integer X X Describes the number of seconds since epoch. int X (X) Describes the number of seconds since epoch.
Output only if type is not nullable. java.lang.Long X X Describes the number of milliseconds since epoch. long X (X) Describes the number of milliseconds since epoch.
Output only if type is not nullable. java.sql.Timestamp X X Describes the number of milliseconds since epoch. org.apache.flink.table.data.TimestampData X X Internal data structure. Python DataTypes.TIMESTAMP_LTZ(p) DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(p) Attention The precision specified in DataTypes.TIMESTAMP_LTZ(p) must be 3 currently.
The type can be declared using TIMESTAMP_LTZ(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.
TIMESTAMP(p) WITH LOCAL TIME ZONE is a synonym for this type.
INTERVAL YEAR TO MONTH # Data type for a group of year-month interval types.
The type must be parameterized to one of the following resolutions:
interval of years, interval of years to months, or interval of months. An interval of year-month consists of +years-months with values ranging from -9999-11 to +9999-11.
The value representation is the same for all types of resolutions. For example, an interval of months of 50 is always represented in an interval-of-years-to-months format (with default year precision): +04-02.
Declaration
SQL INTERVAL YEAR INTERVAL YEAR(p) INTERVAL YEAR(p) TO MONTH INTERVAL MONTH Java/Scala DataTypes.INTERVAL(DataTypes.YEAR()) DataTypes.INTERVAL(DataTypes.YEAR(p)) DataTypes.INTERVAL(DataTypes.YEAR(p), DataTypes.MONTH()) DataTypes.INTERVAL(DataTypes.MONTH()) Bridging to JVM Types
Java Type Input Output Remarks java.time.Period X X Ignores the days part. Default java.lang.Integer X X Describes the number of months. int X (X) Describes the number of months.
Output only if type is not nullable. Python DataTypes.INTERVAL(DataTypes.YEAR()) DataTypes.INTERVAL(DataTypes.YEAR(p)) DataTypes.INTERVAL(DataTypes.YEAR(p), DataTypes.MONTH()) DataTypes.INTERVAL(DataTypes.MONTH()) The type can be declared using the above combinations where p is the number of digits of years (year precision). p must have a value between 1 and 4 (both inclusive). If no year precision is specified, p is equal to 2.
INTERVAL DAY TO SECOND # Data type for a group of day-time interval types.
The type must be parameterized to one of the following resolutions with up to nanosecond precision:
interval of days, interval of days to hours, interval of days to minutes, interval of days to seconds, interval of hours, interval of hours to minutes, interval of hours to seconds, interval of minutes, interval of minutes to seconds, or interval of seconds. An interval of day-time consists of +days hours:months:seconds.fractional with values ranging from -999999 23:59:59.999999999 to +999999 23:59:59.999999999. The value representation is the same for all types of resolutions. For example, an interval of seconds of 70 is always represented in an interval-of-days-to-seconds format (with default precisions): +00 00:01:10.000000.
Declaration
SQL INTERVAL DAY INTERVAL DAY(p1) INTERVAL DAY(p1) TO HOUR INTERVAL DAY(p1) TO MINUTE INTERVAL DAY(p1) TO SECOND(p2) INTERVAL HOUR INTERVAL HOUR TO MINUTE INTERVAL HOUR TO SECOND(p2) INTERVAL MINUTE INTERVAL MINUTE TO SECOND(p2) INTERVAL SECOND INTERVAL SECOND(p2) Java/Scala DataTypes.INTERVAL(DataTypes.DAY()) DataTypes.INTERVAL(DataTypes.DAY(p1)) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.MINUTE(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.SECOND()) DataTypes.INTERVAL(DataTypes.SECOND(p2)) Bridging to JVM Types
Java Type Input Output Remarks java.time.Duration X X Default java.lang.Long X X Describes the number of milliseconds. long X (X) Describes the number of milliseconds.
Output only if type is not nullable. Python DataTypes.INTERVAL(DataTypes.DAY()) DataTypes.INTERVAL(DataTypes.DAY(p1)) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.MINUTE(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.SECOND()) DataTypes.INTERVAL(DataTypes.SECOND(p2)) The type can be declared using the above combinations where p1 is the number of digits of days (day precision) and p2 is the number of digits of fractional seconds (fractional precision). p1 must have a value between 1 and 6 (both inclusive). p2 must have a value between 0 and 9 (both inclusive). If no p1 is specified, it is equal to 2 by default. If no p2 is specified, it is equal to 6 by default.
Constructured Data Types # ARRAY # Data type of an array of elements with same subtype.
Compared to the SQL standard, the maximum cardinality of an array cannot be specified but is fixed at 2,147,483,647. Also, any valid type is supported as a subtype.
Declaration
SQL ARRAY\u0026lt;t\u0026gt; t ARRAY Java/Scala DataTypes.ARRAY(t) Bridging to JVM Types
Java Type Input Output Remarks t[] (X) (X) Depends on the subtype. Default java.util.List\u0026lt;t\u0026gt; X X subclass of java.util.List\u0026lt;t\u0026gt; X org.apache.flink.table.data.ArrayData X X Internal data structure. Python DataTypes.ARRAY(t) The type can be declared using ARRAY\u0026lt;t\u0026gt; where t is the data type of the contained elements.
t ARRAY is a synonym for being closer to the SQL standard. For example, INT ARRAY is equivalent to ARRAY\u0026lt;INT\u0026gt;.
MAP # Data type of an associative array that maps keys (including NULL) to values (including NULL). A map cannot contain duplicate keys; each key can map to at most one value.
There is no restriction of element types; it is the responsibility of the user to ensure uniqueness.
The map type is an extension to the SQL standard.
Declaration
SQL MAP\u0026lt;kt, vt\u0026gt; Java/Scala DataTypes.MAP(kt, vt) Bridging to JVM Types
Java Type Input Output Remarks java.util.Map\u0026lt;kt, vt\u0026gt; X X Default subclass of java.util.Map\u0026lt;kt, vt\u0026gt; X org.apache.flink.table.data.MapData X X Internal data structure. Python DataTypes.MAP(kt, vt) The type can be declared using MAP\u0026lt;kt, vt\u0026gt; where kt is the data type of the key elements and vt is the data type of the value elements.
MULTISET # Data type of a multiset (=bag). Unlike a set, it allows for multiple instances for each of its elements with a common subtype. Each unique value (including NULL) is mapped to some multiplicity.
There is no restriction of element types; it is the responsibility of the user to ensure uniqueness.
Declaration
SQL MULTISET\u0026lt;t\u0026gt; t MULTISET Java/Scala DataTypes.MULTISET(t) Bridging to JVM Types
Java Type Input Output Remarks java.util.Map\u0026lt;t, java.lang.Integer\u0026gt; X X Assigns each value to an integer multiplicity. Default subclass of java.util.Map\u0026lt;t, java.lang.Integer\u0026gt;\u0026gt; X org.apache.flink.table.data.MapData X X Internal data structure. Python DataTypes.MULTISET(t) The type can be declared using MULTISET\u0026lt;t\u0026gt; where t is the data type of the contained elements.
t MULTISET is a synonym for being closer to the SQL standard. For example, INT MULTISET is equivalent to MULTISET\u0026lt;INT\u0026gt;.
ROW # Data type of a sequence of fields.
A field consists of a field name, field type, and an optional description. The most specific type of a row of a table is a row type. In this case, each column of the row corresponds to the field of the row type that has the same ordinal position as the column.
Compared to the SQL standard, an optional field description simplifies the handling with complex structures.
A row type is similar to the STRUCT type known from other non-standard-compliant frameworks.
Declaration
SQL ROW\u0026lt;n0 t0, n1 t1, ...\u0026gt; ROW\u0026lt;n0 t0 \u0026#39;d0\u0026#39;, n1 t1 \u0026#39;d1\u0026#39;, ...\u0026gt; ROW(n0 t0, n1 t1, ...\u0026gt; ROW(n0 t0 \u0026#39;d0\u0026#39;, n1 t1 \u0026#39;d1\u0026#39;, ...) Java/Scala DataTypes.ROW(DataTypes.FIELD(n0, t0), DataTypes.FIELD(n1, t1), ...) DataTypes.ROW(DataTypes.FIELD(n0, t0, d0), DataTypes.FIELD(n1, t1, d1), ...) Bridging to JVM Types
Java Type Input Output Remarks org.apache.flink.types.Row X X Default org.apache.flink.table.data.RowData X X Internal data structure. Python DataTypes.ROW([DataTypes.FIELD(n0, t0), DataTypes.FIELD(n1, t1), ...]) DataTypes.ROW([DataTypes.FIELD(n0, t0, d0), DataTypes.FIELD(n1, t1, d1), ...]) The type can be declared using ROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt; where n is the unique name of a field, t is the logical type of a field, d is the description of a field.
ROW(...) is a synonym for being closer to the SQL standard. For example, ROW(myField INT, myOtherField BOOLEAN) is equivalent to ROW\u0026lt;myField INT, myOtherField BOOLEAN\u0026gt;.
User-Defined Data Types # Java/Scala Attention User-defined data types are not fully supported yet. They are currently (as of Flink 1.11) only exposed as unregistered structured types in parameters and return types of functions.
A structured type is similar to an object in an object-oriented programming language. It contains zero, one or more attributes. Each attribute consists of a name and a type.
There are two kinds of structured types:
Types that are stored in a catalog and are identified by a catalog identifier (like cat.db.MyType). Those are equal to the SQL standard definition of structured types.
Anonymously defined, unregistered types (usually reflectively extracted) that are identified by an implementation class (like com.myorg.model.MyType). Those are useful when programmatically defining a table program. They enable reusing existing JVM classes without manually defining the schema of a data type again.
Registered Structured Types # Currently, registered structured types are not supported. Thus, they cannot be stored in a catalog or referenced in a CREATE TABLE DDL.
Unregistered Structured Types # Unregistered structured types can be created from regular POJOs (Plain Old Java Objects) using automatic reflective extraction.
The implementation class of a structured type must meet the following requirements:
The class must be globally accessible which means it must be declared public, static, and not abstract. The class must offer a default constructor with zero arguments or a full constructor that assigns all fields. All fields of the class must be readable by either public declaration or a getter that follows common coding style such as getField(), isField(), field(). All fields of the class must be writable by either public declaration, fully assigning constructor, or a setter that follows common coding style such as setField(...), field(...). All fields must be mapped to a data type either implicitly via reflective extraction or explicitly using the @DataTypeHint annotations. Fields that are declared static or transient are ignored. The reflective extraction supports arbitrary nesting of fields as long as a field type does not (transitively) refer to itself.
The declared field class (e.g. public int age;) must be contained in the list of supported JVM bridging classes defined for every data type in this document (e.g. java.lang.Integer or int for INT).
For some classes an annotation is required in order to map the class to a data type (e.g. @DataTypeHint(\u0026quot;DECIMAL(10, 2)\u0026quot;) to assign a fixed precision and scale for java.math.BigDecimal).
Python Declaration
Java class User { // extract fields automatically public int age; public String name; // enrich the extraction with precision information public @DataTypeHint(\u0026#34;DECIMAL(10, 2)\u0026#34;) BigDecimal totalBalance; // enrich the extraction with forcing using RAW types public @DataTypeHint(\u0026#34;RAW\u0026#34;) Class\u0026lt;?\u0026gt; modelClass; } DataTypes.of(User.class); Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default org.apache.flink.types.Row X X Represent the structured type as a row. org.apache.flink.table.data.RowData X X Internal data structure. Scala case class User( // extract fields automatically age: Int, name: String, // enrich the extraction with precision information @DataTypeHint(\u0026#34;DECIMAL(10, 2)\u0026#34;) totalBalance: java.math.BigDecimal, // enrich the extraction with forcing using a RAW type @DataTypeHint(\u0026#34;RAW\u0026#34;) modelClass: Class[_] ) DataTypes.of(classOf[User]) Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default org.apache.flink.types.Row X X Represent the structured type as a row. org.apache.flink.table.data.RowData X X Internal data structure. Python Not supported. Other Data Types # BOOLEAN # Data type of a boolean with a (possibly) three-valued logic of TRUE, FALSE, and UNKNOWN.
Declaration
SQL BOOLEAN Java/Scala DataTypes.BOOLEAN() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Boolean X X Default boolean X (X) Output only if type is not nullable. Python DataTypes.BOOLEAN() RAW # Data type of an arbitrary serialized type. This type is a black box within the table ecosystem and is only deserialized at the edges.
The raw type is an extension to the SQL standard.
Declaration
SQL RAW(\u0026#39;class\u0026#39;, \u0026#39;snapshot\u0026#39;) Java/Scala DataTypes.RAW(class, serializer) DataTypes.RAW(class) Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default byte[] X org.apache.flink.table.data.RawValueData X X Internal data structure. Python Not supported. SQL/Java/Scala The type can be declared using RAW('class', 'snapshot') where class is the originating class and snapshot is the serialized TypeSerializerSnapshot in Base64 encoding. Usually, the type string is not declared directly but is generated while persisting the type.
In the API, the RAW type can be declared either by directly supplying a Class + TypeSerializer or by passing Class and letting the framework extract Class + TypeSerializer from there.
Python NULL # Data type for representing untyped NULL values.
The null type is an extension to the SQL standard. A null type has no other value except NULL, thus, it can be cast to any nullable type similar to JVM semantics.
This type helps in representing unknown types in API calls that use a NULL literal as well as bridging to formats such as JSON or Avro that define such a type as well.
This type is not very useful in practice and is just mentioned here for completeness.
Declaration
SQL NULL Java/Scala DataTypes.NULL() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Object X X Default any class (X) Any non-primitive type. Python Not supported. Casting # Flink Table API and SQL can perform casting between a defined input type and target type. While some casting operations can always succeed regardless of the input value, others can fail at runtime (i.e. where there is no way to create a value for the target type). For example, it is always possible to convert INT to STRING, but you cannot always convert a STRING to INT.
During the planning stage, the query validator rejects queries for invalid type pairs with a ValidationException, e.g. when trying to cast a TIMESTAMP to an INTERVAL. Valid type pairs that can fail at runtime will be accepted by the query validator, but requires the user to correctly handle failures.
In Flink Table API and SQL, casting can be performed by using one of the two following built-in functions:
CAST: The regular cast function defined by the SQL standard. It can fail the job if the cast operation is fallible and the provided input is not valid. The type inference will preserve the nullability of the input type. TRY_CAST: An extension to the regular cast function which returns NULL in case the cast operation fails. Its return type is always nullable. For example:
CAST(\u0026#39;42\u0026#39; AS INT) --- returns 42 of type INT NOT NULL CAST(NULL AS VARCHAR) --- returns NULL of type VARCHAR CAST(\u0026#39;non-number\u0026#39; AS INT) --- throws an exception and fails the job TRY_CAST(\u0026#39;42\u0026#39; AS INT) --- returns 42 of type INT TRY_CAST(NULL AS VARCHAR) --- returns NULL of type VARCHAR TRY_CAST(\u0026#39;non-number\u0026#39; AS INT) --- returns NULL of type INT COALESCE(TRY_CAST(\u0026#39;non-number\u0026#39; AS INT), 0) --- returns 0 of type INT NOT NULL The matrix below describes the supported cast pairs, where \u0026ldquo;Y\u0026rdquo; means supported, \u0026ldquo;!\u0026rdquo; means fallible, \u0026ldquo;N\u0026rdquo; means unsupported:
Input\\Target CHAR¹/VARCHAR¹/STRING BINARY¹/VARBINARY¹/BYTES BOOLEAN DECIMAL TINYINT SMALLINT INTEGER BIGINT FLOAT DOUBLE DATE TIME TIMESTAMP TIMESTAMP_LTZ INTERVAL ARRAY MULTISET MAP ROW STRUCTURED RAW CHAR/VARCHAR/STRING Y ! ! ! ! ! ! ! ! ! ! ! ! ! N N N N N N N BINARY/VARBINARY/BYTES Y Y N N N N N N N N N N N N N N N N N N N BOOLEAN Y N Y Y Y Y Y Y Y Y N N N N N N N N N N N DECIMAL Y N N Y Y Y Y Y Y Y N N N N N N N N N N N TINYINT Y N Y Y Y Y Y Y Y Y N N N² N² N N N N N N N SMALLINT Y N Y Y Y Y Y Y Y Y N N N² N² N N N N N N N INTEGER Y N Y Y Y Y Y Y Y Y N N N² N² Y⁵ N N N N N N BIGINT Y N Y Y Y Y Y Y Y Y N N N² N² Y⁶ N N N N N N FLOAT Y N N Y Y Y Y Y Y Y N N N N N N N N N N N DOUBLE Y N N Y Y Y Y Y Y Y N N N N N N N N N N N DATE Y N N N N N N N N N Y N Y Y N N N N N N N TIME Y N N N N N N N N N N Y Y Y N N N N N N N TIMESTAMP Y N N N N N N N N N Y Y Y Y N N N N N N N TIMESTAMP_LTZ Y N N N N N N N N N Y Y Y Y N N N N N N N INTERVAL Y N N N N N Y⁵ Y⁶ N N N N N N Y N N N N N N ARRAY Y N N N N N N N N N N N N N N !³ N N N N N MULTISET Y N N N N N N N N N N N N N N N !³ N N N N MAP Y N N N N N N N N N N N N N N N N !³ N N N ROW Y N N N N N N N N N N N N N N N N N !³ N N STRUCTURED Y N N N N N N N N N N N N N N N N N N !³ N RAW Y ! N N N N N N N N N N N N N N N N N N Y⁴ Notes:
All the casting to constant length or variable length will also trim and pad accordingly to the type definition. TO_TIMESTAMP and TO_TIMESTAMP_LTZ must be used instead of CAST/TRY_CAST. Supported iff the children type pairs are supported. Fallible iff the children type pairs are fallible. Supported iff the RAW class and serializer are equals. Supported iff INTERVAL is a MONTH TO YEAR range. Supported iff INTERVAL is a DAY TO TIME range. Also note that a cast of a NULL value will always return NULL, regardless of whether the function used is CAST or TRY_CAST.
Legacy casting # Pre Flink 1.15 casting behaviour can be enabled by setting table.exec.legacy-cast-behaviour to enabled. In Flink 1.15 this flag is disabled by default.
In particular, this will:
Disable trimming/padding for casting to CHAR/VARCHAR/BINARY/VARBINARY CAST never fails but returns NULL, behaving as TRY_CAST but without inferring the correct type Formatting of some casting to CHAR/VARCHAR/STRING produces slightly different results. We discourage the use of this flag and we strongly suggest for new projects to keep this flag disabled and use the new casting behaviour. This flag will be removed in the next Flink versions. Data Type Extraction # Java/Scala At many locations in the API, Flink tries to automatically extract data type from class information using reflection to avoid repetitive manual schema work. However, extracting a data type reflectively is not always successful because logical information might be missing. Therefore, it might be necessary to add additional information close to a class or field declaration for supporting the extraction logic.
The following table lists classes that can be implicitly mapped to a data type without requiring further information.
If you intend to implement classes in Scala, it is recommended to use boxed types (e.g. java.lang.Integer) instead of Scala\u0026rsquo;s primitives. Scala\u0026rsquo;s primitives (e.g. Int or Double) are compiled to JVM primitives (e.g. int/double) and result in NOT NULL semantics as shown in the table below. Furthermore, Scala primitives that are used in generics (e.g. java.util.Map[Int, Double]) are erased during compilation and lead to class information similar to java.util.Map[java.lang.Object, java.lang.Object].
Class Data Type java.lang.String STRING java.lang.Boolean BOOLEAN boolean BOOLEAN NOT NULL java.lang.Byte TINYINT byte TINYINT NOT NULL java.lang.Short SMALLINT short SMALLINT NOT NULL java.lang.Integer INT int INT NOT NULL java.lang.Long BIGINT long BIGINT NOT NULL java.lang.Float FLOAT float FLOAT NOT NULL java.lang.Double DOUBLE double DOUBLE NOT NULL java.sql.Date DATE java.time.LocalDate DATE java.sql.Time TIME(0) java.time.LocalTime TIME(9) java.sql.Timestamp TIMESTAMP(9) java.time.LocalDateTime TIMESTAMP(9) java.time.OffsetDateTime TIMESTAMP(9) WITH TIME ZONE java.time.Instant TIMESTAMP_LTZ(9) java.time.Duration INTERVAL SECOND(9) java.time.Period INTERVAL YEAR(4) TO MONTH byte[] BYTES T[] ARRAY\u0026lt;T\u0026gt; java.util.Map\u0026lt;K, V\u0026gt; MAP\u0026lt;K, V\u0026gt; structured type T anonymous structured type T Other JVM bridging classes mentioned in this document require a @DataTypeHint annotation.
Data type hints can parameterize or replace the default extraction logic of individual function parameters and return types, structured classes, or fields of structured classes. An implementer can choose to what extent the default extraction logic should be modified by declaring a @DataTypeHint annotation.
The @DataTypeHint annotation provides a set of optional hint parameters. Some of those parameters are shown in the following example. More information can be found in the documentation of the annotation class.
Python Java import org.apache.flink.table.annotation.DataTypeHint; class User { // defines an INT data type with a default conversion class \`java.lang.Integer\` public @DataTypeHint(\u0026#34;INT\u0026#34;) Object o; // defines a TIMESTAMP data type of millisecond precision with an explicit conversion class public @DataTypeHint(value = \u0026#34;TIMESTAMP(3)\u0026#34;, bridgedTo = java.sql.Timestamp.class) Object o; // enrich the extraction with forcing using a RAW type public @DataTypeHint(\u0026#34;RAW\u0026#34;) Class\u0026lt;?\u0026gt; modelClass; // defines that all occurrences of java.math.BigDecimal (also in nested fields) will be // extracted as DECIMAL(12, 2) public @DataTypeHint(defaultDecimalPrecision = 12, defaultDecimalScale = 2) AccountStatement stmt; // defines that whenever a type cannot be mapped to a data type, instead of throwing // an exception, always treat it as a RAW type public @DataTypeHint(allowRawGlobally = HintFlag.TRUE) ComplexModel model; } Scala import org.apache.flink.table.annotation.DataTypeHint class User { // defines an INT data type with a default conversion class \`java.lang.Integer\` @DataTypeHint(\u0026#34;INT\u0026#34;) var o: AnyRef // defines a TIMESTAMP data type of millisecond precision with an explicit conversion class @DataTypeHint(value = \u0026#34;TIMESTAMP(3)\u0026#34;, bridgedTo = java.sql.Timestamp.class) var o: AnyRef // enrich the extraction with forcing using a RAW type @DataTypeHint(\u0026#34;RAW\u0026#34;) var modelClass: Class[_] // defines that all occurrences of java.math.BigDecimal (also in nested fields) will be // extracted as DECIMAL(12, 2) @DataTypeHint(defaultDecimalPrecision = 12, defaultDecimalScale = 2) var stmt: AccountStatement // defines that whenever a type cannot be mapped to a data type, instead of throwing // an exception, always treat it as a RAW type @DataTypeHint(allowRawGlobally = HintFlag.TRUE) var model: ComplexModel } Python Not supported. Back to top
`}),e.add({id:264,href:"/flink/flink-docs-master/docs/dev/python/datastream/operators/",title:"Operators",section:"DataStream API",content:" "}),e.add({id:265,href:"/flink/flink-docs-master/docs/dev/datastream/execution/packaging/",title:"Program Packaging",section:"Managing Execution",content:` Program Packaging and Distributed Execution # As described earlier, Flink programs can be executed on clusters by using a remote environment. Alternatively, programs can be packaged into JAR Files (Java Archives) for execution. Packaging the program is a prerequisite to executing them through the command line interface.
Packaging Programs # To support execution from a packaged JAR file via the command line or web interface, a program must use the environment obtained by StreamExecutionEnvironment.getExecutionEnvironment(). This environment will act as the cluster\u0026rsquo;s environment when the JAR is submitted to the command line or web interface. If the Flink program is invoked differently than through these interfaces, the environment will act like a local environment.
To package the program, simply export all involved classes as a JAR file. The JAR file\u0026rsquo;s manifest must point to the class that contains the program\u0026rsquo;s entry point (the class with the public main method). The simplest way to do this is by putting the main-class entry into the manifest (such as main-class: org.apache.flinkexample.MyProgram). The main-class attribute is the same one that is used by the Java Virtual Machine to find the main method when executing a JAR files through the command java -jar pathToTheJarFile. Most IDEs offer to include that attribute automatically when exporting JAR files.
Summary # The overall procedure to invoke a packaged program consists of two steps:
The JAR\u0026rsquo;s manifest is searched for a main-class or program-class attribute. If both attributes are found, the program-class attribute takes precedence over the main-class attribute. Both the command line and the web interface support a parameter to pass the entry point class name manually for cases where the JAR manifest contains neither attribute.
The system invokes the main method of the class.
Back to top
`}),e.add({id:266,href:"/flink/flink-docs-master/docs/dev/python/table_api_tutorial/",title:"Table API Tutorial",section:"Python API",content:` Table API Tutorial # Apache Flink offers a Table API as a unified, relational API for batch and stream processing, i.e., queries are executed with the same semantics on unbounded, real-time streams or bounded, batch data sets and produce the same results. The Table API in Flink is commonly used to ease the definition of data analytics, data pipelining, and ETL applications.
What Will You Be Building? # In this tutorial, you will learn how to build a pure Python Flink Table API pipeline. The pipeline will read data from an input csv file, compute the word frequency and write the results to an output file.
Prerequisites # This walkthrough assumes that you have some familiarity with Python, but you should be able to follow along even if you come from a different programming language. It also assumes that you are familiar with basic relational concepts such as SELECT and GROUP BY clauses.
Help, I’m Stuck! # If you get stuck, check out the community support resources. In particular, Apache Flink\u0026rsquo;s user mailing list consistently ranks as one of the most active of any Apache project and a great way to get help quickly.
How To Follow Along # If you want to follow along, you will require a computer with:
Java 11 Python 3.6, 3.7, 3.8 or 3.9 Using Python Table API requires installing PyFlink, which is available on PyPI and can be easily installed using pip.
\$ python -m pip install apache-flink Once PyFlink is installed, you can move on to write a Python Table API job.
Writing a Flink Python Table API Program # Table API applications begin by declaring a table environment. This serves as the main entry point for interacting with the Flink runtime. It can be used for setting execution parameters such as restart strategy, default parallelism, etc. The table config allows setting Table API specific configurations.
t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode()) t_env.get_config().set(\u0026#34;parallelism.default\u0026#34;, \u0026#34;1\u0026#34;) You can now create the source and sink tables:
t_env.create_temporary_table( \u0026#39;source\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .build()) .option(\u0026#39;path\u0026#39;, input_path) .format(\u0026#39;csv\u0026#39;) .build()) tab = t_env.from_path(\u0026#39;source\u0026#39;) t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .option(\u0026#39;path\u0026#39;, output_path) .format(FormatDescriptor.for_format(\u0026#39;canal-json\u0026#39;) .build()) .build()) You can also use the TableEnvironment.execute_sql() method to register a source/sink table defined in DDL:
my_source_ddl = \u0026#34;\u0026#34;\u0026#34; create table source ( word STRING ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;.format(input_path) my_sink_ddl = \u0026#34;\u0026#34;\u0026#34; create table sink ( word STRING, \`count\` BIGINT ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;canal-json\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;.format(output_path) t_env.execute_sql(my_source_ddl) t_env.execute_sql(my_sink_ddl) This registers a table named source and a table named sink in the table environment. The table source has only one column, word, and it consumes strings read from file specified by input_path. The table sink has two columns, word and count, and writes data to the file specified by output_path.
You can now create a job which reads input from table source, performs some transformations, and writes the results to table sink.
Finally, you must execute the actual Flink Python Table API job. All operations, such as creating sources, transformations and sinks are lazy. Only when execute_insert(sink_name) is called, the job will be submitted for execution.
@udtf(result_types=[DataTypes.STRING()]) def split(line: Row): for s in line[0].split(): yield Row(s) # compute word count tab.flat_map(split).alias(\u0026#39;word\u0026#39;) \\ .group_by(col(\u0026#39;word\u0026#39;)) \\ .select(col(\u0026#39;word\u0026#39;), lit(1).count) \\ .execute_insert(\u0026#39;sink\u0026#39;) \\ .wait() The complete code so far:
import argparse import logging import sys from pyflink.common import Row from pyflink.table import (EnvironmentSettings, TableEnvironment, TableDescriptor, Schema, DataTypes, FormatDescriptor) from pyflink.table.expressions import lit, col from pyflink.table.udf import udtf word_count_data = [\u0026#34;To be, or not to be,--that is the question:--\u0026#34;, \u0026#34;Whether \u0026#39;tis nobler in the mind to suffer\u0026#34;, \u0026#34;The slings and arrows of outrageous fortune\u0026#34;, \u0026#34;Or to take arms against a sea of troubles,\u0026#34;, \u0026#34;And by opposing end them?--To die,--to sleep,--\u0026#34;, \u0026#34;No more; and by a sleep to say we end\u0026#34;, \u0026#34;The heartache, and the thousand natural shocks\u0026#34;, \u0026#34;That flesh is heir to,--\u0026#39;tis a consummation\u0026#34;, \u0026#34;Devoutly to be wish\u0026#39;d. To die,--to sleep;--\u0026#34;, \u0026#34;To sleep! perchance to dream:--ay, there\u0026#39;s the rub;\u0026#34;, \u0026#34;For in that sleep of death what dreams may come,\u0026#34;, \u0026#34;When we have shuffled off this mortal coil,\u0026#34;, \u0026#34;Must give us pause: there\u0026#39;s the respect\u0026#34;, \u0026#34;That makes calamity of so long life;\u0026#34;, \u0026#34;For who would bear the whips and scorns of time,\u0026#34;, \u0026#34;The oppressor\u0026#39;s wrong, the proud man\u0026#39;s contumely,\u0026#34;, \u0026#34;The pangs of despis\u0026#39;d love, the law\u0026#39;s delay,\u0026#34;, \u0026#34;The insolence of office, and the spurns\u0026#34;, \u0026#34;That patient merit of the unworthy takes,\u0026#34;, \u0026#34;When he himself might his quietus make\u0026#34;, \u0026#34;With a bare bodkin? who would these fardels bear,\u0026#34;, \u0026#34;To grunt and sweat under a weary life,\u0026#34;, \u0026#34;But that the dread of something after death,--\u0026#34;, \u0026#34;The undiscover\u0026#39;d country, from whose bourn\u0026#34;, \u0026#34;No traveller returns,--puzzles the will,\u0026#34;, \u0026#34;And makes us rather bear those ills we have\u0026#34;, \u0026#34;Than fly to others that we know not of?\u0026#34;, \u0026#34;Thus conscience does make cowards of us all;\u0026#34;, \u0026#34;And thus the native hue of resolution\u0026#34;, \u0026#34;Is sicklied o\u0026#39;er with the pale cast of thought;\u0026#34;, \u0026#34;And enterprises of great pith and moment,\u0026#34;, \u0026#34;With this regard, their currents turn awry,\u0026#34;, \u0026#34;And lose the name of action.--Soft you now!\u0026#34;, \u0026#34;The fair Ophelia!--Nymph, in thy orisons\u0026#34;, \u0026#34;Be all my sins remember\u0026#39;d.\u0026#34;] def word_count(input_path, output_path): t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode()) # write all the data to one file t_env.get_config().set(\u0026#34;parallelism.default\u0026#34;, \u0026#34;1\u0026#34;) # define the source if input_path is not None: t_env.create_temporary_table( \u0026#39;source\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .build()) .option(\u0026#39;path\u0026#39;, input_path) .format(\u0026#39;csv\u0026#39;) .build()) tab = t_env.from_path(\u0026#39;source\u0026#39;) else: print(\u0026#34;Executing word_count example with default input data set.\u0026#34;) print(\u0026#34;Use --input to specify file input.\u0026#34;) tab = t_env.from_elements(map(lambda i: (i,), word_count_data), DataTypes.ROW([DataTypes.FIELD(\u0026#39;line\u0026#39;, DataTypes.STRING())])) # define the sink if output_path is not None: t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .option(\u0026#39;path\u0026#39;, output_path) .format(FormatDescriptor.for_format(\u0026#39;canal-json\u0026#39;) .build()) .build()) else: print(\u0026#34;Printing result to stdout. Use --output to specify output path.\u0026#34;) t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;print\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .build()) @udtf(result_types=[DataTypes.STRING()]) def split(line: Row): for s in line[0].split(): yield Row(s) # compute word count tab.flat_map(split).alias(\u0026#39;word\u0026#39;) \\ .group_by(col(\u0026#39;word\u0026#39;)) \\ .select(col(\u0026#39;word\u0026#39;), lit(1).count) \\ .execute_insert(\u0026#39;sink\u0026#39;) \\ .wait() # remove .wait if submitting to a remote cluster, refer to # https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster # for more details if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\u0026#34;%(message)s\u0026#34;) parser = argparse.ArgumentParser() parser.add_argument( \u0026#39;--input\u0026#39;, dest=\u0026#39;input\u0026#39;, required=False, help=\u0026#39;Input file to process.\u0026#39;) parser.add_argument( \u0026#39;--output\u0026#39;, dest=\u0026#39;output\u0026#39;, required=False, help=\u0026#39;Output file to write results to.\u0026#39;) argv = sys.argv[1:] known_args, _ = parser.parse_known_args(argv) word_count(known_args.input, known_args.output) Executing a Flink Python Table API Program # You can run this example on the command line:
\$ python word_count.py The command builds and runs the Python Table API program in a local mini cluster. You can also submit the Python Table API program to a remote cluster, you can refer Job Submission Examples for more details.
Finally, you can see the execution results similar to the following:
+I[To, 1] +I[be,, 1] +I[or, 1] +I[not, 1] ... This should get you started with writing your own Flink Python Table API programs. You can also refer to PyFlink Examples for more examples. To learn more about the Python Table API, you can refer Flink Python API Docs for more details.
`}),e.add({id:267,href:"/flink/flink-docs-master/docs/dev/python/datastream_tutorial/",title:"DataStream API Tutorial",section:"Python API",content:` DataStream API Tutorial # Apache Flink offers a DataStream API for building robust, stateful streaming applications. It provides fine-grained control over state and time, which allows for the implementation of advanced event-driven systems. In this step-by-step guide, you’ll learn how to build a simple streaming application with PyFlink and the DataStream API.
What Will You Be Building? # In this tutorial, you will learn how to write a simple Python DataStream pipeline. The pipeline will read data from a csv file, compute the word frequency and write the results to an output file.
Prerequisites # This walkthrough assumes that you have some familiarity with Python, but you should be able to follow along even if you come from a different programming language.
Help, I’m Stuck! # If you get stuck, check out the community support resources. In particular, Apache Flink\u0026rsquo;s user mailing list consistently ranks as one of the most active of any Apache project and a great way to get help quickly.
How To Follow Along # If you want to follow along, you will require a computer with:
Java 11 Python 3.6, 3.7, 3.8 or 3.9 Using Python DataStream API requires installing PyFlink, which is available on PyPI and can be easily installed using pip.
\$ python -m pip install apache-flink Once PyFlink is installed, you can move on to write a Python DataStream job.
Writing a Flink Python DataStream API Program # DataStream API applications begin by declaring an execution environment (StreamExecutionEnvironment), the context in which a streaming program is executed. This is what you will use to set the properties of your job (e.g. default parallelism, restart strategy), create your sources and finally trigger the execution of the job.
env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) env.set_parallelism(1) Once a StreamExecutionEnvironment is created, you can use it to declare your source. Sources ingest data from external systems, such as Apache Kafka, Rabbit MQ, or Apache Pulsar, into Flink Jobs.
To keep things simple, this walkthrough uses a source which reads data from a file.
ds = env.from_source( source=FileSource.for_record_stream_format(StreamFormat.text_line_format(), input_path) .process_static_file_set().build(), watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;file_source\u0026#34; ) You can now perform transformations on this data stream, or just write the data to an external system using a sink. This walkthrough uses the FileSink sink connector to write the data into a file.
ds.sink_to( sink=FileSink.for_row_format( base_path=output_path, encoder=Encoder.simple_string_encoder()) .with_output_file_config( OutputFileConfig.builder() .with_part_prefix(\u0026#34;prefix\u0026#34;) .with_part_suffix(\u0026#34;.ext\u0026#34;) .build()) .with_rolling_policy(RollingPolicy.default_rolling_policy()) .build() ) def split(line): yield from line.split() # 计算词频 ds = ds.flat_map(split) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) The last step is to execute the actual PyFlink DataStream API job. PyFlink applications are built lazily and shipped to the cluster for execution only once fully formed. To execute an application, you simply call env.execute().
env.execute() The complete code so far:
import argparse import logging import sys from pyflink.common import WatermarkStrategy, Encoder, Types from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy word_count_data = [\u0026#34;To be, or not to be,--that is the question:--\u0026#34;, \u0026#34;Whether \u0026#39;tis nobler in the mind to suffer\u0026#34;, \u0026#34;The slings and arrows of outrageous fortune\u0026#34;, \u0026#34;Or to take arms against a sea of troubles,\u0026#34;, \u0026#34;And by opposing end them?--To die,--to sleep,--\u0026#34;, \u0026#34;No more; and by a sleep to say we end\u0026#34;, \u0026#34;The heartache, and the thousand natural shocks\u0026#34;, \u0026#34;That flesh is heir to,--\u0026#39;tis a consummation\u0026#34;, \u0026#34;Devoutly to be wish\u0026#39;d. To die,--to sleep;--\u0026#34;, \u0026#34;To sleep! perchance to dream:--ay, there\u0026#39;s the rub;\u0026#34;, \u0026#34;For in that sleep of death what dreams may come,\u0026#34;, \u0026#34;When we have shuffled off this mortal coil,\u0026#34;, \u0026#34;Must give us pause: there\u0026#39;s the respect\u0026#34;, \u0026#34;That makes calamity of so long life;\u0026#34;, \u0026#34;For who would bear the whips and scorns of time,\u0026#34;, \u0026#34;The oppressor\u0026#39;s wrong, the proud man\u0026#39;s contumely,\u0026#34;, \u0026#34;The pangs of despis\u0026#39;d love, the law\u0026#39;s delay,\u0026#34;, \u0026#34;The insolence of office, and the spurns\u0026#34;, \u0026#34;That patient merit of the unworthy takes,\u0026#34;, \u0026#34;When he himself might his quietus make\u0026#34;, \u0026#34;With a bare bodkin? who would these fardels bear,\u0026#34;, \u0026#34;To grunt and sweat under a weary life,\u0026#34;, \u0026#34;But that the dread of something after death,--\u0026#34;, \u0026#34;The undiscover\u0026#39;d country, from whose bourn\u0026#34;, \u0026#34;No traveller returns,--puzzles the will,\u0026#34;, \u0026#34;And makes us rather bear those ills we have\u0026#34;, \u0026#34;Than fly to others that we know not of?\u0026#34;, \u0026#34;Thus conscience does make cowards of us all;\u0026#34;, \u0026#34;And thus the native hue of resolution\u0026#34;, \u0026#34;Is sicklied o\u0026#39;er with the pale cast of thought;\u0026#34;, \u0026#34;And enterprises of great pith and moment,\u0026#34;, \u0026#34;With this regard, their currents turn awry,\u0026#34;, \u0026#34;And lose the name of action.--Soft you now!\u0026#34;, \u0026#34;The fair Ophelia!--Nymph, in thy orisons\u0026#34;, \u0026#34;Be all my sins remember\u0026#39;d.\u0026#34;] def word_count(input_path, output_path): env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) # write all the data to one file env.set_parallelism(1) # define the source if input_path is not None: ds = env.from_source( source=FileSource.for_record_stream_format(StreamFormat.text_line_format(), input_path) .process_static_file_set().build(), watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;file_source\u0026#34; ) else: print(\u0026#34;Executing word_count example with default input data set.\u0026#34;) print(\u0026#34;Use --input to specify file input.\u0026#34;) ds = env.from_collection(word_count_data) def split(line): yield from line.split() # compute word count ds = ds.flat_map(split) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) # define the sink if output_path is not None: ds.sink_to( sink=FileSink.for_row_format( base_path=output_path, encoder=Encoder.simple_string_encoder()) .with_output_file_config( OutputFileConfig.builder() .with_part_prefix(\u0026#34;prefix\u0026#34;) .with_part_suffix(\u0026#34;.ext\u0026#34;) .build()) .with_rolling_policy(RollingPolicy.default_rolling_policy()) .build() ) else: print(\u0026#34;Printing result to stdout. Use --output to specify output path.\u0026#34;) ds.print() # submit for execution env.execute() if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\u0026#34;%(message)s\u0026#34;) parser = argparse.ArgumentParser() parser.add_argument( \u0026#39;--input\u0026#39;, dest=\u0026#39;input\u0026#39;, required=False, help=\u0026#39;Input file to process.\u0026#39;) parser.add_argument( \u0026#39;--output\u0026#39;, dest=\u0026#39;output\u0026#39;, required=False, help=\u0026#39;Output file to write results to.\u0026#39;) argv = sys.argv[1:] known_args, _ = parser.parse_known_args(argv) word_count(known_args.input, known_args.output) Executing a Flink Python DataStream API Program # Now that you defined your PyFlink program, you can run the example you just created on the command line:
\$ python word_count.py The command builds and runs your PyFlink program in a local mini cluster. You can alternatively submit it to a remote cluster using the instructions detailed in Job Submission Examples.
Finally, you can see the execution results similar to the following:
(a,5) (Be,1) (Is,1) (No,2) ... This walkthrough gives you the foundations to get started writing your own PyFlink DataStream API programs. You can also refer to PyFlink Examples for more examples. To learn more about the Python DataStream API, please refer to Flink Python API Docs for more details.
`}),e.add({id:268,href:"/flink/flink-docs-master/docs/dev/table/timezone/",title:"Time Zone",section:"Table API \u0026 SQL",content:` Time Zone # Flink provides rich data types for Date and Time, including DATE, TIME, TIMESTAMP, TIMESTAMP_LTZ, INTERVAL YEAR TO MONTH, INTERVAL DAY TO SECOND (please see Date and Time for detailed information). Flink supports setting time zone in session level (please see table.local-time-zone for detailed information). These timestamp data types and time zone support of Flink make it easy to process business data across time zones.
TIMESTAMP vs TIMESTAMP_LTZ # TIMESTAMP type # TIMESTAMP(p) is an abbreviation for TIMESTAMP(p) WITHOUT TIME ZONE, the precision p supports range is from 0 to 9, 6 by default. TIMESTAMP describes a timestamp represents year, month, day, hour, minute, second and fractional seconds. TIMESTAMP can be specified from a string literal, e.g. Flink SQL\u0026gt; SELECT TIMESTAMP \u0026#39;1970-01-01 00:00:04.001\u0026#39;; +-------------------------+ | 1970-01-01 00:00:04.001 | +-------------------------+ TIMESTAMP_LTZ type # TIMESTAMP_LTZ(p) is an abbreviation for TIMESTAMP(p) WITH LOCAL TIME ZONE, the precision p supports range is from 0 to 9, 6 by default. TIMESTAMP_LTZ describes an absolute time point on the time-line, it stores a long value representing epoch-milliseconds and an int representing nanosecond-of-millisecond. The epoch time is measured from the standard Java epoch of 1970-01-01T00:00:00Z. Every datum of TIMESTAMP_LTZ type is interpreted in the local time zone configured in the current session for computation and visualization. TIMESTAMP_LTZ has no literal representation and thus can not specify from literal, it can derives from a long epoch time(e.g. The long time produced by Java System.currentTimeMillis()) Flink SQL\u0026gt; CREATE VIEW T1 AS SELECT TO_TIMESTAMP_LTZ(4001, 3); Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM T1; +---------------------------+ | TO_TIMESTAMP_LTZ(4001, 3) | +---------------------------+ | 1970-01-01 00:00:04.001 | +---------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM T1; +---------------------------+ | TO_TIMESTAMP_LTZ(4001, 3) | +---------------------------+ | 1970-01-01 08:00:04.001 | +---------------------------+ TIMESTAMP_LTZ can be used in cross time zones business because the absolute time point (e.g. above 4001 milliseconds) describes a same instantaneous point in different time zones. Giving a background that at a same time point, the System.currentTimeMillis() of all machines in the world returns same value (e.g. the 4001 milliseconds in above example), this is absolute time point meaning. Time Zone Usage # The local time zone defines current session time zone id. You can config the time zone in Sql Client or Applications.
SQL Client -- set to UTC time zone Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; -- set to Shanghai time zone Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; -- set to Los_Angeles time zone Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;America/Los_Angeles\u0026#39;; Java EnvironmentSettings envSetting = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(envSetting); // set to UTC time zone tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;UTC\u0026#34;)); // set to Shanghai time zone tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;Asia/Shanghai\u0026#34;)); // set to Los_Angeles time zone tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;America/Los_Angeles\u0026#34;)); Scala val envSetting = EnvironmentSettings.inStreamingMode() val tEnv = TableEnvironment.create(envSetting) // set to UTC time zone tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;UTC\u0026#34;)) // set to Shanghai time zone tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;Asia/Shanghai\u0026#34;)) // set to Los_Angeles time zone tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;America/Los_Angeles\u0026#34;)) Python env_setting = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_setting) # set to UTC time zone t_env.get_config().set_local_timezone(\u0026#34;UTC\u0026#34;) # set to Shanghai time zone t_env.get_config().set_local_timezone(\u0026#34;Asia/Shanghai\u0026#34;) # set to Los_Angeles time zone t_env.get_config().set_local_timezone(\u0026#34;America/Los_Angeles\u0026#34;) The session time zone is useful in Flink SQL, the main usages are:
Decide time functions return value # The following time functions are influenced by the configured time zone:
LOCALTIME LOCALTIMESTAMP CURRENT_DATE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_ROW_TIMESTAMP() NOW() PROCTIME() Flink SQL\u0026gt; SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; Flink SQL\u0026gt; CREATE VIEW MyView1 AS SELECT LOCALTIME, LOCALTIMESTAMP, CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_ROW_TIMESTAMP(), NOW(), PROCTIME(); Flink SQL\u0026gt; DESC MyView1; +------------------------+-----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +------------------------+-----------------------------+-------+-----+--------+-----------+ | LOCALTIME | TIME(0) | false | | | | | LOCALTIMESTAMP | TIMESTAMP(3) | false | | | | | CURRENT_DATE | DATE | false | | | | | CURRENT_TIME | TIME(0) | false | | | | | CURRENT_TIMESTAMP | TIMESTAMP_LTZ(3) | false | | | | |CURRENT_ROW_TIMESTAMP() | TIMESTAMP_LTZ(3) | false | | | | | NOW() | TIMESTAMP_LTZ(3) | false | | | | | PROCTIME() | TIMESTAMP_LTZ(3) *PROCTIME* | false | | | | +------------------------+-----------------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView1; +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | LOCALTIME | LOCALTIMESTAMP | CURRENT_DATE | CURRENT_TIME | CURRENT_TIMESTAMP | CURRENT_ROW_TIMESTAMP() | NOW() | PROCTIME() | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | 15:18:36 | 2021-04-15 15:18:36.384 | 2021-04-15 | 15:18:36 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView1; +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | LOCALTIME | LOCALTIMESTAMP | CURRENT_DATE | CURRENT_TIME | CURRENT_TIMESTAMP | CURRENT_ROW_TIMESTAMP() | NOW() | PROCTIME() | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | 23:18:36 | 2021-04-15 23:18:36.384 | 2021-04-15 | 23:18:36 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ TIMESTAMP_LTZ string representation # The session timezone is used when represents a TIMESTAMP_LTZ value to string format, i.e print the value, cast the value to STRING type, cast the value to TIMESTAMP, cast a TIMESTAMP value to TIMESTAMP_LTZ:
Flink SQL\u0026gt; CREATE VIEW MyView2 AS SELECT TO_TIMESTAMP_LTZ(4001, 3) AS ltz, TIMESTAMP \u0026#39;1970-01-01 00:00:01.001\u0026#39; AS ntz; Flink SQL\u0026gt; DESC MyView2; +------+------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +------+------------------+-------+-----+--------+-----------+ | ltz | TIMESTAMP_LTZ(3) | true | | | | | ntz | TIMESTAMP(3) | false | | | | +------+------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView2; +-------------------------+-------------------------+ | ltz | ntz | +-------------------------+-------------------------+ | 1970-01-01 00:00:04.001 | 1970-01-01 00:00:01.001 | +-------------------------+-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView2; +-------------------------+-------------------------+ | ltz | ntz | +-------------------------+-------------------------+ | 1970-01-01 08:00:04.001 | 1970-01-01 00:00:01.001 | +-------------------------+-------------------------+ Flink SQL\u0026gt; CREATE VIEW MyView3 AS SELECT ltz, CAST(ltz AS TIMESTAMP(3)), CAST(ltz AS STRING), ntz, CAST(ntz AS TIMESTAMP_LTZ(3)) FROM MyView2; Flink SQL\u0026gt; DESC MyView3; +-------------------------------+------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +-------------------------------+------------------+-------+-----+--------+-----------+ | ltz | TIMESTAMP_LTZ(3) | true | | | | | CAST(ltz AS TIMESTAMP(3)) | TIMESTAMP(3) | true | | | | | CAST(ltz AS STRING) | STRING | true | | | | | ntz | TIMESTAMP(3) | false | | | | | CAST(ntz AS TIMESTAMP_LTZ(3)) | TIMESTAMP_LTZ(3) | false | | | | +-------------------------------+------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SELECT * FROM MyView3; +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ | ltz | CAST(ltz AS TIMESTAMP(3)) | CAST(ltz AS STRING) | ntz | CAST(ntz AS TIMESTAMP_LTZ(3)) | +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ | 1970-01-01 08:00:04.001 | 1970-01-01 08:00:04.001 | 1970-01-01 08:00:04.001 | 1970-01-01 00:00:01.001 | 1970-01-01 00:00:01.001 | +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ Time Attribute and Time Zone # Please see Time Attribute for more information about time attribute.
Processing Time and Time Zone # Flink SQL defines process time attribute by function PROCTIME(), the function return type is TIMESTAMP_LTZ.
Before Flink 1.13, the function return type of PROCTIME() is TIMESTAMP, and the return value is the TIMESTAMP in UTC time zone, e.g. the wall-clock shows 2021-03-01 12:00:00 at Shanghai, however the PROCTIME() displays 2021-03-01 04:00:00 which is wrong. Flink 1.13 fixes this issue and uses TIMESTAMP_LTZ type as return type of PROCTIME(), users don\u0026rsquo;t need to deal time zone problems anymore. The PROCTIME() always represents your local timestamp value, using TIMESTAMP_LTZ type can also support DayLight Saving Time well.
Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT PROCTIME(); +-------------------------+ | PROCTIME() | +-------------------------+ | 2021-04-15 14:48:31.387 | +-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT PROCTIME(); +-------------------------+ | PROCTIME() | +-------------------------+ | 2021-04-15 22:48:31.387 | +-------------------------+ Flink SQL\u0026gt; CREATE TABLE MyTable1 ( item STRING, price DOUBLE, proctime as PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView3 AS SELECT TUMBLE_START(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_PROCTIME(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_proctime, item, MAX(price) as max_price FROM MyTable1 GROUP BY TUMBLE(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView3; +-----------------+-----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +-----------------+-----------------------------+-------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | false | | | | | window_end | TIMESTAMP(3) | false | | | | | window_proctime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +-----------------+-----------------------------+-------+-----+--------+-----------+ Use the following command to ingest data for MyTable1 in a terminal:
\u0026gt; nc -lk 9999 A,1.1 B,1.2 A,1.8 B,2.5 C,3.8 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView3; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_procime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.005 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.007 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.007 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView3; Returns the different window start, window end and window proctime compared to calculation in UTC timezone.
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_procime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.005 | A | 1.8 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.007 | B | 2.5 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.007 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Processing time window is non-deterministic, so each run will get different windows and different aggregations. The above example is just for explaining how time zone affects processing time window. Event Time and Time Zone # Flink supports defining event time attribute on TIMESTAMP column and TIMESTAMP_LTZ column.
Event Time Attribute on TIMESTAMP # If the timestamp data in the source is represented as year-month-day-hour-minute-second, usually a string value without time-zone information, e.g. 2020-04-15 20:13:40.564, it\u0026rsquo;s recommended to define the event time attribute as a TIMESTAMP column:
Flink SQL\u0026gt; CREATE TABLE MyTable2 ( item STRING, price DOUBLE, ts TIMESTAMP(3), -- TIMESTAMP data type WATERMARK FOR ts AS ts - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView4 AS SELECT TUMBLE_START(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_ROWTIME(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_rowtime, item, MAX(price) as max_price FROM MyTable2 GROUP BY TUMBLE(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView4; +----------------+------------------------+------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +----------------+------------------------+------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | true | | | | | window_end | TIMESTAMP(3) | true | | | | | window_rowtime | TIMESTAMP(3) *ROWTIME* | true | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +----------------+------------------------+------+-----+--------+-----------+ Use the following command to ingest data for MyTable2 in a terminal:
\u0026gt; nc -lk 9999 A,1.1,2021-04-15 14:01:00 B,1.2,2021-04-15 14:02:00 A,1.8,2021-04-15 14:03:00 B,2.5,2021-04-15 14:04:00 C,3.8,2021-04-15 14:05:00 C,3.8,2021-04-15 14:11:00 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView4; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView4; Returns the same window start, window end and window rowtime compared to calculation in UTC timezone.
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Event Time Attribute on TIMESTAMP_LTZ # If the timestamp data in the source is represented as a epoch time, usually a long value, e.g. 1618989564564, it\u0026rsquo;s recommended to define event time attribute as a TIMESTAMP_LTZ column.
Flink SQL\u0026gt; CREATE TABLE MyTable3 ( item STRING, price DOUBLE, ts BIGINT, -- long time value in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView5 AS SELECT TUMBLE_START(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_ROWTIME(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_rowtime, item, MAX(price) as max_price FROM MyTable3 GROUP BY TUMBLE(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView5; +----------------+----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +----------------+----------------------------+-------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | false | | | | | window_end | TIMESTAMP(3) | false | | | | | window_rowtime | TIMESTAMP_LTZ(3) *ROWTIME* | true | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +----------------+----------------------------+-------+-----+--------+-----------+ The input data of MyTable3 is:
A,1.1,1618495260000 # The corresponding utc timestamp is 2021-04-15 14:01:00 B,1.2,1618495320000 # The corresponding utc timestamp is 2021-04-15 14:02:00 A,1.8,1618495380000 # The corresponding utc timestamp is 2021-04-15 14:03:00 B,2.5,1618495440000 # The corresponding utc timestamp is 2021-04-15 14:04:00 C,3.8,1618495500000 # The corresponding utc timestamp is 2021-04-15 14:05:00 C,3.8,1618495860000 # The corresponding utc timestamp is 2021-04-15 14:11:00 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView5; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView5; Returns the different window start, window end and window rowtime compared to calculation in UTC timezone.
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | A | 1.8 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | B | 2.5 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Daylight Saving Time Support # Flink SQL supports defining time attributes on TIMESTAMP_LTZ column, base on this, Flink SQL gracefully uses TIMESTAMP and TIMESTAMP_LTZ type in window processing to support the Daylight Saving Time.
Flink use timestamp literal to split the window and assigns window to data according to the epoch time of the each row. It means Flink uses TIMESTAMP type for window start and window end (e.g. TUMBLE_START and TUMBLE_END), uses TIMESTAMP_LTZ for window time attribute (e.g. TUMBLE_PROCTIME, TUMBLE_ROWTIME). Given an example of tumble window, the DaylightTime in Los_Angeles starts at time 2021-03-14 02:00:00:
long epoch1 = 1615708800000L; // 2021-03-14 00:00:00 long epoch2 = 1615712400000L; // 2021-03-14 01:00:00 long epoch3 = 1615716000000L; // 2021-03-14 03:00:00, skip one hour (2021-03-14 02:00:00) long epoch4 = 1615719600000L; // 2021-03-14 04:00:00 The tumble window [2021-03-14 00:00:00, 2021-03-14 00:04:00] will collect 3 hours\u0026rsquo; data in Los_angele time zone, but it collect 4 hours\u0026rsquo; data in other non-DST time zones, what user to do is only define time attribute on TIMESTAMP_LTZ column.
All windows in Flink like Hop window, Session window, Cumulative window follow this way, and all operations in Flink SQL support TIMESTAMP_LTZ well, thus Flink gracefully supports the Daylight Saving Time zone. Difference between Batch and Streaming Mode # The following time functions:
LOCALTIME LOCALTIMESTAMP CURRENT_DATE CURRENT_TIME CURRENT_TIMESTAMP NOW() Flink evaluates their values according to execution mode. They are evaluated for each record in streaming mode. But in batch mode, they are evaluated once as the query starts and uses the same result for every row.
The following time functions are evaluated for each record no matter in batch or streaming mode:
CURRENT_ROW_TIMESTAMP() PROCTIME() Back to top
`}),e.add({id:269,href:"/flink/flink-docs-master/docs/dev/python/table/",title:"Table API",section:"Python API",content:""}),e.add({id:270,href:"/flink/flink-docs-master/docs/dev/python/datastream/data_types/",title:"Data Types",section:"DataStream API",content:` Data Types # In Apache Flink\u0026rsquo;s Python DataStream API, a data type describes the type of a value in the DataStream ecosystem. It can be used to declare input and output types of operations and informs the system how to serailize elements.
Pickle Serialization # If the type has not been declared, data would be serialized or deserialized using Pickle. For example, the program below specifies no data types.
from pyflink.datastream import StreamExecutionEnvironment def processing(): env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(1) env.from_collection(collection=[(1, \u0026#39;aaa\u0026#39;), (2, \u0026#39;bbb\u0026#39;)]) \\ .map(lambda record: (record[0]+1, record[1].upper())) \\ .print() # note: print to stdout on the worker machine env.execute() if __name__ == \u0026#39;__main__\u0026#39;: processing() However, types need to be specified when:
Passing Python records to Java operations. Improve serialization and deserialization performance. Passing Python records to Java operations # Since Java operators or functions can not identify Python data, types need to be provided to help to convert Python types to Java types for processing. For example, types need to be provided if you want to output data using the FileSink which is implemented in Java.
from pyflink.common.serialization import Encoder from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink def file_sink(): env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(1) env.from_collection(collection=[(1, \u0026#39;aaa\u0026#39;), (2, \u0026#39;bbb\u0026#39;)]) \\ .map(lambda record: (record[0]+1, record[1].upper()), output_type=Types.ROW([Types.INT(), Types.STRING()])) \\ .add_sink(FileSink .for_row_format(\u0026#39;/tmp/output\u0026#39;, Encoder.simple_string_encoder()) .build()) env.execute() if __name__ == \u0026#39;__main__\u0026#39;: file_sink() Improve serialization and deserialization performance # Even though data can be serialized and deserialized through Pickle, performance will be better if types are provided. Explicit types allow PyFlink to use efficient serializers when moving records through the pipeline.
Supported Data Types # You can use pyflink.common.typeinfo.Types to define types in Python DataStream API. The table below shows the types supported now and how to define them:
PyFlink Type Python Type Java Type Types.BOOLEAN() bool java.lang.Boolean Types.BYTE() int java.lang.Byte Types.SHORT() int java.lang.Short Types.INT() int java.lang.Integer Types.LONG() int java.lang.Long Types.FLOAT() float java.lang.Float Types.DOUBLE() float java.lang.Double Types.CHAR() str java.lang.Character Types.STRING() str java.lang.String Types.BIG_INT() int java.math.BigInteger Types.BIG_DEC() decimal.Decimal java.math.BigDecimal Types.INSTANT() pyflink.common.time.Instant java.time.Instant Types.TUPLE() tuple org.apache.flink.api.java.tuple.Tuple0 ~ org.apache.flink.api.java.tuple.Tuple25 Types.ROW() pyflink.common.Row org.apache.flink.types.Row Types.ROW_NAMED() pyflink.common.Row org.apache.flink.types.Row Types.MAP() dict java.util.Map Types.PICKLED_BYTE_ARRAY() The actual unpickled Python object byte[] Types.SQL_DATE() datetime.date java.sql.Date Types.SQL_TIME() datetime.time java.sql.Time Types.SQL_TIMESTAMP() datetime.datetime java.sql.Timestamp Types.LIST() list of Python object java.util.List The table below shows the array types supported:
PyFlink Array Type Python Type Java Type Types.PRIMITIVE_ARRAY(Types.BYTE()) bytes byte[] Types.PRIMITIVE_ARRAY(Types.BOOLEAN()) list of bool boolean[] Types.PRIMITIVE_ARRAY(Types.SHORT()) list of int short[] Types.PRIMITIVE_ARRAY(Types.INT()) list of int int[] Types.PRIMITIVE_ARRAY(Types.LONG()) list of int long[] Types.PRIMITIVE_ARRAY(Types.FLOAT()) list of float float[] Types.PRIMITIVE_ARRAY(Types.DOUBLE()) list of float double[] Types.PRIMITIVE_ARRAY(Types.CHAR()) list of str char[] Types.BASIC_ARRAY(Types.BYTE()) list of int java.lang.Byte[] Types.BASIC_ARRAY(Types.BOOLEAN()) list of bool java.lang.Boolean[] Types.BASIC_ARRAY(Types.SHORT()) list of int java.lang.Short[] Types.BASIC_ARRAY(Types.INT()) list of int java.lang.Integer[] Types.BASIC_ARRAY(Types.LONG()) list of int java.lang.Long[] Types.BASIC_ARRAY(Types.FLOAT()) list of float java.lang.Float[] Types.BASIC_ARRAY(Types.DOUBLE()) list of float java.lang.Double[] Types.BASIC_ARRAY(Types.CHAR()) list of str java.lang.Character[] Types.BASIC_ARRAY(Types.STRING()) list of str java.lang.String[] Types.OBJECT_ARRAY() list of Python object Array `}),e.add({id:271,href:"/flink/flink-docs-master/docs/dev/python/datastream/",title:"DataStream API",section:"Python API",content:""}),e.add({id:272,href:"/flink/flink-docs-master/docs/dev/python/table/intro_to_table_api/",title:"Intro to the Python Table API",section:"Table API",content:` Intro to the Python Table API # This document is a short introduction to the PyFlink Table API, which is used to help novice users quickly understand the basic usage of PyFlink Table API. For advanced usage, please refer to other documents in this user guide.
Common Structure of Python Table API Program # All Table API and SQL programs, both batch and streaming, follow the same pattern. The following code example shows the common structure of Table API and SQL programs.
from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col # 1. create a TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # 2. create source Table table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE datagen ( id INT, data STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39; = \u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 3. create sink Table table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print ( id INT, data STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 4. query from source table and perform calculations # create a Table from a Table API query: source_table = table_env.from_path(\u0026#34;datagen\u0026#34;) # or create a Table from a SQL query: # source_table = table_env.sql_query(\u0026#34;SELECT * FROM datagen\u0026#34;) result_table = source_table.select(col(\u0026#34;id\u0026#34;) + 1, col(\u0026#34;data\u0026#34;)) # 5. emit query result to sink table # emit a Table API result Table to a sink table: result_table.execute_insert(\u0026#34;print\u0026#34;).wait() # or emit results via SQL query: # table_env.execute_sql(\u0026#34;INSERT INTO print SELECT * FROM datagen\u0026#34;).wait() Back to top
Create a TableEnvironment # TableEnvironment is a central concept of the Table API and SQL integration. The following code example shows how to create a TableEnvironment:
from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # or create a batch TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) For more details about the different ways to create a TableEnvironment, please refer to the TableEnvironment Documentation.
TableEnvironment is responsible for:
Table management: Creating Tables, listing Tables, Conversion between Table and DataStream, etc. User-defined function management: User-defined function registration, dropping, listing, etc. See General User-defined Functions and Vectorized User-defined Functions for more details about Python user-defined functions. Executing SQL queries: See Write SQL Queries for more details. Job configuration: See Python Configuration for more details. Python dependency management: See Dependency Management for more details. Job submission: See Emit Results for more details. Back to top
Create Tables # Table is a core component of the Python Table API. A Table object describes a pipeline of data transformations. It does not contain the data itself in any way. Instead, it describes how to read data from a table source, and how to eventually write data to a table sink. The declared pipeline can be printed, optimized, and eventually executed in a cluster. The pipeline can work with bounded or unbounded streams which enables both streaming and batch scenarios.
A Table is always bound to a specific TableEnvironment. It is not possible to combine tables from different TableEnvironments in same query, e.g., to join or union them.
Create using a List Object # You can create a Table from a list object, this is usually used when writing examples or unit tests.
from pyflink.table import EnvironmentSettings, TableEnvironment # create a batch TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) table.execute().print() The results are as following:
+----------------------+--------------------------------+ | _1 | _2 | +----------------------+--------------------------------+ | 1 | Hi | | 2 | Hello | +----------------------+--------------------------------+ You can also create a Table with specified column names:
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table.execute().print() The results are as following:
+----------------------+--------------------------------+ | id | data | +----------------------+--------------------------------+ | 1 | Hi | | 2 | Hello | +----------------------+--------------------------------+ By default, the table schema is extracted from the data automatically. If the automatically generated table schema isn\u0026rsquo;t as expected, you can also specify it manually:
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) # by default, the type of the \u0026#34;id\u0026#34; column is BIGINT print(\u0026#39;By default the type of the \u0026#34;id\u0026#34; column is %s.\u0026#39; % table.get_schema().get_field_data_type(\u0026#34;id\u0026#34;)) from pyflink.table import DataTypes table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.TINYINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) # now the type of the \u0026#34;id\u0026#34; column is set as TINYINT print(\u0026#39;Now the type of the \u0026#34;id\u0026#34; column is %s.\u0026#39; % table.get_schema().get_field_data_type(\u0026#34;id\u0026#34;)) The results are as following:
By default the type of the \u0026#34;id\u0026#34; column is BIGINT. Now the type of the \u0026#34;id\u0026#34; column is TINYINT. Create using DDL statements # You can also create a Table using SQL DDL statements. It represents a Table which reads data from the specified external storage.
from pyflink.table import EnvironmentSettings, TableEnvironment # create a stream TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE random_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;3\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;6\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = table_env.from_path(\u0026#34;random_source\u0026#34;) table.execute().print() The results are as following:
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | +----+----------------------+--------+ Create using TableDescriptor # TableDescriptor is another way to define a Table. It\u0026rsquo;s equivalent to SQL DDL statements.
from pyflink.table import EnvironmentSettings, TableEnvironment, TableDescriptor, Schema, DataTypes # create a stream TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.create_temporary_table( \u0026#39;random_source\u0026#39;, TableDescriptor.for_connector(\u0026#39;datagen\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;id\u0026#39;, DataTypes.BIGINT()) .column(\u0026#39;data\u0026#39;, DataTypes.TINYINT()) .build()) .option(\u0026#39;fields.id.kind\u0026#39;, \u0026#39;sequence\u0026#39;) .option(\u0026#39;fields.id.start\u0026#39;, \u0026#39;1\u0026#39;) .option(\u0026#39;fields.id.end\u0026#39;, \u0026#39;3\u0026#39;) .option(\u0026#39;fields.data.kind\u0026#39;, \u0026#39;sequence\u0026#39;) .option(\u0026#39;fields.data.start\u0026#39;, \u0026#39;4\u0026#39;) .option(\u0026#39;fields.data.end\u0026#39;, \u0026#39;6\u0026#39;) .build()) table = table_env.from_path(\u0026#34;random_source\u0026#34;) table.execute().print() The results are as following:
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | +----+----------------------+--------+ Create using a Catalog # TableEnvironment maintains a map of catalogs of tables which are created with an identifier.
The tables in a catalog may either be temporary, and tied to the lifecycle of a single Flink session, or permanent, and visible across multiple Flink sessions.
The tables and views created via SQL DDL, e.g. \u0026ldquo;create table \u0026hellip;\u0026rdquo; and \u0026ldquo;create view \u0026hellip;\u0026rdquo; are also stored in a catalog.
You can directly access the tables in a catalog via SQL.
If you want to use tables from a catalog with the Table API, you can use the \u0026ldquo;from_path\u0026rdquo; method to create the Table API objects:
# prepare the catalog # register Table API tables in the catalog table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#39;source_table\u0026#39;, table) # create Table API table from catalog new_table = table_env.from_path(\u0026#39;source_table\u0026#39;) new_table.execute().print() The results are as following:
+----+----------------------+--------------------------------+ | op | id | data | +----+----------------------+--------------------------------+ | +I | 1 | Hi | | +I | 2 | Hello | +----+----------------------+--------------------------------+ Back to top
Write Queries # Write Table API Queries # The Table object offers many methods for applying relational operations. These methods return new Table objects representing the result of applying the relational operations on the input Table. These relational operations may be composed of multiple method calls, such as table.group_by(...).select(...).
The Table API documentation describes all Table API operations that are supported on streaming and batch tables.
The following example shows a simple Table API aggregation query:
from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col # using batch table environment to execute the queries env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) orders = table_env.from_elements([(\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 10), (\u0026#39;Rose\u0026#39;, \u0026#39;ENGLAND\u0026#39;, 30), (\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 20)], [\u0026#39;name\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;revenue\u0026#39;]) # compute revenue for all customers from France revenue = orders \\ .select(col(\u0026#34;name\u0026#34;), col(\u0026#34;country\u0026#34;), col(\u0026#34;revenue\u0026#34;)) \\ .where(col(\u0026#34;country\u0026#34;) == \u0026#39;FRANCE\u0026#39;) \\ .group_by(col(\u0026#34;name\u0026#34;)) \\ .select(col(\u0026#34;name\u0026#34;), col(\u0026#34;country\u0026#34;).sum.alias(\u0026#39;rev_sum\u0026#39;)) revenue.execute().print() The results are as following:
+--------------------------------+----------------------+ | name | rev_sum | +--------------------------------+----------------------+ | Jack | 30 | +--------------------------------+----------------------+ The Row-based Operations are also supported in Python Table API, which include Map Operation, FlatMap Operation, Aggregate Operation and FlatAggregate Operation.
The following example shows a simple row-based operation query:
from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table import DataTypes from pyflink.table.udf import udf import pandas as pd # using batch table environment to execute the queries env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) orders = table_env.from_elements([(\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 10), (\u0026#39;Rose\u0026#39;, \u0026#39;ENGLAND\u0026#39;, 30), (\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 20)], [\u0026#39;name\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;revenue\u0026#39;]) map_function = udf(lambda x: pd.concat([x.name, x.revenue * 10], axis=1), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;revenue\u0026#34;, DataTypes.BIGINT())]), func_type=\u0026#34;pandas\u0026#34;) orders.map(map_function).execute().print() The results are as following:
+--------------------------------+----------------------+ | name | revenue | +--------------------------------+----------------------+ | Jack | 100 | | Rose | 300 | | Jack | 200 | +--------------------------------+----------------------+ Write SQL Queries # Flink\u0026rsquo;s SQL integration is based on Apache Calcite, which implements the SQL standard. SQL queries are specified as Strings.
The SQL documentation describes Flink\u0026rsquo;s SQL support for streaming and batch tables.
The following example shows a simple SQL aggregation query:
from pyflink.table import EnvironmentSettings, TableEnvironment # use a stream TableEnvironment to execute the queries env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE random_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;8\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;11\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print_sink ( id BIGINT, data_sum TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; INSERT INTO print_sink SELECT id, sum(data) as data_sum FROM (SELECT id / 2 as id, data FROM random_source) WHERE id \u0026gt; 1 GROUP BY id \u0026#34;\u0026#34;\u0026#34;).wait() The results are as following:
2\u0026gt; +I(4,11) 6\u0026gt; +I(2,8) 8\u0026gt; +I(3,10) 6\u0026gt; -U(2,8) 8\u0026gt; -U(3,10) 6\u0026gt; +U(2,15) 8\u0026gt; +U(3,19) In fact, this shows the change logs received by the print sink. The output format of a change log is:
{subtask id}\u0026gt; {message type}{string format of the value} For example, \u0026ldquo;2\u0026gt; +I(4,11)\u0026rdquo; means this message comes from the 2nd subtask, and \u0026ldquo;+I\u0026rdquo; means it is an insert message. \u0026ldquo;(4, 11)\u0026rdquo; is the content of the message. In addition, \u0026ldquo;-U\u0026rdquo; means a retract record (i.e. update-before), which means this message should be deleted or retracted from the sink. \u0026ldquo;+U\u0026rdquo; means this is an update record (i.e. update-after), which means this message should be updated or inserted by the sink.
So, we get this result from the change logs above:
(4, 11) (2, 15) (3, 19) Mix the Table API and SQL # The Table objects used in Table API and the tables used in SQL can be freely converted to each other.
The following example shows how to use a Table object in SQL:
# create a sink table to emit results table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE table_sink ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # convert the Table API table to a SQL view table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#39;table_api_table\u0026#39;, table) # emit the Table API table table_env.execute_sql(\u0026#34;INSERT INTO table_sink SELECT * FROM table_api_table\u0026#34;).wait() The results are as following:
6\u0026gt; +I(1,Hi) 6\u0026gt; +I(2,Hello) And the following example shows how to use SQL tables in the Table API:
# create a sql source table table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE sql_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;7\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # convert the sql table to Table API table table = table_env.from_path(\u0026#34;sql_source\u0026#34;) # or create the table from a sql query # table = table_env.sql_query(\u0026#34;SELECT * FROM sql_source\u0026#34;) # emit the table table.execute().print() The results are as following:
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | | +I | 4 | 7 | +----+----------------------+--------+ Back to top
Emit Results # Print the Table # You can call the TableResult.print method to print the content of the Table to console. This is usually used when you want to preview the table.
# prepare source tables source = table_env.from_elements([(1, \u0026#34;Hi\u0026#34;, \u0026#34;Hello\u0026#34;), (2, \u0026#34;Hello\u0026#34;, \u0026#34;Hello\u0026#34;)], [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]) # Get TableResult table_result = table_env.execute_sql(\u0026#34;select a + 1, b, c from %s\u0026#34; % source) # Print the table table_result.print() The results are as following:
+----+----------------------+--------------------------------+--------------------------------+ | op | EXPR\$0 | b | c | +----+----------------------+--------------------------------+--------------------------------+ | +I | 2 | Hi | Hello | | +I | 3 | Hello | Hello | +----+----------------------+--------------------------------+--------------------------------+ Note It will trigger the materialization of the table and collect table content to the memory of the client, it\u0026rsquo;s a good practice to limit the number of rows collected via Table.limit .
Collect Results to Client # You can call the TableResult.collect method to collect results of a table to client. The type of the results is an auto closeable iterator.
The following code shows how to use the TableResult.collect() method：
# prepare source tables source = table_env.from_elements([(1, \u0026#34;Hi\u0026#34;, \u0026#34;Hello\u0026#34;), (2, \u0026#34;Hello\u0026#34;, \u0026#34;Hello\u0026#34;)], [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]) # Get TableResult table_result = table_env.execute_sql(\u0026#34;select a + 1, b, c from %s\u0026#34; % source) # Traversal result with table_result.collect() as results: for result in results: print(result) The results are as following：
\u0026lt;Row(2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)\u0026gt; \u0026lt;Row(3, \u0026#39;Hello\u0026#39;, \u0026#39;Hello\u0026#39;)\u0026gt; Note It will trigger the materialization of the table and collect table content to the memory of the client, it\u0026rsquo;s a good practice to limit the number of rows collected via Table.limit .
Collect Results to Client by converting it to pandas DataFrame # You can call the \u0026ldquo;to_pandas\u0026rdquo; method to convert a Table object to a pandas DataFrame:
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) print(table.to_pandas()) The results are as following:
id data 0 1 Hi 1 2 Hello Note It will trigger the materialization of the table and collect table content to the memory of the client, it\u0026rsquo;s a good practice to limit the number of rows collected via Table.limit .
Note Not all the data types are supported.
Emit Results to One Sink Table # You can call the \u0026ldquo;execute_insert\u0026rdquo; method to emit the data in a Table object to a sink table:
table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table.execute_insert(\u0026#34;sink_table\u0026#34;).wait() The results are as following:
6\u0026gt; +I(1,Hi) 6\u0026gt; +I(2,Hello) This could also be done using SQL:
table_env.create_temporary_view(\u0026#34;table_source\u0026#34;, table) table_env.execute_sql(\u0026#34;INSERT INTO sink_table SELECT * FROM table_source\u0026#34;).wait() Emit Results to Multiple Sink Tables # You can use a StatementSet to emit the Tables to multiple sink tables in one job:
# prepare source tables and sink tables table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#34;simple_source\u0026#34;, table) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE first_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE second_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # create a statement set statement_set = table_env.create_statement_set() # emit the \u0026#34;table\u0026#34; object to the \u0026#34;first_sink_table\u0026#34; statement_set.add_insert(\u0026#34;first_sink_table\u0026#34;, table) # emit the \u0026#34;simple_source\u0026#34; to the \u0026#34;second_sink_table\u0026#34; via a insert sql query statement_set.add_insert_sql(\u0026#34;INSERT INTO second_sink_table SELECT * FROM simple_source\u0026#34;) # execute the statement set statement_set.execute().wait() The results are as following:
7\u0026gt; +I(1,Hi) 7\u0026gt; +I(1,Hi) 7\u0026gt; +I(2,Hello) 7\u0026gt; +I(2,Hello) Explain Tables # The Table API provides a mechanism to explain the logical and optimized query plans used to compute a Table. This is done through the Table.explain() or StatementSet.explain() methods. Table.explain() returns the plan of a Table. StatementSet.explain() is used to get the plan for a job which contains multiple sinks. These methods return a string describing three things:
the Abstract Syntax Tree of the relational query, i.e., the unoptimized logical query plan, the optimized logical query plan, and the physical execution plan. TableEnvironment.explain_sql() and TableEnvironment.execute_sql() support executing an EXPLAIN statement to get the plans. Please refer to the EXPLAIN page for more details.
The following code shows how to use the Table.explain() method:
# using a stream TableEnvironment from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table1 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table2 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table = table1 \\ .where(col(\u0026#34;data\u0026#34;).like(\u0026#39;H%\u0026#39;)) \\ .union_all(table2) print(table.explain()) The results are as following:
== Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE(\$1, _UTF-16LE\u0026#39;H%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]]]) == Optimized Logical Plan == Union(all=[true], union=[id, data]) :- Calc(select=[id, data], where=[LIKE(data, _UTF-16LE\u0026#39;H%\u0026#39;)]) : +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) == Physical Execution Plan == Stage 133 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 134 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 135 : Operator content : Calc(select=[id, data], where=[(data LIKE _UTF-16LE\u0026#39;H%\u0026#39;)]) ship_strategy : FORWARD Stage 136 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 137 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD The following code shows how to use the StatementSet.explain() method:
# using a stream TableEnvironment from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table1 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table2 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE black_hole_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) statement_set = table_env.create_statement_set() statement_set.add_insert(\u0026#34;print_sink_table\u0026#34;, table1.where(col(\u0026#34;data\u0026#34;).like(\u0026#39;H%\u0026#39;))) statement_set.add_insert(\u0026#34;black_hole_sink_table\u0026#34;, table2) print(statement_set.explain()) The results are as following:
== Abstract Syntax Tree == LogicalSink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) +- LogicalFilter(condition=[LIKE(\$1, _UTF-16LE\u0026#39;H%\u0026#39;)]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]]]) LogicalSink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]]]) == Optimized Logical Plan == Sink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) +- Calc(select=[id, data], where=[LIKE(data, _UTF-16LE\u0026#39;H%\u0026#39;)]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) Sink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) == Physical Execution Plan == Stage 139 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 140 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 141 : Operator content : Calc(select=[id, data], where=[(data LIKE _UTF-16LE\u0026#39;H%\u0026#39;)]) ship_strategy : FORWARD Stage 143 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 144 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 142 : Data Sink content : Sink: Sink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) ship_strategy : FORWARD Stage 145 : Data Sink content : Sink: Sink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) ship_strategy : FORWARD `}),e.add({id:273,href:"/flink/flink-docs-master/docs/dev/python/table/table_environment/",title:"TableEnvironment",section:"Table API",content:' TableEnvironment # This document is an introduction of PyFlink TableEnvironment. It includes detailed descriptions of every public interface of the TableEnvironment class.\nCreate a TableEnvironment # The recommended way to create a TableEnvironment is to create from an EnvironmentSettings object:\nfrom pyflink.common import Configuration from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment config = Configuration() config.set_string(\u0026#39;execution.buffer-timeout\u0026#39;, \u0026#39;1 min\u0026#39;) env_settings = EnvironmentSettings \\ .new_instance() \\ .in_streaming_mode() \\ .with_configuration(config) \\ .build() table_env = TableEnvironment.create(env_settings) Alternatively, users can create a StreamTableEnvironment from an existing StreamExecutionEnvironment to interoperate with the DataStream API.\nfrom pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment # create a streaming TableEnvironment from a StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) TableEnvironment API # Table/SQL Operations # These APIs are used to create/remove Table API/SQL Tables and write queries:\nAPIs Description Docs from_elements(elements, schema=None, verify_schema=True) Creates a table from a collection of elements. link from_pandas(pdf, schema=None, split_num=1) Creates a table from a pandas DataFrame. link from_path(path) Creates a table from a registered table under the specified path, e.g. tables registered via create_temporary_view. link create_temporary_view(view_path, table) Registers a `Table` object as a temporary view similar to SQL temporary views. link drop_temporary_view(view_path) Drops a temporary view registered under the given path. link drop_temporary_table(table_path) Drops a temporary table registered under the given path. You can use this interface to drop the temporary source table and temporary sink table. link execute_sql(stmt) Executes the given single statement and returns the execution result. The statement can be DDL/DML/DQL/SHOW/DESCRIBE/EXPLAIN/USE. Note that for "INSERT INTO" statement this is an asynchronous operation, which is usually expected when submitting a job to a remote cluster. However, when executing a job in a mini cluster or IDE, you need to wait until the job execution finished, then you can refer to here for more details. Please refer the SQL documentation for more details about SQL statement. link sql_query(query) Evaluates a SQL query and retrieves the result as a `Table` object. link Deprecated APIs\nAPIs Description Docs from_table_source(table_source) Creates a table from a table source. link scan(*table_path) Scans a registered table from catalog and returns the resulting Table. It can be replaced by from_path. link register_table(name, table) Registers a `Table` object under a unique name in the TableEnvironment\'s catalog. Registered tables can be referenced in SQL queries. It can be replaced by create_temporary_view. link register_table_source(name, table_source) Registers an external `TableSource` in the TableEnvironment\'s catalog. link register_table_sink(name, table_sink) Registers an external `TableSink` in the TableEnvironment\'s catalog. link insert_into(target_path, table) Instructs to write the content of a `Table` object into a sink table. Note that this interface would not trigger the execution of jobs. You need to call the "execute" method to execute your job. link sql_update(stmt) Evaluates a SQL statement such as INSERT, UPDATE or DELETE or a DDL statement. It can be replaced by execute_sql. link Execute/Explain Jobs # These APIs are used to explain/execute jobs. Note that the API execute_sql can also be used to execute jobs.\nAPIs Description Docs explain_sql(stmt, *extra_details) Returns the AST and the execution plan of the specified statement. link create_statement_set() Creates a StatementSet instance which accepts DML statements or Tables. It can be used to execute a multi-sink job. link Deprecated APIs\nAPIs Description Docs explain(table=None, extended=False) Returns the AST of the specified Table API and SQL queries and the execution plan to compute the result of the given `Table` object or multi-sinks plan. If you use the insert_into or sql_update method to emit data to multiple sinks, you can use this method to get the plan. It can be replaced by TableEnvironment.explain_sql, Table.explain or StatementSet.explain. link execute(job_name) Triggers the program execution. The environment will execute all parts of the program. If you use the insert_into or sql_update method to emit data to sinks, you can use this method trigger the program execution. This method will block the client program until the job is finished/canceled/failed. link Create/Drop User Defined Functions # These APIs are used to register UDFs or remove the registered UDFs. Note that the API execute_sql can also be used to register/remove UDFs. For more details about the different kinds of UDFs, please refer to User Defined Functions.\nAPIs Description Docs create_temporary_function(path, function) Registers a Python user defined function class as a temporary catalog function. link create_temporary_system_function(name, function) Registers a Python user defined function class as a temporary system function. If the name of a temporary system function is the same as a temporary catalog function, the temporary system function takes precedence. link create_java_function(path, function_class_name, ignore_if_exists=None) Registers a Java user defined function class as a catalog function under the given path. If the catalog is persistent, the registered catalog function can be used across multiple Flink sessions and clusters. link create_java_temporary_function(path, function_class_name) Registers a Java user defined function class as a temporary catalog function. link create_java_temporary_system_function(name, function_class_name) Registers a Java user defined function class as a temporary system function. link drop_function(path) Drops a catalog function registered under the given path. link drop_temporary_function(path) Drops a temporary system function registered under the given name. link drop_temporary_system_function(name) Drops a temporary system function registered under the given name. link Deprecated APIs\nAPIs Description Docs register_function(name, function) Registers a Python user-defined function under a unique name. Replaces already existing user-defined function under this name. It can be replaced by create_temporary_system_function. link register_java_function(name, function_class_name) Registers a Java user defined function under a unique name. Replaces already existing user-defined functions under this name. It can be replaced by create_java_temporary_system_function. link Dependency Management # These APIs are used to manage the Python dependencies which are required by the Python UDFs. Please refer to the Dependency Management documentation for more details.\nAPIs Description Docs add_python_file(file_path) Adds a Python dependency which could be Python files, Python packages or local directories. They will be added to the PYTHONPATH of the Python UDF worker. link set_python_requirements(requirements_file_path, requirements_cache_dir=None) Specifies a requirements.txt file which defines the third-party dependencies. These dependencies will be installed to a temporary directory and added to the PYTHONPATH of the Python UDF worker. link add_python_archive(archive_path, target_dir=None) Adds a Python archive file. The file will be extracted to the working directory of Python UDF worker. link Configuration # APIs Description Docs get_config() Returns the table config to define the runtime behavior of the Table API. You can find all the available configuration options in Configuration and Python Configuration. The following code is an example showing how to set the configuration options through this API:\n# set the parallelism to 8\ntable_env.get_config().set("parallelism.default", "8")\n# set the job name table_env.get_config().set("pipeline.name", "my_first_job") link Catalog APIs # These APIs are used to access catalogs and modules. You can find more detailed introduction in Modules and Catalogs documentation.\nAPIs Description Docs register_catalog(catalog_name, catalog) Registers a `Catalog` under a unique name. link get_catalog(catalog_name) Gets a registered `Catalog` by name. link use_catalog(catalog_name) Sets the current catalog to the given value. It also sets the default database to the catalog\'s default one. link get_current_catalog() Gets the current default catalog name of the current session. link get_current_database() Gets the current default database name of the running session. link use_database(database_name) Sets the current default database. It has to exist in the current catalog. That path will be used as the default one when looking for unqualified object names. link load_module(module_name, module) Loads a `Module` under a unique name. Modules will be kept in the loaded order. link unload_module(module_name) Unloads a `Module` with given name. link use_modules(*module_names) Enables and changes the resolution order of loaded modules. link list_catalogs() Gets the names of all catalogs registered in this environment. link list_modules() Gets the names of all enabled modules registered in this environment. link list_full_modules() Gets the names of all loaded modules (including disabled modules) registered in this environment. link list_databases() Gets the names of all databases in the current catalog. link list_tables() Gets the names of all tables and views in the current database of the current catalog. It returns both temporary and permanent tables and views. link list_views() Gets the names of all views in the current database of the current catalog. It returns both temporary and permanent views. link list_user_defined_functions() Gets the names of all user defined functions registered in this environment. link list_functions() Gets the names of all functions in this environment. link list_temporary_tables() Gets the names of all temporary tables and views available in the current namespace (the current database of the current catalog). link list_temporary_views() Gets the names of all temporary views available in the current namespace (the current database of the current catalog). link Statebackend, Checkpoint and Restart Strategy # Before Flink 1.10 you can configure the statebackend, checkpointing and restart strategy via the StreamExecutionEnvironment. And now you can configure them by setting key-value options in TableConfig, see Fault Tolerance, State Backends and Checkpointing for more details.\nThe following code is an example showing how to configure the statebackend, checkpoint and restart strategy through the Table API:\n# set the restart strategy to \u0026#34;fixed-delay\u0026#34; table_env.get_config().set(\u0026#34;restart-strategy\u0026#34;, \u0026#34;fixed-delay\u0026#34;) table_env.get_config().set(\u0026#34;restart-strategy.fixed-delay.attempts\u0026#34;, \u0026#34;3\u0026#34;) table_env.get_config().set(\u0026#34;restart-strategy.fixed-delay.delay\u0026#34;, \u0026#34;30s\u0026#34;) # set the checkpoint mode to EXACTLY_ONCE table_env.get_config().set(\u0026#34;execution.checkpointing.mode\u0026#34;, \u0026#34;EXACTLY_ONCE\u0026#34;) table_env.get_config().set(\u0026#34;execution.checkpointing.interval\u0026#34;, \u0026#34;3min\u0026#34;) # set the statebackend type to \u0026#34;rocksdb\u0026#34;, other available options are \u0026#34;filesystem\u0026#34; and \u0026#34;jobmanager\u0026#34; # you can also set the full qualified Java class name of the StateBackendFactory to this option # e.g. org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory table_env.get_config().set(\u0026#34;state.backend\u0026#34;, \u0026#34;rocksdb\u0026#34;) # set the checkpoint directory, which is required by the RocksDB statebackend table_env.get_config().set(\u0026#34;state.checkpoints.dir\u0026#34;, \u0026#34;file:///tmp/checkpoints/\u0026#34;) '}),e.add({id:274,href:"/flink/flink-docs-master/docs/dev/python/dependency_management/",title:"Dependency Management",section:"Python API",content:` Dependency Management # There are requirements to use dependencies inside the Python API programs. For example, users may need to use third-party Python libraries in Python user-defined functions. In addition, in scenarios such as machine learning prediction, users may want to load a machine learning model inside the Python user-defined functions.
When the PyFlink job is executed locally, users could install the third-party Python libraries into the local Python environment, download the machine learning model to local, etc. However, this approach doesn\u0026rsquo;t work well when users want to submit the PyFlink jobs to remote clusters. In the following sections, we will introduce the options provided in PyFlink for these requirements.
Note Both Python DataStream API and Python Table API have provided APIs for each kind of dependency. If you are mixing use of Python DataStream API and Python Table API in a single job, you should specify the dependencies via Python DataStream API to make them work for both the Python DataStream API and Python Table API.
JAR Dependencies # If third-party JARs are used, you can specify the JARs in the Python Table API as following:
# Specify a list of jar URLs via \u0026#34;pipeline.jars\u0026#34;. The jars are separated by \u0026#34;;\u0026#34; # and will be uploaded to the cluster. # NOTE: Only local file URLs (start with \u0026#34;file://\u0026#34;) are supported. table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) # It looks like the following on Windows: table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///E:/my/jar/path/connector.jar;file:///E:/my/jar/path/udf.jar\u0026#34;) # Specify a list of URLs via \u0026#34;pipeline.classpaths\u0026#34;. The URLs are separated by \u0026#34;;\u0026#34; # and will be added to the classpath during job execution. # NOTE: The paths must specify a protocol (e.g. file://) and users should ensure that the URLs are accessible on both the client and the cluster. table_env.get_config().set(\u0026#34;pipeline.classpaths\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) or in the Python DataStream API as following:
# Use the add_jars() to add local jars and the jars will be uploaded to the cluster. # NOTE: Only local file URLs (start with \u0026#34;file://\u0026#34;) are supported. stream_execution_environment.add_jars(\u0026#34;file:///my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///my/jar/path/connector2.jar\u0026#34;) # It looks like the following on Windows: stream_execution_environment.add_jars(\u0026#34;file:///E:/my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///E:/my/jar/path/connector2.jar\u0026#34;) # Use the add_classpaths() to add the dependent jars URLs into the classpath. # The URLs will also be added to the classpath of both the client and the cluster. # NOTE: The paths must specify a protocol (e.g. file://) and users should ensure that the # URLs are accessible on both the client and the cluster. stream_execution_environment.add_classpaths(\u0026#34;file:///my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///my/jar/path/connector2.jar\u0026#34;) or through the command line arguments --jarfile when submitting the job.
Note It only supports to specify one jar file with the command line argument --jarfile and so you need to build a fat jar if there are multiple jar files.
Python Dependencies # Python libraries # You may want to use third-part Python libraries in Python user-defined functions. There are multiple ways to specify the Python libraries.
You could specify them inside the code using Python Table API as following:
table_env.add_python_file(file_path) or using Python DataStream API as following:
stream_execution_environment.add_python_file(file_path) You could also specify the Python libraries using configuration python.files or via command line arguments -pyfs or --pyFiles when submitting the job.
Note The Python libraries could be local files or local directories. They will be added to the PYTHONPATH of the Python UDF worker.
requirements.txt # It also allows to specify a requirements.txt file which defines the third-party Python dependencies. These Python dependencies will be installed into the working directory and added to the PYTHONPATH of the Python UDF worker.
You could prepare the requirements.txt manually as following:
echo numpy==1.16.5 \u0026gt;\u0026gt; requirements.txt echo pandas==1.0.0 \u0026gt;\u0026gt; requirements.txt or using pip freeze which lists all the packages installed in the current Python environment:
pip freeze \u0026gt; requirements.txt The content of the requirements.txt file may look like the following:
numpy==1.16.5 pandas==1.0.0 You could manually edit it by removing unnecessary entries or adding extra entries, etc.
The requirements.txt file could then be specified inside the code using Python Table API as following:
# requirements_cache_dir is optional table_env.set_python_requirements( requirements_file_path=\u0026#34;/path/to/requirements.txt\u0026#34;, requirements_cache_dir=\u0026#34;cached_dir\u0026#34;) or using Python DataStream API as following:
# requirements_cache_dir is optional stream_execution_environment.set_python_requirements( requirements_file_path=\u0026#34;/path/to/requirements.txt\u0026#34;, requirements_cache_dir=\u0026#34;cached_dir\u0026#34;) Note For the dependencies which could not be accessed in the cluster, a directory which contains the installation packages of these dependencies could be specified using the parameter requirements_cached_dir. It will be uploaded to the cluster to support offline installation. You could prepare the requirements_cache_dir as following:
pip download -d cached_dir -r requirements.txt --no-binary :all: Note Please make sure that the prepared packages match the platform of the cluster, and the Python version used.
You could also specify the requirements.txt file using configuration python.requirements or via command line arguments -pyreq or --pyRequirements when submitting the job.
Note It will install the packages specified in the requirements.txt file using pip, so please make sure that pip (version \u0026gt;= 20.3) and setuptools (version \u0026gt;= 37.0.0) are available.
Archives # You may also want to specify archive files. The archive files could be used to specify custom Python virtual environments, data files, etc.
You could specify the archive files inside the code using Python Table API as following:
table_env.add_python_archive(archive_path=\u0026#34;/path/to/archive_file\u0026#34;, target_dir=None) or using Python DataStream API as following:
stream_execution_environment.add_python_archive(archive_path=\u0026#34;/path/to/archive_file\u0026#34;, target_dir=None) Note The parameter target_dir is optional. If specified, the archive file will be extracted to a directory with the specified name of target_dir during execution. Otherwise, the archive file will be extracted to a directory with the same name as the archive file.
Suppose you have specified the archive file as following:
table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;myenv\u0026#34;) Then, you could access the content of the archive file in Python user-defined functions as following:
def my_udf(): with open(\u0026#34;myenv/py_env/data/data.txt\u0026#34;) as f: ... If you have not specified the parameter target_dir:
table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;) You could then access the content of the archive file in Python user-defined functions as following:
def my_udf(): with open(\u0026#34;py_env.zip/py_env/data/data.txt\u0026#34;) as f: ... Note The archive file will be extracted to the working directory of Python UDF worker and so you could access the files inside the archive file using relative path.
You could also specify the archive files using configuration python.archives or via command line arguments -pyarch or --pyArchives when submitting the job.
Note If the archive file contains a Python virtual environment, please make sure that the Python virtual environment matches the platform that the cluster is running on.
Note Currently, only zip files (i.e., zip, jar, whl, egg, etc) and tar files (i.e., tar, tar.gz, tgz) are supported.
Python interpreter # It supports to specify the path of the Python interpreter to execute Python worker.
You could specify the Python interpreter inside the code using Python Table API as following:
table_env.get_config().set_python_executable(\u0026#34;/path/to/python\u0026#34;) or using Python DataStream API as following:
stream_execution_environment.set_python_executable(\u0026#34;/path/to/python\u0026#34;) It also supports to use the Python interpreter inside an archive file.
# Python Table API table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;venv\u0026#34;) table_env.get_config().set_python_executable(\u0026#34;venv/py_env/bin/python\u0026#34;) # Python DataStream API stream_execution_environment.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;venv\u0026#34;) stream_execution_environment.set_python_executable(\u0026#34;venv/py_env/bin/python\u0026#34;) You could also specify the Python interpreter using configuration python.executable or via command line arguments -pyexec or --pyExecutable when submitting the job.
Note If the path of the Python interpreter refers to the Python archive file, relative path should be used instead of absolute path.
Python interpreter of client # Python is needed at the client side to parse the Python user-defined functions during compiling the job.
You could specify the custom Python interpreter used at the client side by activating it in the current session.
source my_env/bin/activate or specify it using configuration python.client.executable, command line arguments -pyclientexec or --pyClientExecutable, environment variable PYFLINK_CLIENT_EXECUTABLE
How to specify Python Dependencies in Java/Scala Program # It also supports to use Python user-defined functions in the Java Table API programs or pure SQL programs. The following code shows a simple example on how to use the Python user-defined functions in a Java Table API program:
import org.apache.flink.configuration.CoreOptions; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; TableEnvironment tEnv = TableEnvironment.create( EnvironmentSettings.inBatchMode()); tEnv.getConfig().set(CoreOptions.DEFAULT_PARALLELISM, 1); // register the Python UDF tEnv.executeSql(\u0026#34;create temporary system function add_one as \u0026#39;add_one.add_one\u0026#39; language python\u0026#34;); tEnv.createTemporaryView(\u0026#34;source\u0026#34;, tEnv.fromValues(1L, 2L, 3L).as(\u0026#34;a\u0026#34;)); // use Python UDF in the Java Table API program tEnv.executeSql(\u0026#34;select add_one(a) as a from source\u0026#34;).collect(); You can refer to the SQL statement about CREATE FUNCTION for more details on how to create Python user-defined functions using SQL statements.
The Python dependencies could then be specified via the Python config options, such as python.archives, python.files, python.requirements, python.client.executable, python.executable. etc or through command line arguments when submitting the job.
`}),e.add({id:275,href:"/flink/flink-docs-master/docs/dev/python/table/operations/",title:"Operations",section:"Table API",content:" "}),e.add({id:276,href:"/flink/flink-docs-master/docs/dev/python/table/operations/operations/",title:"Overview",section:"Operations",content:" "}),e.add({id:277,href:"/flink/flink-docs-master/docs/dev/datastream/execution/parallel/",title:"Parallel Execution",section:"Managing Execution",content:` Parallel Execution # This section describes how the parallel execution of programs can be configured in Flink. A Flink program consists of multiple tasks (transformations/operators, data sources, and sinks). A task is split into several parallel instances for execution and each parallel instance processes a subset of the task\u0026rsquo;s input data. The number of parallel instances of a task is called its parallelism.
If you want to use savepoints you should also consider setting a maximum parallelism (or max parallelism). When restoring from a savepoint you can change the parallelism of specific operators or the whole program and this setting specifies an upper bound on the parallelism. This is required because Flink internally partitions state into key-groups and we cannot have +Inf number of key-groups because this would be detrimental to performance.
Setting the Parallelism # The parallelism of a task can be specified in Flink on different levels:
Operator Level # The parallelism of an individual operator, data source, or data sink can be defined by calling its setParallelism() method. For example, like this:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; text = [...]; DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1).setParallelism(5); wordCounts.print(); env.execute(\u0026#34;Word Count Example\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val text = [...] val wordCounts = text .flatMap{ _.split(\u0026#34; \u0026#34;) map { (_, 1) } } .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1).setParallelism(5) wordCounts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() text = [...] word_counts = text .flat_map(lambda x: x.split(\u0026#34; \u0026#34;)) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) \\ .set_parallelism(5) word_counts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Execution Environment Level # As mentioned here Flink programs are executed in the context of an execution environment. An execution environment defines a default parallelism for all operators, data sources, and data sinks it executes. Execution environment parallelism can be overwritten by explicitly configuring the parallelism of an operator.
The default parallelism of an execution environment can be specified by calling the setParallelism() method. To execute all operators, data sources, and data sinks with a parallelism of 3, set the default parallelism of the execution environment as follows:
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(3); DataStream\u0026lt;String\u0026gt; text = [...]; DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = [...]; wordCounts.print(); env.execute(\u0026#34;Word Count Example\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(3) val text = [...] val wordCounts = text .flatMap{ _.split(\u0026#34; \u0026#34;) map { (_, 1) } } .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1) wordCounts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(3) text = [...] word_counts = text .flat_map(lambda x: x.split(\u0026#34; \u0026#34;)) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) word_counts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Client Level # The parallelism can be set at the Client when submitting jobs to Flink. The Client can either be a Java or a Scala program. One example of such a Client is Flink\u0026rsquo;s Command-line Interface (CLI).
For the CLI client, the parallelism parameter can be specified with -p. For example:
./bin/flink run -p 10 ../examples/*WordCount-java*.jar In a Java/Scala program, the parallelism is set as follows:
Java try { PackagedProgram program = new PackagedProgram(file, args); InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\u0026#34;localhost:6123\u0026#34;); Configuration config = new Configuration(); Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader()); // set the parallelism to 10 here client.run(program, 10, true); } catch (ProgramInvocationException e) { e.printStackTrace(); } Scala try { PackagedProgram program = new PackagedProgram(file, args) InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\u0026#34;localhost:6123\u0026#34;) Configuration config = new Configuration() Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader()) // set the parallelism to 10 here client.run(program, 10, true) } catch { case e: Exception =\u0026gt; e.printStackTrace } Python Still not supported in Python API. System Level # A system-wide default parallelism for all execution environments can be defined by setting the parallelism.default property in ./conf/flink-conf.yaml. See the Configuration documentation for details.
Setting the Maximum Parallelism # The maximum parallelism can be set in places where you can also set a parallelism (except client level and system level). Instead of calling setParallelism() you call setMaxParallelism() to set the maximum parallelism.
The default setting for the maximum parallelism is roughly operatorParallelism + (operatorParallelism / 2) with a lower bound of 128 and an upper bound of 32768.
Setting the maximum parallelism to a very large value can be detrimental to performance because some state backends have to keep internal data structures that scale with the number of key-groups (which are the internal implementation mechanism for rescalable state). Back to top
`}),e.add({id:278,href:"/flink/flink-docs-master/docs/dev/python/table/operations/row_based_operations/",title:"Row-based Operations",section:"Operations",content:` Row-based Operations # This page describes how to use row-based operations in PyFlink Table API.
Map # Performs a map operation with a python general scalar function or vectorized scalar function. The output will be flattened if the output type is a composite type.
from pyflink.common import Row from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.types import DataTypes from pyflink.table.udf import udf env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) @udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) def func1(id: int, data: str) -\u0026gt; Row: return Row(id, data * 2) # the input columns are specified as the inputs table.map(func1(col(\u0026#39;id\u0026#39;), col(\u0026#39;data\u0026#39;))).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ It also supports to take a Row object (containing all the columns of the input table) as input.
@udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) def func2(data: Row) -\u0026gt; Row: return Row(data.id, data.data * 2) # specify the function without the input columns table.map(func2).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ Note The input columns should not be specified when using func2 in the map operation.
It also supports to use vectorized scalar function in the map operation. It should be noted that the input type and output type should be pandas.DataFrame instead of Row in this case.
import pandas as pd @udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())]), func_type=\u0026#39;pandas\u0026#39;) def func3(data: pd.DataFrame) -\u0026gt; pd.DataFrame: res = pd.concat([data.id, data.data * 2], axis=1) return res table.map(func3).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ FlatMap # Performs a flat_map operation with a python table function.
from pyflink.common import Row from pyflink.table.udf import udtf from pyflink.table import DataTypes, EnvironmentSettings, TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi,Flink\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) @udtf(result_types=[DataTypes.INT(), DataTypes.STRING()]) def split(x: Row) -\u0026gt; Row: for s in x.data.split(\u0026#34;,\u0026#34;): yield x.id, s # use split in \`flat_map\` table.flat_map(split).execute().print() # result is #+-------------+--------------------------------+ #| f0 | f1 | #+-------------+--------------------------------+ #| 1 | Hi | #| 1 | Flink | #| 2 | Hello | #+-------------+--------------------------------+ The python table function could also be used in join_lateral and left_outer_join_lateral.
# use table function in \`join_lateral\` or \`left_outer_join_lateral\` table.join_lateral(split.alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;)).execute().print() # result is #+----------------------+--------------------------------+-------------+--------------------------------+ #| id | data | a | b | #+----------------------+--------------------------------+-------------+--------------------------------+ #| 1 | Hi,Flink | 1 | Hi | #| 1 | Hi,Flink | 1 | Flink | #| 2 | Hello | 2 | Hello | #+----------------------+--------------------------------+-------------+--------------------------------+ Aggregate # Performs an aggregate operation with a python general aggregate function or vectorized aggregate function.
from pyflink.common import Row from pyflink.table import DataTypes, EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.udf import AggregateFunction, udaf class CountAndSumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return Row(accumulator[0], accumulator[1]) def create_accumulator(self): return Row(0, 0) def accumulate(self, accumulator, row): accumulator[0] += 1 accumulator[1] += row.b def retract(self, accumulator, row): accumulator[0] -= 1 accumulator[1] -= row.b def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] += other_acc[0] accumulator[1] += other_acc[1] def get_accumulator_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) function = CountAndSumAggregateFunction() agg = udaf(function, result_type=function.get_result_type(), accumulator_type=function.get_accumulator_type(), name=str(function.__class__.__name__)) # aggregate with a python general aggregate function env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) t = table_env.from_elements([(1, 2), (2, 1), (1, 3)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) result = t.group_by(col(\u0026#39;a\u0026#39;)) \\ .aggregate(agg.alias(\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;d\u0026#39;)) result.execute().print() # the result is #+----+----------------------+----------------------+----------------------+ #| op | a | c | d | #+----+----------------------+----------------------+----------------------+ #| +I | 1 | 2 | 5 | #| +I | 2 | 1 | 1 | #+----+----------------------+----------------------+----------------------+ # aggregate with a python vectorized aggregate function env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) t = table_env.from_elements([(1, 2), (2, 1), (1, 3)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) t.aggregate(pandas_udaf.alias(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;)).execute().print() # the result is #+--------------------------------+-------------+ #| a | b | #+--------------------------------+-------------+ #| 2.0 | 3 | #+--------------------------------+-------------+ Note Similar to map operation, if you specify the aggregate function without the input columns in aggregate operation, it will take Row or Pandas.DataFrame as input which contains all the columns of the input table including the grouping keys. Note You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement and it should not contain aggregate functions in the select statement. Besides, the output of aggregate will be flattened if it is a composite type.
FlatAggregate # Performs a flat_aggregate operation with a python general Table Aggregate Function
Similar to GroupBy Aggregation, FlatAggregate groups the inputs on the grouping keys. Different from AggregateFunction, TableAggregateFunction could return 0, 1, or more records for a grouping key. Similar to aggregate, you have to close the flat_aggregate with a select statement and the select statement should not contain aggregate functions.
from pyflink.common import Row from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udtaf, TableAggregateFunction class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row): if row.a is not None: if accumulator[0] is None or row.a \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row.a elif accumulator[1] is None or row.a \u0026gt; accumulator[1]: accumulator[1] = row.a def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udtaf decorator: # top2 = udtaf(Top2(), result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]), accumulator_type=DataTypes.ARRAY(DataTypes.BIGINT())) top2 = udtaf(Top2()) t = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # call function \u0026#34;inline\u0026#34; without registration in Table API result = t.group_by(col(\u0026#39;b\u0026#39;)).flat_aggregate(top2).select(col(\u0026#39;*\u0026#39;)).execute().print() # the result is: #+----+--------------------------------+----------------------+ #| op | b | a | #+----+--------------------------------+----------------------+ #| +I | Hi2 | 5 | #| +I | Hi2 | \u0026lt;NULL\u0026gt; | #| +I | Hi | 7 | #| +I | Hi | 3 | #+----+--------------------------------+----------------------+ `}),e.add({id:279,href:"/flink/flink-docs-master/docs/dev/python/datastream/state/",title:"State",section:"DataStream API",content:" "}),e.add({id:280,href:"/flink/flink-docs-master/docs/dev/table/tableapi/",title:"Table API",section:"Table API \u0026 SQL",content:" Table API # The Table API is a unified, relational API for stream and batch processing. Table API queries can be run on batch or streaming input without modifications. The Table API is a super set of the SQL language and is specially designed for working with Apache Flink. The Table API is a language-integrated API for Scala, Java and Python. Instead of specifying queries as String values as common with SQL, Table API queries are defined in a language-embedded style in Java, Scala or Python with IDE support like autocompletion and syntax validation.\nThe Table API shares many concepts and parts of its API with Flink\u0026rsquo;s SQL integration. Have a look at the Common Concepts \u0026amp; API to learn how to register tables or to create a Table object. The Streaming Concepts pages discuss streaming specific concepts such as dynamic tables and time attributes.\nThe following examples assume a registered table called Orders with attributes (a, b, c, rowtime). The rowtime field is either a logical time attribute in streaming or a regular timestamp field in batch.\nOverview \u0026amp; Examples # The Table API is available for Scala, Java and Python. The Scala Table API leverages on Scala expressions, the Java Table API supports both Expression DSL and strings which are parsed and converted into equivalent expressions, the Python Table API currently only supports strings which are parsed and converted into equivalent expressions.\nThe following example shows the differences between the Scala, Java and Python Table API. The table program is executed in a batch environment. It scans the Orders table, groups by field a, and counts the resulting rows per group.\nJava The Java Table API is enabled by importing org.apache.flink.table.api.java.*. The following example shows how a Java Table API program is constructed and how expressions are specified as strings. For the Expression DSL it is also necessary to import static org.apache.flink.table.api.Expressions.*\nimport org.apache.flink.table.api.*; import static org.apache.flink.table.api.Expressions.*; EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() .build(); TableEnvironment tEnv = TableEnvironment.create(settings); // register Orders table in table environment // ... // specify table program Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // schema (a, b, c, rowtime) Table counts = orders .groupBy($(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).count().as(\u0026#34;cnt\u0026#34;)); // print counts.execute().print(); Scala The Scala Table API is enabled by importing org.apache.flink.table.api._, org.apache.flink.api.scala._, and org.apache.flink.table.api.bridge.scala._ (for bridging to/from DataStream).\nThe following example shows how a Scala Table API program is constructed. Table fields are referenced using Scala\u0026rsquo;s String interpolation using a dollar character ($).\nimport org.apache.flink.api.scala._ import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala._ // environment configuration val settings = EnvironmentSettings .newInstance() .inStreamingMode() .build() val tEnv = TableEnvironment.create(settings) // register Orders table in table environment // ... // specify table program val orders = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime) val result = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.count as \u0026#34;cnt\u0026#34;) .execute() .print() Python The following example shows how a Python Table API program is constructed and how expressions are specified as strings.\nfrom pyflink.table import * from pyflink.table.expressions import col # environment configuration t_env = TableEnvironment.create( environment_settings=EnvironmentSettings.in_batch_mode()) # register Orders table and Result table sink in table environment source_data_path = \u0026#34;/path/to/source/directory/\u0026#34; result_data_path = \u0026#34;/path/to/result/directory/\u0026#34; source_ddl = f\u0026#34;\u0026#34;\u0026#34; create table Orders( a VARCHAR, b BIGINT, c BIGINT, rowtime TIMESTAMP(3), WATERMARK FOR rowtime AS rowtime - INTERVAL \u0026#39;1\u0026#39; SECOND ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{source_data_path}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) sink_ddl = f\u0026#34;\u0026#34;\u0026#34; create table `Result`( a VARCHAR, cnt BIGINT ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{result_data_path}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(sink_ddl) # specify table program orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # schema (a, b, c, rowtime) orders.group_by(col(\u0026#34;a\u0026#34;)).select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).count.alias(\u0026#39;cnt\u0026#39;)).execute_insert(\u0026#34;result\u0026#34;).wait() The next example shows a more complex Table API program. The program scans again the Orders table. It filters null values, normalizes the field a of type String, and calculates for each hour and product a the average billing amount b.\nJava // environment configuration // ... // specify table program Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // schema (a, b, c, rowtime) Table result = orders .filter( and( $(\u0026#34;a\u0026#34;).isNotNull(), $(\u0026#34;b\u0026#34;).isNotNull(), $(\u0026#34;c\u0026#34;).isNotNull() )) .select($(\u0026#34;a\u0026#34;).lowerCase().as(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;rowtime\u0026#34;)) .window(Tumble.over(lit(1).hours()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;hourlyWindow\u0026#34;)) .groupBy($(\u0026#34;hourlyWindow\u0026#34;), $(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;hourlyWindow\u0026#34;).end().as(\u0026#34;hour\u0026#34;), $(\u0026#34;b\u0026#34;).avg().as(\u0026#34;avgBillingAmount\u0026#34;)); Scala // environment configuration // ... // specify table program val orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime) val result: Table = orders .filter($\u0026#34;a\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;b\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;c\u0026#34;.isNotNull) .select($\u0026#34;a\u0026#34;.lowerCase() as \u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;rowtime\u0026#34;) .window(Tumble over 1.hour on $\u0026#34;rowtime\u0026#34; as \u0026#34;hourlyWindow\u0026#34;) .groupBy($\u0026#34;hourlyWindow\u0026#34;, $\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;hourlyWindow\u0026#34;.end as \u0026#34;hour\u0026#34;, $\u0026#34;b\u0026#34;.avg as \u0026#34;avgBillingAmount\u0026#34;) Python # specify table program from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # schema (a, b, c, rowtime) result = orders.filter(col(\u0026#34;a\u0026#34;).is_not_null \u0026amp; col(\u0026#34;b\u0026#34;).is_not_null \u0026amp; col(\u0026#34;c\u0026#34;).is_not_null) \\ .select(col(\u0026#34;a\u0026#34;).lower_case.alias(\u0026#39;a\u0026#39;), col(\u0026#34;b\u0026#34;), col(\u0026#34;rowtime\u0026#34;)) \\ .window(Tumble.over(lit(1).hour).on(col(\u0026#34;rowtime\u0026#34;)).alias(\u0026#34;hourly_window\u0026#34;)) \\ .group_by(col(\u0026#39;hourly_window\u0026#39;), col(\u0026#39;a\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;hourly_window\u0026#39;).end.alias(\u0026#39;hour\u0026#39;), col(\u0026#34;b\u0026#34;).avg.alias(\u0026#39;avg_billing_amount\u0026#39;)) Since the Table API is a unified API for batch and streaming data, both example programs can be executed on batch and streaming inputs without any modification of the table program itself. In both cases, the program produces the same results given that streaming records are not late (see Streaming Concepts for details).\nBack to top\nOperations # The Table API supports the following operations. Please note that not all operations are available in both batch and streaming yet; they are tagged accordingly.\nScan, Projection, and Filter # From # Batch Streaming\nSimilar to the FROM clause in a SQL query. Performs a scane of registered table.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) FromValues # Batch Streaming\nSimilar to the VALUES clause in a SQL query. Produces an inline table out of the provided rows.\nYou can use a row(...) expression to create composite rows:\nJava Table table = tEnv.fromValues( row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ); Scala table = tEnv.fromValues( row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) Python table = t_env.from_elements([(1, \u0026#39;ABC\u0026#39;), (2, \u0026#39;ABCDE\u0026#39;)]) will produce a Table with a schema as follows:\nroot |-- f0: BIGINT NOT NULL // original types INT and BIGINT are generalized to BIGINT |-- f1: VARCHAR(5) NOT NULL // original types CHAR(3) and CHAR(5) are generalized // to VARCHAR(5). VARCHAR is used instead of CHAR so that // no padding is applied The method will derive the types automatically from the input expressions. If types at a certain position differ, the method will try to find a common super type for all types. If a common super type does not exist, an exception will be thrown.\nYou can also specify the requested type explicitly. It might be helpful for assigning more generic types like e.g. DECIMAL or naming the columns.\nJava Table table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()) ), row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ); Scala val table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()) ), row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) Python table = t_env.from_elements( [(1, \u0026#39;ABC\u0026#39;), (2, \u0026#39;ABCDE\u0026#39;)], schema=DataTypes.Row([DataTypes.FIELD(\u0026#39;id\u0026#39;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#39;name\u0026#39;, DataTypes.STRING())])) will produce a Table with the following schema:\nroot |-- id: DECIMAL(10, 2) |-- name: STRING Select # Batch Streaming\nSimilar to a SQL SELECT statement. Performs a select operation.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.select($(\u0026#34;a\u0026#34;), $(\u0026#34;c\u0026#34;).as(\u0026#34;d\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) Table result = orders.select($\u0026#34;a\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;d\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.select(col(\u0026#34;a\u0026#34;), col(\u0026#34;c\u0026#34;).alias(\u0026#39;d\u0026#39;)) You can use star (*) to act as a wild card, selecting all of the columns in the table.\nJava Table result = orders.select($(\u0026#34;*\u0026#34;)); Scala Table result = orders.select($\u0026#34;*\u0026#34;) Python from pyflink.table.expressions import col result = orders.select(col(\u0026#34;*\u0026#34;)) As # Batch Streaming\nRenames fields.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.as(\u0026#34;x, y, z, t\u0026#34;); scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;, \u0026#34;t\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.alias(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;, \u0026#34;t\u0026#34;) Where / Filter # Batch Streaming\nSimilar to a SQL WHERE clause. Filters out rows that do not pass the filter predicate.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.where($(\u0026#34;b\u0026#34;).isEqual(\u0026#34;red\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.filter($\u0026#34;a\u0026#34; % 2 === 0) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.where(col(\u0026#34;a\u0026#34;) == \u0026#39;red\u0026#39;) Or\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.filter($(\u0026#34;b\u0026#34;).isEqual(\u0026#34;red\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.filter($\u0026#34;a\u0026#34; % 2 === 0) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.filter(col(\u0026#34;a\u0026#34;) == \u0026#39;red\u0026#39;) Column Operations # AddColumns # Batch Streaming\nPerforms a field add operation. It will throw an exception if the added fields already exist.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.addColumns(concat($(\u0026#34;c\u0026#34;), \u0026#34;sunny\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.addColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;)) Python from pyflink.table.expressions import concat orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.add_columns(concat(col(\u0026#34;c\u0026#34;), \u0026#39;sunny\u0026#39;)) AddOrReplaceColumns # Batch Streaming\nPerforms a field add operation. Existing fields will be replaced if the added column name is the same as the existing column name. Moreover, if the added fields have duplicate field name, then the last one is used.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.addOrReplaceColumns(concat($(\u0026#34;c\u0026#34;), \u0026#34;sunny\u0026#34;).as(\u0026#34;desc\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.addOrReplaceColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;) as \u0026#34;desc\u0026#34;) Python from pyflink.table.expressions import concat orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.add_or_replace_columns(concat(col(\u0026#34;c\u0026#34;), \u0026#39;sunny\u0026#39;).alias(\u0026#39;desc\u0026#39;)) DropColumns # Batch Streaming\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.dropColumns($(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.dropColumns($\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.drop_columns(col(\u0026#34;b\u0026#34;), col(\u0026#34;c\u0026#34;)) RenameColumns # Batch Streaming\nPerforms a field rename operation. The field expressions should be alias expressions, and only the existing fields can be renamed.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.renameColumns($(\u0026#34;b\u0026#34;).as(\u0026#34;b2\u0026#34;), $(\u0026#34;c\u0026#34;).as(\u0026#34;c2\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.renameColumns($\u0026#34;b\u0026#34; as \u0026#34;b2\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;c2\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.rename_columns(col(\u0026#34;b\u0026#34;).alias(\u0026#39;b2\u0026#39;), col(\u0026#34;c\u0026#34;).alias(\u0026#39;c2\u0026#39;)) Back to top\nAggregations # GroupBy Aggregation # Batch Streaming Result Updating\nSimilar to a SQL GROUP BY clause. Groups the rows on the grouping keys with a following running aggregation operator to aggregate rows group-wise.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.groupBy($(\u0026#34;a\u0026#34;)).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().as(\u0026#34;d\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.groupBy($\u0026#34;a\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum().as(\u0026#34;d\u0026#34;)) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.group_by(col(\u0026#34;a\u0026#34;)).select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.alias(\u0026#39;d\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. GroupBy Window Aggregation # Batch Streaming\nGroups and aggregates a table on a group window and possibly one or more grouping keys.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .window(Tumble.over(lit(5).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)) // define window .groupBy($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;)) // group by key and window // access window properties and aggregate .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end(), $(\u0026#34;w\u0026#34;).rowtime(), $(\u0026#34;b\u0026#34;).sum().as(\u0026#34;d\u0026#34;) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // define window .groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) // group by key and window .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.sum as \u0026#34;d\u0026#34;) // access window properties and aggregate Python from pyflink.table.window import Tumble from pyflink.table.expressions import lit, col orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.window(Tumble.over(lit(5).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, col(\u0026#39;b\u0026#39;).sum.alias(\u0026#39;d\u0026#39;)) Over Window Aggregation # Similar to a SQL OVER clause. Over window aggregates are computed for each row, based on a window (range) of preceding and succeeding rows. See the over windows section for more details.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders // define window .window( Over .partitionBy($(\u0026#34;a\u0026#34;)) .orderBy($(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .following(CURRENT_RANGE) .as(\u0026#34;w\u0026#34;)) // sliding aggregate .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).avg().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).max().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).min().over($(\u0026#34;w\u0026#34;)) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders // define window .window( Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE following CURRENT_RANGE as \u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max().over($\u0026#34;w\u0026#34;), $\u0026#34;b\u0026#34;.min().over($\u0026#34;w\u0026#34;)) // sliding aggregate Python from pyflink.table.window import Over from pyflink.table.expressions import col, UNBOUNDED_RANGE, CURRENT_RANGE orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.over_window(Over.partition_by(col(\u0026#34;a\u0026#34;)).order_by(col(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE).following(CURRENT_RANGE) .alias(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).avg.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).max.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).min.over(col(\u0026#39;w\u0026#39;))) All aggregates must be defined over the same window, i.e., same partitioning, sorting, and range. Currently, only windows with PRECEDING (UNBOUNDED and bounded) to CURRENT ROW range are supported. Ranges with FOLLOWING are not supported yet. ORDER BY must be specified on a single time attribute.\nDistinct Aggregation # Batch Streaming Result Updating\nSimilar to a SQL DISTINCT aggregation clause such as COUNT(DISTINCT a). Distinct aggregation declares that an aggregation function (built-in or user-defined) is only applied on distinct input values. Distinct can be applied to GroupBy Aggregation, GroupBy Window Aggregation and Over Window Aggregation.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); // Distinct aggregation on group by Table groupByDistinctResult = orders .groupBy($(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().distinct().as(\u0026#34;d\u0026#34;)); // Distinct aggregation on time window group by Table groupByWindowDistinctResult = orders .window(Tumble .over(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;) ) .groupBy($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().distinct().as(\u0026#34;d\u0026#34;)); // Distinct aggregation on over window Table result = orders .window(Over .partitionBy($(\u0026#34;a\u0026#34;)) .orderBy($(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .as(\u0026#34;w\u0026#34;)) .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).avg().distinct().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).max().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).min().over($(\u0026#34;w\u0026#34;)) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) // Distinct aggregation on group by val groupByDistinctResult = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // Distinct aggregation on time window group by val groupByWindowDistinctResult = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;).groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // Distinct aggregation on over window val result = orders .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg.distinct over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.min over $\u0026#34;w\u0026#34;) Python from pyflink.table.expressions import col, lit, UNBOUNDED_RANGE from pyflink.table.window import Over, Tumble orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # Distinct aggregation on group by group_by_distinct_result = orders.group_by(col(\u0026#34;a\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.distinct.alias(\u0026#39;d\u0026#39;)) # Distinct aggregation on time window group by group_by_window_distinct_result = orders.window(Tumble.over(lit(5).minutes).on(col(\u0026#34;rowtime\u0026#34;)).alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#34;a\u0026#34;), col(\u0026#39;w\u0026#39;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.distinct.alias(\u0026#39;d\u0026#39;)) # Distinct aggregation on over window result = orders.over_window(Over .partition_by(col(\u0026#34;a\u0026#34;)) .order_by(col(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .alias(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).avg.distinct.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).max.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).min.over(col(\u0026#39;w\u0026#39;))) User-defined aggregation function can also be used with DISTINCT modifiers. To calculate the aggregate results only for distinct values, simply add the distinct modifier towards the aggregation function.\nJava Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // Use distinct aggregation for user-defined aggregate functions tEnv.registerFunction(\u0026#34;myUdagg\u0026#34;, new MyUdagg()); orders.groupBy(\u0026#34;users\u0026#34;) .select( $(\u0026#34;users\u0026#34;), call(\u0026#34;myUdagg\u0026#34;, $(\u0026#34;points\u0026#34;)).distinct().as(\u0026#34;myDistinctResult\u0026#34;) ); Scala val orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;) // Use distinct aggregation for user-defined aggregate functions val myUdagg = new MyUdagg() orders.groupBy($\u0026#34;users\u0026#34;).select($\u0026#34;users\u0026#34;, myUdagg.distinct($\u0026#34;points\u0026#34;) as \u0026#34;myDistinctResult\u0026#34;) Python Unsupported For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Distinct # Batch Streaming Result Updating\nSimilar to a SQL DISTINCT clause. Returns records with distinct value combinations.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.distinct(); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.distinct() Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.distinct() For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Back to top\nJoins # Inner Join # Batch Streaming\nSimilar to a SQL JOIN clause. Joins two tables. Both tables must have distinct field names and at least one equality join predicate must be defined through join operator or using a where or filter operator.\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;)); Table result = left.join(right) .where($(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val result = left.join(right).where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;)) result = left.join(right).where(col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Outer Join # Batch Streaming Result Updating\nSimilar to SQL LEFT/RIGHT/FULL OUTER JOIN clauses. Joins two tables. Both tables must have distinct field names and at least one equality join predicate must be defined.\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;)); Table leftOuterResult = left.leftOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Table rightOuterResult = left.rightOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Table fullOuterResult = left.fullOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val leftOuterResult = left.leftOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val rightOuterResult = left.rightOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val fullOuterResult = left.fullOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;)) left_outer_result = left.left_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) right_outer_result = left.right_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) full_outer_result = left.full_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Interval Join # Batch Streaming\nInterval joins are a subset of regular joins that can be processed in a streaming fashion.\nAn interval join requires at least one equi-join predicate and a join condition that bounds the time on both sides. Such a condition can be defined by two appropriate range predicates (\u0026lt;, \u0026lt;=, \u0026gt;=, \u0026gt;) or a single equality predicate that compares time attributes of the same type (i.e., processing time or event time) of both input tables.\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;), $(\u0026#34;ltime\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;), $(\u0026#34;rtime\u0026#34;)); Table result = left.join(right) .where( and( $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;)), $(\u0026#34;ltime\u0026#34;).isGreaterOrEqual($(\u0026#34;rtime\u0026#34;).minus(lit(5).minutes())), $(\u0026#34;ltime\u0026#34;).isLess($(\u0026#34;rtime\u0026#34;).plus(lit(10).minutes())) )) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;ltime\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;, $\u0026#34;ltime\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;, $\u0026#34;rtime\u0026#34;) val result = left.join(right) .where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34; \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026gt;= $\u0026#34;rtime\u0026#34; - 5.minutes \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026lt; $\u0026#34;rtime\u0026#34; + 10.minutes) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;ltime\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;rowtime1\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;), col(\u0026#39;rowtime2\u0026#39;)) joined_table = left.join(right).where((col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)) \u0026amp; (col(\u0026#39;rowtime1\u0026#39;) \u0026gt;= col(\u0026#39;rowtime2\u0026#39;) - lit(1).second) \u0026amp; (col(\u0026#39;rowtime1\u0026#39;) \u0026lt;= col(\u0026#39;rowtime2\u0026#39;) + lit(2).seconds)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;rowtime1\u0026#39;)) Inner Join with Table Function (UDTF) # Batch Streaming\nJoins a table with the results of a table function. Each row of the left (outer) table is joined with all rows produced by the corresponding call of the table function. A row of the left (outer) table is dropped, if its table function call returns an empty result.\nJava // register User-Defined Table Function TableFunction\u0026lt;Tuple3\u0026lt;String,String,String\u0026gt;\u0026gt; split = new MySplitUDTF(); tableEnv.registerFunction(\u0026#34;split\u0026#34;, split); // join Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .joinLateral(call(\u0026#34;split\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;s\u0026#34;), $(\u0026#34;t\u0026#34;), $(\u0026#34;v\u0026#34;)); Scala // instantiate User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .joinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;) Python # register User-Defined Table Function @udtf(result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()]) def split(x): return [Row(1, 2, 3)] # join orders = t_env.from_path(\u0026#34;Orders\u0026#34;) joined_table = orders.join_lateral(split(col(\u0026#39;c\u0026#39;)).alias(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;s\u0026#39;), col(\u0026#39;t\u0026#39;), col(\u0026#39;v\u0026#39;)) Left Outer Join with Table Function (UDTF) # Batch Streaming\nJoins a table with the results of a table function. Each row of the left (outer) table is joined with all rows produced by the corresponding call of the table function. If a table function call returns an empty result, the corresponding outer row is preserved and the result padded with null values.\nCurrently, the predicate of a table function left outer join can only be empty or literal true.\nJava // register User-Defined Table Function TableFunction\u0026lt;Tuple3\u0026lt;String,String,String\u0026gt;\u0026gt; split = new MySplitUDTF(); tableEnv.registerFunction(\u0026#34;split\u0026#34;, split); // join Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .leftOuterJoinLateral(call(\u0026#34;split\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;s\u0026#34;), $(\u0026#34;t\u0026#34;), $(\u0026#34;v\u0026#34;)); Scala // instantiate User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .leftOuterJoinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;) Python # register User-Defined Table Function @udtf(result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()]) def split(x): return [Row(1, 2, 3)] # join orders = t_env.from_path(\u0026#34;Orders\u0026#34;) joined_table = orders.left_outer_join_lateral(split(col(\u0026#39;c\u0026#39;)).alias(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;s\u0026#39;), col(\u0026#39;t\u0026#39;), col(\u0026#39;v\u0026#39;)) Join with Temporal Table # Temporal tables are tables that track changes over time.\nA temporal table function provides access to the state of a temporal table at a specific point in time. The syntax to join a table with a temporal table function is the same as in Inner Join with Table Function.\nCurrently only inner joins with temporal tables are supported.\nJava Table ratesHistory = tableEnv.from(\u0026#34;RatesHistory\u0026#34;); // register temporal table function with a time attribute and primary key TemporalTableFunction rates = ratesHistory.createTemporalTableFunction( \u0026#34;r_proctime\u0026#34;, \u0026#34;r_currency\u0026#34;); tableEnv.registerFunction(\u0026#34;rates\u0026#34;, rates); // join with \u0026#34;Orders\u0026#34; based on the time attribute and key Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .joinLateral(call(\u0026#34;rates\u0026#34;, $(\u0026#34;o_proctime\u0026#34;)), $(\u0026#34;o_currency\u0026#34;).isEqual($(\u0026#34;r_currency\u0026#34;))); Python Currently not supported in Python Table API. Back to top\nSet Operations # Union # Batch Similar to a SQL UNION clause. Unions two tables with duplicate records removed. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.union(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.union(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.union(right) UnionAll # Batch Streaming\nSimilar to a SQL UNION ALL clause. Unions two tables. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.unionAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.unionAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.union_all(right) Intersect # Batch Similar to a SQL INTERSECT clause. Intersect returns records that exist in both tables. If a record is present one or both tables more than once, it is returned just once, i.e., the resulting table has no duplicate records. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.intersect(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.intersect(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.intersect(right) IntersectAll # Batch Similar to a SQL INTERSECT ALL clause. IntersectAll returns records that exist in both tables. If a record is present in both tables more than once, it is returned as many times as it is present in both tables, i.e., the resulting table might have duplicate records. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.intersectAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.intersectAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.intersect_all(right) Minus # Batch Similar to a SQL EXCEPT clause. Minus returns records from the left table that do not exist in the right table. Duplicate records in the left table are returned exactly once, i.e., duplicates are removed. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.minus(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.minus(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.minus(right) MinusAll # Batch Similar to a SQL EXCEPT ALL clause. MinusAll returns the records that do not exist in the right table. A record that is present n times in the left table and m times in the right table is returned (n - m) times, i.e., as many duplicates as are present in the right table are removed. Both tables must have identical field types.\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.minusAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.minusAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.minus_all(right) In # Batch Streaming\nSimilar to a SQL IN clause. In returns true if an expression exists in a given table sub-query. The sub-query table must consist of one column. This column must have the same data type as the expression.\nJava Table left = tableEnv.from(\u0026#34;Orders1\u0026#34;) Table right = tableEnv.from(\u0026#34;Orders2\u0026#34;); Table result = left.select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)).where($(\u0026#34;a\u0026#34;).in(right)); Scala val left = tableEnv.from(\u0026#34;Orders1\u0026#34;) val right = tableEnv.from(\u0026#34;Orders2\u0026#34;) val result = left.select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;).where($\u0026#34;a\u0026#34;.in(right)) Python left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;a\u0026#39;)) result = left.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)).where(col(\u0026#39;a\u0026#39;).in_(right)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Back to top\nOrderBy, Offset \u0026amp; Fetch # Order By # Batch Streaming\nSimilar to a SQL ORDER BY clause. Returns records globally sorted across all parallel partitions. For unbounded tables, this operation requires a sorting on a time attribute or a subsequent fetch operation.\nJava Table result = tab.orderBy($(\u0026#34;a\u0026#34;).asc()); Scala val result = tab.orderBy($\u0026#34;a\u0026#34;.asc) Python result = tab.order_by(col(\u0026#39;a\u0026#39;).asc) Offset \u0026amp; Fetch # Batch Streaming\nSimilar to the SQL OFFSET and FETCH clauses. The offset operation limits a (possibly sorted) result from an offset position. The fetch operation limits a (possibly sorted) result to the first n rows. Usually, the two operations are preceded by an ordering operator. For unbounded tables, a fetch operation is required for an offset operation.\nJava // returns the first 5 records from the sorted result Table result1 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).fetch(5); // skips the first 3 records and returns all following records from the sorted result Table result2 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).offset(3); // skips the first 10 records and returns the next 5 records from the sorted result Table result3 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).offset(10).fetch(5); Scala // returns the first 5 records from the sorted result val result1: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).fetch(5) // skips the first 3 records and returns all following records from the sorted result val result2: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(3) // skips the first 10 records and returns the next 5 records from the sorted result val result3: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(10).fetch(5) Python # returns the first 5 records from the sorted result result1 = table.order_by(col(\u0026#39;a\u0026#39;).asc).fetch(5) # skips the first 3 records and returns all following records from the sorted result result2 = table.order_by(col(\u0026#39;a\u0026#39;).asc).offset(3) # skips the first 10 records and returns the next 5 records from the sorted result result3 = table.order_by(col(\u0026#39;a\u0026#39;).asc).offset(10).fetch(5) Insert # Batch Streaming\nSimilar to the INSERT INTO clause in a SQL query, the method performs an insertion into a registered output table. The insertInto() method will transform the INSERT INTO to a TablePipeline. The pipeline can be explained with TablePipeline.explain() and executed with TablePipeline.execute().\nOutput tables must be registered in the TableEnvironment (see Connector tables). Moreover, the schema of the registered table must match the schema of the query.\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); orders.insertInto(\u0026#34;OutOrders\u0026#34;).execute(); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) orders.insertInto(\u0026#34;OutOrders\u0026#34;).execute() Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) orders.execute_insert(\u0026#34;OutOrders\u0026#34;) Back to top\nGroup Windows # Group window aggregates group rows into finite groups based on time or row-count intervals and evaluate aggregation functions once per group. For batch tables, windows are a convenient shortcut to group records by time intervals.\nJava Windows are defined using the window(GroupWindow w) clause and require an alias, which is specified using the as clause. In order to group a table by a window, the window alias must be referenced in the groupBy(...) clause like a regular grouping attribute. The following example shows how to define a window aggregation on a table.\nTable table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // define window with alias w .groupBy($(\u0026#34;w\u0026#34;)) // group the table by window w .select($(\u0026#34;b\u0026#34;).sum()); // aggregate Scala Windows are defined using the window(w: GroupWindow) clause and require an alias, which is specified using the as clause. In order to group a table by a window, the window alias must be referenced in the groupBy(...) clause like a regular grouping attribute. The following example shows how to define a window aggregation on a table.\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w .groupBy($\u0026#34;w\u0026#34;) // group the table by window w .select($\u0026#34;b\u0026#34;.sum) // aggregate Python Windows are defined using the window(w: GroupWindow) clause and require an alias, which is specified using the alias clause. In order to group a table by a window, the window alias must be referenced in the group_by(...) clause like a regular grouping attribute. The following example shows how to define a window aggregation on a table.\n# define window with alias w, group the table by window w, then aggregate table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;)).select(col(\u0026#39;b\u0026#39;).sum) Java In streaming environments, window aggregates can only be computed in parallel if they group on one or more attributes in addition to the window, i.e., the groupBy(...) clause references a window alias and at least one additional attribute. A groupBy(...) clause that only references a window alias (such as in the example above) can only be evaluated by a single, non-parallel task. The following example shows how to define a window aggregation with additional grouping attributes.\nTable table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // define window with alias w .groupBy($(\u0026#34;w\u0026#34;), $(\u0026#34;a\u0026#34;)) // group the table by attribute a and window w .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum()); // aggregate Scala In streaming environments, window aggregates can only be computed in parallel if they group on one or more attributes in addition to the window, i.e., the groupBy(...) clause references a window alias and at least one additional attribute. A groupBy(...) clause that only references a window alias (such as in the example above) can only be evaluated by a single, non-parallel task. The following example shows how to define a window aggregation with additional grouping attributes.\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // group the table by attribute a and window w .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum) // aggregate Python In streaming environments, window aggregates can only be computed in parallel if they group on one or more attributes in addition to the window, i.e., the group_by(...) clause references a window alias and at least one additional attribute. A group_by(...) clause that only references a window alias (such as in the example above) can only be evaluated by a single, non-parallel task. The following example shows how to define a window aggregation with additional grouping attributes.\n# define window with alias w, group the table by attribute a and window w, # then aggregate table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;a\u0026#39;)).select(col(\u0026#39;b\u0026#39;).sum) Window properties such as the start, end, or rowtime timestamp of a time window can be added in the select statement as a property of the window alias as w.start, w.end, and w.rowtime, respectively. The window start and rowtime timestamps are the inclusive lower and upper window boundaries. In contrast, the window end timestamp is the exclusive upper window boundary. For example a tumbling window of 30 minutes that starts at 2pm would have 14:00:00.000 as start timestamp, 14:29:59.999 as rowtime timestamp, and 14:30:00.000 as end timestamp.\nJava Table table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // define window with alias w .groupBy($(\u0026#34;w\u0026#34;), $(\u0026#34;a\u0026#34;)) // group the table by attribute a and window w .select($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end(), $(\u0026#34;w\u0026#34;).rowtime(), $(\u0026#34;b\u0026#34;).count()); // aggregate and add window start, end, and rowtime timestamps Scala val table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // define window with alias w .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // group the table by attribute a and window w .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.count) // aggregate and add window start, end, and rowtime timestamps Python # define window with alias w, group the table by attribute a and window w, # then aggregate and add window start, end, and rowtime timestamps table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;a\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, col(\u0026#39;w\u0026#39;).rowtime, col(\u0026#39;b\u0026#39;).count) The Window parameter defines how rows are mapped to windows. Window is not an interface that users can implement. Instead, the Table API provides a set of predefined Window classes with specific semantics. The supported window definitions are listed below.\nTumble (Tumbling Windows) # A tumbling window assigns rows to non-overlapping, continuous windows of fixed length. For example, a tumbling window of 5 minutes groups rows in 5 minutes intervals. Tumbling windows can be defined on event-time, processing-time, or on a row-count.\nJava Tumbling windows are defined by using the Tumble class as follows:\nMethod Description over Defines the length the window, either as time or row-count interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Tumbling Event-time Window .window(Tumble.over(lit(10).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(lit(10).minutes()).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(rowInterval(10)).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala Tumbling windows are defined by using the Tumble class as follows:\nMethod Description over Defines the length the window, either as time or row-count interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Tumbling Event-time Window .window(Tumble over 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python Tumbling windows are defined by using the Tumble class as follows:\nMethod Description over Defines the length the window, either as time or row-count interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. alias Assigns an alias to the window. The alias is used to reference the window in the following group_by() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. # Tumbling Event-time Window .window(Tumble.over(lit(10).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(lit(10).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(row_interval(10)).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Slide (Sliding Windows) # A sliding window has a fixed size and slides by a specified slide interval. If the slide interval is smaller than the window size, sliding windows are overlapping. Thus, rows can be assigned to multiple windows. For example, a sliding window of 15 minutes size and 5 minute slide interval assigns each row to 3 different windows of 15 minute size, which are evaluated in an interval of 5 minutes. Sliding windows can be defined on event-time, processing-time, or on a row-count.\nJava Sliding windows are defined by using the Slide class as follows:\nMethod Description over Defines the length of the window, either as time or row-count interval. every Defines the slide interval, either as time or row-count interval. The slide interval must be of the same type as the size interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Sliding Event-time Window .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;)); // Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(\u0026#34;proctime\u0026#34;)) .as(\u0026#34;w\u0026#34;)); // Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(rowInterval(10)).every(rowInterval(5)).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala Sliding windows are defined by using the Slide class as follows:\nMethod Description over Defines the length of the window, either as time or row-count interval. every Defines the slide interval, either as time or row-count interval. The slide interval must be of the same type as the size interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Sliding Event-time Window .window(Slide over 10.minutes every 5.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.minutes every 5.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.rows every 5.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python Sliding windows are defined by using the Slide class as follows:\nMethod Description over Defines the length of the window, either as time or row-count interval. every Defines the slide interval, either as time or row-count interval. The slide interval must be of the same type as the size interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. alias Assigns an alias to the window. The alias is used to reference the window in the following group_by() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. # Sliding Event-time Window .window(Slide.over(lit(10).minutes).every(lit(5).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(lit(10).minutes).every(lit(5).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(row_interval(10)).every(row_interval(5)).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Session (Session Windows) # Session windows do not have a fixed size but their bounds are defined by an interval of inactivity, i.e., a session window is closes if no event appears for a defined gap period. For example a session window with a 30 minute gap starts when a row is observed after 30 minutes inactivity (otherwise the row would be added to an existing window) and is closed if no row is added within 30 minutes. Session windows can work on event-time or processing-time.\nJava A session window is defined by using the Session class as follows:\nMethod Description withGap Defines the gap between two windows as time interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Session Event-time Window .window(Session.withGap(lit(10).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session.withGap(lit(10).minutes()).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala A session window is defined by using the Session class as follows:\nMethod Description withGap Defines the gap between two windows as time interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. as Assigns an alias to the window. The alias is used to reference the window in the following groupBy() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. // Session Event-time Window .window(Session withGap 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session withGap 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python A session window is defined by using the Session class as follows:\nMethod Description with_gap Defines the gap between two windows as time interval. on The time attribute to group (time interval) or sort (row count) on. For batch queries this might be any Long or Timestamp attribute. For streaming queries this must be a declared event-time or processing-time time attribute. alias Assigns an alias to the window. The alias is used to reference the window in the following group_by() clause and optionally to select window properties such as window start, end, or rowtime timestamps in the select() clause. # Session Event-time Window .window(Session.with_gap(lit(10).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session.with_gap(lit(10).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Back to top\nOver Windows # Over window aggregates are known from standard SQL (OVER clause) and defined in the SELECT clause of a query. Unlike group windows, which are specified in the GROUP BY clause, over windows do not collapse rows. Instead over window aggregates compute an aggregate for each input row over a range of its neighboring rows.\nOver windows are defined using the window(w: OverWindow*) clause (using over_window(*OverWindow) in Python API) and referenced via an alias in the select() method. The following example shows how to define an over window aggregation on a table.\nJava Table table = input .window([OverWindow w].as(\u0026#34;w\u0026#34;)) // define over window with alias w .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;c\u0026#34;).min().over($(\u0026#34;w\u0026#34;))); // aggregate over the over window w Scala val table = input .window([w: OverWindow] as $\u0026#34;w\u0026#34;) // define over window with alias w .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum over $\u0026#34;w\u0026#34;, $\u0026#34;c\u0026#34;.min over $\u0026#34;w\u0026#34;) // aggregate over the over window w Python # define over window with alias w and aggregate over the over window w table = input.over_window([w: OverWindow].alias(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;).sum.over(col(\u0026#39;w\u0026#39;)), col(\u0026#39;c\u0026#39;).min.over(col(\u0026#39;w\u0026#39;))) The OverWindow defines a range of rows over which aggregates are computed. OverWindow is not an interface that users can implement. Instead, the Table API provides the Over class to configure the properties of the over window. Over windows can be defined on event-time or processing-time and on ranges specified as time interval or row-count. The supported over window definitions are exposed as methods on Over (and other classes) and are listed below:\nPartition By # Optional\nDefines a partitioning of the input on one or more attributes. Each partition is individually sorted and aggregate functions are applied to each partition separately.\nNote: In streaming environments, over window aggregates can only be computed in parallel if the window includes a partition by clause. Without partitionBy(\u0026hellip;) the stream is processed by a single, non-parallel task.\nOrder By # Required\nDefines the order of rows within each partition and thereby the order in which the aggregate functions are applied to rows.\nNote: For streaming queries this must be a declared event-time or processing-time time attribute. Currently, only a single sort attribute is supported.\nPreceding # Optional\nDefines the interval of rows that are included in the window and precede the current row. The interval can either be specified as time or row-count interval.\nBounded over windows are specified with the size of the interval, e.g., 10.minutes for a time interval or 10.rows for a row-count interval.\nUnbounded over windows are specified using a constant, i.e., UNBOUNDED_RANGE for a time interval or UNBOUNDED_ROW for a row-count interval. Unbounded over windows start with the first row of a partition.\nIf the preceding clause is omitted, UNBOUNDED_RANGE and CURRENT_RANGE are used as the default preceding and following for the window.\nFollowing # Optional\nDefines the window interval of rows that are included in the window and follow the current row. The interval must be specified in the same unit as the preceding interval (time or row-count).\nAt the moment, over windows with rows following the current row are not supported. Instead you can specify one of two constants:\nCURRENT_ROW sets the upper bound of the window to the current row. CURRENT_RANGE sets the upper bound of the window to sort key of the current row, i.e., all rows with the same sort key as the current row are included in the window. If the following clause is omitted, the upper bound of a time interval window is defined as CURRENT_RANGE and the upper bound of a row-count interval window is defined as CURRENT_ROW.\nAs # Required\nAssigns an alias to the over window. The alias is used to reference the over window in the following select() clause.\nNote: Currently, all aggregation functions in the same select() call must be computed of the same over window.\nUnbounded Over Windows # Java // Unbounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(UNBOUNDED_RANGE).as(\u0026#34;w\u0026#34;)); // Unbounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy(\u0026#34;proctime\u0026#34;).preceding(UNBOUNDED_RANGE).as(\u0026#34;w\u0026#34;)); // Unbounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(UNBOUNDED_ROW).as(\u0026#34;w\u0026#34;)); // Unbounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(UNBOUNDED_ROW).as(\u0026#34;w\u0026#34;)); Scala // Unbounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // Unbounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // Unbounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) // Unbounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) Python # Unbounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(UNBOUNDED_RANGE).alias(\u0026#34;w\u0026#34;)) # Unbounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(UNBOUNDED_RANGE).alias(\u0026#34;w\u0026#34;)) # Unbounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(UNBOUNDED_ROW).alias(\u0026#34;w\u0026#34;)) # Unbounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(UNBOUNDED_ROW).alias(\u0026#34;w\u0026#34;)) Bounded Over Windows # Java // Bounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(lit(1).minutes()).as(\u0026#34;w\u0026#34;)); // Bounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(lit(1).minutes()).as(\u0026#34;w\u0026#34;)); // Bounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(rowInterval(10)).as(\u0026#34;w\u0026#34;)); // Bounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(rowInterval(10)).as(\u0026#34;w\u0026#34;)); Scala // Bounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // Bounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // Bounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) // Bounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) Python # Bounded Event-time over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(lit(1).minutes).alias(\u0026#34;w\u0026#34;)) # Bounded Processing-time over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(lit(1).minutes).alias(\u0026#34;w\u0026#34;)) # Bounded Event-time Row-count over window (assuming an event-time attribute \u0026#34;rowtime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(row_interval(10)).alias(\u0026#34;w\u0026#34;)) # Bounded Processing-time Row-count over window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(row_interval(10)).alias(\u0026#34;w\u0026#34;)) Back to top\nRow-based Operations # The row-based operations generate outputs with multiple columns.\nMap # Batch Streaming\nJava Performs a map operation with a user-defined scalar function or built-in scalar function. The output will be flattened if the output type is a composite type.\npublic class MyMapFunction extends ScalarFunction { public Row eval(String a) { return Row.of(a, \u0026#34;pre-\u0026#34; + a); } @Override public TypeInformation\u0026lt;?\u0026gt; getResultType(Class\u0026lt;?\u0026gt;[] signature) { return Types.ROW(Types.STRING, Types.STRING); } } ScalarFunction func = new MyMapFunction(); tableEnv.registerFunction(\u0026#34;func\u0026#34;, func); Table table = input .map(call(\u0026#34;func\u0026#34;, $(\u0026#34;c\u0026#34;))).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Scala Performs a map operation with a user-defined scalar function or built-in scalar function. The output will be flattened if the output type is a composite type.\nclass MyMapFunction extends ScalarFunction { def eval(a: String): Row = { Row.of(a, \u0026#34;pre-\u0026#34; + a) } override def getResultType(signature: Array[Class[_]]): TypeInformation[_] = Types.ROW(Types.STRING, Types.STRING) } val func = new MyMapFunction() val table = input .map(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) Python Performs a map operation with a python general scalar function or vectorized scalar function. The output will be flattened if the output type is a composite type.\nfrom pyflink.common import Row from pyflink.table import DataTypes from pyflink.table.udf import udf def map_function(a: Row) -\u0026gt; Row: return Row(a.a + 1, a.b * a.b) # map operation with a python general scalar function func = udf(map_function, result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())])) table = input.map(func).alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) # map operation with a python vectorized scalar function pandas_func = udf(lambda x: x * 2, result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]), func_type=\u0026#39;pandas\u0026#39;) table = input.map(pandas_func).alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) FlatMap # Batch Streaming\nJava Performs a flatMap operation with a table function.\npublic class MyFlatMapFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(String str) { if (str.contains(\u0026#34;#\u0026#34;)) { String[] array = str.split(\u0026#34;#\u0026#34;); for (int i = 0; i \u0026lt; array.length; ++i) { collect(Row.of(array[i], array[i].length())); } } } @Override public TypeInformation\u0026lt;Row\u0026gt; getResultType() { return Types.ROW(Types.STRING, Types.INT); } } TableFunction func = new MyFlatMapFunction(); tableEnv.registerFunction(\u0026#34;func\u0026#34;, func); Table table = input .flatMap(call(\u0026#34;func\u0026#34;, $(\u0026#34;c\u0026#34;))).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Scala Performs a flatMap operation with a python table function.\nclass MyFlatMapFunction extends TableFunction[Row] { def eval(str: String): Unit = { if (str.contains(\u0026#34;#\u0026#34;)) { str.split(\u0026#34;#\u0026#34;).foreach({ s =\u0026gt; val row = new Row(2) row.setField(0, s) row.setField(1, s.length) collect(row) }) } } override def getResultType: TypeInformation[Row] = { Types.ROW(Types.STRING, Types.INT) } } val func = new MyFlatMapFunction val table = input .flatMap(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) Python Performs a flat_map operation with a python table function.\nfrom pyflink.table.udf import udtf from pyflink.table import DataTypes from pyflink.common import Row @udtf(result_types=[DataTypes.INT(), DataTypes.STRING()]) def split(x: Row) -\u0026gt; Row: for s in x.b.split(\u0026#34;,\u0026#34;): yield x.a, s input.flat_map(split) Aggregate # Batch Streaming Result\nJava Performs an aggregate operation with an aggregate function. You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement and the select statement does not support aggregate functions. The output of aggregate will be flattened if the output type is a composite type.\npublic class MyMinMaxAcc { public int min = 0; public int max = 0; } public class MyMinMax extends AggregateFunction\u0026lt;Row, MyMinMaxAcc\u0026gt; { public void accumulate(MyMinMaxAcc acc, int value) { if (value \u0026lt; acc.min) { acc.min = value; } if (value \u0026gt; acc.max) { acc.max = value; } } @Override public MyMinMaxAcc createAccumulator() { return new MyMinMaxAcc(); } public void resetAccumulator(MyMinMaxAcc acc) { acc.min = 0; acc.max = 0; } @Override public Row getValue(MyMinMaxAcc acc) { return Row.of(acc.min, acc.max); } @Override public TypeInformation\u0026lt;Row\u0026gt; getResultType() { return new RowTypeInfo(Types.INT, Types.INT); } } AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(\u0026#34;myAggFunc\u0026#34;, myAggFunc); Table table = input .groupBy($(\u0026#34;key\u0026#34;)) .aggregate(call(\u0026#34;myAggFunc\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;x\u0026#34;), $(\u0026#34;y\u0026#34;)); Scala Performs an aggregate operation with an aggregate function. You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement and the select statement does not support aggregate functions. The output of aggregate will be flattened if the output type is a composite type.\ncase class MyMinMaxAcc(var min: Int, var max: Int) class MyMinMax extends AggregateFunction[Row, MyMinMaxAcc] { def accumulate(acc: MyMinMaxAcc, value: Int): Unit = { if (value \u0026lt; acc.min) { acc.min = value } if (value \u0026gt; acc.max) { acc.max = value } } override def createAccumulator(): MyMinMaxAcc = MyMinMaxAcc(0, 0) def resetAccumulator(acc: MyMinMaxAcc): Unit = { acc.min = 0 acc.max = 0 } override def getValue(acc: MyMinMaxAcc): Row = { Row.of(Integer.valueOf(acc.min), Integer.valueOf(acc.max)) } override def getResultType: TypeInformation[Row] = { new RowTypeInfo(Types.INT, Types.INT) } } val myAggFunc = new MyMinMax val table = input .groupBy($\u0026#34;key\u0026#34;) .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;) Python Performs an aggregate operation with a python general aggregate function or vectorized aggregate function. You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement and the select statement does not support aggregate functions. The output of aggregate will be flattened if the output type is a composite type.\nfrom pyflink.common import Row from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf class CountAndSumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return Row(accumulator[0], accumulator[1]) def create_accumulator(self): return Row(0, 0) def accumulate(self, accumulator, row: Row): accumulator[0] += 1 accumulator[1] += row.b def retract(self, accumulator, row: Row): accumulator[0] -= 1 accumulator[1] -= row.b def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] += other_acc[0] accumulator[1] += other_acc[1] def get_accumulator_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) function = CountAndSumAggregateFunction() agg = udaf(function, result_type=function.get_result_type(), accumulator_type=function.get_accumulator_type(), name=str(function.__class__.__name__)) # aggregate with a python general aggregate function result = t.group_by(col(\u0026#39;a\u0026#39;)) \\ .aggregate(agg.alias(\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;d\u0026#39;)) # aggregate with a python vectorized aggregate function pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) t.aggregate(pandas_udaf.alias(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;)) Group Window Aggregate # Batch Streaming\nGroups and aggregates a table on a group window and possibly one or more grouping keys. You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement. And the select statement does not support \u0026ldquo;*\u0026rdquo; or aggregate functions.\nJava AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(\u0026#34;myAggFunc\u0026#34;, myAggFunc); Table table = input .window(Tumble.over(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;)) // define window .groupBy($(\u0026#34;key\u0026#34;), $(\u0026#34;w\u0026#34;)) // group by key and window .aggregate(call(\u0026#34;myAggFunc\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;x\u0026#34;), $(\u0026#34;y\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end()); // access window properties and aggregate results Scala val myAggFunc = new MyMinMax val table = input .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // define window .groupBy($\u0026#34;key\u0026#34;, $\u0026#34;w\u0026#34;) // group by key and window .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end) // access window properties and aggregate results Python from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) t.select(col(\u0026#39;b\u0026#39;), col(\u0026#39;rowtime\u0026#39;)) \\ .window(tumble_window) \\ .group_by(col(\u0026#34;w\u0026#34;)) \\ .aggregate(pandas_udaf.alias(\u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;)) \\ .select(col(\u0026#39;w\u0026#39;).rowtime, col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;)) FlatAggregate # Java Similar to a GroupBy Aggregation. Groups the rows on the grouping keys with the following running table aggregation operator to aggregate rows group-wise. The difference from an AggregateFunction is that TableAggregateFunction may return 0 or more records for a group. You have to close the \u0026ldquo;flatAggregate\u0026rdquo; with a select statement. And the select statement does not support aggregate functions.\nInstead of using emitValue to output results, you can also use the emitUpdateWithRetract method. Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated. This method outputs data incrementally in retract mode, i.e., once there is an update, we have to retract old records before sending new updated ones. The emitUpdateWithRetract method will be used in preference to the emitValue method if both methods are defined in the table aggregate function, because the method is treated to be more efficient than emitValue as it can output values incrementally.\n/** * Accumulator for Top2. */ public class Top2Accum { public Integer first; public Integer second; } /** * The top2 user-defined table aggregate function. */ public class Top2 extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2Accum\u0026gt; { @Override public Top2Accum createAccumulator() { Top2Accum acc = new Top2Accum(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accum acc, Integer v) { if (v \u0026gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v \u0026gt; acc.second) { acc.second = v; } } public void merge(Top2Accum acc, java.lang.Iterable\u0026lt;Top2Accum\u0026gt; iterable) { for (Top2Accum otherAcc : iterable) { accumulate(acc, otherAcc.first); accumulate(acc, otherAcc.second); } } public void emitValue(Top2Accum acc, Collector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { // emit the value and rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)); } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)); } } } tEnv.registerFunction(\u0026#34;top2\u0026#34;, new Top2()); Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .groupBy($(\u0026#34;key\u0026#34;)) .flatAggregate(call(\u0026#34;top2\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;v\u0026#34;, \u0026#34;rank\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;v\u0026#34;), $(\u0026#34;rank\u0026#34;); Scala Similar to a GroupBy Aggregation. Groups the rows on the grouping keys with the following running table aggregation operator to aggregate rows group-wise. The difference from an AggregateFunction is that TableAggregateFunction may return 0 or more records for a group. You have to close the \u0026ldquo;flatAggregate\u0026rdquo; with a select statement. And the select statement does not support aggregate functions.\nInstead of using emitValue to output results, you can also use the emitUpdateWithRetract method. Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated. This method outputs data incrementally in retract mode, i.e., once there is an update, we have to retract old records before sending new updated ones. The emitUpdateWithRetract method will be used in preference to the emitValue method if both methods are defined in the table aggregate function, because the method is treated to be more efficient than emitValue as it can output values incrementally.\nimport java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = { val iter = its.iterator() while (iter.hasNext) { val top2 = iter.next() accumulate(acc, top2.first) accumulate(acc, top2.second) } } def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = { // emit the value and rank if (acc.first != Int.MinValue) { out.collect(JTuple2.of(acc.first, 1)) } if (acc.second != Int.MinValue) { out.collect(JTuple2.of(acc.second, 2)) } } } val top2 = new Top2 val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders .groupBy($\u0026#34;key\u0026#34;) .flatAggregate(top2($\u0026#34;a\u0026#34;) as ($\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;) Python Performs a flat_aggregate operation with a python general Table Aggregate Function\nSimilar to a GroupBy Aggregation. Groups the rows on the grouping keys with the following running table aggregation operator to aggregate rows group-wise. The difference from an AggregateFunction is that TableAggregateFunction may return 0 or more records for a group. You have to close the \u0026ldquo;flat_aggregate\u0026rdquo; with a select statement. And the select statement does not support aggregate functions.\nfrom pyflink.common import Row from pyflink.table.udf import TableAggregateFunction, udtaf from pyflink.table import DataTypes from pyflink.table.expressions import col class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row: Row): if row.a is not None: if accumulator[0] is None or row.a \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row.a elif accumulator[1] is None or row.a \u0026gt; accumulator[1]: accumulator[1] = row.a def merge(self, accumulator, accumulators): for other_acc in accumulators: self.accumulate(accumulator, other_acc[0]) self.accumulate(accumulator, other_acc[1]) def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) mytop = udtaf(Top2()) t = t_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) result = t.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;)) \\ .group_by(col(\u0026#39;c\u0026#39;)) \\ .flat_aggregate(mytop) \\ .select(col(\u0026#39;a\u0026#39;)) \\ .flat_aggregate(mytop.alias(\u0026#34;b\u0026#34;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Data Types # Please see the dedicated page about data types.\nGeneric types and (nested) composite types (e.g., POJOs, tuples, rows, Scala case classes) can be fields of a row as well.\nFields of composite types with arbitrary nesting can be accessed with value access functions.\nGeneric types are treated as a black box and can be passed on or processed by user-defined functions.\nBack to top\n"}),e.add({id:281,href:"/flink/flink-docs-master/docs/dev/python/table/python_types/",title:"Data Types",section:"Table API",content:` Data Types # This page describes the data types supported in PyFlink Table API.
Data Type # A data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of Python user-defined functions. Users of the Python Table API work with instances of pyflink.table.types.DataType within the Python Table API or when defining user-defined functions.
A DataType instance declares the logical type which does not imply a concrete physical representation for transmission or storage. All pre-defined data types are available in pyflink.table.types and can be instantiated with the utility methods defined in pyflink.table.types.DataTypes.
A list of all pre-defined data types can be found below.
Data Type and Python Type Mapping # A data type can be used to declare input and/or output types of Python user-defined functions. The inputs will be converted to Python objects corresponding to the data type and the type of the user-defined functions result must also match the defined data type.
For vectorized Python UDF, the input types and output type are pandas.Series. The element type of the pandas.Series corresponds to the specified data type.
Data Type Python Type Pandas Type BOOLEAN bool numpy.bool_ TINYINT int numpy.int8 SMALLINT int numpy.int16 INT int numpy.int32 BIGINT int numpy.int64 FLOAT float numpy.float32 DOUBLE float numpy.float64 VARCHAR str str VARBINARY bytes bytes DECIMAL decimal.Decimal decimal.Decimal DATE datetime.date datetime.date TIME datetime.time datetime.time TimestampType datetime.datetime datetime.datetime LocalZonedTimestampType datetime.datetime datetime.datetime INTERVAL YEAR TO MONTH int Not Supported Yet INTERVAL DAY TO SECOND datetime.timedelta Not Supported Yet ARRAY list numpy.ndarray MULTISET list Not Supported Yet MAP dict Not Supported Yet ROW Row dict `}),e.add({id:282,href:"/flink/flink-docs-master/docs/dev/table/sql/",title:"SQL",section:"Table API \u0026 SQL",content:""}),e.add({id:283,href:"/flink/flink-docs-master/docs/dev/python/table/system_functions/",title:"System (Built-in) Functions",section:"Table API",content:" "}),e.add({id:284,href:"/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/",title:"System (Built-in) Functions",section:"Functions",content:` System (Built-in) Functions # Flink Table API \u0026amp; SQL provides users with a set of built-in functions for data transformations. This page gives a brief overview of them. If a function that you need is not supported yet, you can implement a user-defined function. If you think that the function is general enough, please open a Jira issue for it with a detailed description.
Scalar Functions # The scalar functions take zero, one or more values as the input and return a single value as the result.
Comparison Functions # SQL Function Table Function Description value1 = value2 value1 === value2 Returns TRUE if value1 is equal to value2; returns UNKNOWN if value1 or value2 is NULL. value1 \u0026lt;\u0026gt; value2 value1 !== value2 Returns TRUE if value1 is not equal to value2; returns UNKNOWN if value1 or value2 is NULL. value1 \u0026gt; value2 value1 \u0026gt; value2 Returns TRUE if value1 is greater than value2; returns UNKNOWN if value1 or value2 is NULL. value1 \u0026gt;= value2 value1 \u0026gt;= value2 Returns TRUE if value1 is greater than or equal to value2; returns UNKNOWN if value1 or value2 is NULL. value1 \u0026lt; value2 value1 \u0026lt; value2 Returns TRUE if value1 is less than value2; returns UNKNOWN if value1 or value2 is NULL. value1 \u0026lt;= value2 value1 \u0026lt;= value2 Returns TRUE if value1 is less than or equal to value2; returns UNKNOWN if value1 or value2 is NULL. value IS NULL value.isNull Returns TRUE if value is NULL. value IS NOT NULL value.isNotNull Returns TRUE if value is not NULL. value1 IS DISTINCT FROM value2 N/A Returns TRUE if two values are different. NULL values are treated as identical here. E.g., 1 IS DISTINCT FROM NULL returns TRUE; NULL IS DISTINCT FROM NULL returns FALSE. value1 IS NOT DISTINCT FROM value2 N/A Returns TRUE if two values are equal. NULL values are treated as identical here. E.g., 1 IS NOT DISTINCT FROM NULL returns FALSE; NULL IS NOT DISTINCT FROM NULL returns TRUE. value1 BETWEEN [ ASYMMETRIC | SYMMETRIC ] value2 AND value3 N/A By default (or with the ASYMMETRIC keyword), returns TRUE if value1 is greater than or equal to value2 and less than or equal to value3. With the SYMMETRIC keyword, returns TRUE if value1 is inclusively between value2 and value3. When either value2 or value3 is NULL, returns FALSE or UNKNOWN. E.g., 12 BETWEEN 15 AND 12 returns FALSE; 12 BETWEEN SYMMETRIC 15 AND 12 returns TRUE; 12 BETWEEN 10 AND NULL returns UNKNOWN; 12 BETWEEN NULL AND 10 returns FALSE; 12 BETWEEN SYMMETRIC NULL AND 12 returns UNKNOWN. value1 NOT BETWEEN [ ASYMMETRIC | SYMMETRIC ] value2 AND value3 N/A By default (or with the ASYMMETRIC keyword), returns TRUE if value1 is less than value2 or greater than value3. With the SYMMETRIC keyword, returns TRUE if value1 is not inclusively between value2 and value3. When either value2 or value3 is NULL, returns TRUE or UNKNOWN. E.g., 12 NOT BETWEEN 15 AND 12 returns TRUE; 12 NOT BETWEEN SYMMETRIC 15 AND 12 returns FALSE; 12 NOT BETWEEN NULL AND 15 returns UNKNOWN; 12 NOT BETWEEN 15 AND NULL returns TRUE; 12 NOT BETWEEN SYMMETRIC 12 AND NULL returns UNKNOWN. string1 LIKE string2 [ ESCAPE char ] string1.like(string2) Returns TRUE if string1 matches pattern string2; returns UNKNOWN if string1 or string2 is NULL. An escape character can be defined if necessary. The escape character has not been supported yet. string1 NOT LIKE string2 [ ESCAPE char ] N/A Returns TRUE if string1 does not match pattern string2; returns UNKNOWN if string1 or string2 is NULL. An escape character can be defined if necessary. The escape character has not been supported yet. string1 SIMILAR TO string2 [ ESCAPE char ] string1.similar(string2) Returns TRUE if string1 matches SQL regular expression string2; returns UNKNOWN if string1 or string2 is NULL. An escape character can be defined if necessary. The escape character has not been supported yet. string1 NOT SIMILAR TO string2 [ ESCAPE char ] N/A Returns TRUE if string1 does not match SQL regular expression string2; returns UNKNOWN if string1 or string2 is NULL. An escape character can be defined if necessary. The escape character has not been supported yet. value1 IN (value2 [, value3]* ) value1.in(valu2) Returns TRUE if value1 exists in the given list (value2, value3, \u0026hellip;). When (value2, value3, \u0026hellip;). contains NULL, returns TRUE if the element can be found and UNKNOWN otherwise. Always returns UNKNOWN if value1 is NULL. E.g., 4 IN (1, 2, 3) returns FALSE; 1 IN (1, 2, NULL) returns TRUE; 4 IN (1, 2, NULL) returns UNKNOWN. value1 NOT IN (value2 [, value3]* ) N/A Returns TRUE if value1 does not exist in the given list (value2, value3, \u0026hellip;). When (value2, value3, \u0026hellip;). contains NULL, returns FALSE if value1 can be found and UNKNOWN otherwise. Always returns UNKNOWN if value1 is NULL. E.g., 4 NOT IN (1, 2, 3) returns TRUE; 1 NOT IN (1, 2, NULL) returns FALSE; 4 NOT IN (1, 2, NULL) returns UNKNOWN. EXISTS (sub-query) N/A Returns TRUE if sub-query returns at least one row. Only supported if the operation can be rewritten in a join and group operation. For streaming queries the operation is rewritten in a join and group operation. The required state to compute the query result might grow infinitely depending on the number of distinct input rows. Please provide a query configuration with valid retention interval to prevent excessive state size. value IN (sub-query) value1.in(TABLE) Returns TRUE if value is equal to a row returned by sub-query. value NOT IN (sub-query) N/A Returns TRUE if value is not equal to a row returned by sub-query. N/A value1.between(value2, value3) Returns TRUE if value is greater than or equal to value2 and less than or equal to value3. When either value2 or value3 is NULL, returns FALSE or UNKNOWN. N/A value1.notBetween(value2, value3) Returns FALSE if value is greater than or equal to value2 and less than or equal to value3. When either value2 or value3 is NULL, returns TRUE or UNKNOWN. Logical Functions # SQL Function Table Function Description boolean1 OR boolean2 BOOLEAN1 || BOOLEAN2 Returns TRUE if BOOLEAN1 is TRUE or BOOLEAN2 is TRUE. Supports three-valued logic. E.g., true || Null(BOOLEAN) returns TRUE. boolean1 AND boolean2 BOOLEAN1 \u0026amp;\u0026amp; BOOLEAN2 Returns TRUE if BOOLEAN1 and BOOLEAN2 are both TRUE. Supports three-valued logic. E.g., true \u0026amp;\u0026amp; Null(BOOLEAN) returns UNKNOWN. NOT boolean BOOLEAN.not(), not(BOOLEAN), or \u0026#39;!BOOLEAN\u0026#39; (Scala only) Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE; returns UNKNOWN if boolean is UNKNOWN. boolean IS FALSE BOOLEAN.isFalse Returns TRUE if boolean is FALSE; returns FALSE if boolean is TRUE or UNKNOWN. boolean IS NOT FALSE BOOLEAN.isNotFalse Returns TRUE if BOOLEAN is TRUE or UNKNOWN; returns FALSE if BOOLEAN is FALSE. boolean IS TRUE BOOLEAN.isTrue Returns TRUE if BOOLEAN is TRUE; returns FALSE if BOOLEAN is FALSE or UNKNOWN. boolean IS NOT TRUE BOOLEAN.isNotTrue Returns TRUE if boolean is FALSE or UNKNOWN; returns FALSE if boolean is TRUE. boolean IS UNKNOWN N/A Returns TRUE if boolean is UNKNOWN; returns FALSE if boolean is TRUE or FALSE. boolean IS NOT UNKNOWN N/A Returns TRUE if boolean is TRUE or FALSE; returns FALSE if boolean is UNKNOWN. Arithmetic Functions # SQL Function Table Function Description \u0026#43; numeric \u0026#43; NUMERIC Returns NUMERIC. - numeric - numeric Returns negative Numeric numeric1 \u0026#43; numeric2 NUMERIC1 \u0026#43; NUMERIC2 Returns NUMERIC1 plus NUMERIC2. numeric1 - numeric2 NUMERIC1 - NUMERIC2 Return NUMERIC1 minus NUMERIC2 numeric1 * numberic2 NUMERIC1 * NUMERIC2 Returns NUMERIC1 multiplied by NUMERIC2 numeric1 / numeric2 NUMERIC1 / NUMERIC2 Returns NUMERIC1 divided by NUMERIC2 numeric1 % numeric2 MOD(numeric1, numeric2) Returns the remainder (modulus) of numeric1 divided by numeric2. The result is negative only if numeric1 is negative. POWER(numeric1, numeric2) NUMERIC1.power(NUMERIC2) NUMERIC1.power(NUMERIC2) ABS(numeric) numeric.abs() Returns the absolute value of numeric. SQRT(numeric) NUMERIC.sqrt() Returns the square root of NUMERIC. LN(numeric) NUMERIC.ln() Returns the natural logarithm (base e) of NUMERIC. LOG10(numeric) numeric.log10() Returns the base 10 logarithm of numeric. LOG2(numeric) numeric.log2() Returns the base 2 logarithm of numeric. LOG(numeric2) LOG(numeric1, numeric2) NUMERIC1.log() NUMERIC1.log(NUMERIC2) When called with one argument, returns the natural logarithm of numeric2. When called with two arguments, this function returns the logarithm of numeric2 to the base numeric1. Currently, numeric2 must be greater than 0 and numeric1 must be greater than 1. EXP(numeric) NUMERIC.exp() Returns e raised to the power of numeric. CEIL(numeric) CEILING(numeric) NUMERIC.ceil() NUMERIC.ceiling() Rounds numeric up, and returns the smallest number that is greater than or equal to numeric. FLOOR(numeric) NUMERIC.floor() Rounds numeric down, and returns the largest number that is less than or equal to numeric. SIN(numeric) NUMERIC.sin() Returns the sine of numeric. SINH(numeric) NUMERIC.sinh() Returns the hyperbolic sine of numeric. The return type is DOUBLE. COS(numeric) NUMERIC.cos() Returns the cosine of numeric. TAN(numeric) NUMERIC.tan() Returns the tangent of numeric. TANH(numeric) NUMERIC.tanh() Returns the hyperbolic tangent of numeric. The return type is DOUBLE. COT(numeric) NUMERIC.cot() Returns the cotangent of a numeric. ASIN(numeric) NUMERIC.asin() Returns the arc sine of numeric. ACOS(numeric) NUMERIC.acos() Returns the arc cosine of numeric. ATAN(numeric) NUMERIC.atan() Returns the arc tangent of numeric. ATAN2(numeric1, numeric2) atan2(NUMERIC1, NUMERIC2) Returns the arc tangent of a coordinate (NUMERIC1, NUMERIC2). COSH(numeric) NUMERIC.cosh() Returns the hyperbolic cosine of NUMERIC. Return value type is DOUBLE. DEGREES(numeric) NUMERIC.degrees() Returns the degree representation of a radian NUMERIC. RADIANS(numeric) NUMERIC.radians() Returns the radian representation of a degree NUMERIC. SIGN(numeric) NUMERIC.sign() Returns the signum of NUMERIC. ROUND(NUMERIC, INT) NUMERIC.round(INT) Returns a number rounded to INT decimal places for NUMERIC. PI() pi() Returns a value that is closer than any other values to pi. E() e() Returns a value that is closer than any other values to e. RAND() rand() Returns a pseudorandom double value in the range [0.0, 1.0) RAND(INT) rand(INT) Returns a pseudorandom double value in the range [0.0, 1.0) with an initial seed integer. Two RAND functions will return identical sequences of numbers if they have the same initial seed. RAND_INTEGER(INT) randInteger(INT) Returns a pseudorandom integer value in the range [0, INT) RAND_INTEGER(INT1, INT2) randInteger(INT1, INT2) Returns a pseudorandom integer value in the range [0, INT2) with an initial seed INT1. Two RAND_INTGER functions will return idential sequences of numbers if they have the same initial seed and bound. UUID() uuid() Returns an UUID (Universally Unique Identifier) string (e.g., \u0026ldquo;3d3c68f7-f608-473f-b60c-b0c44ad4cc4e\u0026rdquo;) according to RFC 4122 type 4 (pseudo randomly generated) UUID. The UUID is generated using a cryptographically strong pseudo random number generator. BIN(INT) INT.bin() Returns a string representation of INTEGER in binary format. Returns NULL if INTEGER is NULL. E.g., 4.bin() returns \u0026ldquo;100\u0026rdquo; and 12.bin() returns \u0026ldquo;1100\u0026rdquo;. HEX(numeric) HEX(string) NUMERIC.hex() STRING.hex() Returns a string representation of an integer NUMERIC value or a STRING in hex format. Returns NULL if the argument is NULL. E.g. a numeric 20 leads to \u0026ldquo;14\u0026rdquo;, a numeric 100 leads to \u0026ldquo;64\u0026rdquo;, a string \u0026ldquo;hello,world\u0026rdquo; leads to \u0026ldquo;68656C6C6F2C776F726C64\u0026rdquo;. TRUNCATE(numeric1, integer2) numeric1.truncate(INTEGER2) Returns a numeric of truncated to integer2 decimal places. Returns NULL if numeric1 or integer2 is NULL. If integer2 is 0, the result has no decimal point or fractional part. integer2 can be negative to cause integer2 digits left of the decimal point of the value to become zero. This function can also pass in only one numeric1 parameter and not set Integer2 to use. If Integer2 is not set, the function truncates as if Integer2 were 0. E.g. 42.324.truncate(2) to 42.32. and 42.324.truncate() to 42.0. String Functions # SQL Function Table Function Description string1 || string2 STRING1 \u0026#43; STRING2 Returns the concatenation of STRING1 and STRING2. CHAR_LENGTH(string) CHARACTER_LENGTH(string) STRING.charLength() Returns the number of characters in STRING. UPPER(string) STRING.upperCase() Returns STRING in uppercase. LOWER(string) STRING.lowerCase() Returns string in lowercase. POSITION(string1 IN string2) STRING1.position(STRING2) Returns the position (start from 1) of the first occurrence of STRING1 in STRING2; returns 0 if STRING1 cannot be found in STRING2. TRIM([ BOTH | LEADING | TRAILING ] string1 FROM string2) STRING1.trim(LEADING, STRING2) STRING1.trim(TRAILING, STRING2) STRING1.trim(BOTH, STRING2) STRING1.trim(BOTH) STRING1.trim() Returns a string that removes leading and/or trailing characters STRING2 from STRING1. By default, whitespaces at both sides are removed. LTRIM(string) STRING.ltrim() Returns a string that removes the left whitespaces from STRING. E.g., \u0026rsquo; This is a test String.\u0026rsquo;.ltrim() returns \u0026ldquo;This is a test String.\u0026rdquo;. RTRIM(string) STRING.rtrim() Returns a string that removes the right whitespaces from STRING. E.g., \u0026lsquo;This is a test String. \u0026lsquo;.rtrim() returns \u0026ldquo;This is a test String.\u0026rdquo;. REPEAT(string, int) STRING.repeat(INT) Returns a string that repeats the base string integer times. E.g., REPEAT(\u0026lsquo;This is a test String.\u0026rsquo;, 2) returns \u0026ldquo;This is a test String.This is a test String.\u0026rdquo;. REGEXP_REPLACE(string1, string2, string3) STRING1.regexpReplace(STRING2, STRING3) Returns a string from STRING1 with all the substrings that match a regular expression STRING2 consecutively being replaced with STRING3. E.g., \u0026lsquo;foobar\u0026rsquo;.regexpReplace(\u0026lsquo;oo|ar\u0026rsquo;, \u0026lsquo;\u0026rsquo;) returns \u0026ldquo;fb\u0026rdquo;. OVERLAY(string1 PLACING string2 FROM integer1 [ FOR integer2 ]) STRING1.overlay(STRING2, INT1) STRING1.overlay(STRING2, INT1, INT2) Returns a string that replaces INT2 (STRING2\u0026rsquo;s length by default) characters of STRING1 with STRING2 from position INT1. E.g., \u0026lsquo;xxxxxtest\u0026rsquo;.overlay(\u0026lsquo;xxxx\u0026rsquo;, 6) returns \u0026ldquo;xxxxxxxxx\u0026rdquo;; \u0026lsquo;xxxxxtest\u0026rsquo;.overlay(\u0026lsquo;xxxx\u0026rsquo;, 6, 2) returns \u0026ldquo;xxxxxxxxxst\u0026rdquo;. SUBSTRING(string FROM integer1 [ FOR integer2 ]) STRING.substring(INT1) STRING.substring(INT1, INT2) Returns a substring of STRING starting from position INT1 with length INT2 (to the end by default). REPLACE(string1, string2, string3) STRING1.replace(STRING2, STRING3) Returns a new string which replaces all the occurrences of STRING2 with STRING3 (non-overlapping) from STRING1. E.g., \u0026lsquo;hello world\u0026rsquo;.replace(\u0026lsquo;world\u0026rsquo;, \u0026lsquo;flink\u0026rsquo;) returns \u0026lsquo;hello flink\u0026rsquo;; \u0026lsquo;ababab\u0026rsquo;.replace(\u0026lsquo;abab\u0026rsquo;, \u0026lsquo;z\u0026rsquo;) returns \u0026lsquo;zab\u0026rsquo;. REGEXP_EXTRACT(string1, string2[, integer]) STRING1.regexpExtract(STRING2[, INTEGER1]) Returns a string from string1 which extracted with a specified regular expression string2 and a regex match group index integer.
The regex match group index starts from 1 and 0 means matching the whole regex. In addition, the regex match group index should not exceed the number of the defined groups.
E.g. REGEXP_EXTRACT(\u0026lsquo;foothebar\u0026rsquo;, \u0026lsquo;foo(.*?)(bar)\u0026rsquo;, 2)\u0026quot; returns \u0026ldquo;bar\u0026rdquo;.
INITCAP(string) STRING.initCap() Returns a new form of STRING with the first character of each word converted to uppercase and the rest characters to lowercase. Here a word means a sequences of alphanumeric characters. CONCAT(string1, string2,...) concat(STRING1, STRING2, ...) Returns a string that concatenates string1, string2, \u0026hellip;. Returns NULL if any argument is NULL. E.g., CONCAT(\u0026lsquo;AA\u0026rsquo;, \u0026lsquo;BB\u0026rsquo;, \u0026lsquo;CC\u0026rsquo;) returns \u0026ldquo;AABBCC\u0026rdquo;. CONCAT_WS(string1, string2, string3,...) concat_ws(STRING1, STRING2, STRING3, ...) Returns a string that concatenates STRING2, STRING3, \u0026hellip; with a separator STRING1. The separator is added between the strings to be concatenated. Returns NULL If STRING1 is NULL. Compared with concat(), concat_ws() automatically skips NULL arguments. E.g., concat_ws(\u0026rsquo;~\u0026rsquo;, \u0026lsquo;AA\u0026rsquo;, Null(STRING), \u0026lsquo;BB\u0026rsquo;, \u0026lsquo;\u0026rsquo;, \u0026lsquo;CC\u0026rsquo;) returns \u0026ldquo;AA~BB~~CC\u0026rdquo;. LPAD(string1, integer, string2) STRING1.lpad(INT, STRING2) Returns a new string from string1 left-padded with string2 to a length of integer characters. If the length of string1 is shorter than integer, returns string1 shortened to integer characters. E.g., LPAD(\u0026lsquo;hi\u0026rsquo;, 4, \u0026lsquo;??\u0026rsquo;) returns \u0026ldquo;??hi\u0026rdquo;; LPAD(\u0026lsquo;hi\u0026rsquo;, 1, \u0026lsquo;??\u0026rsquo;) returns \u0026ldquo;h\u0026rdquo;. RPAD(string1, integer, string2) STRING1.rpad(INT, STRING2) Returns a new string from string1 right-padded with string2 to a length of integer characters. If the length of string1 is shorter than integer, returns string1 shortened to integer characters. E.g., RPAD(\u0026lsquo;hi\u0026rsquo;, 4, \u0026lsquo;??\u0026rsquo;) returns \u0026ldquo;hi??\u0026rdquo;, RPAD(\u0026lsquo;hi\u0026rsquo;, 1, \u0026lsquo;??\u0026rsquo;) returns \u0026ldquo;h\u0026rdquo;. FROM_BASE64(string) STRING.fromBase64() Returns the base64-decoded result from string; returns NULL if string is NULL. E.g., FROM_BASE64(\u0026lsquo;aGVsbG8gd29ybGQ=\u0026rsquo;) returns \u0026ldquo;hello world\u0026rdquo;. TO_BASE64(string) STRING.toBase64() Returns the base64-encoded result from string; returns NULL if string is NULL. E.g., TO_BASE64(\u0026lsquo;hello world\u0026rsquo;) returns \u0026ldquo;aGVsbG8gd29ybGQ=\u0026rdquo;. ASCII(string) STRING.ascii() Returns the numeric value of the first character of string. Returns NULL if string is NULL. E.g., ascii(\u0026lsquo;abc\u0026rsquo;) returns 97, and ascii(CAST(NULL AS VARCHAR)) returns NULL. CHR(integer) INT.chr() Returns the ASCII character having the binary equivalent to integer. If integer is larger than 255, we will get the modulus of integer divided by 255 first, and returns CHR of the modulus. Returns NULL if integer is NULL. E.g., chr(97) returns a, chr(353) returns a, and ascii(CAST(NULL AS VARCHAR)) returns NULL. DECODE(binary, string) BINARY.decode(STRING) Decodes the first argument into a String using the provided character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null. ENCODE(string1, string2) STRING1.encode(STRING2) Encodes the string1 into a BINARY using the provided string2 character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null. INSTR(string1, string2) STRING1.instr(STRING2) Returns the position of the first occurrence of string2 in string1. Returns NULL if any of arguments is NULL. LEFT(string, integer) STRING.LEFT(INT) Returns the leftmost integer characters from the string. Returns EMPTY String if integer is negative. Returns NULL if any argument is NULL. RIGHT(string, integer) STRING.RIGHT(INT) Returns the rightmost integer characters from the string. Returns EMPTY String if integer is negative. Returns NULL if any argument is NULL. LOCATE(string1, string2[, integer]) STRING1.locate(STRING2[, INTEGER]) Returns the position of the first occurrence of string1 in string2 after position integer. Returns 0 if not found. Returns NULL if any of arguments is NULL. PARSE_URL(string1, string2[, string3]) STRING1.parseUrl(STRING2[, STRING3]) Returns the specified part from the URL. Valid values for string2 include \u0026lsquo;HOST\u0026rsquo;, \u0026lsquo;PATH\u0026rsquo;, \u0026lsquo;QUERY\u0026rsquo;, \u0026lsquo;REF\u0026rsquo;, \u0026lsquo;PROTOCOL\u0026rsquo;, \u0026lsquo;AUTHORITY\u0026rsquo;, \u0026lsquo;FILE\u0026rsquo;, and \u0026lsquo;USERINFO\u0026rsquo;. Returns NULL if any of arguments is NULL.
E.g., parse_url(\u0026lsquo;http://facebook.com/path1/p.php?k1=v1\u0026k2=v2#Ref1', \u0026lsquo;HOST\u0026rsquo;), returns \u0026lsquo;facebook.com\u0026rsquo;.
Also a value of a particular key in QUERY can be extracted by providing the key as the third argument string3.
E.g., parse_url(\u0026lsquo;http://facebook.com/path1/p.php?k1=v1\u0026k2=v2#Ref1', \u0026lsquo;QUERY\u0026rsquo;, \u0026lsquo;k1\u0026rsquo;) returns \u0026lsquo;v1\u0026rsquo;.
REGEXP(string1, string2) STRING1.regexp(STRING2) Returns TRUE if any (possibly empty) substring of string1 matches the Java regular expression string2, otherwise FALSE. Returns NULL if any of arguments is NULL. REVERSE(string) STRING.reverse() Returns the reversed string. Returns NULL if string is NULL. SPLIT_INDEX(string1, string2, integer1) STRING1.splitIndex(STRING2, INTEGER1) Splits string1 by the delimiter string2, returns the integerth (zero-based) string of the split strings. Returns NULL if integer is negative. Returns NULL if any of arguments is NULL. STR_TO_MAP(string1[, string2, string3]) STRING1.strToMap([STRING2, STRING3]) Returns a map after splitting the string1 into key/value pairs using delimiters. string2 is the pair delimiter, default is \u0026lsquo;,\u0026rsquo;. And string3 is the key-value delimiter, default is \u0026lsquo;=\u0026rsquo;. Both pair delimiter and key-value delimiter are treated as regular expressions. So special characters (e.g. \u0026lt;([{\\^-=\$!|]})?*+.\u0026gt;) need to be properly escaped before using as a delimiter literally. SUBSTR(string, integer1[, integer2]) STRING.substr(INTEGER1[, INTEGER2]) Returns a substring of string starting from position integer1 with length integer2 (to the end by default). Temporal Functions # SQL Function Table Function Description DATE string STRING.toDate() Returns a SQL date parsed from string in form of \u0026ldquo;yyyy-MM-dd\u0026rdquo;. TIME string STRING.toTime() Returns a SQL time parsed from string in form of \u0026ldquo;HH:mm:ss\u0026rdquo;. TIMESTAMP string STRING.toTimestamp() Returns a SQL timestamp parsed from string in form of \u0026ldquo;yyyy-MM-dd HH:mm:ss[.SSS]\u0026rdquo;. INTERVAL string range N/A Parses an interval string in the form \u0026ldquo;dd hh:mm:ss.fff\u0026rdquo; for SQL intervals of milliseconds or \u0026ldquo;yyyy-mm\u0026rdquo; for SQL intervals of months. An interval range might be DAY, MINUTE, DAY TO HOUR, or DAY TO SECOND for intervals of milliseconds; YEAR or YEAR TO MONTH for intervals of months.
E.g., INTERVAL \u0026lsquo;10 00:00:00.004\u0026rsquo; DAY TO SECOND, INTERVAL \u0026lsquo;10\u0026rsquo; DAY, or INTERVAL \u0026lsquo;2-10\u0026rsquo; YEAR TO MONTH return intervals.
N/A NUMERIC.year NUMERIC.years Creates an interval of months for NUMERIC years. N/A NUMERIC.quarter NUMERIC.quarters Creates an interval of months for NUMERIC quarters. E.g., 2.quarters returns 6. N/A NUMERIC.month NUMERIC.months Creates an interval of NUMERIC months. N/A NUMERIC.week NUMERIC.weeks Creates an interval of milliseconds for NUMERIC weeks. E.g., 2.weeks returns 1209600000. N/A NUMERIC.day NUMERIC.days Creates an interval of milliseconds for NUMERIC days. N/A NUMERIC.hour NUMERIC.hours Creates an interval of milliseconds for NUMERIC hours. N/A NUMERIC.minute NUMERIC.minutes Creates an interval of milliseconds for NUMERIC minutes. N/A NUMERIC.second NUMERIC.seconds Creates an interval of milliseconds for NUMERIC seconds. N/A NUMERIC.milli NUMERIC.millis Creates an interval of NUMERIC milliseconds. LOCALTIME localTime() Returns the current SQL time in the local time zone, the return type is TIME(0). It is evaluated for each record in streaming mode. But in batch mode, it is evaluated once as the query starts and uses the same result for every row. LOCALTIMESTAMP localTimestamp() Returns the current SQL timestamp in local time zone, the return type is TIMESTAMP(3). It is evaluated for each record in streaming mode. But in batch mode, it is evaluated once as the query starts and uses the same result for every row. CURRENT_TIME currentTime() Returns the current SQL time in the local time zone, this is a synonym of LOCAL_TIME. CURRENT_DATE currentDate() Returns the current SQL date in the local time zone. It is evaluated for each record in streaming mode. But in batch mode, it is evaluated once as the query starts and uses the same result for every row. CURRENT_TIMESTAMP currentTimestamp() Returns the current SQL timestamp in the local time zone, the return type is TIMESTAMP_LTZ(3). It is evaluated for each record in streaming mode. But in batch mode, it is evaluated once as the query starts and uses the same result for every row. NOW() N/A Returns the current SQL timestamp in the local time zone, this is a synonym of CURRENT_TIMESTAMP. CURRENT_ROW_TIMESTAMP() N/A Returns the current SQL timestamp in the local time zone, the return type is TIMESTAMP_LTZ(3). It is evaluated for each record no matter in batch or streaming mode. EXTRACT(timeinteravlunit FROM temporal) TEMPORAL.extract(TIMEINTERVALUNIT) Returns a long value extracted from the timeintervalunit part of temporal. E.g., EXTRACT(DAY FROM DATE \u0026lsquo;2006-06-05\u0026rsquo;) returns 5. YEAR(date) N/A Returns the year from SQL date. Equivalent to EXTRACT(YEAR FROM date). E.g., YEAR(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 1994. QUARTER(date) N/A Returns the quarter of a year (an integer between 1 and 4) from SQL date. Equivalent to EXTRACT(QUARTER FROM date). E.g., QUARTER(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 3. MONTH(date) N/A Returns the month of a year (an integer between 1 and 12) from SQL date. Equivalent to EXTRACT(MONTH FROM date). E.g., MONTH(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 9. WEEK(date) N/A Returns the week of a year (an integer between 1 and 53) from SQL date. Equivalent to EXTRACT(WEEK FROM date). E.g., WEEK(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 39. DAYOFYEAR(date) N/A Returns the day of a year (an integer between 1 and 366) from SQL date. Equivalent to EXTRACT(DOY FROM date). E.g., DAYOFYEAR(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 270. DAYOFMONTH N/A Returns the day of a month (an integer between 1 and 31) from SQL date. Equivalent to EXTRACT(DAY FROM date). E.g., DAYOFMONTH(DATE \u0026lsquo;1994-09-27\u0026rsquo;) returns 27. HOUR(timestamp) N/A Returns the hour of a day (an integer between 0 and 23) from SQL timestamp timestamp. Equivalent to EXTRACT(HOUR FROM timestamp). E.g., MINUTE(TIMESTAMP \u0026lsquo;1994-09-27 13:14:15\u0026rsquo;) returns 14. MINUTE(timestamp) N/A Returns the minute of an hour (an integer between 0 and 59) from SQL timestamp timestamp. Equivalent to EXTRACT(MINUTE FROM timestamp). E.g., MINUTE(TIMESTAMP \u0026lsquo;1994-09-27 13:14:15\u0026rsquo;) returns 14. SECOND(timestamp) N/A Returns the second of a minute (an integer between 0 and 59) from SQL timestamp. Equivalent to EXTRACT(SECOND FROM timestamp). E.g., SECOND(TIMESTAMP \u0026lsquo;1994-09-27 13:14:15\u0026rsquo;) returns 15. FLOOR(timepoint TO timeintervalunit) TIMEPOINT.floor(TIMEINTERVALUNIT) Returns a value that rounds timepoint down to the time unit timeintervalunit. E.g., FLOOR(TIME \u0026lsquo;12:44:31\u0026rsquo; TO MINUTE) returns 12:44:00. CEIL(timepoint TO timeintervaluntit) TIMEPOINT.ceil(TIMEINTERVALUNIT) Returns a value that rounds timepoint up to the time unit timeintervalunit. E.g., CEIL(TIME \u0026lsquo;12:44:31\u0026rsquo; TO MINUTE) returns 12:45:00. (timepoint1, temporal1) OVERLAPS (timepoint2, temporal2) temporalOverlaps(TIMEPOINT1, TEMPORAL1, TIMEPOINT2, TEMPORAL2) Returns TRUE if two time intervals defined by (timepoint1, temporal1) and (timepoint2, temporal2) overlap. The temporal values could be either a time point or a time interval. E.g., (TIME \u0026lsquo;2:55:00\u0026rsquo;, INTERVAL \u0026lsquo;1\u0026rsquo; HOUR) OVERLAPS (TIME \u0026lsquo;3:30:00\u0026rsquo;, INTERVAL \u0026lsquo;2\u0026rsquo; HOUR) returns TRUE; (TIME \u0026lsquo;9:00:00\u0026rsquo;, TIME \u0026lsquo;10:00:00\u0026rsquo;) OVERLAPS (TIME \u0026lsquo;10:15:00\u0026rsquo;, INTERVAL \u0026lsquo;3\u0026rsquo; HOUR) returns FALSE. DATE_FORMAT(timestamp, string) N/A Converts timestamp to a value of string in the format specified by the date format string. The format string is compatible with Java\u0026rsquo;s SimpleDateFormat. TIMESTAMPADD(timeintervalunit, interval, timepoint) N/A TIMESTAMPDIFF(timepointunit, timepoint1, timepoint2) timestampDiff(TIMEPOINTUNIT, TIMEPOINT1, TIMEPOINT2) Returns the (signed) number of timepointunit between timepoint1 and timepoint2. The unit for the interval is given by the first argument, which should be one of the following values: SECOND, MINUTE, HOUR, DAY, MONTH, or YEAR. CONVERT_TZ(string1, string2, string3) N/A Converts a datetime string1 (with default ISO timestamp format \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;) from time zone string2 to time zone string3. The format of time zone should be either an abbreviation such as \u0026ldquo;PST\u0026rdquo;, a full name such as \u0026ldquo;America/Los_Angeles\u0026rdquo;, or a custom ID such as \u0026ldquo;GMT-08:00\u0026rdquo;. E.g., CONVERT_TZ(\u0026lsquo;1970-01-01 00:00:00\u0026rsquo;, \u0026lsquo;UTC\u0026rsquo;, \u0026lsquo;America/Los_Angeles\u0026rsquo;) returns \u0026lsquo;1969-12-31 16:00:00\u0026rsquo;. FROM_UNIXTIME(numeric[, string]) fromUnixtime(NUMERIC[, STRING]) Returns a representation of the numeric argument as a value in string format (default is \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;). numeric is an internal timestamp value representing seconds since \u0026lsquo;1970-01-01 00:00:00\u0026rsquo; UTC, such as produced by the UNIX_TIMESTAMP() function. The return value is expressed in the session time zone (specified in TableConfig). E.g., FROM_UNIXTIME(44) returns \u0026lsquo;1970-01-01 00:00:44\u0026rsquo; if in UTC time zone, but returns \u0026lsquo;1970-01-01 09:00:44\u0026rsquo; if in \u0026lsquo;Asia/Tokyo\u0026rsquo; time zone. UNIX_TIMESTAMP() N/A Gets current Unix timestamp in seconds. This function is not deterministic which means the value would be recalculated for each record. UNIX_TIMESTAMP(string1[, string2]) N/A Converts date time string string1 in format string2 (by default: yyyy-MM-dd HH:mm:ss if not specified) to Unix timestamp (in seconds), using the specified timezone in table config. TO_DATE(string1[, string2]) N/A Converts a date string string1 with format string2 (by default \u0026lsquo;yyyy-MM-dd\u0026rsquo;) to a date. TO_TIMESTAMP_LTZ(numeric, precision) toTimestampLtz(NUMERIC, PRECISION) Converts a epoch seconds or epoch milliseconds to a TIMESTAMP_LTZ, the valid precision is 0 or 3, the 0 represents TO_TIMESTAMP_LTZ(epochSeconds, 0), the 3 represents TO_TIMESTAMP_LTZ(epochMilliseconds, 3). TO_TIMESTAMP(string1[, string2]) N/A Converts date time string string1 with format string2 (by default: \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;) under the \u0026lsquo;UTC+0\u0026rsquo; time zone to a timestamp. CURRENT_WATERMARK(rowtime) N/A Returns the current watermark for the given rowtime attribute, or NULL if no common watermark of all upstream operations is available at the current operation in the pipeline. The return type of the function is inferred to match that of the provided rowtime attribute, but with an adjusted precision of 3. For example, if the rowtime attribute is TIMESTAMP_LTZ(9), the function will return TIMESTAMP_LTZ(3).
Note that this function can return NULL, and you may have to consider this case. For example, if you want to filter out late data you can use:
WHERE CURRENT_WATERMARK(ts) IS NULL OR ts \u0026gt; CURRENT_WATERMARK(ts) Conditional Functions # SQL Function Table Function Description CASE value WHEN value1_1 [, value1_2]* THEN RESULT1 (WHEN value2_1 [, value2_2 ]* THEN result_2)* (ELSE result_z) END N/A Returns resultX when the first time value is contained in (valueX_1, valueX_2, \u0026hellip;). When no value matches, returns result_z if it is provided and returns NULL otherwise. CASE WHEN condition1 THEN result1 (WHEN condition2 THEN result2)* (ELSE result_z) END N/A Returns resultX when the first conditionX is met. When no condition is met, returns result_z if it is provided and returns NULL otherwise. NULLIF(value1, value2) N/A Returns NULL if value1 is equal to value2; returns value1 otherwise. E.g., NULLIF(5, 5) returns NULL; NULLIF(5, 0) returns 5. COALESCE(value1 [, value2]*) coalesce(value1, [, value2]*) Returns the first argument that is not NULL.
If all arguments are NULL, it returns NULL as well. The return type is the least restrictive, common type of all of its arguments. The return type is nullable if all arguments are nullable as well.
-- Returns \u0026#39;default\u0026#39; COALESCE(NULL, \u0026#39;default\u0026#39;) -- Returns the first non-null value among f0 and f1, -- or \u0026#39;default\u0026#39; if f0 and f1 are both NULL COALESCE(f0, f1, \u0026#39;default\u0026#39;) IF(condition, true_value, false_value) N/A Returns the true_value if condition is met, otherwise false_value. E.g., IF(5 \u0026gt; 3, 5, 3) returns 5. IFNULL(input, null_replacement) input.ifNull(nullReplacement) Returns null_replacement if input is NULL; otherwise input is returned.
Compared to COALESCE or CASE WHEN, this function returns a data type that is very specific in terms of nullability. The returned type is the common type of both arguments but only nullable if the null_replacement is nullable.
The function allows to pass nullable columns into a function or table that is declared with a NOT NULL constraint.
E.g., IFNULL(nullable_column, 5) returns never NULL.
IS_ALPHA(string) N/A Returns true if all characters in string are letter, otherwise false. IS_DECIMAL(string) N/A Returns true if string can be parsed to a valid numeric, otherwise false. IS_DIGIT(string) N/A Returns true if all characters in string are digit, otherwise false. N/A BOOLEAN.?(VALUE1, VALUE2) Returns VALUE1 if BOOLEAN evaluates to TRUE; returns VALUE2 otherwise. E.g., (42 \u0026gt; 5).?(\u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;) returns \u0026ldquo;A\u0026rdquo;. GREATEST(value1[, value2]*) N/A Returns the greatest value of the list of arguments. Returns NULL if any argument is NULL. LEAST(value1[, value2]*) N/A Returns the least value of the list of arguments. Returns NULL if any argument is NULL. Type Conversion Functions # SQL Function Table Function Description CAST(value AS type) ANY.cast(TYPE) Returns a new value being cast to type type. A CAST error throws an exception and fails the job. When performing a cast operation that may fail, like STRING to INT, one should rather use TRY_CAST, in order to handle errors. If \u0026ldquo;table.exec.legacy-cast-behaviour\u0026rdquo; is enabled, CAST behaves like TRY_CAST. E.g., CAST(\u0026lsquo;42\u0026rsquo; AS INT) returns 42; CAST(NULL AS STRING) returns NULL of type STRING; CAST(\u0026rsquo;non-number\u0026rsquo; AS INT) throws an exception and fails the job. TRY_CAST(value AS type) ANY.tryCast(TYPE) Like CAST, but in case of error, returns NULL rather than failing the job. E.g., TRY_CAST(\u0026lsquo;42\u0026rsquo; AS INT) returns 42; TRY_CAST(NULL AS STRING) returns NULL of type STRING; TRY_CAST(\u0026rsquo;non-number\u0026rsquo; AS INT) returns NULL of type INT; COALESCE(TRY_CAST(\u0026rsquo;non-number\u0026rsquo; AS INT), 0) returns 0 of type INT. TYPEOF(input) TYPEOF(input, force_serializable) call(\u0026#34;TYPEOF\u0026#34;, input) call(\u0026#34;TYPEOF\u0026#34;, input, force_serializable) Returns the string representation of the input expression\u0026rsquo;s data type. By default, the returned string is a summary string that might omit certain details for readability. If force_serializable is set to TRUE, the string represents a full data type that could be persisted in a catalog. Note that especially anonymous, inline data types have no serializable string representation. In this case, NULL is returned. Collection Functions # SQL Function Table Function Description CARDINALITY(array) ARRAY.cardinality() Returns the number of elements in array. array \u0026#39;[\u0026#39; INT \u0026#39;]\u0026#39; ARRAY.at(INT) Returns the element at position INT in array. The index starts from 1. ELEMENT(array) ARRAY.element() Returns the sole element of array (whose cardinality should be one); returns NULL if array is empty. Throws an exception if array has more than one element. CARDINALITY(map) MAP.cardinality() Returns the number of entries in map. map ‘[’ value ‘]’ MAP.at(ANY) Returns the value specified by key value in map. ARRAY_CONTAINS(haystack, needle) haystack.arrayContains(needle) Returns whether the given element exists in an array. Checking for null elements in the array is supported. If the array itself is null, the function will return null. The given element is cast implicitly to the array\u0026rsquo;s element type if necessary. JSON Functions # JSON functions make use of JSON path expressions as described in ISO/IEC TR 19075-6 of the SQL standard. Their syntax is inspired by and adopts many features of ECMAScript, but is neither a subset nor superset thereof.
Path expressions come in two flavors, lax and strict. When omitted, it defaults to the strict mode. Strict mode is intended to examine data from a schema perspective and will throw errors whenever data does not adhere to the path expression. However, functions like JSON_VALUE allow defining fallback behavior if an error is encountered. Lax mode, on the other hand, is more forgiving and converts errors to empty sequences.
The special character \$ denotes the root node in a JSON path. Paths can access properties (\$.a), array elements (\$.a[0].b), or branch over all elements in an array (\$.a[*].b).
Known Limitations:
Not all features of Lax mode are currently supported correctly. This is an upstream bug (CALCITE-4717). Non-standard behavior is not guaranteed. SQL Function Table Function Description IS JSON [ { VALUE | SCALAR | ARRAY | OBJECT } ] STRING.isJson([JsonType type]) Determine whether a given string is valid JSON.
Specifying the optional type argument puts a constraint on which type of JSON object is allowed. If the string is valid JSON, but not that type, false is returned. The default is VALUE.
-- TRUE \u0026#39;1\u0026#39; IS JSON \u0026#39;[]\u0026#39; IS JSON \u0026#39;{}\u0026#39; IS JSON -- TRUE \u0026#39;\u0026#34;abc\u0026#34;\u0026#39; IS JSON -- FALSE \u0026#39;abc\u0026#39; IS JSON NULL IS JSON -- TRUE \u0026#39;1\u0026#39; IS JSON SCALAR -- FALSE \u0026#39;1\u0026#39; IS JSON ARRAY -- FALSE \u0026#39;1\u0026#39; IS JSON OBJECT -- FALSE \u0026#39;{}\u0026#39; IS JSON SCALAR -- FALSE \u0026#39;{}\u0026#39; IS JSON ARRAY -- TRUE \u0026#39;{}\u0026#39; IS JSON OBJECT JSON_EXISTS(jsonValue, path [ { TRUE | FALSE | UNKNOWN | ERROR } ON ERROR ]) STRING.jsonExists(STRING path [, JsonExistsOnError onError]) Determines whether a JSON string satisfies a given path search criterion.
If the error behavior is omitted, FALSE ON ERROR is assumed as the default.
-- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;\$.a\u0026#39;); -- FALSE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;\$.b\u0026#39;); -- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: [{ \u0026#34;b\u0026#34;: 1 }]}\u0026#39;, \u0026#39;\$.a[0].b\u0026#39;); -- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict \$.b\u0026#39; TRUE ON ERROR); -- FALSE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict \$.b\u0026#39; FALSE ON ERROR); JSON_STRING(value) jsonString(value) Serializes a value into JSON.
This function returns a JSON string containing the serialized value. If the value is NULL, the function returns NULL.
-- NULL JSON_STRING(CAST(NULL AS INT)) -- \u0026#39;1\u0026#39; JSON_STRING(1) -- \u0026#39;true\u0026#39; JSON_STRING(TRUE) -- \u0026#39;\u0026#34;Hello, World!\u0026#34;\u0026#39; JSON_STRING(\u0026#39;Hello, World!\u0026#39;) -- \u0026#39;[1,2]\u0026#39; JSON_STRING(ARRAY[1, 2]) JSON_VALUE(jsonValue, path [RETURNING \u0026lt;dataType\u0026gt;] [ { NULL | ERROR | DEFAULT \u0026lt;defaultExpr\u0026gt; } ON EMPTY ] [ { NULL | ERROR | DEFAULT \u0026lt;defaultExpr\u0026gt; } ON ERROR ]) STRING.jsonValue(STRING path [, returnType, onEmpty, defaultOnEmpty, onError, defaultOnError]) Extracts a scalar from a JSON string.
This method searches a JSON string for a given path expression and returns the value if the value at that path is scalar. Non-scalar values cannot be returned. By default, the value is returned as STRING. Using returningType a different type can be chosen, with the following types being supported:
VARCHAR / STRING BOOLEAN INTEGER DOUBLE For empty path expressions or errors a behavior can be defined to either return null, raise an error or return a defined default value instead. When omitted, the default is NULL ON EMPTY or NULL ON ERROR, respectively. The default value may be a literal or an expression. If the default value itself raises an error, it falls through to the error behavior for ON EMPTY, and raises an error for ON ERROR.
For path contains special characters such as spaces, you can use ['property'] or [\u0026quot;property\u0026quot;] to select the specified property in a parent object. Be sure to put single or double quotes around the property name. When using JSON_VALUE in SQL, the path is a character parameter which is already single quoted, so you have to escape the single quotes around property name, such as JSON_VALUE('{\u0026quot;a b\u0026quot;: \u0026quot;true\u0026quot;}', '\$.[''a b'']').
-- \u0026#34;true\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;\$.a\u0026#39;) -- TRUE JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;\$.a\u0026#39; RETURNING BOOLEAN) -- \u0026#34;false\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;lax \$.b\u0026#39; DEFAULT FALSE ON EMPTY) -- \u0026#34;false\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict \$.b\u0026#39; DEFAULT FALSE ON ERROR) -- 0.998D JSON_VALUE(\u0026#39;{\u0026#34;a.b\u0026#34;: [0.998,0.996]}\u0026#39;,\u0026#39;\$.[\u0026#34;a.b\u0026#34;][0]\u0026#39; RETURNING DOUBLE) -- \u0026#34;right\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;contains blank\u0026#34;: \u0026#34;right\u0026#34;}\u0026#39;, \u0026#39;strict \$.[\u0026#39;\u0026#39;contains blank\u0026#39;\u0026#39;]\u0026#39; NULL ON EMPTY DEFAULT \u0026#39;wrong\u0026#39; ON ERROR) JSON_QUERY(jsonValue, path [ { WITHOUT | WITH CONDITIONAL | WITH UNCONDITIONAL } [ ARRAY ] WRAPPER ] [ { NULL | EMPTY ARRAY | EMPTY OBJECT | ERROR } ON EMPTY ] [ { NULL | EMPTY ARRAY | EMPTY OBJECT | ERROR } ON ERROR ]) STRING.jsonQuery(path [, JsonQueryWrapper [, JsonQueryOnEmptyOrError, JsonQueryOnEmptyOrError ] ]) Extracts JSON values from a JSON string.
The result is always returned as a STRING. The RETURNING clause is currently not supported.
The wrappingBehavior determines whether the extracted value should be wrapped into an array, and whether to do so unconditionally or only if the value itself isn\u0026rsquo;t an array already.
onEmpty and onError determine the behavior in case the path expression is empty, or in case an error was raised, respectively. By default, in both cases null is returned. Other choices are to use an empty array, an empty object, or to raise an error.
-- \u0026#39;{ \u0026#34;b\u0026#34;: 1 }\u0026#39; JSON_QUERY(\u0026#39;{ \u0026#34;a\u0026#34;: { \u0026#34;b\u0026#34;: 1 } }\u0026#39;, \u0026#39;\$.a\u0026#39;) -- \u0026#39;[1, 2]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;\$\u0026#39;) -- NULL JSON_QUERY(CAST(NULL AS STRING), \u0026#39;\$\u0026#39;) -- \u0026#39;[\u0026#34;c1\u0026#34;,\u0026#34;c2\u0026#34;]\u0026#39; JSON_QUERY(\u0026#39;{\u0026#34;a\u0026#34;:[{\u0026#34;c\u0026#34;:\u0026#34;c1\u0026#34;},{\u0026#34;c\u0026#34;:\u0026#34;c2\u0026#34;}]}\u0026#39;, \u0026#39;lax \$.a[*].c\u0026#39;) -- Wrap result into an array -- \u0026#39;[{}]\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;\$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- \u0026#39;[1, 2]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;\$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- \u0026#39;[[1, 2]]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;\$\u0026#39; WITH UNCONDITIONAL ARRAY WRAPPER) -- Scalars must be wrapped to be returned -- NULL JSON_QUERY(1, \u0026#39;\$\u0026#39;) -- \u0026#39;[1]\u0026#39; JSON_QUERY(1, \u0026#39;\$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- Behavior if path expression is empty / there is an error -- \u0026#39;{}\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;lax \$.invalid\u0026#39; EMPTY OBJECT ON EMPTY) -- \u0026#39;[]\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;strict \$.invalid\u0026#39; EMPTY ARRAY ON ERROR) JSON_OBJECT([[KEY] key VALUE value]* [ { NULL | ABSENT } ON NULL ]) jsonObject(JsonOnNull, keyValues...) Builds a JSON object string from a list of key-value pairs.
Note that keys must be non-NULL string literals, while values may be arbitrary expressions.
This function returns a JSON string. The ON NULL behavior defines how to treat NULL values. If omitted, NULL ON NULL is assumed by default.
Values which are created from another JSON construction function call (JSON_OBJECT, JSON_ARRAY) are inserted directly rather than as a string. This allows building nested JSON structures.
-- \u0026#39;{}\u0026#39; JSON_OBJECT() -- \u0026#39;{\u0026#34;K1\u0026#34;:\u0026#34;V1\u0026#34;,\u0026#34;K2\u0026#34;:\u0026#34;V2\u0026#34;}\u0026#39; JSON_OBJECT(\u0026#39;K1\u0026#39; VALUE \u0026#39;V1\u0026#39;, \u0026#39;K2\u0026#39; VALUE \u0026#39;V2\u0026#39;) -- Expressions as values JSON_OBJECT(\u0026#39;orderNo\u0026#39; VALUE orders.orderId) -- ON NULL JSON_OBJECT(KEY \u0026#39;K1\u0026#39; VALUE CAST(NULL AS STRING) NULL ON NULL) -- \u0026#39;{\u0026#34;K1\u0026#34;:null}\u0026#39; JSON_OBJECT(KEY \u0026#39;K1\u0026#39; VALUE CAST(NULL AS STRING) ABSENT ON NULL) -- \u0026#39;{}\u0026#39; -- \u0026#39;{\u0026#34;K1\u0026#34;:{\u0026#34;K2\u0026#34;:\u0026#34;V\u0026#34;}}\u0026#39; JSON_OBJECT( KEY \u0026#39;K1\u0026#39; VALUE JSON_OBJECT( KEY \u0026#39;K2\u0026#39; VALUE \u0026#39;V\u0026#39; ) ) JSON_OBJECTAGG([KEY] key VALUE value [ { NULL | ABSENT } ON NULL ]) jsonObjectAgg(JsonOnNull, keyExpression, valueExpression) Builds a JSON object string by aggregating key-value expressions into a single JSON object.
The key expression must return a non-nullable character string. Value expressions can be arbitrary, including other JSON functions. If a value is NULL, the ON NULL behavior defines what to do. If omitted, NULL ON NULL is assumed by default.
Note that keys must be unique. If a key occurs multiple times, an error will be thrown.
This function is currently not supported in OVER windows.
-- \u0026#39;{\u0026#34;Apple\u0026#34;:2,\u0026#34;Banana\u0026#34;:17,\u0026#34;Orange\u0026#34;:0}\u0026#39; SELECT JSON_OBJECTAGG(KEY product VALUE cnt) FROM orders JSON_ARRAY([value]* [ { NULL | ABSENT } ON NULL ]) jsonArray(JsonOnNull, values...) Builds a JSON array string from a list of values.
This function returns a JSON string. The values can be arbitrary expressions. The ON NULL behavior defines how to treat NULL values. If omitted, ABSENT ON NULL is assumed by default.
Elements which are created from another JSON construction function call (JSON_OBJECT, JSON_ARRAY) are inserted directly rather than as a string. This allows building nested JSON structures.
-- \u0026#39;[]\u0026#39; JSON_ARRAY() -- \u0026#39;[1,\u0026#34;2\u0026#34;]\u0026#39; JSON_ARRAY(1, \u0026#39;2\u0026#39;) -- Expressions as values JSON_ARRAY(orders.orderId) -- ON NULL JSON_ARRAY(CAST(NULL AS STRING) NULL ON NULL) -- \u0026#39;[null]\u0026#39; JSON_ARRAY(CAST(NULL AS STRING) ABSENT ON NULL) -- \u0026#39;[]\u0026#39; -- \u0026#39;[[1]]\u0026#39; JSON_ARRAY(JSON_ARRAY(1)) JSON_ARRAYAGG(items [ { NULL | ABSENT } ON NULL ]) jsonArrayAgg(JsonOnNull, itemExpression) Builds a JSON object string by aggregating items into an array.
Item expressions can be arbitrary, including other JSON functions. If a value is NULL, the ON NULL behavior defines what to do. If omitted, ABSENT ON NULL is assumed by default.
This function is currently not supported in OVER windows, unbounded session windows, or hop windows.
-- \u0026#39;[\u0026#34;Apple\u0026#34;,\u0026#34;Banana\u0026#34;,\u0026#34;Orange\u0026#34;]\u0026#39; SELECT JSON_ARRAYAGG(product) FROM orders Value Construction Functions # SQL Function Table Function Description -- implicit constructor with parenthesis (value1 [, value2]*) row(ANY1, ANY2, ...) Returns a row created from a list of values (value1, value2,\u0026hellip;).
The implicit row constructor supports arbitrary expressions as fields but requires at least two fields. The explicit row constructor can deal with an arbitrary number of fields but does not support all kinds of field expressions well currently.
ARRAY ‘[’ value1 [, value2 ]* ‘]’ array(ANY1, ANY2, ...) Returns an array created from a list of values (value1, value2, \u0026hellip;). MAP ‘[’ value1, value2 [, value3, value4 ]* ‘]’ map(ANY1, ANY2, ANY3, ANY4, ...) Returns a map created from a list of key-value pairs ((value1, value2), (value3, value4), \u0026hellip;). N/A NUMERIC.rows Creates a NUMERIC interval of rows (commonly used in window creation). Value Access Functions # SQL Function Table Function Description tableName.compositeType.field COMPOSITE.get(STRING) COMPOSITE.get(INT) Returns the value of a field from a Flink composite type (e.g., Tuple, POJO) by name. tableName.compositeType.* ANY.flatten() Returns a flat representation of a Flink composite type (e.g., Tuple, POJO) that converts each of its direct subtype into a separate field. In most cases the fields of the flat representation are named similarly to the original fields but with a dollar separator (e.g., mypojo\$mytuple\$f0). Grouping Functions # SQL Function Table Function Description GROUP_ID() N/A Returns an integer that uniquely identifies the combination of grouping keys. GROUPING(expression1 [, expression2]* ) GROUPING_ID(expression1 [, expression2]* ) N/A Returns a bit vector of the given grouping expressions. Hash Functions # SQL Function Table Function Description MD5(string) STRING.md5() Returns the MD5 hash of string as a string of 32 hexadecimal digits; returns NULL if string is NULL. SHA1(string) STRING.sha1() Returns the SHA-1 hash of string as a string of 40 hexadecimal digits; returns NULL if string is NULL. SHA224(string) STRING.sha224() Returns the SHA-224 hash of string as a string of 56 hexadecimal digits; returns NULL if string is NULL. SHA256(string) STRING.sha256() Returns the SHA-256 hash of string as a string of 64 hexadecimal digits; returns NULL if string is NULL. SHA384(string) STRING.sha384() Returns the SHA-384 hash of string as a string of 96 hexadecimal digits; returns NULL if string is NULL. SHA512(string) STRING.sha512() Returns the SHA-512 hash of string as a string of 128 hexadecimal digits; returns NULL if string is NULL. SHA2(string, hashLength) STRING.sha2(INT) Returns the hash using the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, or SHA-512). The first argument string is the string to be hashed and the second argument hashLength is the bit length of the result (224, 256, 384, or 512). Returns NULL if string or hashLength is NULL. Auxiliary Functions # SQL Function Table Function Description Aggregate Functions # The aggregate functions take an expression across all the rows as the input and return a single aggregated value as the result.
SQL Function Table Function Description COUNT([ ALL ] expression | DISTINCT expression1 [, expression2]*) N/A By default or with ALL, returns the number of input rows for which expression is not NULL. Use DISTINCT for one unique instance of each value. COUNT(*) COUNT(1) FIELD.count Returns the number of input rows. AVG([ ALL | DISTINCT ] expression) FIELD.avg By default or with keyword ALL, returns the average (arithmetic mean) of expression across all input rows. Use DISTINCT for one unique instance of each value. SUM([ ALL | DISTINCT ] expression) FIELD.sum By default or with keyword ALL, returns the sum of expression across all input rows. Use DISTINCT for one unique instance of each value. N/A FIELD.sum0 Returns the sum of numeric field FIELD across all input rows. If all values are NULL, returns 0. MAX([ ALL | DISTINCT ] expression) FIELD.max By default or with keyword ALL, returns the maximum value of expression across all input rows. Use DISTINCT for one unique instance of each value. MIN([ ALL | DISTINCT ] expression ) FIELD.min By default or with keyword ALL, returns the minimum value of expression across all input rows. Use DISTINCT for one unique instance of each value. STDDEV_POP([ ALL | DISTINCT ] expression) FIELD.stddevPop By default or with keyword ALL, returns the population standard deviation of expression across all input rows. Use DISTINCT for one unique instance of each value. STDDEV_SAMP([ ALL | DISTINCT ] expression) FIELD.stddevSamp By default or with keyword ALL, returns the sample standard deviation of expression across all input rows. Use DISTINCT for one unique instance of each value. VAR_POP([ ALL | DISTINCT ] expression) FIELD.varPop By default or with keyword ALL, returns the population variance (square of the population standard deviation) of expression across all input rows. Use DISTINCT for one unique instance of each value. VAR_SAMP([ ALL | DISTINCT ] expression) FIELD.varSamp By default or with keyword ALL, returns the sample variance (square of the sample standard deviation) of expression across all input rows. Use DISTINCT for one unique instance of each value. COLLECT([ ALL | DISTINCT ] expression) FIELD.collect By default or with keyword ALL, returns a multiset of expression across all input rows. NULL values will be ignored. Use DISTINCT for one unique instance of each value. VARIANCE([ ALL | DISTINCT ] expression) N/A Synonyms for VAR_SAMP(). RANK() N/A Returns the rank of a value in a group of values. The result is one plus the number of rows preceding or equal to the current row in the ordering of the partition. The values will produce gaps in the sequence. DENSE_RANK() N/A Returns the rank of a value in a group of values. The result is one plus the previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps in the ranking sequence. ROW_NUMBER() N/A Assigns a unique, sequential number to each row, starting with one, according to the ordering of rows within the window partition. ROW_NUMBER and RANK are similar. ROW_NUMBER numbers all rows sequentially (for example 1, 2, 3, 4, 5). RANK provides the same numeric value for ties (for example 1, 2, 2, 4, 5). LEAD(expression [, offset] [, default]) N/A Returns the value of expression at the offsetth row after the current row in the window. The default value of offset is 1 and the default value of default is NULL. LAG(expression [, offset] [, default]) N/A Returns the value of expression at the offsetth row before the current row in the window. The default value of offset is 1 and the default value of default is NULL. FIRST_VALUE(expression) FIELD.firstValue Returns the first value in an ordered set of values. LAST_VALUE(expression) FIELD.lastValue Returns the last value in an ordered set of values. LISTAGG(expression [, separator]) N/A Concatenates the values of string expressions and places separator values between them. The separator is not added at the end of string. The default value of separator is \u0026lsquo;,\u0026rsquo;. CUME_DIST() N/A Return the cumulative distribution of a value in a group of values. The result is the number of rows preceding or equal to the current row in the ordering of the partition divided by the number of rows in the window partition. PERCENT_RANK() N/A Return the percentage ranking of a value in a group of values. The result is the rank value minus one, divided by the number of rows in the parition minus one. If the partition only contains one row, the function will return 0. NTILE(n) N/A Divides the rows for each window partition into n buckets ranging from 1 to at most n. If the number of rows in the window partition doesn\u0026rsquo;t divide evenly into the number of buckets, then the remainder values are distributed one per bucket, starting with the first bucket. For example, with 6 rows and 4 buckets, the bucket values would be as follows: 1 1 2 2 3 4 Time Interval and Point Unit Specifiers # The following table lists specifiers for time interval and time point units.
For Table API, please use _ for spaces (e.g., DAY_TO_HOUR).
Time Interval Unit Time Point Unit MILLENNIUM CENTURY DECADE YEAR YEAR YEAR TO MONTH QUARTER QUARTER MONTH MONTH WEEK WEEK DAY DAY DAY TO HOUR DAY TO MINUTE DAY TO SECOND HOUR HOUR HOUR TO MINUTE HOUR TO SECOND MINUTE MINUTE MINUTE TO SECOND SECOND SECOND MILLISECOND MILLISECOND MICROSECOND MICROSECOND NANOSECOND EPOCH DOY (SQL-only) DOW (SQL-only) EPOCH (SQL-only) ISODOW (SQL-only) ISOYEAR (SQL-only) SQL_TSI_YEAR (SQL-only) SQL_TSI_QUARTER (SQL-only) SQL_TSI_MONTH (SQL-only) SQL_TSI_WEEK (SQL-only) SQL_TSI_DAY (SQL-only) SQL_TSI_HOUR (SQL-only) SQL_TSI_MINUTE (SQL-only) SQL_TSI_SECOND (SQL-only) Back to top
Column Functions # The column functions are used to select or deselect table columns.
Column functions are only used in Table API. SYNTAX DESC withColumns(\u0026hellip;) select the specified columns withoutColumns(\u0026hellip;) deselect the columns specified The detailed syntax is as follows:
columnFunction: withColumns(columnExprs) withoutColumns(columnExprs) columnExprs: columnExpr [, columnExpr]* columnExpr: columnRef | columnIndex to columnIndex | columnName to columnName columnRef: columnName(The field name that exists in the table) | columnIndex(a positive integer starting from 1) The usage of the column function is illustrated in the following table. (Suppose we have a table with 5 columns: (a: Int, b: Long, c: String, d:String, e: String)):
API Usage Description withColumns(\$(*)) select(withColumns(\$(\u0026quot;*\u0026quot;))) = select(\$(\u0026ldquo;a\u0026rdquo;), \$(\u0026ldquo;b\u0026rdquo;), \$(\u0026ldquo;c\u0026rdquo;), \$(\u0026ldquo;d\u0026rdquo;), \$(\u0026ldquo;e\u0026rdquo;)) all the columns withColumns(m to n) select(withColumns(range(2, 4))) = select(\$(\u0026ldquo;b\u0026rdquo;), \$(\u0026ldquo;c\u0026rdquo;), \$(\u0026ldquo;d\u0026rdquo;)) columns from m to n withColumns(m, n, k) select(withColumns(lit(1), lit(3), \$(\u0026ldquo;e\u0026rdquo;))) = select(\$(\u0026ldquo;a\u0026rdquo;), \$(\u0026ldquo;c\u0026rdquo;), \$(\u0026ldquo;e\u0026rdquo;)) columns m, n, k withColumns(m, n to k) select(withColumns(lit(1), range(3, 5))) = select(\$(\u0026ldquo;a\u0026rdquo;), \$(\u0026ldquo;c\u0026rdquo;), \$(\u0026ldquo;d\u0026rdquo;), \$(\u0026ldquo;e\u0026rdquo;)) mixing of the above two representation withoutColumns(m to n) select(withoutColumns(range(2, 4))) = select(\$(\u0026ldquo;a\u0026rdquo;), \$(\u0026ldquo;e\u0026rdquo;)) deselect columns from m to n withoutColumns(m, n, k) select(withoutColumns(lit(1), lit(3), lit(5))) = select(\$(\u0026ldquo;b\u0026rdquo;), \$(\u0026ldquo;d\u0026rdquo;)) deselect columns m, n, k withoutColumns(m, n to k) select(withoutColumns(lit(1), range(3, 5))) = select(\$(\u0026ldquo;b\u0026rdquo;)) mixing of the above two representation The column functions can be used in all places where column fields are expected, such as select, groupBy, orderBy, UDFs etc. e.g.:
Java table .groupBy(withColumns(range(1, 3))) .select(withColumns(range(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)), myUDAgg(myUDF(withColumns(range(5, 20))))); Scala table .groupBy(withColumns(range(1, 3))) .select(withColumns(\u0026#39;a to \u0026#39;b), myUDAgg(myUDF(withColumns(5 to 20)))) Python table \\ .group_by(with_columns(range_(1, 3))) \\ .select(with_columns(range_(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;)), myUDAgg(myUDF(with_columns(range_(5, 20))))) Back to top
`}),e.add({id:285,href:"/flink/flink-docs-master/docs/dev/python/table/udfs/",title:"User Defined Functions",section:"Table API",content:""}),e.add({id:286,href:"/flink/flink-docs-master/docs/dev/table/functions/",title:"Functions",section:"Table API \u0026 SQL",content:""}),e.add({id:287,href:"/flink/flink-docs-master/docs/dev/datastream/side_output/",title:"Side Outputs",section:"DataStream API",content:` Side Outputs # In addition to the main stream that results from DataStream operations, you can also produce any number of additional side output result streams. The type of data in the result streams does not have to match the type of data in the main stream and the types of the different side outputs can also differ. This operation can be useful when you want to split a stream of data where you would normally have to replicate the stream and then filter out from each stream the data that you don\u0026rsquo;t want to have.
When using side outputs, you first need to define an OutputTag that will be used to identify a side output stream:
Java // this needs to be an anonymous inner class, so that we can analyze the type OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;) {}; Scala val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) Python output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) Notice how the OutputTag is typed according to the type of elements that the side output stream contains.
Emitting data to a side output is possible from the following functions:
ProcessFunction KeyedProcessFunction CoProcessFunction KeyedCoProcessFunction ProcessWindowFunction ProcessAllWindowFunction You can use the Context parameter, which is exposed to users in the above functions, to emit data to a side output identified by an OutputTag. Here is an example of emitting side output data from a ProcessFunction:
Java DataStream\u0026lt;Integer\u0026gt; input = ...; final OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Integer\u0026gt; mainDataStream = input .process(new ProcessFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public void processElement( Integer value, Context ctx, Collector\u0026lt;Integer\u0026gt; out) throws Exception { // emit data to regular output out.collect(value); // emit data to side output ctx.output(outputTag, \u0026#34;sideout-\u0026#34; + String.valueOf(value)); } }); Scala val input: DataStream[Int] = ... val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = input .process(new ProcessFunction[Int, Int] { override def processElement( value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = { // emit data to regular output out.collect(value) // emit data to side output ctx.output(outputTag, \u0026#34;sideout-\u0026#34; + String.valueOf(value)) } }) Python input = ... # type: DataStream output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) class MyProcessFunction(ProcessFunction): def process_element(self, value: int, ctx: ProcessFunction.Context): # emit data to regular output yield value # emit data to side output yield output_tag, \u0026#34;sideout-\u0026#34; + str(value) main_data_stream = input \\ .process(MyProcessFunction(), Types.INT()) For retrieving the side output stream you use getSideOutput(OutputTag) on the result of the DataStream operation. This will give you a DataStream that is typed to the result of the side output stream:
Java final OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Integer\u0026gt; mainDataStream = ...; DataStream\u0026lt;String\u0026gt; sideOutputStream = mainDataStream.getSideOutput(outputTag); Scala val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = ... val sideOutputStream: DataStream[String] = mainDataStream.getSideOutput(outputTag) Python output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) main_data_stream = ... # type: DataStream side_output_stream = main_data_stream.get_side_output(output_tag) # type: DataStream Back to top
`}),e.add({id:288,href:"/flink/flink-docs-master/docs/dev/python/python_execution_mode/",title:"Execution Mode",section:"Python API",content:` Execution Mode # The Python API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job. The Python runtime execution mode defines how the Python user-defined functions will be executed.
Prior to release-1.15, there is the only execution mode called PROCESS execution mode. The PROCESS mode means that the Python user-defined functions will be executed in separate Python processes.
In release-1.15, it has introduced a new execution mode called THREAD execution mode. The THREAD mode means that the Python user-defined functions will be executed in JVM.
NOTE: Multiple Python user-defined functions running in the same JVM are still affected by GIL.
When can/should I use THREAD execution mode? # The purpose of the introduction of THREAD mode is to overcome the overhead of serialization/deserialization and network communication introduced of inter-process communication in the PROCESS mode. So if performance is not your concern, or the computing logic of your Python user-defined functions is the performance bottleneck of the job, PROCESS mode will be the best choice as PROCESS mode provides the best isolation compared to THREAD mode.
Configuring Python execution mode # The execution mode can be configured via the python.execution-mode setting. There are two possible values:
PROCESS: The Python user-defined functions will be executed in separate Python process. (default) THREAD: The Python user-defined functions will be executed in JVM. You could specify the execution mode in Python Table API or Python DataStream API jobs as following:
## Python Table API # Specify \`PROCESS\` mode table_env.get_config().set(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;process\u0026#34;) # Specify \`THREAD\` mode table_env.get_config().set(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;thread\u0026#34;) ## Python DataStream API config = Configuration() # Specify \`PROCESS\` mode config.set_string(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;process\u0026#34;) # Specify \`THREAD\` mode config.set_string(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;thread\u0026#34;) # Create the corresponding StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment(config) Supported Cases # Python Table API # The following table shows where the THREAD execution mode is supported in Python Table API.
UDFs PROCESS THREAD Python UDF Yes Yes Python UDTF Yes Yes Python UDAF Yes No Pandas UDF \u0026amp; Pandas UDAF Yes No Python DataStream API # The following table shows where the PROCESS execution mode and the THREAD execution mode are supported in Python DataStream API.
Operators PROCESS THREAD Map Yes Yes FlatMap Yes Yes Filter Yes Yes Reduce Yes Yes Union Yes Yes Connect Yes Yes CoMap Yes Yes CoFlatMap Yes Yes Process Function Yes Yes Window Apply Yes Yes Window Aggregate Yes Yes Window Reduce Yes Yes Window Process Yes Yes Side Output Yes Yes State Yes Yes Iterate No No Window CoGroup No No Window Join No No Interval Join No No Async I/O No No Currently, it still doesn\u0026rsquo;t support to execute Python UDFs in THREAD execution mode in all places. It will fall back to PROCESS execution mode in these cases. So it may happen that you configure a job to execute in THREAD execution mode, however, it\u0026rsquo;s actually executed in PROCESS execution mode. THREAD execution mode is only supported in Python 3.7+. Execution Behavior # This section provides an overview of the execution behavior of THREAD execution mode and contrasts they with PROCESS execution mode. For more details, please refer to the FLIP that introduced this feature: FLIP-206.
PROCESS Execution Mode # In PROCESS execution mode, the Python user-defined functions will be executed in separate Python Worker process. The Java operator process communicates with the Python worker process using various Grpc services.
THREAD Execution Mode # In THREAD execution mode, the Python user-defined functions will be executed in the same process as Java operators. PyFlink takes use of third part library PEMJA to embed Python in Java Application.
`}),e.add({id:289,href:"/flink/flink-docs-master/docs/dev/python/table/conversion_of_pandas/",title:"Conversions between PyFlink Table and Pandas DataFrame",section:"Table API",content:` Conversions between PyFlink Table and Pandas DataFrame # PyFlink Table API supports conversion between PyFlink Table and Pandas DataFrame.
Convert Pandas DataFrame to PyFlink Table # Pandas DataFrames can be converted into a PyFlink Table. Internally, PyFlink will serialize the Pandas DataFrame using Arrow columnar format on the client. The serialized data will be processed and deserialized in Arrow source during execution. The Arrow source can also be used in streaming jobs, and is integrated with checkpointing to provide exactly-once guarantees.
The following example shows how to create a PyFlink Table from a Pandas DataFrame:
from pyflink.table import DataTypes import pandas as pd import numpy as np # Create a Pandas DataFrame pdf = pd.DataFrame(np.random.rand(1000, 2)) # Create a PyFlink Table from a Pandas DataFrame table = t_env.from_pandas(pdf) # Create a PyFlink Table from a Pandas DataFrame with the specified column names table = t_env.from_pandas(pdf, [\u0026#39;f0\u0026#39;, \u0026#39;f1\u0026#39;]) # Create a PyFlink Table from a Pandas DataFrame with the specified column types table = t_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()]) # Create a PyFlink Table from a Pandas DataFrame with the specified row type table = t_env.from_pandas(pdf, DataTypes.ROW([DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.DOUBLE()), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.DOUBLE())])) Convert PyFlink Table to Pandas DataFrame # PyFlink Tables can additionally be converted into a Pandas DataFrame. The resulting rows will be serialized as multiple Arrow batches of Arrow columnar format on the client. The maximum Arrow batch size is configured via the option python.fn-execution.arrow.batch.size. The serialized data will then be converted to a Pandas DataFrame. Because the contents of the table will be collected on the client, please ensure that the results of the table can fit in memory before calling this method. You can limit the number of rows collected to client side via Table.limit The following example shows how to convert a PyFlink Table to a Pandas DataFrame:
from pyflink.table.expressions import col import pandas as pd import numpy as np # Create a PyFlink Table pdf = pd.DataFrame(np.random.rand(1000, 2)) table = t_env.from_pandas(pdf, [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).filter(col(\u0026#39;a\u0026#39;) \u0026gt; 0.5) # Convert the PyFlink Table to a Pandas DataFrame pdf = table.limit(100).to_pandas() `}),e.add({id:290,href:"/flink/flink-docs-master/docs/dev/python/table/conversion_of_data_stream/",title:"Conversions between Table and DataStream",section:"Table API",content:" "}),e.add({id:291,href:"/flink/flink-docs-master/docs/dev/python/table/sql/",title:"SQL",section:"Table API",content:" "}),e.add({id:292,href:"/flink/flink-docs-master/docs/dev/datastream/application_parameters/",title:"Handling Application Parameters",section:"DataStream API",content:` Handling Application Parameters # Handling Application Parameters # Almost all Flink applications, both batch and streaming, rely on external configuration parameters. They are used to specify input and output sources (like paths or addresses), system parameters (parallelism, runtime configuration), and application specific parameters (typically used within user functions).
Flink provides a simple utility called ParameterTool to provide some basic tooling for solving these problems. Please note that you don\u0026rsquo;t have to use the ParameterTool described here. Other frameworks such as Commons CLI and argparse4j also work well with Flink.
Getting your configuration values into the ParameterTool # The ParameterTool provides a set of predefined static methods for reading the configuration. The tool is internally expecting a Map\u0026lt;String, String\u0026gt;, so it\u0026rsquo;s very easy to integrate it with your own configuration style.
From .properties files # The following method will read a Properties file and provide the key/value pairs:
String propertiesFilePath = \u0026#34;/home/sam/flink/myjob.properties\u0026#34;; ParameterTool parameters = ParameterTool.fromPropertiesFile(propertiesFilePath); File propertiesFile = new File(propertiesFilePath); ParameterTool parameters = ParameterTool.fromPropertiesFile(propertiesFile); InputStream propertiesFileInputStream = new FileInputStream(file); ParameterTool parameters = ParameterTool.fromPropertiesFile(propertiesFileInputStream); From the command line arguments # This allows getting arguments like --input hdfs:///mydata --elements 42 from the command line.
public static void main(String[] args) { ParameterTool parameters = ParameterTool.fromArgs(args); // .. regular code .. From system properties # When starting a JVM, you can pass system properties to it: -Dinput=hdfs:///mydata. You can also initialize the ParameterTool from these system properties:
ParameterTool parameters = ParameterTool.fromSystemProperties(); Using the parameters in your Flink program # Now that we\u0026rsquo;ve got the parameters from somewhere (see above) we can use them in various ways.
Directly from the ParameterTool
The ParameterTool itself has methods for accessing the values.
ParameterTool parameters = // ... parameters.getRequired(\u0026#34;input\u0026#34;); parameters.get(\u0026#34;output\u0026#34;, \u0026#34;myDefaultValue\u0026#34;); parameters.getLong(\u0026#34;expectedCount\u0026#34;, -1L); parameters.getNumberOfParameters(); // .. there are more methods available. You can use the return values of these methods directly in the main() method of the client submitting the application. For example, you could set the parallelism of a operator like this:
ParameterTool parameters = ParameterTool.fromArgs(args); int parallelism = parameters.get(\u0026#34;mapParallelism\u0026#34;, 2); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer()).setParallelism(parallelism); Since the ParameterTool is serializable, you can pass it to the functions itself:
ParameterTool parameters = ParameterTool.fromArgs(args); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer(parameters)); and then use it inside the function for getting values from the command line.
Register the parameters globally # Parameters registered as global job parameters in the ExecutionConfig can be accessed as configuration values from the JobManager web interface and in all functions defined by the user.
Register the parameters globally:
ParameterTool parameters = ParameterTool.fromArgs(args); // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(parameters); Access them in any rich user function:
public static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { ParameterTool parameters = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); parameters.getRequired(\u0026#34;input\u0026#34;); // .. do more .. Back to top
`}),e.add({id:293,href:"/flink/flink-docs-master/docs/ops/state/task_failure_recovery/",title:"Task Failure Recovery",section:"State \u0026 Fault Tolerance",content:` Task Failure Recovery # When a task failure happens, Flink needs to restart the failed task and other affected tasks to recover the job to a normal state.
Restart strategies and failover strategies are used to control the task restarting. Restart strategies decide whether and when the failed/affected tasks can be restarted. Failover strategies decide which tasks should be restarted to recover the job.
Restart Strategies # The cluster can be started with a default restart strategy which is always used when no job specific restart strategy has been defined. In case that the job is submitted with a restart strategy, this strategy overrides the cluster\u0026rsquo;s default setting.
The default restart strategy is set via Flink\u0026rsquo;s configuration file flink-conf.yaml. The configuration parameter restart-strategy defines which strategy is taken. If checkpointing is not enabled, the \u0026ldquo;no restart\u0026rdquo; strategy is used. If checkpointing is activated and the restart strategy has not been configured, the fixed-delay strategy is used with Integer.MAX_VALUE restart attempts. See the following list of available restart strategies to learn what values are supported.
Each restart strategy comes with its own set of parameters which control its behaviour. These values are also set in the configuration file. The description of each restart strategy contains more information about the respective configuration values.
Key Default Type Description restart-strategy (none) String Defines the restart strategy to use in case of job failures.
Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy. More details can be found here.failurerate, failure-rate: Failure rate restart strategy. More details can be found here.exponentialdelay, exponential-delay: Exponential delay restart strategy. More details can be found here.If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay. Apart from defining a default restart strategy, it is possible to define for each Flink job a specific restart strategy. This restart strategy is set programmatically by calling the setRestartStrategy method on the StreamExecutionEnvironment.
The following example shows how we can set a fixed delay restart strategy for our job. In case of a failure the system tries to restart the job 3 times and waits 10 seconds in-between successive restart attempts.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.fixed_delay_restart( 3, # number of restart attempts 10000 # delay(millisecond) )) The following sections describe restart strategy specific configuration options.
Fixed Delay Restart Strategy # The fixed delay restart strategy attempts a given number of times to restart the job. If the maximum number of attempts is exceeded, the job eventually fails. In-between two consecutive restart attempts, the restart strategy waits a fixed amount of time.
This strategy is enabled as default by setting the following configuration parameter in flink-conf.yaml.
restart-strategy: fixed-delay Key Default Type Description restart-strategy.fixed-delay.attempts 1 Integer The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay. restart-strategy.fixed-delay.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s" For example:
restart-strategy.fixed-delay.attempts: 3 restart-strategy.fixed-delay.delay: 10 s The fixed delay restart strategy can also be set programmatically:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // number of restart attempts Time.of(10, TimeUnit.SECONDS) // delay )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.fixed_delay_restart( 3, # number of restart attempts 10000 # delay(millisecond) )) Exponential Delay Restart Strategy # The exponential delay restart strategy attempts to restart the job infinitely, with increasing delay up to the maximum delay. The job never fails. In-between two consecutive restart attempts, the restart strategy keeps exponentially increasing until the maximum number is reached. Then, it keeps the delay at the maximum number.
When the job executes correctly, the exponential delay value resets after some time; this threshold is configurable.
restart-strategy: exponential-delay Key Default Type Description restart-strategy.exponential-delay.backoff-multiplier 2.0 Double Backoff value is multiplied by this value after every failure,until max backoff is reached if restart-strategy has been set to exponential-delay. restart-strategy.exponential-delay.initial-backoff 1 s Duration Starting duration between restarts if restart-strategy has been set to exponential-delay. It can be specified using notation: "1 min", "20 s" restart-strategy.exponential-delay.jitter-factor 0.1 Double Jitter specified as a portion of the backoff if restart-strategy has been set to exponential-delay. It represents how large random value will be added or subtracted to the backoff. Useful when you want to avoid restarting multiple jobs at the same time. restart-strategy.exponential-delay.max-backoff 5 min Duration The highest possible duration between restarts if restart-strategy has been set to exponential-delay. It can be specified using notation: "1 min", "20 s" restart-strategy.exponential-delay.reset-backoff-threshold 1 h Duration Threshold when the backoff is reset to its initial value if restart-strategy has been set to exponential-delay. It specifies how long the job must be running without failure to reset the exponentially increasing backoff to its initial value. It can be specified using notation: "1 min", "20 s" For example:
restart-strategy.exponential-delay.initial-backoff: 10 s restart-strategy.exponential-delay.max-backoff: 2 min restart-strategy.exponential-delay.backoff-multiplier: 2.0 restart-strategy.exponential-delay.reset-backoff-threshold: 10 min restart-strategy.exponential-delay.jitter-factor: 0.1 The exponential delay restart strategy can also be set programmatically:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.exponentialDelayRestart( Time.milliseconds(1), Time.milliseconds(1000), 1.1, // exponential multiplier Time.milliseconds(2000), // threshold duration to reset delay to its initial value 0.1 // jitter )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.exponentialDelayRestart( Time.of(1, TimeUnit.MILLISECONDS), // initial delay between restarts Time.of(1000, TimeUnit.MILLISECONDS), // maximum delay between restarts 1.1, // exponential multiplier Time.of(2, TimeUnit.SECONDS), // threshold duration to reset delay to its initial value 0.1 // jitter )) Python Still not supported in Python API. Failure Rate Restart Strategy # The failure rate restart strategy restarts job after failure, but when failure rate (failures per time interval) is exceeded, the job eventually fails. In-between two consecutive restart attempts, the restart strategy waits a fixed amount of time.
This strategy is enabled as default by setting the following configuration parameter in flink-conf.yaml.
restart-strategy: failure-rate Key Default Type Description restart-strategy.failure-rate.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.failure-rate-interval 1 min Duration Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.max-failures-per-interval 1 Integer Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate. restart-strategy.failure-rate.max-failures-per-interval: 3 restart-strategy.failure-rate.failure-rate-interval: 5 min restart-strategy.failure-rate.delay: 10 s The failure rate restart strategy can also be set programmatically:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per interval Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // max failures per unit Time.of(5, TimeUnit.MINUTES), //time interval for measuring failure rate Time.of(10, TimeUnit.SECONDS) // delay )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.failure_rate_restart( 3, # max failures per interval 300000, # interval for measuring failure rate (millisecond) 10000 # dela(millisecond) )) No Restart Strategy # The job fails directly and no restart is attempted.
restart-strategy: none The no restart strategy can also be set programmatically:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.noRestart()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.noRestart()) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.no_restart()) Fallback Restart Strategy # The cluster defined restart strategy is used. This is helpful for streaming programs which enable checkpointing. By default, a fixed delay restart strategy is chosen if there is no other restart strategy defined.
Failover Strategies # Flink supports different failover strategies which can be configured via the configuration parameter jobmanager.execution.failover-strategy in Flink\u0026rsquo;s configuration file flink-conf.yaml.
Failover Strategy Value for jobmanager.execution.failover-strategy Restart all full Restart pipelined region region Restart All Failover Strategy # This strategy restarts all tasks in the job to recover from a task failure.
Restart Pipelined Region Failover Strategy # This strategy groups tasks into disjoint regions. When a task failure is detected, this strategy computes the smallest set of regions that must be restarted to recover from the failure. For some jobs this can result in fewer tasks that will be restarted compared to the Restart All Failover Strategy.
A region is a set of tasks that communicate via pipelined data exchanges. That is, batch data exchanges denote the boundaries of a region.
All data exchanges in a DataStream job or Streaming Table/SQL job are pipelined. All data exchanges in a Batch Table/SQL job are batched by default. The data exchange types in a DataSet job are determined by the ExecutionMode which can be set through ExecutionConfig. The regions to restart are decided as below:
The region containing the failed task will be restarted. If a result partition is not available while it is required by a region that will be restarted, the region producing the result partition will be restarted as well. If a region is to be restarted, all of its consumer regions will also be restarted. This is to guarantee data consistency because nondeterministic processing or partitioning can result in different partitions. Back to top
`}),e.add({id:294,href:"/flink/flink-docs-master/docs/dev/table/functions/udfs/",title:"User-defined Functions",section:"Functions",content:" User-defined Functions # User-defined functions (UDFs) are extension points to call frequently used logic or custom logic that cannot be expressed otherwise in queries.\nUser-defined functions can be implemented in a JVM language (such as Java or Scala) or Python. An implementer can use arbitrary third party libraries within a UDF. This page will focus on JVM-based languages, please refer to the PyFlink documentation for details on writing general and vectorized UDFs in Python.\nOverview # Currently, Flink distinguishes between the following kinds of functions:\nScalar functions map scalar values to a new scalar value. Table functions map scalar values to new rows. Aggregate functions map scalar values of multiple rows to a new scalar value. Table aggregate functions map scalar values of multiple rows to new rows. Async table functions are special functions for table sources that perform a lookup. The following example shows how to create a simple scalar function and how to call the function in both Table API and SQL.\nFor SQL queries, a function must always be registered under a name. For Table API, a function can be registered or directly used inline.\nJava import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; // define function logic public static class SubstringFunction extends ScalarFunction { public String eval(String s, Integer begin, Integer end) { return s.substring(begin, end); } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(SubstringFunction.class, $(\u0026#34;myField\u0026#34;), 5, 12)); // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, SubstringFunction.class); // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;SubstringFunction\u0026#34;, $(\u0026#34;myField\u0026#34;), 5, 12)); // call registered function in SQL env.sqlQuery(\u0026#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // define function logic class SubstringFunction extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(begin, end) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[SubstringFunction], $\u0026#34;myField\u0026#34;, 5, 12)) // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, classOf[SubstringFunction]) // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;SubstringFunction\u0026#34;, $\u0026#34;myField\u0026#34;, 5, 12)) // call registered function in SQL env.sqlQuery(\u0026#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable\u0026#34;) For interactive sessions, it is also possible to parameterize functions before using or registering them. In this case, function instances instead of function classes can be used as temporary functions.\nIt requires that the parameters are serializable for shipping function instances to the cluster.\nJava import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; // define parameterizable function logic public static class SubstringFunction extends ScalarFunction { private boolean endInclusive; public SubstringFunction(boolean endInclusive) { this.endInclusive = endInclusive; } public String eval(String s, Integer begin, Integer end) { return s.substring(begin, endInclusive ? end + 1 : end); } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(new SubstringFunction(true), $(\u0026#34;myField\u0026#34;), 5, 12)); // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, new SubstringFunction(true)); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // define parameterizable function logic class SubstringFunction(val endInclusive) extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(endInclusive ? end + 1 : end) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(new SubstringFunction(true), $\u0026#34;myField\u0026#34;, 5, 12)) // register function env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, new SubstringFunction(true)) You can use star * expression as one argument of the function call to act as a wildcard in Table API, all columns in the table will be passed to the function at the corresponding position.\nJava import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; public static class MyConcatFunction extends ScalarFunction { public String eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object... fields) { return Arrays.stream(fields) .map(Object::toString) .collect(Collectors.joining(\u0026#34;,\u0026#34;)); } } TableEnvironment env = TableEnvironment.create(...); // call function with $(\u0026#34;*\u0026#34;), if MyTable has 3 fields (a, b, c), // all of them will be passed to MyConcatFunction. env.from(\u0026#34;MyTable\u0026#34;).select(call(MyConcatFunction.class, $(\u0026#34;*\u0026#34;))); // it\u0026#39;s equal to call function with explicitly selecting all columns. env.from(\u0026#34;MyTable\u0026#34;).select(call(MyConcatFunction.class, $(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;))); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction import scala.annotation.varargs class MyConcatFunction extends ScalarFunction { @varargs def eval(@DataTypeHint(inputGroup = InputGroup.ANY) row: AnyRef*): String = { row.map(f =\u0026gt; f.toString).mkString(\u0026#34;,\u0026#34;) } } val env = TableEnvironment.create(...) // call function with $\u0026#34;*\u0026#34;, if MyTable has 3 fields (a, b, c), // all of them will be passed to MyConcatFunction. env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[MyConcatFunction], $\u0026#34;*\u0026#34;)); // it\u0026#39;s equal to call function with explicitly selecting all columns. env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[MyConcatFunction], $\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;)); Back to top\nImplementation Guide # Independent of the kind of function, all user-defined functions follow some basic implementation principles.\nFunction Class # An implementation class must extend from one of the available base classes (e.g. org.apache.flink.table.functions.ScalarFunction).\nThe class must be declared public, not abstract, and should be globally accessible. Thus, non-static inner or anonymous classes are not allowed.\nFor storing a user-defined function in a persistent catalog, the class must have a default constructor and must be instantiable during runtime. Anonymous functions in Table API can only be persisted if the function is not stateful (i.e. containing only transient and static fields).\nEvaluation Methods # The base class provides a set of methods that can be overridden such as open(), close(), or isDeterministic().\nHowever, in addition to those declared methods, the main runtime logic that is applied to every incoming record must be implemented through specialized evaluation methods.\nDepending on the function kind, evaluation methods such as eval(), accumulate(), or retract() are called by code-generated operators during runtime.\nThe methods must be declared public and take a well-defined set of arguments.\nRegular JVM method calling semantics apply. Therefore, it is possible to:\nimplement overloaded methods such as eval(Integer) and eval(LocalDateTime), use var-args such as eval(Integer...), use object inheritance such as eval(Object) that takes both LocalDateTime and Integer, and combinations of the above such as eval(Object...) that takes all kinds of arguments. If you intend to implement functions in Scala, please add the scala.annotation.varargs annotation in case of variable arguments. Furthermore, it is recommended to use boxed primitives (e.g. java.lang.Integer instead of Int) to support NULL.\nThe following snippets shows an example of an overloaded function:\nJava import org.apache.flink.table.functions.ScalarFunction; // function with overloaded evaluation methods public static class SumFunction extends ScalarFunction { public Integer eval(Integer a, Integer b) { return a + b; } public Integer eval(String a, String b) { return Integer.valueOf(a) + Integer.valueOf(b); } public Integer eval(Double... d) { double result = 0; for (double value : d) result += value; return (int) result; } } Scala import org.apache.flink.table.functions.ScalarFunction import java.lang.Integer import java.lang.Double import scala.annotation.varargs // function with overloaded evaluation methods class SumFunction extends ScalarFunction { def eval(a: Integer, b: Integer): Integer = { a + b } def eval(a: String, b: String): Integer = { Integer.valueOf(a) + Integer.valueOf(b) } @varargs // generate var-args like Java def eval(d: Double*): Integer = { d.sum.toInt } } Type Inference # The table ecosystem (similar to the SQL standard) is a strongly typed API. Therefore, both function parameters and return types must be mapped to a data type.\nFrom a logical perspective, the planner needs information about expected types, precision, and scale. From a JVM perspective, the planner needs information about how internal data structures are represented as JVM objects when calling a user-defined function.\nThe logic for validating input arguments and deriving data types for both the parameters and the result of a function is summarized under the term type inference.\nFlink\u0026rsquo;s user-defined functions implement an automatic type inference extraction that derives data types from the function\u0026rsquo;s class and its evaluation methods via reflection. If this implicit reflective extraction approach is not successful, the extraction process can be supported by annotating affected parameters, classes, or methods with @DataTypeHint and @FunctionHint. More examples on how to annotate functions are shown below.\nIf more advanced type inference logic is required, an implementer can explicitly override the getTypeInference() method in every user-defined function. However, the annotation approach is recommended because it keeps custom type inference logic close to the affected locations and falls back to the default behavior for the remaining implementation.\nAutomatic Type Inference # The automatic type inference inspects the function\u0026rsquo;s class and evaluation methods to derive data types for the arguments and result of a function. @DataTypeHint and @FunctionHint annotations support the automatic extraction.\nFor a full list of classes that can be implicitly mapped to a data type, see the data type extraction section.\n@DataTypeHint\nIn many scenarios, it is required to support the automatic extraction inline for parameters and return types of a function\nThe following example shows how to use data type hints. More information can be found in the documentation of the annotation class.\nJava import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.InputGroup; import org.apache.flink.table.functions.ScalarFunction; import org.apache.flink.types.Row; // function with overloaded evaluation methods public static class OverloadedFunction extends ScalarFunction { // no hint required public Long eval(long a, long b) { return a + b; } // define the precision and scale of a decimal public @DataTypeHint(\u0026#34;DECIMAL(12, 3)\u0026#34;) BigDecimal eval(double a, double b) { return BigDecimal.valueOf(a + b); } // define a nested data type @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, t TIMESTAMP_LTZ(3)\u0026gt;\u0026#34;) public Row eval(int i) { return Row.of(String.valueOf(i), Instant.ofEpochSecond(i)); } // allow wildcard input and customly serialized output @DataTypeHint(value = \u0026#34;RAW\u0026#34;, bridgedTo = ByteBuffer.class) public ByteBuffer eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o) { return MyUtils.serializeToByteBuffer(o); } } Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.functions.ScalarFunction import org.apache.flink.types.Row import scala.annotation.varargs // function with overloaded evaluation methods class OverloadedFunction extends ScalarFunction { // no hint required def eval(a: Long, b: Long): Long = { a + b } // define the precision and scale of a decimal @DataTypeHint(\u0026#34;DECIMAL(12, 3)\u0026#34;) def eval(double a, double b): BigDecimal = { java.lang.BigDecimal.valueOf(a + b) } // define a nested data type @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, t TIMESTAMP_LTZ(3)\u0026gt;\u0026#34;) def eval(Int i): Row = { Row.of(java.lang.String.valueOf(i), java.time.Instant.ofEpochSecond(i)) } // allow wildcard input and customly serialized output @DataTypeHint(value = \u0026#34;RAW\u0026#34;, bridgedTo = classOf[java.nio.ByteBuffer]) def eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o): java.nio.ByteBuffer = { MyUtils.serializeToByteBuffer(o) } } @FunctionHint\nIn some scenarios, it is desirable that one evaluation method handles multiple different data types at the same time. Furthermore, in some scenarios, overloaded evaluation methods have a common result type that should be declared only once.\nThe @FunctionHint annotation can provide a mapping from argument data types to a result data type. It enables annotating entire function classes or evaluation methods for input, accumulator, and result data types. One or more annotations can be declared on top of a class or individually for each evaluation method for overloading function signatures. All hint parameters are optional. If a parameter is not defined, the default reflection-based extraction is used. Hint parameters defined on top of a function class are inherited by all evaluation methods.\nThe following example shows how to use function hints. More information can be found in the documentation of the annotation class.\nJava import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.FunctionHint; import org.apache.flink.table.functions.TableFunction; import org.apache.flink.types.Row; // function with overloaded evaluation methods // but globally defined output type @FunctionHint(output = @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, i INT\u0026gt;\u0026#34;)) public static class OverloadedFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(int a, int b) { collect(Row.of(\u0026#34;Sum\u0026#34;, a + b)); } // overloading of arguments is still possible public void eval() { collect(Row.of(\u0026#34;Empty args\u0026#34;, -1)); } } // decouples the type inference from evaluation methods, // the type inference is entirely determined by the function hints @FunctionHint( input = {@DataTypeHint(\u0026#34;INT\u0026#34;), @DataTypeHint(\u0026#34;INT\u0026#34;)}, output = @DataTypeHint(\u0026#34;INT\u0026#34;) ) @FunctionHint( input = {@DataTypeHint(\u0026#34;BIGINT\u0026#34;), @DataTypeHint(\u0026#34;BIGINT\u0026#34;)}, output = @DataTypeHint(\u0026#34;BIGINT\u0026#34;) ) @FunctionHint( input = {}, output = @DataTypeHint(\u0026#34;BOOLEAN\u0026#34;) ) public static class OverloadedFunction extends TableFunction\u0026lt;Object\u0026gt; { // an implementer just needs to make sure that a method exists // that can be called by the JVM public void eval(Object... o) { if (o.length == 0) { collect(false); } collect(o[0]); } } Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row // function with overloaded evaluation methods // but globally defined output type @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, i INT\u0026gt;\u0026#34;)) class OverloadedFunction extends TableFunction[Row] { def eval(a: Int, b: Int): Unit = { collect(Row.of(\u0026#34;Sum\u0026#34;, Int.box(a + b))) } // overloading of arguments is still possible def eval(): Unit = { collect(Row.of(\u0026#34;Empty args\u0026#34;, Int.box(-1))) } } // decouples the type inference from evaluation methods, // the type inference is entirely determined by the function hints @FunctionHint( input = Array(new DataTypeHint(\u0026#34;INT\u0026#34;), new DataTypeHint(\u0026#34;INT\u0026#34;)), output = new DataTypeHint(\u0026#34;INT\u0026#34;) ) @FunctionHint( input = Array(new DataTypeHint(\u0026#34;BIGINT\u0026#34;), new DataTypeHint(\u0026#34;BIGINT\u0026#34;)), output = new DataTypeHint(\u0026#34;BIGINT\u0026#34;) ) @FunctionHint( input = Array(), output = new DataTypeHint(\u0026#34;BOOLEAN\u0026#34;) ) class OverloadedFunction extends TableFunction[AnyRef] { // an implementer just needs to make sure that a method exists // that can be called by the JVM @varargs def eval(o: AnyRef*) = { if (o.length == 0) { collect(Boolean.box(false)) } collect(o(0)) } } Custom Type Inference # For most scenarios, @DataTypeHint and @FunctionHint should be sufficient to model user-defined functions. However, by overriding the automatic type inference defined in getTypeInference(), implementers can create arbitrary functions that behave like built-in system functions.\nThe following example implemented in Java illustrates the potential of a custom type inference logic. It uses a string literal argument to determine the result type of a function. The function takes two string arguments: the first argument represents the string to be parsed, the second argument represents the target type.\nJava import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.catalog.DataTypeFactory; import org.apache.flink.table.functions.ScalarFunction; import org.apache.flink.table.types.inference.TypeInference; import org.apache.flink.types.Row; public static class LiteralFunction extends ScalarFunction { public Object eval(String s, String type) { switch (type) { case \u0026#34;INT\u0026#34;: return Integer.valueOf(s); case \u0026#34;DOUBLE\u0026#34;: return Double.valueOf(s); case \u0026#34;STRING\u0026#34;: default: return s; } } // the automatic, reflection-based type inference is disabled and // replaced by the following logic @Override public TypeInference getTypeInference(DataTypeFactory typeFactory) { return TypeInference.newBuilder() // specify typed arguments // parameters will be casted implicitly to those types if necessary .typedArguments(DataTypes.STRING(), DataTypes.STRING()) // specify a strategy for the result data type of the function .outputTypeStrategy(callContext -\u0026gt; { if (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) { throw callContext.newValidationError(\u0026#34;Literal expected for second argument.\u0026#34;); } // return a data type based on a literal final String literal = callContext.getArgumentValue(1, String.class).orElse(\u0026#34;STRING\u0026#34;); switch (literal) { case \u0026#34;INT\u0026#34;: return Optional.of(DataTypes.INT().notNull()); case \u0026#34;DOUBLE\u0026#34;: return Optional.of(DataTypes.DOUBLE().notNull()); case \u0026#34;STRING\u0026#34;: default: return Optional.of(DataTypes.STRING()); } }) .build(); } } For more examples of custom type inference, see also the flink-examples-table module with advanced function implementation .\nDeterminism # Every user-defined function class can declare whether it produces deterministic results or not by overriding the isDeterministic() method. If the function is not purely functional (like random(), date(), or now()), the method must return false. By default, isDeterministic() returns true.\nFurthermore, the isDeterministic() method might also influence the runtime behavior. A runtime implementation might be called at two different stages:\nDuring planning (i.e. pre-flight phase): If a function is called with constant expressions or constant expressions can be derived from the given statement, a function is pre-evaluated for constant expression reduction and might not be executed on the cluster anymore. Unless isDeterministic() is used to disable constant expression reduction in this case. For example, the following calls to ABS are executed during planning: SELECT ABS(-1) FROM t and SELECT ABS(field) FROM t WHERE field = -1; whereas SELECT ABS(field) FROM t is not.\nDuring runtime (i.e. cluster execution): If a function is called with non-constant expressions or isDeterministic() returns false.\nRuntime Integration # Sometimes it might be necessary for a user-defined function to get global runtime information or do some setup/clean-up work before the actual work. User-defined functions provide open() and close() methods that can be overridden and provide similar functionality as the methods in RichFunction of DataStream API.\nThe open() method is called once before the evaluation method. The close() method after the last call to the evaluation method.\nThe open() method provides a FunctionContext that contains information about the context in which user-defined functions are executed, such as the metric group, the distributed cache files, or the global job parameters.\nThe following information can be obtained by calling the corresponding methods of FunctionContext:\nMethod Description getMetricGroup() Metric group for this parallel subtask. getCachedFile(name) Local temporary file copy of a distributed cache file. getJobParameter(name, defaultValue) Global job parameter value associated with given key. getExternalResourceInfos(resourceName) Returns a set of external resource infos associated with the given key. Note: Depending on the context in which the function is executed, not all methods from above might be available. For example, during constant expression reduction adding a metric is a no-op operation.\nThe following example snippet shows how to use FunctionContext in a scalar function for accessing a global job parameter:\nJava import org.apache.flink.table.api.*; import org.apache.flink.table.functions.FunctionContext; import org.apache.flink.table.functions.ScalarFunction; public static class HashCodeFunction extends ScalarFunction { private int factor = 0; @Override public void open(FunctionContext context) throws Exception { // access the global \u0026#34;hashcode_factor\u0026#34; parameter // \u0026#34;12\u0026#34; would be the default value if the parameter does not exist factor = Integer.parseInt(context.getJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;12\u0026#34;)); } public int eval(String s) { return s.hashCode() * factor; } } TableEnvironment env = TableEnvironment.create(...); // add job parameter env.getConfig().addJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;31\u0026#34;); // register the function env.createTemporarySystemFunction(\u0026#34;hashCode\u0026#34;, HashCodeFunction.class); // use the function env.sqlQuery(\u0026#34;SELECT myField, hashCode(myField) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.FunctionContext import org.apache.flink.table.functions.ScalarFunction class HashCodeFunction extends ScalarFunction { private var factor: Int = 0 override def open(context: FunctionContext): Unit = { // access the global \u0026#34;hashcode_factor\u0026#34; parameter // \u0026#34;12\u0026#34; would be the default value if the parameter does not exist factor = context.getJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;12\u0026#34;).toInt } def eval(s: String): Int = { s.hashCode * factor } } val env = TableEnvironment.create(...) // add job parameter env.getConfig.addJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;31\u0026#34;) // register the function env.createTemporarySystemFunction(\u0026#34;hashCode\u0026#34;, classOf[HashCodeFunction]) // use the function env.sqlQuery(\u0026#34;SELECT myField, hashCode(myField) FROM MyTable\u0026#34;) Back to top\nScalar Functions # A user-defined scalar function maps zero, one, or multiple scalar values to a new scalar value. Any data type listed in the data types section can be used as a parameter or return type of an evaluation method.\nIn order to define a scalar function, one has to extend the base class ScalarFunction in org.apache.flink.table.functions and implement one or more evaluation methods named eval(...).\nThe following example shows how to define your own hash code function and call it in a query. See the Implementation Guide for more details.\nJava import org.apache.flink.table.annotation.InputGroup; import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; public static class HashFunction extends ScalarFunction { // take any data type and return INT public int eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o) { return o.hashCode(); } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(HashFunction.class, $(\u0026#34;myField\u0026#34;))); // register function env.createTemporarySystemFunction(\u0026#34;HashFunction\u0026#34;, HashFunction.class); // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;HashFunction\u0026#34;, $(\u0026#34;myField\u0026#34;))); // call registered function in SQL env.sqlQuery(\u0026#34;SELECT HashFunction(myField) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction class HashFunction extends ScalarFunction { // take any data type and return INT def eval(@DataTypeHint(inputGroup = InputGroup.ANY) o: AnyRef): Int = { o.hashCode() } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[HashFunction], $\u0026#34;myField\u0026#34;)) // register function env.createTemporarySystemFunction(\u0026#34;HashFunction\u0026#34;, classOf[HashFunction]) // call registered function in Table API env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;HashFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) // call registered function in SQL env.sqlQuery(\u0026#34;SELECT HashFunction(myField) FROM MyTable\u0026#34;) If you intend to implement or call functions in Python, please refer to the Python Scalar Functions documentation for more details.\nBack to top\nTable Functions # Similar to a user-defined scalar function, a user-defined table function (UDTF) takes zero, one, or multiple scalar values as input arguments. However, it can return an arbitrary number of rows (or structured types) as output instead of a single value. The returned record may consist of one or more fields. If an output record consists of only a single field, the structured record can be omitted, and a scalar value can be emitted that will be implicitly wrapped into a row by the runtime.\nIn order to define a table function, one has to extend the base class TableFunction in org.apache.flink.table.functions and implement one or more evaluation methods named eval(...). Similar to other functions, input and output data types are automatically extracted using reflection. This includes the generic argument T of the class for determining an output data type. In contrast to scalar functions, the evaluation method itself must not have a return type, instead, table functions provide a collect(T) method that can be called within every evaluation method for emitting zero, one, or more records.\nIn the Table API, a table function is used with .joinLateral(...) or .leftOuterJoinLateral(...). The joinLateral operator (cross) joins each row from the outer table (table on the left of the operator) with all rows produced by the table-valued function (which is on the right side of the operator). The leftOuterJoinLateral operator joins each row from the outer table (table on the left of the operator) with all rows produced by the table-valued function (which is on the right side of the operator) and preserves outer rows for which the table function returns an empty table.\nIn SQL, use LATERAL TABLE(\u0026lt;TableFunction\u0026gt;) with JOIN or LEFT JOIN with an ON TRUE join condition.\nThe following example shows how to define your own split function and call it in a query. See the Implementation Guide for more details.\nJava import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.FunctionHint; import org.apache.flink.table.api.*; import org.apache.flink.table.functions.TableFunction; import org.apache.flink.types.Row; import static org.apache.flink.table.api.Expressions.*; @FunctionHint(output = @DataTypeHint(\u0026#34;ROW\u0026lt;word STRING, length INT\u0026gt;\u0026#34;)) public static class SplitFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(String str) { for (String s : str.split(\u0026#34; \u0026#34;)) { // use collect(...) to emit a row collect(Row.of(s, s.length())); } } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(SplitFunction.class, $(\u0026#34;myField\u0026#34;))) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;word\u0026#34;), $(\u0026#34;length\u0026#34;)); env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(SplitFunction.class, $(\u0026#34;myField\u0026#34;))) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;word\u0026#34;), $(\u0026#34;length\u0026#34;)); // rename fields of the function in Table API env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(SplitFunction.class, $(\u0026#34;myField\u0026#34;)).as(\u0026#34;newWord\u0026#34;, \u0026#34;newLength\u0026#34;)) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;newWord\u0026#34;), $(\u0026#34;newLength\u0026#34;)); // register function env.createTemporarySystemFunction(\u0026#34;SplitFunction\u0026#34;, SplitFunction.class); // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(\u0026#34;SplitFunction\u0026#34;, $(\u0026#34;myField\u0026#34;))) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;word\u0026#34;), $(\u0026#34;length\u0026#34;)); env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(\u0026#34;SplitFunction\u0026#34;, $(\u0026#34;myField\u0026#34;))) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;word\u0026#34;), $(\u0026#34;length\u0026#34;)); // call registered function in SQL env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))\u0026#34;); env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE\u0026#34;); // rename fields of the function in SQL env.sqlQuery( \u0026#34;SELECT myField, newWord, newLength \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE\u0026#34;); Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.api._ import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;word STRING, length INT\u0026gt;\u0026#34;)) class SplitFunction extends TableFunction[Row] { def eval(str: String): Unit = { // use collect(...) to emit a row str.split(\u0026#34; \u0026#34;).foreach(s =\u0026gt; collect(Row.of(s, Int.box(s.length)))) } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) // rename fields of the function in Table API env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], $\u0026#34;myField\u0026#34;).as(\u0026#34;newWord\u0026#34;, \u0026#34;newLength\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;newWord\u0026#34;, $\u0026#34;newLength\u0026#34;) // register function env.createTemporarySystemFunction(\u0026#34;SplitFunction\u0026#34;, classOf[SplitFunction]) // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(\u0026#34;SplitFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(\u0026#34;SplitFunction\u0026#34;, $\u0026#34;myField\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;word\u0026#34;, $\u0026#34;length\u0026#34;) // call registered function in SQL env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))\u0026#34;) env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE\u0026#34;) // rename fields of the function in SQL env.sqlQuery( \u0026#34;SELECT myField, newWord, newLength \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE\u0026#34;) If you intend to implement functions in Scala, do not implement a table function as a Scala object. Scala objects are singletons and will cause concurrency issues.\nIf you intend to implement or call functions in Python, please refer to the Python Table Functions documentation for more details.\nBack to top\nAggregate Functions # A user-defined aggregate function (UDAGG) maps scalar values of multiple rows to a new scalar value.\nThe behavior of an aggregate function is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.\nFor each set of rows that needs to be aggregated, the runtime will create an empty accumulator by calling createAccumulator(). Subsequently, the accumulate(...) method of the function is called for each input row to update the accumulator. Once all rows have been processed, the getValue(...) method of the function is called to compute and return the final result.\nThe following example illustrates the aggregation process:\nIn the example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the highest price of all beverages in the table, i.e., perform a max() aggregation. We need to consider each of the 5 rows. The result is a single numeric value.\nIn order to define an aggregate function, one has to extend the base class AggregateFunction in org.apache.flink.table.functions and implement one or more evaluation methods named accumulate(...). An accumulate method must be declared publicly and not static. Accumulate methods can also be overloaded by implementing multiple methods named accumulate.\nBy default, input, accumulator, and output data types are automatically extracted using reflection. This includes the generic argument ACC of the class for determining an accumulator data type and the generic argument T for determining an accumulator data type. Input arguments are derived from one or more accumulate(...) methods. See the Implementation Guide for more details.\nIf you intend to implement or call functions in Python, please refer to the Python Functions documentation for more details.\nThe following example shows how to define your own aggregate function and call it in a query.\nJava import org.apache.flink.table.api.*; import org.apache.flink.table.functions.AggregateFunction; import static org.apache.flink.table.api.Expressions.*; // mutable accumulator of structured type for the aggregate function public static class WeightedAvgAccumulator { public long sum = 0; public int count = 0; } // function that takes (value BIGINT, weight INT), stores intermediate results in a structured // type of WeightedAvgAccumulator, and returns the weighted average as BIGINT public static class WeightedAvg extends AggregateFunction\u0026lt;Long, WeightedAvgAccumulator\u0026gt; { @Override public WeightedAvgAccumulator createAccumulator() { return new WeightedAvgAccumulator(); } @Override public Long getValue(WeightedAvgAccumulator acc) { if (acc.count == 0) { return null; } else { return acc.sum / acc.count; } } public void accumulate(WeightedAvgAccumulator acc, Long iValue, Integer iWeight) { acc.sum += iValue * iWeight; acc.count += iWeight; } public void retract(WeightedAvgAccumulator acc, Long iValue, Integer iWeight) { acc.sum -= iValue * iWeight; acc.count -= iWeight; } public void merge(WeightedAvgAccumulator acc, Iterable\u0026lt;WeightedAvgAccumulator\u0026gt; it) { for (WeightedAvgAccumulator a : it) { acc.count += a.count; acc.sum += a.sum; } } public void resetAccumulator(WeightedAvgAccumulator acc) { acc.count = 0; acc.sum = 0L; } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($(\u0026#34;myField\u0026#34;)) .select($(\u0026#34;myField\u0026#34;), call(WeightedAvg.class, $(\u0026#34;value\u0026#34;), $(\u0026#34;weight\u0026#34;))); // register function env.createTemporarySystemFunction(\u0026#34;WeightedAvg\u0026#34;, WeightedAvg.class); // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($(\u0026#34;myField\u0026#34;)) .select($(\u0026#34;myField\u0026#34;), call(\u0026#34;WeightedAvg\u0026#34;, $(\u0026#34;value\u0026#34;), $(\u0026#34;weight\u0026#34;))); // call registered function in SQL env.sqlQuery( \u0026#34;SELECT myField, WeightedAvg(`value`, weight) FROM MyTable GROUP BY myField\u0026#34; ); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.AggregateFunction // mutable accumulator of structured type for the aggregate function case class WeightedAvgAccumulator( var sum: Long = 0, var count: Int = 0 ) // function that takes (value BIGINT, weight INT), stores intermediate results in a structured // type of WeightedAvgAccumulator, and returns the weighted average as BIGINT class WeightedAvg extends AggregateFunction[java.lang.Long, WeightedAvgAccumulator] { override def createAccumulator(): WeightedAvgAccumulator = { WeightedAvgAccumulator() } override def getValue(acc: WeightedAvgAccumulator): java.lang.Long = { if (acc.count == 0) { null } else { acc.sum / acc.count } } def accumulate(acc: WeightedAvgAccumulator, iValue: java.lang.Long, iWeight: java.lang.Integer): Unit = { acc.sum += iValue * iWeight acc.count += iWeight } def retract(acc: WeightedAvgAccumulator, iValue: java.lang.Long, iWeight: java.lang.Integer): Unit = { acc.sum -= iValue * iWeight acc.count -= iWeight } def merge(acc: WeightedAvgAccumulator, it: java.lang.Iterable[WeightedAvgAccumulator]): Unit = { val iter = it.iterator() while (iter.hasNext) { val a = iter.next() acc.count += a.count acc.sum += a.sum } } def resetAccumulator(acc: WeightedAvgAccumulator): Unit = { acc.count = 0 acc.sum = 0L } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($\u0026#34;myField\u0026#34;) .select($\u0026#34;myField\u0026#34;, call(classOf[WeightedAvg], $\u0026#34;value\u0026#34;, $\u0026#34;weight\u0026#34;)) // register function env.createTemporarySystemFunction(\u0026#34;WeightedAvg\u0026#34;, classOf[WeightedAvg]) // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($\u0026#34;myField\u0026#34;) .select($\u0026#34;myField\u0026#34;, call(\u0026#34;WeightedAvg\u0026#34;, $\u0026#34;value\u0026#34;, $\u0026#34;weight\u0026#34;)) // call registered function in SQL env.sqlQuery( \u0026#34;SELECT myField, WeightedAvg(`value`, weight) FROM MyTable GROUP BY myField\u0026#34; ) The accumulate(...) method of our WeightedAvg class takes three inputs. The first one is the accumulator and the other two are user-defined inputs. In order to calculate a weighted average value, the accumulator needs to store the weighted sum and count of all the data that has been accumulated. In our example, we define a class WeightedAvgAccumulator to be the accumulator. Accumulators are automatically managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of a failure to ensure exactly-once semantics.\nMandatory and Optional Methods # The following methods are mandatory for each AggregateFunction:\ncreateAccumulator() accumulate(...) getValue(...) Additionally, there are a few methods that can be optionally implemented. While some of these methods allow the system more efficient query execution, others are mandatory for certain use cases. For instance, the merge(...) method is mandatory if the aggregation function should be applied in the context of a session group window (the accumulators of two session windows need to be joined when a row is observed that \u0026ldquo;connects\u0026rdquo; them).\nThe following methods of AggregateFunction are required depending on the use case:\nretract(...) is required for aggregations on OVER windows. merge(...) is required for many bounded aggregations and session window and hop window aggregations. Besides, this method is also helpful for optimizations. For example, two phase aggregation optimization requires all the AggregateFunction support merge method. If the aggregate function can only be applied in an OVER window, this can be declared by returning the requirement FunctionRequirement.OVER_WINDOW_ONLY in getRequirements().\nIf an accumulator needs to store large amounts of data, org.apache.flink.table.api.dataview.ListView and org.apache.flink.table.api.dataview.MapView provide advanced features for leveraging Flink\u0026rsquo;s state backends in unbounded data scenarios. Please see the docs of the corresponding classes for more information about this advanced feature.\nSince some of the methods are optional, or can be overloaded, the runtime invokes aggregate function methods via generated code. This means the base class does not always provide a signature to be overridden by the concrete implementation. Nevertheless, all mentioned methods must be declared publicly, not static, and named exactly as the names mentioned above to be called.\nDetailed documentation for all methods that are not declared in AggregateFunction and called by generated code is given below.\naccumulate(...) Java /* * Processes the input values and updates the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An aggregate function * requires at least one accumulate() method. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ public void accumulate(ACC accumulator, [user defined inputs]) Scala /* * Processes the input values and updates the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An aggregate function * requires at least one accumulate() method. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit retract(...) Java /* * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This method must be implemented for * bounded OVER aggregates over unbounded tables. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ public void retract(ACC accumulator, [user defined inputs]) Scala /* * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This method must be implemented for * bounded OVER aggregates over unbounded tables. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit merge(...) Java /* * Merges a group of accumulator instances into one accumulator instance. This method must be * implemented for unbounded session window grouping aggregates and bounded grouping aggregates. * * param: accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * param: iterable an java.lang.Iterable pointed to a group of accumulators that will be * merged. */ public void merge(ACC accumulator, java.lang.Iterable\u0026lt;ACC\u0026gt; iterable) Scala /* * Merges a group of accumulator instances into one accumulator instance. This method must be * implemented for unbounded session window grouping aggregates and bounded grouping aggregates. * * param: accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * param: iterable an java.lang.Iterable pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, iterable: java.lang.Iterable[ACC]): Unit If you intend to implement or call functions in Python, please refer to the Python Aggregate Functions documentation for more details.\nBack to top\nTable Aggregate Functions # A user-defined table aggregate function (UDTAGG) maps scalar values of multiple rows to zero, one, or multiple rows (or structured types). The returned record may consist of one or more fields. If an output record consists of only a single field, the structured record can be omitted, and a scalar value can be emitted that will be implicitly wrapped into a row by the runtime.\nSimilar to an aggregate function, the behavior of a table aggregate is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.\nFor each set of rows that needs to be aggregated, the runtime will create an empty accumulator by calling createAccumulator(). Subsequently, the accumulate(...) method of the function is called for each input row to update the accumulator. Once all rows have been processed, the emitValue(...) or emitUpdateWithRetract(...) method of the function is called to compute and return the final result.\nThe following example illustrates the aggregation process:\nIn the example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the 2 highest prices of all beverages in the table, i.e., perform a TOP2() table aggregation. We need to consider each of the 5 rows. The result is a table with the top 2 values.\nIn order to define a table aggregate function, one has to extend the base class TableAggregateFunction in org.apache.flink.table.functions and implement one or more evaluation methods named accumulate(...). An accumulate method must be declared publicly and not static. Accumulate methods can also be overloaded by implementing multiple methods named accumulate.\nBy default, input, accumulator, and output data types are automatically extracted using reflection. This includes the generic argument ACC of the class for determining an accumulator data type and the generic argument T for determining an accumulator data type. Input arguments are derived from one or more accumulate(...) methods. See the Implementation Guide for more details.\nIf you intend to implement or call functions in Python, please refer to the Python Functions documentation for more details.\nThe following example shows how to define your own table aggregate function and call it in a query.\nJava import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.table.api.*; import org.apache.flink.table.functions.TableAggregateFunction; import org.apache.flink.util.Collector; import static org.apache.flink.table.api.Expressions.*; // mutable accumulator of structured type for the aggregate function public static class Top2Accumulator { public Integer first; public Integer second; } // function that takes (value INT), stores intermediate results in a structured // type of Top2Accumulator, and returns the result as a structured type of Tuple2\u0026lt;Integer, Integer\u0026gt; // for value and rank public static class Top2 extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2Accumulator\u0026gt; { @Override public Top2Accumulator createAccumulator() { Top2Accumulator acc = new Top2Accumulator(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accumulator acc, Integer value) { if (value \u0026gt; acc.first) { acc.second = acc.first; acc.first = value; } else if (value \u0026gt; acc.second) { acc.second = value; } } public void merge(Top2Accumulator acc, Iterable\u0026lt;Top2Accumulator\u0026gt; it) { for (Top2Accumulator otherAcc : it) { accumulate(acc, otherAcc.first); accumulate(acc, otherAcc.second); } } public void emitValue(Top2Accumulator acc, Collector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { // emit the value and rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)); } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)); } } } TableEnvironment env = TableEnvironment.create(...); // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($(\u0026#34;myField\u0026#34;)) .flatAggregate(call(Top2.class, $(\u0026#34;value\u0026#34;))) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;f0\u0026#34;), $(\u0026#34;f1\u0026#34;)); // call function \u0026#34;inline\u0026#34; without registration in Table API // but use an alias for a better naming of Tuple2\u0026#39;s fields env .from(\u0026#34;MyTable\u0026#34;) .groupBy($(\u0026#34;myField\u0026#34;)) .flatAggregate(call(Top2.class, $(\u0026#34;value\u0026#34;)).as(\u0026#34;value\u0026#34;, \u0026#34;rank\u0026#34;)) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;value\u0026#34;), $(\u0026#34;rank\u0026#34;)); // register function env.createTemporarySystemFunction(\u0026#34;Top2\u0026#34;, Top2.class); // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($(\u0026#34;myField\u0026#34;)) .flatAggregate(call(\u0026#34;Top2\u0026#34;, $(\u0026#34;value\u0026#34;)).as(\u0026#34;value\u0026#34;, \u0026#34;rank\u0026#34;)) .select($(\u0026#34;myField\u0026#34;), $(\u0026#34;value\u0026#34;), $(\u0026#34;rank\u0026#34;)); Scala import java.lang.Integer import org.apache.flink.api.java.tuple.Tuple2 import org.apache.flink.table.api._ import org.apache.flink.table.functions.TableAggregateFunction import org.apache.flink.util.Collector // mutable accumulator of structured type for the aggregate function case class Top2Accumulator( var first: Integer, var second: Integer ) // function that takes (value INT), stores intermediate results in a structured // type of Top2Accumulator, and returns the result as a structured type of Tuple2[Integer, Integer] // for value and rank class Top2 extends TableAggregateFunction[Tuple2[Integer, Integer], Top2Accumulator] { override def createAccumulator(): Top2Accumulator = { Top2Accumulator( Integer.MIN_VALUE, Integer.MIN_VALUE ) } def accumulate(acc: Top2Accumulator, value: Integer): Unit = { if (value \u0026gt; acc.first) { acc.second = acc.first acc.first = value } else if (value \u0026gt; acc.second) { acc.second = value } } def merge(acc: Top2Accumulator, it: java.lang.Iterable[Top2Accumulator]) { val iter = it.iterator() while (iter.hasNext) { val otherAcc = iter.next() accumulate(acc, otherAcc.first) accumulate(acc, otherAcc.second) } } def emitValue(acc: Top2Accumulator, out: Collector[Tuple2[Integer, Integer]]): Unit = { // emit the value and rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)) } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)) } } } val env = TableEnvironment.create(...) // call function \u0026#34;inline\u0026#34; without registration in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($\u0026#34;myField\u0026#34;) .flatAggregate(call(classOf[Top2], $\u0026#34;value\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;f0\u0026#34;, $\u0026#34;f1\u0026#34;) // call function \u0026#34;inline\u0026#34; without registration in Table API // but use an alias for a better naming of Tuple2\u0026#39;s fields env .from(\u0026#34;MyTable\u0026#34;) .groupBy($\u0026#34;myField\u0026#34;) .flatAggregate(call(classOf[Top2], $\u0026#34;value\u0026#34;).as(\u0026#34;value\u0026#34;, \u0026#34;rank\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;value\u0026#34;, $\u0026#34;rank\u0026#34;) // register function env.createTemporarySystemFunction(\u0026#34;Top2\u0026#34;, classOf[Top2]) // call registered function in Table API env .from(\u0026#34;MyTable\u0026#34;) .groupBy($\u0026#34;myField\u0026#34;) .flatAggregate(call(\u0026#34;Top2\u0026#34;, $\u0026#34;value\u0026#34;).as(\u0026#34;value\u0026#34;, \u0026#34;rank\u0026#34;)) .select($\u0026#34;myField\u0026#34;, $\u0026#34;value\u0026#34;, $\u0026#34;rank\u0026#34;) The accumulate(...) method of our Top2 class takes two inputs. The first one is the accumulator and the second one is the user-defined input. In order to calculate a result, the accumulator needs to store the 2 highest values of all the data that has been accumulated. Accumulators are automatically managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of a failure to ensure exactly-once semantics. The result values are emitted together with a ranking index.\nMandatory and Optional Methods # The following methods are mandatory for each TableAggregateFunction:\ncreateAccumulator() accumulate(...) emitValue(...) or emitUpdateWithRetract(...) Additionally, there are a few methods that can be optionally implemented. While some of these methods allow the system more efficient query execution, others are mandatory for certain use cases. For instance, the merge(...) method is mandatory if the aggregation function should be applied in the context of a session group window (the accumulators of two session windows need to be joined when a row is observed that \u0026ldquo;connects\u0026rdquo; them).\nThe following methods of TableAggregateFunction are required depending on the use case:\nretract(...) is required for aggregations on OVER windows. merge(...) is required for many bounded aggregations and unbounded session and hop window aggregations. emitValue(...) is required for bounded and window aggregations. The following methods of TableAggregateFunction are used to improve the performance of streaming jobs:\nemitUpdateWithRetract(...) is used to emit values that have been updated under retract mode. The emitValue(...) method always emits the full data according to the accumulator. In unbounded scenarios, this may bring performance problems. Take a Top N function as an example. The emitValue(...) would emit all N values each time. In order to improve the performance, one can implement emitUpdateWithRetract(...) which outputs data incrementally in retract mode. In other words, once there is an update, the method can retract old records before sending new, updated ones. The method will be used in preference to the emitValue(...) method.\nIf the table aggregate function can only be applied in an OVER window, this can be declared by returning the requirement FunctionRequirement.OVER_WINDOW_ONLY in getRequirements().\nIf an accumulator needs to store large amounts of data, org.apache.flink.table.api.dataview.ListView and org.apache.flink.table.api.dataview.MapView provide advanced features for leveraging Flink\u0026rsquo;s state backends in unbounded data scenarios. Please see the docs of the corresponding classes for more information about this advanced feature.\nSince some of methods are optional or can be overloaded, the methods are called by generated code. The base class does not always provide a signature to be overridden by the concrete implementation class. Nevertheless, all mentioned methods must be declared publicly, not static, and named exactly as the names mentioned above to be called.\nDetailed documentation for all methods that are not declared in TableAggregateFunction and called by generated code is given below.\naccumulate(...) Java /* * Processes the input values and updates the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An aggregate function * requires at least one accumulate() method. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ public void accumulate(ACC accumulator, [user defined inputs]) Scala /* * Processes the input values and updates the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An aggregate function * requires at least one accumulate() method. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit retract(...) Java /* * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This method must be implemented for * bounded OVER aggregates over unbounded tables. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ public void retract(ACC accumulator, [user defined inputs]) Scala /* * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This method must be implemented for * bounded OVER aggregates over unbounded tables. * * param: accumulator the accumulator which contains the current aggregated results * param: [user defined inputs] the input value (usually obtained from new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit merge(...) Java /* * Merges a group of accumulator instances into one accumulator instance. This method must be * implemented for unbounded session window grouping aggregates and bounded grouping aggregates. * * param: accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * param: iterable an java.lang.Iterable pointed to a group of accumulators that will be * merged. */ public void merge(ACC accumulator, java.lang.Iterable\u0026lt;ACC\u0026gt; iterable) Scala /* * Merges a group of accumulator instances into one accumulator instance. This method must be * implemented for unbounded session window grouping aggregates and bounded grouping aggregates. * * param: accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * param: iterable an java.lang.Iterable pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, iterable: java.lang.Iterable[ACC]): Unit emitValue(...) Java /* * Called every time when an aggregation result should be materialized. The returned value could * be either an early and incomplete result (periodically emitted as data arrives) or the final * result of the aggregation. * * param: accumulator the accumulator which contains the current aggregated results * param: out the collector used to output data. */ public void emitValue(ACC accumulator, org.apache.flink.util.Collector\u0026lt;T\u0026gt; out) Scala /* * Called every time when an aggregation result should be materialized. The returned value could * be either an early and incomplete result (periodically emitted as data arrives) or the final * result of the aggregation. * * param: accumulator the accumulator which contains the current aggregated results * param: out the collector used to output data. */ def emitValue(accumulator: ACC, out: org.apache.flink.util.Collector[T]): Unit emitUpdateWithRetract(...) Java /* * Called every time when an aggregation result should be materialized. The returned value could * be either an early and incomplete result (periodically emitted as data arrives) or the final * result of the aggregation. * * Compared to emitValue(), emitUpdateWithRetract() is used to emit values that have been updated. This method * outputs data incrementally in retraction mode (also known as \u0026#34;update before\u0026#34; and \u0026#34;update after\u0026#34;). Once * there is an update, we have to retract old records before sending new updated ones. The emitUpdateWithRetract() * method will be used in preference to the emitValue() method if both methods are defined in the table aggregate * function, because the method is treated to be more efficient than emitValue as it can output * values incrementally. * * param: accumulator the accumulator which contains the current aggregated results * param: out the retractable collector used to output data. Use the collect() method * to output(add) records and use retract method to retract(delete) * records. */ public void emitUpdateWithRetract(ACC accumulator, RetractableCollector\u0026lt;T\u0026gt; out) Scala /* * Called every time when an aggregation result should be materialized. The returned value could * be either an early and incomplete result (periodically emitted as data arrives) or the final * result of the aggregation. * * Compared to emitValue(), emitUpdateWithRetract() is used to emit values that have been updated. This method * outputs data incrementally in retraction mode (also known as \u0026#34;update before\u0026#34; and \u0026#34;update after\u0026#34;). Once * there is an update, we have to retract old records before sending new updated ones. The emitUpdateWithRetract() * method will be used in preference to the emitValue() method if both methods are defined in the table aggregate * function, because the method is treated to be more efficient than emitValue as it can output * values incrementally. * * param: accumulator the accumulator which contains the current aggregated results * param: out the retractable collector used to output data. Use the collect() method * to output(add) records and use retract method to retract(delete) * records. */ def emitUpdateWithRetract(accumulator: ACC, out: RetractableCollector[T]): Unit Retraction Example # The following example shows how to use the emitUpdateWithRetract(...) method to emit only incremental updates. In order to do so, the accumulator keeps both the old and new top 2 values.\nIf the N of Top N is big, it might be inefficient to keep both the old and new values. One way to solve this case is to store only the input record in the accumulator in accumulate method and then perform a calculation in emitUpdateWithRetract.\nJava import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.table.functions.TableAggregateFunction; public static class Top2WithRetractAccumulator { public Integer first; public Integer second; public Integer oldFirst; public Integer oldSecond; } public static class Top2WithRetract extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2WithRetractAccumulator\u0026gt; { @Override public Top2WithRetractAccumulator createAccumulator() { Top2WithRetractAccumulator acc = new Top2WithRetractAccumulator(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; acc.oldFirst = Integer.MIN_VALUE; acc.oldSecond = Integer.MIN_VALUE; return acc; } public void accumulate(Top2WithRetractAccumulator acc, Integer v) { if (v \u0026gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v \u0026gt; acc.second) { acc.second = v; } } public void emitUpdateWithRetract( Top2WithRetractAccumulator acc, RetractableCollector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { if (!acc.first.equals(acc.oldFirst)) { // if there is an update, retract the old value then emit a new value if (acc.oldFirst != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldFirst, 1)); } out.collect(Tuple2.of(acc.first, 1)); acc.oldFirst = acc.first; } if (!acc.second.equals(acc.oldSecond)) { // if there is an update, retract the old value then emit a new value if (acc.oldSecond != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldSecond, 2)); } out.collect(Tuple2.of(acc.second, 2)); acc.oldSecond = acc.second; } } } Scala import org.apache.flink.api.java.tuple.Tuple2 import org.apache.flink.table.functions.TableAggregateFunction import org.apache.flink.table.functions.TableAggregateFunction.RetractableCollector case class Top2WithRetractAccumulator( var first: Integer, var second: Integer, var oldFirst: Integer, var oldSecond: Integer ) class Top2WithRetract extends TableAggregateFunction[Tuple2[Integer, Integer], Top2WithRetractAccumulator] { override def createAccumulator(): Top2WithRetractAccumulator = { Top2WithRetractAccumulator( Integer.MIN_VALUE, Integer.MIN_VALUE, Integer.MIN_VALUE, Integer.MIN_VALUE ) } def accumulate(acc: Top2WithRetractAccumulator, value: Integer): Unit = { if (value \u0026gt; acc.first) { acc.second = acc.first acc.first = value } else if (value \u0026gt; acc.second) { acc.second = value } } def emitUpdateWithRetract( acc: Top2WithRetractAccumulator, out: RetractableCollector[Tuple2[Integer, Integer]]) : Unit = { if (!acc.first.equals(acc.oldFirst)) { // if there is an update, retract the old value then emit a new value if (acc.oldFirst != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldFirst, 1)) } out.collect(Tuple2.of(acc.first, 1)) acc.oldFirst = acc.first } if (!acc.second.equals(acc.oldSecond)) { // if there is an update, retract the old value then emit a new value if (acc.oldSecond != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldSecond, 2)) } out.collect(Tuple2.of(acc.second, 2)) acc.oldSecond = acc.second } } } Back to top\n"}),e.add({id:295,href:"/flink/flink-docs-master/docs/dev/python/table/catalogs/",title:"Catalogs",section:"Table API",content:" "}),e.add({id:296,href:"/flink/flink-docs-master/docs/dev/table/modules/",title:"Modules",section:"Table API \u0026 SQL",content:` Modules # Modules allow users to extend Flink\u0026rsquo;s built-in objects, such as defining functions that behave like Flink built-in functions. They are pluggable, and while Flink provides a few pre-built modules, users can write their own.
For example, users can define their own geo functions and plug them into Flink as built-in functions to be used in Flink SQL and Table APIs. Another example is users can load an out-of-shelf Hive module to use Hive built-in functions as Flink built-in functions.
Furthermore, a module can provide built-in table source and sink factories which disable Flink\u0026rsquo;s default discovery mechanism based on Java’s Service Provider Interfaces (SPI), or influence how connectors of temporary tables should be created without a corresponding catalog.
Module Types # CoreModule # CoreModule contains all of Flink\u0026rsquo;s system (built-in) functions and is loaded and enabled by default.
HiveModule # The HiveModule provides Hive built-in functions as Flink\u0026rsquo;s system functions to SQL and Table API users. Flink\u0026rsquo;s Hive documentation provides full details on setting up the module.
User-Defined Module # Users can develop custom modules by implementing the Module interface. To use custom modules in SQL CLI, users should develop both a module and its corresponding module factory by implementing the ModuleFactory interface.
A module factory defines a set of properties for configuring the module when the SQL CLI bootstraps. Properties are passed to a discovery service where the service tries to match the properties to a ModuleFactory and instantiate a corresponding module instance.
Module Lifecycle and Resolution Order # A module can be loaded, enabled, disabled and unloaded. When TableEnvironment loads a module initially, it enables the module by default. Flink supports multiple modules and keeps track of the loading order to resolve metadata. Besides, Flink only resolves the functions among enabled modules. E.g., when there are two functions of the same name residing in two modules, there will be three conditions.
If both of the modules are enabled, then Flink resolves the function according to the resolution order of the modules. If one of them is disabled, then Flink resolves the function to the enabled module. If both of the modules are disabled, then Flink cannot resolve the function. Users can change the resolution order by using modules in a different declared order. E.g., users can specify Flink to find functions first in Hive by USE MODULES hive, core.
Besides, users can also disable modules by not declaring them. E.g., users can specify Flink to disable core module by USE MODULES hive (However, it is strongly not recommended disabling core module). Disable a module does not unload it, and users can enable it again by using it. E.g., users can bring back core module and place it in the first by USE MODULES core, hive. A module can be enabled only when it is loaded already. Using an unloaded module will throw an Exception. Eventually, users can unload a module.
The difference between disabling and unloading a module is that TableEnvironment still keeps the disabled modules, and users can list all loaded modules to view the disabled modules.
Namespace # Objects provided by modules are considered part of Flink\u0026rsquo;s system (built-in) objects; thus, they don\u0026rsquo;t have any namespaces.
How to Load, Unload, Use and List Modules # Using SQL # Users can use SQL to load/unload/use/list modules in both Table API and SQL CLI.
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // Show initially loaded and enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;); // Show all enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.executeSql(\u0026#34;USE MODULES hive, core\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.executeSql(\u0026#34;UNLOAD MODULE hive\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // Empty set tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(setting) // Show initially loaded and enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;) // Show all enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;) // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.executeSql(\u0026#34;USE MODULES hive, core\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.executeSql(\u0026#34;UNLOAD MODULE hive\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // Empty set tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Python from pyflink.table import * # environment configuration settings = EnvironmentSettings.inStreamingMode() t_env = TableEnvironment.create(settings) # Show initially loaded and enabled modules t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # +-------------+------+ # Load a hive module t_env.execute_sql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;) # Show all enabled modules t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ # Show all loaded modules with both name and use status t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # | hive | true | # +-------------+------+ # Change resolution order t_env.execute_sql(\u0026#34;USE MODULES hive, core\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | hive | # | core | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | hive | true | # | core | true | # +-------------+------+ # Disable core module t_env.execute_sql(\u0026#34;USE MODULES hive\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | hive | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ # Unload hive module t_env.execute_sql(\u0026#34;UNLOAD MODULE hive\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # Empty set t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | false | # +-------------+-------+ SQL Client -- Show initially loaded and enabled modules Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | +-------------+ 1 row in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | +-------------+------+ 1 row in set -- Load a hive module Flink SQL\u0026gt; LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;); -- Show all enabled modules Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | | hive | +-------------+ 2 rows in set -- Show all loaded modules with both name and use status Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | | hive | true | +-------------+------+ 2 rows in set -- Change resolution order Flink SQL\u0026gt; USE MODULES hive, core ; Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | hive | | core | +-------------+ 2 rows in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | hive | true | | core | true | +-------------+------+ 2 rows in set -- Unload hive module Flink SQL\u0026gt; UNLOAD MODULE hive; Flink SQL\u0026gt; SHOW MODULES; Empty set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+-------+ | module name | used | +-------------+-------+ | hive | false | +-------------+-------+ 1 row in set YAML All modules defined using YAML must provide a type property that specifies the type. The following types are supported out of the box.
Module Type Value CoreModule core HiveModule hive modules: - name: core type: core - name: hive type: hive When using SQL, module name is used to perform the module discovery. It is parsed as a simple identifier and case-sensitive. Using Java, Scala or Python # Users can use Java, Scala or Python to load/unload/use/list modules programmatically.
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // Show initially loaded and enabled modules tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.loadModule(\u0026#34;hive\u0026#34;, new HiveModule()); // Show all enabled modules tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.useModules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;); tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.useModules(\u0026#34;hive\u0026#34;); tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.listFullModules(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.unloadModule(\u0026#34;hive\u0026#34;); tableEnv.listModules(); // Empty set tableEnv.listFullModules(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(setting) // Show initially loaded and enabled modules tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.loadModule(\u0026#34;hive\u0026#34;, new HiveModule()) // Show all enabled modules tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.useModules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;) tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.useModules(\u0026#34;hive\u0026#34;) tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.listFullModules() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.unloadModule(\u0026#34;hive\u0026#34;) tableEnv.listModules() // Empty set tableEnv.listFullModules() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Python from pyflink.table import * # environment configuration settings = EnvironmentSettings.inStreamingMode() t_env = TableEnvironment.create(settings) # Show initially loaded and enabled modules t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # +-------------+------+ # Load a hive module t_env.load_module(\u0026#34;hive\u0026#34;, HiveModule()) # Show all enabled modules t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ # Show all loaded modules with both name and use status t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # | hive | true | # +-------------+------+ # Change resolution order t_env.use_modules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;) t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | hive | # | core | # +-------------+ t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | hive | true | # | core | true | # +-------------+------+ # Disable core module t_env.use_modules(\u0026#34;hive\u0026#34;) t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | hive | # +-------------+ t_env.list_full_modules() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ # Unload hive module t_env.unload_module(\u0026#34;hive\u0026#34;) t_env.list_modules() # Empty set t_env.list_full_modules() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | false | # +-------------+-------+ Back to top
`}),e.add({id:297,href:"/flink/flink-docs-master/docs/dev/table/catalogs/",title:"Catalogs",section:"Table API \u0026 SQL",content:` Catalogs # Catalogs provide metadata, such as databases, tables, partitions, views, and functions and information needed to access data stored in a database or other external systems.
One of the most crucial aspects of data processing is managing metadata. It may be transient metadata like temporary tables, or UDFs registered against the table environment. Or permanent metadata, like that in a Hive Metastore. Catalogs provide a unified API for managing metadata and making it accessible from the Table API and SQL Queries.
Catalog enables users to reference existing metadata in their data systems, and automatically maps them to Flink\u0026rsquo;s corresponding metadata. For example, Flink can map JDBC tables to Flink table automatically, and users don\u0026rsquo;t have to manually re-writing DDLs in Flink. Catalog greatly simplifies steps required to get started with Flink with users\u0026rsquo; existing system, and greatly enhanced user experiences.
Catalog Types # GenericInMemoryCatalog # The GenericInMemoryCatalog is an in-memory implementation of a catalog. All objects will be available only for the lifetime of the session.
JdbcCatalog # The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol. Postgres Catalog and MySQL Catalog are the only two implementations of JDBC Catalog at the moment. See JdbcCatalog documentation for more details on setting up the catalog.
HiveCatalog # The HiveCatalog serves two purposes; as persistent storage for pure Flink metadata, and as an interface for reading and writing existing Hive metadata. Flink\u0026rsquo;s Hive documentation provides full details on setting up the catalog and interfacing with an existing Hive installation.
The Hive Metastore stores all meta-object names in lower case. This is unlike GenericInMemoryCatalog which is case-sensitive User-Defined Catalog # Catalogs are pluggable and users can develop custom catalogs by implementing the Catalog interface.
In order to use custom catalogs with Flink SQL, users should implement a corresponding catalog factory by implementing the CatalogFactory interface. The factory is discovered using Java\u0026rsquo;s Service Provider Interfaces (SPI). Classes that implement this interface can be added to META_INF/services/org.apache.flink.table.factories.Factory in JAR files. The provided factory identifier will be used for matching against the required type property in a SQL CREATE CATALOG DDL statement.
How to Create and Register Flink Tables to Catalog # Using SQL DDL # Users can use SQL DDL to create tables in catalogs in both Table API and SQL.
Java TableEnvironment tableEnv = ...; // Create a HiveCatalog Catalog catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;); // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog); // Create a catalog database tableEnv.executeSql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;); // Create a catalog table tableEnv.executeSql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;); tableEnv.listTables(); // should return the tables in current catalog and database. Scala val tableEnv = ... // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog) // Create a catalog database tableEnv.executeSql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;) // Create a catalog table tableEnv.executeSql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;) tableEnv.listTables() // should return the tables in current catalog and database. Python from pyflink.table.catalog import HiveCatalog # Create a HiveCatalog catalog = HiveCatalog(\u0026#34;myhive\u0026#34;, None, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) # Register the catalog t_env.register_catalog(\u0026#34;myhive\u0026#34;, catalog) # Create a catalog database t_env.execute_sql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;) # Create a catalog table t_env.execute_sql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;) # should return the tables in current catalog and database. t_env.list_tables() SQL Client // the catalog should have been registered via yaml file Flink SQL\u0026gt; CREATE DATABASE mydb WITH (...); Flink SQL\u0026gt; CREATE TABLE mytable (name STRING, age INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; mytable For detailed information, please check out Flink SQL CREATE DDL.
Using Java, Scala or Python # Users can use Java, Scala or Python to create catalog tables programmatically.
Java import org.apache.flink.table.api.*; import org.apache.flink.table.catalog.*; import org.apache.flink.table.catalog.hive.HiveCatalog; TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.inStreamingMode()); // Create a HiveCatalog Catalog catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;); // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog); // Create a catalog database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...)); // Create a catalog table final Schema schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build(); tableEnv.createTable(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.forConnector(\u0026#34;kafka\u0026#34;) .schema(schema) // … .build()); List\u0026lt;String\u0026gt; tables = catalog.listTables(\u0026#34;mydb\u0026#34;); // tables should contain \u0026#34;mytable\u0026#34; Scala import org.apache.flink.table.api._ import org.apache.flink.table.catalog._ import org.apache.flink.table.catalog.hive.HiveCatalog val tableEnv = TableEnvironment.create(EnvironmentSettings.inStreamingMode()) // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog) // Create a catalog database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...)) // Create a catalog table val schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build() tableEnv.createTable(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.forConnector(\u0026#34;kafka\u0026#34;) .schema(schema) // … .build()) val tables = catalog.listTables(\u0026#34;mydb\u0026#34;) // tables should contain \u0026#34;mytable\u0026#34; Python from pyflink.table import * from pyflink.table.catalog import HiveCatalog, CatalogDatabase, ObjectPath, CatalogBaseTable settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) # Create a HiveCatalog catalog = HiveCatalog(\u0026#34;myhive\u0026#34;, None, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) # Register the catalog t_env.register_catalog(\u0026#34;myhive\u0026#34;, catalog) # Create a catalog database database = CatalogDatabase.create_instance({\u0026#34;k1\u0026#34;: \u0026#34;v1\u0026#34;}, None) catalog.create_database(\u0026#34;mydb\u0026#34;, database) # Create a catalog table schema = Schema.new_builder() \\ .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .column(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() catalog_table = t_env.create_table(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.for_connector(\u0026#34;kafka\u0026#34;) .schema(schema) # … .build()) # tables should contain \u0026#34;mytable\u0026#34; tables = catalog.list_tables(\u0026#34;mydb\u0026#34;) Catalog API # Note: only catalog program APIs are listed here. Users can achieve many of the same functionalities with SQL DDL. For detailed DDL information, please refer to SQL CREATE DDL.
Database operations # Java/Scala // create database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // drop database catalog.dropDatabase(\u0026#34;mydb\u0026#34;, false); // alter database catalog.alterDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // get database catalog.getDatabase(\u0026#34;mydb\u0026#34;); // check if a database exist catalog.databaseExists(\u0026#34;mydb\u0026#34;); // list databases in a catalog catalog.listDatabases(); Python from pyflink.table.catalog import CatalogDatabase # create database catalog_database = CatalogDatabase.create_instance({\u0026#34;k1\u0026#34;: \u0026#34;v1\u0026#34;}, None) catalog.create_database(\u0026#34;mydb\u0026#34;, catalog_database, False) # drop database catalog.drop_database(\u0026#34;mydb\u0026#34;, False) # alter database catalog.alter_database(\u0026#34;mydb\u0026#34;, catalog_database, False) # get database catalog.get_database(\u0026#34;mydb\u0026#34;) # check if a database exist catalog.database_exists(\u0026#34;mydb\u0026#34;) # list databases in a catalog catalog.list_databases() Table operations # Java/Scala // create table catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // drop table catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), false); // alter table catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // rename table catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), \u0026#34;my_new_table\u0026#34;); // get table catalog.getTable(\u0026#34;mytable\u0026#34;); // check if a table exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list tables in a database catalog.listTables(\u0026#34;mydb\u0026#34;); Python from pyflink.table import * from pyflink.table.catalog import CatalogBaseTable, ObjectPath from pyflink.table.descriptors import Kafka table_schema = TableSchema.builder() \\ .field(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .field(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() table_properties = Kafka() \\ .version(\u0026#34;0.11\u0026#34;) \\ .start_from_earlist() \\ .to_properties() catalog_table = CatalogBaseTable.create_table(schema=table_schema, properties=table_properties, comment=\u0026#34;my comment\u0026#34;) # create table catalog.create_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # drop table catalog.drop_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), False) # alter table catalog.alter_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # rename table catalog.rename_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), \u0026#34;my_new_table\u0026#34;) # get table catalog.get_table(\u0026#34;mytable\u0026#34;) # check if a table exist or not catalog.table_exists(\u0026#34;mytable\u0026#34;) # list tables in a database catalog.list_tables(\u0026#34;mydb\u0026#34;) View operations # Java/Scala // create view catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), new CatalogViewImpl(...), false); // drop view catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), false); // alter view catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogViewImpl(...), false); // rename view catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), \u0026#34;my_new_view\u0026#34;, false); // get view catalog.getTable(\u0026#34;myview\u0026#34;); // check if a view exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list views in a database catalog.listViews(\u0026#34;mydb\u0026#34;); Python from pyflink.table import * from pyflink.table.catalog import CatalogBaseTable, ObjectPath table_schema = TableSchema.builder() \\ .field(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .field(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() catalog_table = CatalogBaseTable.create_view( original_query=\u0026#34;select * from t1\u0026#34;, expanded_query=\u0026#34;select * from test-catalog.db1.t1\u0026#34;, schema=table_schema, properties={}, comment=\u0026#34;This is a view\u0026#34; ) catalog.create_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), catalog_table, False) # drop view catalog.drop_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), False) # alter view catalog.alter_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # rename view catalog.rename_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), \u0026#34;my_new_view\u0026#34;, False) # get view catalog.get_table(\u0026#34;myview\u0026#34;) # check if a view exist or not catalog.table_exists(\u0026#34;mytable\u0026#34;) # list views in a database catalog.list_views(\u0026#34;mydb\u0026#34;) Partition operations # Java/Scala // create view catalog.createPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // drop partition catalog.dropPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), false); // alter partition catalog.alterPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // get partition catalog.getPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // check if a partition exist or not catalog.partitionExists(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;)); // list partitions of a table under a give partition spec catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table by expression filter catalog.listPartitionsByFilter(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), Arrays.asList(epr1, ...)); Python from pyflink.table.catalog import ObjectPath, CatalogPartitionSpec, CatalogPartition catalog_partition = CatalogPartition.create_instance({}, \u0026#34;my partition\u0026#34;) catalog_partition_spec = CatalogPartitionSpec({\u0026#34;third\u0026#34;: \u0026#34;2010\u0026#34;, \u0026#34;second\u0026#34;: \u0026#34;bob\u0026#34;}) catalog.create_partition( ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec, catalog_partition, False) # drop partition catalog.drop_partition(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec, False) # alter partition catalog.alter_partition( ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), CatalogPartitionSpec(...), catalog_partition, False) # get partition catalog.get_partition(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) # check if a partition exist or not catalog.partition_exists(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) # list partitions of a table catalog.list_partitions(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;)) # list partitions of a table under a give partition spec catalog.list_partitions(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) Function operations # Java/Scala // create function catalog.createFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // drop function catalog.dropFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), false); // alter function catalog.alterFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // get function catalog.getFunction(\u0026#34;myfunc\u0026#34;); // check if a function exist or not catalog.functionExists(\u0026#34;myfunc\u0026#34;); // list functions in a database catalog.listFunctions(\u0026#34;mydb\u0026#34;); Python from pyflink.table.catalog import ObjectPath, CatalogFunction catalog_function = CatalogFunction.create_instance(class_name=\u0026#34;my.python.udf\u0026#34;) # create function catalog.create_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), catalog_function, False) # drop function catalog.drop_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), False) # alter function catalog.alter_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), catalog_function, False) # get function catalog.get_function(\u0026#34;myfunc\u0026#34;) # check if a function exist or not catalog.function_exists(\u0026#34;myfunc\u0026#34;) # list functions in a database catalog.list_functions(\u0026#34;mydb\u0026#34;) Table API and SQL for Catalog # Registering a Catalog # Users have access to a default in-memory catalog named default_catalog, that is always created by default. This catalog by default has a single database called default_database. Users can also register additional catalogs into an existing Flink session.
Java/Scala tableEnv.registerCatalog(new CustomCatalog(\u0026#34;myCatalog\u0026#34;)); Python t_env.register_catalog(catalog) YAML All catalogs defined using YAML must provide a type property that specifies the type of catalog. The following types are supported out of the box.
Catalog Type Value GenericInMemory generic_in_memory Hive hive catalogs: - name: myCatalog type: custom_catalog hive-conf-dir: ... Changing the Current Catalog And Database # Flink will always search for tables, views, and UDF\u0026rsquo;s in the current catalog and database.
Java/Scala tableEnv.useCatalog(\u0026#34;myCatalog\u0026#34;); tableEnv.useDatabase(\u0026#34;myDb\u0026#34;); Python t_env.use_catalog(\u0026#34;myCatalog\u0026#34;) t_env.use_database(\u0026#34;myDb\u0026#34;) SQL Flink SQL\u0026gt; USE CATALOG myCatalog; Flink SQL\u0026gt; USE myDB; Metadata from catalogs that are not the current catalog are accessible by providing fully qualified names in the form catalog.database.object.
Java/Scala tableEnv.from(\u0026#34;not_the_current_catalog.not_the_current_db.my_table\u0026#34;); Python t_env.from_path(\u0026#34;not_the_current_catalog.not_the_current_db.my_table\u0026#34;) SQL Flink SQL\u0026gt; SELECT * FROM not_the_current_catalog.not_the_current_db.my_table; List Available Catalogs # Java/Scala tableEnv.listCatalogs(); Python t_env.list_catalogs() SQL Flink SQL\u0026gt; show catalogs; List Available Databases # Java/Scala tableEnv.listDatabases(); Python t_env.list_databases() SQL Flink SQL\u0026gt; show databases; List Available Tables # Java/Scala tableEnv.listTables(); Python t_env.list_tables() SQL Flink SQL\u0026gt; show tables; `}),e.add({id:298,href:"/flink/flink-docs-master/docs/dev/table/sqlclient/",title:"SQL Client",section:"Table API \u0026 SQL",content:` SQL Client # Flink’s Table \u0026amp; SQL API makes it possible to work with queries written in the SQL language, but these queries need to be embedded within a table program that is written in either Java or Scala. Moreover, these programs need to be packaged with a build tool before being submitted to a cluster. This more or less limits the usage of Flink to Java/Scala programmers.
The SQL Client aims to provide an easy way of writing, debugging, and submitting table programs to a Flink cluster without a single line of Java or Scala code. The SQL Client CLI allows for retrieving and visualizing real-time results from the running distributed application on the command line.
Getting Started # This section describes how to setup and run your first Flink SQL program from the command-line.
The SQL Client is bundled in the regular Flink distribution and thus runnable out-of-the-box. It requires only a running Flink cluster where table programs can be executed. For more information about setting up a Flink cluster see the Cluster \u0026amp; Deployment part. If you simply want to try out the SQL Client, you can also start a local cluster with one worker using the following command:
./bin/start-cluster.sh Starting the SQL Client CLI # The SQL Client scripts are also located in the binary directory of Flink. In the future, a user will have two possibilities of starting the SQL Client CLI either by starting an embedded standalone process or by connecting to a remote SQL Client Gateway. At the moment only the embedded mode is supported, and default mode is embedded. You can start the CLI by calling:
./bin/sql-client.sh or explicitly use embedded mode:
./bin/sql-client.sh embedded See SQL Client startup options below for more details.
Running SQL Queries # For validating your setup and cluster connection, you can enter the simple query below and press Enter to execute it.
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; SELECT name, COUNT(*) AS cnt FROM (VALUES (\u0026#39;Bob\u0026#39;), (\u0026#39;Alice\u0026#39;), (\u0026#39;Greg\u0026#39;), (\u0026#39;Bob\u0026#39;)) AS NameTable(name) GROUP BY name; The SQL client will retrieve the results from the cluster and visualize them (you can close the result view by pressing the Q key):
+-------+-----+ | name | cnt | +-------+-----+ | Alice | 1 | | Bob | 2 | | Greg | 1 | +-------+-----+ The SET command allows you to tune the job execution and the sql client behaviour. See SQL Client Configuration below for more details.
After a query is defined, it can be submitted to the cluster as a long-running, detached Flink job. The configuration section explains how to declare table sources for reading data, how to declare table sinks for writing data, and how to configure other table program properties.
Getting help # The documentation of the SQL client commands can be accessed by typing the HELP command.
See also the general SQL documentation.
Back to top
Configuration # SQL Client startup options # The SQL Client can be started with the following optional CLI commands. They are discussed in detail in the subsequent paragraphs.
./bin/sql-client.sh --help Mode \u0026#34;embedded\u0026#34; (default) submits Flink jobs from the local machine. Syntax: [embedded] [OPTIONS] \u0026#34;embedded\u0026#34; mode options: -f,--file \u0026lt;script file\u0026gt; Script file that should be executed. In this mode, the client will not open an interactive terminal. -h,--help Show the help message with descriptions of all options. -hist,--history \u0026lt;History file path\u0026gt; The file which you want to save the command history into. If not specified, we will auto-generate one under your user\u0026#39;s home directory. -i,--init \u0026lt;initialization file\u0026gt; Script file that used to init the session context. If get error in execution, the sql client will exit. Notice it\u0026#39;s not allowed to add query or insert into the init file. -j,--jar \u0026lt;JAR file\u0026gt; A JAR file to be imported into the session. The file might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -l,--library \u0026lt;JAR directory\u0026gt; A JAR file directory with which every new session is initialized. The files might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -pyarch,--pyArchives \u0026lt;arg\u0026gt; Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. \u0026#39;#\u0026#39; could be used as the separator of the archive file path and the target directory name. Comma (\u0026#39;,\u0026#39;) could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g.: --pyArchives file:///tmp/py37.zip,file:///tmp/data .zip#data --pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open(\u0026#39;data/data.txt\u0026#39;, \u0026#39;r\u0026#39;). -pyexec,--pyExecutable \u0026lt;arg\u0026gt; Specify the path of the python interpreter used to execute the python UDF worker (e.g.: --pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u0026gt;= 20.3) and SetupTools (version \u0026gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. -pyfs,--pyFiles \u0026lt;pythonFiles\u0026gt; Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (\u0026#39;,\u0026#39;) could be used as the separator to specify multiple files (e.g.: --pyFiles file:///tmp/myresource.zip,hdfs:///\$n amenode_address/myresource2.zip). -pyreq,--pyRequirements \u0026lt;arg\u0026gt; Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use \u0026#39;#\u0026#39; as the separator if the optional parameter exists (e.g.: --pyRequirements file:///tmp/requirements.txt#file:/// tmp/cached_dir). -s,--session \u0026lt;session identifier\u0026gt; The identifier for a session. \u0026#39;default\u0026#39; is the default identifier. -u,--update \u0026lt;SQL update statement\u0026gt; Deprecated Experimental (for testing only!) feature: Instructs the SQL Client to immediately execute the given update statement after starting up. The process is shut down after the statement has been submitted to the cluster and returns an appropriate return code. Currently, this feature is only supported for INSERT INTO statements that declare the target sink table.Please use option -f to submit update statement. SQL Client Configuration # You can configure the SQL client by setting the options below, or any valid Flink configuration entry:
SET \u0026#39;key\u0026#39; = \u0026#39;value\u0026#39;; Key Default Type Description sql-client.display.max-column-width
Streaming 30 Integer When printing the query results, this parameter determines the number of characters shown on screen before truncating.This only applies to columns with variable-length types (e.g. STRING) in streaming mode.Fixed-length types and all types in batch mode are printed using a deterministic column width sql-client.execution.max-table-result.rows
Batch Streaming 1000000 Integer The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style. sql-client.execution.result-mode
Batch Streaming TABLE Enum
Determines how the query result should be displayed.
Possible values:"TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation."CHANGELOG": Visualizes the result stream that is produced by a continuous query."TABLEAU": Display results in the screen directly in a tableau format. sql-client.verbose
Batch Streaming false Boolean Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause. SQL client result modes # The CLI supports three modes for maintaining and visualizing results.
The table mode materializes results in memory and visualizes them in a regular, paginated table representation. It can be enabled by executing the following command in the CLI:
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;table\u0026#39;; The result of a query would then look like this, you can use the keys indicated at the bottom of the screen as well as the arrows keys to navigate and open the various records:
name age isHappy dob height user1 20 true 1995-12-03 1.7 user2 30 true 1972-08-02 1.89 user3 40 false 1983-12-23 1.63 user4 41 true 1977-11-13 1.72 user5 22 false 1998-02-20 1.61 user6 12 true 1969-04-08 1.58 user7 38 false 1987-12-15 1.6 user8 62 true 1996-08-05 1.82 Q Quit + Inc Refresh G Goto Page N Next Page O Open Row R Refresh - Dec Refresh L Last Page P Prev Page The changelog mode does not materialize results and visualizes the result stream that is produced by a continuous query consisting of insertions (+) and retractions (-).
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;changelog\u0026#39;; The result of a query would then look like this:
op name age isHappy dob height +I user1 20 true 1995-12-03 1.7 +I user2 30 true 1972-08-02 1.89 +I user3 40 false 1983-12-23 1.63 +I user4 41 true 1977-11-13 1.72 +I user5 22 false 1998-02-20 1.61 +I user6 12 true 1969-04-08 1.58 +I user7 38 false 1987-12-15 1.6 +I user8 62 true 1996-08-05 1.82 Q Quit + Inc Refresh O Open Row R Refresh - Dec Refresh The tableau mode is more like a traditional way which will display the results in the screen directly with a tableau format. The displaying content will be influenced by the query execution type (execution.type).
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; The result of a query would then look like this:
+----+--------------------------------+-------------+---------+------------+--------------------------------+ | op | name | age | isHappy | dob | height | +----+--------------------------------+-------------+---------+------------+--------------------------------+ | +I | user1 | 20 | true | 1995-12-03 | 1.7 | | +I | user2 | 30 | true | 1972-08-02 | 1.89 | | +I | user3 | 40 | false | 1983-12-23 | 1.63 | | +I | user4 | 41 | true | 1977-11-13 | 1.72 | | +I | user5 | 22 | false | 1998-02-20 | 1.61 | | +I | user6 | 12 | true | 1969-04-08 | 1.58 | | +I | user7 | 38 | false | 1987-12-15 | 1.6 | | +I | user8 | 62 | true | 1996-08-05 | 1.82 | +----+--------------------------------+-------------+---------+------------+--------------------------------+ Received a total of 8 rows Note that when you use this mode with streaming query, the result will be continuously printed on the console. If the input data of this query is bounded, the job will terminate after Flink processed all input data, and the printing will also be stopped automatically. Otherwise, if you want to terminate a running query, just type CTRL-C in this case, the job and the printing will be stopped.
All these result modes can be useful during the prototyping of SQL queries. In all these modes, results are stored in the Java heap memory of the SQL Client. In order to keep the CLI interface responsive, the changelog mode only shows the latest 1000 changes. The table mode allows for navigating through bigger results that are only limited by the available main memory and the configured maximum number of rows (sql-client.execution.max-table-result.rows).
Attention Queries that are executed in a batch environment, can only be retrieved using the table or tableau result mode.
Initialize Session Using SQL Files # A SQL query needs a configuration environment in which it is executed. SQL Client supports the -i startup option to execute an initialization SQL file to setup environment when starting up the SQL Client. The so-called initialization SQL file can use DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment.
An example of such a file is presented below.
-- Define available catalogs CREATE CATALOG MyCatalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39; ); USE CATALOG MyCatalog; -- Define available database CREATE DATABASE MyDatabase; USE MyDatabase; -- Define TABLE CREATE TABLE MyTable( MyField1 INT, MyField2 STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/something\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); -- Define VIEW CREATE VIEW MyCustomView AS SELECT MyField2 FROM MyTable; -- Define user-defined functions here. CREATE FUNCTION foo.bar.AggregateUDF AS myUDF; -- Properties that change the fundamental execution behavior of a table program. SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- execution mode either \u0026#39;batch\u0026#39; or \u0026#39;streaming\u0026#39; SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;table\u0026#39;; -- available values: \u0026#39;table\u0026#39;, \u0026#39;changelog\u0026#39; and \u0026#39;tableau\u0026#39; SET \u0026#39;sql-client.execution.max-table-result.rows\u0026#39; = \u0026#39;10000\u0026#39;; -- optional: maximum number of maintained rows SET \u0026#39;parallelism.default\u0026#39; = \u0026#39;1\u0026#39;; -- optional: Flink\u0026#39;s parallelism (1 by default) SET \u0026#39;pipeline.auto-watermark-interval\u0026#39; = \u0026#39;200\u0026#39;; --optional: interval for periodic watermarks SET \u0026#39;pipeline.max-parallelism\u0026#39; = \u0026#39;10\u0026#39;; -- optional: Flink\u0026#39;s maximum parallelism SET \u0026#39;table.exec.state.ttl\u0026#39; = \u0026#39;1000\u0026#39;; -- optional: table program\u0026#39;s idle state time SET \u0026#39;restart-strategy\u0026#39; = \u0026#39;fixed-delay\u0026#39;; -- Configuration options for adjusting and tuning table programs. SET \u0026#39;table.optimizer.join-reorder-enabled\u0026#39; = \u0026#39;true\u0026#39;; SET \u0026#39;table.exec.spill-compression.enabled\u0026#39; = \u0026#39;true\u0026#39;; SET \u0026#39;table.exec.spill-compression.block-size\u0026#39; = \u0026#39;128kb\u0026#39;; This configuration:
connects to Hive catalogs and uses MyCatalog as the current catalog with MyDatabase as the current database of the catalog, defines a table MyTable that can read data from a CSV file, defines a view MyCustomView that declares a virtual table using a SQL query, defines a user-defined function myUDF that can be instantiated using the class name, uses streaming mode for running statements and a parallelism of 1, runs exploratory queries in the table result mode, and makes some planner adjustments around join reordering and spilling via configuration options. When using -i \u0026lt;init.sql\u0026gt; option to initialize SQL Client session, the following statements are allowed in an initialization SQL file:
DDL(CREATE/DROP/ALTER), USE CATALOG/DATABASE, LOAD/UNLOAD MODULE, SET command, RESET command. When execute queries or insert statements, please enter the interactive mode or use the -f option to submit the SQL statements.
Attention If SQL Client receives errors during initialization, SQL Client will exit with error messages.
Dependencies # The SQL Client does not require setting up a Java project using Maven, Gradle, or sbt. Instead, you can pass the dependencies as regular JAR files that get submitted to the cluster. You can either specify each JAR file separately (using --jar) or define entire library directories (using --library). For connectors to external systems (such as Apache Kafka) and corresponding data formats (such as JSON), Flink provides ready-to-use JAR bundles. These JAR files can be downloaded for each release from the Maven central repository.
The full list of offered SQL JARs can be found on the connection to external systems page.
You can refer to the configuration section for information on how to configure connector and format dependencies.
Back to top
Use SQL Client to submit job # SQL Client allows users to submit jobs either within the interactive command line or using -f option to execute sql file.
In both modes, SQL Client supports to parse and execute all types of the Flink supported SQL statements.
Interactive Command Line # In interactive Command Line, the SQL Client reads user inputs and executes the statement terminated by a semicolon (;).
SQL Client will print success message if the statement is executed successfully. When getting errors, SQL Client will also print error messages. By default, the error message only contains the error cause. In order to print the full exception stack for debugging, please set the sql-client.verbose to true through command SET 'sql-client.verbose' = 'true';.
Execute SQL Files # SQL Client supports to execute a SQL script file with the -f option. SQL Client will execute statements one by one in the SQL script file and print execution messages for each executed statements. Once a statement fails, the SQL Client will exit and all the remaining statements will not be executed.
An example of such a file is presented below.
CREATE TEMPORARY TABLE users ( user_id BIGINT, user_name STRING, user_level STRING, region STRING, PRIMARY KEY (user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;users\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;key.format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro\u0026#39; ); -- set sync mode SET \u0026#39;table.dml-sync\u0026#39; = \u0026#39;true\u0026#39;; -- set the job name SET \u0026#39;pipeline.name\u0026#39; = \u0026#39;SqlJob\u0026#39;; -- set the queue that the job submit to SET \u0026#39;yarn.application.queue\u0026#39; = \u0026#39;root\u0026#39;; -- set the job parallelism SET \u0026#39;parallelism.default\u0026#39; = \u0026#39;100\u0026#39;; -- restore from the specific savepoint path SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;; INSERT INTO pageviews_enriched SELECT * FROM pageviews AS p LEFT JOIN users FOR SYSTEM_TIME AS OF p.proctime AS u ON p.user_id = u.user_id; This configuration:
defines a temporal table source users that reads from a CSV file, set the properties, e.g job name, set the savepoint path, submit a sql job that load the savepoint from the specified savepoint path. Attention Compared to the interactive mode, SQL Client will stop execution and exits when there are errors.
Execute a set of SQL statements # SQL Client execute each INSERT INTO statement as a single Flink job. However, this is sometimes not optimal because some part of the pipeline can be reused. SQL Client supports STATEMENT SET syntax to execute a set of SQL statements. This is an equivalent feature with StatementSet in Table API. The STATEMENT SET syntax encloses one or more INSERT INTO statements. All statements in a STATEMENT SET block are holistically optimized and executed as a single Flink job. Joint optimization and execution allows for reusing common intermediate results and can therefore significantly improve the efficiency of executing multiple queries.
Syntax # EXECUTE STATEMENT SET BEGIN -- one or more INSERT INTO statements { INSERT INTO|OVERWRITE \u0026lt;select_statement\u0026gt;; }+ END; Attention The statements of enclosed in the STATEMENT SET must be separated by a semicolon (;). The old syntax BEGIN STATEMENT SET; ... END; is deprecated, may be removed in the future version.
SQL CLI Flink SQL\u0026gt; CREATE TABLE pageviews ( \u0026gt; user_id BIGINT, \u0026gt; page_id BIGINT, \u0026gt; viewtime TIMESTAMP, \u0026gt; proctime AS PROCTIME() \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026gt; \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026gt; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026gt; \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; CREATE TABLE pageview ( \u0026gt; page_id BIGINT, \u0026gt; cnt BIGINT \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026gt; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026gt; \u0026#39;table-name\u0026#39; = \u0026#39;pageview\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; CREATE TABLE uniqueview ( \u0026gt; page_id BIGINT, \u0026gt; cnt BIGINT \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026gt; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026gt; \u0026#39;table-name\u0026#39; = \u0026#39;uniqueview\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; EXECUTE STATEMENT SET \u0026gt; BEGIN \u0026gt; \u0026gt; INSERT INTO pageview \u0026gt; SELECT page_id, count(1) \u0026gt; FROM pageviews \u0026gt; GROUP BY page_id; \u0026gt; \u0026gt; INSERT INTO uniqueview \u0026gt; SELECT page_id, count(distinct user_id) \u0026gt; FROM pageviews \u0026gt; GROUP BY page_id; \u0026gt; \u0026gt; END; [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 6b1af540c0c0bb3fcfcad50ac037c862 SQL File CREATE TABLE pageviews ( user_id BIGINT, page_id BIGINT, viewtime TIMESTAMP, proctime AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; ); CREATE TABLE pageview ( page_id BIGINT, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;pageview\u0026#39; ); CREATE TABLE uniqueview ( page_id BIGINT, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;uniqueview\u0026#39; ); EXECUTE STATEMENT SET BEGIN INSERT INTO pageview SELECT page_id, count(1) FROM pageviews GROUP BY page_id; INSERT INTO uniqueview SELECT page_id, count(distinct user_id) FROM pageviews GROUP BY page_id; END; Execute DML statements sync/async # By default, SQL Client executes DML statements asynchronously. That means, SQL Client will submit a job for the DML statement to a Flink cluster, and not wait for the job to finish. So SQL Client can submit multiple jobs at the same time. This is useful for streaming jobs, which are long-running in general.
SQL Client makes sure that a statement is successfully submitted to the cluster. Once the statement is submitted, the CLI will show information about the Flink job.
Flink SQL\u0026gt; INSERT INTO MyTableSink SELECT * FROM MyTableSource; [INFO] Table update statement has been successfully submitted to the cluster: Cluster ID: StandaloneClusterId Job ID: 6f922fe5cba87406ff23ae4a7bb79044 Attention The SQL Client does not track the status of the running Flink job after submission. The CLI process can be shutdown after the submission without affecting the detached query. Flink\u0026rsquo;s restart strategy takes care of the fault-tolerance. A query can be cancelled using Flink\u0026rsquo;s web interface, command-line, or REST API.
However, for batch users, it\u0026rsquo;s more common that the next DML statement requires waiting until the previous DML statement finishes. In order to execute DML statements synchronously, you can set table.dml-sync option to true in SQL Client.
Flink SQL\u0026gt; SET \u0026#39;table.dml-sync\u0026#39; = \u0026#39;true\u0026#39;; [INFO] Session property has been set. Flink SQL\u0026gt; INSERT INTO MyTableSink SELECT * FROM MyTableSource; [INFO] Submitting SQL update statement to the cluster... [INFO] Execute statement in sync mode. Please wait for the execution finish... [INFO] Complete execution of the SQL update statement. Attention If you want to terminate the job, just type CTRL-C to cancel the execution.
Start a SQL Job from a savepoint # Flink supports to start the job with specified savepoint. In SQL Client, it\u0026rsquo;s allowed to use SET command to specify the path of the savepoint.
Flink SQL\u0026gt; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;; [INFO] Session property has been set. -- all the following DML statements will be restroed from the specified savepoint path Flink SQL\u0026gt; INSERT INTO ... When the path to savepoint is specified, Flink will try to restore the state from the savepoint when executing all the following DML statements.
Because the specified savepoint path will affect all the following DML statements, you can use RESET command to reset this config option, i.e. disable restoring from savepoint.
Flink SQL\u0026gt; RESET execution.savepoint.path; [INFO] Session property has been reset. For more details about creating and managing savepoints, please refer to Job Lifecycle Management.
Define a Custom Job Name # SQL Client supports to define job name for queries and DML statements through SET command.
Flink SQL\u0026gt; SET \u0026#39;pipeline.name\u0026#39; = \u0026#39;kafka-to-hive\u0026#39;; [INFO] Session property has been set. -- all the following DML statements will use the specified job name. Flink SQL\u0026gt; INSERT INTO ... Because the specified job name will affect all the following queries and DML statements, you can also use RESET command to reset this configuration, i.e. use default job names.
Flink SQL\u0026gt; RESET pipeline.name; [INFO] Session property has been reset. If the option pipeline.name is not specified, SQL Client will generate a default name for the submitted job, e.g. insert-into_\u0026lt;sink_table_name\u0026gt; for INSERT INTO statements.
Back to top
Limitations \u0026amp; Future # The current SQL Client only supports embedded mode. In the future, the community plans to extend its functionality by providing a REST-based SQL Client Gateway, see more in FLIP-24 and FLIP-91.
Back to top
`}),e.add({id:299,href:"/flink/flink-docs-master/docs/connectors/table/downloads/",title:"Download",section:"Table API Connectors",content:` SQL Connectors download page # Download links are available only for stable releases. The page contains links to optional SQL Client connectors and formats that are not part of the binary distribution.
Optional SQL formats # Name Download link Avro Only available for stable versions. Avro Schema Registry Only available for stable versions. Debezium Only available for stable versions. ORC Only available for stable versions. Parquet Only available for stable versions. Protobuf Only available for stable versions. Optional SQL connectors # Name Version Download Link AWS Kinesis Firehose Only available for stable versions. Elasticsearch 6.x Only available for stable versions. Elasticsearch 7.x and later versions Only available for stable versions. Files Only available for stable versions. HBase 1.4.x Only available for stable versions. HBase 2.2.x Only available for stable versions. JDBC Only available for stable versions. Kafka universal Only available for stable versions. Kinesis Only available for stable versions. Pulsar Only available for stable versions. RabbitMQ Only available for stable versions. Upsert Kafka universal Only available for stable versions. `}),e.add({id:300,href:"/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/",title:"Network Buffer Tuning",section:"Memory Configuration",content:` Network memory tuning guide # Overview # Each record in Flink is sent to the next subtask compounded with other records in a network buffer, the smallest unit for communication between subtasks. In order to maintain consistent high throughput, Flink uses network buffer queues (also known as in-flight data) on the input and output side of the transmission process.
Each subtask has an input queue waiting to consume data and an output queue waiting to send data to the next subtask. Having a larger amount of in-flight data means that Flink can provide higher and more resilient throughput in the pipeline. This will, however, cause longer checkpoint times.
Checkpoints in Flink can only finish once all the subtasks receive all of the injected checkpoint barriers. In aligned checkpoints, those checkpoint barriers are traveling throughout the job graph along with the network buffers. The larger the amount of in-flight data, the longer the checkpoint barrier propagation time. In unaligned checkpoints, the larger the amount of in-flight data, the larger the checkpoint size will be because all of the captured in-flight data has to be persisted as part of the checkpoint.
The Buffer Debloating Mechanism # Previously, the only way to configure the amount of in-flight data was to specify both the buffer amount and the buffer size. However, ideal values can be difficult to choose since they are different for every deployment. The buffer debloating mechanism added in Flink 1.14 attempts to address this issue by automatically adjusting the amount of in-flight data to reasonable values.
The buffer debloating feature calculates the maximum possible throughput for the subtask (in the scenario that it is always busy) and adjusts the amount of in-flight data such that the consumption time of those in-flight data will be equal to the configured value.
The buffer debloat mechanism can be enabled by setting the property taskmanager.network.memory.buffer-debloat.enabled to true. The targeted time to consume the in-flight data can be configured by setting taskmanager.network.memory.buffer-debloat.target to a duration. The default value of the debloat target should be good enough for most cases.
This feature uses past throughput data to predict the time required to consume the remaining in-flight data. If the predictions are incorrect, the debloating mechanism can fail in one of two ways:
There will not be enough buffered data to provide full throughput. There will be too many buffered in-flight data which will negatively affect the aligned checkpoint barriers propagation time or the unaligned checkpoint size. If you have a varying load in your Job (i.e. sudden spikes of incoming records, periodically firing windowed aggregations or joins), you might need to adjust the following settings:
taskmanager.network.memory.buffer-debloat.period - This is the minimum time period between buffer size recalculation. The shorter the period, the faster the reaction time of the debloating mechanism but the higher the CPU overhead for the necessary calculations.
taskmanager.network.memory.buffer-debloat.samples - This adjusts the number of samples over which throughput measurements are averaged out. The frequency of the collected samples can be adjusted via taskmanager.network.memory.buffer-debloat.period. The fewer the samples, the faster the reaction time of the debloating mechanism, but a higher chance of a sudden spike or drop of the throughput which can cause the buffer debloating mechanism to miscalculate the optimal amount of in-flight data.
taskmanager.network.memory.buffer-debloat.threshold-percentages - An optimization for preventing frequent buffer size changes (i.e. if the new size is not much different compared to the old size).
Consult the configuration documentation for more details and additional parameters.
Here are metrics you can use to monitor the current buffer size:
estimatedTimeToConsumeBuffersMs - total time to consume data from all input channels debloatedBufferSize - current buffer size Limitations # Currently, there are a few cases that are not handled automatically by the buffer debloating mechanism.
Multiple inputs and unions # Currently, the throughput calculation and buffer debloating happen on the subtask level.
If your subtask has multiple different inputs or it has a single but unioned input, buffer debloating can cause the input of the low throughput to have too much buffered in-flight data, while the input of the high throughput might have buffers that are too small to sustain that throughput. This might be particularly visible if the different inputs have vastly different throughputs. We recommend paying special attention to such subtasks when testing this feature.
Buffer size and number of buffers # Currently, buffer debloating only caps at the maximal used buffer size. The actual buffer size and the number of buffers remain unchanged. This means that the debloating mechanism cannot reduce the memory usage of your job. You would have to manually reduce either the amount or the size of the buffers.
Furthermore, if you want to reduce the amount of buffered in-flight data below what buffer debloating currently allows, you might want to manually configure the number of buffers.
High parallelism # Currently, the buffer debloating mechanism might not perform correctly with high parallelism (above ~200) using the default configuration. If you observe reduced throughput or higher than expected checkpointing times we suggest increasing the number of floating buffers (taskmanager.network.memory.floating-buffers-per-gate) from the default value to at least the number equal to the parallelism.
The actual value of parallelism from which the problem occurs is various from job to job but normally it should be more than a couple of hundreds.
Network buffer lifecycle # Flink has several local buffer pools - one for the output stream and one for each input gate. Each of those pools is limited to at most
#channels * taskmanager.network.memory.buffers-per-channel + taskmanager.network.memory.floating-buffers-per-gate
The size of the buffer can be configured by setting taskmanager.memory.segment-size.
Input network buffers # Buffers in the input channel are divided into exclusive and floating buffers. Exclusive buffers can be used by only one particular channel. A channel can request additional floating buffers from a buffer pool shared across all channels belonging to the given input gate. The remaining floating buffers are optional and are acquired only if there are enough resources available.
In the initialization phase:
Flink will try to acquire the configured amount of exclusive buffers for each channel all exclusive buffers must be fulfilled or the job will fail with an exception a single floating buffer has to be allocated for Flink to be able to make progress Output network buffers # Unlike the input buffer pool, the output buffer pool has only one type of buffer which it shares among all subpartitions.
In order to avoid excessive data skew, the number of buffers for each subpartition is limited by the taskmanager.network.memory.max-buffers-per-channel setting.
Unlike the input buffer pool, the configured amount of exclusive buffers and floating buffers is only treated as recommended values. If there are not enough buffers available, Flink can make progress with only a single exclusive buffer per output subpartition and zero floating buffers.
Overdraft buffers # For each output subtask can also request up to taskmanager.network.memory.max-overdraft-buffers-per-gate (by default 5) extra overdraft buffers. Those buffers are only used, if the subtask is backpressured by downstream subtasks and the subtask requires more than a single network buffer to finish what its currently doing. This can happen in situations like:
Serializing very large records, that do not fit into a single network buffer. Flat Map like operator, that produces many output records per single input record. Operators that output many records either periodically or on a reaction to some events (for example WindowOperator\u0026rsquo;s triggers). Without overdraft buffers in such situations Flink subtask thread would block on the backpressure, preventing for example unaligned checkpoints from completing. To mitigate this, the overdraft buffers concept has been added. Those overdraft buffers are strictly optional and Flink can gradually make progress using only regular buffers, which means 0 is an acceptable configuration for the taskmanager.network.memory.max-overdraft-buffers-per-gate.
The number of in-flight buffers # The default settings for exclusive buffers and floating buffers should be sufficient for the maximum throughput. If the minimum of in-flight data needs to be set, the exclusive buffers can be set to 0 and the memory segment size can be decreased.
Selecting the buffer size # The buffer collects records in order to optimize network overhead when sending the data portion to the next subtask. The next subtask should receive all parts of the record before consuming it.
If the buffer size is too small, or the buffers are flushed too frequently (execution.buffer-timeout configuration parameter), this can lead to decreased throughput since the per-buffer overhead are significantly higher then per-record overheads in the Flink\u0026rsquo;s runtime.
As a rule of thumb, we don\u0026rsquo;t recommend thinking about increasing the buffer size, or the buffer timeout unless you can observe a network bottleneck in your real life workload (downstream operator idling, upstream backpressured, output buffer queue is full, downstream input queue is empty).
If the buffer size is too large, this can lead to:
high memory usage huge checkpoint data (for unaligned checkpoints) long checkpoint time (for aligned checkpoints) inefficient use of allocated memory with a small execution.buffer-timeout because flushed buffers would only be sent partially filled Selecting the buffer count # The number of buffers is configured by the taskmanager.network.memory.buffers-per-channel and taskmanager.network.memory.floating-buffers-per-gate settings.
For best throughput, we recommend using the default values for the number of exclusive and floating buffers(except you have one of limit cases). If the amount of in-flight data is causing issues, enabling buffer debloating is recommended.
You can tune the number of network buffers manually, but consider the following:
You should adjust the number of buffers according to your expected throughput (in bytes/second). Assigning credits and sending buffers takes some time (around two roundtrip messages between two nodes). The latency also depends on your network. Using the buffer roundtrip time (around 1ms in a healthy local network), the buffer size, and the expected throughput, you can calculate the number of buffers required to sustain the throughput by using this formula:
number_of_buffers = expected_throughput * buffer_roundtrip / buffer_size For example, with an expected throughput of 320MB/s, roundtrip latency of 1ms, and the default memory segment size, 10 is the number of actively used buffers needed to achieve the expected throughput:
number_of_buffers = 320MB/s * 1ms / 32KB = 10 The purpose of floating buffers is to handle data skew scenarios. Ideally, the number of floating buffers (default: 8) and the exclusive buffers (default: 2) that belong to that channel should be able to saturate the network throughput. But this is not always feasible or necessary. It is very rare that only a single channel among all the subtasks in the task manager is being used.
The purpose of exclusive buffers is to provide a fluent throughput. While one buffer is in transit, the other is being filled up. With high throughput setups, the number of exclusive buffers is the main factor that defines the amount of in-flight data Flink uses.
In the case of backpressure in low throughput setups, you should consider reducing the number of exclusive buffers.
Summary # Memory configuration tuning for the network in Flink can be simplified by enabling the buffer debloating mechanism. You may have to tune it.
If this does not work, you can disable the buffer debloating mechanism and manually configure the memory segment size and the number of buffers. For this second scenario, we recommend:
using the default values for max throughput reducing the memory segment size and/or number of exclusive buffers to speed up checkpointing and reduce the memory consumption of the network stack Back to top
`}),e.add({id:301,href:"/flink/flink-docs-master/docs/dev/datastream/testing/",title:"Testing",section:"DataStream API",content:` Testing # Testing is an integral part of every software development process as such Apache Flink comes with tooling to test your application code on multiple levels of the testing pyramid.
Testing User-Defined Functions # Usually, one can assume that Flink produces correct results outside of a user-defined function. Therefore, it is recommended to test those classes that contain the main business logic with unit tests as much as possible.
Unit Testing Stateless, Timeless UDFs # For example, let\u0026rsquo;s take the following stateless MapFunction.
Java public class IncrementMapFunction implements MapFunction\u0026lt;Long, Long\u0026gt; { @Override public Long map(Long record) throws Exception { return record + 1; } } Scala class IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } It is very easy to unit test such a function with your favorite testing framework by passing suitable arguments and verifying the output.
Java public class IncrementMapFunctionTest { @Test public void testIncrement() throws Exception { // instantiate your function IncrementMapFunction incrementer = new IncrementMapFunction(); // call the methods that you have implemented assertEquals(3L, incrementer.map(2L)); } } Scala class IncrementMapFunctionTest extends FlatSpec with Matchers { \u0026#34;IncrementMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function val incrementer: IncrementMapFunction = new IncrementMapFunction() // call the methods that you have implemented incremeter.map(2) should be (3) } } Similarly, a user-defined function which uses an org.apache.flink.util.Collector (e.g. a FlatMapFunction or ProcessFunction) can be easily tested by providing a mock object instead of a real collector. A FlatMapFunction with the same functionality as the IncrementMapFunction could be unit tested as follows.
Java public class IncrementFlatMapFunctionTest { @Test public void testIncrement() throws Exception { // instantiate your function IncrementFlatMapFunction incrementer = new IncrementFlatMapFunction(); Collector\u0026lt;Integer\u0026gt; collector = mock(Collector.class); // call the methods that you have implemented incrementer.flatMap(2L, collector); //verify collector was called with the right output Mockito.verify(collector, times(1)).collect(3L); } } Scala class IncrementFlatMapFunctionTest extends FlatSpec with MockFactory { \u0026#34;IncrementFlatMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function val incrementer : IncrementFlatMapFunction = new IncrementFlatMapFunction() val collector = mock[Collector[Integer]] //verify collector was called with the right output (collector.collect _).expects(3) // call the methods that you have implemented flattenFunction.flatMap(2, collector) } } Unit Testing Stateful or Timely UDFs \u0026amp; Custom Operators # Testing the functionality of a user-defined function, which makes use of managed state or timers is more difficult because it involves testing the interaction between the user code and Flink\u0026rsquo;s runtime. For this Flink comes with a collection of so called test harnesses, which can be used to test such user-defined functions as well as custom operators:
OneInputStreamOperatorTestHarness (for operators on DataStreams) KeyedOneInputStreamOperatorTestHarness (for operators on KeyedStreams) TwoInputStreamOperatorTestHarness (for operators of ConnectedStreams of two DataStreams) KeyedTwoInputStreamOperatorTestHarness (for operators on ConnectedStreams of two KeyedStreams) To use the test harnesses a set of additional dependencies is needed. Refer to the configuration section for more detail.
Now, the test harnesses can be used to push records and watermarks into your user-defined functions or custom operators, control processing time and finally assert on the output of the operator (including side outputs).
Java public class StatefulFlatMapTest { private OneInputStreamOperatorTestHarness\u0026lt;Long, Long\u0026gt; testHarness; private StatefulFlatMap statefulFlatMapFunction; @Before public void setupTestHarness() throws Exception { //instantiate user-defined function statefulFlatMapFunction = new StatefulFlatMapFunction(); // wrap user defined function into a the corresponding operator testHarness = new OneInputStreamOperatorTestHarness\u0026lt;\u0026gt;(new StreamFlatMap\u0026lt;\u0026gt;(statefulFlatMapFunction)); // optionally configured the execution environment testHarness.getExecutionConfig().setAutoWatermarkInterval(50); // open the test harness (will also call open() on RichFunctions) testHarness.open(); } @Test public void testingStatefulFlatMapFunction() throws Exception { //push (timestamped) elements into the operator (and hence user defined function) testHarness.processElement(2L, 100L); //trigger event time timers by advancing the event time of the operator with a watermark testHarness.processWatermark(100L); //trigger processing time timers by advancing the processing time of the operator directly testHarness.setProcessingTime(100L); //retrieve list of emitted records for assertions assertThat(testHarness.getOutput(), containsInExactlyThisOrder(3L)); //retrieve list of records emitted to a specific side output for assertions (ProcessFunction only) //assertThat(testHarness.getSideOutput(new OutputTag\u0026lt;\u0026gt;(\u0026#34;invalidRecords\u0026#34;)), hasSize(0)) } } Scala class StatefulFlatMapFunctionTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[Long, Long] = null private var statefulFlatMap: StatefulFlatMapFunction = null before { //instantiate user-defined function statefulFlatMap = new StatefulFlatMap // wrap user defined function into a the corresponding operator testHarness = new OneInputStreamOperatorTestHarness[Long, Long](new StreamFlatMap(statefulFlatMap)) // optionally configured the execution environment testHarness.getExecutionConfig().setAutoWatermarkInterval(50) // open the test harness (will also call open() on RichFunctions) testHarness.open() } \u0026#34;StatefulFlatMap\u0026#34; should \u0026#34;do some fancy stuff with timers and state\u0026#34; in { //push (timestamped) elements into the operator (and hence user defined function) testHarness.processElement(2, 100) //trigger event time timers by advancing the event time of the operator with a watermark testHarness.processWatermark(100) //trigger proccesign time timers by advancing the processing time of the operator directly testHarness.setProcessingTime(100) //retrieve list of emitted records for assertions testHarness.getOutput should contain (3) //retrieve list of records emitted to a specific side output for assertions (ProcessFunction only) //testHarness.getSideOutput(new OutputTag[Int](\u0026#34;invalidRecords\u0026#34;)) should have size 0 } } KeyedOneInputStreamOperatorTestHarness and KeyedTwoInputStreamOperatorTestHarness are instantiated by additionally providing a KeySelector including TypeInformation for the class of the key.
Java public class StatefulFlatMapFunctionTest { private OneInputStreamOperatorTestHarness\u0026lt;String, Long, Long\u0026gt; testHarness; private StatefulFlatMap statefulFlatMapFunction; @Before public void setupTestHarness() throws Exception { //instantiate user-defined function statefulFlatMapFunction = new StatefulFlatMapFunction(); // wrap user defined function into a the corresponding operator testHarness = new KeyedOneInputStreamOperatorTestHarness\u0026lt;\u0026gt;(new StreamFlatMap\u0026lt;\u0026gt;(statefulFlatMapFunction), new MyStringKeySelector(), Types.STRING); // open the test harness (will also call open() on RichFunctions) testHarness.open(); } //tests } Scala class StatefulFlatMapTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[String, Long, Long] = null private var statefulFlatMapFunction: FlattenFunction = null before { //instantiate user-defined function statefulFlatMapFunction = new StateFulFlatMap // wrap user defined function into a the corresponding operator testHarness = new KeyedOneInputStreamOperatorTestHarness(new StreamFlatMap(statefulFlatMapFunction),new MyStringKeySelector(), Types.STRING()) // open the test harness (will also call open() on RichFunctions) testHarness.open() } //tests } Many more examples for the usage of these test harnesses can be found in the Flink code base, e.g.:
org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest is a good example for testing operators and user-defined functions, which depend on processing or event time. Note Be aware that AbstractStreamOperatorTestHarness and its derived classes are currently not part of the public API and can be subject to change.
Unit Testing ProcessFunction # Given its importance, in addition to the previous test harnesses that can be used directly to test a ProcessFunction, Flink provides a test harness factory named ProcessFunctionTestHarnesses that allows for easier test harness instantiation. Considering this example:
Note Be aware that to use this test harness, you also need to introduce the dependencies mentioned in the last section.
Java public static class PassThroughProcessFunction extends ProcessFunction\u0026lt;Integer, Integer\u0026gt; { @Override public void processElement(Integer value, Context ctx, Collector\u0026lt;Integer\u0026gt; out) throws Exception { out.collect(value); } } Scala class PassThroughProcessFunction extends ProcessFunction[Integer, Integer] { @throws[Exception] override def processElement(value: Integer, ctx: ProcessFunction[Integer, Integer]#Context, out: Collector[Integer]): Unit = { out.collect(value) } } It is very easy to unit test such a function with ProcessFunctionTestHarnesses by passing suitable arguments and verifying the output.
Java public class PassThroughProcessFunctionTest { @Test public void testPassThrough() throws Exception { //instantiate user-defined function PassThroughProcessFunction processFunction = new PassThroughProcessFunction(); // wrap user defined function into a the corresponding operator OneInputStreamOperatorTestHarness\u0026lt;Integer, Integer\u0026gt; harness = ProcessFunctionTestHarnesses .forProcessFunction(processFunction); //push (timestamped) elements into the operator (and hence user defined function) harness.processElement(1, 10); //retrieve list of emitted records for assertions assertEquals(harness.extractOutputValues(), Collections.singletonList(1)); } } Scala class PassThroughProcessFunctionTest extends FlatSpec with Matchers { \u0026#34;PassThroughProcessFunction\u0026#34; should \u0026#34;forward values\u0026#34; in { //instantiate user-defined function val processFunction = new PassThroughProcessFunction // wrap user defined function into a the corresponding operator val harness = ProcessFunctionTestHarnesses.forProcessFunction(processFunction) //push (timestamped) elements into the operator (and hence user defined function) harness.processElement(1, 10) //retrieve list of emitted records for assertions harness.extractOutputValues() should contain (1) } } For more examples on how to use the ProcessFunctionTestHarnesses in order to test the different flavours of the ProcessFunction, e.g. KeyedProcessFunction, KeyedCoProcessFunction, BroadcastProcessFunction, etc, the user is encouraged to look at the ProcessFunctionTestHarnessesTest.
Testing Flink Jobs # JUnit Rule MiniClusterWithClientResource # Apache Flink provides a JUnit rule called MiniClusterWithClientResource for testing complete jobs against a local, embedded mini cluster. called MiniClusterWithClientResource.
To use MiniClusterWithClientResource one additional dependency (test scoped) is needed.
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Let us take the same simple MapFunction as in the previous sections.
Java public class IncrementMapFunction implements MapFunction\u0026lt;Long, Long\u0026gt; { @Override public Long map(Long record) throws Exception { return record + 1; } } Scala class IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } A simple pipeline using this MapFunction can now be tested in a local Flink cluster as follows.
Java public class ExampleIntegrationTest { @ClassRule public static MiniClusterWithClientResource flinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration.Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build()); @Test public void testIncrementPipeline() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // configure your test environment env.setParallelism(2); // values are collected in a static variable CollectSink.values.clear(); // create a stream of custom elements and apply transformations env.fromElements(1L, 21L, 22L) .map(new IncrementMapFunction()) .addSink(new CollectSink()); // execute env.execute(); // verify your results assertTrue(CollectSink.values.containsAll(2L, 22L, 23L)); } // create a testing sink private static class CollectSink implements SinkFunction\u0026lt;Long\u0026gt; { // must be static public static final List\u0026lt;Long\u0026gt; values = Collections.synchronizedList(new ArrayList\u0026lt;\u0026gt;()); @Override public void invoke(Long value, SinkFunction.Context context) throws Exception { values.add(value); } } } Scala class StreamingJobIntegrationTest extends FlatSpec with Matchers with BeforeAndAfter { val flinkCluster = new MiniClusterWithClientResource(new MiniClusterResourceConfiguration.Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build) before { flinkCluster.before() } after { flinkCluster.after() } \u0026#34;IncrementFlatMapFunction pipeline\u0026#34; should \u0026#34;incrementValues\u0026#34; in { val env = StreamExecutionEnvironment.getExecutionEnvironment // configure your test environment env.setParallelism(2) // values are collected in a static variable CollectSink.values.clear() // create a stream of custom elements and apply transformations env.fromElements(1L, 21L, 22L) .map(new IncrementMapFunction()) .addSink(new CollectSink()) // execute env.execute() // verify your results CollectSink.values should contain allOf (2, 22, 23) } } // create a testing sink class CollectSink extends SinkFunction[Long] { override def invoke(value: Long, context: SinkFunction.Context): Unit = { CollectSink.values.add(value) } } object CollectSink { // must be static val values: util.List[Long] = Collections.synchronizedList(new util.ArrayList()) } A few remarks on integration testing with MiniClusterWithClientResource:
In order not to copy your whole pipeline code from production to test, make sources and sinks pluggable in your production code and inject special test sources and test sinks in your tests.
The static variable in CollectSink is used here because Flink serializes all operators before distributing them across a cluster. Communicating with operators instantiated by a local Flink mini cluster via static variables is one way around this issue. Alternatively, you could write the data to files in a temporary directory with your test sink.
You can implement a custom parallel source function for emitting watermarks if your job uses event time timers.
It is recommended to always test your pipelines locally with a parallelism \u0026gt; 1 to identify bugs which only surface for the pipelines executed in parallel.
Prefer @ClassRule over @Rule so that multiple tests can share the same Flink cluster. Doing so saves a significant amount of time since the startup and shutdown of Flink clusters usually dominate the execution time of the actual tests.
If your pipeline contains custom state handling, you can test its correctness by enabling checkpointing and restarting the job within the mini cluster. For this, you need to trigger a failure by throwing an exception from (a test-only) user-defined function in your pipeline.
Back to top
`}),e.add({id:302,href:"/flink/flink-docs-master/docs/dev/datastream/experimental/",title:"Experimental Features",section:"DataStream API",content:` Experimental Features # This section describes experimental features in the DataStream API. Experimental features are still evolving and can be either unstable, incomplete, or subject to heavy change in future versions.
Reinterpreting a pre-partitioned data stream as keyed stream # We can re-interpret a pre-partitioned data stream as a keyed stream to avoid shuffling.
WARNING: The re-interpreted data stream MUST already be pre-partitioned in EXACTLY the same way Flink\u0026rsquo;s keyBy would partition the data in a shuffle w.r.t. key-group assignment. One use-case for this could be a materialized shuffle between two jobs: the first job performs a keyBy shuffle and materializes each output into a partition. A second job has sources that, for each parallel instance, reads from the corresponding partitions created by the first job. Those sources can now be re-interpreted as keyed streams, e.g. to apply windowing. Notice that this trick makes the second job embarrassingly parallel, which can be helpful for a fine-grained recovery scheme.
This re-interpretation functionality is exposed through DataStreamUtils:
static \u0026lt;T, K\u0026gt; KeyedStream\u0026lt;T, K\u0026gt; reinterpretAsKeyedStream( DataStream\u0026lt;T\u0026gt; stream, KeySelector\u0026lt;T, K\u0026gt; keySelector, TypeInformation\u0026lt;K\u0026gt; typeInfo) Given a base stream, a key selector, and type information, the method creates a keyed stream from the base stream.
Code example:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource\u0026lt;Integer\u0026gt; source = ...; DataStreamUtils.reinterpretAsKeyedStream(source, (in) -\u0026gt; in, TypeInformation.of(Integer.class)) .window(TumblingEventTimeWindows.of(Time.seconds(1))) .reduce((a, b) -\u0026gt; a + b) .addSink(new DiscardingSink\u0026lt;\u0026gt;()); env.execute(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) val source = ... new DataStreamUtils(source).reinterpretAsKeyedStream((in) =\u0026gt; in) .window(TumblingEventTimeWindows.of(Time.seconds(1))) .reduce((a, b) =\u0026gt; a + b) .addSink(new DiscardingSink[Int]) env.execute() Back to top
`}),e.add({id:303,href:"/flink/flink-docs-master/docs/dev/table/config/",title:"Configuration",section:"Table API \u0026 SQL",content:` Configuration # By default, the Table \u0026amp; SQL API is preconfigured for producing accurate results with acceptable performance.
Depending on the requirements of a table program, it might be necessary to adjust certain parameters for optimization. For example, unbounded streaming programs may need to ensure that the required state size is capped (see streaming concepts).
Overview # When instantiating a TableEnvironment, EnvironmentSettings can be used to pass the desired configuration for the current session, by passing a Configuration object to the EnvironmentSettings.
Additionally, in every table environment, the TableConfig offers options for configuring the current session.
For common or important configuration options, the TableConfig provides getters and setters methods with detailed inline documentation.
For more advanced configuration, users can directly access the underlying key-value map. The following sections list all available options that can be used to adjust Flink Table \u0026amp; SQL API programs.
Attention Because options are read at different point in time when performing operations, it is recommended to set configuration options early after instantiating a table environment.
Java // instantiate table environment Configuration configuration = new Configuration(); // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); EnvironmentSettings settings = EnvironmentSettings.newInstance() .inStreamingMode().withConfiguration(configuration).build(); TableEnvironment tEnv = TableEnvironment.create(settings); // access flink configuration after table environment instantiation TableConfig tableConfig = tEnv.getConfig(); // set low-level key-value options tableConfig.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); tableConfig.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); tableConfig.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); Scala // instantiate table environment val configuration = new Configuration; // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) val settings = EnvironmentSettings.newInstance .inStreamingMode.withConfiguration(configuration).build val tEnv: TableEnvironment = TableEnvironment.create(settings) // access flink configuration after table environment instantiation val tableConfig = tEnv.getConfig() // set low-level key-value options tableConfig.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) tableConfig.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) tableConfig.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) Python # instantiate table environment configuration = Configuration() configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) settings = EnvironmentSettings.new_instance() \\ ... .in_streaming_mode() \\ ... .with_configuration(configuration) \\ ... .build() t_env = TableEnvironment.create(settings) # access flink configuration after table environment instantiation table_config = t_env.get_config() # set low-level key-value options table_config.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) table_config.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) table_config.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) SQL CLI Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.enabled\u0026#39; = \u0026#39;true\u0026#39;; Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.allow-latency\u0026#39; = \u0026#39;5s\u0026#39;; Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.size\u0026#39; = \u0026#39;5000\u0026#39;; Note: All of the following configuration options can also be set globally in conf/flink-conf.yaml (see configuration and can be later on overridden in the application, through EnvironmentSettings, before instantiating the TableEnvironment, or through the TableConfig of the TableEnvironment. Execution Options # The following options can be used to tune the performance of the query execution.
Key Default Type Description table.exec.async-lookup.buffer-capacity
Batch Streaming 100 Integer The max number of async i/o operation that the async lookup join can trigger. table.exec.async-lookup.output-mode
Batch Streaming ORDERED Enum
Output mode for asynchronous operations which will convert to {@see AsyncDataStream.OutputMode}, ORDERED by default. If set to ALLOW_UNORDERED, will attempt to use {@see AsyncDataStream.OutputMode.UNORDERED} when it does not affect the correctness of the result, otherwise ORDERED will be still used.
Possible values:"ORDERED""ALLOW_UNORDERED" table.exec.async-lookup.timeout
Batch Streaming 3 min Duration The async timeout for the asynchronous operation to complete. table.exec.deduplicate.insert-update-after-sensitive-enabled
Streaming true Boolean Set whether the job (especially the sinks) is sensitive to INSERT messages and UPDATE_AFTER messages. If false, Flink may, sometimes (e.g. deduplication for last row), send UPDATE_AFTER instead of INSERT for the first row. If true, Flink will guarantee to send INSERT for the first row, in that case there will be additional overhead. Default is true. table.exec.deduplicate.mini-batch.compact-changes-enabled
Streaming false Boolean Set whether to compact the changes sent downstream in row-time mini-batch. If true, Flink will compact changes and send only the latest change downstream. Note that if the downstream needs the details of versioned data, this optimization cannot be applied. If false, Flink will send all changes to downstream just like when the mini-batch is not enabled. table.exec.disabled-operators
Batch (none) String Mainly for testing. A comma-separated list of operator names, each name represents a kind of disabled operator. Operators that can be disabled include "NestedLoopJoin", "ShuffleHashJoin", "BroadcastHashJoin", "SortMergeJoin", "HashAgg", "SortAgg". By default no operator is disabled. table.exec.legacy-cast-behaviour
Batch Streaming DISABLED Enum
Determines whether CAST will operate following the legacy behaviour or the new one that introduces various fixes and improvements.
Possible values:"ENABLED": CAST will operate following the legacy behaviour."DISABLED": CAST will operate following the new correct behaviour. table.exec.mini-batch.allow-latency
Streaming 0 ms Duration The maximum latency can be used for MiniBatch to buffer input records. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: If table.exec.mini-batch.enabled is set true, its value must be greater than zero. table.exec.mini-batch.enabled
Streaming false Boolean Specifies whether to enable MiniBatch optimization. MiniBatch is an optimization to buffer input records to reduce state access. This is disabled by default. To enable this, users should set this config to true. NOTE: If mini-batch is enabled, 'table.exec.mini-batch.allow-latency' and 'table.exec.mini-batch.size' must be set. table.exec.mini-batch.size
Streaming -1 Long The maximum number of input records can be buffered for MiniBatch. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: MiniBatch only works for non-windowed aggregations currently. If table.exec.mini-batch.enabled is set true, its value must be positive. table.exec.rank.topn-cache-size
Streaming 10000 Long Rank operators have a cache which caches partial state contents to reduce state access. Cache size is the number of records in each ranking task. table.exec.resource.default-parallelism
Batch Streaming -1 Integer Sets default parallelism for all operators (such as aggregate, join, filter) to run with parallel instances. This config has a higher priority than parallelism of StreamExecutionEnvironment (actually, this config overrides the parallelism of StreamExecutionEnvironment). A value of -1 indicates that no default parallelism is set, then it will fallback to use the parallelism of StreamExecutionEnvironment. table.exec.simplify-operator-name-enabled
Batch Streaming true Boolean When it is true, the optimizer will simplify the operator name with id and type of ExecNode and keep detail in description. Default value is true. table.exec.sink.keyed-shuffle
Streaming AUTO Enum
In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink's parallelism differs from upstream operator and upstream is append only. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).
By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).
Possible values:"NONE""AUTO""FORCE" table.exec.sink.not-null-enforcer
Batch Streaming ERROR Enum
Determines how Flink enforces NOT NULL column constraints when inserting null values.
Possible values:"ERROR": Throw a runtime exception when writing null values into NOT NULL column."DROP": Drop records silently if a null value would have to be inserted into a NOT NULL column. table.exec.sink.type-length-enforcer
Batch Streaming IGNORE Enum
Determines whether values for columns with CHAR(\u0026lt;length\u0026gt;)/VARCHAR(\u0026lt;length\u0026gt;)/BINARY(\u0026lt;length\u0026gt;)/VARBINARY(\u0026lt;length\u0026gt;) types will be trimmed or padded (only for CHAR(\u0026lt;length\u0026gt;)/BINARY(\u0026lt;length\u0026gt;)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.
Possible values:"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive."TRIM_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length. table.exec.sink.upsert-materialize
Streaming AUTO Enum
Because of the disorder of ChangeLog data caused by Shuffle in distributed system, the data received by Sink may not be the order of global upsert. So add upsert materialize operator before upsert sink. It receives the upstream changelog records and generate an upsert view for the downstream.
By default, the materialize operator will be added when a distributed disorder occurs on unique keys. You can also choose no materialization(NONE) or force materialization(FORCE).
Possible values:"NONE""AUTO""FORCE" table.exec.sort.async-merge-enabled
Batch true Boolean Whether to asynchronously merge sorted spill files. table.exec.sort.default-limit
Batch -1 Integer Default limit when user don't set a limit after order by. -1 indicates that this configuration is ignored. table.exec.sort.max-num-file-handles
Batch 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles per operator. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading. table.exec.source.cdc-events-duplicate
Streaming false Boolean Indicates whether the CDC (Change Data Capture) sources in the job will produce duplicate change events that requires the framework to deduplicate and get consistent result. CDC source refers to the source that produces full change events, including INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE, for example Kafka source with Debezium format. The value of this configuration is false by default.
However, it's a common case that there are duplicate change events. Because usually the CDC tools (e.g. Debezium) work in at-least-once delivery when failover happens. Thus, in the abnormal situations Debezium may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions.
Therefore, it is recommended to turn on this configuration if your CDC tool is at-least-once delivery. Enabling this configuration requires to define PRIMARY KEY on the CDC sources. The primary key will be used to deduplicate change events and generate normalized changelog stream at the cost of an additional stateful operator. table.exec.source.idle-timeout
Streaming 0 ms Duration When a source do not receive any elements for the timeout time, it will be marked as temporarily idle. This allows downstream tasks to advance their watermarks without the need to wait for watermarks from this source while it is idle. Default value is 0, which means detecting source idleness is not enabled. table.exec.spill-compression.block-size
Batch 64 kb MemorySize The memory size used to do compress when spilling data. The larger the memory, the higher the compression ratio, but more memory resource will be consumed by the job. table.exec.spill-compression.enabled
Batch true Boolean Whether to compress spilled data. Currently we only support compress spilled data for sort and hash-agg and hash-join operators. table.exec.state.ttl
Streaming 0 ms Duration Specifies a minimum time interval for how long idle state (i.e. state which was not updated), will be retained. State will never be cleared until it was idle for less than the minimum time, and will be cleared at some time after it was idle. Default is never clean-up the state. NOTE: Cleaning up state requires additional overhead for bookkeeping. Default value is 0, which means that it will never clean up state. table.exec.uid.format
Streaming "\u0026lt;id\u0026gt;_\u0026lt;transformation\u0026gt;" String Defines the format pattern for generating the UID of an ExecNode streaming transformation. The pattern can be defined globally or per-ExecNode in the compiled plan. Supported arguments are: \u0026lt;id\u0026gt; (from static counter), \u0026lt;type\u0026gt; (e.g. 'stream-exec-sink'), \u0026lt;version\u0026gt;, and \u0026lt;transformation\u0026gt; (e.g. 'constraint-validator' for a sink). In Flink 1.15.x the pattern was wrongly defined as '\u0026lt;id\u0026gt;_\u0026lt;type\u0026gt;_\u0026lt;version\u0026gt;_\u0026lt;transformation\u0026gt;' which would prevent migrations in the future. table.exec.uid.generation
Streaming PLAN_ONLY Enum
In order to remap state to operators during a restore, it is required that the pipeline's streaming transformations get a UID assigned.
The planner can generate and assign explicit UIDs. If no UIDs have been set by the planner, the UIDs will be auto-generated by lower layers that can take the complete topology into account for uniqueness of the IDs. See the DataStream API for more information.
This configuration option is for experts only and the default should be sufficient for most use cases. By default, only pipelines created from a persisted compiled plan will get UIDs assigned explicitly. Thus, these pipelines can be arbitrarily moved around within the same topology without affecting the stable UIDs.
Possible values:"PLAN_ONLY": Sets UIDs on streaming transformations if and only if the pipeline definition comes from a compiled plan. Pipelines that have been constructed in the API without a compilation step will not set an explicit UID as it might not be stable across multiple translations."ALWAYS": Always sets UIDs on streaming transformations. This strategy is for experts only! Pipelines that have been constructed in the API without a compilation step might not be able to be restored properly. The UID generation depends on previously declared pipelines (potentially across jobs if the same JVM is used). Thus, a stable environment must be ensured. Pipeline definitions that come from a compiled plan are safe to use."DISABLED": No explicit UIDs will be set. table.exec.window-agg.buffer-size-limit
Batch 100000 Integer Sets the window elements buffer size limit used in group window agg operator. Optimizer Options # The following options can be used to adjust the behavior of the query optimizer to get a better execution plan.
Key Default Type Description table.optimizer.agg-phase-strategy
Batch Streaming "AUTO" String Strategy for aggregate phase. Only AUTO, TWO_PHASE or ONE_PHASE can be set. AUTO: No special enforcer for aggregate stage. Whether to choose two stage aggregate or one stage aggregate depends on cost. TWO_PHASE: Enforce to use two stage aggregate which has localAggregate and globalAggregate. Note that if aggregate call does not support optimize into two phase, we will still use one stage aggregate. ONE_PHASE: Enforce to use one stage aggregate which only has CompleteGlobalAggregate. table.optimizer.distinct-agg.split.bucket-num
Streaming 1024 Integer Configure the number of buckets when splitting distinct aggregation. The number is used in the first level aggregation to calculate a bucket key 'hash_code(distinct_key) % BUCKET_NUM' which is used as an additional group key after splitting. table.optimizer.distinct-agg.split.enabled
Streaming false Boolean Tells the optimizer whether to split distinct aggregation (e.g. COUNT(DISTINCT col), SUM(DISTINCT col)) into two level. The first aggregation is shuffled by an additional key which is calculated using the hashcode of distinct_key and number of buckets. This optimization is very useful when there is data skew in distinct aggregation and gives the ability to scale-up the job. Default is false. table.optimizer.dynamic-filtering.enabled
Batch Streaming true Boolean When it is true, the optimizer will try to push dynamic filtering into scan table source, the irrelevant partitions or input data will be filtered to reduce scan I/O in runtime. table.optimizer.join-reorder-enabled
Batch Streaming false Boolean Enables join reorder in optimizer. Default is disabled. table.optimizer.join.broadcast-threshold
Batch 1048576 Long Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 to disable broadcasting. table.optimizer.multiple-input-enabled
Batch true Boolean When it is true, the optimizer will merge the operators with pipelined shuffling into a multiple input operator to reduce shuffling and improve performance. Default value is true. table.optimizer.non-deterministic-update.strategy
Streaming IGNORE Enum
When it is \`TRY_RESOLVE\`, the optimizer tries to resolve the correctness issue caused by 'Non-Deterministic Updates' (NDU) in a changelog pipeline. Changelog may contain kinds of message types: Insert (I), Delete (D), Update_Before (UB), Update_After (UA). There's no NDU problem in an insert only changelog pipeline. For updates, there are three main NDU problems:
1. Non-deterministic functions, include scalar, table, aggregate functions, both builtin and custom ones.
2. LookupJoin on an evolving source
3. Cdc-source carries metadata fields which are system columns, not belongs to the entity data itself.
For the first step, the optimizer automatically enables the materialization for No.2(LookupJoin) if needed, and gives the detailed error message for No.1(Non-deterministic functions) and No.3(Cdc-source with metadata) which is relatively easier to solve by changing the SQL.
Default value is \`IGNORE\`, the optimizer does no changes.
Possible values:"TRY_RESOLVE""IGNORE" table.optimizer.reuse-source-enabled
Batch Streaming true Boolean When it is true, the optimizer will try to find out duplicated table sources and reuse them. This works only when table.optimizer.reuse-sub-plan-enabled is true. table.optimizer.reuse-sub-plan-enabled
Batch Streaming true Boolean When it is true, the optimizer will try to find out duplicated sub-plans and reuse them. table.optimizer.source.aggregate-pushdown-enabled
Batch true Boolean When it is true, the optimizer will push down the local aggregates into the TableSource which implements SupportsAggregatePushDown. table.optimizer.source.predicate-pushdown-enabled
Batch Streaming true Boolean When it is true, the optimizer will push down predicates into the FilterableTableSource. Default value is true. table.optimizer.source.report-statistics-enabled
Batch Streaming true Boolean When it is true, the optimizer will collect and use the statistics from source connectors if the source extends from SupportsStatisticReport and the statistics from catalog is UNKNOWN.Default value is true. Table Options # The following options can be used to adjust the behavior of the table planner.
Key Default Type Description table.builtin-catalog-name
Batch Streaming "default_catalog" String The name of the initial catalog to be created when instantiating a TableEnvironment. table.builtin-database-name
Batch Streaming "default_database" String The name of the default database in the initial catalog to be created when instantiating TableEnvironment. table.dml-sync
Batch Streaming false Boolean Specifies if the DML job (i.e. the insert operation) is executed asynchronously or synchronously. By default, the execution is async, so you can submit multiple DML jobs at the same time. If set this option to true, the insert operation will wait for the job to finish. table.dynamic-table-options.enabled
Batch Streaming true Boolean Enable or disable the OPTIONS hint used to specify table options dynamically, if disabled, an exception would be thrown if any OPTIONS hint is specified table.generated-code.max-length
Batch Streaming 4000 Integer Specifies a threshold where generated code will be split into sub-function calls. Java has a maximum method length of 64 KB. This setting allows for finer granularity if necessary. Default value is 4000 instead of 64KB as by default JIT refuses to work on methods with more than 8K byte code. table.local-time-zone
Batch Streaming "default" String The local time zone defines current session time zone id. It is used when converting to/from \u0026lt;code\u0026gt;TIMESTAMP WITH LOCAL TIME ZONE\u0026lt;/code\u0026gt;. Internally, timestamps with local time zone are always represented in the UTC time zone. However, when converting to data types that don't include a time zone (e.g. TIMESTAMP, TIME, or simply STRING), the session time zone is used during conversion. The input of option is either a full name such as "America/Los_Angeles", or a custom timezone id such as "GMT-08:00". table.plan.compile.catalog-objects
Batch Streaming ALL Enum
Strategy how to persist catalog objects such as tables, functions, or data types into a plan during compilation.
It influences the need for catalog metadata to be present during a restore operation and affects the plan size.
This configuration option does not affect anonymous/inline or temporary objects. Anonymous/inline objects will be persisted entirely (including schema and options) if possible or fail the compilation otherwise. Temporary objects will be persisted only by their identifier and the object needs to be present in the session context during a restore.
Possible values:"ALL": All metadata about catalog tables, functions, or data types will be persisted into the plan during compilation. For catalog tables, this includes the table's identifier, schema, and options. For catalog functions, this includes the function's identifier and class. For catalog data types, this includes the identifier and entire type structure. With this strategy, the catalog's metadata doesn't have to be available anymore during a restore operation."SCHEMA": In addition to an identifier, schema information about catalog tables, functions, or data types will be persisted into the plan during compilation. A schema allows for detecting incompatible changes in the catalog during a plan restore operation. However, all other metadata will still be retrieved from the catalog."IDENTIFIER": Only the identifier of catalog tables, functions, or data types will be persisted into the plan during compilation. All metadata will be retrieved from the catalog during a restore operation. With this strategy, plans become less verbose. table.plan.force-recompile
Streaming false Boolean When false COMPILE PLAN statement will fail if the output plan file is already existing, unless the clause IF NOT EXISTS is used. When true COMPILE PLAN will overwrite the existing output plan file. We strongly suggest to enable this flag only for debugging purpose. table.plan.restore.catalog-objects
Batch Streaming ALL Enum
Strategy how to restore catalog objects such as tables, functions, or data types using a given plan and performing catalog lookups if necessary. It influences the need for catalog metadata to bepresent and enables partial enrichment of plan information.
Possible values:"ALL": Reads all metadata about catalog tables, functions, or data types that has been persisted in the plan. The strategy performs a catalog lookup by identifier to fill in missing information or enrich mutable options. If the original object is not available in the catalog anymore, pipelines can still be restored if all information necessary is contained in the plan."ALL_ENFORCED": Requires that all metadata about catalog tables, functions, or data types has been persisted in the plan. The strategy will neither perform a catalog lookup by identifier nor enrich mutable options with catalog information. A restore will fail if not all information necessary is contained in the plan."IDENTIFIER": Uses only the identifier of catalog tables, functions, or data types and always performs a catalog lookup. A restore will fail if the original object is not available in the catalog anymore. Additional metadata that might be contained in the plan will be ignored. table.resources.download-dir
Batch Streaming System.getProperty("java.io.tmpdir") String Local directory that is used by planner for storing downloaded resources. table.sql-dialect
Batch Streaming "default" String The SQL dialect defines how to parse a SQL query. A different SQL dialect may support different SQL grammar. Currently supported dialects are: default and hive SQL Client Options # The following options can be used to adjust the behavior of the sql client.
Key Default Type Description sql-client.display.max-column-width
Streaming 30 Integer When printing the query results, this parameter determines the number of characters shown on screen before truncating.This only applies to columns with variable-length types (e.g. STRING) in streaming mode.Fixed-length types and all types in batch mode are printed using a deterministic column width sql-client.execution.max-table-result.rows
Batch Streaming 1000000 Integer The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style. sql-client.execution.result-mode
Batch Streaming TABLE Enum
Determines how the query result should be displayed.
Possible values:"TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation."CHANGELOG": Visualizes the result stream that is produced by a continuous query."TABLEAU": Display results in the screen directly in a tableau format. sql-client.verbose
Batch Streaming false Boolean Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause. `}),e.add({id:304,href:"/flink/flink-docs-master/docs/dev/python/table/metrics/",title:"Metrics",section:"Table API",content:` Metrics # PyFlink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from a Python user-defined function by calling function_context.get_metric_group() in the open method. The get_metric_group() method returns a MetricGroup object on which you can create and register new metrics.
Metric types # PyFlink supports Counters, Gauges, Distribution and Meters.
Counter # A Counter is used to count something. The current value can be in- or decremented using inc()/inc(n: int) or dec()/dec(n: int). You can create and register a Counter by calling counter(name: str) on a MetricGroup.
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.counter = None def open(self, function_context): self.counter = function_context.get_metric_group().counter(\u0026#34;my_counter\u0026#34;) def eval(self, i): self.counter.inc(i) return i Gauge # A Gauge provides a value on demand. You can register a gauge by calling gauge(name: str, obj: Callable[[], int]) on a MetricGroup. The Callable object will be used to report the values. Gauge metrics are restricted to integer-only values.
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.length = 0 def open(self, function_context): function_context.get_metric_group().gauge(\u0026#34;my_gauge\u0026#34;, lambda : self.length) def eval(self, i): self.length = i return i - 1 Distribution # A metric that reports information(sum, count, min, max and mean) about the distribution of reported values. The value can be updated using update(n: int). You can register a distribution by calling distribution(name: str) on a MetricGroup. Distribution metrics are restricted to integer-only distributions.
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.distribution = None def open(self, function_context): self.distribution = function_context.get_metric_group().distribution(\u0026#34;my_distribution\u0026#34;) def eval(self, i): self.distribution.update(i) return i - 1 Meter # A Meter measures an average throughput. An occurrence of an event can be registered with the mark_event() method. The occurrence of multiple events at the same time can be registered with mark_event(n: int) method. You can register a meter by calling meter(self, name: str, time_span_in_seconds: int = 60) on a MetricGroup. The default value of time_span_in_seconds is 60.
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.meter = None def open(self, function_context): # an average rate of events per second over 120s, default is 60s. self.meter = function_context.get_metric_group().meter(\u0026#34;my_meter\u0026#34;, time_span_in_seconds=120) def eval(self, i): self.meter.mark_event(i) return i - 1 Scope # You can refer to the Java metric document for more details on Scope definition.
User Scope # You can define a user scope by calling MetricGroup.add_group(key: str, value: str = None). If value is not None, creates a new key-value MetricGroup pair. The key group is added to this group\u0026rsquo;s sub-groups, while the value group is added to the key group\u0026rsquo;s sub-groups. In this case, the value group will be returned, and a user variable will be defined.
Python function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) System Scope # You can refer to the Java metric document for more details on System Scope.
List of all Variables # You can refer to the Java metric document for more details on List of all Variables.
User Variables # You can define a user variable by calling MetricGroup.addGroup(key: str, value: str = None) and specifying the value parameter.
Important: User variables cannot be used in scope formats.
Python function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) Common part between PyFlink and Flink # You can refer to the Java metric document for more details on the following sections:
Reporter. System metrics. Latency tracking. REST API integration. Dashboard integration. Back to top
`}),e.add({id:305,href:"/flink/flink-docs-master/docs/dev/table/tuning/",title:"Performance Tuning",section:"Table API \u0026 SQL",content:` Performance Tuning # SQL is the most widely used language for data analytics. Flink\u0026rsquo;s Table API and SQL enables users to define efficient stream analytics applications in less time and effort. Moreover, Flink Table API and SQL is effectively optimized, it integrates a lot of query optimizations and tuned operator implementations. But not all of the optimizations are enabled by default, so for some workloads, it is possible to improve performance by turning on some options.
In this page, we will introduce some useful optimization options and the internals of streaming aggregation which will bring great improvement in some cases.
The streaming aggregation optimizations mentioned in this page are all supported for Group Aggregations and Window TVF Aggregations now. MiniBatch Aggregation # By default, group aggregation operators process input records one by one, i.e., (1) read accumulator from state, (2) accumulate/retract record to the accumulator, (3) write accumulator back to state, (4) the next record will do the process again from (1). This processing pattern may increase the overhead of StateBackend (especially for RocksDB StateBackend). Besides, data skew, which is very common in production, will worsen the problem and make it easy for the jobs to be under backpressure situations.
The core idea of mini-batch aggregation is caching a bundle of inputs in a buffer inside of the aggregation operator. When the bundle of inputs is triggered to process, only one operation per key to access state is needed. This can significantly reduce the state overhead and get a better throughput. However, this may increase some latency because it buffers some records instead of processing them in an instant. This is a trade-off between throughput and latency.
The following figure explains how the mini-batch aggregation reduces state operations.
MiniBatch optimization is disabled by default for group aggregation. In order to enable this optimization, you should set options table.exec.mini-batch.enabled, table.exec.mini-batch.allow-latency and table.exec.mini-batch.size. Please see configuration page for more details.
MiniBatch optimization is always enabled for Window TVF Aggregation, regardless of the above configuration. Window TVF aggregation buffer records in managed memory instead of JVM Heap, so there is no risk of overloading GC or OOM issues. The following examples show how to enable these options.
Java // instantiate table environment TableEnvironment tEnv = ...; // access flink configuration TableConfig configuration = tEnv.getConfig(); // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); // enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); // use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); // the maximum number of records can be buffered by each aggregate operator task Scala // instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig() // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) // use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) // the maximum number of records can be buffered by each aggregate operator task Python # instantiate table environment t_env = ... # access flink configuration configuration = t_env.get_config() # set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) # enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) # use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) # the maximum number of records can be buffered by each aggregate operator task Local-Global Aggregation # Local-Global is proposed to solve data skew problem by dividing a group aggregation into two stages, that is doing local aggregation in upstream firstly, and followed by global aggregation in downstream, which is similar to Combine + Reduce pattern in MapReduce. For example, considering the following SQL:
SELECT color, sum(id) FROM T GROUP BY color It is possible that the records in the data stream are skewed, thus some instances of aggregation operator have to process much more records than others, which leads to hotspot. The local aggregation can help to accumulate a certain amount of inputs which have the same key into a single accumulator. The global aggregation will only receive the reduced accumulators instead of large number of raw inputs. This can significantly reduce the network shuffle and the cost of state access. The number of inputs accumulated by local aggregation every time is based on mini-batch interval. It means local-global aggregation depends on mini-batch optimization is enabled.
The following figure shows how the local-global aggregation improve performance.
The following examples show how to enable the local-global aggregation.
Java // instantiate table environment TableEnvironment tEnv = ...; // access flink configuration TableConfig configuration = tEnv.getConfig(); // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); // local-global aggregation depends on mini-batch is enabled configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); configuration.set(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;); // enable two-phase, i.e. local-global aggregation Scala // instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig() // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // local-global aggregation depends on mini-batch is enabled configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) configuration.set(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;) // enable two-phase, i.e. local-global aggregation Python # instantiate table environment t_env = ... # access flink configuration configuration = t_env.get_config() # set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) # local-global aggregation depends on mini-batch is enabled configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) configuration.set(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;) # enable two-phase, i.e. local-global aggregation Split Distinct Aggregation # Local-Global optimization is effective to eliminate data skew for general aggregation, such as SUM, COUNT, MAX, MIN, AVG. But its performance is not satisfactory when dealing with distinct aggregation.
For example, if we want to analyse how many unique users logined today. We may have the following query:
SELECT day, COUNT(DISTINCT user_id) FROM T GROUP BY day COUNT DISTINCT is not good at reducing records if the value of distinct key (i.e. user_id) is sparse. Even if local-global optimization is enabled, it doesn\u0026rsquo;t help much. Because the accumulator still contain almost all the raw records, and the global aggregation will be the bottleneck (most of the heavy accumulators are processed by one task, i.e. on the same day).
The idea of this optimization is splitting distinct aggregation (e.g. COUNT(DISTINCT col)) into two levels. The first aggregation is shuffled by group key and an additional bucket key. The bucket key is calculated using HASH_CODE(distinct_key) % BUCKET_NUM. BUCKET_NUM is 1024 by default, and can be configured by table.optimizer.distinct-agg.split.bucket-num option. The second aggregation is shuffled by the original group key, and use SUM to aggregate COUNT DISTINCT values from different buckets. Because the same distinct key will only be calculated in the same bucket, so the transformation is equivalent. The bucket key plays the role of an additional group key to share the burden of hotspot in group key. The bucket key makes the job to be scalability to solve data-skew/hotspot in distinct aggregations.
After split distinct aggregate, the above query will be rewritten into the following query automatically:
SELECT day, SUM(cnt) FROM ( SELECT day, COUNT(DISTINCT user_id) as cnt FROM T GROUP BY day, MOD(HASH_CODE(user_id), 1024) ) GROUP BY day The following figure shows how the split distinct aggregation improve performance (assuming color represents days, and letter represents user_id).
NOTE: Above is the simplest example which can benefit from this optimization. Besides that, Flink supports to split more complex aggregation queries, for example, more than one distinct aggregates with different distinct key (e.g. COUNT(DISTINCT a), SUM(DISTINCT b)), works with other non-distinct aggregates (e.g. SUM, MAX, MIN, COUNT).
Currently, the split optimization doesn\u0026rsquo;t support aggregations which contains user defined AggregateFunction. The following examples show how to enable the split distinct aggregation optimization.
Java // instantiate table environment TableEnvironment tEnv = ...; tEnv.getConfig() .set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;); // enable distinct agg split Scala // instantiate table environment val tEnv: TableEnvironment = ... tEnv.getConfig .set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable distinct agg split Python # instantiate table environment t_env = ... t_env.get_config().set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;) # enable distinct agg split Use FILTER Modifier on Distinct Aggregates # In some cases, user may need to calculate the number of UV (unique visitor) from different dimensions, e.g. UV from Android, UV from iPhone, UV from Web and the total UV. Many users will choose CASE WHEN to support this, for example:
SELECT day, COUNT(DISTINCT user_id) AS total_uv, COUNT(DISTINCT CASE WHEN flag IN (\u0026#39;android\u0026#39;, \u0026#39;iphone\u0026#39;) THEN user_id ELSE NULL END) AS app_uv, COUNT(DISTINCT CASE WHEN flag IN (\u0026#39;wap\u0026#39;, \u0026#39;other\u0026#39;) THEN user_id ELSE NULL END) AS web_uv FROM T GROUP BY day However, it is recommended to use FILTER syntax instead of CASE WHEN in this case. Because FILTER is more compliant with the SQL standard and will get much more performance improvement. FILTER is a modifier used on an aggregate function to limit the values used in an aggregation. Replace the above example with FILTER modifier as following:
SELECT day, COUNT(DISTINCT user_id) AS total_uv, COUNT(DISTINCT user_id) FILTER (WHERE flag IN (\u0026#39;android\u0026#39;, \u0026#39;iphone\u0026#39;)) AS app_uv, COUNT(DISTINCT user_id) FILTER (WHERE flag IN (\u0026#39;wap\u0026#39;, \u0026#39;other\u0026#39;)) AS web_uv FROM T GROUP BY day Flink SQL optimizer can recognize the different filter arguments on the same distinct key. For example, in the above example, all the three COUNT DISTINCT are on user_id column. Then Flink can use just one shared state instance instead of three state instances to reduce state access and state size. In some workloads, this can get significant performance improvements.
Back to top
`}),e.add({id:306,href:"/flink/flink-docs-master/docs/dev/python/python_config/",title:"Configuration",section:"Python API",content:` Configuration # Depending on the requirements of a Python API program, it might be necessary to adjust certain parameters for optimization.
For Python DataStream API program, the config options could be set as following:
from pyflink.common import Configuration from pyflink.datastream import StreamExecutionEnvironment config = Configuration() config.set_integer(\u0026#34;python.fn-execution.bundle.size\u0026#34;, 1000) env = StreamExecutionEnvironment.get_execution_environment(config) For Python Table API program, all the config options available for Java/Scala Table API program could also be used in the Python Table API program. You could refer to the Table API Configuration for more details on all the available config options for Table API programs. The config options could be set as following in a Table API program:
from pyflink.table import TableEnvironment, EnvironmentSettings env_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_settings) t_env.get_config().set(\u0026#34;python.fn-execution.bundle.size\u0026#34;, \u0026#34;1000\u0026#34;) The config options could also be set when creating EnvironmentSettings:
from pyflink.common import Configuration from pyflink.table import TableEnvironment, EnvironmentSettings # create a streaming TableEnvironment config = Configuration() config.set_string(\u0026#34;python.fn-execution.bundle.size\u0026#34;, \u0026#34;1000\u0026#34;) env_settings = EnvironmentSettings \\ .new_instance() \\ .in_streaming_mode() \\ .with_configuration(config) \\ .build() table_env = TableEnvironment.create(env_settings) # or directly pass config into create method table_env = TableEnvironment.create(config) Python Options # Key Default Type Description python.archives (none) String Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory is specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF. The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). The option is equivalent to the command line option "-pyarch". python.client.executable "python" String The path of the Python interpreter used to launch the Python process when submitting the Python jobs via "flink run" or compiling the Java/Scala jobs containing Python UDFs. Equivalent to the command line option "-pyclientexec" or the environment variable PYFLINK_CLIENT_EXECUTABLE. The priority is as following: 1. the configuration 'python.client.executable' defined in the source code(Only used in Flink Java SQL/Table API job call Python UDF);
2. the command line option "-pyclientexec";
3. the configuration 'python.client.executable' defined in flink-conf.yaml
4. the environment variable PYFLINK_CLIENT_EXECUTABLE; python.executable "python" String Specify the path of the python interpreter used to execute the python UDF worker. The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u0026gt;= 20.3) and SetupTools (version \u0026gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. The option is equivalent to the command line option "-pyexec". python.execution-mode "process" String Specify the python runtime execution mode. The optional values are \`process\` and \`thread\`. The \`process\` mode means that the Python user-defined functions will be executed in separate Python process. The \`thread\` mode means that the Python user-defined functions will be executed in the same process of the Java operator. Note that currently it still doesn't support to execute Python user-defined functions in \`thread\` mode in all places. It will fall back to \`process\` mode in these cases. python.files (none) String Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (',') could be used as the separator to specify multiple files. The option is equivalent to the command line option "-pyfs". python.fn-execution.arrow.batch.size 10000 Integer The maximum number of elements to include in an arrow batch for Python user-defined function execution. The arrow batch size should not exceed the bundle size. Otherwise, the bundle size will be used as the arrow batch size. python.fn-execution.bundle.size 100000 Integer The maximum number of elements to include in a bundle for Python user-defined function execution. The elements are processed asynchronously. One bundle of elements are processed before processing the next bundle of elements. A larger value can improve the throughput, but at the cost of more memory usage and higher latency. python.fn-execution.bundle.time 1000 Long Sets the waiting timeout(in milliseconds) before processing a bundle for Python user-defined function execution. The timeout defines how long the elements of a bundle will be buffered before being processed. Lower timeouts lead to lower tail latencies, but may affect throughput. python.fn-execution.memory.managed true Boolean If set, the Python worker will configure itself to use the managed memory budget of the task slot. Otherwise, it will use the Off-Heap Memory of the task slot. In this case, users should set the Task Off-Heap Memory using the configuration key taskmanager.memory.task.off-heap.size. python.map-state.iterate-response-batch-size 1000 Integer The maximum number of the MapState keys/entries sent to Python UDF worker in each batch when iterating a Python MapState. Note that this is an experimental flag and might not be available in future releases. python.map-state.read-cache-size 1000 Integer The maximum number of cached entries for a single Python MapState. Note that this is an experimental flag and might not be available in future releases. python.map-state.write-cache-size 1000 Integer The maximum number of cached write requests for a single Python MapState. The write requests will be flushed to the state backend (managed in the Java operator) when the number of cached write requests exceed this limit. Note that this is an experimental flag and might not be available in future releases. python.metric.enabled true Boolean When it is false, metric for Python will be disabled. You can disable the metric to achieve better performance at some circumstance. python.operator-chaining.enabled true Boolean Python operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization. python.profile.enabled false Boolean Specifies whether to enable Python worker profiling. The profile result will be displayed in the log file of the TaskManager periodically. The interval between each profiling is determined by the config options python.fn-execution.bundle.size and python.fn-execution.bundle.time. python.requirements (none) String Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists. The option is equivalent to the command line option "-pyreq". python.state.cache-size 1000 Integer The maximum number of states cached in a Python UDF worker. Note that this is an experimental flag and might not be available in future releases. `}),e.add({id:307,href:"/flink/flink-docs-master/docs/dev/python/debugging/",title:"Debugging",section:"Python API",content:` Debugging # This page describes how to debug in PyFlink.
Logging Infos # Client Side Logging # You can log contextual and debug information via print or standard Python logging modules in PyFlink jobs in places outside Python UDFs. The logging messages will be printed in the log files of the client during job submission.
from pyflink.table import EnvironmentSettings, TableEnvironment # create a TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) # use logging modules import logging logging.warning(table.get_schema()) # use print function print(table.get_schema()) Note: The default logging level at client side is WARNING and so only messages with logging level WARNING or above will appear in the log files of the client.
Server Side Logging # You can log contextual and debug information via print or standard Python logging modules in Python UDFs. The logging messages will be printed in the log files of the TaskManagers during job execution.
from pyflink.table import DataTypes from pyflink.table.udf import udf import logging @udf(result_type=DataTypes.BIGINT()) def add(i, j): # use logging modules logging.info(\u0026#34;debug\u0026#34;) # use print function print(\u0026#39;debug\u0026#39;) return i + j Note: The default logging level at server side is INFO and so only messages with logging level INFO or above will appear in the log files of the TaskManagers.
Accessing Logs # If environment variable FLINK_HOME is set, logs will be written in the log directory under FLINK_HOME. Otherwise, logs will be placed in the directory of the PyFlink module. You can execute the following command to find the log directory of the PyFlink module:
\$ python -c \u0026#34;import pyflink;import os;print(os.path.dirname(os.path.abspath(pyflink.__file__))+\u0026#39;/log\u0026#39;)\u0026#34; Debugging Python UDFs # Local Debug # You can debug your python functions directly in IDEs such as PyCharm.
Remote Debug # You can make use of the pydevd_pycharm tool of PyCharm to debug Python UDFs.
Create a Python Remote Debug in PyCharm
run -\u0026gt; Python Remote Debug -\u0026gt; + -\u0026gt; choose a port (e.g. 6789)
Install the pydevd-pycharm tool
\$ pip install pydevd-pycharm Add the following command in your Python UDF
import pydevd_pycharm pydevd_pycharm.settrace(\u0026#39;localhost\u0026#39;, port=6789, stdoutToServer=True, stderrToServer=True) Start the previously created Python Remote Debug Server
Run your Python Code
Profiling Python UDFs # You can enable the profile to analyze performance bottlenecks.
t_env.get_config().set(\u0026#34;python.profile.enabled\u0026#34;, \u0026#34;true\u0026#34;) Then you can see the profile result in logs
`}),e.add({id:308,href:"/flink/flink-docs-master/docs/dev/python/table/python_table_api_connectors/",title:"Connectors",section:"Table API",content:` Connectors # This page describes how to use connectors in PyFlink and highlights the details to be aware of when using Flink connectors in Python programs.
Note For general connector information and common configuration, please refer to the corresponding Java/Scala documentation.
Download connector and format jars # Since Flink is a Java/Scala-based project, for both connectors and formats, implementations are available as jars that need to be specified as job dependencies.
table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/json.jar\u0026#34;) How to use connectors # In PyFlink\u0026rsquo;s Table API, DDL is the recommended way to define sources and sinks, executed via the execute_sql() method on the TableEnvironment. This makes the table available for use by the application.
source_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE source_table( a VARCHAR, b INT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;source_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;test_3\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; sink_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table( a VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;sink_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) t_env.execute_sql(sink_ddl) t_env.sql_query(\u0026#34;SELECT a FROM source_table\u0026#34;) \\ .execute_insert(\u0026#34;sink_table\u0026#34;).wait() Below is a complete example of how to use a Kafka source/sink and the JSON format in PyFlink.
from pyflink.table import TableEnvironment, EnvironmentSettings def log_processing(): env_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_settings) # specify connector and format jars t_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/json.jar\u0026#34;) source_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE source_table( a VARCHAR, b INT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;source_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;test_3\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; sink_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table( a VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;sink_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) t_env.execute_sql(sink_ddl) t_env.sql_query(\u0026#34;SELECT a FROM source_table\u0026#34;) \\ .execute_insert(\u0026#34;sink_table\u0026#34;).wait() if __name__ == \u0026#39;__main__\u0026#39;: log_processing() Predefined Sources and Sinks # Some data sources and sinks are built into Flink and are available out-of-the-box. These predefined data sources include reading from Pandas DataFrame, or ingesting data from collections. The predefined data sinks support writing to Pandas DataFrame.
from/to Pandas # PyFlink Tables support conversion to and from Pandas DataFrame.
from pyflink.table.expressions import col import pandas as pd import numpy as np # Create a PyFlink Table pdf = pd.DataFrame(np.random.rand(1000, 2)) table = t_env.from_pandas(pdf, [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).filter(col(\u0026#39;a\u0026#39;) \u0026gt; 0.5) # Convert the PyFlink Table to a Pandas DataFrame pdf = table.to_pandas() from_elements() # from_elements() is used to create a table from a collection of elements. The element types must be acceptable atomic types or acceptable composite types.
from pyflink.table import DataTypes table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) # use the second parameter to specify custom field names table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) # use the second parameter to specify a custom table schema table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.STRING())])) The above query returns a Table like:
+----+-------+ | a | b | +====+=======+ | 1 | Hi | +----+-------+ | 2 | Hello | +----+-------+ User-defined sources \u0026amp; sinks # In some cases, you may want to define custom sources and sinks. Currently, sources and sinks must be implemented in Java/Scala, but you can define a TableFactory to support their use via DDL. More details can be found in the Java/Scala documentation.
`}),e.add({id:309,href:"/flink/flink-docs-master/docs/dev/python/environment_variables/",title:"Environment Variables",section:"Python API",content:` Environment Variables # These environment variables will affect the behavior of PyFlink:
Environment Variable Description FLINK_HOME PyFlink job will be compiled before submitting and it requires Flink's distribution to compile the job. PyFlink's installation package already contains Flink's distribution and it's used by default. This environment allows you to specify a custom Flink's distribution. PYFLINK_CLIENT_EXECUTABLE The path of the Python interpreter used to launch the Python process when submitting the Python jobs via "flink run" or compiling the Java/Scala jobs containing Python UDFs. Equivalent to the configuration option 'python.client.executable'. The priority is as following: The configuration 'python.client.executable' defined in the source code; The environment variable PYFLINK_CLIENT_EXECUTABLE; The configuration 'python.client.executable' defined in flink-conf.yaml If none of above is set, the default Python interpreter 'python' will be used. `}),e.add({id:310,href:"/flink/flink-docs-master/docs/dev/table/sourcessinks/",title:"User-defined Sources \u0026 Sinks",section:"Table API \u0026 SQL",content:` User-defined Sources \u0026amp; Sinks # Dynamic tables are the core concept of Flink\u0026rsquo;s Table \u0026amp; SQL API for processing both bounded and unbounded data in a unified fashion.
Because dynamic tables are only a logical concept, Flink does not own the data itself. Instead, the content of a dynamic table is stored in external systems (such as databases, key-value stores, message queues) or files.
Dynamic sources and dynamic sinks can be used to read and write data from and to an external system. In the documentation, sources and sinks are often summarized under the term connector.
Flink provides pre-defined connectors for Kafka, Hive, and different file systems. See the connector section for more information about built-in table sources and sinks.
This page focuses on how to develop a custom, user-defined connector.
Overview # In many cases, implementers don\u0026rsquo;t need to create a new connector from scratch but would like to slightly modify existing connectors or hook into the existing stack. In other cases, implementers would like to create specialized connectors.
This section helps for both kinds of use cases. It explains the general architecture of table connectors from pure declaration in the API to runtime code that will be executed on the cluster.
The filled arrows show how objects are transformed to other objects from one stage to the next stage during the translation process.
Metadata # Both Table API and SQL are declarative APIs. This includes the declaration of tables. Thus, executing a CREATE TABLE statement results in updated metadata in the target catalog.
For most catalog implementations, physical data in the external system is not modified for such an operation. Connector-specific dependencies don\u0026rsquo;t have to be present in the classpath yet. The options declared in the WITH clause are neither validated nor otherwise interpreted.
The metadata for dynamic tables (created via DDL or provided by the catalog) is represented as instances of CatalogTable. A table name will be resolved into a CatalogTable internally when necessary.
Planning # When it comes to planning and optimization of the table program, a CatalogTable needs to be resolved into a DynamicTableSource (for reading in a SELECT query) and DynamicTableSink (for writing in an INSERT INTO statement).
DynamicTableSourceFactory and DynamicTableSinkFactory provide connector-specific logic for translating the metadata of a CatalogTable into instances of DynamicTableSource and DynamicTableSink. In most of the cases, a factory\u0026rsquo;s purpose is to validate options (such as 'port' = '5022' in the example), configure encoding/decoding formats (if required), and create a parameterized instance of the table connector.
By default, instances of DynamicTableSourceFactory and DynamicTableSinkFactory are discovered using Java\u0026rsquo;s Service Provider Interfaces (SPI). The connector option (such as 'connector' = 'custom' in the example) must correspond to a valid factory identifier.
Although it might not be apparent in the class naming, DynamicTableSource and DynamicTableSink can also be seen as stateful factories that eventually produce concrete runtime implementation for reading/writing the actual data.
The planner uses the source and sink instances to perform connector-specific bidirectional communication until an optimal logical plan could be found. Depending on the optionally declared ability interfaces (e.g. SupportsProjectionPushDown or SupportsOverwrite), the planner might apply changes to an instance and thus mutate the produced runtime implementation.
Runtime # Once the logical planning is complete, the planner will obtain the runtime implementation from the table connector. Runtime logic is implemented in Flink\u0026rsquo;s core connector interfaces such as InputFormat or SourceFunction.
Those interfaces are grouped by another level of abstraction as subclasses of ScanRuntimeProvider, LookupRuntimeProvider, and SinkRuntimeProvider.
For example, both OutputFormatProvider (providing org.apache.flink.api.common.io.OutputFormat) and SinkFunctionProvider (providing org.apache.flink.streaming.api.functions.sink.SinkFunction) are concrete instances of SinkRuntimeProvider that the planner can handle.
Back to top
Project Configuration # If you want to implement a custom connector or a custom format, the following dependency is usually sufficient:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-common\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gtprovided\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. runtime "org.apache.flink:flink-table-common:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. If you want to develop a connector that needs to bridge with DataStream APIs (i.e. if you want to adapt a DataStream connector to the Table API), you need to add this dependency:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-api-java-bridge\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gtprovided\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. runtime "org.apache.flink:flink-table-api-java-bridge:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. When developing the connector/format, we suggest shipping both a thin JAR and an uber JAR, so users can easily load the uber JAR in the SQL client or in the Flink distribution and start using it. The uber JAR should include all the third-party dependencies of the connector, excluding the table dependencies listed above.
You should not depend on flink-table-planner_2.12 in production code. With the new module flink-table-planner-loader introduced in Flink 1.15, the application\u0026rsquo;s classpath will not have direct access to org.apache.flink.table.planner classes anymore. If you need a feature available only internally within the org.apache.flink.table.planner package and subpackages, please open an issue. To learn more, check out Anatomy of Table Dependencies. Extension Points # This section explains the available interfaces for extending Flink\u0026rsquo;s table connectors.
Dynamic Table Factories # Dynamic table factories are used to configure a dynamic table connector for an external storage system from catalog and session information.
org.apache.flink.table.factories.DynamicTableSourceFactory can be implemented to construct a DynamicTableSource.
org.apache.flink.table.factories.DynamicTableSinkFactory can be implemented to construct a DynamicTableSink.
By default, the factory is discovered using the value of the connector option as the factory identifier and Java\u0026rsquo;s Service Provider Interface.
In JAR files, references to new implementations can be added to the service file:
META-INF/services/org.apache.flink.table.factories.Factory
The framework will check for a single matching factory that is uniquely identified by factory identifier and requested base class (e.g. DynamicTableSourceFactory).
The factory discovery process can be bypassed by the catalog implementation if necessary. For this, a catalog needs to return an instance that implements the requested base class in org.apache.flink.table.catalog.Catalog#getFactory.
Dynamic Table Source # By definition, a dynamic table can change over time.
When reading a dynamic table, the content can either be considered as:
A changelog (finite or infinite) for which all changes are consumed continuously until the changelog is exhausted. This is represented by the ScanTableSource interface. A continuously changing or very large external table whose content is usually never read entirely but queried for individual values when necessary. This is represented by the LookupTableSource interface. A class can implement both of these interfaces at the same time. The planner decides about their usage depending on the specified query.
Scan Table Source # A ScanTableSource scans all rows from an external storage system during runtime.
The scanned rows don\u0026rsquo;t have to contain only insertions but can also contain updates and deletions. Thus, the table source can be used to read a (finite or infinite) changelog. The returned changelog mode indicates the set of changes that the planner can expect during runtime.
For regular batch scenarios, the source can emit a bounded stream of insert-only rows.
For regular streaming scenarios, the source can emit an unbounded stream of insert-only rows.
For change data capture (CDC) scenarios, the source can emit bounded or unbounded streams with insert, update, and delete rows.
A table source can implement further ability interfaces such as SupportsProjectionPushDown that might mutate an instance during planning. All abilities can be found in the org.apache.flink.table.connector.source.abilities package and are listed in the source abilities table.
The runtime implementation of a ScanTableSource must produce internal data structures. Thus, records must be emitted as org.apache.flink.table.data.RowData. The framework provides runtime converters such that a source can still work on common data structures and perform a conversion at the end.
Lookup Table Source # A LookupTableSource looks up rows of an external storage system by one or more keys during runtime.
Compared to ScanTableSource, the source does not have to read the entire table and can lazily fetch individual values from a (possibly continuously changing) external table when necessary.
Compared to ScanTableSource, a LookupTableSource does only support emitting insert-only changes currently.
Further abilities are not supported. See the documentation of org.apache.flink.table.connector.source.LookupTableSource for more information.
The runtime implementation of a LookupTableSource is a TableFunction or AsyncTableFunction. The function will be called with values for the given lookup keys during runtime.
Source Abilities # Interface Description SupportsFilterPushDown Enables to push down the filter into the DynamicTableSource. For efficiency, a source can push filters further down in order to be close to the actual data generation. SupportsLimitPushDown Enables to push down a limit (the expected maximum number of produced records) into a DynamicTableSource. SupportsPartitionPushDown Enables to pass available partitions to the planner and push down partitions into a DynamicTableSource. During the runtime, the source will only read data from the passed partition list for efficiency. SupportsProjectionPushDown Enables to push down a (possibly nested) projection into a DynamicTableSource. For efficiency, a source can push a projection further down in order to be close to the actual data generation. If the source also implements SupportsReadingMetadata, the source will also read the required metadata only. SupportsReadingMetadata Enables to read metadata columns from a DynamicTableSource. The source is responsible to add the required metadata at the end of the produced rows. This includes potentially forwarding metadata column from contained formats. SupportsWatermarkPushDown Enables to push down a watermark strategy into a DynamicTableSource. The watermark strategy is a builder/factory for timestamp extraction and watermark generation. During the runtime, the watermark generator is located inside the source and is able to generate per-partition watermarks. SupportsSourceWatermark Enables to fully rely on the watermark strategy provided by the ScanTableSource itself. Thus, a CREATE TABLE DDL is able to use SOURCE_WATERMARK() which is a built-in marker function that will be detected by the planner and translated into a call to this interface if available. Attention The interfaces above are currently only available for ScanTableSource, not for LookupTableSource.
Dynamic Table Sink # By definition, a dynamic table can change over time.
When writing a dynamic table, the content can always be considered as a changelog (finite or infinite) for which all changes are written out continuously until the changelog is exhausted. The returned changelog mode indicates the set of changes that the sink accepts during runtime.
For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded streams.
For regular streaming scenarios, the sink can solely accept insert-only rows and can write out unbounded streams.
For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams with insert, update, and delete rows.
A table sink can implement further ability interfaces such as SupportsOverwrite that might mutate an instance during planning. All abilities can be found in the org.apache.flink.table.connector.sink.abilities package and are listed in the sink abilities table.
The runtime implementation of a DynamicTableSink must consume internal data structures. Thus, records must be accepted as org.apache.flink.table.data.RowData. The framework provides runtime converters such that a sink can still work on common data structures and perform a conversion at the beginning.
Sink Abilities # Interface Description SupportsOverwrite Enables to overwrite existing data in a DynamicTableSink. By default, if this interface is not implemented, existing tables or partitions cannot be overwritten using e.g. the SQL INSERT OVERWRITE clause. SupportsPartitioning Enables to write partitioned data in a DynamicTableSink. SupportsWritingMetadata Enables to write metadata columns into a DynamicTableSource. A table sink is responsible for accepting requested metadata columns at the end of consumed rows and persist them. This includes potentially forwarding metadata columns to contained formats. Encoding / Decoding Formats # Some table connectors accept different formats that encode and decode keys and/or values.
Formats work similar to the pattern DynamicTableSourceFactory -\u0026gt; DynamicTableSource -\u0026gt; ScanRuntimeProvider, where the factory is responsible for translating options and the source is responsible for creating runtime logic.
Because formats might be located in different modules, they are discovered using Java\u0026rsquo;s Service Provider Interface similar to table factories. In order to discover a format factory, the dynamic table factory searches for a factory that corresponds to a factory identifier and connector-specific base class.
For example, the Kafka table source requires a DeserializationSchema as runtime interface for a decoding format. Therefore, the Kafka table source factory uses the value of the value.format option to discover a DeserializationFormatFactory.
The following format factories are currently supported:
org.apache.flink.table.factories.DeserializationFormatFactory org.apache.flink.table.factories.SerializationFormatFactory The format factory translates the options into an EncodingFormat or a DecodingFormat. Those interfaces are another kind of factory that produce specialized format runtime logic for the given data type.
For example, for a Kafka table source factory, the DeserializationFormatFactory would return an EncodingFormat\u0026lt;DeserializationSchema\u0026gt; that can be passed into the Kafka table source.
Back to top
Full Stack Example # This section sketches how to implement a scan table source with a decoding format that supports changelog semantics. The example illustrates how all of the mentioned components play together. It can serve as a reference implementation.
In particular, it shows how to
create factories that parse and validate options, implement table connectors, implement and discover custom formats, and use provided utilities such as data structure converters and the FactoryUtil. The table source uses a simple single-threaded SourceFunction to open a socket that listens for incoming bytes. The raw bytes are decoded into rows by a pluggable format. The format expects a changelog flag as the first column.
We will use most of the interfaces mentioned above to enable the following DDL:
CREATE TABLE UserScores (name STRING, score INT) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;localhost\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;byte-delimiter\u0026#39; = \u0026#39;10\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;changelog-csv\u0026#39;, \u0026#39;changelog-csv.column-delimiter\u0026#39; = \u0026#39;|\u0026#39; ); Because the format supports changelog semantics, we are able to ingest updates during runtime and create an updating view that can continuously evaluate changing data:
SELECT name, SUM(score) FROM UserScores GROUP BY name; Use the following command to ingest data in a terminal:
\u0026gt; nc -lk 9999 INSERT|Alice|12 INSERT|Bob|5 DELETE|Alice|12 INSERT|Alice|18 Factories # This section illustrates how to translate metadata coming from the catalog to concrete connector instances.
Both factories have been added to the META-INF/services directory.
SocketDynamicTableFactory
The SocketDynamicTableFactory translates the catalog table to a table source. Because the table source requires a decoding format, we are discovering the format using the provided FactoryUtil for convenience.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableSourceFactory; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.types.DataType; public class SocketDynamicTableFactory implements DynamicTableSourceFactory { // define all options statically public static final ConfigOption\u0026lt;String\u0026gt; HOSTNAME = ConfigOptions.key(\u0026#34;hostname\u0026#34;) .stringType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; PORT = ConfigOptions.key(\u0026#34;port\u0026#34;) .intType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; BYTE_DELIMITER = ConfigOptions.key(\u0026#34;byte-delimiter\u0026#34;) .intType() .defaultValue(10); // corresponds to \u0026#39;\\n\u0026#39; @Override public String factoryIdentifier() { return \u0026#34;socket\u0026#34;; // used for matching to \`connector = \u0026#39;...\u0026#39;\` } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(HOSTNAME); options.add(PORT); options.add(FactoryUtil.FORMAT); // use pre-defined option for format return options; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(BYTE_DELIMITER); return options; } @Override public DynamicTableSource createDynamicTableSource(Context context) { // either implement your custom validation logic here ... // or use the provided helper utility final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context); // discover a suitable decoding format final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat = helper.discoverDecodingFormat( DeserializationFormatFactory.class, FactoryUtil.FORMAT); // validate all options helper.validate(); // get the validated options final ReadableConfig options = helper.getOptions(); final String hostname = options.get(HOSTNAME); final int port = options.get(PORT); final byte byteDelimiter = (byte) (int) options.get(BYTE_DELIMITER); // derive the produced data type (excluding computed columns) from the catalog table final DataType producedDataType = context.getCatalogTable().getResolvedSchema().toPhysicalRowDataType(); // create and return dynamic table source return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } } ChangelogCsvFormatFactory
The ChangelogCsvFormatFactory translates format-specific options to a format. The FactoryUtil in SocketDynamicTableFactory takes care of adapting the option keys accordingly and handles the prefixing like changelog-csv.column-delimiter.
Because this factory implements DeserializationFormatFactory, it could also be used for other connectors that support deserialization formats such as the Kafka connector.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableFactory; public class ChangelogCsvFormatFactory implements DeserializationFormatFactory { // define all options statically public static final ConfigOption\u0026lt;String\u0026gt; COLUMN_DELIMITER = ConfigOptions.key(\u0026#34;column-delimiter\u0026#34;) .stringType() .defaultValue(\u0026#34;|\u0026#34;); @Override public String factoryIdentifier() { return \u0026#34;changelog-csv\u0026#34;; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { return Collections.emptySet(); } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(COLUMN_DELIMITER); return options; } @Override public DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; createDecodingFormat( DynamicTableFactory.Context context, ReadableConfig formatOptions) { // either implement your custom validation logic here ... // or use the provided helper method FactoryUtil.validateFactoryOptions(this, formatOptions); // get the validated options final String columnDelimiter = formatOptions.get(COLUMN_DELIMITER); // create and return the format return new ChangelogCsvFormat(columnDelimiter); } } Table Source and Decoding Format # This section illustrates how to translate from instances of the planning layer to runtime instances that are shipped to the cluster.
SocketDynamicTableSource
The SocketDynamicTableSource is used during planning. In our example, we don\u0026rsquo;t implement any of the available ability interfaces. Therefore, the main logic can be found in getScanRuntimeProvider(...) where we instantiate the required SourceFunction and its DeserializationSchema for runtime. Both instances are parameterized to return internal data structures (i.e. RowData).
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.ScanTableSource; import org.apache.flink.table.connector.source.SourceFunctionProvider; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; public class SocketDynamicTableSource implements ScanTableSource { private final String hostname; private final int port; private final byte byteDelimiter; private final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat; private final DataType producedDataType; public SocketDynamicTableSource( String hostname, int port, byte byteDelimiter, DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat, DataType producedDataType) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.decodingFormat = decodingFormat; this.producedDataType = producedDataType; } @Override public ChangelogMode getChangelogMode() { // in our example the format decides about the changelog mode // but it could also be the source itself return decodingFormat.getChangelogMode(); } @Override public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) { // create runtime classes that are shipped to the cluster final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer = decodingFormat.createRuntimeDecoder( runtimeProviderContext, producedDataType); final SourceFunction\u0026lt;RowData\u0026gt; sourceFunction = new SocketSourceFunction( hostname, port, byteDelimiter, deserializer); return SourceFunctionProvider.of(sourceFunction, false); } @Override public DynamicTableSource copy() { return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } @Override public String asSummaryString() { return \u0026#34;Socket Table Source\u0026#34;; } } ChangelogCsvFormat
The ChangelogCsvFormat is a decoding format that uses a DeserializationSchema during runtime. It supports emitting INSERT and DELETE changes.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.types.RowKind; public class ChangelogCsvFormat implements DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; { private final String columnDelimiter; public ChangelogCsvFormat(String columnDelimiter) { this.columnDelimiter = columnDelimiter; } @Override @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public DeserializationSchema\u0026lt;RowData\u0026gt; createRuntimeDecoder( DynamicTableSource.Context context, DataType producedDataType) { // create type information for the DeserializationSchema final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo = (TypeInformation\u0026lt;RowData\u0026gt;) context.createTypeInformation( producedDataType); // most of the code in DeserializationSchema will not work on internal data structures // create a converter for conversion at the end final DataStructureConverter converter = context.createDataStructureConverter(producedDataType); // use logical types during runtime for parsing final List\u0026lt;LogicalType\u0026gt; parsingTypes = producedDataType.getLogicalType().getChildren(); // create runtime class return new ChangelogCsvDeserializer(parsingTypes, converter, producedTypeInfo, columnDelimiter); } @Override public ChangelogMode getChangelogMode() { // define that this format can produce INSERT and DELETE rows return ChangelogMode.newBuilder() .addContainedKind(RowKind.INSERT) .addContainedKind(RowKind.DELETE) .build(); } } Runtime # For completeness, this section illustrates the runtime logic for both SourceFunction and DeserializationSchema.
ChangelogCsvDeserializer
The ChangelogCsvDeserializer contains a simple parsing logic for converting bytes into Row of Integer and String with a row kind. The final conversion step converts those into internal data structures.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.RuntimeConverter.Context; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.table.types.logical.LogicalTypeRoot; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class ChangelogCsvDeserializer implements DeserializationSchema\u0026lt;RowData\u0026gt; { private final List\u0026lt;LogicalType\u0026gt; parsingTypes; private final DataStructureConverter converter; private final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo; private final String columnDelimiter; public ChangelogCsvDeserializer( List\u0026lt;LogicalType\u0026gt; parsingTypes, DataStructureConverter converter, TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo, String columnDelimiter) { this.parsingTypes = parsingTypes; this.converter = converter; this.producedTypeInfo = producedTypeInfo; this.columnDelimiter = columnDelimiter; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { // return the type information required by Flink\u0026#39;s core interfaces return producedTypeInfo; } @Override public void open(InitializationContext context) { // converters must be open converter.open(Context.create(ChangelogCsvDeserializer.class.getClassLoader())); } @Override public RowData deserialize(byte[] message) { // parse the columns including a changelog flag final String[] columns = new String(message).split(Pattern.quote(columnDelimiter)); final RowKind kind = RowKind.valueOf(columns[0]); final Row row = new Row(kind, parsingTypes.size()); for (int i = 0; i \u0026lt; parsingTypes.size(); i++) { row.setField(i, parse(parsingTypes.get(i).getTypeRoot(), columns[i + 1])); } // convert to internal data structure return (RowData) converter.toInternal(row); } private static Object parse(LogicalTypeRoot root, String value) { switch (root) { case INTEGER: return Integer.parseInt(value); case VARCHAR: return value; default: throw new IllegalArgumentException(); } } @Override public boolean isEndOfStream(RowData nextElement) { return false; } } SocketSourceFunction
The SocketSourceFunction opens a socket and consumes bytes. It splits records by the given byte delimiter (\\n by default) and delegates the decoding to a pluggable DeserializationSchema. The source function can only work with a parallelism of 1.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.api.java.typeutils.ResultTypeQueryable; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.source.RichSourceFunction; import org.apache.flink.table.data.RowData; public class SocketSourceFunction extends RichSourceFunction\u0026lt;RowData\u0026gt; implements ResultTypeQueryable\u0026lt;RowData\u0026gt; { private final String hostname; private final int port; private final byte byteDelimiter; private final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer; private volatile boolean isRunning = true; private Socket currentSocket; public SocketSourceFunction(String hostname, int port, byte byteDelimiter, DeserializationSchema\u0026lt;RowData\u0026gt; deserializer) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.deserializer = deserializer; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { return deserializer.getProducedType(); } @Override public void open(Configuration parameters) throws Exception { deserializer.open(() -\u0026gt; getRuntimeContext().getMetricGroup()); } @Override public void run(SourceContext\u0026lt;RowData\u0026gt; ctx) throws Exception { while (isRunning) { // open and consume from socket try (final Socket socket = new Socket()) { currentSocket = socket; socket.connect(new InetSocketAddress(hostname, port), 0); try (InputStream stream = socket.getInputStream()) { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); int b; while ((b = stream.read()) \u0026gt;= 0) { // buffer until delimiter if (b != byteDelimiter) { buffer.write(b); } // decode and emit record else { ctx.collect(deserializer.deserialize(buffer.toByteArray())); buffer.reset(); } } } } catch (Throwable t) { t.printStackTrace(); // print and continue } Thread.sleep(1000); } } @Override public void cancel() { isRunning = false; try { currentSocket.close(); } catch (Throwable t) { // ignore } } } Back to top
`}),e.add({id:311,href:"/flink/flink-docs-master/docs/dev/python/faq/",title:"FAQ",section:"Python API",content:` FAQ # This page describes the solutions to some common questions for PyFlink users.
Preparing Python Virtual Environment # You can download a [convenience script]({% link downloads/setup-pyflink-virtual-env.sh %}) to prepare a Python virtual env zip which can be used on Mac OS and most Linux distributions. You can specify the PyFlink version to generate a Python virtual environment required for the corresponding PyFlink version, otherwise the most recent version will be installed.
\$ sh setup-pyflink-virtual-env.sh Execute PyFlink jobs with Python virtual environment # After setting up a python virtual environment, as described in the previous section, you should activate the environment before executing the PyFlink job.
Local # # activate the conda python virtual environment \$ source venv/bin/activate \$ python xxx.py Cluster # \` # specify the Python virtual environment \` table_env.add_python_archive(\u0026#34;venv.zip\u0026#34;) \` # specify the path of the python interpreter which is used to execute the python UDF workers \` table_env.get_config().set_python_executable(\u0026#34;venv.zip/venv/bin/python\u0026#34;) For details on the usage of add_python_archive and set_python_executable, you can refer to the relevant documentation.
Adding Jar Files # A PyFlink job may depend on jar files, i.e. connectors, Java UDFs, etc. You can specify the dependencies with the following Python Table APIs or through command-line arguments directly when submitting the job.
# NOTE: Only local file URLs (start with \u0026#34;file:\u0026#34;) are supported. table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) # NOTE: The Paths must specify a protocol (e.g. \u0026#34;file\u0026#34;) and users should ensure that the URLs are accessible on both the client and the cluster. table_env.get_config().set(\u0026#34;pipeline.classpaths\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) For details about the APIs of adding Java dependency, you can refer to the relevant documentation
Adding Python Files # You can use the command-line arguments pyfs or the API add_python_file of TableEnvironment to add python file dependencies which could be python files, python packages or local directories. For example, if you have a directory named myDir which has the following hierarchy:
myDir ├──utils ├──__init__.py ├──my_util.py You can add the Python files of directory myDir as following:
table_env.add_python_file(\u0026#39;myDir\u0026#39;) def my_udf(): from utils import my_util Wait for jobs to finish when executing jobs in mini cluster # When executing jobs in mini cluster(e.g. when executing jobs in IDE) and using the following APIs in the jobs( e.g. TableEnvironment.execute_sql, StatementSet.execute, etc in the Python Table API; StreamExecutionEnvironment.execute_async in the Python DataStream API), please remember to explicitly wait for the job execution to finish as these APIs are asynchronous. Otherwise you may could not find the execution results as the program will exit before the job execution finishes. Please refer to the following example on how to do that:
# execute SQL / Table API query asynchronously t_result = table_env.execute_sql(...) t_result.wait() # execute DataStream Job asynchronously job_client = stream_execution_env.execute_async(\u0026#39;My DataStream Job\u0026#39;) job_client.get_job_execution_result().result() Note: There is no need to wait for the job execution to finish when executing jobs in remote cluster and so remember to remove these codes when executing jobs in remote cluster.
`}),e.add({id:312,href:"/flink/flink-docs-master/docs/dev/datastream/scala_api_extensions/",title:"Scala API Extensions",section:"DataStream API",content:` Scala API Extensions # In order to keep a fair amount of consistency between the Scala and Java APIs, some of the features that allow a high-level of expressiveness in Scala have been left out from the standard APIs for both batch and streaming.
If you want to enjoy the full Scala experience you can choose to opt-in to extensions that enhance the Scala API via implicit conversions.
To use all the available extensions, you can just add a simple import for the DataStream API
import org.apache.flink.streaming.api.scala.extensions._ Alternatively, you can import individual extensions a-là-carte to only use those you prefer.
Accept partial functions # Normally, the DataStream API does not accept anonymous pattern matching functions to deconstruct tuples, case classes or collections, like the following:
val data: DataStream[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =\u0026gt; // [...] // The previous line causes the following compilation error: // \u0026#34;The argument types of an anonymous function must be fully known. (SLS 8.5)\u0026#34; } This extension introduces new methods in the DataStream Scala API that have a one-to-one correspondence in the extended API. These delegating methods do support anonymous pattern matching functions.
DataStream API # Method Original Example mapWith map (DataStream) data.mapWith { case (_, value) =\u0026gt; value.toString } flatMapWith flatMap (DataStream) data.flatMapWith { case (_, name, visits) =\u0026gt; visits.map(name -\u0026gt; _) } filterWith filter (DataStream) data.filterWith { case Train(_, isOnTime) =\u0026gt; isOnTime } keyingBy keyBy (DataStream) data.keyingBy { case (id, _, _) =\u0026gt; id } mapWith map (ConnectedDataStream) data.mapWith( map1 = case (_, value) =\u0026gt; value.toString, map2 = case (_, _, value, _) =\u0026gt; value + 1 ) flatMapWith flatMap (ConnectedDataStream) data.flatMapWith( flatMap1 = case (_, json) =\u0026gt; parse(json), flatMap2 = case (_, _, json, _) =\u0026gt; parse(json) ) keyingBy keyBy (ConnectedDataStream) data.keyingBy( key1 = case (_, timestamp) =\u0026gt; timestamp, key2 = case (id, _, _) =\u0026gt; id ) reduceWith reduce (KeyedStream, WindowedStream) data.reduceWith { case ((_, sum1), (_, sum2) =\u0026gt; sum1 + sum2 } projecting apply (JoinedStream) data1.join(data2). whereClause(case (pk, _) =\u0026gt; pk). isEqualTo(case (_, fk) =\u0026gt; fk). projecting { case ((pk, tx), (products, fk)) =\u0026gt; tx -\u0026gt; products } For more information on the semantics of each method, please refer to the DataStream API documentation.
To use this extension exclusively, you can add the following import:
import org.apache.flink.api.scala.extensions.acceptPartialFunctions for the DataSet extensions and
import org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions The following snippet shows a minimal example of how to use these extension methods together (with the DataSet API):
object Main { import org.apache.flink.streaming.api.scala.extensions._ case class Point(x: Double, y: Double) def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6)) ds.filterWith { case Point(x, _) =\u0026gt; x \u0026gt; 1 }.reduceWith { case (Point(x1, y1), (Point(x2, y2))) =\u0026gt; Point(x1 + y1, x2 + y2) }.mapWith { case Point(x, y) =\u0026gt; (x, y) }.flatMapWith { case (x, y) =\u0026gt; Seq(\u0026#34;x\u0026#34; -\u0026gt; x, \u0026#34;y\u0026#34; -\u0026gt; y) }.keyingBy { case (id, value) =\u0026gt; id } } } Back to top
`}),e.add({id:313,href:"/flink/flink-docs-master/docs/dev/datastream/java_lambdas/",title:"Java Lambda Expressions",section:"DataStream API",content:` Java Lambda Expressions # Java 8 introduced several new language features designed for faster and clearer coding. With the most important feature, the so-called \u0026ldquo;Lambda Expressions\u0026rdquo;, it opened the door to functional programming. Lambda expressions allow for implementing and passing functions in a straightforward way without having to declare additional (anonymous) classes.
Flink supports the usage of lambda expressions for all operators of the Java API, however, whenever a lambda expression uses Java generics you need to declare type information explicitly. This document shows how to use lambda expressions and describes current limitations. For a general introduction to the Flink API, please refer to the DataSteam API overview
Examples and Limitations # The following example illustrates how to implement a simple, inline map() function that squares its input using a lambda expression. The types of input i and output parameters of the map() function need not to be declared as they are inferred by the Java compiler.
env.fromElements(1, 2, 3) // returns the squared i .map(i -\u0026gt; i*i) .print(); Flink can automatically extract the result type information from the implementation of the method signature OUT map(IN value) because OUT is not generic but Integer.
Unfortunately, functions such as flatMap() with a signature void flatMap(IN value, Collector\u0026lt;OUT\u0026gt; out) are compiled into void flatMap(IN value, Collector out) by the Java compiler. This makes it impossible for Flink to infer the type information for the output type automatically.
Flink will most likely throw an exception similar to the following:
org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of \u0026#39;Collector\u0026#39; are missing. In many cases lambda methods don\u0026#39;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the \u0026#39;org.apache.flink.api.common.functions.FlatMapFunction\u0026#39; interface. Otherwise the type has to be specified explicitly using type information. In this case, the type information needs to be specified explicitly, otherwise the output will be treated as type Object which leads to unefficient serialization.
DataStream\u0026lt;Integer\u0026gt; input = env.fromElements(1, 2, 3); // collector type must be declared input.flatMap((Integer number, Collector\u0026lt;String\u0026gt; out) -\u0026gt; { StringBuilder builder = new StringBuilder(); for(int i = 0; i \u0026lt; number; i++) { builder.append(\u0026#34;a\u0026#34;); out.collect(builder.toString()); } }) // provide type information explicitly .returns(Types.STRING) // prints \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aaa\u0026#34; .print(); Similar problems occur when using a map() function with a generic return type. A method signature Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer value) is erasured to Tuple2 map(Integer value) in the example below.
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple2; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) // no information about fields of Tuple2 .print(); In general, those problems can be solved in multiple ways:
import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.tuple.Tuple2; // use the explicit \u0026#34;.returns(...)\u0026#34; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) .returns(Types.TUPLE(Types.INT, Types.INT)) .print(); // use a class instead env.fromElements(1, 2, 3) .map(new MyTuple2Mapper()) .print(); public static class MyTuple2Mapper extends MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } } // use an anonymous class instead env.fromElements(1, 2, 3) .map(new MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } }) .print(); // or in this example use a tuple subclass instead env.fromElements(1, 2, 3) .map(i -\u0026gt; new DoubleTuple(i, i)) .print(); public static class DoubleTuple extends Tuple2\u0026lt;Integer, Integer\u0026gt; { public DoubleTuple(int f0, int f1) { this.f0 = f0; this.f1 = f1; } } Back to top
`}),e.add({id:314,href:"/flink/flink-docs-master/docs/dev/datastream/execution/",title:"Managing Execution",section:"DataStream API",content:""}),e.add({id:315,href:"/flink/flink-docs-master/docs/dev/table/concepts/temporal_table_function/",title:"Temporal Table Function",section:"Streaming Concepts",content:` Temporal Table Function # A Temporal table function provides access to the version of a temporal table at a specific point in time. In order to access the data in a temporal table, one must pass a time attribute that determines the version of the table that will be returned. Flink uses the SQL syntax of table functions to provide a way to express it.
Unlike a versioned table, temporal table functions can only be defined on top of append-only streams — it does not support changelog inputs. Additionally, a temporal table function cannot be defined in pure SQL DDL.
Defining a Temporal Table Function # Temporal table functions can be defined on top of append-only streams using the Table API. The table is registered with one or more key columns, and a time attribute used for versioning.
Suppose we have an append-only table of currency rates that we would like to register as a temporal table function.
SELECT * FROM currency_rates; update_time currency rate ============= ========= ==== 09:00:00 Yen 102 09:00:00 Euro 114 09:00:00 USD 1 11:15:00 Euro 119 11:49:00 Pounds 108 Using the Table API, we can register this stream using currency for the key and update_time as the versioning time attribute.
Java TemporalTableFunction rates = tEnv .from(\u0026#34;currency_rates\u0026#34;) .createTemporalTableFunction(\u0026#34;update_time\u0026#34;, \u0026#34;currency\u0026#34;); tEnv.registerFunction(\u0026#34;rates\u0026#34;, rates); Scala rates = tEnv .from(\u0026#34;currency_rates\u0026#34;) .createTemporalTableFunction(\u0026#34;update_time\u0026#34;, \u0026#34;currency\u0026#34;) tEnv.registerFunction(\u0026#34;rates\u0026#34;, rates) Python Still not supported in Python API. Temporal Table Function Join # Once defined, a temporal table function is used as a standard table function. Append-only tables (left input/probe side) can join with a temporal table (right input/build side), i.e., a table that changes over time and tracks its changes, to retrieve the value for a key as it was at a particular point in time.
Consider an append-only table orders that tracks customers\u0026rsquo; orders in different currencies.
SELECT * FROM orders; order_time amount currency ========== ====== ========= 10:15 2 Euro 10:30 1 USD 10:32 50 Yen 10:52 3 Euro 11:04 5 USD Given these tables, we would like to convert orders to a common currency — USD.
SQL SELECT SUM(amount * rate) AS amount FROM orders, LATERAL TABLE (rates(order_time)) WHERE rates.currency = orders.currency Java Table result = orders .joinLateral(\$(\u0026#34;rates(order_time)\u0026#34;), \$(\u0026#34;orders.currency = rates.currency\u0026#34;)) .select(\$(\u0026#34;(o_amount * r_rate).sum as amount\u0026#34;)); Scala val result = orders .joinLateral(\$\u0026#34;rates(order_time)\u0026#34;, \$\u0026#34;orders.currency = rates.currency\u0026#34;) .select(\$\u0026#34;(o_amount * r_rate).sum as amount\u0026#34;)) Python Still not supported in Python API. Back to top
`}),e.add({id:316,href:"/flink/flink-docs-master/release-notes/flink-1.10/",title:"Release Notes - Flink 1.10",section:"Release-notes",content:` Release Notes - Flink 1.10 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.9 and Flink 1.10. Please read these notes carefully if you are planning to upgrade your Flink version to 1.10.
Clusters \u0026amp; Deployment # FileSystems should be loaded via Plugin Architecture # FLINK-11956 # s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through plugins but now seamlessly integrate with all credential providers. Other filesystems are strongly recommended to be only used as plugins as we will continue to remove relocations.
Flink Client respects Classloading Policy # FLINK-13749 # The Flink client now also respects the configured classloading policy, i.e., parent-first or child-first classloading. Previously, only cluster components such as the job manager or task manager supported this setting. This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicitly to use parent-first classloading, which was the previous (hard-coded) behaviour.
Enable spreading out Tasks evenly across all TaskManagers # FLINK-12122 # When FLIP-6 was rolled out with Flink 1.5.0, we changed how slots are allocated from TaskManagers (TMs). Instead of evenly allocating the slots from all registered TMs, we had the tendency to exhaust a TM before using another one. To use a scheduling strategy that is more similar to the pre-FLIP-6 behaviour, where Flink tries to spread out the workload across all currently available TMs, one can set cluster.evenly-spread-out-slots: true in the flink-conf.yaml.
Directory Structure Change for highly available Artifacts # FLINK-13633 # All highly available artifacts stored by Flink will now be stored under HA_STORAGE_DIR/HA_CLUSTER_ID with HA_STORAGE_DIR configured by high-availability.storageDir and HA_CLUSTER_ID configured by high-availability.cluster-id.
Resources and JARs shipped via \u0026ndash;yarnship will be ordered in the Classpath # FLINK-13127 # When using the --yarnship command line option, resource directories and jar files will be added to the classpath in lexicographical order with resources directories appearing first.
Removal of \u0026ndash;yn/\u0026ndash;yarncontainer Command Line Options # FLINK-12362 # The Flink CLI no longer supports the deprecated command line options -yn/--yarncontainer, which were used to specify the number of containers to start on YARN. This option has been deprecated since the introduction of FLIP-6. All Flink users are advised to remove this command line option.
Removal of \u0026ndash;yst/\u0026ndash;yarnstreaming Command Line Options # FLINK-14957 # The Flink CLI no longer supports the deprecated command line options -yst/--yarnstreaming, which were used to disable eager pre-allocation of memory. All Flink users are advised to remove this command line option.
Mesos Integration will reject expired Offers faster # FLINK-14029 # Flink\u0026rsquo;s Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on to a lot of expired offers without giving them back to the Mesos resource manager.
Scheduler Rearchitecture # FLINK-14651 # Flink\u0026rsquo;s scheduler was refactored with the goal of making scheduling strategies customizable in the future. Using the legacy scheduler is discouraged as it will be removed in a future release. However, users that experience issues related to scheduling can fallback to the legacy scheduler by setting jobmanager.scheduler to legacy in their flink-conf.yaml for the time being. Note, however, that using the legacy scheduler with the Pipelined Region Failover Strategy enabled has the following caveats:
Exceptions that caused a job to restart will not be shown on the job overview page of the Web UI FLINK-15917 # . However, exceptions that cause a job to fail (e.g., when all restart attempts exhausted) will still be shown.
The uptime metric will not be reset after restarting a job due to task failure FLINK-15918 # .
Note that in the default flink-conf.yaml, the Pipelined Region Failover Strategy is already enabled. That is, users that want to use the legacy scheduler and cannot accept aforementioned caveats should make sure that jobmanager.execution.failover-strategy is set to full or not set at all.
Java 11 Support # FLINK-10725 # Beginning from this release, Flink can be compiled and run with Java 11. All Java 8 artifacts can be also used with Java 11. This means that users that want to run Flink with Java 11 do not have to compile Flink themselves.
When starting Flink with Java 11, the following warnings may be logged:
WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.flink.core.memory.MemoryUtils (file:/opt/flink/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.flink.core.memory.MemoryUtils WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/flinkuser/.m2/repository/org/apache/flink/flink-core/1.10.0/flink-core-1.10.0.jar) to field java.lang.String.value WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/home/flinkuser/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar) to method java.nio.DirectByteBuffer.cleaner() WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by com.esotericsoftware.kryo.util.UnsafeUtil (file:/home/flinkuser/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar) to constructor java.nio.DirectByteBuffer(long,int,java.lang.Object) WARNING: Please consider reporting this to the maintainers of com.esotericsoftware.kryo.util.UnsafeUtil WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release These warnings are considered harmless and will be addressed in future Flink releases.
Lastly, note that the connectors for Cassandra, Hive, HBase, and Kafka 0.8\u0026ndash;0.11 have not been tested with Java 11 because the respective projects did not provide Java 11 support at the time of the Flink 1.10.0 release.
Memory Management # New Task Executor Memory Model # FLINK-13980 # With FLIP-49, a new memory model has been introduced for the task executor. New configuration options have been introduced to control the memory consumption of the task executor process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration. The memory model of the job manager process has not been changed yet but it is planned to be updated as well.
If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes.
Please, check the user documentation for more details.
Deprecation and breaking changes # The following options have been removed and have no effect anymore:
Deprecated/removed config option Note taskmanager.memory.fraction Check also the description of the new option taskmanager.memory.managed.fraction but it has different semantics and the value of the deprecated option usually has to be adjusted taskmanager.memory.off-heap Support for on-heap managed memory has been removed, leaving off-heap managed memory as the only possibility taskmanager.memory.preallocate Pre-allocation is no longer supported, and managed memory is always allocated lazily The following options, if used, are interpreted as other new options in order to maintain backwards compatibility where it makes sense:
Deprecated config option Interpreted as taskmanager.heap.size taskmanager.memory.flink.size for standalone deployment taskmanager.memory.process.size for containerized deployments taskmanager.memory.size taskmanager.memory.managed.size taskmanager.network.memory.min taskmanager.memory.network.min taskmanager.network.memory.max taskmanager.memory.network.max taskmanager.network.memory.fraction taskmanager.memory.network.fraction The container cut-off configuration options, containerized.heap-cutoff-ratio and containerized.heap-cutoff-min, have no effect for task executor processes anymore but they still have the same semantics for the JobManager process.
RocksDB State Backend Memory Control # FLINK-7289 # Together with the introduction of the new Task Executor Memory Model, the memory consumption of the RocksDB state backend will be limited by the total amount of Flink Managed Memory, which can be configured via taskmanager.memory.managed.size or taskmanager.memory.managed.fraction. Furthermore, users can tune RocksDB\u0026rsquo;s write/read memory ratio (state.backend.rocksdb.memory.write-buffer-ratio, by default 0.5) and the reserved memory fraction for indices/filters (state.backend.rocksdb.memory.high-prio-pool-ratio, by default 0.1). More details and advanced configuration options can be found in the Flink user documentation.
Fine-grained Operator Resource Management # FLINK-14058 # Config options table.exec.resource.external-buffer-memory, table.exec.resource.hash-agg.memory, table.exec.resource.hash-join.memory, and table.exec.resource.sort.memory have been deprecated. Beginning from Flink 1.10, these config options are interpreted as weight hints instead of absolute memory requirements. Flink choses sensible default weight hints which should not be adjustment by users.
Table API \u0026amp; SQL # Rename of ANY Type to RAW Type # FLINK-14904 # The identifier raw is a reserved keyword now and must be escaped with backticks when used as a SQL field or function name.
Rename of Table Connector Properties # FLINK-14649 # Some indexed properties for table connectors have been flattened and renamed for a better user experience when writing DDL statements. This affects the Kafka Connector properties connector.properties and connector.specific-offsets. Furthermore, the Elasticsearch Connector property connector.hosts is affected. The aforementioned, old properties are deprecated and will be removed in future versions. Please consult the Table Connectors documentation for the new property names.
Methods for interacting with temporary Tables \u0026amp; Views # FLINK-14490 # Methods registerTable()/registerDataStream()/registerDataSet() have been deprecated in favor of createTemporaryView(), which better adheres to the corresponding SQL term.
The scan() method has been deprecated in favor of the from() method.
Methods registerTableSource()/registerTableSink() become deprecated in favor of ConnectTableDescriptor#createTemporaryTable(). The ConnectTableDescriptor approach expects only a set of string properties as a description of a TableSource or TableSink instead of an instance of a class in case of the deprecated methods. This in return makes it possible to reliably store those definitions in catalogs.
Method insertInto(String path, String... pathContinued) has been removed in favor of in insertInto(String path).
All the newly introduced methods accept a String identifier which will be parsed into a 3-part identifier. The parser supports quoting the identifier. It also requires escaping any reserved SQL keywords.
Removal of ExternalCatalog API # FLINK-13697 # The deprecated ExternalCatalog API has been dropped. This includes:
ExternalCatalog (and all dependent classes, e.g., ExternalTable) SchematicDescriptor, MetadataDescriptor, StatisticsDescriptor Users are advised to use the new Catalog API.
Configuration # Introduction of Type Information for ConfigOptions # FLINK-14493 # Getters of org.apache.flink.configuration.Configuration throw IllegalArgumentException now if the configured value cannot be parsed into the required type. In previous Flink releases the default value was returned in such cases.
Increase of default Restart Delay # FLINK-13884 # The default restart delay for all shipped restart strategies, i.e., fixed-delay and failure-rate, has been raised to 1 s (from originally 0 s).
Simplification of Cluster-Level Restart Strategy Configuration # FLINK-13921 # Previously, if the user had set restart-strategy.fixed-delay.attempts or restart-strategy.fixed-delay.delay but had not configured the option restart-strategy, the cluster-level restart strategy would have been fixed-delay. Now the cluster-level restart strategy is only determined by the config option restart-strategy and whether checkpointing is enabled. See \u0026ldquo;Task Failure Recovery\u0026rdquo; for details.
Disable memory-mapped BoundedBlockingSubpartition by default # FLINK-14952 # The config option taskmanager.network.bounded-blocking-subpartition-type has been renamed to taskmanager.network.blocking-shuffle.type. Moreover, the default value of the aforementioned config option has been changed from auto to file. The reason is that TaskManagers running on YARN with auto, could easily exceed the memory budget of their container, due to incorrectly accounted memory-mapped files memory usage.
Removal of non-credit-based Network Flow Control # FLINK-14516 # The non-credit-based network flow control code was removed alongside of the configuration option taskmanager.network.credit-model. Flink will now always use credit-based flow control.
Removal of HighAvailabilityOptions#HA_JOB_DELAY # FLINK-13885 # The configuration option high-availability.job.delay has been removed since it is no longer used.
State # Enable Background Cleanup of State with TTL by default # FLINK-14898 # Background cleanup of expired state with TTL is activated by default now for all state backends shipped with Flink. Note that the RocksDB state backend implements background cleanup by employing a compaction filter. This has the caveat that even if a Flink job does not store state with TTL, a minor performance penalty during compaction is incurred. Users that experience noticeable performance degradation during RocksDB compaction can disable the TTL compaction filter by setting the config option state.backend.rocksdb.ttl.compaction.filter.enabled to false.
Deprecation of StateTtlConfig#Builder#cleanupInBackground() # FLINK-15606 # StateTtlConfig#Builder#cleanupInBackground() has been deprecated because the background cleanup of state with TTL is already enabled by default.
Timers are stored in RocksDB by default when using RocksDBStateBackend # FLINK-15637 # The default timer store has been changed from Heap to RocksDB for the RocksDB state backend to support asynchronous snapshots for timer state and better scalability, with less than 5% performance cost. Users that find the performance decline critical can set state.backend.rocksdb.timer-service.factory to HEAP in flink-conf.yaml to restore the old behavior.
Removal of StateTtlConfig#TimeCharacteristic # FLINK-15605 # StateTtlConfig#TimeCharacteristic has been removed in favor of StateTtlConfig#TtlTimeCharacteristic.
New efficient Method to check if MapState is empty # FLINK-13034 # We have added a new method MapState#isEmpty() which enables users to check whether a map state is empty. The new method is 40% faster than mapState.keys().iterator().hasNext() when using the RocksDB state backend.
RocksDB Upgrade # FLINK-14483 # We have again released our own RocksDB build (FRocksDB) which is based on RocksDB version 5.17.2 with several feature backports for the Write Buffer Manager to enable limiting RocksDB\u0026rsquo;s memory usage. The decision to release our own RocksDB build was made because later RocksDB versions suffer from a performance regression under certain workloads.
RocksDB Logging disabled by default # FLINK-15068 # Logging in RocksDB (e.g., logging related to flush, compaction, memtable creation, etc.) has been disabled by default to prevent disk space from being filled up unexpectedly. Users that need to enable logging should implement their own RocksDBOptionsFactory that creates DBOptions instances with InfoLogLevel set to INFO_LEVEL.
Improved RocksDB Savepoint Recovery # FLINK-12785 # In previous Flink releases users may encounter an OutOfMemoryError when restoring from a RocksDB savepoint containing large KV pairs. For that reason we introduced a configurable memory limit in the RocksDBWriteBatchWrapper with a default value of 2 MB. RocksDB\u0026rsquo;s WriteBatch will flush before the consumed memory limit is reached. If needed, the limit can be tuned via the state.backend.rocksdb.write-batch-size config option in flink-conf.yaml.
PyFlink # Python 2 Support dropped # FLINK-14469 # Beginning from this release, PyFlink does not support Python 2. This is because Python 2 has reached end of life on January 1, 2020, and several third-party projects that PyFlink depends on are also dropping Python 2 support.
Monitoring # InfluxdbReporter skips Inf and NaN # FLINK-12147 # The InfluxdbReporter now silently skips values that are unsupported by InfluxDB, such as Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, Double.NaN, etc.
Connectors # Kinesis Connector License Change # FLINK-12847 # flink-connector-kinesis is now licensed under the Apache License, Version 2.0, and its artifacts will be deployed to Maven central as part of the Flink releases. Users no longer need to build the Kinesis connector from source themselves.
Miscellaneous Interface Changes # ExecutionConfig#getGlobalJobParameters() cannot return null anymore # FLINK-9787 # ExecutionConfig#getGlobalJobParameters has been changed to never return null. Conversely, ExecutionConfig#setGlobalJobParameters(GlobalJobParameters) will not accept null values anymore.
Change of contract in MasterTriggerRestoreHook interface # FLINK-14344 # Implementations of MasterTriggerRestoreHook#triggerCheckpoint(long, long, Executor) must be non-blocking now. Any blocking operation should be executed asynchronously, e.g., using the given executor.
Client-/ and Server-Side Separation of HA Services # FLINK-13750 # The HighAvailabilityServices have been split up into client-side ClientHighAvailabilityServices and cluster-side HighAvailabilityServices. When implementing custom high availability services, users should follow this separation by overriding the factory method HighAvailabilityServicesFactory#createClientHAServices(Configuration). Moreover, HighAvailabilityServices#getWebMonitorLeaderRetriever() should no longer be implemented since it has been deprecated.
Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() # FLINK-13977 # Implementations of HighAvailabilityServices should implement HighAvailabilityServices#getClusterRestEndpointLeaderElectionService() instead of HighAvailabilityServices#getWebMonitorLeaderElectionService().
Interface Change in LeaderElectionService # FLINK-14287 # LeaderElectionService#confirmLeadership(UUID, String) now takes an additional second argument, which is the address under which the leader will be reachable. All custom LeaderElectionService implementations will need to be updated accordingly.
Deprecation of Checkpoint Lock # FLINK-14857 # The method org.apache.flink.streaming.runtime.tasks.StreamTask#getCheckpointLock() is deprecated now. Users should use MailboxExecutor to run actions that require synchronization with the task\u0026rsquo;s thread (e.g. collecting output produced by an external thread). The methods MailboxExecutor#yield() or MailboxExecutor#tryYield() can be used for actions that need to give up control to other actions temporarily, e.g., if the current operator is blocked. The MailboxExecutor can be accessed by using YieldingOperatorFactory (see AsyncWaitOperator for an example usage).
Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces # FLINK-14926 # Interfaces OptionsFactory and ConfigurableOptionsFactory have been deprecated in favor of RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory, respectively.
Incompatibility of serialized JobGraphs # FLINK-14594 # Serialized JobGraphs which set the ResourceSpec created by Flink versions \u0026lt; 1.10 are no longer compatible with Flink \u0026gt;= 1.10. If you want to migrate these jobs to Flink \u0026gt;= 1.10 you will have to stop the job with a savepoint and then resume it from this savepoint on the Flink \u0026gt;= 1.10 cluster.
`}),e.add({id:317,href:"/flink/flink-docs-master/release-notes/flink-1.11/",title:"Release Notes - Flink 1.11",section:"Release-notes",content:` Release Notes - Flink 1.11 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read these notes carefully if you are planning to upgrade your Flink version to 1.11.
Clusters \u0026amp; Deployment # Support for Application Mode # FLIP-85 # The user can now submit applications and choose to execute their main() method on the cluster rather than the client. This allows for more light-weight application submission. For more details, see the Application Mode documentation.
Web Submission behaves the same as detached mode. # With FLINK-16657 the web submission logic changes and it exposes the same behavior as submitting a job through the CLI in detached mode. This implies that, for instance, jobs based on the DataSet API that were using sinks like print(), count() or collect() will now throw an exception while before the output was simply never printed. See also comments on related PR.
Support for Hadoop 3.0.0 and higher # FLINK-11086 # Flink project does not provide any updated \u0026ldquo;flink-shaded-hadoop-*\u0026rdquo; jars. Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via lib/ folder. Also, the include-hadoop Maven profile has been removed.
flink-csv and flink-json are bundled in lib folder # FLINK-18173 # There is no need to download manually jar files for flink-csv and flink-json formats as they are now bundled in the lib folder.
Removal of LegacyScheduler # FLINK-15629 # Flink no longer supports the legacy scheduler. Hence, setting jobmanager.scheduler: legacy will no longer work and fail with an IllegalArgumentException. The only valid option for jobmanager.scheduler is the default value ng.
Bind user code class loader to lifetime of a slot # FLINK-16408 # The user code class loader is being reused by the TaskExecutor as long as there is at least a single slot allocated for the respective job. This changes Flink\u0026rsquo;s recovery behaviour slightly so that it will not reload static fields. The benefit is that this change drastically reduces pressure on the JVM\u0026rsquo;s metaspace.
Replaced slave file name with workers # FLINK-18307 # For Standalone Setups, the file with the worker nodes is no longer called slaves but workers. Previous setups that use the start-cluster.sh and stop-cluster.sh scripts need to rename that file.
Flink Docker Integration Improvements # The examples of Dockerfiles and docker image build.sh scripts have been removed from the Flink Github repository. The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:
flink-contrib/docker-flink flink-container/docker flink-container/kubernetes Check the updated user documentation for Flink Docker integration instead. It now describes in detail how to use and customize the Flink official docker image: configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:
docker run docker compose docker swarm standalone Kubernetes Memory Management # New JobManager Memory Model # Overview # With FLIP-116, a new memory model has been introduced for the JobManager. New configuration options have been introduced to control the memory consumption of the JobManager process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.
Please, check the user documentation for more details.
If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. In order to start the JobManager process, you have to specify at least one of the following options jobmanager.memory.flink.size, jobmanager.memory.process.size or jobmanager.memory.heap.size. See also the migration guide for more information.
Deprecation and breaking changes # The following options are deprecated:
jobmanager.heap.size jobmanager.heap.mb If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:
JVM Heap (jobmanager.memory.heap.size) for standalone and Mesos deployments Total Process Memory (jobmanager.memory.process.size) for containerized deployments (Kubernetes and Yarn) The following options have been removed and have no effect anymore:
containerized.heap-cutoff-ratio containerized.heap-cutoff-min There is no container cut-off anymore.
JVM arguments # The direct and metaspace memory of the JobManager\u0026rsquo;s JVM process are now limited by configurable values:
jobmanager.memory.off-heap.size jobmanager.memory.jvm-metaspace.size See also JVM Parameters.
These new limits can produce the respective OutOfMemoryError exceptions if they are not configured properly or there is a respective memory leak. See also the troubleshooting guide. Removal of deprecated mesos.resourcemanager.tasks.mem # FLINK-15198 # The mesos.resourcemanager.tasks.mem option, deprecated in 1.10 in favour of taskmanager.memory.process.size, has been completely removed and will have no effect anymore in 1.11+.
Table API \u0026amp; SQL # Blink is now the default planner # FLINK-16934 # The default table planner has been changed to blink.
Changed package structure for Table API # FLINK-15947 # Due to various issues with packages org.apache.flink.table.api.scala/java all classes from those packages were relocated. Moreover the scala expressions were moved to org.apache.flink.table.api as announced in Flink 1.9.
If you used one of:
org.apache.flink.table.api.java.StreamTableEnvironment org.apache.flink.table.api.scala.StreamTableEnvironment org.apache.flink.table.api.java.BatchTableEnvironment org.apache.flink.table.api.scala.BatchTableEnvironment And you do not convert to/from DataStream, switch to:
org.apache.flink.table.api.TableEnvironment If you do convert to/from DataStream/DataSet, change your imports to one of:
org.apache.flink.table.api.bridge.java.StreamTableEnvironment org.apache.flink.table.api.bridge.scala.StreamTableEnvironment org.apache.flink.table.api.bridge.java.BatchTableEnvironment org.apache.flink.table.api.bridge.scala.BatchTableEnvironment For the Scala expressions use the import:
org.apache.flink.table.api._ instead of org.apache.flink.table.api.bridge.scala._ Additionally, if you use Scala\u0026rsquo;s implicit conversions to/from DataStream/DataSet, import org.apache.flink.table.api.bridge.scala._ instead of org.apache.flink.table.api.scala._
Removal of deprecated StreamTableSink # FLINK-16362 # The existing StreamTableSink implementations should remove emitDataStream method.
Removal of BatchTableSink#emitDataSet # FLINK-16535 # The existing BatchTableSink implementations should rename emitDataSet to consumeDataSet and return DataSink.
Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() # FLINK-16363 # In previous versions, TableEnvironment.execute() and StreamExecutionEnvironment.execute() can both trigger table and DataStream programs. Since Flink 1.11.0, table programs can only be triggered by TableEnvironment.execute(). Once table program is converted into DataStream program (through toAppendStream() or toRetractStream() method), it can only be triggered by StreamExecutionEnvironment.execute().
Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() # FLINK-17126 # In previous versions, BatchTableEnvironment.execute() and ExecutionEnvironment.execute() can both trigger table and DataSet programs for legacy batch planner. Since Flink 1.11.0, batch table programs can only be triggered by BatchEnvironment.execute(). Once table program is converted into DataSet program (through toDataSet() method), it can only be triggered by ExecutionEnvironment.execute().
Added a changeflag to Row type # FLINK-16998 # An additional change flag called RowKind was added to the Row type. This changed the serialization format and will trigger a state migration.
Configuration # Renamed log4j-yarn-session.properties and logback-yarn.xml properties files # FLINK-17527 # The logging properties files log4j-yarn-session.properties and logback-yarn.xml have been renamed to log4j-session.properties and logback-session.xml. Moreover, yarn-session.sh and kubernetes-session.sh use these logging properties files.
State # Removal of deprecated background cleanup toggle (State TTL) # FLINK-15620 # The StateTtlConfig#cleanupInBackground has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.
Removal of deprecated option to disable TTL compaction filter # FLINK-15621 # The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+. Because of that the following option and methods have been removed in 1.11:
state.backend.rocksdb.ttl.compaction.filter.enabled StateTtlConfig#cleanupInRocksdbCompactFilter() RocksDBStateBackend#isTtlCompactionFilterEnabled RocksDBStateBackend#enableTtlCompactionFilter RocksDBStateBackend#disableTtlCompactionFilter (state_backend.py) is_ttl_compaction_filter_enabled (state_backend.py) enable_ttl_compaction_filter (state_backend.py) disable_ttl_compaction_filter Changed argument type of StateBackendFactory#createFromConfig # FLINK-16913 # Starting from Flink 1.11 the StateBackendFactory#createFromConfig interface now takes ReadableConfig instead of Configuration. A Configuration class is still a valid argument to that method, as it implements the ReadableConfig interface. Implementors of custom StateBackend should adjust their implementations.
Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes # FLINK-18242 # The deprecated OptionsFactory and ConfigurableOptionsFactory classes have been removed. Please use RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory instead. Please also recompile your application codes if any class extends DefaultConfigurableOptionsFactory.
Enabled by default setTotalOrderSeek # FLINK-17800 # Since Flink-1.11 the option setTotalOrderSeek will be enabled by default for RocksDB\u0026rsquo;s ReadOptions. This is in order to prevent user from miss using optimizeForPointLookup. For backward compatibility we support customizing ReadOptions through RocksDBOptionsFactory. Please set setTotalOrderSeek back to false if any performance regression observed (it shouldn\u0026rsquo;t happen according to our testing).
Increased default size of state.backend.fs.memory-threshold # FLINK-17865 # The default value of state.backend.fs.memory-threshold has been increased from 1K to 20K to prevent too many small files created on remote FS for small states. Jobs with large parallelism on source or stateful operators may have \u0026ldquo;JM OOM\u0026rdquo; or \u0026ldquo;RPC message exceeding maximum frame size\u0026rdquo; problem with this change. If you encounter such issues please manually set the configuration back to 1K.
PyFlink # Throw exceptions for the unsupported data types # FLINK-16606 # DataTypes can be configured with some parameters, e.g., precision. However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used. To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. Changes include:
the precision for TimeType can only be 0 the length for VarBinaryType/VarCharType can only be 0x7fffffff the precision/scale for DecimalType can only be 38/18 the precision for TimestampType/LocalZonedTimestampType can only be 3 the resolution for DayTimeIntervalType can only be SECOND and the fractionalPrecision can only be 3 the resolution for YearMonthIntervalType can only be MONTH and the yearPrecision can only be 2 the CharType/BinaryType/ZonedTimestampType is not supported Monitoring # Converted all MetricReporters to plugins # FLINK-16963 # All MetricReporters that come with Flink have been converted to plugins. They should no longer be placed into /lib directory (doing so may result in dependency conflicts!), but /plugins/\u0026lt;some_directory\u0026gt; instead.
Changed of DataDog\u0026rsquo;s metric reporter Counter metrics # FLINK-15438 # The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. This aligns the count semantics with the DataDog documentation.
Switch to Log4j 2 by default # FLINK-15672 # Flink now uses Log4j2 by default. Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.
Changed behaviour of JobManager API\u0026rsquo;s log request # FLINK-16303 # Requesting an unavailable log or stdout file from the JobManager\u0026rsquo;s HTTP server returns status code 404 now. In previous releases, the HTTP server would return a file with (file unavailable) as its content.
Removal of lastCheckpointAlignmentBuffered metric # FLINK-16404 # Note that the metric lastCheckpointAlignmentBuffered has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. The web UI still displays this value but it is always 0 now.
Connectors # Dropped Kafka 0.8/0.9 connectors # FLINK-15115 # The Kafka 0.8 and 0.9 connectors are no longer under active development and were removed.
Dropped Elasticsearch 2.x connector # FLINK-16046 # The Elasticsearch 2 connector is no longer under active development and was removed. Prior version of these connectors will continue to work with Flink.
Removal of deprecated KafkaPartitioner # FLINK-15862 # Deprecated KafkaPartitioner was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.
Refined fallback filesystems to only handle specific filesystems # FLINK-16015 # By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. Added fs.allowed-fallback-filesystems configuration option to override this behaviour.
Deprecation of FileSystem#getKind # FLINK-16400 # org.apache.flink.core.fs.FileSystem#getKind method has been formally deprecated, as it was not used by Flink.
Runtime # Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint # FLINK-17350 # Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail its Task (and job) immediately, regardless of the configuration parameters. Since Flink 1.5 such failures could be ignored by setting setTolerableCheckpointFailureNumber(...) or its deprecated setFailTaskOnCheckpointError(...) predecessor. Now both options will only affect asynchronous failures.
Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber (FLINK-17351) # Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by CheckpointConfig#setTolerableCheckpointFailureNumber(...).
Miscellaneous Interface Changes # Removal of deprecated StreamTask#getCheckpointLock() # FLINK-12484 # DataStream API no longer provides StreamTask#getCheckpointLock method, which was deprecated in Flink 1.10. Users should use MailboxExecutor to run actions that require synchronization with the task\u0026rsquo;s thread (e.g. collecting output produced by an external thread). MailboxExecutor#yield or MailboxExecutor#tryYield methods can be used for actions that should give control to other actions temporarily (equivalent of StreamTask#getCheckpointLock().wait()), if the current operator is blocked. MailboxExecutor can be accessed by using YieldingOperatorFactory. Example usage can be found in the AsyncWaitOperator.
Note, SourceFunction.SourceContext.getCheckpointLock is still available for custom implementations of SourceFunction interface.
Reversed dependency from flink-streaming-java to flink-client # FLINK-15090 # Starting from Flink 1.11.0, the flink-streaming-java module does not have a dependency on flink-clients anymore. If your project was depending on this transitive dependency you now have to add flink-clients as an explicit dependency.
AsyncWaitOperator is chainable again # FLINK-16219 # AsyncWaitOperator will be allowed to be chained by default with all operators, except of tasks with SourceFunction. This mostly revert limitation introduced as a bug fix for FLINK-13063.
Changed argument types of ShuffleEnvironment#createInputGates and #createResultPartitionWriters methods # FLINK-16586 # The argument type of methods ShuffleEnvironment#createInputGates and #createResultPartitionWriters are adjusted from Collection to List for satisfying the order guarantee requirement in unaligned checkpoint. It will break the compatibility if users already implemented a custom ShuffleService based on ShuffleServiceFactory interface.
Deprecation of CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible # FLINK-17520 # The boolean isOuterSnapshotCompatible(TypeSerializer) on the CompositeTypeSerializerSnapshot class has been deprecated, in favor of a new OuterSchemaCompatibility resolveOuterSchemaCompatibility(TypeSerializer) method. Please implement that instead. Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.
Removal of deprecated TimestampExtractor # FLINK-17655 # The long-deprecated TimestampExtractor was removed along with API methods in the DataStream API. Please use the new TimestampAssigner and WatermarkStrategies for working with timestamps and watermarks in the DataStream API.
Deprecation of ListCheckpointed interface # FLINK-6258 # The ListCheckpointed interface has been deprecated because it uses Java Serialization for checkpointing state which is problematic for savepoint compatibility. Use the CheckpointedFunction interface instead, which gives more control over state serialization.
Removal of deprecated state access methods # FLINK-17376 # We removed deprecated state access methods RuntimeContext#getFoldingState(), OperatorStateStore#getSerializableListState() and OperatorStateStore#getOperatorState(). This means that some code that was compiled against Flink 1.10 will not work with a Flink 1.11 cluster. An example of this is our Kafka connector which internally used OperatorStateStore.getSerializableListState.
`}),e.add({id:318,href:"/flink/flink-docs-master/release-notes/flink-1.12/",title:"Release Notes - Flink 1.12",section:"Release-notes",content:` Release Notes - Flink 1.12 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.11 and Flink 1.12. Please read these notes carefully if you are planning to upgrade your Flink version to 1.12.
Known Issues # Unaligned checkpoint recovery may lead to corrupted data stream # FLINK-20654 # Using unaligned checkpoints in Flink 1.12.0 combined with two/multiple inputs tasks or with union inputs for single input tasks can result in corrupted state.
This can happen if a new checkpoint is triggered before recovery is fully completed. For state to be corrupted a task with two or more input gates must receive a checkpoint barrier exactly at the same time this tasks finishes recovering spilled in-flight data. In such case this new checkpoint can succeed, with corrupted/missing in-flight data, which will result in various deserialisation/corrupted data stream errors when someone attempts to recover from such corrupted checkpoint.
Using unaligned checkpoints in Flink 1.12.1, a corruption may occur in the checkpoint following a declined checkpoint.
A late barrier of a canceled checkpoint may lead to buffers being not written into the successive checkpoint, such that recovery is not possible. This happens, when the next checkpoint barrier arrives at a given operator before all previous barriers arrived, which can only happen after cancellation in unaligned checkpoints.
APIs # Remove deprecated methods in ExecutionConfig # FLINK-19084 # Deprecated method ExecutionConfig#isLatencyTrackingEnabled was removed, you can use ExecutionConfig#getLatencyTrackingInterval instead.
Deprecated and methods without effect were removed: ExecutionConfig#enable/disableSysoutLogging, ExecutionConfig#set/isFailTaskOnCheckpointError.
Removed -q flag from cli. The option had no effect.
Remove deprecated RuntimeContext#getAllAccumulators # FLINK-19032 # The deprecated method RuntimeContext#getAllAccumulators was removed. Please use RuntimeContext#getAccumulator instead.
Deprecated CheckpointConfig#setPreferCheckpointForRecovery due to risk of data loss # FLINK-20441 # The CheckpointConfig#setPreferCheckpointForRecovery method has been deprecated, because preferring older checkpoints over newer savepoints for recovery can lead to data loss.
FLIP-134: Batch execution for the DataStream API # Allow explicitly configuring time behaviour on KeyedStream.intervalJoin() FLINK-19479
Before Flink 1.12 the KeyedStream.intervalJoin() operation was changing behavior based on the globally set Stream TimeCharacteristic. In Flink 1.12 we introduced explicit inProcessingTime() and inEventTime() methods on IntervalJoin and the join no longer changes behaviour based on the global characteristic.
Deprecate timeWindow() operations in DataStream API FLINK-19318
In Flink 1.12 we deprecated the timeWindow() operations in the DataStream API. Please use window(WindowAssigner) with either a TumblingEventTimeWindows, SlidingEventTimeWindows, TumblingProcessingTimeWindows, or SlidingProcessingTimeWindows. For more information, see the deprecation description of TimeCharacteristic/setStreamTimeCharacteristic.
Deprecate StreamExecutionEnvironment.setStreamTimeCharacteristic() and TimeCharacteristic FLINK-19319
In Flink 1.12 the default stream time characteristic has been changed to EventTime, thus you don\u0026rsquo;t need to call this method for enabling event-time support anymore. Explicitly using processing-time windows and timers works in event-time mode. If you need to disable watermarks, please use ExecutionConfig.setAutoWatermarkInterval(long). If you are using IngestionTime, please manually set an appropriate WatermarkStrategy. If you are using generic \u0026ldquo;time window\u0026rdquo; operations (for example KeyedStream.timeWindow()) that change behaviour based on the time characteristic, please use equivalent operations that explicitly specify processing time or event time.
Allow explicitly configuring time behaviour on CEP PatternStream FLINK-19326
Before Flink 1.12 the CEP operations were changing their behavior based on the globally set Stream TimeCharacteristic. In Flink 1.12 we introduced explicit inProcessingTime() and inEventTime() methods on PatternStream and the CEP operations no longer change their behaviour based on the global characteristic.
API cleanups # Remove remaining UdfAnalyzer configurations FLINK-13857
The ExecutionConfig#get/setCodeAnalysisMode method and SkipCodeAnalysis class were removed. They took no effect even before that change, therefore there is no need to use any of these.
Remove deprecated DataStream#split FLINK-19083
The DataStream#split() operation has been removed after being marked as deprecated for a couple of versions. Please use Side Outputs) instead.
Remove deprecated DataStream#fold() method and all related classes FLINK-19035
The long deprecated (Windowed)DataStream#fold was removed in 1.12. Please use other operations such as e.g. (Windowed)DataStream#reduce that perform better in distributed systems.
Extend CompositeTypeSerializerSnapshot to allow composite serializers to signal migration based on outer configuration # FLINK-17520 # The boolean isOuterSnapshotCompatible(TypeSerializer) on the CompositeTypeSerializerSnapshot class has been deprecated, in favor of a new OuterSchemaCompatibility#resolveOuterSchemaCompatibility(TypeSerializer) method. Please implement that instead. Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.
Bump Scala Macros Version to 2.1.1 # FLINK-19278 # Flink now relies on Scala Macros 2.1.1. This means that we no longer support Scala \u0026lt; 2.11.11.
SQL # Use new type inference for SQL DDL of aggregate functions # FLINK-18901 # The CREATE FUNCTION DDL for aggregate functions uses the new type inference now. It might be necessary to update existing implementations to the new reflective type extraction logic. Use StreamTableEnvironment.registerFunction for the old stack.
Update parser module for FLIP-107 # FLINK-19273 # The term METADATA is a reserved keyword now. Use backticks to escape column names and other identifiers with this name.
Update internal aggregate functions to new type system # FLINK-18809 # SQL queries that use the COLLECT function might need to be updated to the new type system.
Connectors and Formats # Remove Kafka 0.10.x and 0.11.x connectors # FLINK-19152 # In Flink 1.12 we removed the Kafka 0.10.x and 0.11.x connectors. Please use the universal Kafka connector which works with any Kafka cluster version after 0.10.2.x.
Please refer to the documentation to learn about how to upgrade the Flink Kafka Connector version.
Csv Serialization schema contains line delimiter # FLINK-19868 # The csv.line-delimiter option has been removed from CSV format. Because the line delimiter should be defined by the connector instead of format. If users have been using this option in previous Flink version, they should alter such table to remove this option when upgrading to Flink 1.12. There should not much users using this option.
Upgrade to Kafka Schema Registry Client 5.5.0 # FLINK-18546 # The flink-avro-confluent-schema-registry module is no longer provided as a fat-jar. You should include its dependencies in your job\u0026rsquo;s fat-jar. Sql-client users can use flink-sql-avro-confluent-schema-registry fat jar.
Upgrade to Avro version 1.10.0 from 1.8.2 # FLINK-18192 # The default version of Avro in flink-avro module was upgraded to 1.10. If for some reason you need an older version (you have Avro coming from Hadoop, or you use classes generated from an older Avro version), please explicitly downgrade the Avro version in your project.
NOTE: We observed a decreased performance of the Avro 1.10 version compared to 1.8.2. If you are concerned with the performance and you are fine working with an older version of Avro, consider downgrading the Avro version.
Create an uber jar when packaging flink-avro for SQL Client # FLINK-18802 # The SQL Client jar was renamed to flink-sql-avro-1.16-SNAPSHOT.jar, previously flink-avro-1.16-SNAPSHOT-sql-jar.jar. Moreover it is no longer needed to add Avro dependencies manually.
Deployment # Default log4j configuration rolls logs after reaching 100 megabytes # FLINK-8357 # The default log4j configuration has changed: Besides the existing rolling of log files on startup of Flink, they also roll once they\u0026rsquo;ve reached a size of 100MB. Flink keeps a total of 10 log files, effectively limiting the total size of the log directory to 1GB (per Flink service logging to that directory).
Use jemalloc by default in the Flink docker image # FLINK-19125 # jemalloc is adopted as the default memory allocator in Flink\u0026rsquo;s docker image to reduce issues with memory fragmentation. Users can roll back to using glibc by passing the \u0026lsquo;disable-jemalloc\u0026rsquo; flag to the docker-entrypoint.sh script. For more details, please refer to the Flink on Docker documentation.
Upgrade Mesos version to 1.7 # FLINK-19783 # The Mesos dependency has been bumped from 1.0.1 to 1.7.0.
Send SIGKILL if Flink process doesn\u0026rsquo;t stop after a timeout # FLINK-17470 # In Flink 1.12 we changed the behavior of the standalone scripts to issue a SIGKILL if a SIGTERM did not succeed in shutting down a Flink process.
Introduce non-blocking job submission # FLINK-16866 # The semantics of submitting a job have slightly changed. The submission call returns almost immediately, with the job being in a new INITIALIZING state. Operations such as triggering a savepoint or retrieving the full job details are not available while the job is in that state.
Once the JobManager for that job has been created, the job is in CREATED state and all calls are available.
Runtime # FLIP-141: Intra-Slot Managed Memory Sharing # The configuration python.fn-execution.buffer.memory.size and python.fn-execution.framework.memory.size have been removed and so will not take effect any more. Besides, the default value of python.fn-execution.memory.managed has been changed to true and so managed memory will be used by default for Python workers. In cases where Python UDFs are used together with the RocksDB state backend in streaming or built-in batch algorithms in batch, the user can control how managed memory should be shared between data processing (RocksDB state backend or batch algorithms) and Python, by overwriting managed memory consumer weights.
FLIP-119 Pipelined Region Scheduling # FLINK-16430 # Beginning from Flink 1.12, jobs will be scheduled in the unit of pipelined regions. A pipelined region is a set of pipelined connected tasks. This means that, for streaming jobs which consist of multiple regions, it no longer waits for all tasks to acquire slots before starting to deploy tasks. Instead, any region can be deployed once it has acquired enough slots for within tasks. For batch jobs, tasks will not be assigned slots and get deployed individually. Instead, a task will be deployed together with all other tasks in the same region, once the region has acquired enough slots.
The old scheduler can be enabled using the jobmanager.scheduler.scheduling-strategy: legacy setting.
`}),e.add({id:319,href:"/flink/flink-docs-master/release-notes/flink-1.13/",title:"Release Notes - Flink 1.13",section:"Release-notes",content:` Release Notes - Flink 1.13 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.12 and Flink 1.13. Please read these notes carefully if you are planning to upgrade your Flink version to 1.13.
Failover # Remove state.backend.async option. # FLINK-21935 # The state.backend.async option is deprecated. Snapshots are always asynchronous now (as they were by default before) and there is no option to configure a synchronous snapshot any more.
The constructors of FsStateBackend and MemoryStateBackend that take a flag for sync/async snapshots are kept for API compatibility, but the flags are ignored now.
Disentangle StateBackends from Checkpointing # FLINK-19463 # Flink has always separated local state storage from fault tolerance. Keyed state is maintained locally in state backends, either on the JVM heap or in embedded RocksDB instances. Fault tolerance comes from checkpoints and savepoints - periodic snapshots of a job\u0026rsquo;s internal state to some durable file system - such as Amazon S3 or HDFS.
Historically, Flink\u0026rsquo;s StateBackend interface intermixed these concepts in a way that confused many users. In 1.13, checkpointing configurations have been extracted into their own interface, CheckpointStorage.
This change does not affect the runtime behavior and simply provides a better mental model to users. Pipelines can be updated to use the new the new abstractions without losing state, consistency, or change in semantics.
Please follow the migration guide or the JavaDoc on the deprecated state backend classes - MemoryStateBackend, FsStateBackend and RocksDBStateBackend for migration details.
Unify binary format for Keyed State savepoints # FLINK-20976 # Flink’s savepoint binary format is unified across all state backends. That means you can take a savepoint with one state backend and then restore it using another.
If you want to switch the state backend you should first upgrade your Flink version to 1.13, then take a savepoint with the new version, and only after that, you can restore it with a different state backend.
FailureRateRestartBackoffTimeStrategy allows one less restart than configured # FLINK-20752 # The Failure Rate Restart Strategy was allowing 1 less restart per interval than configured. Users wishing to keep the current behavior should reduce the maximum number of allowed failures per interval by 1.
Support rescaling for Unaligned Checkpoints # FLINK-17979 # While recovering from unaligned checkpoints, users can now change the parallelism of the job. This change allows users to quickly upscale the job under backpressure.
SQL # Officially deprecate the legacy planner # FLINK-21709 # The old planner of the Table \u0026amp; SQL API is deprecated and will be dropped in Flink 1.14. This means that both the BatchTableEnvironment and DataSet API interop are reaching end of life. Use the unified TableEnvironment for batch and stream processing with the new planner, or the DataStream API in batch execution mode.
Use TIMESTAMP_LTZ as return type for function PROCTIME() # FLINK-21714 # Before Flink 1.13, the function return type of PROCTIME() is TIMESTAMP, and the return value is the TIMESTAMP in UTC time zone, e.g. the wall-clock shows 2021-03-01 12:00:00 at Shanghai, however the PROCTIME() displays 2021-03-01 04:00:00 which is wrong. Flink 1.13 fixes this issue and uses TIMESTAMP_LTZ type as return type of PROCTIME(), users don\u0026rsquo;t need to deal time zone problems anymore.
Support defining event time attribute on TIMESTAMP_LTZ column # FLINK-20387 # Support defining event time attribute on TIMESTAMP_LTZ column, base on this, Flink SQL gracefully support the Daylight Saving Time.
Correct function CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/LOCALTIME/LOCALTIMESTAMP/NOW() # FLINK-21713 # The value of time function CURRENT_TIMESTAMP and NOW() are corrected from UTC time with TIMESTAMP type to epoch time with TIMESTAMP_LTZ type. Time function LOCALTIME, LOCALTIMESTAMP, CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP and NOW() are corrected from evaluates for per record in batch mode to evaluate once at query-start for batch job.
Disable problematic cast conversion between NUMERIC type and TIMESTAMP type # FLINK-21698 # The CAST operation between NUMERIC type and TIMESTAMP type is problematic and is disabled now, e.g. CAST(numeric AS TIMESTAMP(3)) is disabled and should use TO_TIMESTAMP(FROM_UNIXTIME(numeric)) instead.
Support USE MODULES syntax # FLINK-21298 # The term MODULES is a reserved keyword now. Use backticks to escape column names and other identifiers with this name.
Update TableResult.collect()/TableResult.print() to the new type system # FLINK-20613 # Table.execute().collect() might return slightly different results for column types and row kind. The most important differences include:
Structured types are represented as POJOs of the original class and not Row anymore. Raw types are serialized according to the configuration in TableConfig. Add new StreamTableEnvironment.fromDataStream # FLINK-19977 # StreamTableEnvironment.fromDataStream has slightly different semantics now because it has been integrated into the new type system. Esp. row fields derived from composite type information might be in a different order compared to 1.12. The old behavior is still available via the overloaded method that takes expressions like fromDataStream(ds, \$(\u0026quot;field1\u0026quot;), \$(\u0026quot;field2\u0026quot;)).
Update the Row.toString method # FLINK-18090 # The Row.toSting() method has been reworked. This is an incompatible change. If the legacy representation is still required for tests, the old behavior can be restored via the flag RowUtils.USE_LEGACY_TO_STRING for the local JVM. However, relying on the row\u0026rsquo;s string representation for tests is not a good idea in general as field data types are not verified.
Support start SQL Client with an initialization SQL file # FLINK-20320 # The sql-client-defaults.yaml YAML file is deprecated and not provided in the release package. To be compatible, it\u0026rsquo;s still supported to initialize the SQL Client with the YAML file if manually provided. But it\u0026rsquo;s recommend to use the new introduced -i startup option to execute an initialization SQL file to setup the SQL Client session. The so-called initialization SQL file can use Flink DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment. The support of legacy SQL Client YAML file will be totally dropped in Flink 1.14.
Hive dialect no longer supports Flink syntax for DML and DQL # FLINK-21808 # Hive dialect supports HiveQL for DML and DQL. Please switch to default dialect in order to write in Flink syntax.
Runtime # BoundedOneInput.endInput is called when taking synchronous savepoint # FLINK-21132 # endInput() is not called anymore (on BoundedOneInput and BoundedMultiInput) when the job is stopping with savepoint.
Remove JobManagerOptions.SCHEDULING_STRATEGY # FLINK-20591 # The configuration parameter jobmanager.scheduler.scheduling-strategy has been removed, because the legacy scheduler has been removed from Flink 1.13.0.
Warn user if System.exit() is called in user code # FLINK-15156 # A new configuration value cluster.intercept-user-system-exit allows to log a warning, or throw an exception if user code calls System.exit().
This feature is not covering all locations in Flink where user code is executed. It just adds the infrastructure for such an interception. We are tracking this improvement in FLINK-21307.
MiniClusterJobClient#getAccumulators was infinitely blocking in local environment for a streaming job # FLINK-18685 # The semantics for accumulators have now changed in MiniClusterJobClient to fix this bug and comply with other JobClient implementations: Previously MiniClusterJobClient assumed that getAccumulator() was called on a bounded pipeline and that the user wanted to acquire the final accumulator values after the job is finished. But now it returns the current value of accumulators immediately to be compatible with unbounded pipelines.
If it is run on a bounded pipeline, then to get the final accumulator values after the job is finished, one needs to call
getJobExecutionResult().thenApply(JobExecutionResult::getAllAccumulatorResults)
Docker # Consider removing automatic configuration fo number of slots from docker # FLINK-21036 # The docker images no longer set the default number of taskmanager slots to the number of CPU cores. This behavior was inconsistent with all other deployment methods and ignored any limits on the CPU usage set via docker.
Rework jemalloc switch to use an environment variable # FLINK-21034 # The docker switch for disabling the jemalloc memory allocator has been reworked from a script argument to an environment variable called DISABLE_JEMALLOC. If set to \u0026ldquo;true\u0026rdquo; jemalloc will not be enabled.
Connectors # Remove swift FS filesystem # FLINK-21819 # The Swift filesystem is no longer being actively developed and has been removed from the project and distribution.
FLINK-22133 # The unified source API for connectors has a minor breaking change. The SplitEnumerator.snapshotState() method was adjusted to accept the Checkpoint ID of the checkpoint for which the snapshot is created.
Monitoring \u0026amp; debugging # Introduce latency tracking state # FLINK-21736 # State access latency metrics are introduced to track all kinds of keyed state access to help debug state performance. This feature is not enabled by default and can be turned on by setting state.backend.latency-track.keyed-state-enabled to true.
Support for CPU flame graphs in web UI # FLINK-13550 # Flink now offers flame graphs for each node in the job graph. Please enable this experimental feature by setting the respective configuration flag rest.flamegraph.enabled.
Display last n exceptions/causes for job restarts in Web UI # FLINK-6042 # Flink exposes the exception history now through the REST API and the UI. The amount of most-recently handled exceptions that shall be tracked can be defined through web.exception-history-size. Some values of the exception history\u0026rsquo;s REST API Json response are deprecated as part of this effort.
Create backPressuredTimeMsPerSecond metric # FLINK-20717 # Previously idleTimeMsPerSecond was defined as the time task spent waiting for either the input or the back pressure. Now idleTimeMsPerSecond excludes back pressured time, so if the task is back pressured it is not idle. The back pressured time is now measured separately as backPressuredTimeMsPerSecond.
Enable log4j2 monitor interval by default # FLINK-20510 # The Log4j support for updating the Log4j configuration at runtime has been enabled by default. The configuration files are checked for changes every 30 seconds.
ZooKeeper quorum fails to start due to missing log4j library # FLINK-20404 # The Zookeeper scripts in the Flink distribution have been modified to disable the Log4j JMX integration due to an incompatibility between Zookeeper 3.4 and Log4j 2. To re-enable this feature, remove the line in the zookeeper.sh file that sets zookeeper.jmx.log4j.disable.
Expose stage of task initialization # FLINK-17012 # Task\u0026rsquo;s RUNNING state was split into two states: INITIALIZING and RUNNING. Task is INITIALIZING while state is initialising and in case of unaligned checkpoints, until all the in-flight data has been recovered.
Deployment # Officially deprecate Mesos support # FLINK-22352 # The community decided to deprecate the Apache Mesos support for Apache Flink. It is subject to removal in the future. Users are encouraged to switch to a different resource manager.
`}),e.add({id:320,href:"/flink/flink-docs-master/release-notes/flink-1.14/",title:"Release Notes - Flink 1.14",section:"Release-notes",content:` Release notes - Flink 1.14 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.13 and Flink 1.14. Please read these notes carefully if you are planning to upgrade your Flink version to 1.14.
Known issues # State migration issues # Some of our internal serializers i.e. RowSerializer, TwoPhaseCommitSinkFunction\u0026rsquo;s serializer, LinkedListSerializer might prevent a successful job starts if state migration is necessary. The fix is tracked by FLINK-24858. We recommend to immediately upgrade to 1.14.3 when migrating from 1.13.x.
Memory leak with Pulsar connector on Java 11 # Netty, which the Pulsar client uses underneath, allocates memory differently on Java 11 and Java 8. On Java 11, it will allocate memory from the pool of Java Direct Memory and is affected by the MaxDirectMemory limit. The current Pulsar client has no configuration options for controlling the memory limits, which can lead to OOM(s).
Users are advised to use the Pulsar connector with Java 8 or overprovision memory for Flink. Read the memory setup guide on how to configure memory for Flink and track the proper solution in FLINK-24302.
Summary of changed dependency names # There are two changes in Flink 1.14 that require updating dependency names when upgrading from earlier versions.
The removal of the Blink planner (FLINK-22879) requires the removal of the blink infix. Due to FLINK-14105, if you have a dependency on flink-runtime, flink-optimizer and/or flink-queryable-state-runtime, the Scala suffix (_2.11/_2.12) needs to be removed from the artifactId. Table API \u0026amp; SQL # Use pipeline name consistently across DataStream API and Table API # FLINK-23646 # The default job name for DataStream API programs in batch mode has changed from \u0026quot;Flink Streaming Job\u0026quot; to \u0026quot;Flink Batch Job\u0026quot;. A custom name can be set with the config option pipeline.name.
Propagate unique keys for fromChangelogStream # FLINK-24033 # Compared to 1.13.2, StreamTableEnvironment.fromChangelogStream might produce a different stream because primary keys were not properly considered before.
Support new type inference for Table#flatMap # FLINK-16769 # Table.flatMap() supports the new type system now. Users are requested to upgrade their functions.
Add Scala implicit conversions for new API methods # FLINK-22590 # The Scala implicits that convert between DataStream API and Table API have been updated to the new methods of FLIP-136.
The changes might require an update of pipelines that used toTable or implicit conversions from Table to DataStream[Row].
Remove YAML environment file support in SQL Client # FLINK-22540 # The sql-client-defaults.yaml file was deprecated in the 1.13 release and is now completely removed. As an alternative, you can use the -i startup option to execute an SQL initialization file to set up the SQL Client session. The SQL initialization file can use Flink DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment.
See more: https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/sqlclient/#initialize-session-using-sql-files
Remove the legacy planner code base # FLINK-22864 # The old Table/SQL planner has been removed. BatchTableEnvironment and DataSet API interoperability with Table API are not supported anymore. Use the unified TableEnvironment for batch and stream processing with the new planner or the DataStream API in batch execution mode.
Users are encouraged to update their pipelines. Otherwise Flink 1.13 is the last version that offers the old functionality.
Remove the \u0026ldquo;blink\u0026rdquo; suffix from table modules # FLINK-22879 # The following Maven modules have been renamed:
flink-table-planner-blink -\u0026gt; flink-table-planner flink-table-runtime-blink -\u0026gt; flink-table-runtime flink-table-uber-blink -\u0026gt; flink-table-uber It might be required to update job JAR dependencies. Note that flink-table-planner and flink-table-uber used to contain the legacy planner before Flink 1.14 and now they contain the only officially supported planner (i.e. previously known as \u0026lsquo;Blink\u0026rsquo; planner).
Remove BatchTableEnvironment and related API classes # FLINK-22877 # Due to the removal of BatchTableEnvironment, BatchTableSource and BatchTableSink have been removed as well. Use DynamicTableSource and DynamicTableSink instead. They support the old InputFormat and OutputFormat interfaces as runtime providers if necessary.
Remove TableEnvironment#connect # FLINK-23063 # The deprecated TableEnvironment.connect() method has been removed. Use the new TableEnvironment.createTemporaryTable(String, TableDescriptor) to create tables programmatically. Please note that this method only supports sources and sinks that comply with FLIP-95. This is also indicated by the new property design 'connector'='kafka' instead of 'connector.type'='kafka'.
Deprecate toAppendStream and toRetractStream # FLINK-23330 # The outdated variants of StreamTableEnvironment.{fromDataStream|toAppendStream|toRetractStream) have been deprecated. Use the (from|to)(Data|Changelog)Stream alternatives introduced in 1.13.
Remove old connectors and formats stack around descriptors # FLINK-23513 # The legacy versions of the SQL Kafka connector and SQL Elasticsearch connector have been removed together with their corresponding legacy formats. DDL or descriptors that still use 'connector.type=' or 'format.type=' options need to be updated to the new connector and formats available via the 'connector=' option.
Drop BatchTableSource/Sink HBaseTableSource/Sink and related classes # FLINK-22623 # The HBaseTableSource/Sink and related classes including various HBaseInputFormats and HBaseSinkFunction have been removed. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API (or vice versa) if necessary. The DataSet API is not supported anymore.
Drop BatchTableSource ParquetTableSource and related classes # FLINK-22622 # The ParquetTableSource and related classes including various ParquetInputFormats have been removed. Use the FileSystem connector with a Parquet format as a replacement. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API if necessary. The DataSet API is not supported anymore.
Drop BatchTableSource OrcTableSource and related classes # FLINK-22620 # The OrcTableSource and related classes (including OrcInputFormat) have been removed. Use the FileSystem connector with an ORC format as a replacement. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API if necessary. The DataSet API is not supported anymore.
Drop usages of BatchTableEnvironment and the old planner in Python # FLINK-22619 # The Python API does not offer a dedicated BatchTableEnvironment anymore. Instead, users can switch to the unified TableEnvironment for both batch and stream processing. Only the Blink planner (the only remaining planner in 1.14) is supported.
Migrate ModuleFactory to the new factory stack # FLINK-23720 # The LOAD/UNLOAD MODULE architecture for table modules has been updated to the new factory stack of FLIP-95. Users of this feature should update their ModuleFactory implementations.
Migrate Table API to new KafkaSink # FLINK-23639 # Table API/SQL now writes to Kafka with the new KafkaSink. When migrating from a query writing to Kafka in exactly-once mode from an earlier Flink version, make sure to terminate the old application with stop-with-savepoint to avoid lingering Kafka transactions. To run in exactly-once processing mode, the sink needs a user-configured and unique transaction prefix, such that transactions of different applications do not interfere with each other.
DataStream API # Fixed idleness handling for two/multi input operators # FLINK-18934 # FLINK-23767 # We added processWatermarkStatusX method to classes such as AbstractStreamOperator, Input etc. It allows to take the WatermarkStatus into account when combining watermarks in two/multi input operators.
Note that with this release, we renamed the previously internal StreamStatus to WatermarkStatus in order to better reflect its purpose.
Allow @TypeInfo annotation on POJO field declarations # FLINK-12141 # @TypeInfo annotations can now also be used on POJO fields which, for example, can help to define custom serializers for third-party classes that can otherwise not be annotated themselves.
Clarify SourceFunction#cancel() contract about interruptions # FLINK-23527 # Contract of the SourceFunction.cancel() method with respect to interruptions has been clarified:
source itself should not be interrupting the source thread interrupt should not be expected in the clean cancellation case Expose a consistent GlobalDataExchangeMode # FLINK-23402 # The default DataStream API shuffle mode for batch executions has been changed to blocking exchanges for all edges of the stream graph. A new option execution.batch-shuffle-mode allows you to change it to pipelined behavior if necessary.
Python API # Support loopback mode to allow Python UDF worker and client to reuse the same Python VM # FLINK-21222 # Instead of launching a separate Python process, the Python UDF worker will reuse the Python process of the client side when running jobs locally. This makes it easier to debug Python UDFs.
Support Python UDF chaining in Python DataStream API # FLINK-22913 # The job graph of Python DataStream API jobs may be different from before as Python functions will be chained as much as possible to optimize performance. You could disable Python functions chaining by explicitly setting python.operator-chaining.enabled as false.
Connectors # Expose standardized operator metrics (FLIP-179) # FLINK-23652 # Connectors using the unified Source and Sink interface will expose certain standardized metrics automatically. Applications that use RuntimeContext#getMetricGroup need to be rebuild against 1.14 before being submitted to a 1.14 cluster.
Port KafkaSink to new Unified Sink API (FLIP-143) # FLINK-22902 # KafkaSink supersedes FlinkKafkaProducer and provides efficient exactly-once and at-least-once writing with the new unified sink interface, supporting both batch and streaming mode of DataStream API. To upgrade, please stop with savepoint. To run in exactly-once processing mode, KafkaSink needs a user-configured and unique transaction prefix, such that transactions of different applications do not interfere with each other.
Deprecate FlinkKafkaConsumer # FLINK-24055 # FlinkKafkaConsumer has been deprecated in favor of KafkaSource. To upgrade to the new version, please store the offsets in Kafka with setCommitOffsetsOnCheckpoints in the old FlinkKafkaConsumer and then stop with a savepoint. When resuming from the savepoint, please use setStartingOffsets(OffsetsInitializer.committedOffsets()) in the new KafkaSourceBuilder to transfer the offsets to the new source.
InputStatus should not contain END_OF_RECOVERY # FLINK-23474 # InputStatus.END_OF_RECOVERY was removed. It was an internal flag that should never be returned from SourceReaders. Returning that value in earlier versions might lead to misbehavior.
Connector-base exposes dependency to flink-core. # FLINK-22964 # Connectors do not transitively hold a reference to flink-core anymore. That means that a fat JAR with a connector does not include flink-core with this fix.
Runtime \u0026amp; Coordination # Increase akka.ask.timeout for tests using the MiniCluster # FLINK-23906 # The default akka.ask.timeout used by the MiniCluster has been increased to 5 minutes. If you want to use a smaller value, then you have to set it explicitly in the passed configuration.
The change is due to the fact that messages can not get lost in a single-process MiniCluster, so this timeout (which otherwise helps to detect message loss in distributed setups) has no benefit here.
The increased timeout reduces the number of false-positive timeouts, for example, during heavy tests on loaded CI/CD workers or during debugging.
The node IP obtained in NodePort mode is a VIP # FLINK-23507 # When using kubernetes.rest-service.exposed.type=NodePort, the connection string for the REST gateway is now correctly constructed in the form \u0026lt;nodeIp\u0026gt;:\u0026lt;nodePort\u0026gt; instead of \u0026lt;kubernetesApiServerUrl\u0026gt;:\u0026lt;nodePort\u0026gt;. This may be a breaking change for some users.
This also introduces a new config option kubernetes.rest-service.exposed.node-port-address-type that lets you select \u0026lt;nodeIp\u0026gt; from a desired range.
Timeout heartbeat if the heartbeat target is no longer reachable # FLINK-23209 # Flink now supports detecting dead TaskManagers via the number of consecutive failed heartbeat RPCs. The threshold until a TaskManager is marked as unreachable can be configured via heartbeat.rpc-failure-threshold. This can speed up the detection of dead TaskManagers significantly.
RPCs fail faster when target is unreachable # FLINK-23202 # The same way Flink detects unreachable heartbeat targets faster, Flink now also immediately fails RPCs where the target is known by the OS to be unreachable on a network level, instead of waiting for a timeout (akka.ask.timeout).
This creates faster task failovers because cancelling tasks on a dead TaskExecutor no longer gets delayed by the RPC timeout.
If this faster failover is a problem in certain setups (which might rely on the fact that external systems hit timeouts), we recommend configuring the application\u0026rsquo;s restart strategy with a restart delay.
Changes in accounting of IOExceptions when triggering checkpoints on JobManager # FLINK-23189 # In previous versions, IOExceptions thrown from the JobManager would not fail the entire Job. We changed the way we track those exceptions and now they do increase the number of checkpoint failures.
The number of tolerable checkpoint failures can be adjusted or disabled via: org.apache.flink.streaming.api.environment.CheckpointConfig#setTolerableCheckpointFailureNumber (which is set to 0 by default).
Refine ShuffleMaster lifecycle management for pluggable shuffle service framework # FLINK-22910 # We improved the ShuffleMaster interface by adding some lifecycle methods, including open, close, registerJob and unregisterJob. Besides, the ShuffleMaster now becomes a cluster level service which can be shared by multiple jobs. This is a breaking change to the pluggable shuffle service framework and the customized shuffle plugin needs to adapt to the new interface accordingly.
Group job specific ZooKeeper HA services under common jobs/ zNode # FLINK-22636 # The ZooKeeper job-specific HA services are now grouped under a zNode with the respective JobID. Moreover, the config options high-availability.zookeeper.path.latch, high-availability.zookeeper.path.leader, high-availability.zookeeper.path.checkpoints, and high-availability.zookeeper.path.checkpoint-counter have been removed and, thus, no longer have an effect.
Fallback value for taskmanager.slot.timeout # FLINK-22002 # The config option taskmanager.slot.timeout now falls back to akka.ask.timeout if no value has been configured. Previously, the default value for taskmanager.slot.timeout was 10 s.
DuplicateJobSubmissionException after JobManager failover # FLINK-21928 # The fix for this problem only works if the ApplicationMode is used with a single job submission and if the user code does not access the JobExecutionResult. If any of these conditions is violated, then Flink cannot guarantee that the whole Flink application is executed.
Additionally, it is still required that the user cleans up the corresponding HA entries for the running jobs registry because these entries won\u0026rsquo;t be reliably cleaned up when encountering the situation described by FLINK-21928.
Zookeeper node under leader and leaderlatch is not deleted after job finished # FLINK-20695 # The HighAvailabilityServices interface has received a new method cleanupJobData which can be implemented in order to clean up job-related HA data after a given job has terminated.
Optimize scheduler performance for large-scale jobs # FLINK-21110 # The performance of the scheduler has been improved to reduce the time of execution graph creation, task deployment, and task failover. This improvement is significant to large scale jobs which currently may spend minutes on the processes mentioned above. This improvement also helps to avoid cases when the job manager main thread gets blocked for too long and leads to heartbeat timeout.
Checkpoints # The semantic of alignmentTimeout configuration has changed meaning # FLINK-23041 # The semantic of alignmentTimeout configuration has changed meaning and now it is measured as the time between the start of a checkpoint (on the checkpoint coordinator) and the time when the checkpoint barrier is received by a task.
Disable unaligned checkpoints for BROADCAST exchanges # FLINK-22815 # Broadcast partitioning can not work with unaligned checkpointing. There are no guarantees that records are consumed at the same rate in all channels. This can result in some tasks applying state changes corresponding to a certain broadcasted event while others do not. Upon restore, it may lead to an inconsistent state.
DefaultCompletedCheckpointStore drops unrecoverable checkpoints silently # FLINK-22502 # On recovery, if a failure occurs during retrieval of a checkpoint, the job is restarted (instead of skipping the checkpoint in some circumstances). This prevents potential consistency violations.
Remove the CompletedCheckpointRecover#recover() method. # FLINK-22483 # Flink no longer reloads checkpoint metadata from the external storage before restoring the task state after failover (except when the JobManager fails over / changes leadership). This results in less external I/O and faster failover.
Please note that this changes public interfaces around CompletedCheckpointStore, that we allow overriding by providing custom implementation of HA Services.
Remove the deprecated CheckpointConfig#setPreferCheckpointForRecovery method. # FLINK-20427 # The deprecated method CheckpointConfig#setPreferCheckpointForRecovery was removed, because preferring older checkpoints over newer savepoints for recovery can lead to data loss.
Dependency upgrades # Bump up RocksDb version to 6.20.3 # FLINK-14482 # RocksDB has been upgraded to 6.20.3. The new version contains lots of bug fixes, ARM platform support, musl library support, and more attractive features. However, the new version can entail at most 8% performance regression according to our tests.
See the corresponding ticket for more information.
Support configuration of the RocksDB info logging via configuration # FLINK-23812 # With RocksDB upgraded to 6.20.3 (FLINK-14482), you can now also configure a rolling info logging strategy by configuring it accordingly via the newly added state.backend.rocksdb.log.* settings. This can be helpful for debugging RocksDB (performance) issues in containerized environments where the local data directory is volatile but the logs should be retained on a separate volume mount.
Make flink-runtime scala-free # FLINK-14105 # Flink\u0026rsquo;s Akka dependency is now loaded with a separate classloader and no longer accessible from the outside.
As a result, various modules (most prominently, flink-runtime) no longer have a scala suffix in their artifactId.
Drop Mesos support # FLINK-23118 # The support for Apache Mesos has been removed.
`}),e.add({id:321,href:"/flink/flink-docs-master/release-notes/flink-1.15/",title:"Release Notes - Flink 1.15",section:"Release-notes",content:` Release notes - Flink 1.15 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.14 and Flink 1.15. Please read these notes carefully if you are planning to upgrade your Flink version to 1.15.
Summary of changed dependency names # There are Several changes in Flink 1.15 that require updating dependency names when upgrading from earlier versions, mainly including the effort to opting-out Scala dependencies from non-scala modules and reorganize table modules. A quick checklist of the dependency changes is as follows:
Any dependency to one of the following modules needs to be updated to no longer include a suffix:
flink-cep flink-clients flink-connector-elasticsearch-base flink-connector-elasticsearch6 flink-connector-elasticsearch7 flink-connector-gcp-pubsub flink-connector-hbase-1.4 flink-connector-hbase-2.2 flink-connector-hbase-base flink-connector-jdbc flink-connector-kafka flink-connector-kinesis flink-connector-nifi flink-connector-pulsar flink-connector-rabbitmq flink-container flink-dstl-dfs flink-gelly flink-hadoop-bulk flink-kubernetes flink-runtime-web flink-sql-connector-elasticsearch6 flink-sql-connector-elasticsearch7 flink-sql-connector-hbase-1.4 flink-sql-connector-hbase-2.2 flink-sql-connector-kafka flink-sql-connector-kinesis flink-sql-connector-rabbitmq flink-state-processor-api flink-statebackend-rocksdb flink-streaming-java flink-test-utils flink-yarn flink-table-api-java-bridge flink-table-runtime flink-sql-client flink-orc flink-orc-nohive flink-parquet For Table / SQL users, the new module flink-table-planner-loader replaces flink-table-planner_2.12 and avoids the need for a Scala suffix. For backwards compatibility, users can still swap it with flink-table-planner_2.12 located in opt/. flink-table-uber has been split into flink-table-api-java-uber, flink-table-planner(-loader), and flink-table-runtime. Scala users need to explicitly add a dependency to flink-table-api-scala or flink-table-api-scala-bridge.
The detail of the involved issues are listed as follows.
Add support for opting-out of Scala # FLINK-20845 # The Java DataSet/-Stream APIs are now independent of Scala and no longer transitively depend on it.
The implications are the following:
If you only intend to use the Java APIs, with Java types, then you can opt-in to a Scala-free Flink by removing the flink-scala jar from the lib/ directory of the distribution. You are then free to use any Scala version and Scala libraries. You can either bundle Scala itself in your user-jar; or put into the lib/ directory of the distribution.
If you relied on the Scala APIs, without an explicit dependency on them, then you may experience issues when building your projects. You can solve this by adding explicit dependencies to the APIs that you are using. This should primarily affect users of the Scala DataStream/CEP APIs.
A lot of modules have lost their Scala suffix. Further caution is advised when mixing dependencies from different Flink versions (e.g., an older connector), as you may now end up pulling in multiple versions of a single module (that would previously be prevented by the name being equal).
Reorganize table modules and introduce flink-table-planner-loader # FLINK-25128 # The new module flink-table-planner-loader replaces flink-table-planner_2.12 and avoids the need for a Scala suffix. It is included in the Flink distribution under lib/. For backwards compatibility, users can still swap it with flink-table-planner_2.12 located in opt/. As a consequence, flink-table-uber has been split into flink-table-api-java-uber, flink-table-planner(-loader), and flink-table-runtime. flink-sql-client has no Scala suffix anymore.
It is recommended to let new projects depend on flink-table-planner-loader (without Scala suffix) in provided scope.
Note that the distribution does not include the Scala API by default. Scala users need to explicitly add a dependency to flink-table-api-scala or flink-table-api-scala-bridge.
Remove flink-scala dependency from flink-table-runtime # FLINK-25114 # The flink-table-runtime has no Scala suffix anymore. Make sure to include flink-scala if the legacy type system (based on TypeInformation) with case classes is still used within Table API.
flink-table uber jar should not include flink-connector-files dependency # FLINK-24687 # The table file system connector is not part of the flink-table-uber JAR anymore but is a dedicated (but removable) flink-connector-files JAR in the /lib directory of a Flink distribution.
JDK Upgrade # The support of Java 8 is now deprecated and will be removed in a future release (FLINK-25247). We recommend all users to migrate to Java 11.
The default Java version in the Flink docker images is now Java 11 (FLINK-25251). There are images built with Java 8, tagged with “java8”.
Drop support for Scala 2.11 # Support for Scala 2.11 has been removed in FLINK-20845. All Flink dependencies that (transitively) depend on Scala are suffixed with the Scala version that they are built for, for example flink-streaming-scala_2.12. Users should update all Flink dependecies, changing \u0026ldquo;2.11\u0026rdquo; to \u0026ldquo;2.12\u0026rdquo;.
Scala versions (2.11, 2.12, etc.) are not binary compatible with one another. That also means that there\u0026rsquo;s no guarantee that you can restore from a savepoint, made with a Flink Scala 2.11 application, if you\u0026rsquo;re upgrading to a Flink Scala 2.12 application. This depends on the data types that you have been using in your application.
The Scala Shell/REPL has been removed in FLINK-24360.
Table API \u0026amp; SQL # Disable the legacy casting behavior by default # FLINK-26551 # The legacy casting behavior has been disabled by default. This might have implications on corner cases (string parsing, numeric overflows, to string representation, varchar/binary precisions). Set table.exec.legacy-cast-behaviour=ENABLED to restore the old behavior.
Enforce CHAR/VARCHAR precision when outputting to a Sink # FLINK-24753 # CHAR/VARCHAR lengths are enforced (trimmed/padded) by default now before entering the table sink.
Support the new type inference in Scala Table API table functions # FLINK-26518 # Table functions that are called using Scala implicit conversions have been updated to use the new type system and new type inference. Users are requested to update their UDFs or use the deprecated TableEnvironment.registerFunction to restore the old behavior temporarily by calling the function via name.
Propagate executor config to TableConfig # FLINK-26421 # flink-conf.yaml and other configurations from outer layers (e.g. CLI) are now propagated into TableConfig. Even though configuration set directly in TableConfig has still precedence, this change can have side effects if table configuration was accidentally set in other layers.
Remove pre FLIP-84 methods # FLINK-26090 # The previously deprecated methods TableEnvironment.execute, Table.insertInto, TableEnvironment.fromTableSource, TableEnvironment.sqlUpdate, and TableEnvironment.explain have been removed. Please use TableEnvironment.executeSql, TableEnvironment.explainSql, TableEnvironment.createStatementSet, as well as Table.executeInsert, Table.explain and Table.execute and the newly introduces classes TableResult, ResultKind, StatementSet and ExplainDetail.
Fix parser generator warnings # FLINK-26053 # STATEMENT is a reserved keyword now. Use backticks to escape tables, fields and other references.
Expose uid generator for DataStream/Transformation providers # FLINK-25990 # DataStreamScanProvider and DataStreamSinkProvider for table connectors received an additional method that might break implementations that used lambdas before. We recommend static classes as a replacement and future robustness.
Add new STATEMENT SET syntax # FLINK-25392 # It is recommended to update statement sets to the new SQL syntax:
EXECUTE STATEMENT SET BEGIN ... END; EXPLAIN STATEMENT SET BEGIN ... END; Check \u0026amp; possible fix decimal precision and scale for all Aggregate functions # FLINK-24809 # This changes the result of a decimal SUM() with retraction and AVG(). Part of the behavior is restored back to be the same with 1.13 so that the behavior as a whole could be consistent with Hive / Spark.
Clarify semantics of DecodingFormat and its data type # FLINK-24776 # The DecodingFormat interface was used for both projectable and non-projectable formats which led to inconsistent implementations. The FileSystemTableSource has been updated to distinguish between those two interfaces now. Users that implement custom formats for FileSystemTableSource might need to verify the implementation and make sure to implement ProjectableDecodingFormat if necessary.
Push down partitions before filters # FLINK-24717 # This might have an impact on existing table source implementations as push down filters might not contain partition predicates anymore. However, the connector implementation for table sources that implement both partition and filter push down became easier with this change.
Flink SQL SUM() causes a precision error # FLINK-24691 # This changes the result of a decimal SUM() between 1.14.0 and 1.14.1. It restores the behavior of 1.13 to be consistent with Hive/Spark.
Use the new casting rules in TableResult#print # FLINK-24685 # The string representation of BOOLEAN columns from DDL results (true/false -\u0026gt; TRUE/FALSE), and row columns in DQL results (+I[...] -\u0026gt; (...)) has changed for printing.
Casting from a string to a DATE and TIME allows incomplete strings # FLINK-24421 # The defaults for casting incomplete strings like \u0026quot;12\u0026quot; to TIME have changed from 12:01:01 to 12:00:00.
Casting from STRING to TIMESTAMP_LTZ looses fractional seconds # FLINK-24446 # STRING to TIMESTAMP(_LTZ) casting now considers fractional seconds. Previously fractional seconds of any precision were ignored.
Sinks built with the unified sink framework do not receive timestamps when used in Table API # FLINK-24608 # This adds an additional operator to the topology if the new sink interfaces are used (e.g. for Kafka). It could cause issues in 1.14.1 when restoring from a 1.14 savepoint. A workaround is to cast the time attribute to a regular timestamp in the SQL statement closely before the sink.
SQL functions should return STRING instead of VARCHAR(2000) # FLINK-24586 # Functions that returned VARCHAR(2000) in 1.14, return VARCHAR with maximum length now. In particular this includes:
SON_VALUE CHR REVERSE SPLIT_INDEX REGEXP_EXTRACT PARSE_URL FROM_UNIXTIME DECODE DATE_FORMAT CONVERT_TZ Support IS JSON for Table API # FLINK-16501 # This issue added IS JSON for Table API. Notes that IS JSON does not return NULL anymore but always FALSE (even if the argument is NULL).
Disable upsert into syntax in Flink SQL # FLINK-22942 # Disabled UPSERT INTO statement. UPSERT INTO syntax was exposed by mistake in previous releases without detailed discussed. From this release every UPSERT INTO is going to throw an exception. Users of UPSERT INTO should use the documented INSERT INTO statement instead.
RuntimeException: while resolving method \u0026lsquo;booleanValue\u0026rsquo; in class class java.math.BigDecimal # FLINK-23271 # Casting to BOOLEAN is not allowed from decimal numeric types anymore.
Upsert materializer is not inserted for all sink providers # FLINK-23895 # This issue aims to fix various primary key issues that effectively made it impossible to use this feature. The change might affect savepoint backwards compatibility for those incorrect pipelines. Also the resulting changelog stream might be different after these changes. Pipelines that were correct before should be restorable from a savepoint.
Propagate unique keys for fromChangelogStream # FLINK-24033 # StreamTableEnvironment.fromChangelogStream might produce a different stream because primary keys were not properly considered before.
TableResult#print() should use internal data types # FLINK-24461 # The results of Table#print have changed to be closer to actual SQL data types. E.g. decimal is printing correctly with leading/trailing zeros.
Connectors # Remove MapR filesystem # FLINK-25553 # Support for the MapR FileSystem has been dropped.
Merge flink-connector-testing into flink-connector-test-utils # FLINK-25712 # The flink-connector-testing module has been removed and users should use flink-connector-test-utils module instead.
Support partition keys through metadata (for FileSystem connector) # FLINK-24617 # Now the formats implementing BulkWriterFormatFactory don\u0026rsquo;t need to implement partition keys reading anymore, as it\u0026rsquo;s managed internally by FileSystemTableSource.
Port ElasticSearch Sink to new Unified Sink API (FLIP-143) # FLINK-24323 # ElasticsearchXSinkBuilder supersedes ElasticsearchSink.Builder and provides at-least-once writing with the new unified sink interface supporting both batch and streaming mode of DataStream API.
For Elasticsearch 7 users that use the old ElasticsearchSink interface (org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink) and depend on their own elasticsearch-rest-high-level-client version, updating the client dependency to a version \u0026gt;= 7.14.0 is required due to internal changes.
Reduce legacy in Table API connectors # FLINK-24397 # The old JDBC connector (indicated by connector.type=jdbc in DDL) has been removed. If not done already, users need to upgrade to the newer stack (indicated by connector=jdbc in DDL).
Extensible unified Sink uses new metric to capture outgoing records # FLINK-26126 # New metrics numRecordsSend and numRecordsSendErrors have been introduced for users to monitor the number of records sent to the external system. The numRecordsOut should be used to monitor the number of records transferred between sink tasks.
Connector developers should pay attention to the usage of these metrics numRecordsOut, numRecordsSend and numRecordsSendErrors while building sink connectors. Please refer to the new Kafka Sink for details. Additionally, since numRecordsOut now only counts the records sent between sink tasks and numRecordsOutErrors was designed for counting the records sent to the external system, we deprecated numRecordsOutErrors and recommend using numRecordsSendErrors instead.
Runtime \u0026amp; Coordination # Integrate retry strategy for cleanup stage # FLINK-25433 # Adds retry logic to the cleanup steps of a finished job. This feature changes the way Flink jobs are cleaned up. Instead of trying once to clean up the job, this step will be repeated until it succeeds. Users are meant to fix the issue that prevents Flink from finalizing the job cleanup. The retry functionality can be configured and disabled. More details can be found in the documentation.
Introduce explicit shutdown signaling between TaskManager and JobManager # FLINK-25277 # TaskManagers now explicitly send a signal to the JobManager when shutting down. This reduces the down-scaling delay in reactive mode (which was previously bound to the heartbeat timeout).
Release TaskManagerJobMetricGroup with the last slot rather than task # FLINK-24864 # Job metrics on the TaskManager are now removed when the last slot is released, rather than the last task. This means they may be reported for a longer time than before and when no tasks are running on the TaskManager.
Make errors happened during JobMaster initialization accessible through the exception history # FLINK-25096 # Fixes issue where the failover is not listed in the exception history but as a root cause. That could have happened if the failure occurred during JobMaster initialization.
DispatcherResourceManagerComponent fails to deregister application if no leading ResourceManager # FLINK-24038 # A new multiple component leader election service was implemented that only runs a single leader election per Flink process. If this should cause any problems, then you can set high-availability.use-old-ha-services: true in the flink-conf.yaml to use the old high availability services.
Allow idempotent job cancellation # FLINK-24275 # Attempting to cancel a FINISHED/FAILED job now returns 409 Conflict instead of 404 Not Found.
Move async savepoint operation cache into Dispatcher # FLINK-18312 # All JobManagers can now be queried for the status of a savepoint operation, irrespective of which JobManager received the initial request.
Standby per job mode Dispatchers don\u0026rsquo;t know job\u0026rsquo;s JobSchedulingStatus # FLINK-11813 # The issue of re-submitting a job in Application Mode when the job finished but failed during cleanup is fixed through the introduction of the new component JobResultStore which enables Flink to persist the cleanup state of a job to the file system. (see FLINK-25431)
Change some default config values of blocking shuffle for better usability # FLINK-25636 # Since 1.15, sort-shuffle has become the default blocking shuffle implementation and shuffle data compression is enabled by default. These changes influence batch jobs only, for more information, please refer to the official document.
Checkpoints # FLIP-193: Snapshots ownership # FLINK-25154 # When restoring from a savepoint or retained externalized checkpoint you can choose the mode in which you want to perform the operation. You can choose from CLAIM, NO_CLAIM, LEGACY (the old behavior).
In CLAIM mode Flink takes ownership of the snapshot and will potentially try to remove the snapshot at a certain point in time. On the other hand the NO_CLAIM mode will make sure Flink does not depend on the existence of any files belonging to the initial snapshot.
For a more thorough description see the documentation.
Support native savepoints (w/o modifying the statebackend specific snapshot strategies) # FLINK-25744 # When taking a savepoint you can specify the binary format. You can choose from native (specific to a particular state backend) or canonical (unified across all state backends).
Prevent JM from discarding state on checkpoint abortion # FLINK-24611 # Shared state tracking changed to use checkpoint ID instead of reference counts. Shared state is not cleaned up on abortion anymore (but rather on subsumption or job termination).
This might result in delays in discarding the state of aborted checkpoints.
Introduce incremental/full checkpoint size stats # FLINK-25557 # Introduce metrics of persistent bytes within each checkpoint (via REST API and UI), which could help users to know how much data size had been persisted during the incremental or change-log based checkpoint.
Enables final checkpoint by default # FLINK-25105 # In 1.15 we enabled the support of checkpoints after part of tasks finished by default, and made tasks waiting for the final checkpoint before exit to ensure all data got committed.
However, it\u0026rsquo;s worth noting that this change forces tasks to wait for one more checkpoint before exiting. In other words, this change will block the tasks until the next checkpoint get triggered and completed. If the checkpoint interval is long, the tasks\u0026rsquo; execution time would also be extended largely. In the worst case if the checkpoint interval is Long.MAX_VALUE, the tasks would be in fact blocked forever.
More information about this feature and how to disable it could be found in the documentation.
Migrate state processor API to DataStream API # FLINK-24912 # The State Processor API has been migrated from Flinks legacy DataSet API to now run over DataStreams run under BATCH execution.
Relocate RocksDB\u0026rsquo;s log under flink log directory by default # FLINK-24785 # The internal log of RocksDB would stay under flink\u0026rsquo;s log directory by default.
Dependency upgrades # Upgrade the minimal supported hadoop version to 2.8.5 # FLINK-25224 # Minimal supported Hadoop client version is now 2.8.5 (version of the Flink runtime dependency). The client can still talk to older server versions as the binary protocol should be backward compatible.
Update Elasticsearch Sinks to latest minor versions # FLINK-25189 # Elasticsearch libraries used by the connector are bumped to 7.15.2 and 6.8.20 respectively.
For Elasticsearch 7 users that use the old ElasticsearchSink interface (org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink) and depend on their own elasticsearch-rest-high-level-client version, will need to update the client dependency to a version \u0026gt;= 7.14.0 due to internal changes.
Drop support for Zookeeper 3.4 # FLINK-25146 # Support for using Zookeeper 3.4 for HA has been dropped. Users relying on Zookeeper need to upgrade to 3.5/3.6. By default Flink now uses a Zookeeper 3.5 client.
Upgrade Kafka dependency # FLINK-24765 # Kafka connector uses Kafka client 2.8.1 by default now.
`}),e.add({id:322,href:"/flink/flink-docs-master/release-notes/flink-1.5/",title:"Release Notes - Flink 1.5",section:"Release-notes",content:` Release Notes - Flink 1.5 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.
Update Configuration for Reworked Job Deployment # Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos. Flink will automatically determine the number of containers from the parallelism of the application.
Although the deployment logic was completely reworked, we aimed to not unnecessarily change the previous behavior to enable a smooth transition. Nonetheless, there are a few options that you should update in your conf/flink-conf.yaml or know about.
The allocation of TaskManagers with multiple slots is not fully supported yet. Therefore, we recommend to configure TaskManagers with a single slot, i.e., set taskmanager.numberOfTaskSlots: 1 If you observed any problems with the new deployment mode, you can always switch back to the pre-1.5 behavior by configuring mode: legacy. Please report any problems or possible improvements that you notice to the Flink community, either by posting to a mailing list or by opening a JIRA issue.
Note: We plan to remove the legacy mode in the next release.
Update Configuration for Reworked Network Stack # The changes on the networking stack for credit-based flow control and improved latency affect the configuration of network buffers. In a nutshell, the networking stack can require more memory to run applications. Hence, you might need to adjust the network configuration of your Flink setup.
There are two ways to address problems of job submissions that fail due to lack of network buffers.
Reduce the number of buffers per channel, i.e., taskmanager.network.memory.buffers-per-channel or Increase the amount of TaskManager memory that is used by the network stack, i.e., increase taskmanager.network.memory.fraction and/or taskmanager.network.memory.max. Please consult the section about network buffer configuration in the Flink documentation for details. In case you experience issues with the new credit-based flow control mode, you can disable flow control by setting taskmanager.network.credit-model: false.
Note: We plan to remove the old model and this configuration in the next release.
Hadoop Classpath Discovery # We removed the automatic Hadoop classpath discovery via the Hadoop binary. If you want Flink to pick up the Hadoop classpath you have to export HADOOP_CLASSPATH. On cloud environments and most Hadoop distributions you would do
export HADOOP_CLASSPATH=\`hadoop classpath\`. Breaking Changes of the REST API # In an effort to harmonize, extend, and improve the REST API, a few handlers and return values were changed.
The jobs overview handler is now registered under /jobs/overview (before /joboverview) and returns a list of job details instead of the pre-grouped view of running, finished, cancelled and failed jobs. The REST API to cancel a job was changed. The REST API to cancel a job with savepoint was changed. Please check the REST API documentation for details.
Kafka Producer Flushes on Checkpoint by Default # The Flink Kafka Producer now flushes on checkpoints by default. Prior to version 1.5, the behaviour was disabled by default and users had to explicitly call setFlushOnCheckpoints(true) on the producer to enable it.
Updated Kinesis Dependency # The Kinesis dependencies of Flink’s Kinesis connector have been updated to the following versions.
\u0026lt;aws.sdk.version\u0026gt;1.11.319\u0026lt;/aws.sdk.version\u0026gt; \u0026lt;aws.kinesis-kcl.version\u0026gt;1.9.0\u0026lt;/aws.kinesis-kcl.version\u0026gt; \u0026lt;aws.kinesis-kpl.version\u0026gt;0.12.9\u0026lt;/aws.kinesis-kcl.version\u0026gt; Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
Back to top
`}),e.add({id:323,href:"/flink/flink-docs-master/release-notes/flink-1.6/",title:"Release Notes - Flink 1.6",section:"Release-notes",content:` Release Notes - Flink 1.6 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.5 and Flink 1.6. Please read these notes carefully if you are planning to upgrade your Flink version to 1.6.
Changed Configuration Default Values # The default value of the slot idle timeout slot.idle.timeout is set to the default value of the heartbeat timeout (50 s).
Changed ElasticSearch 5.x Sink API # Previous APIs in the Flink ElasticSearch 5.x Sink\u0026rsquo;s RequestIndexer interface have been deprecated in favor of new signatures. When adding requests to the RequestIndexer, the requests now must be of type IndexRequest, DeleteRequest, or UpdateRequest, instead of the base ActionRequest.
Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
Back to top
`}),e.add({id:324,href:"/flink/flink-docs-master/release-notes/flink-1.7/",title:"Release Notes - Flink 1.7",section:"Release-notes",content:` Release Notes - Flink 1.7 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.6 and Flink 1.7. Please read these notes carefully if you are planning to upgrade your Flink version to 1.7.
Scala 2.12 support # When using Scala 2.12 you might have to add explicit type annotations in places where they were not required when using Scala 2.11. This is an excerpt from the TransitiveClosureNaive.scala example in the Flink code base that shows the changes that could be required.
Previous code:
val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) { (prev, next, out: Collector[(Long, Long)]) =\u0026gt; { val prevPaths = prev.toSet for (n \u0026lt;- next) if (!prevPaths.contains(n)) out.collect(n) } } With Scala 2.12 you have to change it to:
val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) { (prev: Iterator[(Long, Long)], next: Iterator[(Long, Long)], out: Collector[(Long, Long)]) =\u0026gt; { val prevPaths = prev.toSet for (n \u0026lt;- next) if (!prevPaths.contains(n)) out.collect(n) } } The reason for this is that Scala 2.12 changes how lambdas are implemented. They now use the lambda support using SAM interfaces introduced in Java 8. This makes some method calls ambiguous because now both Scala-style lambdas and SAMs are candidates for methods were it was previously clear which method would be invoked.
State evolution # Before Flink 1.7, serializer snapshots were implemented as a TypeSerializerConfigSnapshot (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new TypeSerializerSnapshot interface introduced in 1.7). Moreover, the responsibility of serializer schema compatibility checks lived within the TypeSerializer, implemented in the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method.
To be future-proof and to have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. Details and migration guides can be found here.
Removal of the legacy mode # Flink no longer supports the legacy mode. If you depend on this, then please use Flink 1.6.x.
Savepoints being used for recovery # Savepoints are now used while recovering. Previously when using exactly-once sink one could get into problems with duplicate output data when a failure occurred after a savepoint was taken but before the next checkpoint occurred. This results in the fact that savepoints are no longer exclusively under the control of the user. Savepoint should not be moved nor deleted if there was no newer checkpoint or savepoint taken.
MetricQueryService runs in separate thread pool # The metric query service runs now in its own ActorSystem. It needs consequently to open a new port for the query services to communicate with each other. The query service port can be configured in flink-conf.yaml.
Granularity of latency metrics # The default granularity for latency metrics has been modified. To restore the previous behavior users have to explicitly set the granularity to subtask.
Latency marker activation # Latency metrics are now disabled by default, which will affect all jobs that do not explicitly set the latencyTrackingInterval via ExecutionConfig#setLatencyTrackingInterval. To restore the previous default behavior users have to configure the latency interval in flink-conf.yaml.
Relocation of Hadoop\u0026rsquo;s Netty dependency # We now also relocate Hadoop\u0026rsquo;s Netty dependency from io.netty to org.apache.flink.hadoop.shaded.io.netty. You can now bundle your own version of Netty into your job but may no longer assume that io.netty is present in the flink-shaded-hadoop2-uber-*.jar file.
Local recovery fixed # With the improvements to Flink\u0026rsquo;s scheduling, it can no longer happen that recoveries require more slots than before if local recovery is enabled. Consequently, we encourage our users to enable local recovery in flink-conf.yaml.
Support for multi slot TaskManagers # Flink now properly supports TaskManagers with multiple slots. Consequently, TaskManagers can now be started with an arbitrary number of slots and it is no longer recommended to start them with a single slot.
StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID # The StandaloneJobClusterEntrypoint, which is launched by the script standalone-job.sh and used for the job-mode container images, now starts all jobs with a fixed JobID. Thus, in order to run a cluster in HA mode, one needs to set a different cluster id for each job/cluster.
Scala shell does not work with Scala 2.12 # Flink\u0026rsquo;s Scala shell does not work with Scala 2.12. Therefore, the module flink-scala-shell is not being released for Scala 2.12.
See FLINK-10911 for more details.
Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
SQL over window preceding clause # The over window preceding clause is now optional. It defaults to UNBOUNDED if not specified.
OperatorSnapshotUtil writes v2 snapshots # Snapshots created with OperatorSnapshotUtil are now written in the savepoint format v2.
SBT projects and the MiniClusterResource # If you have a sbt project which uses the MiniClusterResource, you now have to add the flink-runtime test-jar dependency explicitly via:
libraryDependencies += \u0026quot;org.apache.flink\u0026quot; %% \u0026quot;flink-runtime\u0026quot; % flinkVersion % Test classifier \u0026quot;tests\u0026quot;
The reason for this is that the MiniClusterResource has been moved from flink-test-utils to flink-runtime. The module flink-test-utils has correctly a test-jar dependency on flink-runtime. However, sbt does not properly pull in transitive test-jar dependencies as described in this sbt issue. Consequently, it is necessary to specify the test-jar dependency explicitly.
Back to top
`}),e.add({id:325,href:"/flink/flink-docs-master/release-notes/flink-1.8/",title:"Release Notes - Flink 1.8",section:"Release-notes",content:` Release Notes - Flink 1.8 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.7 and Flink 1.8. Please read these notes carefully if you are planning to upgrade your Flink version to 1.8.
State # Continuous incremental cleanup of old Keyed State with TTL # We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (FLINK-9510). This feature allowed to clean up and make inaccessible keyed state entries when accessing them. In addition state would now also being cleaned up when writing a savepoint/checkpoint.
Flink 1.8 introduces continuous cleanup of old entries for both the RocksDB state backend (FLINK-10471) and the heap state backend (FLINK-10473). This means that old entries (according to the ttl setting) are continuously being cleanup up.
New Support for Schema Migration when restoring Savepoints # With Flink 1.7.0 we added support for changing the schema of state when using the AvroSerializer (FLINK-10605). With Flink 1.8.0 we made great progress migrating all built-in TypeSerializers to a new serializer snapshot abstraction that theoretically allows schema migration. Of the serializers that come with Flink, we now support schema migration for the PojoSerializer (FLINK-11485), and Java EnumSerializer (FLINK-11334), As well as for Kryo in limited cases (FLINK-11323).
Savepoint compatibility # Savepoints from Flink 1.2 that contain a Scala TraversableSerializer are not compatible with Flink 1.8 anymore because of an update in this serializer (FLINK-11539). You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8.
RocksDB version bump and switch to FRocksDB (FLINK-10471) # We needed to switch to a custom build of RocksDB called FRocksDB because we need certain changes in RocksDB for supporting continuous state cleanup with TTL. The used build of FRocksDB is based on the upgraded version 5.17.2 of RocksDB. For Mac OS X, RocksDB version 5.17.2 is supported only for OS X version \u0026gt;= 10.13. See also: https://github.com/facebook/rocksdb/issues/4862.
Maven Dependencies # Changes to bundling of Hadoop libraries with Flink (FLINK-11266) # Convenience binaries that include hadoop are no longer released.
If a deployment relies on flink-shaded-hadoop2 being included in flink-dist, then you must manually download a pre-packaged Hadoop jar from the optional components section of the download page and copy it into the /lib directory. Alternatively, a Flink distribution that includes hadoop can be built by packaging flink-dist and activating the include-hadoop maven profile.
As hadoop is no longer included in flink-dist by default, specifying -DwithoutHadoop when packaging flink-dist no longer impacts the build.
Configuration # TaskManager configuration (FLINK-11716) # TaskManagers now bind to the host IP address instead of the hostname by default . This behaviour can be controlled by the configuration option taskmanager.network.bind-policy. If your Flink cluster should experience inexplicable connection problems after upgrading, try to set taskmanager.network.bind-policy: name in your flink-conf.yaml to return to the pre-1.8 behaviour.
Table API # Deprecation of direct Table constructor usage (FLINK-11447) # Flink 1.8 deprecates direct usage of the constructor of the Table class in the Table API. This constructor would previously be used to perform a join with a lateral table. You should now use table.joinLateral() or table.leftOuterJoinLateral() instead.
This change is necessary for converting the Table class into an interface, which will make the API more maintainable and cleaner in the future.
Introduction of new CSV format descriptor (FLINK-9964) # This release introduces a new format descriptor for CSV files that is compliant with RFC 4180. The new descriptor is available as org.apache.flink.table.descriptors.Csv. For now, this can only be used together with the Kafka connector. The old descriptor is available as org.apache.flink.table.descriptors.OldCsv for use with file system connectors.
Deprecation of static builder methods on TableEnvironment (FLINK-11445) # In order to separate API from actual implementation, the static methods TableEnvironment.getTableEnvironment() are deprecated. You should now use Batch/StreamTableEnvironment.create() instead.
Change in the Maven modules of Table API (FLINK-11064) # Users that had a flink-table dependency before, need to update their dependencies to flink-table-planner and the correct dependency of flink-table-api-*, depending on whether Java or Scala is used: one of flink-table-api-java-bridge or flink-table-api-scala-bridge.
Change to External Catalog Table Builders (FLINK-11522) # ExternalCatalogTable.builder() is deprecated in favour of ExternalCatalogTableBuilder().
Change to naming of Table API connector jars (FLINK-11026) # The naming scheme for kafka/elasticsearch6 sql-jars has been changed.
In maven terms, they no longer have the sql-jar qualifier and the artifactId is now prefixed with flink-sql instead of flink, e.g., flink-sql-connector-kafka....
Change to how Null Literals are specified (FLINK-11785) # Null literals in the Table API need to be defined with nullOf(type) instead of Null(type) from now on. The old approach is deprecated.
Connectors # Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (FLINK-8354) # For the Flink KafkaConsumers, we introduced a new KafkaDeserializationSchema that gives direct access to the Kafka ConsumerRecord. This subsumes the KeyedSerializationSchema functionality, which is deprecated but still available for now.
FlinkKafkaConsumer will now filter restored partitions based on topic specification (FLINK-10342) # Starting from Flink 1.8.0, the FlinkKafkaConsumer now always filters out restored partitions that are no longer associated with a specified topic to subscribe to in the restored execution. This behaviour did not exist in previous versions of the FlinkKafkaConsumer. If you wish to retain the previous behaviour, please use the disableFilterRestoredPartitionsWithSubscribedTopics() configuration method on the FlinkKafkaConsumer.
Consider this example: if you had a Kafka Consumer that was consuming from topic A, you did a savepoint, then changed your Kafka consumer to instead consume from topic B, and then restarted your job from the savepoint. Before this change, your consumer would now consume from both topic A and B because it was stored in state that the consumer was consuming from topic A. With the change, your consumer would only consume from topic B after restore because we filter the topics that are stored in state using the configured topics.
Miscellaneous Interface changes # The canEqual() method was dropped from the TypeSerializer interface (FLINK-9803) # The canEqual() methods are usually used to make proper equality checks across hierarchies of types. The TypeSerializer actually doesn\u0026rsquo;t require this property, so the method is now removed.
Removal of the CompositeSerializerSnapshot utility class (FLINK-11073) # The CompositeSerializerSnapshot utility class has been removed. You should now use CompositeTypeSerializerSnapshot instead, for snapshots of composite serializers that delegate serialization to multiple nested serializers. Please see here for instructions on using CompositeTypeSerializerSnapshot.
Memory management # In Flink 1.8.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.
Back to top
`}),e.add({id:326,href:"/flink/flink-docs-master/release-notes/flink-1.9/",title:"Release Notes - Flink 1.9",section:"Release-notes",content:` Release Notes - Flink 1.9 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.8 and Flink 1.9. It also provides an overview on known shortcoming or limitations with new experimental features introduced in 1.9.
Please read these notes carefully if you are planning to upgrade your Flink version to 1.9.
Known shortcomings or limitations for new features # New Table / SQL Blink planner # Flink 1.9.0 provides support for two planners for the Table API, namely Flink\u0026rsquo;s original planner and the new Blink planner. The original planner maintains same behaviour as previous releases, while the new Blink planner is still considered experimental and has the following limitations:
The Blink planner can not be used with BatchTableEnvironment, and therefore Table programs ran with the planner can not be transformed to DataSet programs. This is by design and will also not be supported in the future. Therefore, if you want to run a batch job with the Blink planner, please use the new TableEnvironment. For streaming jobs, both StreamTableEnvironment and TableEnvironment works. Implementations of StreamTableSink should implement the consumeDataStream method instead of emitDataStream if it is used with the Blink planner. Both methods work with the original planner. This is by design to make the returned DataStreamSink accessible for the planner. Due to a bug with how transformations are not being cleared on execution, TableEnvironment instances should not be reused across multiple SQL statements when using the Blink planner. Table.flatAggregate is not supported Session and count windows are not supported when running batch jobs. The Blink planner only supports the new Catalog API, and does not support ExternalCatalog which is now deprecated. Related issues:
FLINK-13708: Transformations should be cleared because a table environment could execute multiple job FLINK-13473: Add GroupWindowed FlatAggregate support to stream Table API (Blink planner), i.e, align with Flink planner FLINK-13735: Support session window with Blink planner in batch mode FLINK-13736: Support count window with Blink planner in batch mode SQL DDL # In Flink 1.9.0, the community also added a preview feature about SQL DDL, but only for batch style DDLs. Therefore, all streaming related concepts are not supported yet, for example watermarks.
Related issues:
FLINK-13661: Add a stream specific CREATE TABLE SQL DDL FLINK-13568: DDL create table doesn\u0026rsquo;t allow STRING data type Java 9 support # Since Flink 1.9.0, Flink can now be compiled and run on Java 9. Note that certain components interacting with external systems (connectors, filesystems, metric reporters, etc.) may not work since the respective projects may have skipped Java 9 support.
Related issues:
FLINK-8033: JDK 9 support Memory management # In Flink 1.9.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.
Related issues:
FLINK-14123: Lower the default value of taskmanager.memory.fraction Deprecations and breaking changes # Scala expression DSL for Table API moved to flink-table-api-scala # Since 1.9.0, the implicit conversions for the Scala expression DSL for the Table API has been moved to flink-table-api-scala. This requires users to update the imports in their Table programs.
Users of pure Table programs should define their imports like:
import org.apache.flink.table.api._ TableEnvironment.create(...) Users of the DataStream API should define their imports like:
import org.apache.flink.table.api._ import org.apache.flink.table.api.scala._ StreamTableEnvironment.create(...) Related issues:
FLINK-13045: Move Scala expression DSL to flink-table-api-scala Failover strategies # As a result of completing fine-grained recovery (FLIP-1), Flink will now attempt to only restart tasks that are connected to failed tasks through a pipelined connection. By default, the region failover strategy is used.
Users who were not using a restart strategy or have already configured a failover strategy should not be affected. Moreover, users who already enabled the region failover strategy, along with a restart strategy that enforces a certain number of restarts or introduces a restart delay, will see changes in behavior. The region failover strategy now correctly respects constraints that are defined by the restart strategy.
Streaming users who were not using a failover strategy may be affected if their jobs are embarrassingly parallel or contain multiple independent jobs. In this case, only the failed parallel pipeline or affected jobs will be restarted.
Batch users may be affected if their job contains blocking exchanges (usually happens for shuffles) or the ExecutionMode was set to BATCH or BATCH_FORCED via the ExecutionConfig.
Overall, users should see an improvement in performance.
Related issues:
FLINK-13223: Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml FLINK-13060: FailoverStrategies should respect restart constraints Job termination via CLI # With the support of graceful job termination with savepoints for semantic correctness (FLIP-34), a few changes related to job termination has been made to the CLI.
From now on, the stop command with no further arguments stops the job with a savepoint targeted at the default savepoint location (as configured via the state.savepoints.dir property in the job configuration), or a location explicitly specified using the -p \u0026lt;savepoint-path\u0026gt; option. Please make sure to configure the savepoint path using either one of these options.
Since job terminations are now always accompanied with a savepoint, stopping jobs is expected to take longer now.
Related issues:
FLINK-13123: Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation FLINK-11458: Add TERMINATE/SUSPEND Job with Savepoint Network stack # A few changes in the network stack related to changes in the threading model of StreamTask to a mailbox-based approach requires close attention to some related configuration:
Due to changes in the lifecycle management of result partitions, partition requests as well as re-triggers will now happen sooner. Therefore, it is possible that some jobs with long deployment times and large state might start failing more frequently with PartitionNotFound exceptions compared to previous versions. If that\u0026rsquo;s the case, users should increase the value of taskmanager.network.request-backoff.max in order to have the same effective partition request timeout as it was prior to 1.9.0.
To avoid a potential deadlock, a timeout has been added for how long a task will wait for assignment of exclusive memory segments. The default timeout is 30 seconds, and is configurable via taskmanager.network.memory.exclusive-buffers-request-timeout-ms. It is possible that for some previously working deployments this default timeout value is too low and might have to be increased.
Please also notice that several network I/O metrics have had their scope changed. See the 1.9 metrics documentation for which metrics are affected. In 1.9.0, these metrics will still be available under their previous scopes, but this may no longer be the case in future versions.
Related issues:
FLINK-13013: Make sure that SingleInputGate can always request partitions FLINK-12852: Deadlock occurs when requiring exclusive buffer for RemoteInputChannel FLINK-12555: Introduce an encapsulated metric group layout for shuffle API and deprecate old one AsyncIO # Due to a bug in the AsyncWaitOperator, in 1.9.0 the default chaining behaviour of the operator is now changed so that it is never chained after another operator. This should not be problematic for migrating from older version snapshots as long as an uid was assigned to the operator. If an uid was not assigned to the operator, please see the instructions here for a possible workaround.
Related issues:
FLINK-13063: AsyncWaitOperator shouldn\u0026rsquo;t be releasing checkpointingLock Connectors and Libraries # Introduced KafkaSerializationSchema to fully replace KeyedSerializationSchema # The universal FlinkKafkaProducer (in flink-connector-kafka) supports a new KafkaSerializationSchema that will fully replace KeyedSerializationSchema in the long run. This new schema allows directly generating Kafka ProducerRecords for sending to Kafka, therefore enabling the user to use all available Kafka features (in the context of Kafka records).
Dropped connectors and libraries # The Elasticsearch 1 connector has been dropped and will no longer receive patches. Users may continue to use the connector from a previous series (like 1.8) with newer versions of Flink. It is being dropped due to being used significantly less than more recent versions (Elasticsearch versions 2.x and 5.x are downloaded 4 to 5 times more), and hasn\u0026rsquo;t seen any development for over a year.
The older Python APIs for batch and streaming have been removed and will no longer receive new patches. A new API is being developed based on the Table API as part of FLINK-12308: Support python language in Flink Table API. Existing users may continue to use these older APIs with future versions of Flink by copying both the flink-streaming-python and flink-python jars into the /lib directory of the distribution and the corresponding start scripts pyflink-stream.sh and pyflink.sh into the /bin directory of the distribution.
The older machine learning libraries have been removed and will no longer receive new patches. This is due to efforts towards a new Table-based machine learning library (FLIP-39). Users can still use the 1.8 version of the legacy library if their projects still rely on it.
Related issues:
FLINK-11693: Add KafkaSerializationSchema that directly uses ProducerRecord FLINK-12151: Drop Elasticsearch 1 connector FLINK-12903: Remove legacy flink-python APIs FLINK-12308: Support python language in Flink Table API FLINK-12597: Remove the legacy flink-libraries/flink-ml MapR dependency removed # Dependency on MapR vendor-specific artifacts has been removed, by changing the MapR filesystem connector to work purely based on reflection. This does not introduce any regression in the support for the MapR filesystem. The decision to remove hard dependencies on the MapR artifacts was made due to very flaky access to the secure https endpoint of the MapR artifact repository, and affected build stability of Flink.
Related issues:
FLINK-12578: Use secure URLs for Maven repositories FLINK-13499: Remove dependency on MapR artifact repository StateDescriptor interface change # Access to the state serializer in StateDescriptor is now modified from protected to private access. Subclasses should use the StateDescriptor#getSerializer() method as the only means to obtain the wrapped state serializer.
Related issues:
FLINK-12688: Make serializer lazy initialization thread safe in StateDescriptor Web UI dashboard # The web frontend of Flink has been updated to use the latest Angular version (7.x). The old frontend remains available in Flink 1.9.x, but will be removed in a later Flink release once the new frontend is considered stable.
Related issues:
FLINK-10705: Rework Flink Web Dashoard `}),e.add({id:327,href:"/flink/flink-docs-master/versions/",title:"Versions",section:"Apache Flink Documentation",content:` Versions # An appendix of hosted documentation for all versions of Apache Flink.
v1.15 v1.14 v1.13 v1.12 v1.11 v1.10 v1.9 v1.8 v1.7 v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 `})})(),!function(e,t){"use strict";"object"==typeof module&&"object"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error("jQuery requires a window with a document");return t(e)}:t(e)}("undefined"!=typeof window?window:this,function(e,t){"use strict";var n,l,h,b,S,T,z,N,R,$,K,G,J,g=[],yt=Object.getPrototypeOf,j=g.slice,bt=g.flat?function(e){return g.flat.call(e)}:function(e){return g.concat.apply([],e)},_e=g.push,U=g.indexOf,Y={},vt=Y.toString,X=Y.hasOwnProperty,gt=X.toString,Gt=gt.call(Object),a={},o=function(e){return"function"==typeof e&&"number"!=typeof e.nodeType&&"function"!=typeof e.item},O=function(e){return e!=null&&e===e.window},i=e.document,Yt={type:!0,src:!0,nonce:!0,noModule:!0},he,se,it,st,nt,tt,Ge,ce,Fe,ye,ot,_t,at,Ot,ft,pt,jt,Oe,le,Ue,qe,xe,ut,mt,Be;function Ce(e,t,n){var s,a,o=(n=n||i).createElement("script");if(o.text=e,t)for(s in Yt)(a=t[s]||t.getAttribute&&t.getAttribute(s))&&o.setAttribute(s,a);n.head.appendChild(o).parentNode.removeChild(o)}function M(e){return e==null?e+"":"object"==typeof e||"function"==typeof e?Y[vt.call(e)]||"object":typeof e}he="3.6.1",n=function(e,t){return new n.fn.init(e,t)};function de(e){var t=!!e&&"length"in e&&e.length,n=M(e);return!o(e)&&!O(e)&&("array"===n||0===t||"number"==typeof t&&0<t&&t-1 in e)}n.fn=n.prototype={jquery:he,constructor:n,length:0,toArray:function(){return j.call(this)},get:function(e){return e==null?j.call(this):e<0?this[e+this.length]:this[e]},pushStack:function(e){var t=n.merge(this.constructor(),e);return t.prevObject=this,t},each:function(e){return n.each(this,e)},map:function(e){return this.pushStack(n.map(this,function(t,n){return e.call(t,n,t)}))},slice:function(){return this.pushStack(j.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},even:function(){return this.pushStack(n.grep(this,function(e,t){return(t+1)%2}))},odd:function(){return this.pushStack(n.grep(this,function(e,t){return t%2}))},eq:function(e){var n=this.length,t=+e+(e<0?n:0);return this.pushStack(0<=t&&t<n?[this[t]]:[])},end:function(){return this.prevObject||this.constructor()},push:_e,sort:g.sort,splice:g.splice},n.extend=n.fn.extend=function(){var t,i,a,r,c,d,e=arguments[0]||{},s=1,u=arguments.length,l=!1;for("boolean"==typeof e&&(l=e,e=arguments[s]||{},s++),"object"==typeof e||o(e)||(e={}),s===u&&(e=this,s--);s<u;s++)if(null!=(c=arguments[s]))for(i in c)t=c[i],"__proto__"!==i&&e!==t&&(l&&t&&(n.isPlainObject(t)||(r=Array.isArray(t)))?(a=e[i],d=r&&!Array.isArray(a)?[]:r||n.isPlainObject(a)?a:{},r=!1,e[i]=n.extend(l,d,t)):void 0!==t&&(e[i]=t));return e},n.extend({expando:"jQuery"+(he+Math.random()).replace(/\D/g,""),isReady:!0,error:function(e){throw new Error(e)},noop:function(){},isPlainObject:function(e){var t,n;return!!e&&"[object Object]"===vt.call(e)&&(!(t=yt(e))||"function"==typeof(n=X.call(t,"constructor")&&t.constructor)&&gt.call(n)===Gt)},isEmptyObject:function(e){var t;for(t in e)return!1;return!0},globalEval:function(e,t,n){Ce(e,{nonce:t&&t.nonce},n)},each:function(e,t){var s,n=0;if(de(e)){for(s=e.length;n<s;n++)if(!1===t.call(e[n],n,e[n]))break}else for(n in e)if(!1===t.call(e[n],n,e[n]))break;return e},makeArray:function(e,t){var s=t||[];return e!=null&&(de(Object(e))?n.merge(s,"string"==typeof e?[e]:e):_e.call(s,e)),s},inArray:function(e,t,n){return t==null?-1:U.call(t,e,n)},merge:function(e,t){for(var o=+t.length,n=0,s=e.length;n<o;n++)e[s++]=t[n];return e.length=s,e},grep:function(e,t,n){for(var o=[],s=0,i=e.length,a=!n;s<i;s++)!t(e[s],s)!==a&&o.push(e[s]);return o},map:function(e,t,n){var o,a,s=0,i=[];if(de(e))for(a=e.length;s<a;s++)null!=(o=t(e[s],s,n))&&i.push(o);else for(s in e)null!=(o=t(e[s],s,n))&&i.push(o);return bt(i)},guid:1,support:a}),"function"==typeof Symbol&&(n.fn[Symbol.iterator]=g[Symbol.iterator]),n.each("Boolean Number String Function Array Date RegExp Object Error Symbol".split(" "),function(e,t){Y["[object "+t+"]"]=t.toLowerCase()}),b=function(e){var t,n,i,r,c,l,g,b,j,x,C,k,A,F,N,R,I,ae,te,a="sizzle"+1*new Date,d=e.document,p=0,Oe=0,ne=P(),J=P(),Z=P(),z=P(),Y=function(e,t){return e===t&&(x=!0),0},we={}.hasOwnProperty,y=[],xe=y.pop,_e=y.push,v=y.push,ee=y.slice,w=function(e,t){for(var n=0,s=e.length;n<s;n++)if(e[n]===t)return n;return-1},B="checked|selected|async|autofocus|autoplay|controls|defer|disabled|hidden|ismap|loop|multiple|open|readonly|required|scoped",s=`[\\x20\\t\\r\\n\\f]`,O="(?:\\\\[\\da-fA-F]{1,6}"+s+`?|\\\\[^\\r\\n\\f]|[\\w-]|[^\0-\\x7f])+`,re="\\["+s+"*("+O+")(?:"+s+"*([*^$|!~]?=)"+s+`*(?:'((?:\\\\.|[^\\\\'])*)'|"((?:\\\\.|[^\\\\"])*)"|(`+O+"))|)"+s+"*\\]",U=":("+O+`)(?:\\((('((?:\\\\.|[^\\\\'])*)'|"((?:\\\\.|[^\\\\"])*)")|((?:\\\\.|[^\\\\()[\\]]|`+re+")*)|.*)\\)|)",ye=new RegExp(s+"+","g"),D=new RegExp("^"+s+"+|((?:^|[^\\\\])(?:\\\\.)*)"+s+"+$","g"),je=new RegExp("^"+s+"*,"+s+"*"),Q=new RegExp("^"+s+"*([>+~]|"+s+")"+s+"*"),be=new RegExp(s+"|>"),ve=new RegExp(U),ge=new RegExp("^"+O+"$"),S={ID:new RegExp("^#("+O+")"),CLASS:new RegExp("^\\.("+O+")"),TAG:new RegExp("^("+O+"|[*])"),ATTR:new RegExp("^"+re),PSEUDO:new RegExp("^"+U),CHILD:new RegExp("^:(only|first|last|nth|nth-last)-(child|of-type)(?:\\("+s+"*(even|odd|(([+-]|)(\\d*)n|)"+s+"*(?:([+-]|)"+s+"*(\\d+)|))"+s+"*\\)|)","i"),bool:new RegExp("^(?:"+B+")$","i"),needsContext:new RegExp("^"+s+"*[>+~]|:(even|odd|eq|gt|lt|nth|first|last)(?:\\("+s+"*((?:-\\d)?\\d*)"+s+"*\\)|)(?=[^-]|$)","i")},pe=/HTML$/i,he=/^(?:input|select|textarea|button)$/i,ue=/^h\d$/i,E=/^[^{]+\{\s*\[native \w/,de=/^(?:#([\w-]+)|(\w+)|\.([\w-]+))$/,q=/[+~]/,f=new RegExp("\\\\[\\da-fA-F]{1,6}"+s+`?|\\\\([^\\r\\n\\f])`,"g"),m=function(e,t){var n="0x"+e.slice(1)-65536;return t||(n<0?String.fromCharCode(n+65536):String.fromCharCode(n>>10|55296,1023&n|56320))},se=/([\0-\x1f\x7f]|^-?\d)|^-$|[^\0-\x1f\x7f-\uFFFF\w-]/g,oe=function(e,t){return t?"\0"===e?"\ufffd":e.slice(0,-1)+"\\"+e.charCodeAt(e.length-1).toString(16)+" ":"\\"+e},ie=function(){g()},le=L(function(e){return!0===e.disabled&&"fieldset"===e.nodeName.toLowerCase()},{dir:"parentNode",next:"legend"});try{v.apply(y=ee.call(d.childNodes),d.childNodes),y[d.childNodes.length].nodeType}catch{v={apply:y.length?function(e,t){_e.apply(e,ee.call(t))}:function(e,t){for(var n=e.length,s=0;e[n++]=t[s++];);e.length=n-1}}}function o(e,t,s,o){var c,d,u,f,p,b,j,m=t&&t.ownerDocument,h=t?t.nodeType:9;if(s=s||[],"string"!=typeof e||!e||1!==h&&9!==h&&11!==h)return s;if(!o&&(g(t),t=t||n,l)){if(11!==h&&(f=de.exec(e)))if(c=f[1]){if(9===h){{if(!(d=t.getElementById(c)))return s;if(d.id===c)return s.push(d),s}}else if(m&&(d=m.getElementById(c))&&A(t,d)&&d.id===c)return s.push(d),s}else{if(f[2])return v.apply(s,t.getElementsByTagName(e)),s;if((c=f[3])&&i.getElementsByClassName&&t.getElementsByClassName)return v.apply(s,t.getElementsByClassName(c)),s}if(i.qsa&&!z[e+" "]&&(!r||!r.test(e))&&(1!==h||"object"!==t.nodeName.toLowerCase())){if(j=e,m=t,1===h&&(be.test(e)||Q.test(e))){for((m=q.test(e)&&K(t.parentNode)||t)===t&&i.scope||((u=t.getAttribute("id"))?u=u.replace(se,oe):t.setAttribute("id",u=a)),b=(p=k(e)).length;b--;)p[b]=(u?"#"+u:":scope")+" "+T(p[b]);j=p.join(",")}try{return v.apply(s,m.querySelectorAll(j)),s}catch{z(e,!0)}finally{u===a&&t.removeAttribute("id")}}}return te(e.replace(D,"$1"),t,s,o)}function P(){var e=[];return function n(s,o){return e.push(s+" ")>t.cacheLength&&delete n[e.shift()],n[s+" "]=o}}function u(e){return e[a]=!0,e}function h(e){var t=n.createElement("fieldset");try{return!!e(t)}catch{return!1}finally{t.parentNode&&t.parentNode.removeChild(t),t=null}}function W(e,n){for(var s=e.split("|"),o=s.length;o--;)t.attrHandle[s[o]]=n}function G(e,t){var n=t&&e,s=n&&1===e.nodeType&&1===t.nodeType&&e.sourceIndex-t.sourceIndex;if(s)return s;if(n)for(;n=n.nextSibling;)if(n===t)return-1;return e?1:-1}function me(e){return function(t){return"input"===t.nodeName.toLowerCase()&&t.type===e}}function fe(e){return function(t){var n=t.nodeName.toLowerCase();return("input"===n||"button"===n)&&t.type===e}}function ce(e){return function(t){return"form"in t?t.parentNode&&!1===t.disabled?"label"in t?"label"in t.parentNode?t.parentNode.disabled===e:t.disabled===e:t.isDisabled===e||t.isDisabled!==!e&&le(t)===e:t.disabled===e:"label"in t&&t.disabled===e}}function _(e){return u(function(t){return t=+t,u(function(n,s){for(var o,i=e([],n.length,t),a=i.length;a--;)n[o=i[a]]&&(n[o]=!(s[o]=n[o]))})})}function K(e){return e&&"undefined"!=typeof e.getElementsByTagName&&e}for(C in i=o.support={},ae=o.isXML=function(e){var n=e&&e.namespaceURI,t=e&&(e.ownerDocument||e).documentElement;return!pe.test(n||t&&t.nodeName||"HTML")},g=o.setDocument=function(e){var o,p,u=e?e.ownerDocument||e:d;return u!=n&&9===u.nodeType&&u.documentElement&&(c=(n=u).documentElement,l=!ae(n),d!=n&&(o=n.defaultView)&&o.top!==o&&(o.addEventListener?o.addEventListener("unload",ie,!1):o.attachEvent&&o.attachEvent("onunload",ie)),i.scope=h(function(e){return c.appendChild(e).appendChild(n.createElement("div")),"undefined"!=typeof e.querySelectorAll&&!e.querySelectorAll(":scope fieldset div").length}),i.attributes=h(function(e){return e.className="i",!e.getAttribute("className")}),i.getElementsByTagName=h(function(e){return e.appendChild(n.createComment("")),!e.getElementsByTagName("*").length}),i.getElementsByClassName=E.test(n.getElementsByClassName),i.getById=h(function(e){return c.appendChild(e).id=a,!n.getElementsByName||!n.getElementsByName(a).length}),i.getById?(t.filter.ID=function(e){var t=e.replace(f,m);return function(e){return e.getAttribute("id")===t}},t.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&l){var n=t.getElementById(e);return n?[n]:[]}}):(t.filter.ID=function(e){var t=e.replace(f,m);return function(e){var n="undefined"!=typeof e.getAttributeNode&&e.getAttributeNode("id");return n&&n.value===t}},t.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&l){var s,o,i,n=t.getElementById(e);if(n){if((s=n.getAttributeNode("id"))&&s.value===e)return[n];for(i=t.getElementsByName(e),o=0;n=i[o++];)if((s=n.getAttributeNode("id"))&&s.value===e)return[n]}return[]}}),t.find.TAG=i.getElementsByTagName?function(e,t){return"undefined"!=typeof t.getElementsByTagName?t.getElementsByTagName(e):i.qsa?t.querySelectorAll(e):void 0}:function(e,t){var n,s=[],i=0,o=t.getElementsByTagName(e);if("*"===e){for(;n=o[i++];)1===n.nodeType&&s.push(n);return s}return o},t.find.CLASS=i.getElementsByClassName&&function(e,t){if("undefined"!=typeof t.getElementsByClassName&&l)return t.getElementsByClassName(e)},j=[],r=[],(i.qsa=E.test(n.querySelectorAll))&&(h(function(e){var t;c.appendChild(e).innerHTML="<a id='"+a+"'></a><select id='"+a+`-\\' msallowcapture=''><option selected=''></option></select>`,e.querySelectorAll("[msallowcapture^='']").length&&r.push("[*^$]="+s+`*(?:''|"")`),e.querySelectorAll("[selected]").length||r.push("\\["+s+"*(?:value|"+B+")"),e.querySelectorAll("[id~="+a+"-]").length||r.push("~="),(t=n.createElement("input")).setAttribute("name",""),e.appendChild(t),e.querySelectorAll("[name='']").length||r.push("\\["+s+"*name"+s+"*="+s+`*(?:''|"")`),e.querySelectorAll(":checked").length||r.push(":checked"),e.querySelectorAll("a#"+a+"+*").length||r.push(".#.+[+~]"),e.querySelectorAll("\\"),r.push(`[\\r\\n\\f]`)}),h(function(e){e.innerHTML="<a href='' disabled='disabled'></a><select disabled='disabled'><option/></select>";var t=n.createElement("input");t.setAttribute("type","hidden"),e.appendChild(t).setAttribute("name","D"),e.querySelectorAll("[name=d]").length&&r.push("name"+s+"*[*^$|!~]?="),2!==e.querySelectorAll(":enabled").length&&r.push(":enabled",":disabled"),c.appendChild(e).disabled=!0,2!==e.querySelectorAll(":disabled").length&&r.push(":enabled",":disabled"),e.querySelectorAll("*,:x"),r.push(",.*:")})),(i.matchesSelector=E.test(F=c.matches||c.webkitMatchesSelector||c.mozMatchesSelector||c.oMatchesSelector||c.msMatchesSelector))&&h(function(e){i.disconnectedMatch=F.call(e,"*"),F.call(e,"[s!='']:x"),j.push("!=",U)}),r=r.length&&new RegExp(r.join("|")),j=j.length&&new RegExp(j.join("|")),p=E.test(c.compareDocumentPosition),A=p||E.test(c.contains)?function(e,t){var s=9===e.nodeType?e.documentElement:e,n=t&&t.parentNode;return e===n||!(!n||1!==n.nodeType||!(s.contains?s.contains(n):e.compareDocumentPosition&&16&e.compareDocumentPosition(n)))}:function(e,t){if(t)for(;t=t.parentNode;)if(t===e)return!0;return!1},Y=p?function(e,t){if(e===t)return x=!0,0;var s=!e.compareDocumentPosition-!t.compareDocumentPosition;return s||(1&(s=(e.ownerDocument||e)==(t.ownerDocument||t)?e.compareDocumentPosition(t):1)||!i.sortDetached&&t.compareDocumentPosition(e)===s?e==n||e.ownerDocument==d&&A(d,e)?-1:t==n||t.ownerDocument==d&&A(d,t)?1:b?w(b,e)-w(b,t):0:4&s?-1:1)}:function(e,t){if(e===t)return x=!0,0;var s,o=0,r=e.parentNode,c=t.parentNode,i=[e],a=[t];if(!r||!c)return e==n?-1:t==n?1:r?-1:c?1:b?w(b,e)-w(b,t):0;if(r===c)return G(e,t);for(s=e;s=s.parentNode;)i.unshift(s);for(s=t;s=s.parentNode;)a.unshift(s);for(;i[o]===a[o];)o++;return o?G(i[o],a[o]):i[o]==d?-1:a[o]==d?1:0}),n},o.matches=function(e,t){return o(e,null,null,t)},o.matchesSelector=function(e,t){if(g(e),i.matchesSelector&&l&&!z[t+" "]&&(!j||!j.test(t))&&(!r||!r.test(t)))try{var s=F.call(e,t);if(s||i.disconnectedMatch||e.document&&11!==e.document.nodeType)return s}catch{z(t,!0)}return 0<o(t,n,null,[e]).length},o.contains=function(e,t){return(e.ownerDocument||e)!=n&&g(e),A(e,t)},o.attr=function(e,s){(e.ownerDocument||e)!=n&&g(e);var a=t.attrHandle[s.toLowerCase()],o=a&&we.call(t.attrHandle,s.toLowerCase())?a(e,s,!l):void 0;return void 0!==o?o:i.attributes||!l?e.getAttribute(s):(o=e.getAttributeNode(s))&&o.specified?o.value:null},o.escape=function(e){return(e+"").replace(se,oe)},o.error=function(e){throw new Error("Syntax error, unrecognized expression: "+e)},o.uniqueSort=function(e){var s,o=[],t=0,n=0;if(x=!i.detectDuplicates,b=!i.sortStable&&e.slice(0),e.sort(Y),x){for(;s=e[n++];)s===e[n]&&(t=o.push(n));for(;t--;)e.splice(o[t],1)}return b=null,e},N=o.getText=function(e){var s,n="",o=0,t=e.nodeType;if(t){if(1===t||9===t||11===t){if("string"==typeof e.textContent)return e.textContent;for(e=e.firstChild;e;e=e.nextSibling)n+=N(e)}else if(3===t||4===t)return e.nodeValue}else for(;s=e[o++];)n+=N(s);return n},(t=o.selectors={cacheLength:50,createPseudo:u,match:S,attrHandle:{},find:{},relative:{">":{dir:"parentNode",first:!0}," ":{dir:"parentNode"},"+":{dir:"previousSibling",first:!0},"~":{dir:"previousSibling"}},preFilter:{ATTR:function(e){return e[1]=e[1].replace(f,m),e[3]=(e[3]||e[4]||e[5]||"").replace(f,m),"~="===e[2]&&(e[3]=" "+e[3]+" "),e.slice(0,4)},CHILD:function(e){return e[1]=e[1].toLowerCase(),"nth"===e[1].slice(0,3)?(e[3]||o.error(e[0]),e[4]=+(e[4]?e[5]+(e[6]||1):2*("even"===e[3]||"odd"===e[3])),e[5]=+(e[7]+e[8]||"odd"===e[3])):e[3]&&o.error(e[0]),e},PSEUDO:function(e){var n,t=!e[6]&&e[2];return S.CHILD.test(e[0])?null:(e[3]?e[2]=e[4]||e[5]||"":t&&ve.test(t)&&(n=k(t,!0))&&(n=t.indexOf(")",t.length-n)-t.length)&&(e[0]=e[0].slice(0,n),e[2]=t.slice(0,n)),e.slice(0,3))}},filter:{TAG:function(e){var t=e.replace(f,m).toLowerCase();return"*"===e?function(){return!0}:function(e){return e.nodeName&&e.nodeName.toLowerCase()===t}},CLASS:function(e){var t=ne[e+" "];return t||(t=new RegExp("(^|"+s+")"+e+"("+s+"|$)"))&&ne(e,function(e){return t.test("string"==typeof e.className&&e.className||"undefined"!=typeof e.getAttribute&&e.getAttribute("class")||"")})},ATTR:function(e,t,n){return function(s){var i=o.attr(s,e);return i==null?"!="===t:!t||(i+="","="===t?i===n:"!="===t?i!==n:"^="===t?n&&0===i.indexOf(n):"*="===t?n&&-1<i.indexOf(n):"$="===t?n&&i.slice(-n.length)===n:"~="===t?-1<(" "+i.replace(ye," ")+" ").indexOf(n):"|="===t&&(i===n||i.slice(0,n.length+1)===n+"-"))}},CHILD:function(e,t,n,s,o){var c="nth"!==e.slice(0,3),r="last"!==e.slice(-4),i="of-type"===t;return 1===s&&0===o?function(e){return!!e.parentNode}:function(t,n,l){var d,h,m,f,v,j,b=c!==r?"nextSibling":"previousSibling",g=t.parentNode,_=i&&t.nodeName.toLowerCase(),y=!l&&!i,u=!1;if(g){if(c){for(;b;){for(d=t;d=d[b];)if(i?d.nodeName.toLowerCase()===_:1===d.nodeType)return!1;v=b="only"===e&&!v&&"nextSibling"}return!0}if(v=[r?g.firstChild:g.lastChild],r&&y){for(u=(h=(f=(j=(m=(d=g)[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]||[])[0]===p&&f[1])&&f[2],d=h&&g.childNodes[h];d=++h&&d&&d[b]||(u=h=0)||v.pop();)if(1===d.nodeType&&++u&&d===t){j[e]=[p,h,u];break}}else if(y&&(u=h=(f=(j=(m=(d=t)[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]||[])[0]===p&&f[1]),!1===u)for(;d=++h&&d&&d[b]||(u=h=0)||v.pop();)if((i?d.nodeName.toLowerCase()===_:1===d.nodeType)&&++u&&(y&&((j=(m=d[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]=[p,u]),d===t))break;return(u-=o)===s||u%s==0&&0<=u/s}}},PSEUDO:function(e,n){var i,s=t.pseudos[e]||t.setFilters[e.toLowerCase()]||o.error("unsupported pseudo: "+e);return s[a]?s(n):1<s.length?(i=[e,e,"",n],t.setFilters.hasOwnProperty(e.toLowerCase())?u(function(e,t){for(var a,o=s(e,n),i=o.length;i--;)e[a=w(e,o[i])]=!(t[a]=o[i])}):function(e){return s(e,0,i)}):s}},pseudos:{not:u(function(e){var t=[],s=[],n=I(e.replace(D,"$1"));return n[a]?u(function(e,t,s,o){for(var a,r=n(e,null,o,[]),i=e.length;i--;)(a=r[i])&&(e[i]=!(t[i]=a))}):function(e,o,i){return t[0]=e,n(t,null,i,s),t[0]=null,!s.pop()}}),has:u(function(e){return function(t){return 0<o(e,t).length}}),contains:u(function(e){return e=e.replace(f,m),function(t){return-1<(t.textContent||N(t)).indexOf(e)}}),lang:u(function(e){return ge.test(e||"")||o.error("unsupported lang: "+e),e=e.replace(f,m).toLowerCase(),function(t){var n;do if(n=l?t.lang:t.getAttribute("xml:lang")||t.getAttribute("lang"))return(n=n.toLowerCase())===e||0===n.indexOf(e+"-");while((t=t.parentNode)&&1===t.nodeType)return!1}}),target:function(t){var n=e.location&&e.location.hash;return n&&n.slice(1)===t.id},root:function(e){return e===c},focus:function(e){return e===n.activeElement&&(!n.hasFocus||n.hasFocus())&&!!(e.type||e.href||~e.tabIndex)},enabled:ce(!1),disabled:ce(!0),checked:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&!!e.checked||"option"===t&&!!e.selected},selected:function(e){return e.parentNode&&e.parentNode.selectedIndex,!0===e.selected},empty:function(e){for(e=e.firstChild;e;e=e.nextSibling)if(e.nodeType<6)return!1;return!0},parent:function(e){return!t.pseudos.empty(e)},header:function(e){return ue.test(e.nodeName)},input:function(e){return he.test(e.nodeName)},button:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&"button"===e.type||"button"===t},text:function(e){var t;return"input"===e.nodeName.toLowerCase()&&"text"===e.type&&(null==(t=e.getAttribute("type"))||"text"===t.toLowerCase())},first:_(function(){return[0]}),last:_(function(e,t){return[t-1]}),eq:_(function(e,t,n){return[n<0?n+t:n]}),even:_(function(e,t){for(var n=0;n<t;n+=2)e.push(n);return e}),odd:_(function(e,t){for(var n=1;n<t;n+=2)e.push(n);return e}),lt:_(function(e,t,n){for(var s=n<0?n+t:t<n?t:n;0<=--s;)e.push(s);return e}),gt:_(function(e,t,n){for(var s=n<0?n+t:n;++s<t;)e.push(s);return e})}}).pseudos.nth=t.pseudos.eq,{radio:!0,checkbox:!0,file:!0,password:!0,image:!0})t.pseudos[C]=me(C);for(C in{submit:!0,reset:!0})t.pseudos[C]=fe(C);function X(){}function T(e){for(var t=0,s=e.length,n="";t<s;t++)n+=e[t].value;return n}function L(e,t,n){var s=t.dir,o=t.next,i=o||s,r=n&&"parentNode"===i,c=Oe++;return t.first?function(t,n,o){for(;t=t[s];)if(1===t.nodeType||r)return e(t,n,o);return!1}:function(t,n,l){var d,u,h,m=[p,c];if(l){for(;t=t[s];)if((1===t.nodeType||r)&&e(t,n,l))return!0}else for(;t=t[s];)if(1===t.nodeType||r)if(u=(h=t[a]||(t[a]={}))[t.uniqueID]||(h[t.uniqueID]={}),o&&o===t.nodeName.toLowerCase())t=t[s]||t;else{if((d=u[i])&&d[0]===p&&d[1]===c)return m[2]=d[2];if((u[i]=m)[2]=e(t,n,l))return!0}return!1}}function $(e){return 1<e.length?function(t,n,s){for(var o=e.length;o--;)if(!e[o](t,n,s))return!1;return!0}:e[0]}function M(e,t,n,s,o){for(var a,r=[],i=0,c=e.length,l=t!=null;i<c;i++)(a=e[i])&&(n&&!n(a,s,o)||(r.push(a),l&&t.push(i)));return r}function V(e,t,n,s,i,r){return s&&!s[a]&&(s=V(s)),i&&!i[a]&&(i=V(i,r)),u(function(a,r,c,l){var u,h,m,g=[],p=[],b=r.length,j=a||function(e,t,n){for(var s=0,i=t.length;s<i;s++)o(e,t[s],n);return n}(t||"*",c.nodeType?[c]:c,[]),f=!e||!a&&t?j:M(j,g,e,c,l),d=n?i||(a?e:b||s)?[]:r:f;if(n&&n(f,d,c,l),s)for(h=M(d,p),s(h,[],c,l),u=h.length;u--;)(m=h[u])&&(d[p[u]]=!(f[p[u]]=m));if(a){if(i||e){if(i){for(h=[],u=d.length;u--;)(m=d[u])&&h.push(f[u]=m);i(null,d=[],h,l)}for(u=d.length;u--;)(m=d[u])&&-1<(h=i?w(a,m):g[u])&&(a[h]=!(r[h]=m))}}else d=M(d===r?d.splice(b,d.length):d),i?i(null,r,d,l):v.apply(r,d)})}function H(e){for(var s,o,r,c=e.length,l=t.relative[e[0].type],d=l||t.relative[" "],n=l?1:0,u=L(function(e){return e===r},d,!0),h=L(function(e){return-1<w(r,e)},d,!0),i=[function(e,t,n){var s=!l&&(n||t!==R)||((r=t).nodeType?u(e,t,n):h(e,t,n));return r=null,s}];n<c;n++)if(o=t.relative[e[n].type])i=[L($(i),o)];else{if((o=t.filter[e[n].type].apply(null,e[n].matches))[a]){for(s=++n;s<c;s++)if(t.relative[e[s].type])break;return V(1<n&&$(i),1<n&&T(e.slice(0,n-1).concat({value:" "===e[n-2].type?"*":""})).replace(D,"$1"),o,n<s&&H(e.slice(n,s)),s<c&&H(e=e.slice(s)),s<c&&T(e))}i.push(o)}return $(i)}return X.prototype=t.filters=t.pseudos,t.setFilters=new X,k=o.tokenize=function(e,n){var s,i,a,r,c,l,d,u=J[e+" "];if(u)return n?0:u.slice(0);for(s=e,l=[],d=t.preFilter;s;){for(r in a&&!(i=je.exec(s))||(i&&(s=s.slice(i[0].length)||s),l.push(c=[])),a=!1,(i=Q.exec(s))&&(a=i.shift(),c.push({value:a,type:i[0].replace(D," ")}),s=s.slice(a.length)),t.filter)!(i=S[r].exec(s))||d[r]&&!(i=d[r](i))||(a=i.shift(),c.push({value:a,type:r,matches:i}),s=s.slice(a.length));if(!a)break}return n?s.length:s?o.error(e):J(e,l).slice(0)},I=o.compile=function(e,s){var r,c,d,h,m,f,b=[],j=[],i=Z[e+" "];if(!i){for(s||(s=k(e)),c=s.length;c--;)(i=H(s[c]))[a]?b.push(i):j.push(i);(i=Z(e,(d=j,r=0<(f=b).length,h=0<d.length,m=function(e,s,i,a,c){var u,j,_,y=0,m="0",w=e&&[],b=[],O=R,x=e||h&&t.find.TAG("*",c),C=p+=O==null?1:Math.random()||.1,E=x.length;for(c&&(R=s==n||s||c);m!==E&&null!=(u=x[m]);m++){if(h&&u){for(_=0,s||u.ownerDocument==n||(g(u),i=!l);j=d[_++];)if(j(u,s||n,i)){a.push(u);break}c&&(p=C)}r&&((u=!j&&u)&&y--,e&&w.push(u))}if(y+=m,r&&m!==y){for(_=0;j=f[_++];)j(w,b,s,i);if(e){if(0<y)for(;m--;)w[m]||b[m]||(b[m]=xe.call(a));b=M(b)}v.apply(a,b),c&&!e&&0<b.length&&1<y+f.length&&o.uniqueSort(a)}return c&&(p=C,R=O),w},r?u(m):m))).selector=e}return i},te=o.select=function(e,n,s,o){var i,a,r,u,h,d="function"==typeof e&&e,c=!o&&k(e=d.selector||e);if(s=s||[],1===c.length){if(2<(i=c[0]=c[0].slice(0)).length&&"ID"===(a=i[0]).type&&9===n.nodeType&&l&&t.relative[i[1].type]){if(!(n=(t.find.ID(a.matches[0].replace(f,m),n)||[])[0]))return s;d&&(n=n.parentNode),e=e.slice(i.shift().value.length)}for(r=S.needsContext.test(e)?0:i.length;r--;){if(a=i[r],t.relative[u=a.type])break;if((h=t.find[u])&&(o=h(a.matches[0].replace(f,m),q.test(i[0].type)&&K(n.parentNode)||n))){if(i.splice(r,1),!(e=o.length&&T(i)))return v.apply(s,o),s;break}}}return(d||I(e,c))(o,n,!l,s,!n||q.test(e)&&K(n.parentNode)||n),s},i.sortStable=a.split("").sort(Y).join("")===a,i.detectDuplicates=!!x,g(),i.sortDetached=h(function(e){return 1&e.compareDocumentPosition(n.createElement("fieldset"))}),h(function(e){return e.innerHTML="<a href='#'></a>","#"===e.firstChild.getAttribute("href")})||W("type|href|height|width",function(e,t,n){if(!n)return e.getAttribute(t,"type"===t.toLowerCase()?1:2)}),i.attributes&&h(function(e){return e.innerHTML="<input/>",e.firstChild.setAttribute("value",""),""===e.firstChild.getAttribute("value")})||W("value",function(e,t,n){if(!n&&"input"===e.nodeName.toLowerCase())return e.defaultValue}),h(function(e){return null==e.getAttribute("disabled")})||W(B,function(e,t,n){var s;if(!n)return!0===e[t]?t.toLowerCase():(s=e.getAttributeNode(t))&&s.specified?s.value:null}),o}(e),n.find=b,n.expr=b.selectors,n.expr[":"]=n.expr.pseudos,n.uniqueSort=n.unique=b.uniqueSort,n.text=b.getText,n.isXMLDoc=b.isXML,n.contains=b.contains,n.escapeSelector=b.escape;var x=function(e,t,s){for(var o=[],i=void 0!==s;(e=e[t])&&9!==e.nodeType;)if(1===e.nodeType){if(i&&n(e).is(s))break;o.push(e)}return o},lt=function(e,t){for(var n=[];e;e=e.nextSibling)1===e.nodeType&&e!==t&&n.push(e);return n},rt=n.expr.match.needsContext;function d(e,t){return e.nodeName&&e.nodeName.toLowerCase()===t.toLowerCase()}se=/^<([a-z][^/\0>:\x20\t\r\n\f]*)[\x20\t\r\n\f]*\/?>(?:<\/\1>|)$/i;function re(e,t,s){return o(t)?n.grep(e,function(e,n){return!!t.call(e,n,e)!==s}):t.nodeType?n.grep(e,function(e){return e===t!==s}):"string"!=typeof t?n.grep(e,function(e){return-1<U.call(t,e)!==s}):n.filter(t,e,s)}n.filter=function(e,t,s){var o=t[0];return s&&(e=":not("+e+")"),1===t.length&&1===o.nodeType?n.find.matchesSelector(o,e)?[o]:[]:n.find.matches(e,n.grep(t,function(e){return 1===e.nodeType}))},n.fn.extend({find:function(e){var t,s,o=this.length,i=this;if("string"!=typeof e)return this.pushStack(n(e).filter(function(){for(t=0;t<o;t++)if(n.contains(i[t],this))return!0}));for(s=this.pushStack([]),t=0;t<o;t++)n.find(e,i[t],s);return 1<o?n.uniqueSort(s):s},filter:function(e){return this.pushStack(re(this,e||[],!1))},not:function(e){return this.pushStack(re(this,e||[],!0))},is:function(e){return!!re(this,"string"==typeof e&&rt.test(e)?n(e):e||[],!1).length}}),st=/^(?:\s*(<[\w\W]+>)[^>]*|#([\w-]+))$/,(n.fn.init=function(e,t,s){var a,r;if(!e)return this;if(s=s||it,"string"==typeof e){if(!(a="<"===e[0]&&">"===e[e.length-1]&&3<=e.length?[null,e,null]:st.exec(e))||!a[1]&&t)return!t||t.jquery?(t||s).find(e):this.constructor(t).find(e);if(a[1]){if(t=t instanceof n?t[0]:t,n.merge(this,n.parseHTML(a[1],t&&t.nodeType?t.ownerDocument||t:i,!0)),se.test(a[1])&&n.isPlainObject(t))for(a in t)o(this[a])?this[a](t[a]):this.attr(a,t[a]);return this}return(r=i.getElementById(a[2]))&&(this[0]=r,this.length=1),this}return e.nodeType?(this[0]=e,this.length=1,this):o(e)?void 0!==s.ready?s.ready(e):e(n):n.makeArray(e,this)}).prototype=n.fn,it=n(i),nt=/^(?:parents|prev(?:Until|All))/,tt={children:!0,contents:!0,next:!0,prev:!0};function et(e,t){for(;(e=e[t])&&1!==e.nodeType;);return e}n.fn.extend({has:function(e){var t=n(e,this),s=t.length;return this.filter(function(){for(var e=0;e<s;e++)if(n.contains(this,t[e]))return!0})},closest:function(e,t){var s,i=0,r=this.length,o=[],a="string"!=typeof e&&n(e);if(!rt.test(e))for(;i<r;i++)for(s=this[i];s&&s!==t;s=s.parentNode)if(s.nodeType<11&&(a?-1<a.index(s):1===s.nodeType&&n.find.matchesSelector(s,e))){o.push(s);break}return this.pushStack(1<o.length?n.uniqueSort(o):o)},index:function(e){return e?"string"==typeof e?U.call(n(e),this[0]):U.call(this,e.jquery?e[0]:e):this[0]&&this[0].parentNode?this.first().prevAll().length:-1},add:function(e,t){return this.pushStack(n.uniqueSort(n.merge(this.get(),n(e,t))))},addBack:function(e){return this.add(e==null?this.prevObject:this.prevObject.filter(e))}}),n.each({parent:function(e){var t=e.parentNode;return t&&11!==t.nodeType?t:null},parents:function(e){return x(e,"parentNode")},parentsUntil:function(e,t,n){return x(e,"parentNode",n)},next:function(e){return et(e,"nextSibling")},prev:function(e){return et(e,"previousSibling")},nextAll:function(e){return x(e,"nextSibling")},prevAll:function(e){return x(e,"previousSibling")},nextUntil:function(e,t,n){return x(e,"nextSibling",n)},prevUntil:function(e,t,n){return x(e,"previousSibling",n)},siblings:function(e){return lt((e.parentNode||{}).firstChild,e)},children:function(e){return lt(e.firstChild)},contents:function(e){return null!=e.contentDocument&&yt(e.contentDocument)?e.contentDocument:(d(e,"template")&&(e=e.content||e),n.merge([],e.childNodes))}},function(e,t){n.fn[e]=function(s,o){var i=n.map(this,t,s);return"Until"!==e.slice(-5)&&(o=s),o&&"string"==typeof o&&(i=n.filter(o,i)),1<this.length&&(tt[e]||n.uniqueSort(i),nt.test(e)&&i.reverse()),this.pushStack(i)}}),h=/[^\x20\t\r\n\f]+/g;function L(e){return e}function Q(e){throw e}function Ze(e,t,n,s){var i;try{e&&o(i=e.promise)?i.call(e).done(t).fail(n):e&&o(i=e.then)?i.call(e,t,n):t.apply(void 0,[e].slice(s))}catch(e){n.apply(void 0,[e])}}n.Callbacks=function(e){e="string"==typeof e?(u=e,d={},n.each(u.match(h)||[],function(e,t){d[t]=!0}),d):n.extend({},e);var s,a,c,d,u,f,t=[],r=[],i=-1,m=function(){for(a=a||e.once,f=c=!0;r.length;i=-1)for(s=r.shift();++i<t.length;)!1===t[i].apply(s[0],s[1])&&e.stopOnFalse&&(i=t.length,s=!1);e.memory||(s=!1),c=!1,a&&(t=s?[]:"")},l={add:function(){return t&&(s&&!c&&(i=t.length-1,r.push(s)),function s(i){n.each(i,function(n,i){o(i)?e.unique&&l.has(i)||t.push(i):i&&i.length&&"string"!==M(i)&&s(i)})}(arguments),s&&!c&&m()),this},remove:function(){return n.each(arguments,function(e,s){for(var o;-1<(o=n.inArray(s,t,o));)t.splice(o,1),o<=i&&i--}),this},has:function(e){return e?-1<n.inArray(e,t):0<t.length},empty:function(){return t&&(t=[]),this},disable:function(){return a=r=[],t=s="",this},disabled:function(){return!t},lock:function(){return a=r=[],s||c||(t=s=""),this},locked:function(){return!!a},fireWith:function(e,t){return a||(t=[e,(t=t||[]).slice?t.slice():t],r.push(t),c||m()),this},fire:function(){return l.fireWith(this,arguments),this},fired:function(){return!!f}};return l},n.extend({Deferred:function(t){var i=[["notify","progress",n.Callbacks("memory"),n.Callbacks("memory"),2],["resolve","done",n.Callbacks("once memory"),n.Callbacks("once memory"),0,"resolved"],["reject","fail",n.Callbacks("once memory"),n.Callbacks("once memory"),1,"rejected"]],r="pending",a={state:function(){return r},always:function(){return s.done(arguments).fail(arguments),this},catch:function(e){return a.then(null,e)},pipe:function(){var e=arguments;return n.Deferred(function(t){n.each(i,function(n,i){var a=o(e[i[4]])&&e[i[4]];s[i[1]](function(){var e=a&&a.apply(this,arguments);e&&o(e.promise)?e.promise().progress(t.notify).done(t.resolve).fail(t.reject):t[i[0]+"With"](this,a?[e]:arguments)})}),e=null}).promise()},then:function(t,s,a){var r=0;function c(t,s,i,a){return function(){var l=this,d=arguments,h=function(){var e,n;if(!(t<r)){if((e=i.apply(l,d))===s.promise())throw new TypeError("Thenable self-resolution");n=e&&("object"==typeof e||"function"==typeof e)&&e.then,o(n)?a?n.call(e,c(r,s,L,a),c(r,s,Q,a)):(r++,n.call(e,c(r,s,L,a),c(r,s,Q,a),c(r,s,L,s.notifyWith))):(i!==L&&(l=void 0,d=[e]),(a||s.resolveWith)(l,d))}},u=a?h:function(){try{h()}catch(e){n.Deferred.exceptionHook&&n.Deferred.exceptionHook(e,u.stackTrace),r<=t+1&&(i!==Q&&(l=void 0,d=[e]),s.rejectWith(l,d))}};t?u():(n.Deferred.getStackHook&&(u.stackTrace=n.Deferred.getStackHook()),e.setTimeout(u))}}return n.Deferred(function(e){i[0][3].add(c(0,e,o(a)?a:L,e.notifyWith)),i[1][3].add(c(0,e,o(t)?t:L)),i[2][3].add(c(0,e,o(s)?s:Q))}).promise()},promise:function(e){return e!=null?n.extend(e,a):a}},s={};return n.each(i,function(e,t){var n=t[2],o=t[5];a[t[1]]=n.add,o&&n.add(function(){r=o},i[3-e][2].disable,i[3-e][3].disable,i[0][2].lock,i[0][3].lock),n.add(t[3].fire),s[t[0]]=function(){return s[t[0]+"With"](this===s?void 0:this,arguments),this},s[t[0]+"With"]=n.fireWith}),a.promise(s),t&&t.call(s,s),s},when:function(e){var a=arguments.length,t=a,r=Array(t),i=j.call(arguments),s=n.Deferred(),c=function(e){return function(t){r[e]=this,i[e]=1<arguments.length?j.call(arguments):t,--a||s.resolveWith(r,i)}};if(a<=1&&(Ze(e,s.done(c(t)).resolve,s.reject,!a),"pending"===s.state()||o(i[t]&&i[t].then)))return s.then();for(;t--;)Ze(i[t],c(t),s.reject);return s.promise()}}),Ge=/^(Eval|Internal|Range|Reference|Syntax|Type|URI)Error$/,n.Deferred.exceptionHook=function(t,n){e.console&&e.console.warn&&t&&Ge.test(t.name)&&e.console.warn("jQuery.Deferred exception: "+t.message,t.stack,n)},n.readyException=function(t){e.setTimeout(function(){throw t})},G=n.Deferred();function q(){i.removeEventListener("DOMContentLoaded",q),e.removeEventListener("load",q),n.ready()}n.fn.ready=function(e){return G.then(e).catch(function(e){n.readyException(e)}),this},n.extend({isReady:!1,readyWait:1,ready:function(e){(!0===e?--n.readyWait:n.isReady)||(n.isReady=!0)!==e&&0<--n.readyWait||G.resolveWith(i,[n])}}),n.ready.then=G.then,"complete"===i.readyState||"loading"!==i.readyState&&!i.documentElement.doScroll?e.setTimeout(n.ready):(i.addEventListener("DOMContentLoaded",q),e.addEventListener("load",q));var p=function(e,t,s,i,a,r,c){var l=0,u=e.length,d=s==null;if("object"===M(s))for(l in a=!0,s)p(e,t,l,s[l],!0,r,c);else if(void 0!==i&&(a=!0,o(i)||(c=!0),d&&(c?(t.call(e,i),t=null):(d=t,t=function(e,t,s){return d.call(n(e),s)})),t))for(;l<u;l++)t(e[l],s,c?i:i.call(e[l],l,t(e[l],s)));return a?e:d?t.call(e):u?t(e[0],s):r},qt=/^-ms-/,Kt=/-([a-z])/g;function Ut(e,t){return t.toUpperCase()}function f(e){return e.replace(qt,"ms-").replace(Kt,Ut)}T=function(e){return 1===e.nodeType||9===e.nodeType||!+e.nodeType};function H(){this.expando=n.expando+H.uid++}H.uid=1,H.prototype={cache:function(e){var t=e[this.expando];return t||(t={},T(e)&&(e.nodeType?e[this.expando]=t:Object.defineProperty(e,this.expando,{value:t,configurable:!0}))),t},set:function(e,t,n){var s,o=this.cache(e);if("string"==typeof t)o[f(t)]=n;else for(s in t)o[f(s)]=t[s];return o},get:function(e,t){return void 0===t?this.cache(e):e[this.expando]&&e[this.expando][f(t)]},access:function(e,t,n){return void 0===t||t&&"string"==typeof t&&void 0===n?this.get(e,t):(this.set(e,t,n),void 0!==n?n:t)},remove:function(e,t){var o,s=e[this.expando];if(void 0!==s){if(void 0!==t)for(o=(t=Array.isArray(t)?t.map(f):(t=f(t))in s?[t]:t.match(h)||[]).length;o--;)delete s[t[o]];(void 0===t||n.isEmptyObject(s))&&(e.nodeType?e[this.expando]=void 0:delete e[this.expando])}},hasData:function(e){var t=e[this.expando];return void 0!==t&&!n.isEmptyObject(t)}};var s=new H,r=new H,Wt=/^(?:\{[\w\W]*\}|\[[\w\W]*\])$/,$t=/[A-Z]/g;function Ve(e,t,n){var s,o;if(void 0===n&&1===e.nodeType)if(o="data-"+t.replace($t,"-$&").toLowerCase(),"string"==typeof(n=e.getAttribute(o))){try{n="true"===(s=n)||"false"!==s&&("null"===s?null:s===+s+""?+s:Wt.test(s)?JSON.parse(s):s)}catch{}r.set(e,t,n)}else n=void 0;return n}n.extend({hasData:function(e){return r.hasData(e)||s.hasData(e)},data:function(e,t,n){return r.access(e,t,n)},removeData:function(e,t){r.remove(e,t)},_data:function(e,t,n){return s.access(e,t,n)},_removeData:function(e,t){s.remove(e,t)}}),n.fn.extend({data:function(e,t){var o,i,a,n=this[0],c=n&&n.attributes;if(void 0===e){if(this.length&&(a=r.get(n),1===n.nodeType&&!s.get(n,"hasDataAttrs"))){for(i=c.length;i--;)c[i]&&0===(o=c[i].name).indexOf("data-")&&(o=f(o.slice(5)),Ve(n,o,a[o]));s.set(n,"hasDataAttrs",!0)}return a}return"object"==typeof e?this.each(function(){r.set(this,e)}):p(this,function(t){var s;if(n&&void 0===t)return void 0!==(s=r.get(n,e))?s:void 0!==(s=Ve(n,e))?s:void 0;this.each(function(){r.set(this,e,t)})},null,t,1<arguments.length,null,!0)},removeData:function(e){return this.each(function(){r.remove(this,e)})}}),n.extend({queue:function(e,t,o){var i;if(e)return t=(t||"fx")+"queue",i=s.get(e,t),o&&(!i||Array.isArray(o)?i=s.access(e,t,n.makeArray(o)):i.push(o)),i||[]},dequeue:function(e,t){t=t||"fx";var s=n.queue(e,t),a=s.length,o=s.shift(),i=n._queueHooks(e,t);"inprogress"===o&&(o=s.shift(),a--),o&&("fx"===t&&s.unshift("inprogress"),delete i.stop,o.call(e,function(){n.dequeue(e,t)},i)),!a&&i&&i.empty.fire()},_queueHooks:function(e,t){var o=t+"queueHooks";return s.get(e,o)||s.access(e,o,{empty:n.Callbacks("once memory").add(function(){s.remove(e,[t+"queue",o])})})}}),n.fn.extend({queue:function(e,t){var s=2;return"string"!=typeof e&&(t=e,e="fx",s--),arguments.length<s?n.queue(this[0],e):void 0===t?this:this.each(function(){var s=n.queue(this,e,t);n._queueHooks(this,e),"fx"===e&&"inprogress"!==s[0]&&n.dequeue(this,e)})},dequeue:function(e){return this.each(function(){n.dequeue(this,e)})},clearQueue:function(e){return this.queue(e||"fx",[])},promise:function(e,t){var o,a=1,r=n.Deferred(),i=this,c=this.length,l=function(){--a||r.resolveWith(i,[i])};for("string"!=typeof e&&(t=e,e=void 0),e=e||"fx";c--;)(o=s.get(i[c],e+"queueHooks"))&&o.empty&&(a++,o.empty.add(l));return l(),r.promise(t)}});var Re=/[+-]?(?:\d*\.|)\d+(?:[eE][+-]?\d+|)/.source,P=new RegExp("^(?:([+-])=|)("+Re+")([a-z%]*)$","i"),v=["Top","Right","Bottom","Left"],_=i.documentElement,E=function(e){return n.contains(e.ownerDocument,e)},Vt={composed:!0};_.getRootNode&&(E=function(e){return n.contains(e.ownerDocument,e)||e.getRootNode(Vt)===e.ownerDocument}),$=function(e,t){return"none"===(e=t||e).style.display||""===e.style.display&&E(e)&&"none"===n.css(e,"display")};function Le(e,t,s,o){var c,l,d=20,u=o?function(){return o.cur()}:function(){return n.css(e,t,"")},r=u(),a=s&&s[3]||(n.cssNumber[t]?"":"px"),i=e.nodeType&&(n.cssNumber[t]||"px"!==a&&+r)&&P.exec(n.css(e,t));if(i&&i[3]!==a){for(r/=2,a=a||i[3],i=+r||1;d--;)n.style(e,t,i+a),(1-c)*(1-(c=u()/r||.5))<=0&&(d=0),i/=c;i*=2,n.style(e,t,i+a),s=s||[]}return s&&(i=+i||+r||0,l=s[1]?i+(s[1]+1)*s[2]:+s[2],o&&(o.unit=a,o.start=i,o.end=l)),l}ce={};function C(e,t){for(var i,a,c,l,d,u,h,r=[],o=0,m=e.length;o<m;o++)(i=e[o]).style&&(u=i.style.display,t?("none"===u&&(r[o]=s.get(i,"display")||null,r[o]||(i.style.display="")),""===i.style.display&&$(i)&&(r[o]=(a=l=c=void 0,l=(h=i).ownerDocument,d=h.nodeName,(a=ce[d])||(c=l.body.appendChild(l.createElement(d)),a=n.css(c,"display"),c.parentNode.removeChild(c),"none"===a&&(a="block"),ce[d]=a)))):"none"!==u&&(r[o]="none",s.set(i,"display",u)));for(o=0;o<m;o++)null!=r[o]&&(e[o].style.display=r[o]);return e}n.fn.extend({show:function(){return C(this,!0)},hide:function(){return C(this)},toggle:function(e){return"boolean"==typeof e?e?this.show():this.hide():this.each(function(){$(this)?n(this).show():n(this).hide()})}});var Z,I=/^(?:checkbox|radio)$/i,Ne=/<([a-z][^/\0>\x20\t\r\n\f]*)/i,ze=/^$|^module$|\/(?:java|ecma)script/i,k=i.createDocumentFragment().appendChild(i.createElement("div"));(Z=i.createElement("input")).setAttribute("type","radio"),Z.setAttribute("checked","checked"),Z.setAttribute("name","t"),k.appendChild(Z),a.checkClone=k.cloneNode(!0).cloneNode(!0).lastChild.checked,k.innerHTML="<textarea>x</textarea>",a.noCloneChecked=!!k.cloneNode(!0).lastChild.defaultValue,k.innerHTML="<option></option>",a.option=!!k.lastChild,l={thead:[1,"<table>","</table>"],col:[2,"<table><colgroup>","</colgroup></table>"],tr:[2,"<table><tbody>","</tbody></table>"],td:[3,"<table><tbody><tr>","</tr></tbody></table>"],_default:[0,"",""]};function c(e,t){var s;return s="undefined"!=typeof e.getElementsByTagName?e.getElementsByTagName(t||"*"):"undefined"!=typeof e.querySelectorAll?e.querySelectorAll(t||"*"):[],void 0===t||t&&d(e,t)?n.merge([e],s):s}function ve(e,t){for(var n=0,o=e.length;n<o;n++)s.set(e[n],"globalEval",!t||s.get(t[n],"globalEval"))}l.tbody=l.tfoot=l.colgroup=l.caption=l.thead,l.th=l.td,a.option||(l.optgroup=l.option=[1,"<select multiple='multiple'>","</select>"]),Fe=/<|&#?\w+;/;function Ae(e,t,s,o,i){for(var a,r,h,m,p,g,u=t.createDocumentFragment(),f=[],d=0,v=e.length;d<v;d++)if((a=e[d])||0===a)if("object"===M(a))n.merge(f,a.nodeType?[a]:a);else if(Fe.test(a)){for(r=r||u.appendChild(t.createElement("div")),g=(Ne.exec(a)||["",""])[1].toLowerCase(),h=l[g]||l._default,r.innerHTML=h[1]+n.htmlPrefilter(a)+h[2],m=h[0];m--;)r=r.lastChild;n.merge(f,r.childNodes),(r=u.firstChild).textContent=""}else f.push(t.createTextNode(a));for(u.textContent="",d=0;a=f[d++];)if(o&&-1<n.inArray(a,o))i&&i.push(a);else if(p=E(a),r=c(u.appendChild(a),"script"),p&&ve(r),s)for(m=0;a=r[m++];)ze.test(a.type||"")&&s.push(a);return u}ye=/^([^.]*)(?:\.(.+)|)/;function F(){return!0}function D(){return!1}function Bt(e,t){return e===function(){try{return i.activeElement}catch{}}()==("focus"===t)}function ae(e,t,s,o,i,a){var r,c;if("object"==typeof t){for(c in"string"!=typeof s&&(o=o||s,s=void 0),t)ae(e,c,s,o,t[c],a);return e}if(o==null&&i==null?(i=s,o=s=void 0):i==null&&("string"==typeof s?(i=o,o=void 0):(i=o,o=s,s=void 0)),!1===i)i=D;else if(!i)return e;return 1===a&&(r=i,(i=function(e){return n().off(e),r.apply(this,arguments)}).guid=r.guid||(r.guid=n.guid++)),e.each(function(){n.event.add(this,t,i,o,s)})}function W(e,t,o){o?(s.set(e,t,!1),n.event.add(e,t,{namespace:!1,handler:function(e){var a,r,i=s.get(this,t);if(1&e.isTrigger&&this[t]){if(i.length)(n.event.special[t]||{}).delegateType&&e.stopPropagation();else if(i=j.call(arguments),s.set(this,t,i),r=o(this,t),this[t](),i!==(a=s.get(this,t))||r?s.set(this,t,!1):a={},i!==a)return e.stopImmediatePropagation(),e.preventDefault(),a&&a.value}else i.length&&(s.set(this,t,{value:n.event.trigger(n.extend(i[0],n.Event.prototype),i.slice(1),this)}),e.stopImmediatePropagation())}})):void 0===s.get(e,t)&&n.event.add(e,t,F)}n.event={global:{},add:function(e,t,o,i,a){var r,c,l,d,u,m,p,g,v,b,j,f=s.get(e);if(T(e))for(o.handler&&(o=(g=o).handler,a=g.selector),a&&n.find.matchesSelector(_,a),o.guid||(o.guid=n.guid++),(u=f.events)||(u=f.events=Object.create(null)),(m=f.handle)||(m=f.handle=function(t){return"undefined"!=typeof n&&n.event.triggered!==t.type?n.event.dispatch.apply(e,arguments):void 0}),p=(t=(t||"").match(h)||[""]).length;p--;)r=j=(b=ye.exec(t[p])||[])[1],v=(b[2]||"").split(".").sort(),r&&(c=n.event.special[r]||{},r=(a?c.delegateType:c.bindType)||r,c=n.event.special[r]||{},l=n.extend({type:r,origType:j,data:i,handler:o,guid:o.guid,selector:a,needsContext:a&&n.expr.match.needsContext.test(a),namespace:v.join(".")},g),(d=u[r])||((d=u[r]=[]).delegateCount=0,c.setup&&!1!==c.setup.call(e,i,v,m)||e.addEventListener&&e.addEventListener(r,m)),c.add&&(c.add.call(e,l),l.handler.guid||(l.handler.guid=o.guid)),a?d.splice(d.delegateCount++,0,l):d.push(l),n.event.global[r]=!0)},remove:function(e,t,o,i,a){var r,c,l,d,u,m,f,p,v,b,j,g=s.hasData(e)&&s.get(e);if(g&&(m=g.events)){for(f=(t=(t||"").match(h)||[""]).length;f--;)if(r=j=(u=ye.exec(t[f])||[])[1],v=(u[2]||"").split(".").sort(),r){for(l=n.event.special[r]||{},d=m[r=(i?l.delegateType:l.bindType)||r]||[],u=u[2]&&new RegExp("(^|\\.)"+v.join("\\.(?:.*\\.|)")+"(\\.|$)"),b=p=d.length;p--;)c=d[p],!a&&j!==c.origType||o&&o.guid!==c.guid||u&&!u.test(c.namespace)||i&&i!==c.selector&&("**"!==i||!c.selector)||(d.splice(p,1),c.selector&&d.delegateCount--,l.remove&&l.remove.call(e,c));b&&!d.length&&(l.teardown&&!1!==l.teardown.call(e,v,g.handle)||n.removeEvent(e,r,g.handle),delete m[r])}else for(r in m)n.event.remove(e,r+t[f],o,i,!0);n.isEmptyObject(m)&&s.remove(e,"handle events")}},dispatch:function(e){var o,i,a,l,d,u,c=new Array(arguments.length),t=n.event.fix(e),h=(s.get(this,"events")||Object.create(null))[t.type]||[],r=n.event.special[t.type]||{};for(c[0]=t,o=1;o<arguments.length;o++)c[o]=arguments[o];if(t.delegateTarget=this,!r.preDispatch||!1!==r.preDispatch.call(this,t)){for(d=n.event.handlers.call(this,t,h),o=0;(a=d[o++])&&!t.isPropagationStopped();)for(t.currentTarget=a.elem,u=0;(i=a.handlers[u++])&&!t.isImmediatePropagationStopped();)t.rnamespace&&!1!==i.namespace&&!t.rnamespace.test(i.namespace)||(t.handleObj=i,t.data=i.data,void 0!==(l=((n.event.special[i.origType]||{}).handle||i.handler).apply(a.elem,c))&&!1===(t.result=l)&&(t.preventDefault(),t.stopPropagation()));return r.postDispatch&&r.postDispatch.call(this,t),t.result}},handlers:function(e,t){var o,i,a,r,d,l=[],c=t.delegateCount,s=e.target;if(c&&s.nodeType&&!("click"===e.type&&1<=e.button))for(;s!==this;s=s.parentNode||this)if(1===s.nodeType&&("click"!==e.type||!0!==s.disabled)){for(a=[],r={},i=0;i<c;i++)void 0===r[o=(d=t[i]).selector+" "]&&(r[o]=d.needsContext?-1<n(o,this).index(s):n.find(o,this,null,[s]).length),r[o]&&a.push(d);a.length&&l.push({elem:s,handlers:a})}return s=this,c<t.length&&l.push({elem:s,handlers:t.slice(c)}),l},addProp:function(e,t){Object.defineProperty(n.Event.prototype,e,{enumerable:!0,configurable:!0,get:o(t)?function(){if(this.originalEvent)return t(this.originalEvent)}:function(){if(this.originalEvent)return this.originalEvent[e]},set:function(t){Object.defineProperty(this,e,{enumerable:!0,configurable:!0,writable:!0,value:t})}})},fix:function(e){return e[n.expando]?e:new n.Event(e)},special:{load:{noBubble:!0},click:{setup:function(e){var t=this||e;return I.test(t.type)&&t.click&&d(t,"input")&&W(t,"click",F),!1},trigger:function(e){var t=this||e;return I.test(t.type)&&t.click&&d(t,"input")&&W(t,"click"),!0},_default:function(e){var t=e.target;return I.test(t.type)&&t.click&&d(t,"input")&&s.get(t,"click")||d(t,"a")}},beforeunload:{postDispatch:function(e){void 0!==e.result&&e.originalEvent&&(e.originalEvent.returnValue=e.result)}}}},n.removeEvent=function(e,t,n){e.removeEventListener&&e.removeEventListener(t,n)},n.Event=function(e,t){if(!(this instanceof n.Event))return new n.Event(e,t);e&&e.type?(this.originalEvent=e,this.type=e.type,this.isDefaultPrevented=e.defaultPrevented||void 0===e.defaultPrevented&&!1===e.returnValue?F:D,this.target=e.target&&3===e.target.nodeType?e.target.parentNode:e.target,this.currentTarget=e.currentTarget,this.relatedTarget=e.relatedTarget):this.type=e,t&&n.extend(this,t),this.timeStamp=e&&e.timeStamp||Date.now(),this[n.expando]=!0},n.Event.prototype={constructor:n.Event,isDefaultPrevented:D,isPropagationStopped:D,isImmediatePropagationStopped:D,isSimulated:!1,preventDefault:function(){var e=this.originalEvent;this.isDefaultPrevented=F,e&&!this.isSimulated&&e.preventDefault()},stopPropagation:function(){var e=this.originalEvent;this.isPropagationStopped=F,e&&!this.isSimulated&&e.stopPropagation()},stopImmediatePropagation:function(){var e=this.originalEvent;this.isImmediatePropagationStopped=F,e&&!this.isSimulated&&e.stopImmediatePropagation(),this.stopPropagation()}},n.each({altKey:!0,bubbles:!0,cancelable:!0,changedTouches:!0,ctrlKey:!0,detail:!0,eventPhase:!0,metaKey:!0,pageX:!0,pageY:!0,shiftKey:!0,view:!0,char:!0,code:!0,charCode:!0,key:!0,keyCode:!0,button:!0,buttons:!0,clientX:!0,clientY:!0,offsetX:!0,offsetY:!0,pointerId:!0,pointerType:!0,screenX:!0,screenY:!0,targetTouches:!0,toElement:!0,touches:!0,which:!0},n.event.addProp),n.each({focus:"focusin",blur:"focusout"},function(e,t){n.event.special[e]={setup:function(){return W(this,e,Bt),!1},trigger:function(){return W(this,e),!0},_default:function(t){return s.get(t.target,e)},delegateType:t}}),n.each({mouseenter:"mouseover",mouseleave:"mouseout",pointerenter:"pointerover",pointerleave:"pointerout"},function(e,t){n.event.special[e]={delegateType:t,bindType:t,handle:function(e){var o,s=e.relatedTarget,i=e.handleObj;return s&&(s===this||n.contains(this,s))||(e.type=i.origType,o=i.handler.apply(this,arguments),e.type=t),o}}}),n.fn.extend({on:function(e,t,n,s){return ae(this,e,t,n,s)},one:function(e,t,n,s){return ae(this,e,t,n,s,1)},off:function(e,t,s){var o,i;if(e&&e.preventDefault&&e.handleObj)return o=e.handleObj,n(e.delegateTarget).off(o.namespace?o.origType+"."+o.namespace:o.origType,o.selector,o.handler),this;if("object"==typeof e){for(i in e)this.off(i,t,e[i]);return this}return!1!==t&&"function"!=typeof t||(s=t,t=void 0),!1===s&&(s=D),this.each(function(){n.event.remove(this,e,s,t)})}});var St=/<script|<style|<link/i,It=/checked\s*(?:[^=]|=\s*.checked.)/i,Ht=/^\s*<!\[CDATA\[|\]\]>\s*$/g;function Se(e,t){return d(e,"table")&&d(11!==t.nodeType?t:t.firstChild,"tr")&&n(e).children("tbody")[0]||e}function Pt(e){return e.type=(null!==e.getAttribute("type"))+"/"+e.type,e}function Rt(e){return"true/"===(e.type||"").slice(0,5)?e.type=e.type.slice(5):e.removeAttribute("type"),e}function Te(e,t){var o,i,a,c,l,d;if(1===t.nodeType){if(s.hasData(e)&&(a=s.get(e).events))for(i in s.remove(t,"handle events"),a)for(o=0,c=a[i].length;o<c;o++)n.event.add(t,i,a[i][o]);r.hasData(e)&&(l=r.access(e),d=n.extend({},l),r.set(t,d))}}function A(e,t,i,r){t=bt(t);var l,u,h,f,p,v,d=0,m=e.length,j=m-1,g=t[0],b=o(g);if(b||1<m&&"string"==typeof g&&!a.checkClone&&It.test(g))return e.each(function(n){var s=e.eq(n);b&&(t[0]=g.call(this,n,s.html())),A(s,t,i,r)});if(m&&(v=(h=Ae(t,e[0].ownerDocument,!1,e,r)).firstChild,1===h.childNodes.length&&(h=v),v||r)){for(f=(u=n.map(c(h,"script"),Pt)).length;d<m;d++)l=h,d!==j&&(l=n.clone(l,!0,!0),f&&n.merge(u,c(l,"script"))),i.call(e[d],l,d);if(f)for(p=u[u.length-1].ownerDocument,n.map(u,Rt),d=0;d<f;d++)l=u[d],ze.test(l.type||"")&&!s.access(l,"globalEval")&&n.contains(p,l)&&(l.src&&"module"!==(l.type||"").toLowerCase()?n._evalUrl&&!l.noModule&&n._evalUrl(l.src,{nonce:l.nonce||l.getAttribute("nonce")},p):Ce(l.textContent.replace(Ht,""),l,p))}return e}function De(e,t,s){for(var o,a=t?n.filter(t,e):e,i=0;null!=(o=a[i]);i++)s||1!==o.nodeType||n.cleanData(c(o)),o.parentNode&&(s&&E(o)&&ve(c(o,"script")),o.parentNode.removeChild(o));return e}n.extend({htmlPrefilter:function(e){return e},clone:function(e,t,s){var o,i,r,d,u,h,m,l=e.cloneNode(!0),f=E(e);if(!(a.noCloneChecked||1!==e.nodeType&&11!==e.nodeType||n.isXMLDoc(e)))for(i=c(l),o=0,h=(r=c(e)).length;o<h;o++)d=r[o],u=i[o],void 0,"input"===(m=u.nodeName.toLowerCase())&&I.test(d.type)?u.checked=d.checked:"input"!==m&&"textarea"!==m||(u.defaultValue=d.defaultValue);if(t)if(s)for(r=r||c(e),i=i||c(l),o=0,h=r.length;o<h;o++)Te(r[o],i[o]);else Te(e,l);return 0<(i=c(l,"script")).length&&ve(i,!f&&c(e,"script")),l},cleanData:function(e){for(var t,o,i,c=n.event.special,a=0;void 0!==(t=e[a]);a++)if(T(t)){if(o=t[s.expando]){if(o.events)for(i in o.events)c[i]?n.event.remove(t,i):n.removeEvent(t,i,o.handle);t[s.expando]=void 0}t[r.expando]&&(t[r.expando]=void 0)}}}),n.fn.extend({detach:function(e){return De(this,e,!0)},remove:function(e){return De(this,e)},text:function(e){return p(this,function(e){return void 0===e?n.text(this):this.empty().each(function(){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||(this.textContent=e)})},null,e,arguments.length)},append:function(){return A(this,arguments,function(e){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||Se(this,e).appendChild(e)})},prepend:function(){return A(this,arguments,function(e){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var t=Se(this,e);t.insertBefore(e,t.firstChild)}})},before:function(){return A(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this)})},after:function(){return A(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this.nextSibling)})},empty:function(){for(var e,t=0;null!=(e=this[t]);t++)1===e.nodeType&&(n.cleanData(c(e,!1)),e.textContent="");return this},clone:function(e,t){return e=e!=null&&e,t=t??e,this.map(function(){return n.clone(this,e,t)})},html:function(e){return p(this,function(e){var t=this[0]||{},s=0,o=this.length;if(void 0===e&&1===t.nodeType)return t.innerHTML;if("string"==typeof e&&!St.test(e)&&!l[(Ne.exec(e)||["",""])[1].toLowerCase()]){e=n.htmlPrefilter(e);try{for(;s<o;s++)1===(t=this[s]||{}).nodeType&&(n.cleanData(c(t,!1)),t.innerHTML=e);t=0}catch{}}t&&this.empty().append(e)},null,e,arguments.length)},replaceWith:function(){var e=[];return A(this,arguments,function(t){var s=this.parentNode;n.inArray(this,e)<0&&(n.cleanData(c(this)),s&&s.replaceChild(t,this))},e)}}),n.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(e,t){n.fn[e]=function(e){for(var o,i=[],a=n(e),r=a.length-1,s=0;s<=r;s++)o=s===r?this:this.clone(!0),n(a[s])[t](o),_e.apply(i,o.get());return this.pushStack(i)}});var me=new RegExp("^("+Re+")(?!px)[a-z%]+$","i"),ne=/^--/,ee=function(t){var n=t.ownerDocument.defaultView;return n&&n.opener||(n=e),n.getComputedStyle(t)},Pe=function(e,t,n){var s,o,i={};for(s in t)i[s]=e.style[s],e.style[s]=t[s];for(s in o=n.call(e),t)e.style[s]=i[s];return o},Lt=new RegExp(v.join("|"),"i"),Ie=`[\\x20\\t\\r\\n\\f]`,Ft=new RegExp("^"+Ie+"+|((?:^|[^\\\\])(?:\\\\.)*)"+Ie+"+$","g");function V(e,t,s){var o,r,c,l,d=ne.test(t),i=e.style;return(s=s||ee(e))&&(o=s.getPropertyValue(t)||s[t],d&&(o=o.replace(Ft,"$1")),""!==o||E(e)||(o=n.style(e,t)),!a.pixelBoxStyles()&&me.test(o)&&Lt.test(t)&&(r=i.width,c=i.minWidth,l=i.maxWidth,i.minWidth=i.maxWidth=i.width=o,o=s.width,i.width=r,i.minWidth=c,i.maxWidth=l)),void 0!==o?o+"":o}function $e(e,t){return{get:function(){if(!e())return(this.get=t).apply(this,arguments);delete this.get}}}!function(){function s(){if(t){r.style.cssText="position:absolute;left:-11111px;width:60px;margin-top:1px;padding:0;border:0",t.style.cssText="position:relative;display:block;box-sizing:border-box;overflow:scroll;margin:auto;border:1px;padding:1px;width:60%;top:1%",_.appendChild(r).appendChild(t);var n=e.getComputedStyle(t),h="1%"!==n.top,u=12===o(n.marginLeft);t.style.right="60%",d=36===o(n.right),m=36===o(n.width),t.style.position="absolute",l=12===o(t.offsetWidth/3),_.removeChild(r),t=null}}function o(e){return Math.round(parseFloat(e))}var c,l,d,u,h,m,r=i.createElement("div"),t=i.createElement("div");t.style&&(t.style.backgroundClip="content-box",t.cloneNode(!0).style.backgroundClip="",a.clearCloneStyle="content-box"===t.style.backgroundClip,n.extend(a,{boxSizingReliable:function(){return s(),m},pixelBoxStyles:function(){return s(),d},pixelPosition:function(){return s(),h},reliableMarginLeft:function(){return s(),u},scrollboxSize:function(){return s(),l},reliableTrDimensions:function(){var t,n,s,o;return c==null&&(n=i.createElement("table"),t=i.createElement("tr"),s=i.createElement("div"),n.style.cssText="position:absolute;left:-11111px;border-collapse:separate",t.style.cssText="border:1px solid",t.style.height="1px",s.style.height="9px",s.style.display="block",_.appendChild(n).appendChild(t).appendChild(s),o=e.getComputedStyle(t),c=parseInt(o.height,10)+parseInt(o.borderTopWidth,10)+parseInt(o.borderBottomWidth,10)===t.offsetHeight,_.removeChild(n)),c}}))}();var We=["Webkit","Moz","ms"],wt=i.createElement("div").style,Ke={};function ge(e){var t=n.cssProps[e]||Ke[e];return t||(e in wt?e:Ke[e]=function(e){for(var n=e[0].toUpperCase()+e.slice(1),t=We.length;t--;)if((e=We[t]+n)in wt)return e}(e)||e)}var Et=/^(none|table(?!-c[ea]).+)/,xt={position:"absolute",visibility:"hidden",display:"block"},Xe={letterSpacing:"0",fontWeight:"400"};function Qe(e,t,n){var s=P.exec(t);return s?Math.max(0,s[2]-(n||0))+(s[3]||"px"):t}function fe(e,t,s,o,i,a){var r="width"===t?1:0,l=0,c=0;if(s===(o?"border":"content"))return 0;for(;r<4;r+=2)"margin"===s&&(c+=n.css(e,s+v[r],!0,i)),o?("content"===s&&(c-=n.css(e,"padding"+v[r],!0,i)),"margin"!==s&&(c-=n.css(e,"border"+v[r]+"Width",!0,i))):(c+=n.css(e,"padding"+v[r],!0,i),"padding"!==s?c+=n.css(e,"border"+v[r]+"Width",!0,i):l+=n.css(e,"border"+v[r]+"Width",!0,i));return!o&&0<=a&&(c+=Math.max(0,Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-a-c-l-.5))||0),c}function Je(e,t,s){var i=ee(e),r=(!a.boxSizingReliable()||s)&&"border-box"===n.css(e,"boxSizing",!1,i),c=r,o=V(e,t,i),l="offset"+t[0].toUpperCase()+t.slice(1);if(me.test(o)){if(!s)return o;o="auto"}return(!a.boxSizingReliable()&&r||!a.reliableTrDimensions()&&d(e,"tr")||"auto"===o||!parseFloat(o)&&"inline"===n.css(e,"display",!1,i))&&e.getClientRects().length&&(r="border-box"===n.css(e,"boxSizing",!1,i),(c=l in e)&&(o=e[l])),(o=parseFloat(o)||0)+fe(e,t,s||(r?"border":"content"),c,i,o)+"px"}function m(e,t,n,s,o){return new m.prototype.init(e,t,n,s,o)}n.extend({cssHooks:{opacity:{get:function(e,t){if(t){var n=V(e,"opacity");return""===n?"1":n}}}},cssNumber:{animationIterationCount:!0,columnCount:!0,fillOpacity:!0,flexGrow:!0,flexShrink:!0,fontWeight:!0,gridArea:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnStart:!0,gridRow:!0,gridRowEnd:!0,gridRowStart:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,widows:!0,zIndex:!0,zoom:!0},cssProps:{},style:function(e,t,s,o){if(e&&3!==e.nodeType&&8!==e.nodeType&&e.style){var i,r,l,d=f(t),u=ne.test(t),c=e.style;if(u||(t=ge(d)),r=n.cssHooks[t]||n.cssHooks[d],void 0===s)return r&&"get"in r&&void 0!==(i=r.get(e,!1,o))?i:c[t];"string"===(l=typeof s)&&(i=P.exec(s))&&i[1]&&(s=Le(e,t,i),l="number"),s!=null&&s==s&&("number"!==l||u||(s+=i&&i[3]||(n.cssNumber[d]?"":"px")),a.clearCloneStyle||""!==s||0!==t.indexOf("background")||(c[t]="inherit"),r&&"set"in r&&void 0===(s=r.set(e,s,o))||(u?c.setProperty(t,s):c[t]=s))}},css:function(e,t,s,o){var i,a,r,c=f(t);return ne.test(t)||(t=ge(c)),(r=n.cssHooks[t]||n.cssHooks[c])&&"get"in r&&(i=r.get(e,!0,s)),void 0===i&&(i=V(e,t,o)),"normal"===i&&t in Xe&&(i=Xe[t]),""===s||s?(a=parseFloat(i),!0===s||isFinite(a)?a||0:i):i}}),n.each(["height","width"],function(e,t){n.cssHooks[t]={get:function(e,s,o){if(s)return!Et.test(n.css(e,"display"))||e.getClientRects().length&&e.getBoundingClientRect().width?Je(e,t,o):Pe(e,xt,function(){return Je(e,t,o)})},set:function(e,s,o){var c,i=ee(e),l=!a.scrollboxSize()&&"absolute"===i.position,d=(l||o)&&"border-box"===n.css(e,"boxSizing",!1,i),r=o?fe(e,t,o,d,i):0;return d&&l&&(r-=Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-parseFloat(i[t])-fe(e,t,"border",!1,i)-.5)),r&&(c=P.exec(s))&&"px"!==(c[3]||"px")&&(e.style[t]=s,s=n.css(e,t)),Qe(0,s,r)}}}),n.cssHooks.marginLeft=$e(a.reliableMarginLeft,function(e,t){if(t)return(parseFloat(V(e,"marginLeft"))||e.getBoundingClientRect().left-Pe(e,{marginLeft:0},function(){return e.getBoundingClientRect().left}))+"px"}),n.each({margin:"",padding:"",border:"Width"},function(e,t){n.cssHooks[e+t]={expand:function(n){for(var s=0,i={},o="string"==typeof n?n.split(" "):[n];s<4;s++)i[e+v[s]+t]=o[s]||o[s-2]||o[0];return i}},"margin"!==e&&(n.cssHooks[e+t].set=Qe)}),n.fn.extend({css:function(e,t){return p(this,function(e,t,s){var i,a,r={},o=0;if(Array.isArray(t)){for(i=ee(e),a=t.length;o<a;o++)r[t[o]]=n.css(e,t[o],!1,i);return r}return void 0!==s?n.style(e,t,s):n.css(e,t)},e,t,1<arguments.length)}}),((n.Tween=m).prototype={constructor:m,init:function(e,t,s,o,i,a){this.elem=e,this.prop=s,this.easing=i||n.easing._default,this.options=t,this.start=this.now=this.cur(),this.end=o,this.unit=a||(n.cssNumber[s]?"":"px")},cur:function(){var e=m.propHooks[this.prop];return e&&e.get?e.get(this):m.propHooks._default.get(this)},run:function(e){var t,s=m.propHooks[this.prop];return this.options.duration?this.pos=t=n.easing[this.easing](e,this.options.duration*e,0,1,this.options.duration):this.pos=t=e,this.now=(this.end-this.start)*t+this.start,this.options.step&&this.options.step.call(this.elem,this.now,this),s&&s.set?s.set(this):m.propHooks._default.set(this),this}}).init.prototype=m.prototype,(m.propHooks={_default:{get:function(e){var t;return 1!==e.elem.nodeType||null!=e.elem[e.prop]&&null==e.elem.style[e.prop]?e.elem[e.prop]:(t=n.css(e.elem,e.prop,""))&&"auto"!==t?t:0},set:function(e){n.fx.step[e.prop]?n.fx.step[e.prop](e):1!==e.elem.nodeType||!n.cssHooks[e.prop]&&null==e.elem.style[ge(e.prop)]?e.elem[e.prop]=e.now:n.style(e.elem,e.prop,e.now+e.unit)}}}).scrollTop=m.propHooks.scrollLeft={set:function(e){e.elem.nodeType&&e.elem.parentNode&&(e.elem[e.prop]=e.now)}},n.easing={linear:function(e){return e},swing:function(e){return.5-Math.cos(e*Math.PI)/2},_default:"swing"},n.fx=m.prototype.init,n.fx.step={},_t=/^(?:toggle|show|hide)$/,at=/queueHooks$/;function oe(){J&&(!1===i.hidden&&e.requestAnimationFrame?e.requestAnimationFrame(oe):e.setTimeout(oe,n.fx.interval),n.fx.tick())}function ct(){return e.setTimeout(function(){z=void 0}),z=Date.now()}function te(e,t){var o,s=0,n={height:e};for(t=t?1:0;s<4;s+=2-t)n["margin"+(o=v[s])]=n["padding"+o]=e;return t&&(n.opacity=n.width=e),n}function dt(e,t,n){for(var o,i=(u.tweeners[t]||[]).concat(u.tweeners["*"]),s=0,a=i.length;s<a;s++)if(o=i[s].call(n,t,e))return o}function u(e,t,s){var r,c,l=0,m=u.prefilters.length,a=n.Deferred().always(function(){delete h.elem}),h=function(){if(c)return!1;for(var r=z||ct(),t=Math.max(0,i.startTime+i.duration-r),n=1-(t/i.duration||0),s=0,o=i.tweens.length;s<o;s++)i.tweens[s].run(n);return a.notifyWith(e,[i,n,t]),n<1&&o?t:(o||a.notifyWith(e,[i,1,0]),a.resolveWith(e,[i]),!1)},i=a.promise({elem:e,props:n.extend({},t),opts:n.extend(!0,{specialEasing:{},easing:n.easing._default},s),originalProperties:t,originalOptions:s,startTime:z||ct(),duration:s.duration,tweens:[],createTween:function(t,s){var o=n.Tween(e,i.opts,t,s,i.opts.specialEasing[t]||i.opts.easing);return i.tweens.push(o),o},stop:function(t){var n=0,s=t?i.tweens.length:0;if(c)return this;for(c=!0;n<s;n++)i.tweens[n].run(1);return t?(a.notifyWith(e,[i,1,0]),a.resolveWith(e,[i,t])):a.rejectWith(e,[i,t]),this}}),d=i.props;for(!function(e,t){var s,o,i,a,r;for(s in e)if(a=t[i=f(s)],o=e[s],Array.isArray(o)&&(a=o[1],o=e[s]=o[0]),s!==i&&(e[i]=o,delete e[s]),(r=n.cssHooks[i])&&"expand"in r)for(s in o=r.expand(o),delete e[i],o)s in e||(e[s]=o[s],t[s]=a);else t[i]=a}(d,i.opts.specialEasing);l<m;l++)if(r=u.prefilters[l].call(i,e,d,i.opts))return o(r.stop)&&(n._queueHooks(i.elem,i.opts.queue).stop=r.stop.bind(r)),r;return n.map(d,dt,i),o(i.opts.start)&&i.opts.start.call(e,i),i.progress(i.opts.progress).done(i.opts.done,i.opts.complete).fail(i.opts.fail).always(i.opts.always),n.fx.timer(n.extend(h,{elem:e,anim:i,queue:i.opts.queue})),i}n.Animation=n.extend(u,{tweeners:{"*":[function(e,t){var n=this.createTween(e,t);return Le(n.elem,e,P.exec(t),n),n}]},tweener:function(e,t){o(e)?(t=e,e=["*"]):e=e.match(h);for(var n,s=0,i=e.length;s<i;s++)n=e[s],u.tweeners[n]=u.tweeners[n]||[],u.tweeners[n].unshift(t)},prefilters:[function(e,t,o){var i,r,l,d,u,p,g,v,b="width"in t||"height"in t,m=this,f={},c=e.style,h=e.nodeType&&$(e),a=s.get(e,"fxshow");for(i in o.queue||(null==(d=n._queueHooks(e,"fx")).unqueued&&(d.unqueued=0,v=d.empty.fire,d.empty.fire=function(){d.unqueued||v()}),d.unqueued++,m.always(function(){m.always(function(){d.unqueued--,n.queue(e,"fx").length||d.empty.fire()})})),t)if(p=t[i],_t.test(p)){if(delete t[i],g=g||"toggle"===p,p===(h?"hide":"show")){if("show"!==p||!a||void 0===a[i])continue;h=!0}f[i]=a&&a[i]||n.style(e,i)}if((l=!n.isEmptyObject(t))||!n.isEmptyObject(f))for(i in b&&1===e.nodeType&&(o.overflow=[c.overflow,c.overflowX,c.overflowY],null==(r=a&&a.display)&&(r=s.get(e,"display")),"none"===(u=n.css(e,"display"))&&(r?u=r:(C([e],!0),r=e.style.display||r,u=n.css(e,"display"),C([e]))),("inline"===u||"inline-block"===u&&r!=null)&&"none"===n.css(e,"float")&&(l||(m.done(function(){c.display=r}),r==null&&(u=c.display,r="none"===u?"":u)),c.display="inline-block")),o.overflow&&(c.overflow="hidden",m.always(function(){c.overflow=o.overflow[0],c.overflowX=o.overflow[1],c.overflowY=o.overflow[2]})),l=!1,f)l||(a?"hidden"in a&&(h=a.hidden):a=s.access(e,"fxshow",{display:r}),g&&(a.hidden=!h),h&&C([e],!0),m.done(function(){for(i in h||C([e]),s.remove(e,"fxshow"),f)n.style(e,i,f[i])})),l=dt(h?a[i]:0,i,m),i in a||(a[i]=l.start,h&&(l.end=l.start,l.start=0))}],prefilter:function(e,t){t?u.prefilters.unshift(e):u.prefilters.push(e)}}),n.speed=function(e,t,s){var i=e&&"object"==typeof e?n.extend({},e):{complete:s||!s&&t||o(e)&&e,duration:e,easing:s&&t||t&&!o(t)&&t};return n.fx.off?i.duration=0:"number"!=typeof i.duration&&(i.duration in n.fx.speeds?i.duration=n.fx.speeds[i.duration]:i.duration=n.fx.speeds._default),null!=i.queue&&!0!==i.queue||(i.queue="fx"),i.old=i.complete,i.complete=function(){o(i.old)&&i.old.call(this),i.queue&&n.dequeue(this,i.queue)},i},n.fn.extend({fadeTo:function(e,t,n,s){return this.filter($).css("opacity",0).show().end().animate({opacity:t},e,n,s)},animate:function(e,t,o,i){var c=n.isEmptyObject(e),r=n.speed(t,o,i),a=function(){var t=u(this,n.extend({},e),r);(c||s.get(this,"finish"))&&t.stop(!0)};return a.finish=a,c||!1===r.queue?this.each(a):this.queue(r.queue,a)},stop:function(e,t,o){var i=function(e){var t=e.stop;delete e.stop,t(o)};return"string"!=typeof e&&(o=t,t=e,e=void 0),t&&this.queue(e||"fx",[]),this.each(function(){var c=!0,t=e!=null&&e+"queueHooks",r=n.timers,a=s.get(this);if(t)a[t]&&a[t].stop&&i(a[t]);else for(t in a)a[t]&&a[t].stop&&at.test(t)&&i(a[t]);for(t=r.length;t--;)r[t].elem!==this||e!=null&&r[t].queue!==e||(r[t].anim.stop(o),c=!1,r.splice(t,1));!c&&o||n.dequeue(this,e)})},finish:function(e){return!1!==e&&(e=e||"fx"),this.each(function(){var t,a=s.get(this),o=a[e+"queue"],r=a[e+"queueHooks"],i=n.timers,c=o?o.length:0;for(a.finish=!0,n.queue(this,e,[]),r&&r.stop&&r.stop.call(this,!0),t=i.length;t--;)i[t].elem===this&&i[t].queue===e&&(i[t].anim.stop(!0),i.splice(t,1));for(t=0;t<c;t++)o[t]&&o[t].finish&&o[t].finish.call(this);delete a.finish})}}),n.each(["toggle","show","hide"],function(e,t){var s=n.fn[t];n.fn[t]=function(e,n,o){return e==null||"boolean"==typeof e?s.apply(this,arguments):this.animate(te(t,!0),e,n,o)}}),n.each({slideDown:te("show"),slideUp:te("hide"),slideToggle:te("toggle"),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"},fadeToggle:{opacity:"toggle"}},function(e,t){n.fn[e]=function(e,n,s){return this.animate(t,e,n,s)}}),n.timers=[],n.fx.tick=function(){var s,e=0,t=n.timers;for(z=Date.now();e<t.length;e++)(s=t[e])()||t[e]!==s||t.splice(e--,1);t.length||n.fx.stop(),z=void 0},n.fx.timer=function(e){n.timers.push(e),n.fx.start()},n.fx.interval=13,n.fx.start=function(){J||(J=!0,oe())},n.fx.stop=function(){J=null},n.fx.speeds={slow:600,fast:200,_default:400},n.fn.delay=function(t,s){return t=n.fx&&n.fx.speeds[t]||t,s=s||"fx",this.queue(s,function(n,s){var o=e.setTimeout(n,t);s.stop=function(){e.clearTimeout(o)}})},S=i.createElement("input"),ot=i.createElement("select").appendChild(i.createElement("option")),S.type="checkbox",a.checkOn=""!==S.value,a.optSelected=ot.selected,(S=i.createElement("input")).value="t",S.type="radio",a.radioValue="t"===S.value,N=n.expr.attrHandle,n.fn.extend({attr:function(e,t){return p(this,n.attr,e,t,1<arguments.length)},removeAttr:function(e){return this.each(function(){n.removeAttr(this,e)})}}),n.extend({attr:function(e,t,s){var o,i,a=e.nodeType;if(3!==a&&8!==a&&2!==a)return"undefined"==typeof e.getAttribute?n.prop(e,t,s):(1===a&&n.isXMLDoc(e)||(o=n.attrHooks[t.toLowerCase()]||(n.expr.match.bool.test(t)?Ot:void 0)),void 0!==s?null===s?void n.removeAttr(e,t):o&&"set"in o&&void 0!==(i=o.set(e,s,t))?i:(e.setAttribute(t,s+""),s):o&&"get"in o&&null!==(i=o.get(e,t))?i:null==(i=n.find.attr(e,t))?void 0:i)},attrHooks:{type:{set:function(e,t){if(!a.radioValue&&"radio"===t&&d(e,"input")){var n=e.value;return e.setAttribute("type",t),n&&(e.value=n),t}}}},removeAttr:function(e,t){var n,o=0,s=t&&t.match(h);if(s&&1===e.nodeType)for(;n=s[o++];)e.removeAttribute(n)}}),Ot={set:function(e,t,s){return!1===t?n.removeAttr(e,s):e.setAttribute(s,s),s}},n.each(n.expr.match.bool.source.match(/\w+/g),function(e,t){var s=N[t]||n.find.attr;N[t]=function(e,t,n){var i,a,o=t.toLowerCase();return n||(a=N[o],N[o]=i,i=null!=s(e,t,n)?o:null,N[o]=a),i}}),ft=/^(?:input|select|textarea|button)$/i,pt=/^(?:a|area)$/i;function y(e){return(e.match(h)||[]).join(" ")}function w(e){return e.getAttribute&&e.getAttribute("class")||""}function we(e){return Array.isArray(e)?e:"string"==typeof e&&e.match(h)||[]}n.fn.extend({prop:function(e,t){return p(this,n.prop,e,t,1<arguments.length)},removeProp:function(e){return this.each(function(){delete this[n.propFix[e]||e]})}}),n.extend({prop:function(e,t,s){var o,i,a=e.nodeType;if(3!==a&&8!==a&&2!==a)return 1===a&&n.isXMLDoc(e)||(t=n.propFix[t]||t,o=n.propHooks[t]),void 0!==s?o&&"set"in o&&void 0!==(i=o.set(e,s,t))?i:e[t]=s:o&&"get"in o&&null!==(i=o.get(e,t))?i:e[t]},propHooks:{tabIndex:{get:function(e){var t=n.find.attr(e,"tabindex");return t?parseInt(t,10):ft.test(e.nodeName)||pt.test(e.nodeName)&&e.href?0:-1}}},propFix:{for:"htmlFor",class:"className"}}),a.optSelected||(n.propHooks.selected={get:function(e){var t=e.parentNode;return t&&t.parentNode&&t.parentNode.selectedIndex,null},set:function(e){var t=e.parentNode;t&&(t.selectedIndex,t.parentNode&&t.parentNode.selectedIndex)}}),n.each(["tabIndex","readOnly","maxLength","cellSpacing","cellPadding","rowSpan","colSpan","useMap","frameBorder","contentEditable"],function(){n.propFix[this.toLowerCase()]=this}),n.fn.extend({addClass:function(e){var t,s,i,a,r,c;return o(e)?this.each(function(t){n(this).addClass(e.call(this,t,w(this)))}):(i=we(e)).length?this.each(function(){if(a=w(this),t=1===this.nodeType&&" "+y(a)+" "){for(s=0;s<i.length;s++)r=i[s],t.indexOf(" "+r+" ")<0&&(t+=r+" ");c=y(t),a!==c&&this.setAttribute("class",c)}}):this},removeClass:function(e){var t,s,i,a,r,c;return o(e)?this.each(function(t){n(this).removeClass(e.call(this,t,w(this)))}):arguments.length?(i=we(e)).length?this.each(function(){if(a=w(this),t=1===this.nodeType&&" "+y(a)+" "){for(s=0;s<i.length;s++)for(r=i[s];-1<t.indexOf(" "+r+" ");)t=t.replace(" "+r+" "," ");c=y(t),a!==c&&this.setAttribute("class",c)}}):this:this.attr("class","")},toggleClass:function(e,t){var i,a,r,c,l=typeof e,d="string"===l||Array.isArray(e);return o(e)?this.each(function(s){n(this).toggleClass(e.call(this,s,w(this),t),t)}):"boolean"==typeof t&&d?t?this.addClass(e):this.removeClass(e):(c=we(e),this.each(function(){if(d)for(r=n(this),a=0;a<c.length;a++)i=c[a],r.hasClass(i)?r.removeClass(i):r.addClass(i);else void 0!==e&&"boolean"!==l||((i=w(this))&&s.set(this,"__className__",i),this.setAttribute&&this.setAttribute("class",i||!1===e?"":s.get(this,"__className__")||""))}))},hasClass:function(e){for(var t,s=0,n=" "+e+" ";t=this[s++];)if(1===t.nodeType&&-1<(" "+y(w(t))+" ").indexOf(n))return!0;return!1}}),jt=/\r/g,n.fn.extend({val:function(e){var t,s,a,i=this[0];return arguments.length?(a=o(e),this.each(function(s){var o;1===this.nodeType&&(null==(o=a?e.call(this,s,n(this).val()):e)?o="":"number"==typeof o?o+="":Array.isArray(o)&&(o=n.map(o,function(e){return e==null?"":e+""})),(t=n.valHooks[this.type]||n.valHooks[this.nodeName.toLowerCase()])&&"set"in t&&void 0!==t.set(this,o,"value")||(this.value=o))})):i?(t=n.valHooks[i.type]||n.valHooks[i.nodeName.toLowerCase()])&&"get"in t&&void 0!==(s=t.get(i,"value"))?s:"string"==typeof(s=i.value)?s.replace(jt,""):s??"":void 0}}),n.extend({valHooks:{option:{get:function(e){var t=n.find.attr(e,"value");return t??y(n.text(e))}},select:{get:function(e){var t,s,a,r=e.options,o=e.selectedIndex,i="select-one"===e.type,c=i?null:[],l=i?o+1:r.length;for(s=o<0?l:i?o:0;s<l;s++)if(((t=r[s]).selected||s===o)&&!t.disabled&&(!t.parentNode.disabled||!d(t.parentNode,"optgroup"))){if(a=n(t).val(),i)return a;c.push(a)}return c},set:function(e,t){for(var s,o,i=e.options,a=n.makeArray(t),r=i.length;r--;)((o=i[r]).selected=-1<n.inArray(n.valHooks.option.get(o),a))&&(s=!0);return s||(e.selectedIndex=-1),a}}}}),n.each(["radio","checkbox"],function(){n.valHooks[this]={set:function(e,t){if(Array.isArray(t))return e.checked=-1<n.inArray(n(e).val(),t)}},a.checkOn||(n.valHooks[this].get=function(e){return null===e.getAttribute("value")?"on":e.value})}),a.focusin="onfocusin"in e,Oe=/^(?:focusinfocus|focusoutblur)$/,le=function(e){e.stopPropagation()},n.extend(n.event,{trigger:function(t,a,r,c){var d,u,h,m,f,p,b,j,g=[r||i],l=X.call(t,"type")?t.type:t,v=X.call(t,"namespace")?t.namespace.split("."):[];if(d=p=h=r=r||i,3!==r.nodeType&&8!==r.nodeType&&!Oe.test(l+n.event.triggered)&&(-1<l.indexOf(".")&&(l=(v=l.split(".")).shift(),v.sort()),m=l.indexOf(":")<0&&"on"+l,(t=t[n.expando]?t:new n.Event(l,"object"==typeof t&&t)).isTrigger=c?2:3,t.namespace=v.join("."),t.rnamespace=t.namespace?new RegExp("(^|\\.)"+v.join("\\.(?:.*\\.|)")+"(\\.|$)"):null,t.result=void 0,t.target||(t.target=r),a=a==null?[t]:n.makeArray(a,[t]),u=n.event.special[l]||{},c||!u.trigger||!1!==u.trigger.apply(r,a))){if(!c&&!u.noBubble&&!O(r)){for(j=u.delegateType||l,Oe.test(j+l)||(d=d.parentNode);d;d=d.parentNode)g.push(d),h=d;h===(r.ownerDocument||i)&&g.push(h.defaultView||h.parentWindow||e)}for(b=0;(d=g[b++])&&!t.isPropagationStopped();)p=d,t.type=1<b?j:u.bindType||l,(f=(s.get(d,"events")||Object.create(null))[t.type]&&s.get(d,"handle"))&&f.apply(d,a),(f=m&&d[m])&&f.apply&&T(d)&&(t.result=f.apply(d,a),!1===t.result&&t.preventDefault());return t.type=l,c||t.isDefaultPrevented()||u._default&&!1!==u._default.apply(g.pop(),a)||!T(r)||m&&o(r[l])&&!O(r)&&((h=r[m])&&(r[m]=null),n.event.triggered=l,t.isPropagationStopped()&&p.addEventListener(l,le),r[l](),t.isPropagationStopped()&&p.removeEventListener(l,le),n.event.triggered=void 0,h&&(r[m]=h)),t.result}},simulate:function(e,t,s){var o=n.extend(new n.Event,s,{type:e,isSimulated:!0});n.event.trigger(o,null,t)}}),n.fn.extend({trigger:function(e,t){return this.each(function(){n.event.trigger(e,t,this)})},triggerHandler:function(e,t){var s=this[0];if(s)return n.event.trigger(e,t,s,!0)}}),a.focusin||n.each({focus:"focusin",blur:"focusout"},function(e,t){var o=function(e){n.event.simulate(t,e.target,n.event.fix(e))};n.event.special[t]={setup:function(){var n=this.ownerDocument||this.document||this,i=s.access(n,t);i||n.addEventListener(e,o,!0),s.access(n,t,(i||0)+1)},teardown:function(){var n=this.ownerDocument||this.document||this,i=s.access(n,t)-1;i?s.access(n,t,i):(n.removeEventListener(e,o,!0),s.remove(n,t))}}});var B=e.location,ht={guid:Date.now()},pe=/\?/;n.parseXML=function(t){var s,o;if(!t||"string"!=typeof t)return null;try{s=(new e.DOMParser).parseFromString(t,"text/xml")}catch{}return o=s&&s.getElementsByTagName("parsererror")[0],s&&!o||n.error("Invalid XML: "+(o?n.map(o.childNodes,function(e){return e.textContent}).join(`
`):t)),s};var Ct=/\[\]$/,Ye=/\r?\n/g,kt=/^(?:submit|button|image|reset|file)$/i,At=/^(?:input|select|textarea|keygen)/i;function ue(e,t,s,o){var i;if(Array.isArray(t))n.each(t,function(t,n){s||Ct.test(e)?o(e,n):ue(e+"["+("object"==typeof n&&n!=null?t:"")+"]",n,s,o)});else if(s||"object"!==M(t))o(e,t);else for(i in t)ue(e+"["+i+"]",t[i],s,o)}n.param=function(e,t){var s,i=[],a=function(e,t){var n=o(t)?t():t;i[i.length]=encodeURIComponent(e)+"="+encodeURIComponent(n??"")};if(e==null)return"";if(Array.isArray(e)||e.jquery&&!n.isPlainObject(e))n.each(e,function(){a(this.name,this.value)});else for(s in e)ue(s,e[s],t,a);return i.join("&")},n.fn.extend({serialize:function(){return n.param(this.serializeArray())},serializeArray:function(){return this.map(function(){var e=n.prop(this,"elements");return e?n.makeArray(e):this}).filter(function(){var e=this.type;return this.name&&!n(this).is(":disabled")&&At.test(this.nodeName)&&!kt.test(e)&&(this.checked||!I.test(e))}).map(function(e,t){var s=n(this).val();return s==null?null:Array.isArray(s)?n.map(s,function(e){return{name:t.name,value:e.replace(Ye,`
`)}}):{name:t.name,value:s.replace(Ye,`
`)}}).get()}});var Mt=/%20/g,Xt=/#.*$/,Tt=/([?&])_=[^&]*/,zt=/^(.*?):[ \t]*([^\r\n]*)$/gm,Dt=/^(?:GET|HEAD)$/,Nt=/^\/\//,He={},be={},Me="*/".concat("*"),je=i.createElement("a");function ke(e){return function(t,n){"string"!=typeof t&&(n=t,t="*");var s,i=0,a=t.toLowerCase().match(h)||[];if(o(n))for(;s=a[i++];)"+"===s[0]?(s=s.slice(1)||"*",(e[s]=e[s]||[]).unshift(n)):(e[s]=e[s]||[]).push(n)}}function Ee(e,t,s,o){var i={},r=e===be;function a(c){var l;return i[c]=!0,n.each(e[c]||[],function(e,n){var c=n(t,s,o);return"string"!=typeof c||r||i[c]?r?!(l=c):void 0:(t.dataTypes.unshift(c),a(c),!1)}),l}return a(t.dataTypes[0])||!i["*"]&&a("*")}function ie(e,t){var s,o,i=n.ajaxSettings.flatOptions||{};for(s in t)void 0!==t[s]&&((i[s]?e:o||(o={}))[s]=t[s]);return o&&n.extend(!0,e,o),e}return je.href=B.href,n.extend({active:0,lastModified:{},etag:{},ajaxSettings:{url:B.href,type:"GET",isLocal:/^(?:about|app|app-storage|.+-extension|file|res|widget):$/.test(B.protocol),global:!0,processData:!0,async:!0,contentType:"application/x-www-form-urlencoded; charset=UTF-8",accepts:{"*":Me,text:"text/plain",html:"text/html",xml:"application/xml, text/xml",json:"application/json, text/javascript"},contents:{xml:/\bxml\b/,html:/\bhtml/,json:/\bjson\b/},responseFields:{xml:"responseXML",text:"responseText",json:"responseJSON"},converters:{"* text":String,"text html":!0,"text json":JSON.parse,"text xml":n.parseXML},flatOptions:{url:!0,context:!0}},ajaxSetup:function(e,t){return t?ie(ie(e,n.ajaxSettings),t):ie(n.ajaxSettings,e)},ajaxPrefilter:ke(He),ajaxTransport:ke(be),ajax:function(t,s){"object"==typeof t&&(s=t,t=void 0),s=s||{};var r,c,d,u,m,p,g,b,_,w,o=n.ajaxSetup({},s),l=o.context||o,y=o.context&&(l.nodeType||l.jquery)?n(l):n.event,j=n.Deferred(),E=n.Callbacks("once memory"),v=o.statusCode||{},C={},O={},x="canceled",a={readyState:0,getResponseHeader:function(e){if(c){if(!u){u={};for(var t;t=zt.exec(w);)u[t[1].toLowerCase()+" "]=(u[t[1].toLowerCase()+" "]||[]).concat(t[2])}t=u[e.toLowerCase()+" "]}return t==null?null:t.join(", ")},getAllResponseHeaders:function(){return c?w:null},setRequestHeader:function(e,t){return c==null&&(e=O[e.toLowerCase()]=O[e.toLowerCase()]||e,C[e]=t),this},overrideMimeType:function(e){return c==null&&(o.mimeType=e),this},statusCode:function(e){var t;if(e)if(c)a.always(e[a.status]);else for(t in e)v[t]=[v[t],e[t]];return this},abort:function(e){var t=e||x;return m&&m.abort(t),f(0,t),this}};if(j.promise(a),o.url=((t||o.url||B.href)+"").replace(Nt,B.protocol+"//"),o.type=s.method||s.type||o.method||o.type,o.dataTypes=(o.dataType||"*").toLowerCase().match(h)||[""],null==o.crossDomain){d=i.createElement("a");try{d.href=o.url,d.href=d.href,o.crossDomain=je.protocol+"//"+je.host!=d.protocol+"//"+d.host}catch{o.crossDomain=!0}}if(o.data&&o.processData&&"string"!=typeof o.data&&(o.data=n.param(o.data,o.traditional)),Ee(He,o,s,a),c)return a;for(b in(g=n.event&&o.global)&&0==n.active++&&n.event.trigger("ajaxStart"),o.type=o.type.toUpperCase(),o.hasContent=!Dt.test(o.type),r=o.url.replace(Xt,""),o.hasContent?o.data&&o.processData&&0===(o.contentType||"").indexOf("application/x-www-form-urlencoded")&&(o.data=o.data.replace(Mt,"+")):(p=o.url.slice(r.length),o.data&&(o.processData||"string"==typeof o.data)&&(r+=(pe.test(r)?"&":"?")+o.data,delete o.data),!1===o.cache&&(r=r.replace(Tt,"$1"),p=(pe.test(r)?"&":"?")+"_="+ht.guid+++p),o.url=r+p),o.ifModified&&(n.lastModified[r]&&a.setRequestHeader("If-Modified-Since",n.lastModified[r]),n.etag[r]&&a.setRequestHeader("If-None-Match",n.etag[r])),(o.data&&o.hasContent&&!1!==o.contentType||s.contentType)&&a.setRequestHeader("Content-Type",o.contentType),a.setRequestHeader("Accept",o.dataTypes[0]&&o.accepts[o.dataTypes[0]]?o.accepts[o.dataTypes[0]]+("*"!==o.dataTypes[0]?", "+Me+"; q=0.01":""):o.accepts["*"]),o.headers)a.setRequestHeader(b,o.headers[b]);if(o.beforeSend&&(!1===o.beforeSend.call(l,a,o)||c))return a.abort();if(x="abort",E.add(o.complete),a.done(o.success),a.fail(o.error),m=Ee(be,o,s,a)){if(a.readyState=1,g&&y.trigger("ajaxSend",[a,o]),c)return a;o.async&&0<o.timeout&&(_=e.setTimeout(function(){a.abort("timeout")},o.timeout));try{c=!1,m.send(C,f)}catch(e){if(c)throw e;f(-1,e)}}else f(-1,"No Transport");function f(t,s,i,d){var h,f,p,b,O,u=s;c||(c=!0,_&&e.clearTimeout(_),m=void 0,w=d||"",a.readyState=0<t?4:0,h=200<=t&&t<300||304===t,i&&(f=function(e,t,n){for(var o,i,a,r,c=e.contents,s=e.dataTypes;"*"===s[0];)s.shift(),void 0===a&&(a=e.mimeType||t.getResponseHeader("Content-Type"));if(a)for(o in c)if(c[o]&&c[o].test(a)){s.unshift(o);break}if(s[0]in n)i=s[0];else{for(o in n){if(!s[0]||e.converters[o+" "+s[0]]){i=o;break}r||(r=o)}i=i||r}if(i)return i!==s[0]&&s.unshift(i),n[i]}(o,a,i)),!h&&-1<n.inArray("script",o.dataTypes)&&n.inArray("json",o.dataTypes)<0&&(o.converters["text script"]=function(){}),f=function(e,t,n,s){var o,i,a,c,l,r={},d=e.dataTypes.slice();if(d[1])for(i in e.converters)r[i.toLowerCase()]=e.converters[i];for(o=d.shift();o;)if(e.responseFields[o]&&(n[e.responseFields[o]]=t),!a&&s&&e.dataFilter&&(t=e.dataFilter(t,e.dataType)),a=o,o=d.shift())if("*"===o)o=a;else if("*"!==a&&a!==o){if(!(i=r[a+" "+o]||r["* "+o]))for(l in r)if((c=l.split(" "))[1]===o&&(i=r[a+" "+c[0]]||r["* "+c[0]])){!0===i?i=r[l]:!0!==r[l]&&(o=c[0],d.unshift(c[1]));break}if(!0!==i)if(i&&e.throws)t=i(t);else try{t=i(t)}catch(e){return{state:"parsererror",error:i?e:"No conversion from "+a+" to "+o}}}return{state:"success",data:t}}(o,f,a,h),h?(o.ifModified&&((b=a.getResponseHeader("Last-Modified"))&&(n.lastModified[r]=b),(b=a.getResponseHeader("etag"))&&(n.etag[r]=b)),204===t||"HEAD"===o.type?u="nocontent":304===t?u="notmodified":(u=f.state,O=f.data,h=!(p=f.error))):(p=u,!t&&u||(u="error",t<0&&(t=0))),a.status=t,a.statusText=(s||u)+"",h?j.resolveWith(l,[O,u,a]):j.rejectWith(l,[a,u,p]),a.statusCode(v),v=void 0,g&&y.trigger(h?"ajaxSuccess":"ajaxError",[a,o,h?O:p]),E.fireWith(l,[a,u]),g&&(y.trigger("ajaxComplete",[a,o]),--n.active||n.event.trigger("ajaxStop")))}return a},getJSON:function(e,t,s){return n.get(e,t,s,"json")},getScript:function(e,t){return n.get(e,void 0,t,"script")}}),n.each(["get","post"],function(e,t){n[t]=function(e,s,i,a){return o(s)&&(a=a||i,i=s,s=void 0),n.ajax(n.extend({url:e,type:t,dataType:a,data:s,success:i},n.isPlainObject(e)&&e))}}),n.ajaxPrefilter(function(e){var t;for(t in e.headers)"content-type"===t.toLowerCase()&&(e.contentType=e.headers[t]||"")}),n._evalUrl=function(e,t,s){return n.ajax({url:e,type:"GET",dataType:"script",cache:!0,async:!1,global:!1,converters:{"text script":function(){}},dataFilter:function(e){n.globalEval(e,t,s)}})},n.fn.extend({wrapAll:function(e){var t;return this[0]&&(o(e)&&(e=e.call(this[0])),t=n(e,this[0].ownerDocument).eq(0).clone(!0),this[0].parentNode&&t.insertBefore(this[0]),t.map(function(){for(var e=this;e.firstElementChild;)e=e.firstElementChild;return e}).append(this)),this},wrapInner:function(e){return o(e)?this.each(function(t){n(this).wrapInner(e.call(this,t))}):this.each(function(){var t=n(this),s=t.contents();s.length?s.wrapAll(e):t.append(e)})},wrap:function(e){var t=o(e);return this.each(function(s){n(this).wrapAll(t?e.call(this,s):e)})},unwrap:function(e){return this.parent(e).not("body").each(function(){n(this).replaceWith(this.childNodes)}),this}}),n.expr.pseudos.hidden=function(e){return!n.expr.pseudos.visible(e)},n.expr.pseudos.visible=function(e){return!!(e.offsetWidth||e.offsetHeight||e.getClientRects().length)},n.ajaxSettings.xhr=function(){try{return new e.XMLHttpRequest}catch{}},Ue={0:200,1223:204},R=n.ajaxSettings.xhr(),a.cors=!!R&&"withCredentials"in R,a.ajax=R=!!R,n.ajaxTransport(function(t){var n,s;if(a.cors||R&&!t.crossDomain)return{send:function(o,i){var r,a=t.xhr();if(a.open(t.type,t.url,t.async,t.username,t.password),t.xhrFields)for(r in t.xhrFields)a[r]=t.xhrFields[r];for(r in t.mimeType&&a.overrideMimeType&&a.overrideMimeType(t.mimeType),t.crossDomain||o["X-Requested-With"]||(o["X-Requested-With"]="XMLHttpRequest"),o)a.setRequestHeader(r,o[r]);n=function(e){return function(){n&&(n=s=a.onload=a.onerror=a.onabort=a.ontimeout=a.onreadystatechange=null,"abort"===e?a.abort():"error"===e?"number"!=typeof a.status?i(0,"error"):i(a.status,a.statusText):i(Ue[a.status]||a.status,a.statusText,"text"!==(a.responseType||"text")||"string"!=typeof a.responseText?{binary:a.response}:{text:a.responseText},a.getAllResponseHeaders()))}},a.onload=n(),s=a.onerror=a.ontimeout=n("error"),void 0!==a.onabort?a.onabort=s:a.onreadystatechange=function(){4===a.readyState&&e.setTimeout(function(){n&&s()})},n=n("abort");try{a.send(t.hasContent&&t.data||null)}catch(e){if(n)throw e}},abort:function(){n&&n()}}}),n.ajaxPrefilter(function(e){e.crossDomain&&(e.contents.script=!1)}),n.ajaxSetup({accepts:{script:"text/javascript, application/javascript, application/ecmascript, application/x-ecmascript"},contents:{script:/\b(?:java|ecma)script\b/},converters:{"text script":function(e){return n.globalEval(e),e}}}),n.ajaxPrefilter("script",function(e){void 0===e.cache&&(e.cache=!1),e.crossDomain&&(e.type="GET")}),n.ajaxTransport("script",function(e){var t,s;if(e.crossDomain||e.scriptAttrs)return{send:function(o,a){s=n("<script>").attr(e.scriptAttrs||{}).prop({charset:e.scriptCharset,src:e.url}).on("load error",t=function(e){s.remove(),t=null,e&&a("error"===e.type?404:200,e.type)}),i.head.appendChild(s[0])},abort:function(){t&&t()}}}),xe=[],K=/(=)\?(?=&|$)|\?\?/,n.ajaxSetup({jsonp:"callback",jsonpCallback:function(){var e=xe.pop()||n.expando+"_"+ht.guid++;return this[e]=!0,e}}),n.ajaxPrefilter("json jsonp",function(t,s,i){var a,r,c,l=!1!==t.jsonp&&(K.test(t.url)?"url":"string"==typeof t.data&&0===(t.contentType||"").indexOf("application/x-www-form-urlencoded")&&K.test(t.data)&&"data");if(l||"jsonp"===t.dataTypes[0])return a=t.jsonpCallback=o(t.jsonpCallback)?t.jsonpCallback():t.jsonpCallback,l?t[l]=t[l].replace(K,"$1"+a):!1!==t.jsonp&&(t.url+=(pe.test(t.url)?"&":"?")+t.jsonp+"="+a),t.converters["script json"]=function(){return c||n.error(a+" was not called"),c[0]},t.dataTypes[0]="json",r=e[a],e[a]=function(){c=arguments},i.always(function(){void 0===r?n(e).removeProp(a):e[a]=r,t[a]&&(t.jsonpCallback=s.jsonpCallback,xe.push(a)),c&&o(r)&&r(c[0]),c=r=void 0}),"script"}),a.createHTMLDocument=((qe=i.implementation.createHTMLDocument("").body).innerHTML="<form></form><form></form>",2===qe.childNodes.length),n.parseHTML=function(e,t,s){return"string"!=typeof e?[]:("boolean"==typeof t&&(s=t,t=!1),t||(a.createHTMLDocument?((c=(t=i.implementation.createHTMLDocument("")).createElement("base")).href=i.location.href,t.head.appendChild(c)):t=i),o=!s&&[],(r=se.exec(e))?[t.createElement(r[1])]:(r=Ae([e],t,o),o&&o.length&&n(o).remove(),n.merge([],r.childNodes)));var o,r,c},n.fn.load=function(e,t,s){var i,c,l,a=this,r=e.indexOf(" ");return-1<r&&(i=y(e.slice(r)),e=e.slice(0,r)),o(t)?(s=t,t=void 0):t&&"object"==typeof t&&(c="POST"),0<a.length&&n.ajax({url:e,type:c||"GET",dataType:"html",data:t}).done(function(e){l=arguments,a.html(i?n("<div>").append(n.parseHTML(e)).find(i):e)}).always(s&&function(e,t){a.each(function(){s.apply(this,l||[e.responseText,t,e])})}),this},n.expr.pseudos.animated=function(e){return n.grep(n.timers,function(t){return e===t.elem}).length},n.offset={setOffset:function(e,t,s){var a,c,l,d,h,m,r=n.css(e,"position"),u=n(e),i={};"static"===r&&(e.style.position="relative"),a=u.offset(),h=n.css(e,"top"),d=n.css(e,"left"),("absolute"===r||"fixed"===r)&&-1<(h+d).indexOf("auto")?(l=(m=u.position()).top,c=m.left):(l=parseFloat(h)||0,c=parseFloat(d)||0),o(t)&&(t=t.call(e,s,n.extend({},a))),null!=t.top&&(i.top=t.top-a.top+l),null!=t.left&&(i.left=t.left-a.left+c),"using"in t?t.using.call(e,i):u.css(i)}},n.fn.extend({offset:function(e){if(arguments.length)return void 0===e?this:this.each(function(t){n.offset.setOffset(this,e,t)});var s,o,t=this[0];return t?t.getClientRects().length?(s=t.getBoundingClientRect(),o=t.ownerDocument.defaultView,{top:s.top+o.pageYOffset,left:s.left+o.pageXOffset}):{top:0,left:0}:void 0},position:function(){if(this[0]){var e,s,o,t=this[0],i={top:0,left:0};if("fixed"===n.css(t,"position"))s=t.getBoundingClientRect();else{for(s=this.offset(),o=t.ownerDocument,e=t.offsetParent||o.documentElement;e&&(e===o.body||e===o.documentElement)&&"static"===n.css(e,"position");)e=e.parentNode;e&&e!==t&&1===e.nodeType&&((i=n(e).offset()).top+=n.css(e,"borderTopWidth",!0),i.left+=n.css(e,"borderLeftWidth",!0))}return{top:s.top-i.top-n.css(t,"marginTop",!0),left:s.left-i.left-n.css(t,"marginLeft",!0)}}},offsetParent:function(){return this.map(function(){for(var e=this.offsetParent;e&&"static"===n.css(e,"position");)e=e.offsetParent;return e||_})}}),n.each({scrollLeft:"pageXOffset",scrollTop:"pageYOffset"},function(e,t){var s="pageYOffset"===t;n.fn[e]=function(n){return p(this,function(e,n,o){var i;if(O(e)?i=e:9===e.nodeType&&(i=e.defaultView),void 0===o)return i?i[t]:e[n];i?i.scrollTo(s?i.pageXOffset:o,s?o:i.pageYOffset):e[n]=o},e,n,arguments.length)}}),n.each(["top","left"],function(e,t){n.cssHooks[t]=$e(a.pixelPosition,function(e,s){if(s)return s=V(e,t),me.test(s)?n(e).position()[t]+"px":s})}),n.each({Height:"height",Width:"width"},function(e,t){n.each({padding:"inner"+e,content:t,"":"outer"+e},function(s,o){n.fn[o]=function(i,a){var r=arguments.length&&(s||"boolean"!=typeof i),c=s||(!0===i||!0===a?"margin":"border");return p(this,function(t,s,i){var a;return O(t)?0===o.indexOf("outer")?t["inner"+e]:t.document.documentElement["client"+e]:9===t.nodeType?(a=t.documentElement,Math.max(t.body["scroll"+e],a["scroll"+e],t.body["offset"+e],a["offset"+e],a["client"+e])):void 0===i?n.css(t,s,c):n.style(t,s,i,c)},t,r?i:void 0,r)}})}),n.each(["ajaxStart","ajaxStop","ajaxComplete","ajaxError","ajaxSuccess","ajaxSend"],function(e,t){n.fn[t]=function(e){return this.on(t,e)}}),n.fn.extend({bind:function(e,t,n){return this.on(e,null,t,n)},unbind:function(e,t){return this.off(e,null,t)},delegate:function(e,t,n,s){return this.on(t,e,n,s)},undelegate:function(e,t,n){return 1===arguments.length?this.off(e,"**"):this.off(t,e||"**",n)},hover:function(e,t){return this.mouseenter(e).mouseleave(t||e)}}),n.each("blur focus focusin focusout resize scroll click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup contextmenu".split(" "),function(e,t){n.fn[t]=function(e,n){return 0<arguments.length?this.on(t,null,e,n):this.trigger(t)}}),ut=/^[\s\uFEFF\xA0]+|([^\s\uFEFF\xA0])[\s\uFEFF\xA0]+$/g,n.proxy=function(e,t){var s,i,a;if("string"==typeof t&&(s=e[t],t=e,e=s),o(e))return i=j.call(arguments,2),(a=function(){return e.apply(t||this,i.concat(j.call(arguments)))}).guid=e.guid=e.guid||n.guid++,a},n.holdReady=function(e){e?n.readyWait++:n.ready(!0)},n.isArray=Array.isArray,n.parseJSON=JSON.parse,n.nodeName=d,n.isFunction=o,n.isWindow=O,n.camelCase=f,n.type=M,n.now=Date.now,n.isNumeric=function(e){var t=n.type(e);return("number"===t||"string"===t)&&!isNaN(e-parseFloat(e))},n.trim=function(e){return e==null?"":(e+"").replace(ut,"$1")},"function"==typeof define&&define.amd&&define("jquery",[],function(){return n}),mt=e.jQuery,Be=e.$,n.noConflict=function(t){return e.$===n&&(e.$=Be),t&&e.jQuery===n&&(e.jQuery=mt),n},"undefined"==typeof t&&(e.jQuery=e.$=n),n})