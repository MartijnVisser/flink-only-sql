<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Flink Documentation on Apache Flink</title>
    <link>//localhost/flink/flink-docs-master/zh/</link>
    <description>Recent content in Apache Flink Documentation on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//localhost/flink/flink-docs-master/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Formats</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/overview/</guid>
      <description> Formats # Flink 提供了一套与表连接器（table connector）一起使用的表格式（table format）。表格式是一种存储格式，定义了如何把二进制数据映射到表的列上。
Flink 支持以下格式：
Formats Supported Connectors CSV Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem JSON Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem, Elasticsearch Apache Avro Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem Confluent Avro Apache Kafka, Upsert Kafka Debezium CDC Apache Kafka, Filesystem Canal CDC Apache Kafka, Filesystem Maxwell CDC Apache Kafka, Filesystem OGG CDC Apache Kafka, Filesystem Apache Parquet Filesystem Apache ORC Filesystem Raw Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem </description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/overview/</guid>
      <description>DataStream Formats # Available Formats # Formats define how information is encoded for storage. Currently these formats are supported:
Avro Azure Table Hadoop Parquet Text files Back to top</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/overview/</guid>
      <description>Hive # Apache Hive 已经成为了数据仓库生态系统中的核心。 它不仅仅是一个用于大数据分析和ETL场景的SQL引擎，同样它也是一个数据管理平台，可用于发现，定义，和演化数据。
Flink 与 Hive 的集成包含两个层面。
一是利用了 Hive 的 MetaStore 作为持久化的 Catalog，用户可通过HiveCatalog将不同会话中的 Flink 元数据存储到 Hive Metastore 中。 例如，用户可以使用HiveCatalog将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。
二是利用 Flink 来读写 Hive 的表。
HiveCatalog的设计提供了与 Hive 良好的兼容性，用户可以&amp;quot;开箱即用&amp;quot;的访问其已有的 Hive 数仓。 您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。
支持的Hive版本 # Flink 支持一下的 Hive 版本。
1.0 1.0.0 1.0.1 1.1 1.1.0 1.1.1 1.2 1.2.0 1.2.1 1.2.2 2.0 2.0.0 2.0.1 2.1 2.1.0 2.1.1 2.2 2.2.0 2.3 2.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/overview/</guid>
      <description>Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
DataStream Transformations # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., mapping, filtering, reducing). Please see operators for an overview of the available transformations in Python DataStream API.
Functions # Transformations accept user-defined functions as input to define the functionality of the transformations.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/overview/</guid>
      <description>Gelly: Flink Graph API # Gelly is a Graph API for Flink. It contains a set of methods and utilities which aim to simplify the development of graph analysis applications in Flink. In Gelly, graphs can be transformed and modified using high-level functions similar to the ones provided by the batch processing API. Gelly provides methods to create, transform and modify graphs, as well as a library of graph algorithms.</description>
    </item>
    
    <item>
      <title>Python DataStream API 简介</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/intro_to_datastream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/intro_to_datastream_api/</guid>
      <description>Intro to the Python DataStream API # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal).
Python DataStream API is a Python version of DataStream API which allows Python users could write Python DatStream API jobs.</description>
    </item>
    
    <item>
      <title>本地模式安装</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/try-flink/local_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/try-flink/local_installation/</guid>
      <description>本地模式安装 # 注意：Apache Flink 社区只发布 Apache Flink 的 release 版本。
由于你当前正在查看的是文档最新的 SNAPSHOT 版本，因此相关内容会被隐藏。请通过左侧菜单底部的版本选择将文档切换到最新的 release 版本。
请按照以下几个步骤下载最新的稳定版本开始使用。
步骤 1：下载 # 为了运行Flink，只需提前安装好 Java 11。你可以通过以下命令来检查 Java 是否已经安装正确。
java -version 下载 release 1.16-SNAPSHOT 并解压。
$ tar -xzf flink-1.16-SNAPSHOT-bin-scala_2.12.tgz $ cd flink-1.16-SNAPSHOT-bin-scala_2.12 步骤 2：启动集群 # Flink 附带了一个 bash 脚本，可以用于启动本地集群。
$ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host. Starting taskexecutor daemon on host. 步骤 3：提交作业（Job） # Flink 的 Releases 附带了许多的示例作业。你可以任意选择一个，快速部署到已运行的集群上。
$ ./bin/flink run examples/streaming/WordCount.jar $ tail log/flink-*-taskexecutor-*.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/concepts/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/concepts/overview/</guid>
      <description>概念透析 # 实践练习章节介绍了作为 Flink API 根基的有状态实时流处理的基本概念，并且举例说明了如何在 Flink 应用中使用这些机制。其中 Data Pipelines &amp;amp; ETL 小节介绍了有状态流处理的概念，并且在 Fault Tolerance 小节中进行了深入介绍。Streaming Analytics 小节介绍了实时流处理的概念。
本章将深入分析 Flink 分布式运行时架构如何实现这些概念。
Flink 中的 API # Flink 为流式/批式处理应用程序的开发提供了不同级别的抽象。
Flink API 最底层的抽象为有状态实时流处理。其抽象实现是 Process Function，并且 Process Function 被 Flink 框架集成到了 DataStream API 中来为我们使用。它允许用户在应用程序中自由地处理来自单流或多流的事件（数据），并提供具有全局一致性和容错保障的状态。此外，用户可以在此层抽象中注册事件时间（event time）和处理时间（processing time）回调方法，从而允许程序可以实现复杂计算。
Flink API 第二层抽象是 Core APIs。实际上，许多应用程序不需要使用到上述最底层抽象的 API，而是可以使用 Core APIs 进行编程：其中包含 DataStream API（应用于有界/无界数据流场景）和 DataSet API（应用于有界数据集场景）两部分。Core APIs 提供的流式 API（Fluent API）为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换（transformations）、联接（joins）、聚合（aggregations）、窗口（windows）和状态（state）操作等。此层 API 中处理的数据类型在每种编程语言中都有其对应的类。
Process Function 这类底层抽象和 DataStream API 的相互集成使得用户可以选择使用更底层的抽象 API 来实现自己的需求。DataSet API 还额外提供了一些原语，比如循环/迭代（loop/iteration）操作。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/overview/</guid>
      <description>DataStream Connectors # 预定义的 Source 和 Sink # 一些比较基本的 Source 和 Sink 已经内置在 Flink 里。 预定义 data sources 支持从文件、目录、socket，以及 collections 和 iterators 中读取数据。 预定义 data sinks 支持把数据写入文件、标准输出（stdout）、标准错误输出（stderr）和 socket。
附带的连接器 # 连接器可以和多种多样的第三方系统进行交互。目前支持以下系统:
Apache Kafka (source/sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) FileSystem (sink) RabbitMQ (source/sink) Google PubSub (source/sink) Hybrid Source (source) Apache Pulsar (source) JDBC (sink) 请记住，在使用一种连接器时，通常需要额外的第三方组件，比如：数据存储服务器或者消息队列。 要注意这些列举的连接器是 Flink 工程的一部分，包含在发布的源码中，但是不包含在二进制发行版中。 更多说明可以参考对应的子部分。
Apache Bahir 中的连接器 # Flink 还有些一些额外的连接器通过 Apache Bahir 发布, 包括:</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/overview/</guid>
      <description>Table &amp;amp; SQL Connectors # Flink&amp;rsquo;s Table API &amp;amp; SQL programs can be connected to other external systems for reading and writing both batch and streaming tables. A table source provides access to data which is stored in external systems (such as a database, key-value store, message queue, or file system). A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, Avro, Parquet, or ORC.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/overview/</guid>
      <description>高可用 # JobManager 高可用（HA）模式加强了 Flink 集群防止 JobManager 故障的能力。 此特性确保 Flink 集群将始终持续执行你提交的作业。
JobManager 高可用 # JobManager 协调每个 Flink 的部署。它同时负责 调度 和 资源管理。
默认情况下，每个 Flink 集群只有一个 JobManager 实例。这会导致 单点故障（SPOF）：如果 JobManager 崩溃，则不能提交任何新程序，运行中的程序也会失败。
使用 JobManager 高可用模式，你可以从 JobManager 失败中恢复，从而消除单点故障。你可以为每个集群部署配置高可用模式。 有关更多信息，请参阅 高可用服务。
如何启用集群高可用 # JobManager 高可用一般概念是指，在任何时候都有 一个领导者 JobManager，如果领导者出现故障，则有多个备用 JobManager 来接管领导。这保证了 不存在单点故障，只要有备用 JobManager 担任领导者，程序就可以继续运行。
如下是一个使用三个 JobManager 实例的例子：
Flink 的 高可用服务 封装了所需的服务，使一切可以正常工作：
领导者选举：从 n 个候选者中选出一个领导者 服务发现：检索当前领导者的地址 状态持久化：继承程序恢复作业所需的持久化状态（JobGraphs、用户代码jar、已完成的检查点） Back to top
高可用服务 # Flink 提供了两种高可用服务实现：
ZooKeeper：每个 Flink 集群部署都可以使用 ZooKeeper HA 服务。它们需要一个运行的 ZooKeeper 复制组（quorum）。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/overview/</guid>
      <description>Deployment # Flink is a versatile framework, supporting many different deployment scenarios in a mix and match fashion.
Below, we briefly explain the building blocks of a Flink cluster, their purpose and available implementations. If you just want to start Flink locally, we recommend setting up a Standalone Cluster.
Overview and Reference Architecture # The figure below shows the building blocks of every Flink cluster. There is always somewhere a client running.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/overview/</guid>
      <description>项目配置 # 本节将向你展示如何通过流行的构建工具 (Maven、Gradle) 配置你的项目，必要的依赖项（比如连接器和格式），以及覆盖一些高级配置主题。
每个 Flink 应用程序都依赖于一组 Flink 库。应用程序至少依赖于 Flink API，此外还依赖于某些连接器库（比如 Kafka、Cassandra），以及用户开发的自定义的数据处理逻辑所需要的第三方依赖项。
开始 # 要开始使用 Flink 应用程序，请使用以下命令、脚本和模板来创建 Flink 项目。
Maven 你可以使用如下的 Maven 命令或快速启动脚本，基于原型创建一个项目。
Maven 命令 # $ mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.16-SNAPSHOT 这允许你命名新建的项目，而且会交互式地询问 groupId、artifactId、package 的名字。
快速启动脚本 # $ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.16-SNAPSHOT Gradle 你可以创建一个空项目，你需要在其中手动创建 src/main/java 和 src/main/resources 目录并开始在其中编写一些类，使用如下 Gradle 构建脚本或下面提供的快速启动脚本以获得功能齐全的启动项目。
Gradle 构建脚本 # 请在脚本的所在目录执行 gradle 命令来执行这些构建配置脚本。
build.gradle
plugins { id &amp;#39;java&amp;#39; id &amp;#39;application&amp;#39; // shadow plugin to produce fat JARs id &amp;#39;com.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/overview/</guid>
      <description>DataSet API 编程指南 # DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/types_serialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/types_serialization/</guid>
      <description>数据类型以及序列化 # Apache Flink 以其独特的方式来处理数据类型以及序列化，这种方式包括它自身的类型描述符、泛型类型提取以及类型序列化框架。 本文档描述了它们背后的概念和基本原理。
Supported Data Types # Flink places some restrictions on the type of elements that can be in a DataStream. The reason for this is that the system analyzes the types to determine efficient execution strategies.
There are seven different categories of data types:
Java Tuples and Scala Case Classes Java POJOs Primitive Types Regular Classes Values Hadoop Writables Special Types Tuples and Case Classes # Java Tuples are composite types that contain a fixed number of fields with various types.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/overview/</guid>
      <description>算子 # 用户通过算子能将一个或多个 DataStream 转换成新的 DataStream，在应用程序中可以将多个数据转换算子合并成一个复杂的数据流拓扑。
这部分内容将描述 Flink DataStream API 中基本的数据转换 API，数据转换后各种数据分区方式，以及算子的链接策略。
数据流转换 # Map # DataStream → DataStream # 输入一个元素同时输出一个元素。下面是将输入流中元素数值加倍的 map function：
Java DataStream&amp;lt;Integer&amp;gt; dataStream = //... dataStream.map(new MapFunction&amp;lt;Integer, Integer&amp;gt;() { @Override public Integer map(Integer value) throws Exception { return 2 * value; } }); Scala dataStream.map { x =&amp;gt; x * 2 } Python data_stream = env.from_collection(collection=[1, 2, 3, 4, 5]) data_stream.map(lambda x: 2 * x, output_type=Types.INT()) FlatMap # DataStream → DataStream # 输入一个元素同时产生零个、一个或多个元素。下面是将句子拆分为单词的 flatmap function：</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/overview/</guid>
      <description>Flink DataStream API 编程指南 # Flink 中的 DataStream 程序是对数据流（例如过滤、更新状态、定义窗口、聚合）进行转换的常规程序。数据流的起始是从各种源（例如消息队列、套接字流、文件）创建的。结果通过 sink 返回，例如可以将数据写入文件或标准输出（例如命令行终端）。Flink 程序可以在各种上下文中运行，可以独立运行，也可以嵌入到其它程序中。任务执行可以运行在本地 JVM 中，也可以运行在多台机器的集群上。
为了创建你自己的 Flink DataStream 程序，我们建议你从 Flink 程序剖析开始，然后逐渐添加自己的 stream transformation。其余部分作为附加的算子和高级特性的参考。
DataStream 是什么? # DataStream API 得名于特殊的 DataStream 类，该类用于表示 Flink 程序中的数据集合。你可以认为 它们是可以包含重复项的不可变数据集合。这些数据可以是有界（有限）的，也可以是无界（无限）的，但用于处理它们的API是相同的。
DataStream 在用法上类似于常规的 Java 集合，但在某些关键方面却大不相同。它们是不可变的，这意味着一旦它们被创建，你就不能添加或删除元素。你也不能简单地察看内部元素，而只能使用 DataStream API 操作来处理它们，DataStream API 操作也叫作转换（transformation）。
你可以通过在 Flink 程序中添加 source 创建一个初始的 DataStream。然后，你可以基于 DataStream 派生新的流，并使用 map、filter 等 API 方法把 DataStream 和派生的流连接在一起。
Flink 程序剖析 # Flink 程序看起来像一个转换 DataStream 的常规程序。每个程序由相同的基本部分组成：
获取一个执行环境（execution environment）； 加载/创建初始数据； 指定数据相关的转换； 指定计算结果的存储位置； 触发程序执行。 Java 现在我们将对这些步骤逐一进行概述，更多细节请参考相关章节。请注意，Java DataStream API 的所有核心类都可以在 org.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/overview/</guid>
      <description>Python API # PyFlink 是 Apache Flink 的 Python API，你可以使用它构建可扩展的批处理和流处理任务，例如实时数据处理管道、大规模探索性数据分析、机器学习（ML）管道和 ETL 处理。 如果你对 Python 和 Pandas 等库已经比较熟悉，那么 PyFlink 可以让你更轻松地利用 Flink 生态系统的全部功能。 根据你需要的抽象级别的不同，有两种不同的 API 可以在 PyFlink 中使用：
PyFlink Table API 允许你使用类似于 SQL 或者在 Python 中处理表格数据的方式编写强大的关系查询。 与此同时，PyFlink DataStream API 允许你对 Flink 的核心组件 state 和 time 进行细粒度的控制，以便构建更复杂的流处理应用。 尝试 PyFlink # 如果你有兴趣使用 PyFlink，可以尝试以下教程：
PyFlink DataStream API 介绍 PyFlink Table API 介绍 如果你想了解更多关于 PyFlink 的示例，可以参考 PyFlink 示例 深入 PyFlink # 这些参考文档涵盖了 PyFlink 的所有细节，可以从以下链接入手：
PyFlink DataStream API PyFlink Table API &amp;amp; SQL 获取有关 PyFlink 的帮助 # 如果你遇到困难，请查看我们的社区支持资源，特别是 Apache Flink 的用户邮件列表，Apache Flink 的用户邮件列表一直是所有 Apache 项目中最活跃的项目邮件列表之一，是快速获得帮助的好方法。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/overview/</guid>
      <description>User-defined Functions # PyFlink Table API empowers users to do data transformations with Python user-defined functions.
Currently, it supports two kinds of Python user-defined functions: the general Python user-defined functions which process data one row at a time and vectorized Python user-defined functions which process data one batch at a time.
打包 UDFs # 如果你在非 local 模式下运行 Python UDFs 和 Pandas UDFs，且 Python UDFs 没有定义在含 main() 入口的 Python 主文件中，强烈建议你通过 python-files 配置项指定 Python UDF 的定义。 否则，如果你将 Python UDFs 定义在名为 my_udf.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/overview/</guid>
      <description>Table API &amp;amp; SQL # Apache Flink 有两种关系型 API 来做流批统一处理：Table API 和 SQL。Table API 是用于 Scala 和 Java 语言的查询API，它可以用一种非常直观的方式来组合使用选取、过滤、join 等关系型算子。Flink SQL 是基于 Apache Calcite 来实现的标准 SQL。无论输入是连续的（流式）还是有界的（批处理），在两个接口中指定的查询都具有相同的语义，并指定相同的结果。
Table API 和 SQL 两种 API 是紧密集成的，以及 DataStream API。你可以在这些 API 之间，以及一些基于这些 API 的库之间轻松的切换。比如，你可以先用 CEP 从 DataStream 中做模式匹配，然后用 Table API 来分析匹配的结果；或者你可以用 SQL 来扫描、过滤、聚合一个批式的表，然后再跑一个 Gelly 图算法 来处理已经预处理好的数据。
Table 程序依赖 # 您需要将 Table API 作为依赖项添加到项目中，以便用 Table API 和 SQL 定义数据管道。
有关如何为 Java 和 Scala 配置这些依赖项的更多细节，请查阅项目配置小节。
如果您使用 Python，请查阅 Python API 文档。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/overview/</guid>
      <description>SQL # 本页面描述了 Flink 所支持的 SQL 语言，包括数据定义语言（Data Definition Language，DDL）、数据操纵语言（Data Manipulation Language，DML）以及查询语言。Flink 对 SQL 的支持基于实现了 SQL 标准的 Apache Calcite。
本页面列出了目前 Flink SQL 所支持的所有语句：
SELECT (Queries) CREATE TABLE, CATALOG, DATABASE, VIEW, FUNCTION DROP TABLE, DATABASE, VIEW, FUNCTION ALTER TABLE, DATABASE, FUNCTION ANALYZE TABLE INSERT SQL HINTS DESCRIBE EXPLAIN USE SHOW LOAD UNLOAD 数据类型 # 请参考专门描述该主题的页面 数据类型。
通用类型与（嵌套的）复合类型 （如：POJO、tuples、rows、Scala case 类) 都可以作为行的字段。
复合类型的字段任意的嵌套可被 值访问函数 访问。
通用类型将会被视为一个黑箱，且可以被 用户自定义函数 传递或引用。
对于 DDL 语句而言，我们支持所有在 数据类型 页面中定义的数据类型。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/overview/</guid>
      <description>查询 # SELECT statements and VALUES statements are specified with the sqlQuery() method of the TableEnvironment. The method returns the result of the SELECT statement (or the VALUES statements) as a Table. A Table can be used in subsequent SQL and Table API queries, be converted into a DataStream, or written to a TableSink. SQL and Table API queries can be seamlessly mixed and are holistically optimized and translated into a single program.</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/overview/</guid>
      <description>实践练习 # 本章教程的目标及涵盖范围 # 本章教程对 Apache Flink 的基本概念进行了介绍，虽然省略了许多重要细节，但是如果你掌握了本章内容，就足以实现可扩展并行度的 ETL、数据分析以及事件驱动的流式应用程序。本章重点对 Flink API 中的状态管理和时间进行了介绍，掌握了这些基础知识后，你将能更好地从其他详细参考文档中获取和掌握你所需要的知识。每小节结尾都有链接去引导你了解更多内容。
具体来说，你将在本章学习到以下内容：
如何实现流数据处理管道（pipelines） Flink 如何管理状态以及为何需要管理状态 如何使用事件时间（event time）来一致并准确地进行计算分析 如何在源源不断的数据流上构建事件驱动的应用程序 Flink 如何提供具有精确一次（exactly-once）计算语义的可容错、有状态流处理 本章教程着重介绍四个概念：源源不断的流式数据处理、事件时间、有状态流处理和状态快照。基本概念介绍如下。
每小节教程都有实践练习引导你如何在程序中使用其所述的概念，并在小节结尾都提供了相关实践练习的代码链接。 Back to top
流处理 # 在自然环境中，数据的产生原本就是流式的。无论是来自 Web 服务器的事件数据，证券交易所的交易数据，还是来自工厂车间机器上的传感器数据，其数据都是流式的。但是当你分析数据时，可以围绕 有界流（bounded）或 无界流（unbounded）两种模型来组织处理数据，当然，选择不同的模型，程序的执行和处理方式也都会不同。
批处理是有界数据流处理的范例。在这种模式下，你可以选择在计算结果输出之前输入整个数据集，这也就意味着你可以对整个数据集的数据进行排序、统计或汇总计算后再输出结果。
流处理正相反，其涉及无界数据流。至少理论上来说，它的数据输入永远不会结束，因此程序必须持续不断地对到达的数据进行处理。
在 Flink 中，应用程序由用户自定义算子转换而来的流式 dataflows 所组成。这些流式 dataflows 形成了有向图，以一个或多个源（source）开始，并以一个或多个汇（sink）结束。
通常，程序代码中的 transformation 和 dataflow 中的算子（operator）之间是一一对应的。但有时也会出现一个 transformation 包含多个算子的情况，如上图所示。
Flink 应用程序可以消费来自消息队列或分布式日志这类流式数据源（例如 Apache Kafka 或 Kinesis）的实时数据，也可以从各种的数据源中消费有界的历史数据。同样，Flink 应用程序生成的结果流也可以发送到各种数据汇中。
并行 Dataflows # Flink 程序本质上是分布式并行程序。在程序执行期间，一个流有一个或多个流分区（Stream Partition），每个算子有一个或多个算子子任务（Operator Subtask）。每个子任务彼此独立，并在不同的线程中运行，或在不同的计算机或容器中运行。
算子子任务数就是其对应算子的并行度。在同一程序中，不同算子也可能具有不同的并行度。
Flink 算子之间可以通过一对一（直传）模式或重新分发模式传输数据：
一对一模式（例如上图中的 Source 和 map() 算子之间）可以保留元素的分区和顺序信息。这意味着 map() 算子的 subtask[1] 输入的数据以及其顺序与 Source 算子的 subtask[1] 输出的数据和顺序完全相同，即同一分区的数据只会进入到下游算子的同一分区。</description>
    </item>
    
    <item>
      <title>概念与通用 API</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/common/</guid>
      <description>概念与通用 API # Table API 和 SQL 集成在同一套 API 中。 这套 API 的核心概念是Table，用作查询的输入和输出。 本文介绍 Table API 和 SQL 查询程序的通用结构、如何注册 Table 、如何查询 Table 以及如何输出 Table 。
Table API 和 SQL 程序的结构 # 所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例展示了 Table API 和 SQL 程序的通用结构。
Java import org.apache.flink.table.api.*; import org.apache.flink.connector.datagen.table.DataGenConnectorOptions; // Create a TableEnvironment for batch or streaming execution. // See the &amp;#34;Create a TableEnvironment&amp;#34; section for details. TableEnvironment tableEnv = TableEnvironment.create(/*…*/); // Create a source table tableEnv.</description>
    </item>
    
    <item>
      <title>函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/overview/</guid>
      <description>函数 # Flink 允许用户在 Table API 和 SQL 中使用函数进行数据的转换。
函数类型 # Flink 中的函数有两个划分标准。
一个划分标准是：系统（内置）函数和 Catalog 函数。系统函数没有名称空间，只能通过其名称来进行引用。 Catalog 函数属于 Catalog 和数据库，因此它们拥有 Catalog 和数据库命名空间。 用户可以通过全/部分限定名（catalog.db.func 或 db.func）或者函数名 来对 Catalog 函数进行引用。
另一个划分标准是：临时函数和持久化函数。 临时函数始终由用户创建，它容易改变并且仅在会话的生命周期内有效。 持久化函数不是由系统提供，就是存储在 Catalog 中，它在会话的整个生命周期内都有效。
这两个划分标准给 Flink 用户提供了 4 种函数：
临时性系统函数 系统函数 临时性 Catalog 函数 Catalog 函数 请注意，系统函数始终优先于 Catalog 函数解析，临时函数始终优先于持久化函数解析， 函数解析优先级如下所述。
函数引用 # 用户在 Flink 中可以通过精确、模糊两种引用方式引用函数。
精确函数引用 # 精确函数引用允许用户跨 Catalog，跨数据库调用 Catalog 函数。 例如：select mycatalog.mydb.myfunc(x) from mytable 和 select mydb.myfunc(x) from mytable。</description>
    </item>
    
    <item>
      <title>流式概念</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/overview/</guid>
      <description>流式概念 # Flink 的 Table API 和 SQL 是流批统一的 API。 这意味着 Table API &amp;amp; SQL 在无论有限的批式输入还是无限的流式输入下，都具有相同的语义。 因为传统的关系代数以及 SQL 最开始都是为了批式处理而设计的， 关系型查询在流式场景下不如在批式场景下容易懂。
下面这些页面包含了概念、实际的限制，以及流式数据处理中的一些特定的配置。
状态管理 # 流模式下运行的表程序利用了 Flink 作为有状态流处理器的所有能力。
事实上，一个表程序（Table program）可以配置一个 state backend 和多个不同的 checkpoint 选项 以处理对不同状态大小和容错需求。这可以对正在运行的 Table API &amp;amp; SQL 管道（pipeline）生成 savepoint，并在这之后用其恢复应用程序的状态。
状态使用 # 由于 Table API &amp;amp; SQL 程序是声明式的，管道内的状态会在哪以及如何被使用并不明确。 Planner 会确认是否需要状态来得到正确的计算结果， 管道会被现有优化规则集优化成尽可能少地使用状态。
从概念上讲， 源表从来不会在状态中被完全保存。 实现者处理的是逻辑表（即动态表）。 它们的状态取决于用到的操作。 形如 SELECT ... FROM ... WHERE 这种只包含字段映射或过滤器的查询的查询语句通常是无状态的管道。 然而诸如 join、 聚合或去重操作需要在 Flink 抽象的容错存储内保存中间结果。
请参考独立的算子文档来获取更多关于状态需求量和限制潜在增长状态大小的信息。 例如对两个表进行 join 操作的普通 SQL 需要算子保存两个表的全部输入。基于正确的 SQL 语义，运行时假设两表会在任意时间点进行匹配。 Flink 提供了 优化窗口和时段 Join 聚合 以利用 watermarks 概念来让保持较小的状态规模。</description>
    </item>
    
    <item>
      <title>通用配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/common/</guid>
      <description>通用配置 # Apache Flink 提供了一些对所有文件系统均适用的基本配置。
默认文件系统 # 如果文件路径未明确指定文件系统的 scheme（和 authority），将会使用默认的 scheme（和 authority）：
fs.default-scheme: &amp;lt;default-fs&amp;gt; 例如默认的文件系统配置为 fs.default-scheme: hdfs://localhost:9000/，则文件路径 /user/hugo/in.txt 将被处理为 hdfs://localhost:9000/user/hugo/in.txt。
连接限制 # 如果文件系统不能处理大量并发读/写操作或连接，可以为文件系统同时打开的总连接数设置上限。
例如在一个大型 Flink 任务建立 checkpoint 时，具有少量 RPC handler 的小型 HDFS 集群可能会由于建立了过多的连接而过载。
要限制文件系统的连接数，可将下列配置添加至 Flink 配置中。设置限制的文件系统由其 scheme 指定：
fs.&amp;lt;scheme&amp;gt;.limit.total: (数量，0/-1 表示无限制) fs.&amp;lt;scheme&amp;gt;.limit.input: (数量，0/-1 表示无限制) fs.&amp;lt;scheme&amp;gt;.limit.output: (数量，0/-1 表示无限制) fs.&amp;lt;scheme&amp;gt;.limit.timeout: (毫秒，0 表示无穷) fs.&amp;lt;scheme&amp;gt;.limit.stream-timeout: (毫秒，0 表示无穷) 输入和输出连接（流）的数量可以分别进行限制（fs.&amp;lt;scheme&amp;gt;.limit.input 和 fs.&amp;lt;scheme&amp;gt;.limit.output），也可以限制并发流的总数（fs.&amp;lt;scheme&amp;gt;.limit.total）。如果文件系统尝试打开更多的流，操作将被阻塞直至某些流关闭。如果打开流的时间超过 fs.&amp;lt;scheme&amp;gt;.limit.timeout，则流打开失败。
为避免不活动的流占满整个连接池（阻止新连接的建立），可以在配置中添加无活动超时时间，如果连接至少在 fs.&amp;lt;scheme&amp;gt;.limit.stream-timeout 时间内没有读/写操作，则连接会被强制关闭。
连接数是按每个 TaskManager/文件系统来进行限制的。因为文件系统的创建是按照 scheme 和 authority 进行的，所以不同的 authority 具有独立的连接池，例如 hdfs://myhdfs:50010/ 和 hdfs://anotherhdfs:4399/ 会有单独的连接池。</description>
    </item>
    
    <item>
      <title>文件系统</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/overview/</guid>
      <description>文件系统 # Apache Flink 使用文件系统来消费和持久化地存储数据，以处理应用结果以及容错与恢复。以下是一些最常用的文件系统：本地存储，hadoop-compatible，Amazon S3，阿里云 OSS 和 Azure Blob Storage。
文件使用的文件系统通过其 URI Scheme 指定。例如 file:///home/user/text.txt 表示一个在本地文件系统中的文件，hdfs://namenode:50010/data/user/text.txt 表示一个在指定 HDFS 集群中的文件。
文件系统在每个进程实例化一次，然后进行缓存/池化，从而避免每次创建流时的配置开销，并强制执行特定的约束，如连接/流的限制。
本地文件系统 # Flink 原生支持本地机器上的文件系统，包括任何挂载到本地文件系统的 NFS 或 SAN 驱动器，默认即可使用，无需额外配置。本地文件可通过 file:// URI Scheme 引用。
外部文件系统 # Apache Flink 支持下列文件系统：
Amazon S3 对象存储由 flink-s3-fs-presto 和 flink-s3-fs-hadoop 两种替代实现提供支持。这两种实现都是独立的，没有依赖项。
阿里云对象存储 由 flink-oss-fs-hadoop 支持，并通过 oss:// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。
Azure Blob Storage 由flink-azure-fs-hadoop 支持，并通过 abfs(s):// 和 wasb(s):// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。
Google Cloud Storage 由gcs-connector 支持，并通过 gs:// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。</description>
    </item>
    
    <item>
      <title>Amazon S3</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/s3/</guid>
      <description>Amazon S3 # Amazon Simple Storage Service (Amazon S3) 提供用于多种场景的云对象存储。S3 可与 Flink 一起使用以读取、写入数据，并可与 流的 State backends 相结合使用。
通过以下格式指定路径，S3 对象可类似于普通文件使用：
s3://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt; Endpoint 可以是一个文件或目录，例如：
// 读取 S3 bucket env.readTextFile(&amp;#34;s3://&amp;lt;bucket&amp;gt;/&amp;lt;endpoint&amp;gt;&amp;#34;); // 写入 S3 bucket stream.writeAsText(&amp;#34;s3://&amp;lt;bucket&amp;gt;/&amp;lt;endpoint&amp;gt;&amp;#34;); // 使用 S3 作为 FsStatebackend env.setStateBackend(new FsStateBackend(&amp;#34;s3://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt;&amp;#34;)); 注意这些例子并不详尽，S3 同样可以用在其他场景，包括 JobManager 高可用配置 或 RocksDBStateBackend，以及所有 Flink 需要使用文件系统 URI 的位置。
在大部分使用场景下，可使用 flink-s3-fs-hadoop 或 flink-s3-fs-presto 两个独立且易于设置的 S3 文件系统插件。然而在某些情况下，例如使用 S3 作为 YARN 的资源存储目录时，可能需要配置 Hadoop S3 文件系统。
Hadoop/Presto S3 文件系统插件 # 如果您在使用 Flink on EMR，您无需手动对此进行配置。 Flink 提供两种文件系统用来与 S3 交互：flink-s3-fs-presto 和 flink-s3-fs-hadoop。两种实现都是独立的且没有依赖项，因此使用时无需将 Hadoop 添加至 classpath。</description>
    </item>
    
    <item>
      <title>CSV</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/</guid>
      <description>CSV Format # Format: Serialization Schema Format: Deserialization Schema
CSV Format 允许我们基于 CSV schema 进行解析和生成 CSV 数据。 目前 CSV schema 是基于 table schema 推断而来的。
依赖 # In order to use the CSV format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-csv&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>Graph API</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_api/</guid>
      <description>Graph API # Graph Representation # In Gelly, a Graph is represented by a DataSet of vertices and a DataSet of edges.
The Graph nodes are represented by the Vertex type. A Vertex is defined by a unique ID and a value. Vertex IDs should implement the Comparable interface. Vertices without value can be represented by setting the value type to NullValue.
Java // create a new vertex with a Long ID and a String value Vertex&amp;lt;Long, String&amp;gt; v = new Vertex&amp;lt;Long, String&amp;gt;(1L, &amp;#34;foo&amp;#34;); // create a new vertex with a Long ID and no value Vertex&amp;lt;Long, NullValue&amp;gt; v = new Vertex&amp;lt;Long, NullValue&amp;gt;(1L, NullValue.</description>
    </item>
    
    <item>
      <title>Hints</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/hints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/hints/</guid>
      <description>Hints # Batch Streaming
SQL hints 是和 SQL 语句一起使用来改变执行计划的。本章介绍如何使用 SQL hints 增强各种方法。
SQL hints 一般可以用于以下：
增强 planner：没有完美的 planner，所以实现 SQL hints 让用户更好地控制执行是非常有意义的； 增加元数据（或者统计信息）：如&amp;quot;已扫描的表索引&amp;quot;和&amp;quot;一些混洗键（shuffle keys）的倾斜信息&amp;quot;的一些统计数据对于查询来说是动态的，用 hints 来配置它们会非常方便，因为我们从 planner 获得的计划元数据通常不那么准确； 算子（Operator）资源约束：在许多情况下，我们会为执行算子提供默认的资源配置，即最小并行度或托管内存（UDF 资源消耗）或特殊资源需求（GPU 或 SSD 磁盘）等，可以使用 SQL hints 非常灵活地为每个查询（非作业）配置资源。 动态表（Dynamic Table）选项 # 动态表选项允许动态地指定或覆盖表选项，不同于用 SQL DDL 或 连接 API 定义的静态表选项，这些选项可以在每个查询的每个表范围内灵活地指定。
因此，它非常适合用于交互式终端中的特定查询，例如，在 SQL-CLI 中，你可以通过添加动态选项/*+ OPTIONS(&#39;csv.ignore-parse-errors&#39;=&#39;true&#39;) */来指定忽略 CSV 源的解析错误。
语法 # 为了不破坏 SQL 兼容性，我们使用 Oracle 风格的 SQL hints 语法：
table_path /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral 示例 # CREATE TABLE kafka_table1 (id BIGINT, name STRING, age INT) WITH (.</description>
    </item>
    
    <item>
      <title>Hive Catalog</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_catalog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_catalog/</guid>
      <description>Hive Catalog # Hive Metastore has evolved into the de facto metadata hub over the years in Hadoop ecosystem. Many companies have a single Hive Metastore service instance in their production to manage all of their metadata, either Hive metadata or non-Hive metadata, as the source of truth.
For users who have both Hive and Flink deployments, HiveCatalog enables them to use Hive Metastore to manage Flink&amp;rsquo;s metadata.
For users who have just Flink deployment, HiveCatalog is the only persistent catalog provided out-of-box by Flink.</description>
    </item>
    
    <item>
      <title>SSL 设置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/security/security-ssl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/security/security-ssl/</guid>
      <description>SSL 设置 # This page provides instructions on how to enable TLS/SSL authentication and encryption for network communication with and between Flink processes. NOTE: TLS/SSL authentication is not enabled by default.
Internal and External Connectivity # When securing network connections between machines processes through authentication and encryption, Apache Flink differentiates between internal and external connectivity. Internal Connectivity refers to all connections made between Flink processes. These connections run Flink custom protocols.</description>
    </item>
    
    <item>
      <title>Transformations</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/transformations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/transformations/</guid>
      <description>DataSet Transformations # This document gives a deep-dive into the available transformations on DataSets. For a general introduction to the Flink Java API, please refer to the Programming Guide.
For zipping elements in a data set with a dense index, please refer to the Zip Elements Guide.
Map # The Map transformation applies a user-defined map function on each element of a DataSet. It implements a one-to-one mapping, that is, exactly one element must be returned by the function.</description>
    </item>
    
    <item>
      <title>Windows</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/windows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/windows/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Working with State</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state/</guid>
      <description>使用状态 # 本章节您将了解 Flink 用于编写有状态程序的 API。要了解有状态流处理背后的概念，请参阅Stateful Stream Processing。
Keyed DataStream # 如果你希望使用 keyed state，首先需要为DataStream指定 key（主键）。这个主键用于状态分区（也会给数据流中的记录本身分区）。 你可以使用 DataStream 中 Java/Scala API 的 keyBy(KeySelector) 或者是 Python API 的 key_by(KeySelector) 来指定 key。 它将生成 KeyedStream，接下来允许使用 keyed state 操作。
Key selector 函数接收单条记录作为输入，返回这条记录的 key。该 key 可以为任何类型，但是它的计算产生方式必须是具备确定性的。
Flink 的数据模型不基于 key-value 对，因此实际上将数据集在物理上封装成 key 和 value 是没有必要的。 Key 是“虚拟”的。它们定义为基于实际数据的函数，用以操纵分组算子。
下面的例子展示了 key selector 函数。它仅返回了对象当中的字段。
Java // some ordinary POJO public class WC { public String word; public int count; public String getWord() { return word; } } DataStream&amp;lt;WC&amp;gt; words = // [.</description>
    </item>
    
    <item>
      <title>ZooKeeper 高可用服务</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/zookeeper_ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/zookeeper_ha/</guid>
      <description>ZooKeeper 高可用服务 # Flink 的 ZooKeeper 高可用模式使用 ZooKeeper 提供高可用服务。
Flink 利用 ZooKeeper 在所有运行的 JobManager 实例之间进行 分布式协调。ZooKeeper 是一个独立于 Flink 的服务，它通过 leader 选举和轻量级的一致性状态存储来提供高可靠的分布式协调。查看 ZooKeeper入门指南，了解更多关于 ZooKeeper 的信息。Flink 包含 启动一个简单的ZooKeeper 的安装脚本。
配置 # 为了启用高可用集群（HA-cluster），你必须设置以下配置项:
high-availability (必要的): high-availability 配置项必须设置为 zookeeper。
high-availability: zookeeper high-availability.storageDir (必要的): JobManager 元数据持久化到文件系统 high-availability.storageDir 配置的路径中，并且在 ZooKeeper 中只能有一个目录指向此位置。
high-availability.storageDir: hdfs:///flink/recovery storageDir 存储要从 JobManager 失败恢复时所需的所有元数据。
high-availability.zookeeper.quorum (必要的): ZooKeeper quorum 是一个提供分布式协调服务的复制组。
high-availability.zookeeper.quorum: address1:2181[,...],addressX:2181 每个 addressX:port 指的是一个 ZooKeeper 服务器，它可以被 Flink 在给定的地址和端口上访问。
high-availability.zookeeper.path.root (推荐的): ZooKeeper 根节点，集群的所有节点都放在该节点下。</description>
    </item>
    
    <item>
      <title>窗口</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/</guid>
      <description>窗口 # 窗口（Window）是处理无界流的关键所在。窗口可以将数据流装入大小有限的“桶”中，再对每个“桶”加以处理。 本文的重心将放在 Flink 如何进行窗口操作以及开发者如何尽可能地利用 Flink 所提供的功能。
下面展示了 Flink 窗口在 keyed streams 和 non-keyed streams 上使用的基本结构。 我们可以看到，这两者唯一的区别仅在于：keyed streams 要调用 keyBy(...)后再调用 window(...) ， 而 non-keyed streams 只用直接调用 windowAll(...)。留意这个区别，它能帮我们更好地理解后面的内容。
Keyed Windows
Java/Scala stream .keyBy(...) &amp;lt;- 仅 keyed 窗口需要 .window(...) &amp;lt;- 必填项：&amp;quot;assigner&amp;quot; [.trigger(...)] &amp;lt;- 可选项：&amp;quot;trigger&amp;quot; (省略则使用默认 trigger) [.evictor(...)] &amp;lt;- 可选项：&amp;quot;evictor&amp;quot; (省略则不使用 evictor) [.allowedLateness(...)] &amp;lt;- 可选项：&amp;quot;lateness&amp;quot; (省略则为 0) [.sideOutputLateData(...)] &amp;lt;- 可选项：&amp;quot;output tag&amp;quot; (省略则不对迟到数据使用 side output) .reduce/aggregate/apply() &amp;lt;- 必填项：&amp;quot;function&amp;quot; [.getSideOutput(...)] &amp;lt;- 可选项：&amp;quot;output tag&amp;quot; Python stream .</description>
    </item>
    
    <item>
      <title>调试窗口与事件时间</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/debugging_event_time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/debugging_event_time/</guid>
      <description>调试窗口与事件时间 # 监控当前事件时间（Event Time） # Flink 的事件时间和 watermark 支持对于处理乱序事件是十分强大的特性。然而，由于是系统内部跟踪时间进度，所以很难了解究竟正在发生什么。
可以通过 Flink web 界面或指标系统访问 task 的 low watermarks。
Flink 中的 task 通过调用 currentInputWatermark 方法暴露一个指标，该指标表示当前 task 所接收到的 the lowest watermark。这个 long 类型值表示“当前事件时间”。该值通过获取上游算子收到的所有 watermarks 的最小值来计算。这意味着用 watermarks 跟踪的事件时间总是由最落后的 source 控制。
使用 web 界面可以访问 low watermark 指标，在指标选项卡中选择一个 task，然后选择 &amp;lt;taskNr&amp;gt;.currentInputWatermark 指标。在新的显示框中，你可以看到此 task 的当前 low watermark。
获取指标的另一种方式是使用指标报告器之一，如指标系统文档所述。对于本地集群设置，我们推荐使用 JMX 指标报告器和类似于 VisualVM 的工具。
处理散乱的事件时间 # 方式 1：延迟的 Watermark（表明完整性），窗口提前触发 方式 2：具有最大延迟启发式的 Watermark，窗口接受迟到的数据 Back to top</description>
    </item>
    
    <item>
      <title>动态表 (Dynamic Table)</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/dynamic_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/dynamic_tables/</guid>
      <description>动态表 (Dynamic Table) # SQL 和关系代数在设计时并未考虑流数据。因此，在关系代数(和 SQL)之间几乎没有概念上的差异。
本文会讨论这种差异，并介绍 Flink 如何在无界数据集上实现与数据库引擎在有界数据上的处理具有相同的语义。
DataStream 上的关系查询 # 下表比较了传统的关系代数和流处理与输入数据、执行和输出结果的关系。
关系代数 / SQL 流处理 关系(或表)是有界(多)元组集合。 流是一个无限元组序列。 对批数据(例如关系数据库中的表)执行的查询可以访问完整的输入数据。 流式查询在启动时不能访问所有数据，必须“等待”数据流入。 批处理查询在产生固定大小的结果后终止。 流查询不断地根据接收到的记录更新其结果，并且始终不会结束。 尽管存在这些差异，但是使用关系查询和 SQL 处理流并不是不可能的。高级关系数据库系统提供了一个称为 物化视图(Materialized Views) 的特性。物化视图被定义为一条 SQL 查询，就像常规的虚拟视图一样。与虚拟视图相反，物化视图缓存查询的结果，因此在访问视图时不需要对查询进行计算。缓存的一个常见难题是防止缓存为过期的结果提供服务。当其定义查询的基表被修改时，物化视图将过期。 即时视图维护(Eager View Maintenance) 是一种一旦更新了物化视图的基表就立即更新视图的技术。
如果我们考虑以下问题，那么即时视图维护和流上的SQL查询之间的联系就会变得显而易见:
数据库表是 INSERT、UPDATE 和 DELETE DML 语句的 stream 的结果，通常称为 changelog stream 。 物化视图被定义为一条 SQL 查询。为了更新视图，查询不断地处理视图的基本关系的changelog 流。 物化视图是流式 SQL 查询的结果。 了解了这些要点之后，我们将在下一节中介绍 动态表(Dynamic tables) 的概念。
动态表 &amp;amp; 连续查询(Continuous Query) # 动态表 是 Flink 的支持流数据的 Table API 和 SQL 的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。可以像查询静态批处理表一样查询它们。查询动态表将生成一个 连续查询 。一个连续查询永远不会终止，结果会生成一个动态表。查询不断更新其(动态)结果表，以反映其(动态)输入表上的更改。本质上，动态表上的连续查询非常类似于定义物化视图的查询。</description>
    </item>
    
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/overview/</guid>
      <description>Standalone # 本页面提供了关于如何在静态（但可能异构）集群上以完全分布式方式运行 Flink 的说明。
需求 # 软件需求 # Flink 运行在所有类 UNIX 环境下，例如 Linux，Mac OS X 和 Cygwin （Windows），集群由一个 master 节点以及一个或多个 worker 节点构成。在配置系统之前，请确保在每个节点上安装有以下软件：
Java 1.8.x 或更高版本， ssh （必须运行 sshd 以执行用于管理 Flink 各组件的脚本） 如果集群不满足软件要求，那么你需要安装/更新这些软件。
使集群中所有节点使用免密码 SSH 以及拥有相同的目录结构可以让你使用脚本来控制一切。
Back to top
JAVA_HOME 配置 # Flink 需要 master 和所有 worker 节点设置 JAVA_HOME 环境变量，并指向你的 Java 安装目录。
你可以在 conf/flink-conf.yaml 文件中通过 env.java.home 配置项来设置此变量。
Back to top
Flink 设置 # 前往 下载页面 获取可运行的软件包。
在下载完最新的发布版本后，复制压缩文件到 master 节点并解压：</description>
    </item>
    
    <item>
      <title>监控 Checkpoint</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/monitoring/checkpoint_monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/monitoring/checkpoint_monitoring/</guid>
      <description>监控 Checkpoint # 概览（Overview） # Flink 的 Web 界面提供了选项卡/标签（tab）来监视作业的 checkpoint 信息。作业终止后，这些统计信息仍然可用。有四个不同的选项卡可显示有关 checkpoint 的信息：概览（Overview），历史记录（History），摘要信息（Summary）和配置信息（Configuration）。以下各节将依次介绍这些内容。
监控（Monitoring） # 概览（Overview）选项卡 # 概览选项卡列出了以下统计信息。请注意，这些统计信息在 JobManager 丢失时无法保存，如果 JobManager 发生故障转移，这些统计信息将重置。
Checkpoint Counts Triggered：自作业开始以来触发的 checkpoint 总数。 In Progress：当前正在进行的 checkpoint 数量。 Completed：自作业开始以来成功完成的 checkpoint 总数。 Failed：自作业开始以来失败的 checkpoint 总数。 Restored：自作业开始以来进行的恢复操作的次数。这还表示自 提交以来已重新启动多少次。请注意，带有 savepoint 的初始提交也算作一次恢复，如果 JobManager 在此操作过程中丢失，则该统计将重新计数。 Latest Completed Checkpoint：最新（最近）成功完成的 checkpoint。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Failed Checkpoint：最新失败的 checkpoint。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Savepoint：最新触发的 savepoint 及其外部路径。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Restore：有两种类型的恢复操作。 Restore from Checkpoint：从 checkpoint 恢复。 Restore from Savepoint：从 savepoint 恢复。 历史记录（History）选项卡 # Checkpoint 历史记录保存有关最近触发的 checkpoint 的统计信息，包括当前正在进行的 checkpoint。</description>
    </item>
    
    <item>
      <title>扩展资源</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/external_resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/external_resources/</guid>
      <description>扩展资源框架 # 许多计算任务需要使用除了 CPU 与内存外的资源，如用深度学习场景需要使用 GPU 来进行加速。为了支持这种扩展资源，Flink 提供了一个扩展资源框架。 该框架支持从底层资源管理系统（如 Kubernetes）请求各种类型的资源，并向算子提供使用这些资源所需的信息。该框架以插件形式支持不同的资源类型。 目前 Flink 仅内置了支持 GPU 资源的插件，你可以为你想使用的资源类型实现第三方插件。
扩展资源框架做了什么 # 扩展资源（External Resource）框架主要做了以下两件事：
根据你的配置，在 Flink 从底层资源管理系统中申请资源时，设置与扩展资源相关的请求字段
为算子提供使用这些资源所需要的信息
当 Flink 部署在资源管理系统（Kubernetes、Yarn）上时，扩展资源框架将确保分配的 Pod、Container 包含所需的扩展资源。目前，许多资源管理系统都支持扩展资源。 例如，Kubernetes 从 v1.10 开始通过 Device Plugin 机制支持 GPU、FPGA 等资源调度，Yarn 从 2.10 和 3.1 开始支持 GPU 和 FPGA 的调度。 在 Standalone 模式下，由用户负责确保扩展资源的可用性。
扩展资源框架向算子提供扩展资源相关信息，这些信息由你配置的扩展资源 Driver 生成，包含了使用扩展资源所需要的基本属性。
启用扩展资源框架 # 为了启用扩展资源框架来使用扩展资源，你需要：
为该扩展资源准备扩展资源框架的插件
为该扩展资源设置相关的配置
在你的算子中，从 RuntimeContext 来获取扩展资源的信息并使用这些资源
准备插件 # 你需要为使用的扩展资源准备插件，并将其放入 Flink 发行版的 plugins/ 文件夹中, 参看 Flink Plugins。 Flink 提供了第一方的 GPU 资源插件。你同样可以为你所使用的扩展资源实现自定义插件实现自定义插件。</description>
    </item>
    
    <item>
      <title>配置 Flink 进程的内存</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup/</guid>
      <description>配置 Flink 进程的内存 # Apache Flink 基于 JVM 的高效处理能力，依赖于其对各组件内存用量的细致掌控。 考虑到用户在 Flink 上运行的应用的多样性，尽管社区已经努力为所有配置项提供合理的默认值，仍无法满足所有情况下的需求。 为了给用户生产提供最大化的价值， Flink 允许用户在整体上以及细粒度上对集群的内存分配进行调整。为了优化内存需求，参考网络内存调优指南。
本文接下来介绍的内存配置方法适用于 1.10 及以上版本的 TaskManager 进程和 1.11 及以上版本的 JobManager 进程。 Flink 在 1.10 和 1.11 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
配置总内存 # Flink JVM 进程的*进程总内存（Total Process Memory）*包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 Flink 总内存（Total Flink Memory）包括 JVM 堆内存（Heap Memory）和堆外内存（Off-Heap Memory）。 其中堆外内存包括直接内存（Direct Memory）和本地内存（Native Memory）。
配置 Flink 进程内存最简单的方法是指定以下两个配置项中的任意一个：
配置项 TaskManager 配置参数 JobManager 配置参数 Flink 总内存 taskmanager.memory.flink.size jobmanager.memory.flink.size 进程总内存 taskmanager.memory.process.size jobmanager.</description>
    </item>
    
    <item>
      <title>容错保证</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/guarantees/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/guarantees/</guid>
      <description>Data Source 和 Sink 的容错保证 # 当程序出现错误的时候，Flink 的容错机制能恢复并继续运行程序。这种错误包括机器硬件故障、网络故障、瞬态程序故障等等。
只有当 source 参与了快照机制的时候，Flink 才能保证对自定义状态的精确一次更新。下表列举了 Flink 与其自带连接器的状态更新的保证。
请阅读各个连接器的文档来了解容错保证的细节。
Source Guarantees Notes Apache Kafka 精确一次 根据你的版本用恰当的 Kafka 连接器 AWS Kinesis Streams 精确一次 RabbitMQ 至多一次 (v 0.10) / 精确一次 (v 1.0) Google PubSub 至少一次 Collections 精确一次 Files 精确一次 Sockets 至多一次 为了保证端到端精确一次的数据交付（在精确一次的状态语义上更进一步），sink需要参与 checkpointing 机制。下表列举了 Flink 与其自带 sink 的交付保证（假设精确一次状态更新）。
Sink Guarantees Notes Elasticsearch 至少一次 Kafka producer 至少一次 / 精确一次 当使用事务生产者时，保证精确一次 (v 0.11+) Cassandra sink 至少一次 / 精确一次 只有当更新是幂等时，保证精确一次 AWS Kinesis Streams 至少一次 File sinks 精确一次 Socket sinks 至少一次 Standard output 至少一次 Redis sink 至少一次 Back to top</description>
    </item>
    
    <item>
      <title>入门</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/gettingstarted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/gettingstarted/</guid>
      <description>入门 # Flink SQL 使得使用标准 SQL 开发流应用程序变的简单。如果你曾经在工作中使用过兼容 ANSI-SQL 2011 的数据库或类似的 SQL 系统，那么就很容易学习 Flink。本教程将帮助你在 Flink SQL 开发环境下快速入门。
先决条件 # 你只需要具备 SQL 的基础知识即可，不需要其他编程经验。
安装 # 安装 Flink 有多种方式。对于实验而言，最常见的选择是下载二进制包并在本地运行。你可以按照本地模式安装中的步骤为本教程的剩余部分设置环境。
完成所有设置后，在安装文件夹中使用以下命令启动本地集群：
./bin/start-cluster.sh 启动完成后，就可以在本地访问 Flink WebUI localhost:8081，通过它，你可以监控不同的作业。
SQL 客户端 # SQL 客户端是一个交互式的客户端，用于向 Flink 提交 SQL 查询并将结果可视化。 在安装文件夹中运行 sql-client 脚本来启动 SQL 客户端。
./bin/sql-client.sh Hello World # SQL 客户端（我们的查询编辑器）启动并运行后，就可以开始编写查询了。 让我们使用以下简单查询打印出 &amp;lsquo;Hello World&amp;rsquo;：
SELECT &amp;#39;Hello World&amp;#39;; 运行 HELP 命令会列出所有支持的 SQL 语句。让我们运行一个 SHOW 命令，来查看 Flink 内置函数的完整列表。
SHOW FUNCTIONS; 这些函数为用户在开发 SQL 查询时提供了一个功能强大的工具箱。 例如，CURRENT_TIMESTAMP 将在执行时打印出机器的当前系统时间。</description>
    </item>
    
    <item>
      <title>生成 Watermark</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/event-time/generating_watermarks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/event-time/generating_watermarks/</guid>
      <description>生成 Watermark # 在本节中，你将了解 Flink 中用于处理事件时间的时间戳和 watermark 相关的 API。有关事件时间，处理时间和摄取时间的介绍，请参阅事件时间概览小节。
Watermark 策略简介 # 为了使用事件时间语义，Flink 应用程序需要知道事件时间戳对应的字段，意味着数据流中的每个元素都需要拥有可分配的事件时间戳。其通常通过使用 TimestampAssigner API 从元素中的某个字段去访问/提取时间戳。
时间戳的分配与 watermark 的生成是齐头并进的，其可以告诉 Flink 应用程序事件时间的进度。其可以通过指定 WatermarkGenerator 来配置 watermark 的生成方式。
使用 Flink API 时需要设置一个同时包含 TimestampAssigner 和 WatermarkGenerator 的 WatermarkStrategy。WatermarkStrategy 工具类中也提供了许多常用的 watermark 策略，并且用户也可以在某些必要场景下构建自己的 watermark 策略。WatermarkStrategy 接口如下：
public interface WatermarkStrategy&amp;lt;T&amp;gt; extends TimestampAssignerSupplier&amp;lt;T&amp;gt;, WatermarkGeneratorSupplier&amp;lt;T&amp;gt;{ /** * 根据策略实例化一个可分配时间戳的 {@link TimestampAssigner}。 */ @Override TimestampAssigner&amp;lt;T&amp;gt; createTimestampAssigner(TimestampAssignerSupplier.Context context); /** * 根据策略实例化一个 watermark 生成器。 */ @Override WatermarkGenerator&amp;lt;T&amp;gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context); } 如上所述，通常情况下，你不用实现此接口，而是可以使用 WatermarkStrategy 工具类中通用的 watermark 策略，或者可以使用这个工具类将自定义的 TimestampAssigner 与 WatermarkGenerator 进行绑定。例如，你想要要使用有界无序（bounded-out-of-orderness）watermark 生成器和一个 lambda 表达式作为时间戳分配器，那么可以按照如下方式实现：</description>
    </item>
    
    <item>
      <title>使用 Maven</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/maven/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/maven/</guid>
      <description>如何使用 Maven 配置您的项目 # 本指南将向您展示如何使用 Maven 配置 Flink 作业项目，Maven是 由 Apache Software Foundation 开源的自动化构建工具，使您能够构建、发布和部署项目。您可以使用它来管理软件项目的整个生命周期。
要求 # Maven 3.0.4 (or higher) Java 11 将项目导入 IDE # 创建项目目录和文件后，我们建议您将此项目导入到 IDE 进行开发和测试。
IntelliJ IDEA 支持开箱即用的 Maven 项目。Eclipse 提供了 m2e 插件 来导入 Maven 项目。
注意： Java 的默认 JVM 堆大小对于 Flink 来说可能太小，您应该手动增加它。在 Eclipse 中，选中 Run Configurations -&amp;gt; Arguments 并在 VM Arguments 框里填上：-Xmx800m。在 IntelliJ IDEA 中，推荐选中 Help | Edit Custom VM Options 菜单修改 JVM 属性。详情请查阅本文。
关于 IntelliJ 的注意事项： 要使应用程序在 IntelliJ IDEA 中运行，需要在运行配置中的 Include dependencies with &amp;ldquo;Provided&amp;rdquo; scope打勾。如果此选项不可用（可能是由于使用了较旧的 IntelliJ IDEA 版本），可创建一个调用应用程序main()` 方法的测试用例。</description>
    </item>
    
    <item>
      <title>事件处理 (CEP)</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/cep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/cep/</guid>
      <description>FlinkCEP - Flink的复杂事件处理 # FlinkCEP是在Flink上层实现的复杂事件处理库。 它可以让你在无限事件流中检测出特定的事件模型，有机会掌握数据中重要的那部分。
本页讲述了Flink CEP中可用的API，我们首先讲述模式API，它可以让你指定想在数据流中检测的模式，然后讲述如何检测匹配的事件序列并进行处理。 再然后我们讲述Flink在按照事件时间处理迟到事件时的假设， 以及如何从旧版本的Flink向1.13之后的版本迁移作业。
开始 # 如果你想现在开始尝试，创建一个 Flink 程序， 添加 FlinkCEP 的依赖到项目的pom.xml文件中。
Java &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-cep&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Scala &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-cep-scala_2.12&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! FlinkCEP 不是二进制发布包的一部分。在集群上执行如何链接它可以看这里。 现在可以开始使用Pattern API写你的第一个CEP程序了。
DataStream中的事件，如果你想在上面进行模式匹配的话，必须实现合适的 equals()和hashCode()方法， 因为FlinkCEP使用它们来比较和匹配事件。 Java DataStream&amp;lt;Event&amp;gt; input = ...; Pattern&amp;lt;Event, ?&amp;gt; pattern = Pattern.&amp;lt;Event&amp;gt;begin(&amp;#34;start&amp;#34;).where( new SimpleCondition&amp;lt;Event&amp;gt;() { @Override public boolean filter(Event event) { return event.getId() == 42; } } ).next(&amp;#34;middle&amp;#34;).subtype(SubEvent.class).where( new SimpleCondition&amp;lt;SubEvent&amp;gt;() { @Override public boolean filter(SubEvent subEvent) { return subEvent.</description>
    </item>
    
    <item>
      <title>有状态流处理</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/concepts/stateful-stream-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/concepts/stateful-stream-processing/</guid>
      <description>有状态流处理 # What is State? # While many operations in a dataflow simply look at one individual event at a time (for example an event parser), some operations remember information across multiple events (for example window operators). These operations are called stateful.
Some examples of stateful operations:
When an application searches for certain event patterns, the state will store the sequence of events encountered so far. When aggregating events per minute/hour/day, the state holds the pending aggregates.</description>
    </item>
    
    <item>
      <title>执行模式（流/批）</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution_mode/</guid>
      <description>执行模式（流/批） # DataStream API 支持不同的运行时执行模式，你可以根据你的用例需要和作业特点进行选择。
DataStream API 有一种”经典“的执行行为，我们称之为流（STREAMING）执行模式。这种模式适用于需要连续增量处理，而且预计无限期保持在线的无边界作业。
此外，还有一种批式执行模式，我们称之为批（BATCH）执行模式。这种执行作业的方式更容易让人联想到批处理框架，比如 MapReduce。这种执行模式适用于有一个已知的固定输入，而且不会连续运行的有边界作业。
Apache Flink 对流处理和批处理统一方法，意味着无论配置何种执行模式，在有界输入上执行的 DataStream 应用都会产生相同的最终 结果。重要的是要注意最终 在这里是什么意思：一个在流模式执行的作业可能会产生增量更新（想想数据库中的插入（upsert）操作），而批作业只在最后产生一个最终结果。尽管计算方法不同，只要呈现方式得当，最终结果会是相同的。
通过启用批执行，我们允许 Flink 应用只有在我们知道输入是有边界的时侯才会使用到的额外的优化。例如，可以使用不同的关联（join）/ 聚合（aggregation）策略，允许实现更高效的任务调度和故障恢复行为的不同 shuffle。下面我们将介绍一些执行行为的细节。
什么时候可以/应该使用批执行模式？ # 批执行模式只能用于 有边界 的作业/Flink 程序。边界是数据源的一个属性，告诉我们在执行前，来自该数据源的所有输入是否都是已知的，或者是否会有新的数据出现，可能是无限的。而对一个作业来说，如果它的所有源都是有边界的，则它就是有边界的，否则就是无边界的。
而流执行模式，既可用于有边界任务，也可用于无边界任务。
一般来说，在你的程序是有边界的时候，你应该使用批执行模式，因为这样做会更高效。当你的程序是无边界的时候，你必须使用流执行模式，因为只有这种模式足够通用，能够处理连续的数据流。
一个明显的例外是当你想使用一个有边界作业去自展一些作业状态，并将状态使用在之后的无边界作业的时候。例如，通过流模式运行一个有边界作业，取一个 savepoint，然后在一个无边界作业上恢复这个 savepoint。这是一个非常特殊的用例，当我们允许将 savepoint 作为批执行作业的附加输出时，这个用例可能很快就会过时。
另一个你可能会使用流模式运行有边界作业的情况是当你为最终会在无边界数据源写测试代码的时候。对于测试来说，在这些情况下使用有边界数据源可能更自然。
配置批执行模式 # 执行模式可以通过 execute.runtime-mode 设置来配置。有三种可选的值：
STREAMING: 经典 DataStream 执行模式（默认) BATCH: 在 DataStream API 上进行批量式执行 AUTOMATIC: 让系统根据数据源的边界性来决定 这可以通过 bin/flink run ... 的命令行参数进行配置，或者在创建/配置 StreamExecutionEnvironment 时写进程序。
下面是如何通过命令行配置执行模式：
$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar 这个例子展示了如何在代码中配置执行模式：
StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Broadcast State 模式</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/broadcast_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/broadcast_state/</guid>
      <description>Broadcast State 模式 # 你将在本节中了解到如何实际使用 broadcast state。想了解更多有状态流处理的概念，请参考 Stateful Stream Processing。
提供的 API # 在这里我们使用一个例子来展现 broadcast state 提供的接口。假设存在一个序列，序列中的元素是具有不同颜色与形状的图形，我们希望在序列里相同颜色的图形中寻找满足一定顺序模式的图形对（比如在红色的图形里，有一个长方形跟着一个三角形）。 同时，我们希望寻找的模式也会随着时间而改变。
在这个例子中，我们定义两个流，一个流包含图形（Item），具有颜色和形状两个属性。另一个流包含特定的规则（Rule），代表希望寻找的模式。
在图形流中，我们需要首先使用颜色将流进行进行分区（keyBy），这能确保相同颜色的图形会流转到相同的物理机上。
Java // 将图形使用颜色进行划分 KeyedStream&amp;lt;Item, Color&amp;gt; colorPartitionedStream = itemStream .keyBy(new KeySelector&amp;lt;Item, Color&amp;gt;(){...}); Python # 将图形使用颜色进行划分 color_partitioned_stream = item_stream.key_by(lambda item: ...) 对于规则流，它应该被广播到所有的下游 task 中，下游 task 应当存储这些规则并根据它寻找满足规则的图形对。下面这段代码会完成： i) 将规则广播给所有下游 task； ii) 使用 MapStateDescriptor 来描述并创建 broadcast state 在下游的存储结构
Java // 一个 map descriptor，它描述了用于存储规则名称与规则本身的 map 存储结构 MapStateDescriptor&amp;lt;String, Rule&amp;gt; ruleStateDescriptor = new MapStateDescriptor&amp;lt;&amp;gt;( &amp;#34;RulesBroadcastState&amp;#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.</description>
    </item>
    
    <item>
      <title>DataStream API Integration</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/data_stream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/data_stream_api/</guid>
      <description>DataStream API Integration # Both Table API and DataStream API are equally important when it comes to defining a data processing pipeline.
The DataStream API offers the primitives of stream processing (namely time, state, and dataflow management) in a relatively low-level imperative programming API. The Table API abstracts away many internals and provides a structured and declarative API.
Both APIs can work with bounded and unbounded streams.
Bounded streams need to be managed when processing historical data.</description>
    </item>
    
    <item>
      <title>DataStream API 简介</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/datastream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/datastream_api/</guid>
      <description>DataStream API 简介 # 该练习的重点是充分全面地了解 DataStream API，以便于编写流式应用入门。
什么能被转化成流？ # Flink 的 Java 和 Scala DataStream API 可以将任何可序列化的对象转化为流。Flink 自带的序列化器有
基本类型，即 String、Long、Integer、Boolean、Array 复合类型：Tuples、POJOs 和 Scala case classes 而且 Flink 会交给 Kryo 序列化其他类型。也可以将其他序列化器和 Flink 一起使用。特别是有良好支持的 Avro。
Java tuples 和 POJOs # Flink 的原生序列化器可以高效地操作 tuples 和 POJOs
Tuples # 对于 Java，Flink 自带有 Tuple0 到 Tuple25 类型。
Tuple2&amp;lt;String, Integer&amp;gt; person = Tuple2.of(&amp;#34;Fred&amp;#34;, 35); // zero based index! String name = person.f0; Integer age = person.</description>
    </item>
    
    <item>
      <title>Google Cloud Storage</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/gcs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/gcs/</guid>
      <description>Google Cloud Storage # Google Cloud storage (GCS) provides cloud storage for a variety of use cases. You can use it for reading and writing data, and for checkpoint storage when using FileSystemCheckpointStorage) with the streaming state backends.
You can use GCS objects like regular files by specifying paths in the following format:
gs://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt; The endpoint can either be a single file or a directory, for example:
// Read from GSC bucket env.</description>
    </item>
    
    <item>
      <title>History Server</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/historyserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/historyserver/</guid>
      <description>History Server # Flink 提供了 history server，可以在相应的 Flink 集群关闭之后查询已完成作业的统计信息。
此外，它暴露了一套 REST API，该 API 接受 HTTP 请求并返回 JSON 格式的数据。
概览 # HistoryServer 允许查询 JobManager 存档的已完成作业的状态和统计信息。
在配置 HistoryServer 和 JobManager 之后，你可以使用相应的脚本来启动和停止 HistoryServer：
# 启动或者停止 HistoryServer bin/historyserver.sh (start|start-foreground|stop) 默认情况下，此服务器绑定到 localhost 的 8082 端口。
目前，只能将 HistoryServer 作为独立的进程运行。
配置参数 # 配置项 jobmanager.archive.fs.dir 和 historyserver.archive.fs.refresh-interval 需要根据 作业存档目录 和 刷新作业存档目录的时间间隔 进行调整。
JobManager
已完成作业的存档在 JobManager 上进行，将已存档的作业信息上传到文件系统目录中。你可以在 flink-conf.yaml 文件中通过 jobmanager.archive.fs.dir 设置一个目录存档已完成的作业。
# 上传已完成作业信息的目录 jobmanager.archive.fs.dir: hdfs:///completed-jobs HistoryServer
可以通过 historyserver.archive.fs.dir 设置 HistoryServer 监视以逗号分隔的目录列表。定期轮询已配置的目录以查找新的存档；轮询间隔可以通过 historyserver.</description>
    </item>
    
    <item>
      <title>Hive 方言</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_dialect/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_dialect/</guid>
      <description>Hive 方言 # 从 1.11.0 开始，在使用 Hive 方言时，Flink 允许用户用 Hive 语法来编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在改善与 Hive 的互操作性，并减少用户需要在 Flink 和 Hive 之间切换来执行不同语句的情况。
使用 Hive 方言 # Flink 目前支持两种 SQL 方言: default 和 hive。你需要先切换到 Hive 方言，然后才能使用 Hive 语法编写。下面介绍如何使用 SQL 客户端和 Table API 设置方言。 还要注意，你可以为执行的每个语句动态切换方言。无需重新启动会话即可使用其他方言。
SQL 客户端 # SQL 方言可以通过 table.sql-dialect 属性指定。因此你可以通过 SQL 客户端 yaml 文件中的 configuration 部分来设置初始方言。
execution: type: batch result-mode: table configuration: table.sql-dialect: hive 你同样可以在 SQL 客户端启动后设置方言。
Flink SQL&amp;gt; set table.</description>
    </item>
    
    <item>
      <title>Iterative Graph Processing</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/iterative_graph_processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/iterative_graph_processing/</guid>
      <description>Iterative Graph Processing # Gelly exploits Flink&amp;rsquo;s efficient iteration operators to support large-scale iterative graph processing. Currently, we provide implementations of the vertex-centric, scatter-gather, and gather-sum-apply models. In the following sections, we describe these abstractions and show how you can use them in Gelly.
Vertex-Centric Iterations # The vertex-centric model, also known as &amp;ldquo;think like a vertex&amp;rdquo; or &amp;ldquo;Pregel&amp;rdquo;, expresses computation from the perspective of a vertex in the graph.</description>
    </item>
    
    <item>
      <title>Joining</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/joining/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/joining/</guid>
      <description>Joining # Window Join # Window join 作用在两个流中有相同 key 且处于相同窗口的元素上。这些窗口可以通过 window assigner 定义，并且两个流中的元素都会被用于计算窗口的结果。
两个流中的元素在组合之后，会被传递给用户定义的 JoinFunction 或 FlatJoinFunction，用户可以用它们输出符合 join 要求的结果。
常见的用例可以总结为以下代码：
stream.join(otherStream) .where(&amp;lt;KeySelector&amp;gt;) .equalTo(&amp;lt;KeySelector&amp;gt;) .window(&amp;lt;WindowAssigner&amp;gt;) .apply(&amp;lt;JoinFunction&amp;gt;); 语义上有一些值得注意的地方：
从两个流中创建成对的元素与 inner-join 类似，即一个流中的元素在与另一个流中对应的元素完成 join 之前不会被输出。 完成 join 的元素会将他们的 timestamp 设为对应窗口中允许的最大 timestamp。比如一个边界为 [5, 10) 窗口中的元素在 join 之后的 timestamp 为 9。 接下来我们会用例子说明各种 window join 如何运作。
滚动 Window Join # 使用滚动 window join 时，所有 key 相同且共享一个滚动窗口的元素会被组合成对，并传递给 JoinFunction 或 FlatJoinFunction。因为这个行为与 inner join 类似，所以一个流中的元素如果没有与另一个流中的元素组合起来，它就不会被输出！
如图所示，我们定义了一个大小为 2 毫秒的滚动窗口，即形成了边界为 [0,1], [2,3], .</description>
    </item>
    
    <item>
      <title>JSON</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/json/</guid>
      <description>JSON Format # Format: Serialization Schema Format: Deserialization Schema
JSON Format 能读写 JSON 格式的数据。当前，JSON schema 是从 table schema 中自动推导而得的。
依赖 # In order to use the Json format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-json&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Built-in 如何创建一张基于 JSON Format 的表 # 以下是一个利用 Kafka 以及 JSON Format 构建表的例子。</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kafka/</guid>
      <description>Apache Kafka 连接器 # Flink 提供了 Apache Kafka 连接器使用精确一次（Exactly-once）的语义在 Kafka topic 中读取和写入数据。
依赖 # Apache Flink 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。 该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。 当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。 有关 Kafka 兼容性的更多细节，请参考 Kafka 官方文档。
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kafka&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 如果使用 Kafka source，flink-connector-base 也需要包含在依赖中：
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-base&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Flink 目前的流连接器还不是二进制发行版的一部分。 在此处可以了解到如何链接它们，从而在集群中运行。
为了在 PyFlink 作业中使用 Kafka connector ，需要添加下列依赖： Kafka version PyFlink JAR universal Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/kafka/</guid>
      <description>Apache Kafka SQL 连接器 # Scan Source: Unbounded Sink: Streaming Append Mode
Kafka 连接器提供从 Kafka topic 中消费和写入数据的能力。
依赖 # In order to use the Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Kafka version Maven dependency SQL Client JAR universal &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kafka&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Kerberos</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/security/security-kerberos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/security/security-kerberos/</guid>
      <description>Kerberos 身份认证设置和配置 # 本文简要描述了 Flink 如何在各种部署机制（Standalone, native Kubernetes, YARN）、文件系统、connector 以及 state backend 的上下文中安全工作。
目标 # Flink Kerberos 安全框架的主要目标如下：
在集群内使用 connector（例如 Kafka）时确保作业安全地访问数据； 对 zookeeper 进行身份认证（如果配置了 SASL）； 对 Hadoop 组件进行身份认证（例如 HDFS，HBASE）。 生产部署场景中，流式作业通常会运行很长一段时间（天、周、月级别的时间段），并且需要在作业的整个生命周期中对其进行身份认证以保护数据源。与 Hadoop delegation token 和 ticket 缓存项不同，Kerberos keytab 不会在该时间段内过期。
当前的实现支持使用可配置的 keytab credential 或 Hadoop delegation token 来运行 Flink 集群（JobManager / TaskManager / 作业）。
请注意，所有作业都能共享为指定集群配置的凭据。如果想为一个作业使用不同的 keytab，只需单独启动一个具有不同配置的 Flink 集群。多个 Flink 集群可以在 Kubernetes 或 YARN 环境中并行运行。
Flink Security 如何工作 # 理论上，Flink 程序可以使用自己的或第三方的 connector（Kafka、HDFS、Cassandra、Flume、Kinesis 等），同时需要支持任意的认证方式（Kerberos、SSL/TLS、用户名/密码等）。满足所有 connector 的安全需求还在进行中，不过 Flink 提供了针对 Kerberos 身份认证的一流支持。Kerberos 身份认证支持以下服务和 connector：</description>
    </item>
    
    <item>
      <title>Kubernetes 高可用服务</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/kubernetes_ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/ha/kubernetes_ha/</guid>
      <description>Kubernetes 高可用服务 # Flink 的 Kubernetes 高可用模式使用 Kubernetes 提供高可用服务。
Kubernetes 高可用服务只能在部署到 Kubernetes 时使用。因此，当使用 在 Kubernetes 上单节点部署 Flink 或 Flink 原生 Kubernetes 集成 两种模式时，可以对它们进行配置。
准备 # 为了使用 Flink 的 Kubernetes 高可用服务，你必须满足以下先决条件:
Kubernetes &amp;gt;= 1.9. 具有创建、编辑、删除 ConfigMaps 权限的服务帐户。想了解更多信息，请查看如何在 Flink 原生 Kubernetes 集成 和 在 Kubernetes 上单节点部署 Flink 两种模式中配置服务帐户。 配置 # 为了启用高可用集群（HA-cluster），你必须设置以下配置项:
high-availability (必要的): high-availability 选项必须设置为 KubernetesHaServicesFactory. high-availability: kubernetes high-availability.storageDir (必要的): JobManager 元数据持久化到文件系统 high-availability.storageDir 配置的路径中，并且在 Kubernetes 中只能有一个目录指向此位置。 high-availability.storageDir: s3://flink/recovery storageDir 存储要从 JobManager 失败恢复时所需的所有元数据。</description>
    </item>
    
    <item>
      <title>Native Kubernetes</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/native_kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/native_kubernetes/</guid>
      <description>Native Kubernetes # This page describes how to deploy Flink natively on Kubernetes.
Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on Kubernetes.
Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&amp;rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.</description>
    </item>
    
    <item>
      <title>State Processor API</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/state_processor_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/state_processor_api/</guid>
      <description>State Processor API # Apache Flink&amp;rsquo;s State Processor API provides powerful functionality to reading, writing, and modifying savepoints and checkpoints using Flink’s DataStream API under BATCH execution. Due to the interoperability of DataStream and Table API, you can even use relational Table API or SQL queries to analyze and process state data.
For example, you can take a savepoint of a running stream processing application and analyze it with a DataStream batch program to verify that the application behaves correctly.</description>
    </item>
    
    <item>
      <title>WITH 子句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/with/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/with/</guid>
      <description>WITH 子句 # Batch Streaming
WITH 子句提供了一种用于更大查询而编写辅助语句的方法。这些编写的语句通常被称为公用表表达式，表达式可以理解为仅针对某个查询而存在的临时视图。
WITH 子句的语法
WITH &amp;lt;with_item_definition&amp;gt; [ , ... ] SELECT ... FROM ...; &amp;lt;with_item_defintion&amp;gt;: with_item_name (column_name[, ...n]) AS ( &amp;lt;select_query&amp;gt; ) 下面的示例中定义了一个公用表表达式 orders_with_total ，并在一个 GROUP BY 查询中使用它。
WITH orders_with_total AS ( SELECT order_id, price + tax AS total FROM Orders ) SELECT order_id, SUM(total) FROM orders_with_total GROUP BY order_id; Back to top</description>
    </item>
    
    <item>
      <title>Working Directory</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/working_directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/working_directory/</guid>
      <description>Working Directory # Flink supports to configure a working directory (FLIP-198) for Flink processes (JobManager and TaskManager). The working directory is used by the processes to store information that can be recovered upon a process restart. The requirement for this to work is that the process is started with the same identity and has access to the volume on which the working directory is stored.
Configuring the Working Directory # The working directories for the Flink processes are:</description>
    </item>
    
    <item>
      <title>Zipping Elements</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/zip_elements_guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/zip_elements_guide/</guid>
      <description>给 DataSet 中的元素编号 # 在一些算法中，可能需要为数据集元素分配唯一标识符。 本文档阐述了如何将 DataSetUtils 用于此目的。
以密集索引编号 # zipWithIndex 为元素分配连续的标签，接收数据集作为输入并返回一个新的 (unique id, initial value) 二元组的数据集。 这个过程需要分为两个（子）过程，首先是计数，然后是标记元素，由于计数操作的同步性，这个过程不能被 pipelined（流水线化）。
可供备选的 zipWithUniqueId 是以 pipelined 的方式进行工作的。当唯一标签足够时，首选 zipWithUniqueId 。 例如，下面的代码：
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataSet&amp;lt;String&amp;gt; in = env.fromElements(&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;, &amp;#34;C&amp;#34;, &amp;#34;D&amp;#34;, &amp;#34;E&amp;#34;, &amp;#34;F&amp;#34;, &amp;#34;G&amp;#34;, &amp;#34;H&amp;#34;); DataSet&amp;lt;Tuple2&amp;lt;Long, String&amp;gt;&amp;gt; result = DataSetUtils.zipWithIndex(in); result.writeAsCsv(resultPath, &amp;#34;\n&amp;#34;, &amp;#34;,&amp;#34;); env.execute(); Scala import org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(&amp;#34;A&amp;#34;, &amp;#34;B&amp;#34;, &amp;#34;C&amp;#34;, &amp;#34;D&amp;#34;, &amp;#34;E&amp;#34;, &amp;#34;F&amp;#34;, &amp;#34;G&amp;#34;, &amp;#34;H&amp;#34;) val result: DataSet[(Long, String)] = input.</description>
    </item>
    
    <item>
      <title>阿里云 OSS</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/oss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/oss/</guid>
      <description>阿里云对象存储服务 (OSS) # OSS：对象存储服务 # 阿里云对象存储服务 (Aliyun OSS) 使用广泛，尤其在中国云用户中十分流行，能提供多种应用场景下的云对象存储。OSS 可与 Flink 一起使用以读取与存储数据，以及与流 State Backend 结合使用。
通过以下格式指定路径，OSS 对象可类似于普通文件使用：
oss://&amp;lt;your-bucket&amp;gt;/&amp;lt;object-name&amp;gt; 以下代码展示了如何在 Flink 作业中使用 OSS：
// 读取 OSS bucket env.readTextFile(&amp;#34;oss://&amp;lt;your-bucket&amp;gt;/&amp;lt;object-name&amp;gt;&amp;#34;); // 写入 OSS bucket stream.writeAsText(&amp;#34;oss://&amp;lt;your-bucket&amp;gt;/&amp;lt;object-name&amp;gt;&amp;#34;); // 将 OSS 用作 FsStatebackend env.setStateBackend(new FsStateBackend(&amp;#34;oss://&amp;lt;your-bucket&amp;gt;/&amp;lt;object-name&amp;gt;&amp;#34;)); Shaded Hadoop OSS 文件系统 # 为使用 flink-oss-fs-hadoop，在启动 Flink 之前，将对应的 JAR 文件从 opt 目录复制到 Flink 发行版中的 plugin 目录下的一个文件夹中，例如：
mkdir ./plugins/oss-fs-hadoop cp ./opt/flink-oss-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/oss-fs-hadoop/ flink-oss-fs-hadoop 为使用 oss:// scheme 的 URI 注册了默认的文件系统包装器。
配置设置 # 在设置好 OSS 文件系统包装器之后，需要添加一些配置以保证 Flink 有权限访问 OSS buckets。</description>
    </item>
    
    <item>
      <title>调试类加载</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/debugging_classloading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/debugging_classloading/</guid>
      <description>调试类加载 # Flink中的类加载概述 # Flink应用程序运行时，JVM会随着时间不断加载各种不同的类。 根据起源不同这些类可以分为三组类型：
Java Classpath: Java共有的classpath类加载路径，包括JDK库和Flink的/lib目录（Apache Flink及相关依赖的类）中的代码。它们通过AppClassLoader进行加载。
Flink插件类组件：存放于Flink的 /plugins 目录中的插件代码。Flink的插件机制确保在启动时对它们进行动态加载。
动态用户代码：动态提交job（通过REST、命令行或者web UI方式）时存在JAR文件中的类。job运行时它们通过FlinkUserCodeClassLoader进行动态加载或卸载。
作为通用规则，每当Flink进程先启动，之后job提交时，job相关的类都是动态加载的。 如果Flink进程与job或应用程序一起启动，或者应用代码启动Flink组件（JobManager, TaskManager等），这时所有job的类存在于Java的classpath下。
每个插件中的组件代码会由一个专用的类加载器进行动态加载。
下面是不同部署模式的一些细节信息：
Session模式(Standalone/Yarn/Kubernetes)
当Flink Session集群启动时，JobManager和TaskManager由Java classpath中的Flink框架类（Flink framework classes）进行启动加载。而通过session提交（REST或命令行方式）的job或应用程序由FlinkUserCodeClassLoader进行加载。
Per-Job模式（已弃用）（Yarn）
当前只有Yarn支持Per-Job模式。默认情况下，Flink集群运行在Per-Job模式下时会将用户的jar文件包含在系统的classpath中。 这种模式可以由yarn.classpath.include-user-jar 参数控制。 当该参数设定为DISABLED时，Flink会将用户jar文件含在用户的classpath中，并由FlinkUserCodeClassLoader进行动态加载。
详细信息参见Flink on Yarn。
Application模式（Standalone/Yarn/Kubernetes）
当Application模式的Flink集群基于Standalone或Kubernetes方式运行时，用户jar文件（启动命令指定的jar文件和Flink的usrlib目录中的jar包）会由FlinkUserCodeClassLoader进行动态加载。
当Flink集群以Application模式运行时，用户jar文件（启动命令指定的jar文件和Flink的usrlib目录中的jar包）默认情况下会包含在系统classpath（AppClassLoader）。与Per-Job模式相同，当yarn.classpath.include-user-jar设置为DISABLED时，Flink会将用户jar文件含在用户的classpath中，并由FlinkUserCodeClassLoader进行动态加载。
倒置类加载（Inverted Class Loading）和ClassLoader解析顺序 # 涉及到动态类加载的层次结构涉及两种ClassLoader： （1）Java的application classloader，包含classpath中的所有类； （2）动态的plugin/user code classloader，用来加载插件代码或用户代码的jar文件。动态的ClassLoader将应用程序classloader作为parent。
默认情况下Flink会倒置类加载顺序，首先Flink会查找动态类加载器，如果该类不属于动态加载的代码时才会去查找其parent（application classloader）。
倒置类加载的好处在于插件和job可以使用与Flink核心不同的库版本，尤其在使用不同版本的库从而出现不兼容的情况下。这种机制可以帮助避免常见的类似 IllegalAccessError 或NoSuchMethodError的依赖冲突错误。代码的不同部分会有独立的拷贝（Flink内核及它的不同依赖包可使用与用户代码或插件代码不同的拷贝），多数情况下这种方式可以正常运行，并且不需要用户进行额外配置。
然而有些情况下，倒置类加载可能会引起一些问题，参见下面的&amp;ldquo;X cannot be cast to X&amp;rdquo;。
对于用户代码的类加载，您可以通过调整Flink的classloader.resolve-order配置将ClassLoader解析顺序还原至Java的默认模式（从Flink默认的child-first调整为parent-first）。
请注意由于有些类在Flink内核与插件或用户代码间共享，它们总是以parent-first方式进行解析的。这些类相关的包通过classloader.parent-first-patterns-default和classloader.parent-first-patterns-additional进行配置。如果需要新添加parent-first 方式的包，请调整classloader.parent-first-patterns-additional 配置选项。
避免用户代码的动态类加载 # Flink的组件（JobManager, TaskManager, Client, ApplicationMaster等）在启动时会在日志开头的环境信息部分记录classpath的设定。
当JobManager和TaskManager的运行模式为指定一个job时，可以通过将用户代码的JAR文件放置在/lib目录下，从而包含在classpath路径中，以保证它们不会被动态加载。</description>
    </item>
    
    <item>
      <title>迭代</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/iterations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/iterations/</guid>
      <description>迭代 # Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. Such algorithms are crucial in order to realize the promise of Big Data to extract meaningful information out of your data. With increasing interest to run these kinds of algorithms on very large data sets, there is a need to execute iterations in a massively parallel fashion.
Flink programs implement iterative algorithms by defining a step function and embedding it into a special iteration operator.</description>
    </item>
    
    <item>
      <title>火焰图</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/flame_graphs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/flame_graphs/</guid>
      <description>Flame Graphs # Flame Graphs are a visualization that effectively surfaces answers to questions like:
Which methods are currently consuming CPU resources? How does consumption by one method compare to the others? Which series of calls on the stack led to executing a particular method? Flame Graph Flame Graphs are constructed by sampling stack traces a number of times. Each method call is presented by a bar, where the length of the bar is proportional to the number of times it is present in the samples.</description>
    </item>
    
    <item>
      <title>基于 DataStream API 实现欺诈检测</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/try-flink/datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/try-flink/datastream/</guid>
      <description>基于 DataStream API 实现欺诈检测 # Apache Flink 提供了 DataStream API 来实现稳定可靠的、有状态的流处理应用程序。 Flink 支持对状态和时间的细粒度控制，以此来实现复杂的事件驱动数据处理系统。 这个入门指导手册讲述了如何通过 Flink DataStream API 来实现一个有状态流处理程序。
你要搭建一个什么系统 # 在当今数字时代，信用卡欺诈行为越来越被重视。 罪犯可以通过诈骗或者入侵安全级别较低系统来盗窃信用卡卡号。 用盗得的信用卡进行很小额度的例如一美元或者更小额度的消费进行测试。 如果测试消费成功，那么他们就会用这个信用卡进行大笔消费，来购买一些他们希望得到的，或者可以倒卖的财物。
在这个教程中，你将会建立一个针对可疑信用卡交易行为的反欺诈检测系统。 通过使用一组简单的规则，你将了解到 Flink 如何为我们实现复杂业务逻辑并实时执行。
准备条件 # 这个代码练习假定你对 Java 或 Scala 有一定的了解，当然，如果你之前使用的是其他开发语言，你也应该能够跟随本教程进行学习。
在 IDE 中运行 # 在 IDE 中运行该项目可能会遇到 java.langNoClassDefFoundError 的异常。这很可能是因为运行所需要的 Flink 的依赖库没有默认被全部加载到类路径（classpath）里。
IntelliJ IDE：前往 运行 &amp;gt; 编辑配置 &amp;gt; 修改选项 &amp;gt; 选中 将带有 &amp;ldquo;provided&amp;rdquo; 范围的依赖项添加到类路径。这样的话，运行配置将会包含所有在 IDE 中运行所必须的类。
困难求助 # 如果遇到困难，可以参考 社区支持资源。 当然也可以在邮件列表提问，Flink 的 用户邮件列表 一直被评为所有Apache项目中最活跃的一个，这也是快速获得帮助的好方法。
怎样跟着教程练习 # 首先，你需要在你的电脑上准备以下环境：</description>
    </item>
    
    <item>
      <title>及时流处理</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/concepts/time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/concepts/time/</guid>
      <description>及时流处理 # Introduction # Timely stream processing is an extension of stateful stream processing in which time plays some role in the computation. Among other things, this is the case when you do time series analysis, when doing aggregations based on certain time periods (typically called windows), or when you do event processing where the time when an event occurred is important.
In the following sections we will highlight some of the topics that you should consider when working with timely Flink Applications.</description>
    </item>
    
    <item>
      <title>监控反压</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/monitoring/back_pressure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/monitoring/back_pressure/</guid>
      <description>监控反压 # Flink Web 界面提供了一个选项卡来监控正在运行 jobs 的反压行为。
反压 # 如果你看到一个 task 发生 反压警告（例如： High），意味着它生产数据的速率比下游 task 消费数据的速率要快。 在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。
以一个简单的 Source -&amp;gt; Sink job 为例。如果看到 Source 发生了警告，意味着 Sink 消费数据的速率比 Source 生产数据的速率要慢。 Sink 正在向上游的 Source 算子产生反压。
Task 性能指标 # Task（SubTask）的每个并行实例都可以用三个一组的指标评价：
backPressureTimeMsPerSecond，subtask 被反压的时间 idleTimeMsPerSecond，subtask 等待某类处理的时间 busyTimeMsPerSecond，subtask 实际工作时间 在任何时间点，这三个指标相加都约等于1000ms。 这些指标每两秒更新一次，上报的值表示 subtask 在最近两秒被反压（或闲或忙）的平均时长。 当你的工作负荷是变化的时需要尤其引起注意。比如，一个以恒定50%负载工作的 subtask 和另一个每秒钟在满负载和闲置切换的 subtask 的busyTimeMsPerSecond值相同，都是500ms。
在内部，反压根据输出 buffers 的可用性来进行判断的。 如果一个 task 没有可用的输出 buffers，那么这个 task 就被认定是在被反压。 相反，如果有可用的输入，则可认定为闲置，
示例 # WebUI 集合了所有 subTasks 的反压和繁忙指标的最大值，并在 JobGraph 中将集合的值进行显示。除了显示原始的数值，tasks 也用颜色进行了标记，使检查更加容易。</description>
    </item>
    
    <item>
      <title>内置 Watermark 生成器</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/event-time/built_in/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/event-time/built_in/</guid>
      <description>内置 Watermark 生成器 # 如生成 Watermark 小节中所述，Flink 提供的抽象方法可以允许用户自己去定义时间戳分配方式和 watermark 生成的方式。你可以通过实现 WatermarkGenerator 接口来实现上述功能。
为了进一步简化此类任务的编程工作，Flink 框架预设了一些时间戳分配器。本节后续内容有举例。除了开箱即用的已有实现外，其还可以作为自定义实现的示例以供参考。
单调递增时间戳分配器 # 周期性 watermark 生成方式的一个最简单特例就是你给定的数据源中数据的时间戳升序出现。在这种情况下，当前时间戳就可以充当 watermark，因为后续到达数据的时间戳不会比当前的小。
注意：在 Flink 应用程序中，如果是并行数据源，则只要求并行数据源中的每个单分区数据源任务时间戳递增。例如，设置每一个并行数据源实例都只读取一个 Kafka 分区，则时间戳只需在每个 Kafka 分区内递增即可。Flink 的 watermark 合并机制会在并行数据流进行分发（shuffle）、联合（union）、连接（connect）或合并（merge）时生成正确的 watermark。
Java WatermarkStrategy.forMonotonousTimestamps(); Scala WatermarkStrategy.forMonotonousTimestamps() Python WatermarkStrategy.for_monotonous_timestamps() 数据之间存在最大固定延迟的时间戳分配器 # 另一个周期性 watermark 生成的典型例子是，watermark 滞后于数据流中最大（事件时间）时间戳一个固定的时间量。该示例可以覆盖的场景是你预先知道数据流中的数据可能遇到的最大延迟，例如，在测试场景下创建了一个自定义数据源，并且这个数据源的产生的数据的时间戳在一个固定范围之内。Flink 针对上述场景提供了 boundedOutfordernessWatermarks 生成器，该生成器将 maxOutOfOrderness 作为参数，该参数代表在计算给定窗口的结果时，允许元素被忽略计算之前延迟到达的最长时间。其中延迟时长就等于 t - t_w ，其中 t 代表元素的（事件时间）时间戳，t_w 代表前一个 watermark 对应的（事件时间）时间戳。如果 lateness &amp;gt; 0，则认为该元素迟到了，并且在计算相应窗口的结果时默认会被忽略。有关使用延迟元素的详细内容，请参阅有关允许延迟的文档。
Java WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); Scala WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(10)) Python WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(10)) Back to top</description>
    </item>
    
    <item>
      <title>配置 TaskManager 内存</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_tm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_tm/</guid>
      <description>配置 TaskManager 内存 # Flink 的 TaskManager 负责执行用户代码。 根据实际需求为 TaskManager 配置内存将有助于减少 Flink 的资源占用，增强作业运行的稳定性。
本文接下来介绍的内存配置方法适用于 1.10 及以上版本。 Flink 在 1.10 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
提示 本篇内存配置文档仅针对 TaskManager！ 与 JobManager 相比，TaskManager 具有相似但更加复杂的内存模型。
配置总内存 # Flink JVM 进程的*进程总内存（Total Process Memory）*包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 其中，*Flink 总内存（Total Flink Memory）*包括 JVM 堆内存（Heap Memory）、*托管内存（Managed Memory）*以及其他直接内存（Direct Memory）或本地内存（Native Memory）。
如果你是在本地运行 Flink（例如在 IDE 中）而非创建一个集群，那么本文介绍的配置并非所有都是适用的，详情请参考本地执行。
其他情况下，配置 Flink 内存最简单的方法就是配置总内存。 此外，Flink 也支持更细粒度的内存配置方式。
Flink 会根据默认值或其他配置参数自动调整剩余内存部分的大小。 接下来的章节将介绍关于各内存部分的更多细节。
配置堆内存和托管内存 # 如配置总内存中所述，另一种配置 Flink 内存的方式是同时设置任务堆内存和托管内存。 通过这种方式，用户可以更好地掌控用于 Flink 任务的 JVM 堆内存及 Flink 的托管内存大小。</description>
    </item>
    
    <item>
      <title>配置参数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/config/</guid>
      <description>配置参数 # All configuration is done in conf/flink-conf.yaml, which is expected to be a flat collection of YAML key value pairs with format key: value.
The configuration is parsed and evaluated when the Flink processes are started. Changes to the configuration file require restarting the relevant processes.
The out of the box configuration will use your default Java installation. You can manually set the environment variable JAVA_HOME or the configuration key env.</description>
    </item>
    
    <item>
      <title>时间属性</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/time_attributes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/time_attributes/</guid>
      <description>时间属性 # Flink 可以基于几种不同的 时间 概念来处理数据。
处理时间 指的是执行具体操作时的机器时间（大家熟知的绝对时间, 例如 Java的 System.currentTimeMillis()) ） 事件时间 指的是数据本身携带的时间。这个时间是在事件产生时的时间。 摄入时间 指的是数据进入 Flink 的时间；在系统内部，会把它当做事件时间来处理。 对于时间相关的更多信息，可以参考 事件时间和Watermark。
本页面说明了如何在 Flink Table API &amp;amp; SQL 里面定义时间以及相关的操作。
时间属性介绍 # 像窗口（在 Table API 和 SQL ）这种基于时间的操作，需要有时间信息。因此，Table API 中的表就需要提供逻辑时间属性来表示时间，以及支持时间相关的操作。
每种类型的表都可以有时间属性，可以在用CREATE TABLE DDL创建表的时候指定、也可以在 DataStream 中指定、也可以在定义 TableSource 时指定。一旦时间属性定义好，它就可以像普通列一样使用，也可以在时间相关的操作中使用。
只要时间属性没有被修改，而是简单地从一个表传递到另一个表，它就仍然是一个有效的时间属性。时间属性可以像普通的时间戳的列一样被使用和计算。一旦时间属性被用在了计算中，它就会被物化，进而变成一个普通的时间戳。普通的时间戳是无法跟 Flink 的时间以及watermark等一起使用的，所以普通的时间戳就无法用在时间相关的操作中。
Table API 程序需要在 streaming environment 中指定时间属性：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // default // 或者: // env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Scala val env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>使用 Gradle</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/gradle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/gradle/</guid>
      <description>如何使用 Gradle 配置您的项目 # 您可能需要一个构建工具来配置您的 Flink 项目，本指南将向您展示如何使用 Gradle 执行此操作。Gradle 是一个开源的通用构建工具，可用于在开发过程中自动化执行任务。
要求 # Gradle 7.x Java 11 将项目导入 IDE # 创建项目目录和文件后，我们建议您将此项目导入到 IDE 进行开发和测试。
IntelliJ IDEA 通过 Gradle 插件支持 Gradle 项目。
Eclipse 通过 Eclipse Buildship 插件执行此操作（确保在导入向导的最后一步中指定 Gradle 版本 &amp;gt;= 3.0，shadow 插件会用到它）。您还可以使用 Gradle 的 IDE 集成 来使用 Gradle 创建项目文件。
注意： Java 的默认 JVM 堆大小对于 Flink 来说可能太小，您应该手动增加它。在 Eclipse 中，选中 Run Configurations -&amp;gt; Arguments 并在 VM Arguments 框里填上：-Xmx800m。在 IntelliJ IDEA 中，推荐选中 Help | Edit Custom VM Options 菜单修改 JVM 属性。详情请查阅本文。</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/avro/</guid>
      <description>Avro format # Flink 内置支持 Apache Avro 格式。在 Flink 中将更容易地读写基于 Avro schema 的 Avro 数据。 Flink 的序列化框架可以处理基于 Avro schemas 生成的类。为了能够使用 Avro format，需要在自动构建工具（例如 Maven 或 SBT）中添加如下依赖到项目中。
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-avro&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 为了在 PyFlink 作业中使用 Avro format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 如果读取 Avro 文件数据，你必须指定 AvroInputFormat。
示例：
AvroInputFormat&amp;lt;User&amp;gt; users = new AvroInputFormat&amp;lt;User&amp;gt;(in, User.class); DataStream&amp;lt;User&amp;gt; usersDS = env.createInput(users); 注意，User 是一个通过 Avro schema生成的 POJO 类。Flink 还允许选择 POJO 中字符串类型的键。例如：</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro/</guid>
      <description>Avro Format # Format: Serialization Schema Format: Deserialization Schema
Apache Avro format 允许基于 Avro schema 读取和写入 Avro 数据。目前，Avro schema 从 table schema 推导而来。
依赖 # In order to use the Avro format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-avro&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Azure Blob 存储</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/azure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/azure/</guid>
      <description>Azure Blob 存储 # Azure Blob 存储 是一项由 Microsoft 管理的服务，能提供多种应用场景下的云存储。 Azure Blob 存储可与 Flink 一起使用以读取和写入数据，以及与流 State Backend 结合使用。
Flink 支持使用 wasb:// 或 abfs:// 访问 Azure Blob 存储。
Azure 建议使用 abfs:// 访问 ADLS Gen2 存储帐户，尽管 wasb:// 通过向后兼容也可以工作。 abfs:// 只能用于访问 ADLS Gen2 存储帐户。 请访问Azure文档，了解如何识别 ADLS Gen2 存储帐户。 通过以下格式指定路径，Azure Blob 存储对象可类似于普通文件使用：
// WASB unencrypted access wasb://&amp;lt;your-container&amp;gt;@$&amp;lt;your-azure-account&amp;gt;.blob.core.windows.net/&amp;lt;object-path&amp;gt; // WASB SSL encrypted access wasbs://&amp;lt;your-container&amp;gt;@$&amp;lt;your-azure-account&amp;gt;.blob.core.windows.net/&amp;lt;object-path&amp;gt; // ABFS unecrypted access abfs://&amp;lt;your-container&amp;gt;@$&amp;lt;your-azure-account&amp;gt;.dfs.core.windows.net/&amp;lt;object-path&amp;gt; // ABFS SSL encrypted access abfss://&amp;lt;your-container&amp;gt;@$&amp;lt;your-azure-account&amp;gt;.</description>
    </item>
    
    <item>
      <title>Azure Table storage</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/azure_table_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/azure_table_storage/</guid>
      <description>Azure Table Storage # 本例使用 HadoopInputFormat 包装器来使用现有的 Hadoop input format 实现访问 Azure&amp;rsquo;s Table Storage.
下载并编译 azure-tables-hadoop 项目。该项目开发的 input format 在 Maven 中心尚不存在，因此，我们必须自己构建该项目。 执行如下命令： git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install 使用 quickstarts 创建一个新的 Flink 项目： curl https://flink.apache.org/q/quickstart.sh | bash 在你的 pom.xml 文件 &amp;lt;dependencies&amp;gt; 部分添加如下依赖： &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-hadoop-compatibility_2.12&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.microsoft.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;microsoft-hadoop-azure&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.0.5&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; flink-hadoop-compatibility 是一个提供 Hadoop input format 包装器的 Flink 包。 microsoft-hadoop-azure 可以将之前构建的部分添加到项目中。
现在可以开始进行项目的编码。我们建议将项目导入 IDE，例如 IntelliJ。你应该将其作为 Maven 项目导入。 跳转到文件 Job.</description>
    </item>
    
    <item>
      <title>Batch Shuffle</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/batch/batch_shuffle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/batch/batch_shuffle/</guid>
      <description>Batch Shuffle # 总览 # Flink DataStream API 和 Table / SQL 都支持通过批处理执行模式处理有界输入。 在批处理模式下，Flink 提供了两种网络交换模式: Blocking Shuffle 和 Hybrid Shuffle.
Blocking Shuffle 是批处理的默认数据交换模式。它会持久化所有的中间数据，只有当数据产出完全后才能被消费。 Hybrid Shuffle 是下一代的批处理数据交换模式. 他会更加智能地持久化数据, 并且允许在数据生产的同时进行消费. 该特性目前仍处于实验阶段并且存在一些已知的 限制. Blocking Shuffle # 与流式应用使用管道 shuffle 交换数据的方式不同，blocking 交换持久化数据到存储中，然后下游任务通过网络获取这些值。这种交换减少了执行作业所需的资源，因为它不需要同时运行上游和下游任务。
总的来说，Flink 提供了两种不同类型的 blocking shuffles：Hash shuffle 和 Sort shuffle。
Hash Shuffle # 对于 1.14 以及更低的版本，Hash Shuffle 是 blocking shuffle 的默认实现，它为每个下游任务将每个上游任务的结果以单独文件的方式保存在 TaskManager 本地磁盘上。当下游任务运行时会向上游的 TaskManager 请求分片，TaskManager 读取文件之后通过网络传输（给下游任务）。
Hash Shuffle 为读写文件提供了不同的机制:
file: 通过标准文件 IO 写文件，读取和传输文件需要通过 Netty 的 FileRegion。FileRegion 依靠系统调用 sendfile 来减少数据拷贝和内存消耗。 mmap: 通过系统调用 mmap 来读写文件。 auto: 通过标准文件 IO 写文件，对于文件读取，在 32 位机器上降级到 file 选项并且在 64 位机器上使用 mmap 。这是为了避免在 32 位机器上 java 实现 mmap 的文件大小限制。 可通过设置 TaskManager 参数 选择不同的机制。</description>
    </item>
    
    <item>
      <title>Cassandra</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/cassandra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/cassandra/</guid>
      <description>Apache Cassandra Connector # This connector provides sinks that writes data into a Apache Cassandra database.
To use this connector, add the following dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-cassandra_2.12&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here.
Installing Apache Cassandra # There are multiple ways to bring up a Cassandra instance on local machine:</description>
    </item>
    
    <item>
      <title>Checkpointing</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/checkpointing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/checkpointing/</guid>
      <description>Checkpointing # Flink 中的每个方法或算子都能够是有状态的（阅读 working with state 了解更多）。 状态化的方法在处理单个 元素/事件 的时候存储数据，让状态成为使各个类型的算子更加精细的重要部分。 为了让状态容错，Flink 需要为状态添加 checkpoint（检查点）。Checkpoint 使得 Flink 能够恢复状态和在流中的位置，从而向应用提供和无故障执行时一样的语义。
容错文档 中介绍了 Flink 流计算容错机制内部的技术原理。
前提条件 # Flink 的 checkpoint 机制会和持久化存储进行交互，读写流与状态。一般需要：
一个能够回放一段时间内数据的持久化数据源，例如持久化消息队列（例如 Apache Kafka、RabbitMQ、 Amazon Kinesis、 Google PubSub 等）或文件系统（例如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。 存放状态的持久化存储，通常为分布式文件系统（比如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。 开启与配置 Checkpoint # 默认情况下 checkpoint 是禁用的。通过调用 StreamExecutionEnvironment 的 enableCheckpointing(n) 来启用 checkpoint，里面的 n 是进行 checkpoint 的间隔，单位毫秒。
Checkpoint 其他的属性包括：
精确一次（exactly-once）对比至少一次（at-least-once）：你可以选择向 enableCheckpointing(long interval, CheckpointingMode mode) 方法中传入一个模式来选择使用两种保证等级中的哪一种。 对于大多数应用来说，精确一次是较好的选择。至少一次可能与某些延迟超低（始终只有几毫秒）的应用的关联较大。</description>
    </item>
    
    <item>
      <title>Confluent Avro</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro-confluent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro-confluent/</guid>
      <description>Confluent Avro Format # Format: Serialization Schema Format: Deserialization Schema
Avro Schema Registry (avro-confluent) 格式能让你读取被 io.confluent.kafka.serializers.KafkaAvroSerializer 序列化的记录，以及可以写入成能被 io.confluent.kafka.serializers.KafkaAvroDeserializer 反序列化的记录。
当以这种格式读取（反序列化）记录时，将根据记录中编码的 schema 版本 id 从配置的 Confluent Schema Registry 中获取 Avro writer schema ，而从 table schema 中推断出 reader schema。
当以这种格式写入（序列化）记录时，Avro schema 是从 table schema 中推断出来的，并会用来检索要与数据一起编码的 schema id。我们会在配置的 Confluent Schema Registry 中配置的 subject 下，检索 schema id。subject 通过 avro-confluent.subject 参数来制定。
Avro Schema Registry 格式只能与 Apache Kafka SQL 连接器或 Upsert Kafka SQL 连接器一起使用。
依赖 # In order to use the Avro Schema Registry format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    
    <item>
      <title>CREATE 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/create/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/create/</guid>
      <description>CREATE 语句 # CREATE 语句用于向当前或指定的 Catalog 中注册表、视图或函数。注册后的表、视图和函数可以在 SQL 查询中使用。
目前 Flink SQL 支持下列 CREATE 语句：
CREATE TABLE CREATE CATALOG CREATE DATABASE CREATE VIEW CREATE FUNCTION 执行 CREATE 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，execute_sql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。</description>
    </item>
    
    <item>
      <title>CSV</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/csv/</guid>
      <description>CSV format # To use the CSV format you need to add the Flink CSV dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-csv&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading CSV files using CsvReaderFormat. The reader utilizes Jackson library and allows passing the corresponding configuration for the CSV schema and parsing options.
CsvReaderFormat can be initialized and used like this:
CsvReaderFormat&amp;lt;SomePojo&amp;gt; csvFormat = CsvReaderFormat.</description>
    </item>
    
    <item>
      <title>Docker 设置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/docker/</guid>
      <description>Docker Setup # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate containers) of a Flink cluster using Docker containers.
Introduction # Docker is a popular container runtime. There are official Docker images for Apache Flink available on Docker Hub. You can use the Docker images to deploy a Session or Application cluster on Docker. This page focuses on the setup of Flink on Docker, Docker Swarm and Docker Compose.</description>
    </item>
    
    <item>
      <title>Flink 架构</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/concepts/flink-architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/concepts/flink-architecture/</guid>
      <description>Flink 架构 # Flink 是一个分布式系统，需要有效分配和管理计算资源才能执行流应用程序。它集成了所有常见的集群资源管理器，例如Hadoop YARN，但也可以设置作为独立集群甚至库运行。
本节概述了 Flink 架构，并且描述了其主要组件如何交互以执行应用程序和从故障中恢复。
Flink 集群剖析 # Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。
Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run ...中运行。
可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。
JobManager # JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：
ResourceManager
ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位（请参考TaskManagers）。Flink 为不同的环境和资源提供者（例如 YARN、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。
Dispatcher
Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</description>
    </item>
    
    <item>
      <title>Hadoop</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/hadoop/</guid>
      <description>Hadoop formats # Project Configuration # 对 Hadoop 的支持位于 flink-hadoop-compatibility Maven 模块中。
将以下依赖添加到 pom.xml 中使用 hadoop
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-hadoop-compatibility_2.12&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 如果你想在本地运行你的 Flink 应用（例如在 IDE 中），你需要按照如下所示将 hadoop-client 依赖也添加到 pom.xml：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.8.5&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Using Hadoop InputFormats # 在 Flink 中使用 Hadoop InputFormats，必须首先使用 HadoopInputs 工具类的 readHadoopFile 或 createHadoopInput 包装 Input Format。 前者用于从 FileInputFormat 派生的 Input Format，而后者必须用于通用的 Input Format。 生成的 InputFormat 可通过使用 ExecutionEnvironment#createInput 创建数据源。
生成的 DataStream 包含 2 元组，其中第一个字段是键，第二个字段是从 Hadoop InputFormat 接收的值。</description>
    </item>
    
    <item>
      <title>Hive Read &amp; Write</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_read_write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_read_write/</guid>
      <description>Hive 读 &amp;amp; 写 # 通过使用 HiveCatalog，Apache Flink 可以对 Apache Hive 表做统一的批和流处理。这意味着 Flink 可以成为 Hive 批处理引擎的一个性能更好的选择，或者连续读写 Hive 表中的数据以支持实时数据仓库应用。
读 # Flink 支持以批和流两种模式从 Hive 表中读取数据。批读的时候，Flink 会基于执行查询时表的状态进行查询。流读时将持续监控表，并在表中新数据可用时进行增量获取，默认情况下，Flink 将以批模式读取数据。
流读支持消费分区表和非分区表。对于分区表，Flink 会监控新分区的生成，并且在数据可用的情况下增量获取数据。对于非分区表，Flink 将监控文件夹中新文件的生成，并增量地读取新文件。
键 默认值 类型 描述 streaming-source.enable false Boolean 是否启动流读。注意：请确保每个分区/文件都应该原子地写入，否则读取不到完整的数据。 streaming-source.partition.include all String 选择读取的分区，可选项为 `all` 和 `latest`，`all` 读取所有分区；`latest` 读取按照 &#39;streaming-source.partition.order&#39; 排序后的最新分区，`latest` 仅在流模式的 Hive 源表作为时态表时有效。默认的选项是 `all`。在开启 &#39;streaming-source.enable&#39; 并设置 &#39;streaming-source.partition.include&#39; 为 &#39;latest&#39; 时，Flink 支持 temporal join 最新的 Hive 分区，同时，用户可以通过配置分区相关的选项来配置分区比较顺序和数据更新时间间隔。 streaming-source.monitor-interval None Duration 连续监控分区/文件的时间间隔。 注意: 默认情况下，流式读 Hive 的间隔为 &#39;1 min&#39;，但流读 Hive 的 temporal join 的默认时间间隔是 &#39;60 min&#39;，这是因为当前流读 Hive 的 temporal join 实现上有一个框架限制，即每个 TM 都要访问 Hive metastore，这可能会对 metastore 产生压力，这个问题将在未来得到改善。 streaming-source.</description>
    </item>
    
    <item>
      <title>JSON</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/json/</guid>
      <description>Json format # To use the JSON format you need to add the Flink JSON dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-json&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading/writing JSON records via the JsonSerializationSchema/JsonDeserializationSchema. These utilize the Jackson library, and support any type that is supported by Jackson, including, but not limited to, POJOs and ObjectNode.
The JsonDeserializationSchema can be used with any connector that supports the DeserializationSchema.</description>
    </item>
    
    <item>
      <title>Library Methods</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/library_methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/library_methods/</guid>
      <description>Library Methods # Gelly has a growing collection of graph algorithms for easily analyzing large-scale Graphs.
Gelly&amp;rsquo;s library methods can be used by simply calling the run() method on the input graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph&amp;lt;Long, Long, NullValue&amp;gt; graph = ...; // run Label Propagation for 30 iterations to detect communities on the input graph DataSet&amp;lt;Vertex&amp;lt;Long, Long&amp;gt;&amp;gt; verticesWithCommunity = graph.run(new LabelPropagation&amp;lt;Long&amp;gt;(30)); // print the result verticesWithCommunity.print(); Scala val env = ExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Parquet</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/parquet/</guid>
      <description>Parquet format # Flink 支持读取 Parquet 文件并生成 Flink RowData 和 Avro 记录。 要使用 Parquet format，你需要将 flink-parquet 依赖添加到项目中：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-parquet&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 要使用 Avro 格式，你需要将 parquet-avro 依赖添加到项目中：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.parquet&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;parquet-avro&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.12.2&amp;lt;/version&amp;gt; &amp;lt;optional&amp;gt;true&amp;lt;/optional&amp;gt; &amp;lt;exclusions&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;groupId&amp;gt;it.unimi.dsi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;fastutil&amp;lt;/artifactId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;/exclusions&amp;gt; &amp;lt;/dependency&amp;gt; 为了在 PyFlink 作业中使用 Parquet format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 此格式与新的 Source 兼容，可以同时在批和流模式下使用。 因此，你可使用此格式处理以下两类数据：
有界数据: 列出所有文件并全部读取。 无界数据：监控目录中出现的新文件 当你开启一个 File Source，会被默认为有界读取。 如果你想在连续读取模式下使用 File Source，你必须额外调用 AbstractFileSource.</description>
    </item>
    
    <item>
      <title>Process Function</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/process_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/process_function/</guid>
      <description>Process Function # The ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).</description>
    </item>
    
    <item>
      <title>Process Function</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/process_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/process_function/</guid>
      <description>Process Function # ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).</description>
    </item>
    
    <item>
      <title>Protobuf</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/protobuf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/protobuf/</guid>
      <description>Protobuf Format # Format: Serialization Schema Format: Deserialization Schema
The Protocol Buffers Protobuf format allows you to read and write Protobuf data, based on Protobuf generated classes.
Dependencies # In order to use the Protobuf format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-protobuf&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>SELECT 与 WHERE</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select/</guid>
      <description>SELECT 与 WHERE 子句 # Batch Streaming
SELECT 语句的常见语法格式如下所示：
SELECT select_list FROM table_expression [ WHERE boolean_expression ] 这里的 table_expression 可以是任意的数据源。它可以是一张已经存在的表、视图或者 VALUES 子句，也可以是多个现有表的关联结果、或一个子查询。这里我们假设 Orders 表在 Catalog 中处于可用状态，那么下面的语句会从 Orders 表中读出所有的行。
SELECT * FROM Orders 在 select_list 处的 * 表示查询操作将会解析所有列。但是，我们不鼓励在生产中使用 *，因为它会使查询操作在应对 Catalog 变化的时候鲁棒性降低。相反，可以在 select_list 处指定可用列的子集，或者使用声明的列进行计算。例如，假设 Orders 表中有名为 order_id、price 和 tax 的列，那么你可以编写如下查询：
SELECT order_id, price + tax FROM Orders 查询操作还可以在 VALUES 子句中使用内联数据。每一个元组对应一行，另外可以通过设置别名来为每一列指定名称。
SELECT order_id, price FROM (VALUES (1, 2.0), (2, 3.1)) AS t (order_id, price) 可以根据 WHERE 子句对行数据进行过滤。</description>
    </item>
    
    <item>
      <title>Text files</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/text_files/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/formats/text_files/</guid>
      <description>Text files format # Flink 支持使用 TextLineInputFormat 从文件中读取文本行。此 format 使用 Java 的内置 InputStreamReader 以支持的字符集编码来解码字节流。 要使用该 format，你需要将 Flink Connector Files 依赖项添加到项目中：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-files&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; PyFlink 用户可直接使用相关接口，无需添加依赖。
此 format 与新 Source 兼容，可以在批处理和流模式下使用。 因此，你可以通过两种方式使用此 format：
批处理模式的有界读取 流模式的连续读取：监视目录中出现的新文件 有界读取示例:
在此示例中，我们创建了一个 DataStream，其中包含作为字符串的文本文件的行。 此处不需要水印策略，因为记录不包含事件时间戳。
Java final FileSource&amp;lt;String&amp;gt; source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), /* Flink Path */) .build(); final DataStream&amp;lt;String&amp;gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), &amp;#34;file-source&amp;#34;); Python source = FileSource.for_record_stream_format(StreamFormat.text_line_format(), *path).build() stream = env.from_source(source, WatermarkStrategy.no_watermarks(), &amp;#34;file-source&amp;#34;) 连续读取示例: 在此示例中，我们创建了一个 DataStream，随着新文件被添加到目录中，其中包含的文本文件行的字符串流将无限增长。我们每秒会进行新文件监控。 此处不需要水印策略，因为记录不包含事件时间戳。</description>
    </item>
    
    <item>
      <title>Upsert Kafka</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/upsert-kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/upsert-kafka/</guid>
      <description>Upsert Kafka SQL 连接器 # Scan Source: Unbounded Sink: Streaming Upsert Mode
Upsert Kafka 连接器支持以 upsert 方式从 Kafka topic 中读取数据并将数据写入 Kafka topic。
作为 source，upsert-kafka 连接器生产 changelog 流，其中每条数据记录代表一个更新或删除事件。更准确地说，数据记录中的 value 被解释为同一 key 的最后一个 value 的 UPDATE，如果有这个 key（如果不存在相应的 key，则该更新被视为 INSERT）。用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。
作为 sink，upsert-kafka 连接器可以消费 changelog 流。它会将 INSERT/UPDATE_AFTER 数据作为正常的 Kafka 消息写入，并将 DELETE 数据以 value 为空的 Kafka 消息写入（表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中。
依赖 # In order to use the Upsert Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    
    <item>
      <title>导入 Flink 到 IDE 中</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/flinkdev/ide_setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/flinkdev/ide_setup/</guid>
      <description>导入 Flink 到 IDE 中 # 以下章节描述了如何将 Flink 项目导入到 IDE 中以进行 Flink 本身的源码开发。有关 Flink 程序编写的信息，请参阅 Java API 和 Scala API 快速入门指南。
每当你的 IDE 无法正常工作时，请优先尝试使用 Maven 命令行（mvn clean package -DskipTests），因为它可能是由于你的 IDE 中存在错误或未正确设置。 准备 # 首先，请从我们的仓库中拉取 Flink 源，例如：
git clone https://github.com/apache/flink.git 忽略重构提交 # 我们在 .git-blame-ignore-revs 中保留了一个大的重构提交列表。使用 git blame 查看更改注释时，忽略这些注释会很有帮助。你可以使用以下方法来配置 git 和你的 IDE：
git config blame.ignoreRevsFile .git-blame-ignore-revs IntelliJ IDEA # 该指南介绍了关于如何设置 IntelliJ IDEA IDE 来进行 Flink 核心开发。众所周知由于 Eclipse 混合 Scala 和 Java 项目时存在问题，因此越来越多的贡献者正在迁移到 IntelliJ IDEA。</description>
    </item>
    
    <item>
      <title>基于 Table API 实现实时报表</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/try-flink/table_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/try-flink/table_api/</guid>
      <description>基于 Table API 实现实时报表 # Apache Flink 提供了 Table API 作为批流统一的关系型 API。也就是说，在无界的实时流数据或者有界的批数据集上进行查询具有相同的语义，得到的结果一致。 Flink 的 Table API 可以简化数据分析、构建数据流水线以及 ETL 应用的定义。
你接下来要搭建的是什么系统？ # 在本教程中，你将学习构建一个通过账户来追踪金融交易的实时看板。 数据流水线为：先从 Kafka 中读取数据，再将结果写入到 MySQL 中，最后通过 Grafana 展示。
准备条件 # 我们默认你对 Java 或者 Scala 有一定了解，当然如果你使用的是其他编程语言，也可以继续学习。 同时也默认你了解基本的关系型概念，例如 SELECT 、GROUP BY 等语句。
困难求助 # 如果遇到问题，可以参考 社区支持资源。 Flink 的 用户邮件列表 是 Apahe 项目中最活跃的一个，这也是快速寻求帮助的重要途径。
在 Windows 环境下，如果用来生成数据的 docker 容器启动失败，请检查使用的脚本是否正确。 例如 docker-entrypoint.sh 是容器 table-walkthrough_data-generator_1 所需的 bash 脚本。 如果不可用，会报 standard_init_linux.go:211: exec user process caused &amp;ldquo;no such file or directory&amp;rdquo; 的错误。 一种解决办法是在 docker-entrypoint.</description>
    </item>
    
    <item>
      <title>配置 JobManager 内存</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_jobmanager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_jobmanager/</guid>
      <description>配置 JobManager 内存 # JobManager 是 Flink 集群的控制单元。 它由三种不同的组件组成：ResourceManager、Dispatcher 和每个正在运行作业的 JobMaster。 本篇文档将介绍 JobManager 内存在整体上以及细粒度上的配置方法。
本文接下来介绍的内存配置方法适用于 1.11 及以上版本。 Flink 在 1.11 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
提示 本篇内存配置文档仅针对 JobManager！ 与 TaskManager 相比，JobManager 具有相似但更加简单的内存模型。
配置总内存 # 配置 JobManager 内存最简单的方法就是进程的配置总内存。 本地执行模式下不需要为 JobManager 进行内存配置，配置参数将不会生效。
详细配置 # 如上图所示，下表中列出了 Flink JobManager 内存模型的所有组成部分，以及影响其大小的相关配置参数。
组成部分 配置参数 描述 JVM 堆内存 jobmanager.memory.heap.size JobManager 的 JVM 堆内存。 堆外内存 jobmanager.memory.off-heap.size JobManager 的堆外内存（直接内存或本地内存）。 JVM Metaspace jobmanager.memory.jvm-metaspace.size Flink JVM 进程的 Metaspace。 JVM 开销 jobmanager.memory.jvm-overhead.min jobmanager.memory.jvm-overhead.max jobmanager.memory.jvm-overhead.fraction 用于其他 JVM 开销的本地内存，例如栈空间、垃圾回收空间等。该内存部分为基于进程总内存的受限的等比内存部分。 配置 JVM 堆内存 # 如配置总内存中所述，另一种配置 JobManager 内存的方式是明确指定 JVM 堆内存的大小（jobmanager.</description>
    </item>
    
    <item>
      <title>日志</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/advanced/logging/</guid>
      <description>如何使用日志记录 # 所有 Flink 进程都会创建一个文本格式的日志文件，其中包含该进程中发生的各种事件的信息。 这些日志提供了深入了解 Flink 内部工作的途径，同时可以用来输出检测出的问题（以 WARN/ERROR 消息的形式），还可以辅助调试问题。
日志文件可以通过 Job-/TaskManager 对应的 WebUI 页面访问。所使用的 Resource Provider（如 YARN）可能会提供额外的访问方式来访问日志。
Flink 中的日志记录是使用 SLF4J 日志接口实现的。这允许你不需要修改 Flink 的源代码就可以使用任何支持 SLF4J 的日志框架。
默认情况下，使用 Log4j 2 作为底层日志框架。
配置 Log4j 2 # Log4j 2 是通过 property 配置文件进行配置的。
Flink 发行版在 conf 目录中附带了以下 log4j 配置文件，如果启用了 Log4j 2，则会自动使用如下文件：
log4j-cli.properties：Flink 命令行使用（例如 flink run）； log4j-session.properties：Flink 命令行在启动基于 Kubernetes/Yarn 的 Session 集群时使用（例如 kubernetes-session.sh/yarn-session.sh）； log4j-console.properties：Job-/TaskManagers 在前台模式运行时使用（例如 Kubernetes）； log4j.properties： Job-/TaskManagers 默认使用的日志配置。 Log4j 会定期扫描这些文件的变更，并在必要时调整日志记录行为。默认情况下30秒检查一次，监测间隔可以通过 Log4j 配置文件的 monitorInterval 配置项进行设置。</description>
    </item>
    
    <item>
      <title>时态表（Temporal Tables）</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/versioned_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/versioned_tables/</guid>
      <description>时态表（Temporal Tables） # 时态表（Temporal Table）是一张随时间变化的表 &amp;ndash; 在 Flink 中称为动态表，时态表中的每条记录都关联了一个或多个时间段，所有的 Flink 表都是时态的（动态的）。
时态表包含表的一个或多个有版本的表快照，时态表可以是一张跟踪所有变更记录的表（例如数据库表的 changelog，包含多个表快照），也可以是物化所有变更之后的表（例如数据库表，只有最新表快照）。
版本: 时态表可以划分成一系列带版本的表快照集合，表快照中的版本代表了快照中所有记录的有效区间，有效区间的开始时间和结束时间可以通过用户指定，根据时态表是否可以追踪自身的历史版本与否，时态表可以分为 版本表 和 普通表。
版本表: 如果时态表中的记录可以追踪和并访问它的历史版本，这种表我们称之为版本表，来自数据库的 changelog 可以定义成版本表。
普通表: 如果时态表中的记录仅仅可以追踪并和它的最新版本，这种表我们称之为普通表，来自数据库 或 HBase 的表可以定义成普通表。
设计初衷 # 关联一张版本表 # 以订单流关联产品表这个场景举例，orders 表包含了来自 Kafka 的实时订单流，product_changelog 表来自数据库表 products 的 changelog , 产品的价格在数据库表 products 中是随时间实时变化的。
SELECT * FROM product_changelog; (changelog kind) update_time product_id product_name price ================= =========== ========== ============ ===== +(INSERT) 00:01:00 p_001 scooter 11.11 +(INSERT) 00:02:00 p_002 basketball 23.11 -(UPDATE_BEFORE) 12:00:00 p_001 scooter 11.</description>
    </item>
    
    <item>
      <title>数据管道 &amp; ETL</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/etl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/etl/</guid>
      <description>数据管道 &amp;amp; ETL # Apache Flink 的一种常见应用场景是 ETL（抽取、转换、加载）管道任务。从一个或多个数据源获取数据，进行一些转换操作和信息补充，将结果存储起来。在这个教程中，我们将介绍如何使用 Flink 的 DataStream API 实现这类应用。
这里注意，Flink 的 Table 和 SQL API 完全可以满足很多 ETL 使用场景。但无论你最终是否直接使用 DataStream API，对这里介绍的基本知识有扎实的理解都是有价值的。
无状态的转换 # 本节涵盖了 map() 和 flatmap()，这两种算子可以用来实现无状态转换的基本操作。本节中的示例建立在你已经熟悉 flink-training-repo 中的出租车行程数据的基础上。
map() # 在第一个练习中，你将过滤出租车行程数据中的事件。在同一代码仓库中，有一个 GeoUtils 类，提供了一个静态方法 GeoUtils.mapToGridCell(float lon, float lat)，它可以将位置坐标（经度，维度）映射到 100x100 米的对应不同区域的网格单元。
现在让我们为每个出租车行程时间的数据对象增加 startCell 和 endCell 字段。你可以创建一个继承 TaxiRide 的 EnrichedRide 类，添加这些字段：
public static class EnrichedRide extends TaxiRide { public int startCell; public int endCell; public EnrichedRide() {} public EnrichedRide(TaxiRide ride) { this.</description>
    </item>
    
    <item>
      <title>应用程序分析与调试</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/application_profiling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/debugging/application_profiling/</guid>
      <description>应用程序分析与调试 # Apache Flink 自定义日志概述 # 每个独立的 JobManager，TaskManager，HistoryServer，ZooKeeper 守护进程都将 stdout 和 stderr 重定向到名称后缀为 .out 的文件，并将其内部的日志记录写入到 .log 后缀的文件。用户可以在 env.java.opts，env.java.opts.jobmanager，env.java.opts.taskmanager，env.java.opts.historyserver 和 env.java.opts.client 配置项中配置 Java 选项（包括 log 相关的选项），同样也可以使用脚本变量 FLINK_LOG_PREFIX 定义日志文件，并将选项括在双引号中以供后期使用。日志文件将使用 FLINK_LOG_PREFIX 与默认的 .out 和 .log 后缀一起滚动。
使用 Java Flight Recorder 分析 # Java Flight Recorder 是 Oracle JDK 内置的分析和事件收集框架。Java Mission Control 是一套先进的工具，可以对 Java Flight Recorder 收集的大量数据进行高效和详细的分析。配置示例：
env.java.opts: &amp;#34;-XX:+UnlockCommercialFeatures -XX:+UnlockDiagnosticVMOptions -XX:+FlightRecorder -XX:+DebugNonSafepoints -XX:FlightRecorderOptions=defaultrecording=true,dumponexit=true,dumponexitpath=${FLINK_LOG_PREFIX}.jfr&amp;#34; 使用 JITWatch 分析 # JITWatch Java HotSpot JIT 编译器的日志分析器和可视化工具，用于检查内联决策、热点方法、字节码和汇编。配置示例：
env.java.opts: &amp;#34;-XX:+UnlockDiagnosticVMOptions -XX:+TraceClassLoading -XX:+LogCompilation -XX:LogFile=${FLINK_LOG_PREFIX}.</description>
    </item>
    
    <item>
      <title>Debezium</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/debezium/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/debezium/</guid>
      <description>Debezium Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Debezium 是一个 CDC（Changelog Data Capture，变更数据捕获）的工具，可以把来自 MySQL、PostgreSQL、Oracle、Microsoft SQL Server 和许多其他数据库的更改实时流式传输到 Kafka 中。 Debezium 为变更日志提供了统一的格式结构，并支持使用 JSON 和 Apache Avro 序列化消息。
Flink 支持将 Debezium JSON 和 Avro 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Debezium 格式的 JSON 或 Avro 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Debezium 消息。</description>
    </item>
    
    <item>
      <title>DROP 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/drop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/drop/</guid>
      <description>DROP 语句 # DROP 语句可用于删除指定的 catalog，也可用于从当前或指定的 Catalog 中删除一个已经注册的表、视图或函数。
Flink SQL 目前支持以下 DROP 语句：
DROP CATALOG DROP TABLE DROP DATABASE DROP VIEW DROP FUNCTION 执行 DROP 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句。
Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/elasticsearch/</guid>
      <description>Elasticsearch 连接器 # 此连接器提供可以向 Elasticsearch 索引请求文档操作的 sinks。 要使用此连接器，请根据 Elasticsearch 的安装版本将以下依赖之一添加到你的项目中：
Elasticsearch 版本 Maven 依赖 6.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch6&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 7.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch7&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 Elasticsearch connector ，需要添加下列依赖： Elasticsearch version PyFlink JAR 6.x Only available for stable releases. 7.x and later versions Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 请注意，流连接器目前不是二进制发行版的一部分。 有关如何将程序和用于集群执行的库一起打包，参考此文档。
安装 Elasticsearch # Elasticsearch 集群的设置可以参考此文档。</description>
    </item>
    
    <item>
      <title>Fine-Grained Resource Management</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/finegrained_resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/finegrained_resource/</guid>
      <description>Fine-Grained Resource Management # Apache Flink works hard to auto-derive sensible default resource requirements for all applications out of the box. For users who wish to fine-tune their resource consumption, based on knowledge of their specific scenarios, Flink offers fine-grained resource management.
This page describes the fine-grained resource management’s usage, applicable scenarios, and how it works.
Note: This feature is currently an MVP (“minimum viable product”) feature and only available to DataStream API.</description>
    </item>
    
    <item>
      <title>Firehose</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/firehose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/firehose/</guid>
      <description>Amazon Kinesis Data Firehose Sink # The Firehose sink writes to Amazon Kinesis Data Firehose.
Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to setup a Kinesis Data Firehose delivery stream.
To use the connector, add the following Maven dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-firehose&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 AWS Kinesis Firehose connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 The KinesisFirehoseSink uses AWS v2 SDK for Java to write data from a Flink stream into a Firehose delivery stream.</description>
    </item>
    
    <item>
      <title>Firehose</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/firehose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/firehose/</guid>
      <description>Amazon Kinesis Data Firehose SQL Connector # Sink: Streaming Append Mode The Kinesis Data Firehose connector allows for writing data into Amazon Kinesis Data Firehose (KDF).
Dependencies # In order to use the AWS Kinesis Firehose connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kinesis-firehose&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.</description>
    </item>
    
    <item>
      <title>Graph Algorithms</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_algorithms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_algorithms/</guid>
      <description>Graph Algorithms # The logic blocks with which the Graph API and top-level algorithms are assembled are accessible in Gelly as graph algorithms in the org.apache.flink.graph.asm package. These algorithms provide optimization and tuning through configuration parameters and may provide implicit runtime reuse when processing the same input with a similar configuration.
VertexInDegree # Annoate vertices of a directed graph with the in-degree.
DataSet&amp;lt;Vertex&amp;lt;K, LongValue&amp;gt;&amp;gt; inDegree = graph .run(new VertexInDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:</description>
    </item>
    
    <item>
      <title>Hive Functions</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_functions/</guid>
      <description>Hive Functions # Use Hive Built-in Functions via HiveModule # The HiveModule provides Hive built-in functions as Flink system (built-in) functions to Flink SQL and Table API users.
For detailed information, please refer to HiveModule.
Java String name = &amp;#34;myhive&amp;#34;; String version = &amp;#34;2.3.4&amp;#34;; tableEnv.loadModue(name, new HiveModule(version)); Scala val name = &amp;#34;myhive&amp;#34; val version = &amp;#34;2.3.4&amp;#34; tableEnv.loadModue(name, new HiveModule(version)); Python from pyflink.table.module import HiveModule name = &amp;#34;myhive&amp;#34; version = &amp;#34;2.</description>
    </item>
    
    <item>
      <title>Kinesis</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kinesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kinesis/</guid>
      <description>亚马逊 Kinesis 数据流 SQL 连接器 # Kinesis 连接器提供访问 Amazon Kinesis Data Streams 。
使用此连接器, 取决于您是否读取数据和/或写入数据，增加下面依赖项的一个或多个到您的项目中:
KDS Connectivity Maven Dependency Source &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kinesis&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Sink &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-streams&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 由于许可证问题，以前的版本中 flink-connector-kinesis 工件没有部署到Maven中心库。有关更多信息，请参阅特定版本的文档。
为了在 PyFlink 作业中使用 Kinesis connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 使用亚马逊 Kinesis 流服务 # 遵循 Amazon Kinesis Streams Developer Guide 的指令建立 Kinesis 流。</description>
    </item>
    
    <item>
      <title>Kinesis</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/kinesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/kinesis/</guid>
      <description>Amazon Kinesis Data Streams SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode
The Kinesis connector allows for reading data from and writing data into Amazon Kinesis Data Streams (KDS).
Dependencies # In order to use the Kinesis connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.</description>
    </item>
    
    <item>
      <title>Kubernetes 设置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/kubernetes/</guid>
      <description>Kubernetes 安装 # 入门 # 本 入门 指南描述了如何在 Kubernetes 上部署 Flink Session 集群。
介绍 # 本文描述了如何使用 Flink standalone 部署模式在 Kubernetes 上部署 standalone 模式的 Flink 集群。通常我们建议新用户使用 native Kubernetes 部署模式在 Kubernetes上部署 Flink。
准备 # 本指南假设存在一个 Kubernets 的运行环境。你可以通过运行 kubectl get nodes 命令来确保 Kubernetes 环境运行正常，该命令展示所有连接到 Kubernets 集群的 node 节点信息。
如果你想在本地运行 Kubernetes，建议使用 MiniKube。
如果使用 MiniKube，请确保在部署 Flink 集群之前先执行 minikube ssh &#39;sudo ip link set docker0 promisc on&#39;，否则 Flink 组件不能自动地将自己映射到 Kubernetes Service 中。 Kubernetes 上的 Flink session 集群 # Flink session 集群 是以一种长期运行的 Kubernetes Deployment 形式执行的。你可以在一个 session 集群 上运行多个 Flink 作业。当然，只有 session 集群部署好以后才可以在上面提交 Flink 作业。</description>
    </item>
    
    <item>
      <title>Plugins</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/filesystems/plugins/</guid>
      <description>Plugins # Plugins facilitate a strict separation of code through restricted classloaders. Plugins cannot access classes from other plugins or from Flink that have not been specifically whitelisted. This strict isolation allows plugins to contain conflicting versions of the same library without the need to relocate classes or to converge to common versions. Currently, file systems and metric reporters are pluggable but in the future, connectors, formats, and even user code should also be pluggable.</description>
    </item>
    
    <item>
      <title>Queryable State</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/queryable_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/queryable_state/</guid>
      <description>Queryable State # 目前 querable state 的客户端 API 还在不断演进，不保证现有接口的稳定性。在后续的 Flink 版本中有可能发生 API 变化。 简而言之, 这个特性将 Flink 的 managed keyed (partitioned) state (参考 Working with State) 暴露给外部，从而用户可以在 Flink 外部查询作业 state。 在某些场景中，Queryable State 消除了对外部系统的分布式操作以及事务的需求，比如 KV 存储系统，而这些外部系统往往会成为瓶颈。除此之外，这个特性对于调试作业非常有用。
注意: 进行查询时，state 会在并发线程中被访问，但 state 不会进行同步和拷贝。这种设计是为了避免同步和拷贝带来的作业延时。对于使用 Java 堆内存的 state backend， 比如 MemoryStateBackend 或者 FsStateBackend，它们获取状态时不会进行拷贝，而是直接引用状态对象，所以对状态的 read-modify-write 是不安全的，并且可能会因为并发修改导致查询失败。但 RocksDBStateBackend 是安全的，不会遇到上述问题。 架构 # 在展示如何使用 Queryable State 之前，先简单描述一下该特性的组成部分，主要包括以下三部分:
QueryableStateClient，默认运行在 Flink 集群外部，负责提交用户的查询请求； QueryableStateClientProxy，运行在每个 TaskManager 上(即 Flink 集群内部)，负责接收客户端的查询请求，从所负责的 Task Manager 获取请求的 state，并返回给客户端； QueryableStateServer, 运行在 TaskManager 上，负责服务本地存储的 state。 客户端连接到一个代理，并发送请求获取特定 k 对应的 state。 如 Working with State 所述，keyed state 按照 Key Groups 进行划分，每个 TaskManager 会分配其中的一些 key groups。代理会询问 JobManager 以找到 k 所属 key group 的 TaskManager。根据返回的结果, 代理将会向运行在 TaskManager 上的 QueryableStateServer 查询 k 对应的 state， 并将结果返回给客户端。</description>
    </item>
    
    <item>
      <title>SELECT DISTINCT</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select-distinct/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select-distinct/</guid>
      <description>SELECT DISTINCT # Batch Streaming
如果使用”SELECT DISTINCT“查询,所有的复制行都会从结果集(每个分组只会保留一行)中被删除.
SELECT DISTINCT id FROM Orders 对于流式查询, 计算查询结果所需要的状态可能会源源不断地增长,而状态大小又依赖不同行的数量.此时,可以通过配置文件为状态设置合适的存活时间(TTL),以防止过大的状态可能对查询结果的正确性的影响.具体配置可参考:查询相关的配置.
Back to top</description>
    </item>
    
    <item>
      <title>Speculative Execution</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/speculative_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/speculative_execution/</guid>
      <description>预测执行 # 这个文档描述了预测执行的背景，使用方法，以及如何验证其有效性。
背景 # 预测执行是一种用于缓解异常机器节点导致作业执行缓慢的机制。机器节点异常包括硬件异常，偶发的输入输出繁忙，高 CPU 负载等问题。 这些问题会导致运行在其上的任务比起在其他节点上运行的任务慢很多，从而影响到整个作业的执行时长。
在这种情况下，预测执行会为这些慢任务创建一些新的执行实例并部署在正常的机器节点上。这些新的执行实例和其对应的老执行实例(慢任务) 会消费相同的数据，并产出相同的结果。而那些老执行实例也会被保留继续执行。这些执行实例(包括新实例和老实例)中首先成功结束的执行 实例会被认可，其产出的结果会对下游任务可见，其他实例则会被取消掉。
为了实现这个机制，Flink 会通过一个慢任务检测器来检测慢任务。检测到的慢任务位于的机器节点会被识别为异常机器节点，并被加入机器 节点黑名单中。调度器则会为这些慢节点创建新的执行实例，并将其部署到未被加黑的机器节点上。
使用方法 # 本章节描述了如何使用预测执行，包含如何启用，调优，以及开发/改进自定义 source 来支持预测执行。
注意: Flink 尚不支持 sink 的预测执行。这个能力会在后续版本中得到完善。 注意：Flink 不支持 DataSet 作业的预测执行，因为 DataSet API 在不久的将来会被废弃。现在推荐使用 DataStream API 来开发 Flink 批处理作业。 启用预测执行 # 要启用预测执行，你需要设置以下配置项：
jobmanager.scheduler: AdaptiveBatch 因为当前只有 Adaptive Batch Scheduler 支持预测执行. jobmanager.adaptive-batch-scheduler.speculative.enabled: true 配置调优 # 考虑到不同作业的差异，为了让预测执行获得更好的效果，你可以调优下列调度器配置项：
jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 你还可以调优下列慢任务检测器的配置项：
slow-task-detector.check-interval slow-task-detector.execution-time.baseline-lower-bound slow-task-detector.execution-time.baseline-multiplier slow-task-detector.execution-time.baseline-ratio 让 Source 支持预测执行 # 如果你的作业有用到自定义 Source , 并且这个 Source 用到了自定义的 SourceEvent , 你需要修改该 Source 的 SplitEnumerator 实现接口 SupportsHandleExecutionAttemptSourceEvent 。</description>
    </item>
    
    <item>
      <title>YARN</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/yarn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/resource-providers/yarn/</guid>
      <description>Apache Hadoop YARN # Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on YARN.
Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&amp;rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.</description>
    </item>
    
    <item>
      <title>弹性扩缩容</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/elastic_scaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/elastic_scaling/</guid>
      <description>弹性扩缩容 # 在 Apache Flink 中，可以通过手动停止 Job，然后从停止时创建的 Savepoint 恢复，最后重新指定并行度的方式来重新扩缩容 Job。
这个文档描述了那些可以使 Flink 自动调整并行度的选项。
Reactive 模式 # Reactive 模式是一个 MVP （minimum viable product，最小可行产品）特性。目前 Flink 社区正在积极地从邮件列表中获取用户的使用反馈。请注意文中列举的一些局限性。 在 Reactive 模式下，Job 会使用集群中所有的资源。当增加 TaskManager 时，Job 会自动扩容。当删除时，就会自动缩容。Flink 会管理 Job 的并行度，始终会尽可能地使用最大值。
当发生扩缩容时，Job 会被重启，并且会从最新的 Checkpoint 中恢复。这就意味着不需要花费额外的开销去创建 Savepoint。当然，所需要重新处理的数据量取决于 Checkpoint 的间隔时长，而恢复的时间取决于状态的大小。
借助 Reactive 模式，Flink 用户可以通过一些外部的监控服务产生的指标，例如：消费延迟、CPU 利用率汇总、吞吐量、延迟等，实现一个强大的自动扩缩容机制。当上述的这些指标超出或者低于一定的阈值时，增加或者减少 TaskManager 的数量。在 Kubernetes 中，可以通过改变 Deployment 的副本数（Replica Factor） 实现。而在 AWS 中，可以通过改变 Auto Scaling 组 来实现。这类外部服务只需要负责资源的分配以及回收，而 Flink 则负责在这些资源上运行 Job。
入门 # 你可以参考下面的步骤试用 Reactive 模式。以下步骤假设你使用的是单台机器部署 Flink。</description>
    </item>
    
    <item>
      <title>调优指南</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_tuning/</guid>
      <description>调优指南 # 本文在的基本的配置指南的基础上，介绍如何根据具体的使用场景调整内存配置，以及在不同使用场景下分别需要重点关注哪些配置参数。
独立部署模式（Standalone Deployment）下的内存配置 # 独立部署模式下，我们通常更关注 Flink 应用本身使用的内存大小。 建议配置 Flink 总内存（taskmanager.memory.flink.size 或者 jobmanager.memory.flink.size）或其组成部分。 此外，如果出现 Metaspace 不足的问题，可以调整 JVM Metaspace 的大小。
这种情况下通常无需配置进程总内存，因为不管是 Flink 还是部署环境都不会对 JVM 开销 进行限制，它只与机器的物理资源相关。
容器（Container）的内存配置 # 在容器化部署模式（Containerized Deployment）下（Kubernetes 或 Yarn），建议配置进程总内存（taskmanager.memory.process.size 或者 jobmanager.memory.process.size）。 该配置参数用于指定分配给 Flink JVM 进程的总内存，也就是需要申请的容器大小。
提示 如果配置了 Flink 总内存，Flink 会自动加上 JVM 相关的内存部分，根据推算出的进程总内存大小申请容器。
注意： 如果 Flink 或者用户代码分配超过容器大小的非托管的堆外（本地）内存，部署环境可能会杀掉超用内存的容器，造成作业执行失败。 请参考容器内存超用中的相关描述。
State Backend 的内存配置 # 本章节内容仅与 TaskManager 相关。
在部署 Flink 流处理应用时，可以根据 State Backend 的类型对集群的配置进行优化。
Heap State Backend # 执行无状态作业或者使用 Heap State Backend（MemoryStateBackend 或 FsStateBackend）时，建议将托管内存设置为 0。 这样能够最大化分配给 JVM 上用户代码的内存。</description>
    </item>
    
    <item>
      <title>连接器和格式</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/connector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/connector/</guid>
      <description> 连接器和格式 # Flink 应用程序可以通过连接器读取和写入各种外部系统。它支持多种格式，以便对数据进行编码和解码以匹配 Flink 的数据结构。
DataStream 和 Table API/SQL 都提供了连接器和格式的概述。
可用的组件 # 为了使用连接器和格式，您需要确保 Flink 可以访问实现了这些功能的组件。对于 Flink 社区支持的每个连接器，我们在 Maven Central 发布了两类组件：
flink-connector-&amp;lt;NAME&amp;gt; 这是一个精简 JAR，仅包括连接器代码，但不包括最终的第三方依赖项； flink-sql-connector-&amp;lt;NAME&amp;gt; 这是一个包含连接器第三方依赖项的 uber JAR； 这同样适用于格式。请注意，某些连接器可能没有相应的 flink-sql-connector-&amp;lt;NAME&amp;gt; 组件，因为它们不需要第三方依赖项。
uber/fat JAR 主要与SQL 客户端一起使用，但您也可以在任何 DataStream/Table 应用程序中使用它们。 使用组件 # 为了使用连接器/格式模块，您可以：
把精简 JAR 及其传递依赖项打包进您的作业 JAR； 把 uber JAR 打包进您的作业 JAR； 把 uber JAR 直接复制到 Flink 发行版的 /lib 文件夹内； 关于打包依赖项，请查看 Maven 和 Gradle 指南。有关 Flink 发行版的参考，请查看Flink 依赖剖析。
决定是打成 uber JAR、精简 JAR 还是仅在发行版包含依赖项取决于您和您的使用场景。如果您使用 uber JAR，您将对作业里的依赖项版本有更多的控制权；如果您使用精简 JAR，由于您可以在不更改连接器版本的情况下更改版本（允许二进制兼容），您将对传递依赖项有更多的控制权；如果您直接在 Flink 发行版的 /lib 目录里内嵌连接器 uber JAR，您将能够在一处控制所有作业的连接器版本。 </description>
    </item>
    
    <item>
      <title>流式分析</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/streaming_analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/streaming_analytics/</guid>
      <description>流式分析 # Event Time and Watermarks # 概要 # Flink 明确支持以下三种时间语义:
事件时间(event time)： 事件产生的时间，记录的是设备生产(或者存储)事件的时间
摄取时间(ingestion time)： Flink 读取事件时记录的时间
处理时间(processing time)： Flink pipeline 中具体算子处理事件的时间
为了获得可重现的结果，例如在计算过去的特定一天里第一个小时股票的最高价格时，我们应该使用事件时间。这样的话，无论什么时间去计算都不会影响输出结果。然而如果使用处理时间的话，实时应用程序的结果是由程序运行的时间所决定。多次运行基于处理时间的实时程序，可能得到的结果都不相同，也可能会导致再次分析历史数据或者测试新代码变得异常困难。
使用 Event Time # 如果想要使用事件时间，需要额外给 Flink 提供一个时间戳提取器和 Watermark 生成器，Flink 将使用它们来跟踪事件时间的进度。这将在选节使用 Watermarks 中介绍，但是首先我们需要解释一下 watermarks 是什么。
Watermarks # 让我们通过一个简单的示例来演示为什么需要 watermarks 及其工作方式。
在此示例中，我们将看到带有混乱时间戳的事件流，如下所示。显示的数字表达的是这些事件实际发生时间的时间戳。到达的第一个事件发生在时间 4，随后发生的事件发生在更早的时间 2，依此类推：
··· 23 19 22 24 21 14 17 13 12 15 9 11 7 2 4 → 假设我们要对数据流排序，我们想要达到的目的是：应用程序应该在数据流里的事件到达时就有一个算子（我们暂且称之为排序）开始处理事件，这个算子所输出的流是按照时间戳排序好的。
让我们重新审视这些数据:
(1) 我们的排序器看到的第一个事件的时间戳是 4，但是我们不能立即将其作为已排序的流释放。因为我们并不能确定它是有序的，并且较早的事件有可能并未到达。事实上，如果站在上帝视角，我们知道，必须要等到时间戳为 2 的元素到来时，排序器才可以有事件输出。</description>
    </item>
    
    <item>
      <title>命令行界面</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/cli/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/cli/</guid>
      <description>命令行界面 # Flink provides a Command-Line Interface (CLI) bin/flink to run programs that are packaged as JAR files and to control their execution. The CLI is part of any Flink setup, available in local single node setups and in distributed setups. It connects to the running JobManager specified in conf/flink-conf.yaml.
Job Lifecycle Management # A prerequisite for the commands listed in this section to work is to have a running Flink deployment like Kubernetes, YARN or any other option available.</description>
    </item>
    
    <item>
      <title>异步 I/O</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/asyncio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/operators/asyncio/</guid>
      <description>用于外部数据访问的异步 I/O # 本文讲解 Flink 用于访问外部数据存储的异步 I/O API。 对于不熟悉异步或者事件驱动编程的用户，建议先储备一些关于 Future 和事件驱动编程的知识。
提示：这篇文档 FLIP-12: 异步 I/O 的设计和实现介绍了关于设计和实现异步 I/O 功能的细节。 对于新增的重试支持的实现细节可以参考FLIP-232: 为 DataStream API 异步 I/O 操作增加重试支持。
对于异步 I/O 操作的需求 # 在与外部系统交互（用数据库中的数据扩充流数据）的时候，需要考虑与外部系统的通信延迟对整个流处理应用的影响。
简单地访问外部数据库的数据，比如使用 MapFunction，通常意味着同步交互： MapFunction 向数据库发送一个请求然后一直等待，直到收到响应。在许多情况下，等待占据了函数运行的大部分时间。
与数据库异步交互是指一个并行函数实例可以并发地处理多个请求和接收多个响应。这样，函数在等待的时间可以发送其他请求和接收其他响应。至少等待的时间可以被多个请求摊分。大多数情况下，异步交互可以大幅度提高流处理的吞吐量。
注意： 仅仅提高 MapFunction 的并行度（parallelism）在有些情况下也可以提升吞吐量，但是这样做通常会导致非常高的资源消耗：更多的并行 MapFunction 实例意味着更多的 Task、更多的线程、更多的 Flink 内部网络连接、 更多的与数据库的网络连接、更多的缓冲和更多程序内部协调的开销。
先决条件 # 如上节所述，正确地实现数据库（或键/值存储）的异步 I/O 交互需要支持异步请求的数据库客户端。许多主流数据库都提供了这样的客户端。
如果没有这样的客户端，可以通过创建多个客户端并使用线程池处理同步调用的方法，将同步客户端转换为有限并发的客户端。然而，这种方法通常比正规的异步客户端效率低。
异步 I/O API # Flink 的异步 I/O API 允许用户在流处理中使用异步请求客户端。API 处理与数据流的集成，同时还能处理好顺序、事件时间和容错等。
在具备异步数据库客户端的基础上，实现数据流转换操作与数据库的异步 I/O 交互需要以下三部分：
实现分发请求的 AsyncFunction 获取数据库交互的结果并发送给 ResultFuture 的 回调 函数 将异步 I/O 操作应用于 DataStream 作为 DataStream 的一次转换操作, 启用或者不启用重试。 下面是基本的代码模板：</description>
    </item>
    
    <item>
      <title>用户自定义 Functions</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/user_defined_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/user_defined_functions/</guid>
      <description>&amp;lsquo;用户自定义 Functions&amp;rsquo; # 大多数操作都需要用户自定义 function。本节列出了实现用户自定义 function 的不同方式。还会介绍 Accumulators（累加器），可用于深入了解你的 Flink 应用程序。
Java 实现接口 # 最基本的方法是实现提供的接口：
class MyMapFunction implements MapFunction&amp;lt;String, Integer&amp;gt; { public Integer map(String value) { return Integer.parseInt(value); } } data.map(new MyMapFunction()); 匿名类 # 你可以将 function 当做匿名类传递：
data.map(new MapFunction&amp;lt;String, Integer&amp;gt; () { public Integer map(String value) { return Integer.parseInt(value); } }); Java 8 Lambdas # Flink 在 Java API 中还支持 Java 8 Lambdas 表达式。
data.filter(s -&amp;gt; s.startsWith(&amp;#34;http://&amp;#34;)); data.reduce((i1,i2) -&amp;gt; i1 + i2); Rich functions # 所有需要用户自定义 function 的转化操作都可以将 rich function 作为参数。例如，你可以将下面代码</description>
    </item>
    
    <item>
      <title>作业调度</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/internals/job_scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/internals/job_scheduling/</guid>
      <description>作业调度 # 这篇文档简要描述了 Flink 怎样调度作业, 怎样在 JobManager 里描述和追踪作业状态
调度 # Flink 通过 Task Slots 来定义执行资源。每个 TaskManager 有一到多个 task slot，每个 task slot 可以运行一条由多个并行 task 组成的流水线。 这样一条流水线由多个连续的 task 组成，比如并行度为 n 的 MapFunction 和 并行度为 n 的 ReduceFunction。需要注意的是 Flink 经常并发执行连续的 task，不仅在流式作业中到处都是，在批量作业中也很常见。
下图很好的阐释了这一点，一个由数据源、MapFunction 和 ReduceFunction 组成的 Flink 作业，其中数据源和 MapFunction 的并行度为 4 ，ReduceFunction 的并行度为 3 。流水线由一系列的 Source - Map - Reduce 组成，运行在 2 个 TaskManager 组成的集群上，每个 TaskManager 包含 3 个 slot，整个作业的运行如下图所示。
Flink 内部通过 SlotSharingGroup 和 CoLocationGroup 来定义哪些 task 可以共享一个 slot， 哪些 task 必须严格放到同一个 slot。</description>
    </item>
    
    <item>
      <title>ALTER 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/alter/</guid>
      <description>ALTER 语句 # ALTER 语句用于修改一个已经在 Catalog 中注册的表、视图或函数定义。
Flink SQL 目前支持以下 ALTER 语句：
ALTER TABLE ALTER VIEW ALTER DATABASE ALTER FUNCTION 执行 ALTER 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 ALTER 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 ALTER 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，execute_sql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。</description>
    </item>
    
    <item>
      <title>Canal</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/canal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/canal/</guid>
      <description>Canal Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Canal 是一个 CDC（ChangeLog Data Capture，变更日志数据捕获）工具，可以实时地将 MySQL 变更传输到其他系统。Canal 为变更日志提供了统一的数据格式，并支持使用 JSON 或 protobuf 序列化消息（Canal 默认使用 protobuf）。
Flink 支持将 Canal 的 JSON 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Canal 格式的 JSON 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Canal 消息。</description>
    </item>
    
    <item>
      <title>Flink 操作场景</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/try-flink/flink-operations-playground/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/try-flink/flink-operations-playground/</guid>
      <description>Flink 操作场景 # Apache Flink 可以以多种方式在不同的环境中部署，抛开这种多样性而言，Flink 集群的基本构建方式和操作原则仍然是相同的。
在这篇文章里，你将会学习如何管理和运行 Flink 任务，了解如何部署和监控应用程序、Flink 如何从失败作业中进行恢复，同时你还会学习如何执行一些日常操作任务，如升级和扩容。
注意：本文中使用的 Apache Flink Docker 镜像仅适用于 Apache Flink 发行版。 由于你目前正在浏览快照版的文档，因此下文中引用的分支可能已经不存在了，请先通过左侧菜单下方的版本选择器切换到发行版文档再查看。 场景说明 # 这篇文章中的所有操作都是基于如下两个集群进行的： Flink Session Cluster 以及一个 Kafka 集群， 我们会在下文带领大家一起搭建这两个集群。
一个 Flink 集群总是包含一个 JobManager 以及一个或多个 Flink TaskManager。JobManager 负责处理 Job 提交、 Job 监控以及资源管理。Flink TaskManager 运行 worker 进程， 负责实际任务 Tasks 的执行，而这些任务共同组成了一个 Flink Job。 在这篇文章中， 我们会先运行一个 TaskManager，接下来会扩容到多个 TaskManager。 另外，这里我们会专门使用一个 client 容器来提交 Flink Job， 后续还会使用该容器执行一些操作任务。需要注意的是，Flink 集群的运行并不需要依赖 client 容器， 我们这里引入只是为了使用方便。
这里的 Kafka 集群由一个 Zookeeper 服务端和一个 Kafka Broker 组成。</description>
    </item>
    
    <item>
      <title>Graph Generators</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_generators/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/graph_generators/</guid>
      <description>Graph Generators # Gelly provides a collection of scalable graph generators. Each generator is
parallelizable, in order to create large datasets scale-free, generating the same graph regardless of parallelism thrifty, using as few operators as possible Graph generators are configured using the builder pattern. The parallelism of generator operators can be set explicitly by calling setParallelism(parallelism). Lowering the parallelism will reduce the allocation of memory and network buffers.
Graph-specific configuration must be called first, then configuration common to all generators, and lastly the call to generate().</description>
    </item>
    
    <item>
      <title>JDBC</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/jdbc/</guid>
      <description>JDBC SQL 连接器 # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append &amp;amp; Upsert Mode
JDBC 连接器允许使用 JDBC 驱动向任意类型的关系型数据库读取或者写入数据。本文档描述了针对关系型数据库如何通过建立 JDBC 连接器来执行 SQL 查询。
如果在 DDL 中定义了主键，JDBC sink 将以 upsert 模式与外部系统交换 UPDATE/DELETE 消息；否则，它将以 append 模式与外部系统交换消息且不支持消费 UPDATE/DELETE 消息。
依赖 # In order to use the JDBC connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    
    <item>
      <title>State Backends</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state_backends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state_backends/</guid>
      <description>State Backends # Flink 提供了多种 state backends，它用于指定状态的存储方式和位置。
状态可以位于 Java 的堆或堆外内存。取决于你的 state backend，Flink 也可以自己管理应用程序的状态。 为了让应用程序可以维护非常大的状态，Flink 可以自己管理内存（如果有必要可以溢写到磁盘）。 默认情况下，所有 Flink Job 会使用配置文件 flink-conf.yaml 中指定的 state backend。
但是，配置文件中指定的默认 state backend 会被 Job 中指定的 state backend 覆盖，如下所示。
关于可用的 state backend 更多详细信息，包括其优点、限制和配置参数等，请参阅部署和运维的相应部分。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(...); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(...) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(...) Back to top</description>
    </item>
    
    <item>
      <title>Task 生命周期</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/internals/task_lifecycle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/internals/task_lifecycle/</guid>
      <description>Task 生命周期 # Task 是 Flink 的基本执行单元。算子的每个并行实例都在 task 里执行。例如，一个并行度为 5 的算子，它的每个实例都由一个单独的 task 来执行。
StreamTask 是 Flink 流式计算引擎中所有不同 task 子类的基础。本文会深入讲解 StreamTask 生命周期的不同阶段，并阐述每个阶段的主要方法。
算子生命周期简介 # 因为 task 是算子并行实例的执行实体，所以它的生命周期跟算子的生命周期紧密联系在一起。因此，在深入介绍 StreamTask 生命周期之前，先简要介绍一下代表算子生命周期的基本方法。这些方法按调用的先后顺序如下所示。考虑到算子可能是用户自定义函数（UDF），因此我们在每个算子下也展示（以缩进的方式）了 UDF 生命周期中调用的各个方法。AbstractUdfStreamOperator 是所有执行 UDF 的算子的基类，如果算子继承了 AbstractUdfStreamOperator，那么这些方法都是可用的。
// 初始化阶段 OPERATOR::setup UDF::setRuntimeContext OPERATOR::initializeState OPERATOR::open UDF::open // 处理阶段（对每个 element 或 watermark 调用） OPERATOR::processElement UDF::run OPERATOR::processWatermark // checkpointing 阶段（对每个 checkpoint 异步调用） OPERATOR::snapshotState // 通知 operator 处理记录的过程结束 OPERATOR::finish // 结束阶段 OPERATOR::close UDF::close 简而言之，在算子初始化时调用 setup() 来初始化算子的特定设置，比如 RuntimeContext 和指标收集的数据结构。在这之后，算子通过 initializeState() 初始化状态，算子的所有初始化工作在 open() 方法中执行，比如在继承 AbstractUdfStreamOperator 的情况下，初始化用户自定义函数。</description>
    </item>
    
    <item>
      <title>测试的依赖项</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/testing/</guid>
      <description>用于测试的依赖项 # Flink 提供了用于测试作业的实用程序，您可以将其添加为依赖项。
DataStream API 测试 # 如果要为使用 DataStream API 构建的作业开发测试用例，则需要添加以下依赖项：
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-test-utils&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;ltscope&amp;gttest&amp;lt/scope&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. testCompile &#34;org.apache.flink:flink-test-utils:1.16-SNAPSHOT&#34; Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script.</description>
    </item>
    
    <item>
      <title>常见问题</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_trouble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_trouble/</guid>
      <description>常见问题 # IllegalConfigurationException # 如果遇到从 TaskExecutorProcessUtils 或 JobManagerProcessUtils 抛出的 IllegalConfigurationException 异常，这通常说明您的配置参数中存在无效值（例如内存大小为负数、占比大于 1 等）或者配置冲突。 请根据异常信息，确认出错的内存部分的相关文档及配置信息。
OutOfMemoryError: Java heap space # 该异常说明 JVM 的堆空间过小。 可以通过增大总内存、TaskManager 的任务堆内存、JobManager 的 JVM 堆内存等方法来增大 JVM 堆空间。
提示 也可以增大 TaskManager 的框架堆内存。 这是一个进阶配置，只有在确认是 Flink 框架自身需要更多内存时才应该去调整。
OutOfMemoryError: Direct buffer memory # 该异常通常说明 JVM 的直接内存限制过小，或者存在直接内存泄漏（Direct Memory Leak）。 请确认用户代码及外部依赖中是否使用了 JVM 直接内存，以及如果使用了直接内存，是否配置了足够的内存空间。 可以通过调整堆外内存来增大直接内存限制。 有关堆外内存的配置方法，请参考 TaskManager、JobManager 以及 JVM 参数的相关文档。
OutOfMemoryError: Metaspace # 该异常说明 JVM Metaspace 限制过小。 可以尝试调整 TaskManager、JobManager 的 JVM Metaspace。
IOException: Insufficient number of network buffers # 该异常仅与 TaskManager 相关。</description>
    </item>
    
    <item>
      <title>窗口函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-tvf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-tvf/</guid>
      <description>Windowing table-valued functions (Windowing TVFs) # Batch Streaming
Windows are at the heart of processing infinite streams. Windows split the stream into “buckets” of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink SQL and how the programmer can benefit to the maximum from its offered functionality.
Apache Flink provides several window table-valued functions (TVF) to divide the elements of your table into windows, including:</description>
    </item>
    
    <item>
      <title>事件驱动应用</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/event_driven/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/event_driven/</guid>
      <description>事件驱动应用 # 处理函数（Process Functions） # 简介 # ProcessFunction 将事件处理与 Timer，State 结合在一起，使其成为流处理应用的强大构建模块。 这是使用 Flink 创建事件驱动应用程序的基础。它和 RichFlatMapFunction 十分相似， 但是增加了 Timer。
示例 # 如果你已经体验了 流式分析训练 的动手实践， 你应该记得，它是采用 TumblingEventTimeWindow 来计算每个小时内每个司机的小费总和， 像下面的示例这样：
// 计算每个司机每小时的小费总和 DataStream&amp;lt;Tuple3&amp;lt;Long, Long, Float&amp;gt;&amp;gt; hourlyTips = fares .keyBy((TaxiFare fare) -&amp;gt; fare.driverId) .window(TumblingEventTimeWindows.of(Time.hours(1))) .process(new AddTips()); 使用 KeyedProcessFunction 去实现相同的操作更加直接且更有学习意义。 让我们开始用以下代码替换上面的代码：
// 计算每个司机每小时的小费总和 DataStream&amp;lt;Tuple3&amp;lt;Long, Long, Float&amp;gt;&amp;gt; hourlyTips = fares .keyBy((TaxiFare fare) -&amp;gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); 在这个代码片段中，一个名为 PseudoWindow 的 KeyedProcessFunction 被应用于 KeyedStream， 其结果是一个 DataStream&amp;lt;Tuple3&amp;lt;Long, Long, Float&amp;gt;&amp;gt; （与使用 Flink 内置时间窗口的实现生成的流相同）。</description>
    </item>
    
    <item>
      <title>文件系统</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/filesystem/</guid>
      <description>文件系统 # 连接器提供了 BATCH 模式和 STREAMING 模式统一的 Source 和 Sink。Flink FileSystem abstraction 支持连接器对文件系统进行（分区）文件读写。文件系统连接器为 BATCH 和 STREAMING 模式提供了相同的保证，而且对 STREAMING 模式执行提供了精确一次（exactly-once）语义保证。
连接器支持对任意（分布式的）文件系统（例如，POSIX、 S3、 HDFS）以某种数据格式 format (例如，Avro、 CSV、 Parquet) 对文件进行写入，或者读取后生成数据流或一组记录。
File Source # File Source 是基于 Source API 同时支持批模式和流模式文件读取的统一 Source。 File Source 分为以下两个部分：SplitEnumerator 和 SourceReader。
SplitEnumerator 负责发现和识别需要读取的文件，并将这些文件分配给 SourceReader 进行读取。 SourceReader 请求需要处理的文件，并从文件系统中读取该文件。 可能需要指定某种 format 与 File Source 联合进行解析 CSV、解码AVRO、或者读取 Parquet 列式文件。
有界流和无界流 # 有界的 File Source（通过 SplitEnumerator）列出所有文件（一个过滤出隐藏文件的递归目录列表）并读取。
无界的 File Source 由配置定期扫描文件的 enumerator 创建。 在无界的情况下，SplitEnumerator 将像有界的 File Source 一样列出所有文件，但是不同的是，经过一个时间间隔之后，重复上述操作。 对于每一次列举操作，SplitEnumerator 会过滤掉之前已经检测过的文件，将新扫描到的文件发送给 SourceReader。</description>
    </item>
    
    <item>
      <title>指标</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/metrics/</guid>
      <description>指标 # Flink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from any user function that extends RichFunction by calling getRuntimeContext().getMetricGroup(). This method returns a MetricGroup object on which you can create and register new metrics.
Metric types # Flink supports Counters, Gauges, Histograms and Meters.
Counter # A Counter is used to count something. The current value can be in- or decremented using inc()/inc(long n) or dec()/dec(long n).</description>
    </item>
    
    <item>
      <title>Bipartite Graph</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/bipartite_graph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/libs/gelly/bipartite_graph/</guid>
      <description>Bipartite Graph # Bipartite Graph currently only supported in Gelly Java API. Bipartite Graph # A bipartite graph (also called a two-mode graph) is a type of graph where vertices are separated into two disjoint sets. These sets are usually called top and bottom vertices. An edge in this graph can only connect vertices from opposite sets (i.e. bottom vertex to top vertex) and cannot connect two vertices in the same set.</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/elasticsearch/</guid>
      <description>Elasticsearch SQL 连接器 # Sink: Batch Sink: Streaming Append &amp;amp; Upsert Mode
Elasticsearch 连接器允许将数据写入到 Elasticsearch 引擎的索引中。本文档描述运行 SQL 查询时如何设置 Elasticsearch 连接器。
连接器可以工作在 upsert 模式，使用 DDL 中定义的主键与外部系统交换 UPDATE/DELETE 消息。
如果 DDL 中没有定义主键，那么连接器只能工作在 append 模式，只能与外部系统交换 INSERT 消息。
依赖 # In order to use the Elasticsearch connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Elasticsearch version Maven dependency SQL Client JAR 6.</description>
    </item>
    
    <item>
      <title>INSERT 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/insert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/insert/</guid>
      <description>INSERT 语句 # INSERT 语句用来向表中添加行。
执行 INSERT 语句 # Java 单条 INSERT 语句，可以使用 TableEnvironment 中的 executeSql() 方法执行。executeSql() 方法执行 INSERT 语句时会立即提交一个 Flink 作业，并且返回一个 TableResult 对象，通过该对象可以获取 JobClient 方便的操作提交的作业。 多条 INSERT 语句，使用 TableEnvironment 中的 createStatementSet 创建一个 StatementSet 对象，然后使用 StatementSet 中的 addInsertSql() 方法添加多条 INSERT 语句，最后通过 StatementSet 中的 execute() 方法来执行。
以下的例子展示了如何在 TableEnvironment 中执行一条 INSERT 语句，或者通过 StatementSet 执行多条 INSERT 语句。
Scala 单条 INSERT 语句，可以使用 TableEnvironment 中的 executeSql() 方法执行。executeSql() 方法执行 INSERT 语句时会立即提交一个 Flink 作业，并且返回一个 TableResult 对象，通过该对象可以获取 JobClient 方便的操作提交的作业。 多条 INSERT 语句，使用 TableEnvironment 中的 createStatementSet 创建一个 StatementSet 对象，然后使用 StatementSet 中的 addInsertSql() 方法添加多条 INSERT 语句，最后通过 StatementSet 中的 execute() 方法来执行。</description>
    </item>
    
    <item>
      <title>Maxwell</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/maxwell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/maxwell/</guid>
      <description>Maxwell Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Maxwell is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into Kafka, Kinesis and other streaming connectors. Maxwell provides a unified format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Maxwell JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as</description>
    </item>
    
    <item>
      <title>Metric Reporters</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/metric_reporters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/metric_reporters/</guid>
      <description>Metric Reporters # Flink 支持用户将 Flink 的各项运行时指标发送给外部系统。 了解更多指标方面信息可查看 metric 系统相关文档。
你可以通过 conf/flink-conf.yaml 文件来配置一种或多种发送器，将运行时指标暴露给外部系统。 发送器会在 TaskManager、Flink 作业启动时进行实例化。
下面列出了所有发送器都适用的参数，可以通过配置文件中的 metrics.reporter.&amp;lt;reporter_name&amp;gt;.&amp;lt;property&amp;gt; 项进行配置。有些发送器有自己特有的配置，详见该发送器章节下的具体说明。
键 默认值 数据类型 描述 factory.class (none) String 命名为 &amp;lt;name&amp;gt; 发送器的工厂类名称。 interval 10 s Duration 命名为 &amp;lt;name&amp;gt; 发送器的发送间隔，只支持 push 类型发送器。 scope.delimiter &#34;.&#34; String 命名为 &amp;lt;name&amp;gt; 发送器的指标标识符中的间隔符。 scope.variables.additional Map 命名为 &amp;lt;name&amp;gt; 发送器的 map 形式的变量列表，只支持 tags 类型发送器。 scope.variables.excludes &#34;.&#34; String 命名为 &amp;lt;name&amp;gt; 发送器应该忽略的一组变量，只支持 tags 类型发送器。 filter.includes &#34;*:*:*&#34; List&amp;lt;String&amp;gt; 命名为 &amp;lt;name&amp;gt; 发送器应包含的运行指标，其过滤条件以列表形式表示，该列表中每一个过滤条件都应遵循如下规范：</description>
    </item>
    
    <item>
      <title>Python REPL</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/repls/python_shell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/repls/python_shell/</guid>
      <description>Python REPL # Flink附带了一个集成的交互式Python Shell。 它既能够运行在本地启动的local模式，也能够运行在集群启动的cluster模式下。 本地安装Flink，请看本地安装页面。 您也可以从源码安装Flink，请看从源码构建 Flink页面。
注意 Python Shell会调用“python”命令。关于Python执行环境的要求，请参考Python Table API环境安装。
你可以通过PyPi安装PyFlink，然后使用Python Shell:
# 安装 PyFlink $ python -m pip install apache-flink # 执行脚本 $ pyflink-shell.sh local 关于如何在一个Cluster集群上运行Python shell，可以参考启动章节介绍。
使用 # 当前Python shell支持Table API的功能。 在启动之后，Table Environment的相关内容将会被自动加载。 可以通过变量&amp;quot;bt_env&amp;quot;来使用BatchTableEnvironment，通过变量&amp;quot;st_env&amp;quot;来使用StreamTableEnvironment。
Table API # 下面是一个通过Python Shell 运行的简单示例: stream &amp;gt;&amp;gt;&amp;gt; import tempfile &amp;gt;&amp;gt;&amp;gt; import os &amp;gt;&amp;gt;&amp;gt; import shutil &amp;gt;&amp;gt;&amp;gt; sink_path = tempfile.gettempdir() + &amp;#39;/streaming.csv&amp;#39; &amp;gt;&amp;gt;&amp;gt; if os.path.exists(sink_path): ... if os.path.isfile(sink_path): ... os.</description>
    </item>
    
    <item>
      <title>RabbitMQ</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/rabbitmq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/rabbitmq/</guid>
      <description>RabbitMQ 连接器 # RabbitMQ 连接器的许可证 # Flink 的 RabbitMQ 连接器依赖了 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo;，它基于三种协议下发行：Mozilla Public License 1.1 (&amp;ldquo;MPL&amp;rdquo;)、GNU General Public License version 2 (&amp;ldquo;GPL&amp;rdquo;) 和 Apache License version 2 (&amp;ldquo;ASL&amp;rdquo;)。
Flink 自身既没有复用 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; 的代码，也没有将 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; 打二进制包。
如果用户发布的内容是基于 Flink 的 RabbitMQ 连接器的（进而重新发布了 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; ），那么一定要注意这可能会受到 Mozilla Public License 1.1 (&amp;ldquo;MPL&amp;rdquo;)、GNU General Public License version 2 (&amp;ldquo;GPL&amp;rdquo;)、Apache License version 2 (&amp;ldquo;ASL&amp;rdquo;) 协议的限制.
RabbitMQ 连接器 # 这个连接器可以访问 RabbitMQ 的数据流。使用这个连接器，需要在工程里添加下面的依赖：</description>
    </item>
    
    <item>
      <title>REST API</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/rest_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/rest_api/</guid>
      <description>REST API # Flink 具有监控 API ，可用于查询正在运行的作业以及最近完成的作业的状态和统计信息。该监控 API 被用于 Flink 自己的仪表盘，同时也可用于自定义监控工具。
该监控 API 是 REST-ful 风格的，可以接受 HTTP 请求并返回 JSON 格式的数据。
概览 # 该监控 API 由作为 JobManager 一部分运行的 web 服务器提供支持。默认情况下，该服务器监听 8081 端口，端口号可以通过修改 flink-conf.yaml 文件的 rest.port 进行配置。请注意，该监控 API 的 web 服务器和仪表盘的 web 服务器目前是相同的，因此在同一端口一起运行。不过，它们响应不同的 HTTP URL 。
在多个 JobManager 的情况下（为了高可用），每个 JobManager 将运行自己的监控 API 实例，当 JobManager 被选举成为集群 leader 时，该实例将提供已完成和正在运行作业的相关信息。
拓展 # 该 REST API 后端位于 flink-runtime 项目中。核心类是 org.apache.flink.runtime.webmonitor.WebMonitorEndpoint ，用来配置服务器和请求路由。
我们使用 Netty 和 Netty Router 库来处理 REST 请求和转换 URL 。选择该选项是因为这种组合具有轻量级依赖关系，并且 Netty HTTP 的性能非常好。</description>
    </item>
    
    <item>
      <title>窗口聚合</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-agg/</guid>
      <description>Window Aggregation # Window TVF Aggregation # Batch Streaming
Window aggregations are defined in the GROUP BY clause contains &amp;ldquo;window_start&amp;rdquo; and &amp;ldquo;window_end&amp;rdquo; columns of the relation applied Windowing TVF. Just like queries with regular GROUP BY clauses, queries with a group by window aggregation will compute a single result row per group.
SELECT ... FROM &amp;lt;windowed_table&amp;gt; -- relation applied windowing TVF GROUP BY window_start, window_end, ... Unlike other aggregations on continuous tables, window aggregation do not emit intermediate results but only a final result, the total aggregation at the end of the window.</description>
    </item>
    
    <item>
      <title>容错处理</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/learn-flink/fault_tolerance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/learn-flink/fault_tolerance/</guid>
      <description>通过状态快照实现容错处理 # State Backends # 由 Flink 管理的 keyed state 是一种分片的键/值存储，每个 keyed state 的工作副本都保存在负责该键的 taskmanager 本地中。另外，Operator state 也保存在机器节点本地。Flink 定期获取所有状态的快照，并将这些快照复制到持久化的位置，例如分布式文件系统。
如果发生故障，Flink 可以恢复应用程序的完整状态并继续处理，就如同没有出现过异常。
Flink 管理的状态存储在 state backend 中。Flink 有两种 state backend 的实现 &amp;ndash; 一种基于 RocksDB 内嵌 key/value 存储将其工作状态保存在磁盘上的，另一种基于堆的 state backend，将其工作状态保存在 Java 的堆内存中。这种基于堆的 state backend 有两种类型：FsStateBackend，将其状态快照持久化到分布式文件系统；MemoryStateBackend，它使用 JobManager 的堆保存状态快照。
名称 Working State 状态备份 快照 RocksDBStateBackend 本地磁盘（tmp dir） 分布式文件系统 全量 / 增量 支持大于内存大小的状态 经验法则：比基于堆的后端慢10倍 FsStateBackend JVM Heap 分布式文件系统 全量 快速，需要大的堆内存 受限制于 GC MemoryStateBackend JVM Heap JobManager JVM Heap 全量 适用于小状态（本地）的测试和实验 当使用基于堆的 state backend 保存状态时，访问和更新涉及在堆上读写对象。但是对于保存在 RocksDBStateBackend 中的对象，访问和更新涉及序列化和反序列化，所以会有更大的开销。但 RocksDB 的状态量仅受本地磁盘大小的限制。还要注意，只有 RocksDBStateBackend 能够进行增量快照，这对于具有大量变化缓慢状态的应用程序来说是大有裨益的。</description>
    </item>
    
    <item>
      <title>升级指南</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/mem_migration/</guid>
      <description>升级指南 # 在 1.10 和 1.11 版本中，Flink 分别对 TaskManager 和 JobManager 的内存配置方法做出了较大的改变。 部分配置参数被移除了，或是语义上发生了变化。 本篇升级指南将介绍如何将 Flink 1.9 及以前版本的 TaskManager 内存配置升级到 Flink 1.10 及以后版本， 以及如何将 Flink 1.10 及以前版本的 JobManager 内存配置升级到 Flink 1.11 及以后版本。
toc 注意： 请仔细阅读本篇升级指南。 使用原本的和新的内存配制方法可能会使内存组成部分具有截然不同的大小。 未经调整直接沿用 Flink 1.10 以前版本的 TaskManager 配置文件或 Flink 1.11 以前版本的 JobManager 配置文件，可能导致应用的行为、性能发生变化，甚至造成应用执行失败。 提示 在 1.10/1.11 版本之前，Flink 不要求用户一定要配置 TaskManager/JobManager 内存相关的参数，因为这些参数都具有默认值。 新的内存配置要求用户至少指定下列配置参数（或参数组合）的其中之一，否则 Flink 将无法启动。
TaskManager: JobManager: taskmanager.memory.flink.size jobmanager.memory.flink.size taskmanager.memory.process.size jobmanager.memory.process.size taskmanager.memory.task.heap.size 和 taskmanager.memory.managed.size jobmanager.memory.heap.size Flink 自带的默认 flink-conf.yaml 文件指定了 taskmanager.</description>
    </item>
    
    <item>
      <title>状态数据结构升级</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/</guid>
      <description>状态数据结构升级 # Apache Flink 流应用通常被设计为永远或者长时间运行。 与所有长期运行的服务一样，应用程序需要随着业务的迭代而进行调整。 应用所处理的数据 schema 也会随着进行变化。
此页面概述了如何升级状态类型的数据 schema 。 目前对不同类型的状态结构（ValueState、ListState 等）有不同的限制
请注意，此页面的信息只与 Flink 自己生成的状态序列化器相关 类型序列化框架。 也就是说，在声明状态时，状态描述符不可以配置为使用特定的 TypeSerializer 或 TypeInformation ， 在这种情况下，Flink 会推断状态类型的信息：
ListStateDescriptor&amp;lt;MyPojoType&amp;gt; descriptor = new ListStateDescriptor&amp;lt;&amp;gt;( &amp;#34;state-name&amp;#34;, MyPojoType.class); checkpointedState = getRuntimeContext().getListState(descriptor); 在内部，状态是否可以进行升级取决于用于读写持久化状态字节的序列化器。 简而言之，状态数据结构只有在其序列化器正确支持时才能升级。 这一过程是被 Flink 的类型序列化框架生成的序列化器透明处理的（下面 列出了当前的支持范围）。
如果你想要为你的状态类型实现自定义的 TypeSerializer 并且想要学习如何实现支持状态数据结构升级的序列化器， 可以参考 自定义状态序列化器。 本文档也包含一些用于支持状态数据结构升级的状态序列化器与 Flink 状态后端存储相互作用的必要内部细节。
升级状态数据结构 # 为了对给定的状态类型进行升级，你需要采取以下几个步骤：
对 Flink 流作业进行 savepoint 操作。 升级程序中的状态类型（例如：修改你的 Avro 结构）。 从 savepoint 恢复作业。当第一次访问状态数据时，Flink 会判断状态数据 schema 是否已经改变，并进行必要的迁移。 用来适应状态结构的改变而进行的状态迁移过程是自动发生的，并且状态之间是互相独立的。 Flink 内部是这样来进行处理的，首先会检查新的序列化器相对比之前的序列化器是否有不同的状态结构；如果有， 那么之前的序列化器用来读取状态数据字节到对象，然后使用新的序列化器将对象回写为字节。</description>
    </item>
    
    <item>
      <title>ANALYZE 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/analyze/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/analyze/</guid>
      <description>ANALYZE 语句 # ANALYZE 语句被用于为存在的表收集统计信息，并将统计信息写入该表的 catalog 中。当前版本中，ANALYZE 语句只支持 ANALYZE TABLE， 且只能由用户手动触发。
注意 现在, ANALYZE TABLE 只支持批模式（Batch Mode），且只能用于已存在的表， 如果表不存在或者是视图（View）则会报错。
执行 ANALYZE TABLE 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 ANALYZE TABLE 语句。
以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 ANALYZE TABLE 语句。
以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 ANALYZE TABLE 语句。
以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。
SQL CLI ANALYZE TABLE 语句可以在 SQL CLI 中执行。</description>
    </item>
    
    <item>
      <title>Checkpoints</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpoints/</guid>
      <description>Checkpoints # 概述 # Checkpoint 使 Flink 的状态具有良好的容错性，通过 checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。
参考 Checkpointing 查看如何在 Flink 程序中开启和配置 checkpoint。
要了解 checkpoints 和 savepoints 之间的区别，请参阅 checkpoints 与 savepoints。
保留 Checkpoint # Checkpoint 在默认的情况下仅用于恢复失败的作业，并不保留，当程序取消时 checkpoint 就会被删除。当然，你可以通过配置来保留 checkpoint，这些被保留的 checkpoint 在作业失败或取消时不会被清除。这样，你就可以使用该 checkpoint 来恢复失败的作业。
CheckpointConfig config = env.getCheckpointConfig(); config.setExternalizedCheckpointCleanup(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); ExternalizedCheckpointCleanup 配置项定义了当作业取消时，对作业 checkpoint 的操作：
ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：当作业取消时，保留作业的 checkpoint。注意，这种情况下，需要手动清除该作业保留的 checkpoint。 ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：当作业取消时，删除作业的 checkpoint。仅当作业失败时，作业的 checkpoint 才会被保留。 目录结构 # 与 savepoints 相似，checkpoint 由元数据文件、数据文件（与 state backend 相关）组成。可通过配置文件中 &amp;ldquo;state.checkpoints.dir&amp;rdquo; 配置项来指定元数据文件和数据文件的存储路径，另外也可以在代码中针对单个作业特别指定该配置项。
当前的 checkpoint 目录结构（由 FLINK-8531 引入）如下所示:</description>
    </item>
    
    <item>
      <title>Custom State Serialization</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/</guid>
      <description>Custom Serialization for Managed State # This page is targeted as a guideline for users who require the use of custom serialization for their state, covering how to provide a custom state serializer as well as guidelines and best practices for implementing serializers that allow state schema evolution.
If you&amp;rsquo;re simply using Flink&amp;rsquo;s own serializers, this page is irrelevant and can be ignored.
Using custom state serializers # When registering a managed operator or keyed state, a StateDescriptor is required to specify the state&amp;rsquo;s name, as well as information about the type of the state.</description>
    </item>
    
    <item>
      <title>DESCRIBE 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/describe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/describe/</guid>
      <description>DESCRIBE 语句 # DESCRIBE 语句用于描述表或视图的 schema。
执行 DESCRIBE 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，executeSql() 方法会返回给定表的 schema，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，executeSql() 方法会返回给定表的 schema，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，execute_sql() 方法会返回给定表的 schema，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。
SQL CLI DESCRIBE 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 DESCRIBE 语句。</description>
    </item>
    
    <item>
      <title>Google Cloud PubSub</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pubsub/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pubsub/</guid>
      <description>Google Cloud PubSub # 这个连接器可向 Google Cloud PubSub 读取与写入数据。添加下面的依赖来使用此连接器:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-pubsub&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 注意：此连接器最近才加到 Flink 里，还未接受广泛测试。 注意连接器目前还不是二进制发行版的一部分，添加依赖、打包配置以及集群运行信息请参考这里
Consuming or Producing PubSubMessages # 连接器可以接收和发送 Google PubSub 的信息。和 Google PubSub 一样，这个连接器能够保证至少一次的语义。
PubSub SourceFunction # PubSubSource 类的对象由构建类来构建: PubSubSource.newBuilder(...)
有多种可选的方法来创建 PubSubSource，但最低要求是要提供 Google Project、Pubsub 订阅和反序列化 PubSubMessages 的方法。
Example:
Java StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment(); DeserializationSchema&amp;lt;SomeObject&amp;gt; deserializer = (...); SourceFunction&amp;lt;SomeObject&amp;gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializer) .withProjectName(&amp;#34;project&amp;#34;) .withSubscriptionName(&amp;#34;subscription&amp;#34;) .build(); streamExecEnv.addSource(pubsubSource); 当前还不支持 PubSub 的 source functions pulls messages 和 push endpoints。</description>
    </item>
    
    <item>
      <title>Hadoop 兼容</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/hadoop_compatibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/hadoop_compatibility/</guid>
      <description>Hadoop 兼容 # Flink is compatible with Apache Hadoop MapReduce interfaces and therefore allows reusing code that was implemented for Hadoop MapReduce.
You can:
use Hadoop&amp;rsquo;s Writable data types in Flink programs. use any Hadoop InputFormat as a DataSource. use any Hadoop OutputFormat as a DataSink. use a Hadoop Mapper as FlatMapFunction. use a Hadoop Reducer as GroupReduceFunction. This document shows how to use existing Hadoop MapReduce code with Flink.</description>
    </item>
    
    <item>
      <title>Hybrid Source</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/hybridsource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/hybridsource/</guid>
      <description>Hybrid Source # HybridSource is a source that contains a list of concrete sources. It solves the problem of sequentially reading input from heterogeneous sources to produce a single input stream.
For example, a bootstrap use case may need to read several days worth of bounded input from S3 before continuing with the latest unbounded input from Kafka. HybridSource switches from FileSource to KafkaSource when the bounded file input finishes without interrupting the application.</description>
    </item>
    
    <item>
      <title>Ogg</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/ogg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/ogg/</guid>
      <description>Ogg Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Oracle GoldenGate (a.k.a ogg) 是一个实现异构 IT 环境间数据实时数据集成和复制的综合软件包。 该产品集支持高可用性解决方案、实时数据集成、事务更改数据捕获、运营和分析企业系统之间的数据复制、转换和验证。Ogg 为变更日志提供了统一的格式结构，并支持使用 JSON 序列化消息。
Flink 支持将 Ogg JSON 消息解析为 INSERT/UPDATE/DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等 Flink 还支持将 Flink SQL 中的 INSERT/UPDATE/DELETE 消息编码为 Ogg JSON 格式的消息, 输出到 Kafka 等存储中。 但需要注意, 目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息. 因此, Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Ogg 消息。</description>
    </item>
    
    <item>
      <title>分组聚合</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/group-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/group-agg/</guid>
      <description>Group Aggregation # Batch Streaming
Like most data systems, Apache Flink supports aggregate functions; both built-in and user-defined. User-defined functions must be registered in a catalog before use.
An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT, SUM, AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.
SELECT COUNT(*) FROM Orders For streaming queries, it is important to understand that Flink runs continuous queries that never terminate.</description>
    </item>
    
    <item>
      <title>文件系统</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/filesystem/</guid>
      <description>文件系统 SQL 连接器 # 此连接器提供了对 Flink FileSystem abstraction 支持的文件系统中分区文件的访问。
在 Flink 中包含了该文件系统连接器，不需要添加额外的依赖。相应的 jar 包可以在 Flink 工程项目的 /lib 目录下找到。从文件系统中读取或者向文件系统中写入行时，需要指定相应的 format。
文件系统连接器允许从本地或分布式文件系统进行读写。文件系统表可以定义为：
CREATE TABLE MyUserTable ( column_name1 INT, column_name2 STRING, ... part_name1 INT, part_name2 STRING ) PARTITIONED BY (part_name1, part_name2) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;filesystem&amp;#39;, -- 必选：指定连接器类型 &amp;#39;path&amp;#39; = &amp;#39;file:///path/to/whatever&amp;#39;, -- 必选：指定路径 &amp;#39;format&amp;#39; = &amp;#39;...&amp;#39;, -- 必选：文件系统连接器指定 format -- 有关更多详情，请参考 Table Formats &amp;#39;partition.default-name&amp;#39; = &amp;#39;...&amp;#39;, -- 可选：默认的分区名，动态分区模式下分区字段值是 null 或空字符串 -- 可选：该属性开启了在 sink 阶段通过动态分区字段来 shuffle 数据，该功能可以大大减少文件系统 sink 的文件数，但是可能会导致数据倾斜，默认值是 false &amp;#39;sink.</description>
    </item>
    
    <item>
      <title>Checkpointing under backpressure</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpointing_under_backpressure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpointing_under_backpressure/</guid>
      <description>Checkpointing under backpressure # 通常情况下，对齐 Checkpoint 的时长主要受 Checkpointing 过程中的同步和异步两个部分的影响。 然而，当 Flink 作业正运行在严重的背压下时，Checkpoint 端到端延迟的主要影响因子将会是传递 Checkpoint Barrier 到 所有的算子/子任务的时间。这在 checkpointing process) 的概述中有说明原因。并且可以通过高 alignment time and start delay metrics 观察到。 当这种情况发生并成为一个问题时，有三种方法可以解决这个问题：
消除背压源头，通过优化 Flink 作业，通过调整 Flink 或 JVM 参数，抑或是通过扩容。 减少 Flink 作业中缓冲在 In-flight 数据的数据量。 启用非对齐 Checkpoints。 这些选项并不是互斥的，可以组合在一起。本文档重点介绍后两个选项。 缓冲区 Debloating # Flink 1.14 引入了一个新的工具，用于自动控制在 Flink 算子/子任务之间缓冲的 In-flight 数据的数据量。缓冲区 Debloating 机 制可以通过将属性taskmanager.network.memory.buffer-debloat.enabled设置为true来启用。
此特性对对齐和非对齐 Checkpoint 都生效，并且在这两种情况下都能缩短 Checkpointing 的时间，不过 Debloating 的效果对于 对齐 Checkpoint 最明显。 当在非对齐 Checkpoint 情况下使用缓冲区 Debloating 时，额外的好处是 Checkpoint 大小会更小，并且恢复时间更快 (需要保存 和恢复的 In-flight 数据更少)。</description>
    </item>
    
    <item>
      <title>EXPLAIN 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/explain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/explain/</guid>
      <description>EXPLAIN 语句 # EXPLAIN 语句用于解释 query 或 INSERT 语句的执行逻辑，也用于优化 query 语句的查询计划。
执行 EXPLAIN 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，executeSql() 方法会返回解释结果，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，executeSql() 方法会返回解释结果，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，execute_sql() 方法会返回解释结果，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。
SQL CLI EXPLAIN 语句可以在 SQL CLI 中执行。</description>
    </item>
    
    <item>
      <title>HBase</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hbase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/hbase/</guid>
      <description>HBase SQL 连接器 # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Upsert Mode
HBase 连接器支持读取和写入 HBase 集群。本文档介绍如何使用 HBase 连接器基于 HBase 进行 SQL 查询。
HBase 连接器在 upsert 模式下运行，可以使用 DDL 中定义的主键与外部系统交换更新操作消息。但是主键只能基于 HBase 的 rowkey 字段定义。如果没有声明主键，HBase 连接器默认取 rowkey 作为主键。
依赖 # In order to use the HBase connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    
    <item>
      <title>Over聚合</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/over-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/over-agg/</guid>
      <description>Over Aggregation # Batch Streaming
OVER aggregates compute an aggregated value for every input row over a range of ordered rows. In contrast to GROUP BY aggregates, OVER aggregates do not reduce the number of result rows to a single row for every group. Instead OVER aggregates produce an aggregated value for every input row.
The following query computes for every order the sum of amounts of all orders for the same product that were received within one hour before the current order.</description>
    </item>
    
    <item>
      <title>Parquet</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/parquet/</guid>
      <description>Parquet 格式 # Format: Serialization Schema Format: Deserialization Schema
Apache Parquet 格式允许读写 Parquet 数据.
依赖 # In order to use the Parquet format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-parquet&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases. 如何创建基于 Parquet 格式的表 # 以下为用 Filesystem 连接器和 Parquet 格式创建表的示例，</description>
    </item>
    
    <item>
      <title>Pulsar</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pulsar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pulsar/</guid>
      <description>Apache Pulsar 连接器 # Flink 当前提供 Apache Pulsar Source 和 Sink 连接器，用户可以使用它从 Pulsar 读取数据，并保证每条数据只被处理一次。
添加依赖 # Pulsar Source 当前支持 Pulsar 2.8.1 之后的版本，但是 Pulsar Source 使用到了 Pulsar 的事务机制，建议在 Pulsar 2.9.2 及其之后的版本上使用 Pulsar Source 进行数据读取。
如果想要了解更多关于 Pulsar API 兼容性设计，可以阅读文档 PIP-72。
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-pulsar&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 Pulsar connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 Flink 的流连接器并不会放到发行文件里面一同发布，阅读此文档，了解如何将连接器添加到集群实例内。
Pulsar Source # Pulsar Source 基于 Flink 最新的批流一体 API 进行开发。 使用示例 # Pulsar Source 提供了 builder 类来构造 PulsarSource 实例。下面的代码实例使用 builder 类创建的实例会从 “persistent://public/default/my-topic” 的数据开始端进行消费。对应的 Pulsar Source 使用了 Exclusive（独占）的订阅方式消费消息，订阅名称为 my-subscription，并把消息体的二进制字节流以 UTF-8 的方式编码为字符串。</description>
    </item>
    
    <item>
      <title>Savepoints</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/savepoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/savepoints/</guid>
      <description>Savepoints # 什么是 Savepoint ？ # Savepoint 是依据 Flink checkpointing 机制所创建的流作业执行状态的一致镜像。 你可以使用 Savepoint 进行 Flink 作业的停止与重启、fork 或者更新。 Savepoint 由两部分组成：稳定存储（列入 HDFS，S3，&amp;hellip;) 上包含二进制文件的目录（通常很大），和元数据文件（相对较小）。 稳定存储上的文件表示作业执行状态的数据镜像。 Savepoint 的元数据文件以（相对路径）的形式包含（主要）指向作为 Savepoint 一部分的稳定存储上的所有文件的指针。
注意: 为了允许程序和 Flink 版本之间的升级，请务必查看以下有关分配算子 ID 的部分 。 为了正确使用 savepoints，了解 checkpoints 与 savepoints 之间的区别非常重要，checkpoints 与 savepoints 中对此进行了描述。
分配算子 ID # 强烈建议你按照本节所述调整你的程序，以便将来能够升级你的程序。主要通过 uid(String) 方法手动指定算子 ID 。这些 ID 将用于恢复每个算子的状态。
DataStream&amp;lt;String&amp;gt; stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(&amp;#34;source-id&amp;#34;) // ID for the source operator .</description>
    </item>
    
    <item>
      <title>本地执行</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/local_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/local_execution/</guid>
      <description>本地执行 # Flink can run on a single machine, even in a single Java Virtual Machine. This allows users to test and debug Flink programs locally. This section gives an overview of the local execution mechanisms.
The local environments and executors allow you to run Flink programs in a local Java Virtual Machine, or with within any JVM as part of existing programs. Most examples can be launched locally by simply hitting the &amp;ldquo;Run&amp;rdquo; button of your IDE.</description>
    </item>
    
    <item>
      <title>Checkpoints 与 Savepoints</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpoints_vs_savepoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/checkpoints_vs_savepoints/</guid>
      <description>Checkpoints 与 Savepoints # 概述 # 从概念上讲，Flink 的 savepoints 与 checkpoints 的不同之处类似于传统数据库系统中的备份与恢复日志之间的差异。
Checkpoints 的主要目的是为意外失败的作业提供恢复机制。 Checkpoint 的生命周期 由 Flink 管理， 即 Flink 创建，管理和删除 checkpoint - 无需用户交互。 由于 checkpoint 被经常触发，且被用于作业恢复，所以 Checkpoint 的实现有两个设计目标：i）轻量级创建和 ii）尽可能快地恢复。 可能会利用某些特定的属性来达到这个目标，例如， 作业的代码在执行尝试时不会改变。
在用户终止作业后，会自动删除 Checkpoint（除非明确配置为保留的 Checkpoint）。 Checkpoint 以状态后端特定的（原生的）数据格式存储（有些状态后端可能是增量的）。 尽管 savepoints 在内部使用与 checkpoints 相同的机制创建，但它们在概念上有所不同，并且生成和恢复的成本可能会更高一些。Savepoints的设计更侧重于可移植性和操作灵活性，尤其是在 job 变更方面。Savepoint 的用例是针对计划中的、手动的运维。例如，可能是更新你的 Flink 版本，更改你的作业图等等。
Savepoint 仅由用户创建、拥有和删除。这意味着 Flink 在作业终止后和恢复后都不会删除 savepoint。 Savepoint 以状态后端独立的（标准的）数据格式存储（注意：从 Flink 1.15 开始，savepoint 也可以以后端特定的原生格式存储，这种格式创建和恢复速度更快，但有一些限制）。 功能和限制 # 下表概述了各种类型的 savepoint 和 checkpoint 的功能和限制。
✓ - Flink 完全支持这种类型的快照 x - Flink 不支持这种类型的快照 !</description>
    </item>
    
    <item>
      <title>JDBC</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/jdbc/</guid>
      <description>JDBC Connector # 该连接器可以向 JDBC 数据库写入数据。
添加下面的依赖以便使用该连接器（同时添加 JDBC 驱动）：
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-jdbc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 注意该连接器目前还 不是 二进制发行版的一部分，如何在集群中运行请参考 这里。
已创建的 JDBC Sink 能够保证至少一次的语义。 更有效的精确执行一次可以通过 upsert 语句或幂等更新实现。
用法示例： Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .fromElements(...) .addSink(JdbcSink.sink( &amp;#34;insert into books (id, title, author, price, qty) values (?,?,?,?,?)&amp;#34;, (ps, t) -&amp;gt; { ps.setInt(1, t.id); ps.setString(2, t.title); ps.setString(3, t.author); ps.setDouble(4, t.price); ps.setInt(5, t.qty); }, new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withUrl(getDbMetadata().getUrl()) .withDriverName(getDbMetadata().getDriverClass()) .build())); env.execute(); Python env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Join</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/joins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/joins/</guid>
      <description>Joins # Batch Streaming
Flink SQL supports complex and flexible join operations over dynamic tables. There are several different types of joins to account for the wide variety of semantics queries may require.
By default, the order of joins is not optimized. Tables are joined in the order in which they are specified in the FROM clause. You can tweak the performance of your join queries, by listing the tables with the lowest update frequency first and the tables with the highest update frequency last.</description>
    </item>
    
    <item>
      <title>Orc</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/orc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/orc/</guid>
      <description>Orc Format # Format: Serialization Schema Format: Deserialization Schema
Apache Orc Format 允许读写 ORC 数据。
依赖 # In order to use the ORC format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-orc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases. 如何用 Orc 格式创建一个表格 # 下面是一个用 Filesystem connector 和 Orc format 创建表格的例子</description>
    </item>
    
    <item>
      <title>USE 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/use/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/use/</guid>
      <description>USE 语句 # USE 语句用来设置当前的 catalog 或者 database。
运行一个 USE 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 USE 语句。 若 USE 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 USE 语句。 若 USE 操作执行成功，executeSql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 USE 语句。 若 USE 操作执行成功，execute_sql() 方法返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
SQL CLI 可以在 SQL CLI 中执行 USE 语句。</description>
    </item>
    
    <item>
      <title>窗口关联</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-join/</guid>
      <description>Window Join # Batch Streaming
A window join adds the dimension of time into the join criteria themselves. In doing so, the window join joins the elements of two streams that share a common key and are in the same window. The semantic of window join is same to the DataStream window join
For streaming queries, unlike other joins on continuous tables, window join does not emit intermediate results but only emits final results at the end of the window.</description>
    </item>
    
    <item>
      <title>高级配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/advanced/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/configuration/advanced/</guid>
      <description>高级配置主题 # Flink 依赖剖析 # Flink 自身由一组类和依赖项组成，这些共同构成了 Flink 运行时的核心，在 Flink 应用程序启动时必须存在，会提供诸如通信协调、网络管理、检查点、容错、API、算子（如窗口）、资源管理等领域的服务。
这些核心类和依赖项都打包在 flink-dist.jar，可以在下载的发行版 /lib 文件夹中找到，也是 Flink 容器镜像的基础部分。您可以将其近似地看作是包含 String 和 List 等公用类的 Java 核心库。
为了保持核心依赖项尽可能小并避免依赖冲突，Flink Core Dependencies 不包含任何连接器或库（如 CEP、SQL、ML），以避免在类路径中有过多的类和依赖项。
Flink 发行版的 /lib 目录里还有包括常用模块在内的各种 JAR 文件，例如 执行 Table 作业的必需模块 、一组连接器和 format。默认情况下会自动加载，若要禁止加载只需将它们从 classpath 中的 /lib 目录中删除即可。
Flink 还在 /opt 文件夹下提供了额外的可选依赖项，可以通过移动这些 JAR 文件到 /lib 目录来启用这些依赖项。
有关类加载的更多细节，请查阅 Flink 类加载。
Scala 版本 # 不同的 Scala 版本二进制不兼容，所有（传递地）依赖于 Scala 的 Flink 依赖项都以它们构建的 Scala 版本为后缀（如 flink-streaming-scala_2.12）。
如果您只使用 Flink 的 Java API，您可以使用任何 Scala 版本。如果您使用 Flink 的 Scala API，则需要选择与应用程序的 Scala 匹配的 Scala 版本。</description>
    </item>
    
    <item>
      <title>升级应用程序和 Flink 版本</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/upgrading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/upgrading/</guid>
      <description>升级应用程序和 Flink 版本 # Flink DataStream 程序通常设计为长时间运行，例如数周、数月甚至数年。与所有长时间运行的服务一样，Flink 流式应用程序需要维护，包括修复错误、实施改进或将应用程序迁移到更高版本的 Flink 集群。
本文档介绍了如何更新 Flink 流式应用程序以及如何将正在运行的流式应用程序迁移到不同的 Flink 集群。
重启流式应用程序 # 升级流式应用程序或将应用程序迁移到不同集群的操作线基于 Flink 的 Savepoint 功能。Savepoint 是应用程序在特定时间点的状态的一致快照。 有两种方法可以从正在运行的流应用程序中获取 savepoint。
获取 Savepoint 并继续处理。 &amp;gt; ./bin/flink savepoint &amp;lt;jobID&amp;gt; [ Savepoint 的路径] 建议定期获取 Savepoint ，以便能够从之前的时间点重新启动应用程序。
作获取 Savepoint 并停止应用程序。 &amp;gt; ./bin/flink cancel -s [ Savepoint 的路径] &amp;lt;jobID&amp;gt; 这意味着应用程序在 Savepoint 完成后立即取消，即在 Savepoint 之后不进行其他 checkpoint。
给定从应用程序获取的 Savepoint ，可以从该 Savepoint 启动相同或兼容的应用程序（请参阅下面的 应用程序状态兼容性 部分）。从 Savepoint 启动应用程序意味着其算子的状态被初始化为 Savepoint 中保存的算子状态。这是通过使用 Savepoint 启动应用程序来完成的。
&amp;gt; .</description>
    </item>
    
    <item>
      <title>DataSet Connectors</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/dataset/</guid>
      <description>DataSet Connectors # TOC Reading from and writing to file systems # The Apache Flink project supports multiple file systems that can be used as backing stores for input and output connectors.
Connecting to other systems using Input/OutputFormat wrappers for Hadoop # Apache Flink allows users to access many different systems as data sources or sinks. The system is designed for very easy extensibility. Similar to Apache Hadoop, Flink has the concept of so called InputFormats and OutputFormats.</description>
    </item>
    
    <item>
      <title>Raw</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/raw/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/raw/</guid>
      <description>Raw Format # Format: Serialization Schema Format: Deserialization Schema
Raw format 允许读写原始（基于字节）值作为单个列。
注意: 这种格式将 null 值编码成 byte[] 类型的 null。这样在 upsert-kafka 中使用时可能会有限制，因为 upsert-kafka 将 null 值视为 墓碑消息（在键上删除）。因此，如果该字段可能具有 null 值，我们建议避免使用 upsert-kafka 连接器和 raw format 作为 value.format。
Raw format 连接器是内置的。
示例 # 例如，你可能在 Kafka 中具有原始日志数据，并希望使用 Flink SQL 读取和分析此类数据。
47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] &amp;#34;GET /?p=1 HTTP/2.0&amp;#34; 200 5316 &amp;#34;https://domain.com/?p=1&amp;#34; &amp;#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36&amp;#34; &amp;#34;2.75&amp;#34; 下面的代码创建了一张表，使用 raw format 以 UTF-8 编码的形式从中读取（也可以写入）底层的 Kafka topic 作为匿名字符串值：</description>
    </item>
    
    <item>
      <title>SHOW 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/show/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/show/</guid>
      <description>SHOW 语句 # SHOW 语句用于列出其相应父对象中的对象，例如 catalog、database、table 和 view、column、function 和 module。有关详细信息和其他选项，请参见各个命令。
SHOW CREATE 语句用于打印给定对象的创建 DDL 语句。当前的 SHOW CREATE 语句仅在打印给定表和视图的 DDL 语句时可用。
目前 Flink SQL 支持下列 SHOW 语句：
SHOW CATALOGS SHOW CURRENT CATALOG SHOW DATABASES SHOW CURRENT DATABASE SHOW TABLES SHOW CREATE TABLE SHOW COLUMNS SHOW VIEWS SHOW CREATE VIEW SHOW FUNCTIONS SHOW MODULES SHOW FULL MODULES SHOW JARS 执行 SHOW 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 SHOW 语句。 若 SHOW 操作执行成功，executeSql() 方法返回所有对象，否则会抛出异常。</description>
    </item>
    
    <item>
      <title>词汇表</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/concepts/glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/concepts/glossary/</guid>
      <description>词汇表 # Flink Application Cluster # Flink Application 集群是专用的 Flink Cluster，仅从 Flink Application 执行 Flink Jobs。 Flink Cluster 的寿命与 Flink Application 的寿命有关。
Flink Job Cluster # Flink Job 集群是专用的 Flink Cluster，仅执行一个 Flink Job。 Flink Cluster 的寿命与 Flink Job 的寿命有关。
Flink Cluster # 一般情况下，Flink 集群是由一个 Flink JobManager 和一个或多个 Flink TaskManager 进程组成的分布式系统。
Event # Event 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。
ExecutionGraph # 见 Physical Graph。
Function # Function 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。</description>
    </item>
    
    <item>
      <title>集合操作</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/set-ops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/set-ops/</guid>
      <description>Set Operations # Batch Streaming
UNION # UNION and UNION ALL return the rows that are found in either table. UNION takes only distinct rows while UNION ALL does not remove duplicates from the result rows.
Flink SQL&amp;gt; create view t1(s) as values (&amp;#39;c&amp;#39;), (&amp;#39;a&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;c&amp;#39;); Flink SQL&amp;gt; create view t2(s) as values (&amp;#39;d&amp;#39;), (&amp;#39;e&amp;#39;), (&amp;#39;a&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;b&amp;#39;); Flink SQL&amp;gt; (SELECT s FROM t1) UNION (SELECT s FROM t2); +---+ | s| +---+ | c| | a| | b| | d| | e| +---+ Flink SQL&amp;gt; (SELECT s FROM t1) UNION ALL (SELECT s FROM t2); +---+ | c| +---+ | c| | a| | b| | b| | c| | d| | e| | a| | b| | b| +---+ INTERSECT # INTERSECT and INTERSECT ALL return the rows that are found in both tables.</description>
    </item>
    
    <item>
      <title>流式聚合</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/tuning/</guid>
      <description>Performance Tuning # SQL 是数据分析中使用最广泛的语言。Flink Table API 和 SQL 使用户能够以更少的时间和精力定义高效的流分析应用程序。此外，Flink Table API 和 SQL 是高效优化过的，它集成了许多查询优化和算子优化。但并不是所有的优化都是默认开启的，因此对于某些工作负载，可以通过打开某些选项来提高性能。
在这一页，我们将介绍一些实用的优化选项以及流式聚合的内部原理，它们在某些情况下能带来很大的提升。
The streaming aggregation optimizations mentioned in this page are all supported for Group Aggregations and Window TVF Aggregations now. MiniBatch 聚合 # 默认情况下，无界聚合算子是逐条处理输入的记录，即：（1）从状态中读取累加器，（2）累加/撤回记录至累加器，（3）将累加器写回状态，（4）下一条记录将再次从（1）开始处理。这种处理模式可能会增加 StateBackend 开销（尤其是对于 RocksDB StateBackend ）。此外，生产中非常常见的数据倾斜会使这个问题恶化，并且容易导致 job 发生反压。
MiniBatch 聚合的核心思想是将一组输入的数据缓存在聚合算子内部的缓冲区中。当输入的数据被触发处理时，每个 key 只需一个操作即可访问状态。这样可以大大减少状态开销并获得更好的吞吐量。但是，这可能会增加一些延迟，因为它会缓冲一些记录而不是立即处理它们。这是吞吐量和延迟之间的权衡。
下图说明了 mini-batch 聚合如何减少状态操作。
默认情况下，对于无界聚合算子来说，mini-batch 优化是被禁用的。开启这项优化，需要设置选项 table.exec.mini-batch.enabled、table.exec.mini-batch.allow-latency 和 table.exec.mini-batch.size。更多详细信息请参见配置页面。
MiniBatch optimization is always enabled for Window TVF Aggregation, regardless of the above configuration.</description>
    </item>
    
    <item>
      <title>生产就绪情况核对清单</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/production_ready/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/production_ready/</guid>
      <description>生产就绪情况核对清单 # The production readiness checklist provides an overview of configuration options that should be carefully considered before bringing an Apache Flink job into production. While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs.
Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale.</description>
    </item>
    
    <item>
      <title>数据源</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/sources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/sources/</guid>
      <description>数据源 # 当前页面所描述的是 Flink 的 Data Source API 及其背后的概念和架构。 如果您对 Flink 中的 Data Source 如何工作感兴趣，或者您想实现一个新的数据 source，请阅读本文。
如果您正在寻找预定义的 source 连接器，请查看连接器文档.
Data Source 原理 # 核心组件
一个数据 source 包括三个核心组件：分片（Splits）、分片枚举器（SplitEnumerator） 以及 源阅读器（SourceReader）。
分片（Split） 是对一部分 source 数据的包装，如一个文件或者日志分区。分片是 source 进行任务分配和数据并行读取的基本粒度。
源阅读器（SourceReader） 会请求分片并进行处理，例如读取分片所表示的文件或日志分区。SourceReader 在 TaskManagers 上的 SourceOperators 并行运行，并产生并行的事件流/记录流。
分片枚举器（SplitEnumerator） 会生成分片并将它们分配给 SourceReader。该组件在 JobManager 上以单并行度运行，负责对未分配的分片进行维护，并以均衡的方式将其分配给 reader。
Source 类作为API入口，将上述三个组件结合在了一起。
流处理和批处理的统一
Data Source API 以统一的方式对无界流数据和有界批数据进行处理。
事实上，这两种情况之间的区别是非常小的：在有界/批处理情况中，枚举器生成固定数量的分片，而且每个分片都必须是有限的。但在无界流的情况下，则无需遵从限制，也就是分片大小可以不是有限的，或者枚举器将不断生成新的分片。
示例 # 以下是一些简化的概念示例，以说明在流和批处理情况下 data source 组件如何交互。
请注意，以下内容并没有准确地描述出 Kafka 和 File source 的工作方式，因为出于说明的目的，部分内容被简化处理。
有界 File Source</description>
    </item>
    
    <item>
      <title>文件系统</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/internals/filesystems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/internals/filesystems/</guid>
      <description>文件系统 # Flink has its own file system abstraction via the org.apache.flink.core.fs.FileSystem class. This abstraction provides a common set of operations and minimal guarantees across various types of file system implementations.
The FileSystem&amp;rsquo;s set of available operations is quite limited, in order to support a wide range of file systems. For example, appending to or mutating existing files is not supported.
File systems are identified by a file system scheme, such as file://, hdfs://, etc.</description>
    </item>
    
    <item>
      <title>执行配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/execution_configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/execution_configuration/</guid>
      <description>执行配置 # StreamExecutionEnvironment 包含了 ExecutionConfig，它允许在运行时设置作业特定的配置值。要更改影响所有作业的默认值，请参阅配置。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ExecutionConfig executionConfig = env.getConfig(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment var executionConfig = env.getConfig Python env = StreamExecutionEnvironment.get_execution_environment() execution_config = env.get_config() 以下是可用的配置选项：（默认为粗体）
setClosureCleanerLevel()。closure cleaner 的级别默认设置为 ClosureCleanerLevel.RECURSIVE。closure cleaner 删除 Flink 程序中对匿名 function 的调用类的不必要引用。禁用 closure cleaner 后，用户的匿名 function 可能正引用一些不可序列化的调用类。这将导致序列化器出现异常。可设置的值是： NONE：完全禁用 closure cleaner ，TOP_LEVEL：只清理顶级类而不递归到字段中，RECURSIVE：递归清理所有字段。
getParallelism() / setParallelism(int parallelism)。为作业设置默认的并行度。
getMaxParallelism() / setMaxParallelism(int parallelism)。为作业设置默认的最大并行度。此设置决定最大并行度并指定动态缩放的上限。
getNumberOfExecutionRetries() / setNumberOfExecutionRetries(int numberOfExecutionRetries)。设置失败任务重新执行的次数。值为零会有效地禁用容错。-1 表示使用系统默认值（在配置中定义）。该配置已弃用，请改用重启策略 。
getExecutionRetryDelay() / setExecutionRetryDelay(long executionRetryDelay)。设置系统在作业失败后重新执行之前等待的延迟（以毫秒为单位）。在 TaskManagers 上成功停止所有任务后，开始计算延迟，一旦延迟过去，任务会被重新启动。此参数对于延迟重新执行的场景很有用，当尝试重新执行作业时，由于相同的问题，作业会立刻再次失败，该参数便于作业再次失败之前让某些超时相关的故障完全浮出水面（例如尚未完全超时的断开连接）。此参数仅在执行重试次数为一次或多次时有效。该配置已被弃用，请改用重启策略 。</description>
    </item>
    
    <item>
      <title>自定义序列化器</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serializers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serializers/</guid>
      <description>为你的 Flink 程序注册自定义序列化器 # 如果在 Flink 程序中使用了 Flink 类型序列化器无法进行序列化的用户自定义类型，Flink 会回退到通用的 Kryo 序列化器。 可以使用 Kryo 注册自己的序列化器或序列化系统，比如 Google Protobuf 或 Apache Thrift。 使用方法是在 Flink 程序中的 ExecutionConfig 注册类类型以及序列化器。
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 为类型注册序列化器类 env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, MyCustomSerializer.class); // 为类型注册序列化器实例 MySerializer mySerializer = new MySerializer(); env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, mySerializer); 需要确保你的自定义序列化器继承了 Kryo 的序列化器类。 对于 Google Protobuf 或 Apache Thrift，这一点已经为你做好了：
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 使用 Kryo 注册 Google Protobuf 序列化器 env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class); // 注册 Apache Thrift 序列化器为标准序列化器 // TBaseSerializer 需要初始化为默认的 kryo 序列化器 env.</description>
    </item>
    
    <item>
      <title>LOAD 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/load/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/load/</guid>
      <description>LOAD 语句 # LOAD 语句用于加载内置的或用户自定义的模块。
执行 LOAD 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，executeSql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，executeSql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，execute_sql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
SQL CLI LOAD 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 LOAD 语句。</description>
    </item>
    
    <item>
      <title>ORDER BY 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/orderby/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/orderby/</guid>
      <description>ORDER BY 语句 # Batch Streaming
ORDER BY 子句使结果行根据指定的表达式进行排序。 如果两行根据最左边的表达式相等，则根据下一个表达式进行比较，依此类推。 如果根据所有指定的表达式它们相等，则它们以与实现相关的顺序返回。
在流模式下运行时，表的主要排序顺序必须按时间属性升序。 所有后续的 orders 都可以自由选择。 但是批处理模式没有这个限制。
SELECT * FROM Orders ORDER BY order_time, order_id Back to top</description>
    </item>
    
    <item>
      <title>State Backends</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/state_backends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/state_backends/</guid>
      <description>State Backends # 用 Data Stream API 编写的程序通常以各种形式保存状态：
在 Window 触发之前要么收集元素、要么聚合 转换函数可以使用 key/value 格式的状态接口来存储状态 转换函数可以实现 CheckpointedFunction 接口，使其本地变量具有容错能力 另请参阅 Streaming API 指南中的 状态部分 。
在启动 CheckPoint 机制时，状态会随着 CheckPoint 而持久化，以防止数据丢失、保障恢复时的一致性。 状态内部的存储格式、状态在 CheckPoint 时如何持久化以及持久化在哪里均取决于选择的 State Backend。
可用的 State Backends # Flink 内置了以下这些开箱即用的 state backends ：
HashMapStateBackend EmbeddedRocksDBStateBackend 如果不设置，默认使用 HashMapStateBackend。
HashMapStateBackend # 在 HashMapStateBackend 内部，数据以 Java 对象的形式存储在堆中。 Key/value 形式的状态和窗口算子会持有一个 hash table，其中存储着状态值、触发器。
HashMapStateBackend 的适用场景：
有较大 state，较长 window 和较大 key/value 状态的 Job。 所有的高可用场景。 建议同时将 managed memory 设为0，以保证将最大限度的内存分配给 JVM 上的用户代码。</description>
    </item>
    
    <item>
      <title>DataGen</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/datagen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/datagen/</guid>
      <description>DataGen SQL 连接器 # Scan Source: 有界 Scan Source: 无界
DataGen 连接器允许按数据生成规则进行读取。
DataGen 连接器可以使用计算列语法。 这使您可以灵活地生成记录。
DataGen 连接器是内置的。
注意 不支持复杂类型: Array，Map，Row。 请用计算列构造这些类型。
怎么创建一个 DataGen 的表 # 表的有界性：当表中字段的数据全部生成完成后，source 就结束了。 因此，表的有界性取决于字段的有界性。
每个列，都有两种生成数据的方法：
随机生成器是默认的生成器，您可以指定随机生成的最大和最小值。char、varchar、binary、varbinary, string （类型）可以指定长度。它是无界的生成器。
序列生成器，您可以指定序列的起始和结束值。它是有界的生成器，当序列数字达到结束值，读取结束。
CREATE TABLE datagen ( f_sequence INT, f_random INT, f_random_str STRING, ts AS localtimestamp, WATERMARK FOR ts AS ts ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;datagen&amp;#39;, -- optional options -- &amp;#39;rows-per-second&amp;#39;=&amp;#39;5&amp;#39;, &amp;#39;fields.f_sequence.kind&amp;#39;=&amp;#39;sequence&amp;#39;, &amp;#39;fields.f_sequence.start&amp;#39;=&amp;#39;1&amp;#39;, &amp;#39;fields.f_sequence.end&amp;#39;=&amp;#39;1000&amp;#39;, &amp;#39;fields.f_random.min&amp;#39;=&amp;#39;1&amp;#39;, &amp;#39;fields.f_random.max&amp;#39;=&amp;#39;1000&amp;#39;, &amp;#39;fields.f_random_str.length&amp;#39;=&amp;#39;10&amp;#39; ) 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必须 (none) String 指定要使用的连接器，这里是 &#39;datagen&#39;。 rows-per-second 可选 10000 Long 每秒生成的行数，用以控制数据发出速率。 fields.</description>
    </item>
    
    <item>
      <title>LIMIT 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/limit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/limit/</guid>
      <description>LIMIT 语句 # Batch LIMIT 子句限制 SELECT 语句返回的行数。 通常，此子句与 ORDER BY 结合使用，以确保结果是确定性的。
以下示例选择 Orders 表中的前 3 行。
SELECT * FROM Orders ORDER BY orderTime LIMIT 3 Back to top</description>
    </item>
    
    <item>
      <title>UNLOAD 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/unload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/unload/</guid>
      <description>UNLOAD 语句 # UNLOAD 语句用于卸载内置的或用户自定义的模块。
执行 UNLOAD 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，executeSql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，executeSql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，execute_sql() 方法会返回 &amp;lsquo;OK&amp;rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
SQL CLI UNLOAD 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 UNLOAD 语句。</description>
    </item>
    
    <item>
      <title>大状态与 Checkpoint 调优</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/large_state_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/large_state_tuning/</guid>
      <description>大状态与 Checkpoint 调优 # 本文提供了如何配置和调整使用大状态的应用程序指南。
概述 # Flink 应用要想在大规模场景下可靠地运行，必须要满足如下两个条件：
应用程序需要能够可靠地创建 checkpoints。
在应用故障后，需要有足够的资源追赶数据输入流。
第一部分讨论如何大规模获得良好性能的 checkpoints。 后一部分解释了一些关于要规划使用多少资源的最佳实践。
监控状态和 Checkpoints # 监控 checkpoint 行为最简单的方法是通过 UI 的 checkpoint 部分。 监控 Checkpoint 的文档说明了如何查看可用的 checkpoint 指标。
这两个指标（均通过 Task 级别 Checkpointing 指标 展示） 以及在 监控 Checkpoint)中，当看 checkpoint 详细信息时，特别有趣的是:
算子收到第一个 checkpoint barrier 的时间。当触发 checkpoint 的耗费时间一直很高时，这意味着 checkpoint barrier 需要很长时间才能从 source 到达 operators。 这通常表明系统处于反压下运行。
Alignment Duration，为处理第一个和最后一个 checkpoint barrier 之间的时间。在 unaligned checkpoints 下，exactly-once 和 at-least-once checkpoints 的 subtasks 处理来自上游 subtasks 的所有数据，且没有任何中断。 然而，对于 aligned exactly-once checkpoints，已经收到 checkpoint barrier 的通道被阻止继续发送数据，直到所有剩余的通道都赶上并接收它们的 checkpoint barrier（对齐时间）。</description>
    </item>
    
    <item>
      <title>集群执行</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/cluster_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/cluster_execution/</guid>
      <description>集群执行 # Flink 程序可以分布式运行在多机器集群上。有两种方式可以将程序提交到集群上执行：
命令行界面（Interface） # 命令行界面使你可以将打包的程序（JARs）提交到集群（或单机设置）。
有关详细信息，请参阅命令行界面文档。
远程环境（Remote Environment） # 远程环境使你可以直接在集群上执行 Flink Java 程序。远程环境指向你要执行程序的集群。
Maven Dependency # 如果将程序作为 Maven 项目开发，则必须添加 flink-clients 模块的依赖：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-clients&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 示例 # 下面演示了 RemoteEnvironment 的用法：
public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment .createRemoteEnvironment(&amp;#34;flink-jobmanager&amp;#34;, 8081, &amp;#34;/home/user/udfs.jar&amp;#34;); DataSet&amp;lt;String&amp;gt; data = env.readTextFile(&amp;#34;hdfs://path/to/file&amp;#34;); data .filter(new FilterFunction&amp;lt;String&amp;gt;() { public boolean filter(String value) { return value.startsWith(&amp;#34;http://&amp;#34;); } }) .writeAsText(&amp;#34;hdfs://path/to/result&amp;#34;); env.execute(); } 请注意，该程序包含用户自定义代码，因此需要一个带有附加代码类的 JAR 文件。远程环境的构造函数使用 JAR 文件的路径进行构造。</description>
    </item>
    
    <item>
      <title>Print</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/print/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/print/</guid>
      <description>Print SQL 连接器 # Sink Print 连接器允许将每一行写入标准输出流或者标准错误流。
设计目的：
简单的流作业测试。 对生产调试带来极大便利。 四种 format 选项：
打印内容 条件 1 条件 2 标识符:任务 ID&gt; 输出数据 需要提供前缀打印标识符 parallelism &gt; 1 标识符&gt; 输出数据 需要提供前缀打印标识符 parallelism == 1 任务 ID&gt; 输出数据 不需要提供前缀打印标识符 parallelism &gt; 1 输出数据 不需要提供前缀打印标识符 parallelism == 1 输出字符串格式为 &amp;ldquo;$row_kind(f0,f1,f2&amp;hellip;)&amp;quot;，row_kind是一个 RowKind 类型的短字符串，例如：&amp;quot;+I(1,1)&amp;quot;。
Print 连接器是内置的。
注意 在任务运行时使用 Print Sinks 打印记录，你需要注意观察任务日志。
如何创建一张基于 Print 的表 # CREATE TABLE print_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;print&amp;#39; ) 或者，也可以通过 LIKE子句 基于已有表的结构去创建新表。</description>
    </item>
    
    <item>
      <title>SET 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/set/</guid>
      <description>SET 语句 # SET 语句用于修改配置或展示配置。
执行 SET 语句 # SQL CLI SET 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 SET 语句。
SQL CLI Flink SQL&amp;gt; SET &amp;#39;table.local-time-zone&amp;#39; = &amp;#39;Europe/Berlin&amp;#39;; [INFO] Session property has been set. Flink SQL&amp;gt; SET; &amp;#39;table.local-time-zone&amp;#39; = &amp;#39;Europe/Berlin&amp;#39; Syntax # SET (&amp;#39;key&amp;#39; = &amp;#39;value&amp;#39;)? 如果没有指定 key 和 value，它仅仅打印所有属性。否则，它会为 key 设置指定的 value 值。
Back to top</description>
    </item>
    
    <item>
      <title>Top-N</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/topn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/topn/</guid>
      <description>Top-N # Batch Streaming
Top-N queries ask for the N smallest or largest values ordered by columns. Both smallest and largest values sets are considered Top-N queries. Top-N queries are useful in cases where the need is to display only the N bottom-most or the N top- most records from batch/streaming table on a condition. This result set can be used for further analysis.
Flink uses the combination of a OVER window clause and a filter condition to express a Top-N query.</description>
    </item>
    
    <item>
      <title>BlackHole</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/blackhole/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/blackhole/</guid>
      <description> BlackHole SQL 连接器 # Sink: Bounded Sink: UnBounded
BlackHole 连接器允许接收所有输入记录。它被设计用于：
高性能测试。 UDF 输出，而不是实质性 sink。 就像类 Unix 操作系统上的 /dev/null。
BlackHole 连接器是内置的。
如何创建 BlackHole 表 # CREATE TABLE blackhole_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;blackhole&amp;#39; ); 或者，可以基于现有模式使用 LIKE 子句 创建。
CREATE TABLE blackhole_table WITH (&amp;#39;connector&amp;#39; = &amp;#39;blackhole&amp;#39;) LIKE source_table (EXCLUDING ALL) 连接器选项 # 选项 是否必要 默认值 类型 描述 connector 必要 (none) String 指定需要使用的连接器，此处应为‘blackhole’。 </description>
    </item>
    
    <item>
      <title>RESET 语句</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/reset/</guid>
      <description>RESET 语句 # RESET 语句用于将配置重置为默认值。
执行 RESET 语句 # SQL CLI RESET 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 RESET 语句。
SQL CLI Flink SQL&amp;gt; RESET &amp;#39;table.planner&amp;#39;; [INFO] Session property has been reset. Flink SQL&amp;gt; RESET; [INFO] All session properties have been set to their default values. Syntax # RESET (&amp;#39;key&amp;#39;)? 如果未指定 key，则将所有属性重置为默认值。否则，将指定的 key 重置为默认值。
Back to top</description>
    </item>
    
    <item>
      <title>窗口 Top-N</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-topn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-topn/</guid>
      <description>Window Top-N # Batch Streaming
Window Top-N is a special Top-N which returns the N smallest or largest values for each window and other partitioned keys.
For streaming queries, unlike regular Top-N on continuous tables, window Top-N does not emit intermediate results but only a final result, the total top N records at the end of the window. Moreover, window Top-N purges all intermediate state when no longer needed. Therefore, window Top-N queries have better performance if users don&amp;rsquo;t need results updated per record.</description>
    </item>
    
    <item>
      <title>JAR Statements</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/jar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/jar/</guid>
      <description>JAR 语句 # JAR 语句用于将用户 jar 添加到 classpath、或将用户 jar 从 classpath 中删除或展示运行时 classpath 中添加的 jar。
目前 Flink SQL 支持以下 JAR 语句：
ADD JAR REMOVE JAR SHOW JARS 注意 JAR 语句仅适用于 SQL CLI。
执行 JAR 语句 # SQL CLI 以下示例展示了如何在 SQL CLI 中运行 JAR 语句。 SQL CLI Flink SQL&amp;gt; ADD JAR &amp;#39;/path/hello.jar&amp;#39;; [INFO] The specified jar is added into session classloader. Flink SQL&amp;gt; SHOW JARS; /path/hello.jar Flink SQL&amp;gt; REMOVE JAR &amp;#39;/path/hello.</description>
    </item>
    
    <item>
      <title>窗口去重</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-deduplication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-deduplication/</guid>
      <description>Window Deduplication # Streaming Window Deduplication is a special Deduplication which removes rows that duplicate over a set of columns, keeping the first one or the last one for each window and partitioned keys.
For streaming queries, unlike regular Deduplicate on continuous tables, Window Deduplication does not emit intermediate results but only a final result at the end of the window. Moreover, window Deduplication purges all intermediate state when no longer needed.</description>
    </item>
    
    <item>
      <title>环境安装</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/installation/</guid>
      <description>环境安装 # 环境要求 # 注意 PyFlink 需要 Python 3.6 以上版本（3.6, 3.7 或 3.8）。请运行以下命令，以确保 Python 版本满足要求。
$ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 环境设置 # 你的系统也许安装了好几个版本的 Python。你可以运行下面的 ls 命令来查看当前系统中安装的 Python 版本有哪些:
$ ls /usr/bin/python* 为了满足 Python 版本要求，你可以选择通过软链接的方式将 python 指向 python3 解释器:
ln -s /usr/bin/python3 python 除了软链接的方式，你也可以选择创建一个 Python virtual env（venv）的方式。关于如何创建一个 virtual env，你可以参考准备 Python 虚拟环境。
如果你不想使用软链接的方式改变系统 Python 解释器的路径，你也可以通过配置的方式指定 Python 解释器。 你可以参考配置python.client.executable，了解如何指定编译作业时所使用的 Python 解释器路径， 以及参考配置python.executable，了解如何指定执行 Python UDF 时所使用的 Python 解释器路径。</description>
    </item>
    
    <item>
      <title>去重</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/deduplication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/deduplication/</guid>
      <description>Deduplication # Batch Streaming
Deduplication removes rows that duplicate over a set of columns, keeping only the first one or the last one. In some cases, the upstream ETL jobs are not end-to-end exactly-once; this may result in duplicate records in the sink in case of failover. However, the duplicate records will affect the correctness of downstream analytical jobs - e.g. SUM, COUNT - so deduplication is needed before further analysis.</description>
    </item>
    
    <item>
      <title>模式检测</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/match_recognize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sql/queries/match_recognize/</guid>
      <description>模式检测 # Streaming 搜索一组事件模式（event pattern）是一种常见的用例，尤其是在数据流情景中。Flink 提供复杂事件处理（CEP）库，该库允许在事件流中进行模式检测。此外，Flink 的 SQL API 提供了一种关系式的查询表达方式，其中包含大量内置函数和基于规则的优化，可以开箱即用。
2016 年 12 月，国际标准化组织（ISO）发布了新版本的 SQL 标准，其中包括在 SQL 中的行模式识别（Row Pattern Recognition in SQL）(ISO/IEC TR 19075-5:2016)。它允许 Flink 使用 MATCH_RECOGNIZE 子句融合 CEP 和 SQL API，以便在 SQL 中进行复杂事件处理。
MATCH_RECOGNIZE 子句启用以下任务：
使用 PARTITION BY 和 ORDER BY 子句对数据进行逻辑分区和排序。 使用 PATTERN 子句定义要查找的行模式。这些模式使用类似于正则表达式的语法。 在 DEFINE 子句中指定行模式变量的逻辑组合。 measures 是指在 MEASURES 子句中定义的表达式，这些表达式可用于 SQL 查询中的其他部分。 下面的示例演示了基本模式识别的语法：
SELECT T.aid, T.bid, T.cid FROM MyTable MATCH_RECOGNIZE ( PARTITION BY userid ORDER BY proctime MEASURES A.</description>
    </item>
    
    <item>
      <title>Batch 示例</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/dataset/examples/</guid>
      <description>Batch 示例 # 以下示例展示了 Flink 从简单的WordCount到图算法的应用。示例代码展示了 Flink&amp;rsquo;s DataSet API 的使用。
完整的源代码可以在 Flink 源代码库的 flink-examples-batch 模块找到。
运行一个示例 # 在开始运行一个示例前，我们假设你已经有了 Flink 的运行示例。导航栏中的“快速开始（Quickstart）”和“安装（Setup）” 标签页提供了启动 Flink 的不同方法。
最简单的方法就是执行 ./bin/start-cluster.sh，从而启动一个只有一个 JobManager 和 TaskManager 的本地 Flink 集群。
每个 Flink 的 binary release 都会包含一个examples（示例）目录，其中可以找到这个页面上每个示例的 jar 包文件。
可以通过执行以下命令来运行WordCount 示例:
./bin/flink run ./examples/batch/WordCount.jar 其他的示例也可以通过类似的方式执行。
注意很多示例在不传递执行参数的情况下都会使用内置数据。如果需要利用 WordCount 程序计算真实数据，你需要传递存储数据的文件路径。
./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result 注意非本地文件系统需要一个对应前缀，例如 hdfs://。
Word Count # WordCount 是大数据系统中的 “Hello World”。他可以计算一个文本集合中不同单词的出现频次。这个算法分两步进行： 第一步，把所有文本切割成单独的单词。第二步，把单词分组并分别统计。
Java ExecutionEnvironment env = ExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Table API 教程</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table_api_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table_api_tutorial/</guid>
      <description>Table API 教程 # Apache Flink 提供 Table API 关系型 API 来统一处理流和批，即查询在无边界的实时流或有边界的批处理数据集上以相同的语义执行，并产生相同的结果。 Flink 的 Table API 易于编写，通常能简化数据分析，数据管道和ETL应用的编码。
概要 # 在该教程中，我们会从零开始，介绍如何创建一个 Flink Python 项目及运行 Python Table API 作业。该作业读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
先决条件 # 本练习假定你对 Python 有一定的了解，但是即使你来自其他编程语言，也应该能够继续学习。 它还假定你熟悉基本的关系操作，例如 SELECT 和 GROUP BY 子句。
如何寻求帮助 # 如果你遇到问题，可以访问 社区信息页面。 与此同时，Apache Flink 的用户邮件列表 一直被列为 Apache 项目中最活跃的项目邮件列表之一，也是快速获得帮助的好方法。
继续我们的旅程 # 如果要继续我们的旅程，你需要一台具有以下功能的计算机：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python Table API 需要安装 PyFlink，它已经被发布到 PyPi，你可以通过如下方式安装 PyFlink：</description>
    </item>
    
    <item>
      <title>程序打包</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/packaging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/packaging/</guid>
      <description>程序打包和分布式运行 # 正如之前所描述的，Flink 程序可以使用 remote environment 在集群上执行。或者，程序可以被打包成 JAR 文件（Java Archives）执行。如果使用命令行的方式执行程序，将程序打包是必需的。
打包程序 # 为了能够通过命令行或 web 界面执行打包的 JAR 文件，程序必须使用通过 StreamExecutionEnvironment.getExecutionEnvironment() 获取的 environment。当 JAR 被提交到命令行或 web 界面后，该 environment 会扮演集群环境的角色。如果调用 Flink 程序的方式与上述接口不同，该 environment 会扮演本地环境的角色。
打包程序只要简单地将所有相关的类导出为 JAR 文件，JAR 文件的 manifest 必须指向包含程序入口点（拥有公共 main 方法）的类。实现的最简单方法是将 main-class 写入 manifest 中（比如 main-class: org.apache.flinkexample.MyProgram）。main-class 属性与 Java 虚拟机通过指令 java -jar pathToTheJarFile 执行 JAR 文件时寻找 main 方法的类是相同的。大多数 IDE 提供了在导出 JAR 文件时自动包含该属性的功能。
总结 # 调用打包后程序的完整流程包括两步：
搜索 JAR 文件 manifest 中的 main-class 或 program-class 属性。如果两个属性同时存在，program-class 属性会优先于 main-class 属性。对于 JAR manifest 中两个属性都不存在的情况，命令行和 web 界面支持手动传入入口点类名参数。</description>
    </item>
    
    <item>
      <title>从源码构建 Flink</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/flinkdev/building/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/flinkdev/building/</guid>
      <description>从源码构建 Flink # 本篇主题是如何从版本 1.16-SNAPSHOT 的源码构建 Flink。
构建 Flink # 首先需要准备源码。可以从发布版本下载源码 或者从 Git 库克隆 Flink 源码。
还需要准备 Maven 3 和 JDK (Java开发套件)。Flink 依赖 Java 11 或更新的版本来进行构建。
*注意：Maven 3.3.x 可以构建 Flink，但是不能正确地屏蔽掉指定的依赖。Maven 3.2.5 可以正确地构建库文件。
输入以下命令从 Git 克隆代码
git clone https://github.com/apache/flink.git 最简单的构建 Flink 的方法是执行如下命令：
mvn clean install -DskipTests 上面的 Maven 指令（mvn）首先删除（clean）所有存在的构建，然后构建一个新的 Flink 运行包（install）。
为了加速构建，可以：
使用 &amp;rsquo; -DskipTests&amp;rsquo; 跳过测试 使用 fast Maven profile 跳过 QA 的插件和 JavaDocs 的生成 使用 skip-webui-build Maven profile 跳过 WebUI 编译 使用 Maven 并行构建功能，比如 &amp;lsquo;mvn package -T 1C&amp;rsquo; 会尝试并行使用多核 CPU，同时让每一个 CPU 核构建1个模块。 maven-shade-plugin 现存的 bug 可能会在并行构建时产生死锁。建议分2步进行构建：首先使用并行方式运行 mvn validate/test-compile/test，然后使用单线程方式运行 mvn package/verify/install。 构建脚本如下：</description>
    </item>
    
    <item>
      <title>普通自定义函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/python_udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/python_udfs/</guid>
      <description>普通自定义函数（UDF） # 用户自定义函数是重要的功能，因为它们极大地扩展了 Python Table API 程序的表达能力。
注意: 要执行 Python 用户自定义函数，客户端和集群端都需要安装 Python 3.6 以上版本(3.6、3.7 或 3.8)，并安装 PyFlink。
标量函数（ScalarFunction） # PyFlink 支持在 Python Table API 程序中使用 Python 标量函数。 如果要定义 Python 标量函数， 可以继承 pyflink.table.udf 中的基类 ScalarFunction，并实现 eval 方法。 Python 标量函数的行为由名为 eval 的方法定义，eval 方法支持可变长参数，例如 eval(* args)。
以下示例显示了如何定义自己的 Python 哈希函数、如何在 TableEnvironment 中注册它以及如何在作业中使用它。
from pyflink.table.expressions import call, col from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.udf import ScalarFunction, udf class HashCode(ScalarFunction): def __init__(self): self.factor = 12 def eval(self, s): return hash(s) * self.</description>
    </item>
    
    <item>
      <title>数据类型</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/types/</guid>
      <description>数据类型 # Flink SQL 为用户提供了一系列丰富的原始数据类型。
数据类型 # 在 Flink 的 Table 生态系统中，数据类型 描述了数据的逻辑类型，可以用来表示转换过程中输入、输出的类型。
Flink 的数据类型类似于 SQL 标准中的术语数据类型，但包含了值的可空性，以便于更好地处理标量表达式。
以下是一些数据类型的例子：
INT INT NOT NULL INTERVAL DAY TO SECOND(3) ROW&amp;lt;myField ARRAY&amp;lt;BOOLEAN&amp;gt;, myOtherField TIMESTAMP(3)&amp;gt; 可在下文中找到所有预先定义好的数据类型。
Table API 中的数据类型 # Java/Scala 在定义 connector、catalog、用户自定义函数时，使用 JVM 相关 API 的用户可能会使用到 Table API 中基于 org.apache.flink.table.types.DataType 的一些实例。
数据类型 实例有两个职责：
作为逻辑类型的表现形式，定义 JVM 类语言或 Python 语言与 Table 生态系统的边界，而不是以具体的物理表现形式存在于数据的传输过程或存储中。 可选的: 在与其他 API 进行数据交换时，为 Planner 提供这些数据物理层面的相关提示。 对于基于 JVM 的语言，所有预定义的数据类型都可以在 org.apache.flink.table.api.DataTypes 下找到。
Python 在 Python 语言定义用户自定义函数时，使用 Python API 的用户 可能会使用到 Python API 中基于 pyflink.</description>
    </item>
    
    <item>
      <title>DataStream API 教程</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream_tutorial/</guid>
      <description>DataStream API 教程 # Apache Flink 提供了 DataStream API，用于构建健壮的、有状态的流式应用程序。它提供了对状态和时间细粒度控制，从而允许实现高级事件驱动系统。 在这篇教程中，你将学习如何使用 PyFlink 和 DataStream API 构建一个简单的流式应用程序。
你要搭建一个什么系统 # 在本教程中，你将学习如何编写一个简单的 Python DataStream 作业。 该程序读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
准备条件 # 本教程假设你对 Python 有一定的了解，但是即使你使用的是其它编程语言，你也应该能够学会。
困难求助 # 如果你有疑惑，可以查阅 社区支持资源。 特别是，Apache Flink 用户邮件列表 一直被评为所有 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。
怎样跟着教程练习 # 首先，你需要在你的电脑上准备以下环境：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python DataStream API 需要安装 PyFlink，PyFlink 发布在 PyPI上，可以通过 pip 快速安装。
$ python -m pip install apache-flink 一旦 PyFlink 安装完成之后，你就可以开始编写 Python DataStream 作业了。</description>
    </item>
    
    <item>
      <title>时区</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/timezone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/timezone/</guid>
      <description>时区 # Flink 为日期和时间提供了丰富的数据类型， 包括 DATE， TIME， TIMESTAMP， TIMESTAMP_LTZ， INTERVAL YEAR TO MONTH， INTERVAL DAY TO SECOND (更多详情请参考 Date and Time)。 Flink 支持在 session （会话）级别设置时区（更多详情请参考 table.local-time-zone）。 Flink 对多种时间类型和时区的支持使得跨时区的数据处理变得非常容易。
TIMESTAMP vs TIMESTAMP_LTZ # TIMESTAMP 类型 # TIMESTAMP(p) 是 TIMESTAMP(p) WITHOUT TIME ZONE 的简写， 精度 p 支持的范围是0-9， 默认是6。 TIMESTAMP 用于描述年， 月， 日， 小时， 分钟， 秒 和 小数秒对应的时间戳。 TIMESTAMP 可以通过一个字符串来指定，例如： Flink SQL&amp;gt; SELECT TIMESTAMP &amp;#39;1970-01-01 00:00:04.001&amp;#39;; +-------------------------+ | 1970-01-01 00:00:04.001 | +-------------------------+ TIMESTAMP_LTZ 类型 # TIMESTAMP_LTZ(p) 是 TIMESTAMP(p) WITH LOCAL TIME ZONE 的简写， 精度 p 支持的范围是0-9， 默认是6。 TIMESTAMP_LTZ 用于描述时间线上的绝对时间点， 使用 long 保存从 epoch 至今的毫秒数， 使用int保存毫秒中的纳秒数。 epoch 时间是从 java 的标准 epoch 时间 1970-01-01T00:00:00Z 开始计算。 在计算和可视化时， 每个 TIMESTAMP_LTZ 类型的数据都是使用的 session （会话）中配置的时区。 TIMESTAMP_LTZ 没有字符串表达形式因此无法通过字符串来指定， 可以通过一个 long 类型的 epoch 时间来转化(例如: 通过 Java 来产生一个 long 类型的 epoch 时间 System.</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/data_types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/data_types/</guid>
      <description>Data Types # In Apache Flink&amp;rsquo;s Python DataStream API, a data type describes the type of a value in the DataStream ecosystem. It can be used to declare input and output types of operations and informs the system how to serailize elements.
Pickle Serialization # If the type has not been declared, data would be serialized or deserialized using Pickle. For example, the program below specifies no data types.</description>
    </item>
    
    <item>
      <title>Python Table API 简介</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/intro_to_table_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/intro_to_table_api/</guid>
      <description>Python Table API 简介 # 本文档是对 PyFlink Table API 的简要介绍，用于帮助新手用户快速理解 PyFlink Table API 的基本用法。 关于高级用法，请参阅用户指南中的其他文档。
Python Table API 程序的基本结构 # 所有的 Table API 和 SQL 程序，不管批模式，还是流模式，都遵循相同的结构。下面代码示例展示了 Table API 和 SQL 程序的基本结构。
from pyflink.table import EnvironmentSettings, TableEnvironment # 1. 创建 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # 2. 创建 source 表 table_env.execute_sql(&amp;#34;&amp;#34;&amp;#34; CREATE TABLE datagen ( id INT, data STRING ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;datagen&amp;#39;, &amp;#39;fields.id.kind&amp;#39; = &amp;#39;sequence&amp;#39;, &amp;#39;fields.</description>
    </item>
    
    <item>
      <title>TableEnvironment</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/table_environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/table_environment/</guid>
      <description>TableEnvironment # 本篇文档是对 PyFlink TableEnvironment 的介绍。 文档包括对 TableEnvironment 类中每个公共接口的详细描述。
创建 TableEnvironment # 创建 TableEnvironment 的推荐方式是通过 EnvironmentSettings 对象创建:
from pyflink.common import Configuration from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment config = Configuration() config.set_string(&amp;#39;execution.buffer-timeout&amp;#39;, &amp;#39;1 min&amp;#39;) env_settings = EnvironmentSettings \ .new_instance() \ .in_streaming_mode() \ .with_configuration(config) \ .build() table_env = TableEnvironment.create(env_settings) 或者，用户可以从现有的 StreamExecutionEnvironment 创建 StreamTableEnvironment，以与 DataStream API 进行互操作。
from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment # create a streaming TableEnvironment from a StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/operations/operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/operations/operations/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>依赖管理</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/dependency_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/dependency_management/</guid>
      <description>Dependency Management # There are requirements to use dependencies inside the Python API programs. For example, users may need to use third-party Python libraries in Python user-defined functions. In addition, in scenarios such as machine learning prediction, users may want to load a machine learning model inside the Python user-defined functions.
When the PyFlink job is executed locally, users could install the third-party Python libraries into the local Python environment, download the machine learning model to local, etc.</description>
    </item>
    
    <item>
      <title>Row-based Operations</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/operations/row_based_operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/operations/row_based_operations/</guid>
      <description>Row-based Operations # This page describes how to use row-based operations in PyFlink Table API.
Map # Performs a map operation with a python general scalar function or vectorized scalar function. The output will be flattened if the output type is a composite type.
from pyflink.common import Row from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.types import DataTypes from pyflink.table.udf import udf env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.</description>
    </item>
    
    <item>
      <title>State</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream/state/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Table API</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/tableapi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/tableapi/</guid>
      <description>Table API # Table API 是批处理和流处理的统一的关系型 API。Table API 的查询不需要修改代码就可以采用批输入或流输入来运行。Table API 是 SQL 语言的超集，并且是针对 Apache Flink 专门设计的。Table API 集成了 Scala，Java 和 Python 语言的 API。Table API 的查询是使用 Java，Scala 或 Python 语言嵌入的风格定义的，有诸如自动补全和语法校验的 IDE 支持，而不是像普通 SQL 一样使用字符串类型的值来指定查询。
Table API 和 Flink SQL 共享许多概念以及部分集成的 API。通过查看公共概念 &amp;amp; API来学习如何注册表或如何创建一个表对象。流概念页面讨论了诸如动态表和时间属性等流特有的概念。
下面的例子中假定有一张叫 Orders 的表，表中有属性 (a, b, c, rowtime) 。rowtime 字段是流任务中的逻辑时间属性或是批任务中的普通时间戳字段。
概述 &amp;amp; 示例 # Table API 支持 Scala, Java 和 Python 语言。Scala 语言的 Table API 利用了 Scala 表达式，Java 语言的 Table API 支持 DSL 表达式和解析并转换为等价表达式的字符串，Python 语言的 Table API 仅支持解析并转换为等价表达式的字符串。</description>
    </item>
    
    <item>
      <title>并行执行</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/parallel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/execution/parallel/</guid>
      <description>并行执行 # 本节描述了在 Flink 中配置程序的并行执行。一个 Flink 程序由多个任务 task 组成（转换/算子、数据源和数据接收器）。一个 task 包括多个并行执行的实例，且每一个实例都处理 task 输入数据的一个子集。一个 task 的并行实例数被称为该 task 的 并行度 (parallelism)。
使用 savepoints 时，应该考虑设置最大并行度。当作业从一个 savepoint 恢复时，你可以改变特定算子或着整个程序的并行度，并且此设置会限定整个程序的并行度的上限。由于在 Flink 内部将状态划分为了 key-groups，且性能所限不能无限制地增加 key-groups，因此设定最大并行度是有必要的。
toc 设置并行度 # 一个 task 的并行度可以从多个层次指定：
算子层次 # 单个算子、数据源和数据接收器的并行度可以通过调用 setParallelism()方法来指定。如下所示：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&amp;lt;String&amp;gt; text = [...]; DataStream&amp;lt;Tuple2&amp;lt;String, Integer&amp;gt;&amp;gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(value -&amp;gt; value.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1).setParallelism(5); wordCounts.print(); env.execute(&amp;#34;Word Count Example&amp;#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val text = [.</description>
    </item>
    
    <item>
      <title>向量化自定义函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/vectorized_python_udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/udfs/vectorized_python_udfs/</guid>
      <description>向量化自定义函数 # 向量化 Python 用户自定义函数，是在执行时，通过在 JVM 和 Python VM 之间以 Arrow 列存格式批量传输数据，来执行的函数。 向量化 Python 用户自定义函数的性能通常比非向量化 Python 用户自定义函数要高得多， 因为向量化 Python 用户自定义函数可以大大减少序列化/反序列化的开销和调用开销。 此外，用户可以利用流行的 Python 库（例如 Pandas，Numpy 等）来实现向量化 Python 用户自定义函数的逻辑。 这些 Python 库通常经过高度优化，并提供了高性能的数据结构和功能。 向量化用户自定义函数的定义，与非向量化用户自定义函数具有相似的方式， 用户只需要在调用 udf 或者 udaf 装饰器时添加一个额外的参数 func_type=&amp;quot;pandas&amp;quot;，将其标记为一个向量化用户自定义函数即可。
注意: 要执行 Python 向量化自定义函数，客户端和集群端都需要安装 Python 3.6 以上版本(3.6、3.7 或 3.8)，并安装 PyFlink。
向量化标量函数 # 向量化 Python 标量函数以 pandas.Series 类型的参数作为输入，并返回与输入长度相同的 pandas.Series。 在内部实现中，Flink 会将输入数据拆分为多个批次，并将每一批次的输入数据转换为 Pandas.Series 类型， 然后为每一批输入数据调用用户自定义的向量化 Python 标量函数。请参阅配置选项 python.fn-execution.arrow.batch.size， 以获取有关如何配置批次大小的更多详细信息。
向量化 Python 标量函数可以在任何可以使用非向量化 Python 标量函数的地方使用。</description>
    </item>
    
    <item>
      <title>数据类型</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/python_types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/python_types/</guid>
      <description>数据类型 # 本节描述PyFlink Table API中所支持的数据类型.
Data Type # 在Table生态系统中，数据类型用于描述值的逻辑类型。它可以用来声明Python用户自定义函数的输入／输出类型。 Python Table API的用户可以在Python Table API中，或者定义Python用户自定义函数时，使用pyflink.table.types.DataType实例。
DataType实例声明了数据的逻辑类型，这并不能用于推断数据在进行传输或存储时的具体物理表示形式。 所有预定义的数据类型都位于pyflink.table.types中，并且可以通过类pyflink.table.types.DataTypes中所定义的方法创建。
可以在下面找到所有预定义数据类型的列表。
数据类型（Data Type）和Python类型的映射关系 # 数据类型可用于声明Python用户自定义函数的输入/输出类型。输入数据将被转换为与所定义的数据类型相对应的Python对象，用户自定义函数的执行结果的类型也必须与所定义的数据类型匹配。
对于向量化Python UDF，输入类型和输出类型都为pandas.Series。pandas.Series中的元素类型对应于指定的数据类型。
Data Type Python Type Pandas Type BOOLEAN bool numpy.bool_ TINYINT int numpy.int8 SMALLINT int numpy.int16 INT int numpy.int32 BIGINT int numpy.int64 FLOAT float numpy.float32 DOUBLE float numpy.float64 VARCHAR str str VARBINARY bytes bytes DECIMAL decimal.Decimal decimal.Decimal DATE datetime.date datetime.date TIME datetime.time datetime.time TimestampType datetime.datetime datetime.datetime LocalZonedTimestampType datetime.</description>
    </item>
    
    <item>
      <title>系统（内置）函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/system_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/system_functions/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>系统（内置）函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/systemfunctions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/systemfunctions/</guid>
      <description>系统（内置）函数 # Flink Table API &amp;amp; SQL 为用户提供了一组内置的数据转换函数。本页简要介绍了它们。如果你需要的函数尚不支持，你可以实现 用户自定义函数。如果你觉得这个函数够通用，请 创建一个 Jira issue并详细 说明。
标量函数 # 标量函数将零、一个或多个值作为输入并返回单个值作为结果。
比较函数 # SQL 函数 Table 函数 描述 value1 = value2 value1 === value2 如果 value1 等于 value2 返回 TRUE；如果 value1 或者 value2 为 NULL 返回 UNKNOW。 value1 &amp;lt;&amp;gt; value2 value1 !== value2 如果 value1 不等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 &amp;gt; value2 value1 &amp;gt; value2 如果 value1 大于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 &amp;gt;= value2 value1 &amp;gt;= value2 如果 value1 大于或等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 &amp;lt; value2 value1 &amp;lt; value2 如果 value1 小于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 &amp;lt;= value2 value1 &amp;lt;= value2 如果 value1 小于或等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value IS NULL value.</description>
    </item>
    
    <item>
      <title>旁路输出</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/side_output/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/side_output/</guid>
      <description>旁路输出 # 除了由 DataStream 操作产生的主要流之外，你还可以产生任意数量的旁路输出结果流。结果流中的数据类型不必与主要流中的数据类型相匹配，并且不同旁路输出的类型也可以不同。当你需要拆分数据流时，通常必须复制该数据流，然后从每个流中过滤掉不需要的数据，这个操作十分有用。
使用旁路输出时，首先需要定义用于标识旁路输出流的 OutputTag：
Java // 这需要是一个匿名的内部类，以便我们分析类型 OutputTag&amp;lt;String&amp;gt; outputTag = new OutputTag&amp;lt;String&amp;gt;(&amp;#34;side-output&amp;#34;) {}; Scala val outputTag = OutputTag[String](&amp;#34;side-output&amp;#34;) Python output_tag = OutputTag(&amp;#34;side-output&amp;#34;, Types.STRING()) 注意 OutputTag 是如何根据旁路输出流所包含的元素类型进行类型化的。
可以通过以下方法将数据发送到旁路输出：
ProcessFunction KeyedProcessFunction CoProcessFunction KeyedCoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 你可以使用在上述方法中向用户暴露的 Context 参数，将数据发送到由 OutputTag 标识的旁路输出。这是从 ProcessFunction 发送数据到旁路输出的示例：
Java DataStream&amp;lt;Integer&amp;gt; input = ...; final OutputTag&amp;lt;String&amp;gt; outputTag = new OutputTag&amp;lt;String&amp;gt;(&amp;#34;side-output&amp;#34;){}; SingleOutputStreamOperator&amp;lt;Integer&amp;gt; mainDataStream = input .process(new ProcessFunction&amp;lt;Integer, Integer&amp;gt;() { @Override public void processElement( Integer value, Context ctx, Collector&amp;lt;Integer&amp;gt; out) throws Exception { // 发送数据到主要的输出 out.</description>
    </item>
    
    <item>
      <title>执行模式</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_execution_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_execution_mode/</guid>
      <description>Execution Mode # The Python API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job. The Python runtime execution mode defines how the Python user-defined functions will be executed.
Prior to release-1.15, there is the only execution mode called PROCESS execution mode. The PROCESS mode means that the Python user-defined functions will be executed in separate Python processes.</description>
    </item>
    
    <item>
      <title>PyFlink Table 和 Pandas DataFrame 互转</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_pandas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_pandas/</guid>
      <description>PyFlink Table 和 Pandas DataFrame 互转 # PyFlink 支持 PyFlink Table 和 Pandas DataFrame 之间进行互转。
将 Pandas DataFrame 转换为 PyFlink Table # PyFlink 支持将 Pandas DataFrame 转换成 PyFlink Table。在内部实现上，会在客户端将 Pandas DataFrame 序列化成 Arrow 列存格式，序列化后的数据 在作业执行期间，在 Arrow 源中会被反序列化，并进行处理。Arrow 源除了可以用在批作业中外，还可以用于流作业，它将正确处理检查点并提供恰好一次的保证。
以下示例显示如何从 Pandas DataFrame 创建 PyFlink Table：
from pyflink.table import DataTypes import pandas as pd import numpy as np # 创建一个Pandas DataFrame pdf = pd.DataFrame(np.random.rand(1000, 2)) # 由Pandas DataFrame创建PyFlink表 table = t_env.from_pandas(pdf) # 由Pandas DataFrame创建指定列名的PyFlink表 table = t_env.</description>
    </item>
    
    <item>
      <title>Table 和 DataStream 互转</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_data_stream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_data_stream/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>SQL</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/sql/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Handling Application Parameters</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/application_parameters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/application_parameters/</guid>
      <description>应用程序参数处理 # 应用程序参数处理 # 几乎所有的批和流的 Flink 应用程序，都依赖于外部配置参数。这些配置参数可以用于指定输入和输出源（如路径或地址）、系统参数（并行度，运行时配置）和特定的应用程序参数（通常使用在用户自定义函数）。
为解决以上问题，Flink 提供一个名为 Parametertool 的简单公共类，其中包含了一些基本的工具。请注意，这里说的 Parametertool 并不是必须使用的。Commons CLI 和 argparse4j 等其他框架也可以非常好地兼容 Flink。
用 ParameterTool 读取配置值 # ParameterTool 定义了一组静态方法，用于读取配置信息。该工具类内部使用了 Map&amp;lt;string，string&amp;gt; 类型，这样使得它可以很容易地与你的配置集成在一起。
配置值来自 .properties 文件 # 以下方法可以读取 Properties 文件并解析出键/值对：
String propertiesFilePath = &amp;#34;/home/sam/flink/myjob.properties&amp;#34;; ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath); File propertiesFile = new File(propertiesFilePath); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile); InputStream propertiesFileInputStream = new FileInputStream(file); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFileInputStream); 配置值来自命令行 # 以下方法可以从命令行中获取参数，如 --input hdfs:///mydata --elements 42。
public static void main(String[] args) { ParameterTool parameter = ParameterTool.</description>
    </item>
    
    <item>
      <title>Task 故障恢复</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/ops/state/task_failure_recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/ops/state/task_failure_recovery/</guid>
      <description>Task 故障恢复 # 当 Task 发生故障时，Flink 需要重启出错的 Task 以及其他受到影响的 Task ，以使得作业恢复到正常执行状态。
Flink 通过重启策略和故障恢复策略来控制 Task 重启：重启策略决定是否可以重启以及重启的间隔；故障恢复策略决定哪些 Task 需要重启。
Restart Strategies # Flink 作业如果没有定义重启策略，则会遵循集群启动时加载的默认重启策略。 如果提交作业时设置了重启策略，该策略将覆盖掉集群的默认策略。
通过 Flink 的配置文件 flink-conf.yaml 来设置默认的重启策略。配置参数 restart-strategy 定义了采取何种策略。 如果没有启用 checkpoint，就采用“不重启”策略。如果启用了 checkpoint 且没有配置重启策略，那么就采用固定延时重启策略， 此时最大尝试重启次数由 Integer.MAX_VALUE 参数设置。下表列出了可用的重启策略和与其对应的配置值。
每个重启策略都有自己的一组配置参数来控制其行为。 这些参数也在配置文件中设置。 后文的描述中会详细介绍每种重启策略的配置项。
Key Default Type Description restart-strategy (none) String Defines the restart strategy to use in case of job failures.
Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy.</description>
    </item>
    
    <item>
      <title>自定义函数</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/functions/udfs/</guid>
      <description>自定义函数 # 自定义函数（UDF）是一种扩展开发机制，可以用来在查询语句里调用难以用其他方式表达的频繁使用或自定义的逻辑。
自定义函数可以用 JVM 语言（例如 Java 或 Scala）或 Python 实现，实现者可以在 UDF 中使用任意第三方库，本文聚焦于使用 JVM 语言开发自定义函数。
概述 # 当前 Flink 有如下几种函数：
标量函数 将标量值转换成一个新标量值； 表值函数 将标量值转换成新的行数据； 聚合函数 将多行数据里的标量值转换成一个新标量值； 表值聚合函数 将多行数据里的标量值转换成新的行数据； 异步表值函数 是异步查询外部数据系统的特殊函数。 注意 标量和表值函数已经使用了新的基于数据类型的类型系统，聚合函数仍然使用基于 TypeInformation 的旧类型系统。
以下示例展示了如何创建一个基本的标量函数，以及如何在 Table API 和 SQL 里调用这个函数。
函数用于 SQL 查询前要先经过注册；而在用于 Table API 时，函数可以先注册后调用，也可以 内联 后直接使用。
Java import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; // 定义函数逻辑 public static class SubstringFunction extends ScalarFunction { public String eval(String s, Integer begin, Integer end) { return s.</description>
    </item>
    
    <item>
      <title>Catalogs</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/catalogs/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>模块</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/modules/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/modules/</guid>
      <description>Modules # Modules allow users to extend Flink&amp;rsquo;s built-in objects, such as defining functions that behave like Flink built-in functions. They are pluggable, and while Flink provides a few pre-built modules, users can write their own.
For example, users can define their own geo functions and plug them into Flink as built-in functions to be used in Flink SQL and Table APIs. Another example is users can load an out-of-shelf Hive module to use Hive built-in functions as Flink built-in functions.</description>
    </item>
    
    <item>
      <title>Catalogs</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/catalogs/</guid>
      <description>Catalogs # Catalog 提供了元数据信息，例如数据库、表、分区、视图以及数据库或其他外部系统中存储的函数和信息。
数据处理最关键的方面之一是管理元数据。 元数据可以是临时的，例如临时表、或者通过 TableEnvironment 注册的 UDF。 元数据也可以是持久化的，例如 Hive Metastore 中的元数据。Catalog 提供了一个统一的API，用于管理元数据，并使其可以从 Table API 和 SQL 查询语句中来访问。
Catalog 类型 # GenericInMemoryCatalog # GenericInMemoryCatalog 是基于内存实现的 Catalog，所有元数据只在 session 的生命周期内可用。
JdbcCatalog # JdbcCatalog 使得用户可以将 Flink 通过 JDBC 协议连接到关系数据库。Postgres Catalog 和 MySQL Catalog 是目前 JDBC Catalog 仅有的两种实现。 参考 JdbcCatalog 文档 获取关于配置 JDBC catalog 的详细信息。
HiveCatalog # HiveCatalog 有两个用途：作为原生 Flink 元数据的持久化存储，以及作为读写现有 Hive 元数据的接口。 Flink 的 Hive 文档 提供了有关设置 HiveCatalog 以及访问现有 Hive 元数据的详细信息。</description>
    </item>
    
    <item>
      <title>SQL 客户端</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sqlclient/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sqlclient/</guid>
      <description>SQL 客户端 # Flink 的 Table &amp;amp; SQL API 可以处理 SQL 语言编写的查询语句，但是这些查询需要嵌入用 Java 或 Scala 编写的表程序中。此外，这些程序在提交到集群前需要用构建工具打包。这或多或少限制了 Java/Scala 程序员对 Flink 的使用。
SQL 客户端 的目的是提供一种简单的方式来编写、调试和提交表程序到 Flink 集群上，而无需写一行 Java 或 Scala 代码。SQL 客户端命令行界面（CLI） 能够在命令行中检索和可视化分布式应用中实时产生的结果。
入门 # 本节介绍如何在命令行里启动（setup）和运行你的第一个 Flink SQL 程序。
SQL 客户端捆绑在常规 Flink 发行版中，因此可以直接运行。它仅需要一个正在运行的 Flink 集群就可以在其中执行表程序。有关设置 Flink 群集的更多信息，请参见集群和部署部分。如果仅想试用 SQL 客户端，也可以使用以下命令启动本地集群：
./bin/start-cluster.sh 启动 SQL 客户端命令行界面 # SQL Client 脚本也位于 Flink 的 bin 目录中。将来，用户可以通过启动嵌入式 standalone 进程或通过连接到远程 SQL 客户端网关来启动 SQL 客户端命令行界面。目前仅支持 embedded，模式默认值embedded。可以通过以下方式启动 CLI：
./bin/sql-client.sh 或者显式使用 embedded 模式:</description>
    </item>
    
    <item>
      <title>测试</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/testing/</guid>
      <description>测试 # 测试是每个软件开发过程中不可或缺的一部分， Apache Flink 同样提供了在测试金字塔的多个级别上测试应用程序代码的工具。
测试用户自定义函数 # 通常，我们可以假设 Flink 在用户自定义函数之外产生了正确的结果。因此，建议尽可能多的用单元测试来测试那些包含主要业务逻辑的类。
单元测试无状态、无时间限制的 UDF # 例如，让我们以以下无状态的 MapFunction 为例。
Java public class IncrementMapFunction implements MapFunction&amp;lt;Long, Long&amp;gt; { @Override public Long map(Long record) throws Exception { return record + 1; } } Scala class IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } 通过传递合适地参数并验证输出，你可以很容易的使用你喜欢的测试框架对这样的函数进行单元测试。
Java public class IncrementMapFunctionTest { @Test public void testIncrement() throws Exception { // instantiate your function IncrementMapFunction incrementer = new IncrementMapFunction(); // call the methods that you have implemented assertEquals(3L, incrementer.</description>
    </item>
    
    <item>
      <title>网络缓冲调优</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/network_mem_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/deployment/memory/network_mem_tuning/</guid>
      <description>网络内存调优指南 # 概述 # Flink 中每条消息都会被放到网络缓冲（network buffer） 中，并以此为最小单位发送到下一个 subtask。 为了维持连续的高吞吐，Flink 在传输过程的输入端和输出端使用了网络缓冲队列。
每个 subtask 都有一个输入队列来接收数据和一个输出队列来发送数据到下一个 subtask。 在 pipeline 场景，拥有更多的中间缓存数据可以使 Flink 提供更高、更富有弹性的吞吐量，但是也会增加快照时间。
只有所有的 subtask 都收到了全部注入的 checkpoint barrier 才能完成快照。 在对齐的 checkpoints 中，checkpoint barrier 会跟着网络缓冲数据在 job graph 中流动。 缓冲数据越多，checkpoint barrier 流动的时间就越长。在非对齐的 checkpoints 中，缓冲数据越多，checkpoint 就会越大，因为这些数据都会被持久化到 checkpoint 中。
缓冲消胀机制（Buffer Debloating） # 之前，配置缓冲数据量的唯一方法是指定缓冲区的数量和大小。然而，因为每次部署的不同很难配置一组完美的参数。 Flink 1.14 新引入的缓冲消胀机制尝试通过自动调整缓冲数据量到一个合理值来解决这个问题。
缓冲消胀功能计算 subtask 可能达到的最大吞吐（始终保持繁忙状态时）并且通过调整缓冲数据量来使得数据的消费时间达到配置值。
可以通过设置 taskmanager.network.memory.buffer-debloat.enabled 为 true 来开启缓冲消胀机制。 通过设置 taskmanager.network.memory.buffer-debloat.target 为 duration 类型的值来指定消费缓冲数据的目标时间。 默认值应该能满足大多数场景。
这个功能使用过去的吞吐数据来预测消费剩余缓冲数据的时间。如果预测不准，缓冲消胀机制会导致以下问题：
没有足够的缓存数据来提供全量吞吐。 有太多缓冲数据对 checkpoint barrier 推进或者非对齐的 checkpoint 的大小造成不良影响。 如果您的作业负载经常变化（即，突如其来的数据尖峰，定期的窗口聚合触发或者 join ），您可能需要调整以下设置：</description>
    </item>
    
    <item>
      <title>下载页面</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/downloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/downloads/</guid>
      <description>SQL Connectors 下载页面 # Download links are available only for stable releases. The page contains links to optional sql-client connectors and formats that are not part of the binary distribution.
可选的 SQL formats # Name Download link Avro Only available for stable versions. Avro Schema Registry Only available for stable versions. Debezium Only available for stable versions. ORC Only available for stable versions. Parquet Only available for stable versions. Protobuf Only available for stable versions.</description>
    </item>
    
    <item>
      <title>实验功能</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/experimental/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/experimental/</guid>
      <description>实验功能 # This section describes experimental features in the DataStream API. Experimental features are still evolving and can be either unstable, incomplete, or subject to heavy change in future versions.
Reinterpreting a pre-partitioned data stream as keyed stream # We can re-interpret a pre-partitioned data stream as a keyed stream to avoid shuffling.
WARNING: The re-interpreted data stream MUST already be pre-partitioned in EXACTLY the same way Flink&amp;rsquo;s keyBy would partition the data in a shuffle w.</description>
    </item>
    
    <item>
      <title>配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/config/</guid>
      <description>配置 # Table 和 SQL API 的默认配置能够确保结果准确，同时也提供可接受的性能。
根据 Table 程序的需求，可能需要调整特定的参数用于优化。例如，无界流程序可能需要保证所需的状态是有限的(请参阅 流式概念).
概览 # 当实例化一个 TableEnvironment 时，可以使用 EnvironmentSettings 来传递用于当前会话的所期望的配置项 —— 传递一个 Configuration 对象到 EnvironmentSettings。
此外，在每个 TableEnvironment 中，TableConfig 提供用于当前会话的配置项。
对于常见或者重要的配置项，TableConfig 提供带有详细注释的 getters 和 setters 方法。
对于更加高级的配置，用户可以直接访问底层的 key-value 配置项。以下章节列举了所有可用于调整 Flink Table 和 SQL API 程序的配置项。
注意 因为配置项会在执行操作的不同时间点被读取，所以推荐在实例化 TableEnvironment 后尽早地设置配置项。
Java // instantiate table environment Configuration configuration = new Configuration(); // set low-level key-value options configuration.setString(&amp;#34;table.exec.mini-batch.enabled&amp;#34;, &amp;#34;true&amp;#34;); configuration.setString(&amp;#34;table.exec.mini-batch.allow-latency&amp;#34;, &amp;#34;5 s&amp;#34;); configuration.setString(&amp;#34;table.exec.mini-batch.size&amp;#34;, &amp;#34;5000&amp;#34;); EnvironmentSettings settings = EnvironmentSettings.</description>
    </item>
    
    <item>
      <title>指标</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/metrics/</guid>
      <description>指标 # PyFlink 支持指标系统，该指标系统允许收集指标并将其暴露给外部系统。
注册指标 # 您可以通过在Python 用户自定义函数的 open 方法中调用 function_context.get_metric_group() 来访问指标系统。 get_metric_group() 方法返回一个 MetricGroup 对象，您可以在该对象上创建和注册新指标。
指标类型 # PyFlink 支持计数器 Counters，量表 Gauges ，分布 Distribution 和仪表 Meters。
计数器 Counter # Counter 用于计算某个东西的出现次数。可以通过 inc()/inc(n: int) 或 dec()/dec(n: int) 增加或减少当前值。 您可以通过在 MetricGroup 上调用 counter(name: str) 来创建和注册 Counter。
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.counter = None def open(self, function_context): self.counter = function_context.get_metric_group().counter(&amp;#34;my_counter&amp;#34;) def eval(self, i): self.counter.inc(i) return i 量表 # Gauge 可按需返回数值。您可以通过在 MetricGroup 上调用 gauge(name: str, obj: Callable[[], int]) 来注册一个量表。Callable 对象将用于汇报数值。量表指标(Gauge metrics)只能用于汇报整数值。</description>
    </item>
    
    <item>
      <title>配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_config/</guid>
      <description>配置 # Depending on the requirements of a Python API program, it might be necessary to adjust certain parameters for optimization.
For Python DataStream API program, the config options could be set as following:
from pyflink.common import Configuration from pyflink.datastream import StreamExecutionEnvironment config = Configuration() config.set_integer(&amp;#34;python.fn-execution.bundle.size&amp;#34;, 1000) env = StreamExecutionEnvironment.get_execution_environment(config) For Python Table API program, all the config options available for Java/Scala Table API program could also be used in the Python Table API program.</description>
    </item>
    
    <item>
      <title>调试</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/debugging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/debugging/</guid>
      <description>调试 # 本页介绍如何在PyFlink进行调试
打印日志信息 # 客户端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 PyFlink 作业中，Python UDF 之外的地方打印上下文和调试信息。 在提交作业时，日志信息会打印在客户端的日志文件中。
from pyflink.table import EnvironmentSettings, TableEnvironment # 创建 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, &amp;#39;Hi&amp;#39;), (2, &amp;#39;Hello&amp;#39;)]) # 使用 logging 模块 import logging logging.warning(table.get_schema()) # 使用 print 函数 print(table.get_schema()) 注意: 客户端缺省的日志级别是 WARNING，因此，只有日志级别在 WARNING 及以上的日志信息才会打印在客户端的日志文件中。
服务器端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 Python UDF 中打印上下文和调试信息。 在作业运行的过程中，日志信息会打印在 TaskManager 的日志文件中。</description>
    </item>
    
    <item>
      <title>User-defined Sources &amp; Sinks</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/sourcessinks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/sourcessinks/</guid>
      <description>User-defined Sources &amp;amp; Sinks # Dynamic tables are the core concept of Flink&amp;rsquo;s Table &amp;amp; SQL API for processing both bounded and unbounded data in a unified fashion.
Because dynamic tables are only a logical concept, Flink does not own the data itself. Instead, the content of a dynamic table is stored in external systems (such as databases, key-value stores, message queues) or files.
Dynamic sources and dynamic sinks can be used to read and write data from and to an external system.</description>
    </item>
    
    <item>
      <title>环境变量</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/environment_variables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/environment_variables/</guid>
      <description>Environment Variables # These environment variables will affect the behavior of PyFlink:
Environment Variable Description FLINK_HOME PyFlink job will be compiled before submitting and it requires Flink&#39;s distribution to compile the job. PyFlink&#39;s installation package already contains Flink&#39;s distribution and it&#39;s used by default. This environment allows you to specify a custom Flink&#39;s distribution. PYFLINK_CLIENT_EXECUTABLE The path of the Python interpreter used to launch the Python process when submitting the Python jobs via &#34;</description>
    </item>
    
    <item>
      <title>连接器</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/python_table_api_connectors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table/python_table_api_connectors/</guid>
      <description>连接器 # 本篇描述了如何在 PyFlink 中使用连接器，并着重介绍了在 Python 程序中使用 Flink 连接器时需要注意的细节。
Note 想要了解常见的连接器信息和通用配置，请查阅相关的 Java/Scala 文档。
下载连接器（connector）和格式（format）jar 包 # 由于 Flink 是一个基于 Java/Scala 的项目，连接器（connector）和格式（format）的实现是作为 jar 包存在的， 要在 PyFlink 作业中使用，首先需要将其指定为作业的 依赖。
table_env.get_config().set(&amp;#34;pipeline.jars&amp;#34;, &amp;#34;file:///my/jar/path/connector.jar;file:///my/jar/path/json.jar&amp;#34;) 如何使用连接器 # 在 PyFlink Table API 中，DDL 是定义 source 和 sink 比较推荐的方式，这可以通过 TableEnvironment 中的 execute_sql() 方法来完成，然后就可以在作业中使用这张表了。
source_ddl = &amp;#34;&amp;#34;&amp;#34; CREATE TABLE source_table( a VARCHAR, b INT ) WITH ( &amp;#39;connector&amp;#39; =&amp;#39; = &amp;#39;kafka&amp;#39;, &amp;#39;topic&amp;#39; = &amp;#39;source_topic&amp;#39;, &amp;#39;properties.bootstrap.servers&amp;#39; = &amp;#39;kafka:9092&amp;#39;, &amp;#39;properties.group.id&amp;#39; = &amp;#39;test_3&amp;#39;, &amp;#39;scan.</description>
    </item>
    
    <item>
      <title>常见问题</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/faq/</guid>
      <description>常见问题 # 本页介绍了针对PyFlink用户的一些常见问题的解决方案。
准备Python虚拟环境 # 您可以下载[便捷脚本]({% link downloads/setup-pyflink-virtual-env.sh %})，以准备可在Mac OS和大多数Linux发行版上使用的Python虚拟环境包(virtual env zip)。 您可以指定PyFlink的版本，来生成对应的PyFlink版本所需的Python虚拟环境，否则将安装最新版本的PyFlink所对应的Python虚拟环境。
$ sh setup-pyflink-virtual-env.sh 集群（Cluster） # $ # 指定Python虚拟环境 $ table_env.add_python_archive(&amp;#34;venv.zip&amp;#34;) $ # 指定用于执行python UDF workers (用户自定义函数工作者) 的python解释器的路径 $ table_env.get_config().set_python_executable(&amp;#34;venv.zip/venv/bin/python&amp;#34;) 如果需要了解add_python_archive和set_python_executable用法的详细信息，请参阅相关文档。
添加Jar文件 # PyFlink作业可能依赖jar文件，比如connector，Java UDF等。 您可以在提交作业时使用以下Python Table API或通过命令行参数来指定依赖项。
# 注意：仅支持本地文件URL（以&amp;#34;file:&amp;#34;开头）。 table_env.get_config().set(&amp;#34;pipeline.jars&amp;#34;, &amp;#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar&amp;#34;) # 注意：路径必须指定协议（例如：文件——&amp;#34;file&amp;#34;），并且用户应确保在客户端和群集上都可以访问这些URL。 table_env.get_config().set(&amp;#34;pipeline.classpaths&amp;#34;, &amp;#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar&amp;#34;) 有关添加Java依赖项的API的详细信息，请参阅相关文档。
添加Python文件 # 您可以使用命令行参数pyfs或TableEnvironment的API add_python_file添加python文件依赖，这些依赖可以是python文件，python软件包或本地目录。 例如，如果您有一个名为myDir的目录，该目录具有以下层次结构：
myDir ├──utils ├──__init__.py ├──my_util.py 您可以将添加目录myDir添加到Python依赖中，如下所示：
table_env.add_python_file(&amp;#39;myDir&amp;#39;) def my_udf(): from utils import my_util 当在 mini cluster 环境执行作业时，显式等待作业执行结束 # 当在 mini cluster 环境执行作业（比如，在IDE中执行作业）且在作业中使用了如下API（比如 Python Table API 的 TableEnvironment.</description>
    </item>
    
    <item>
      <title>Scala API 扩展</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/scala_api_extensions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/scala_api_extensions/</guid>
      <description>Scala API 扩展 # 为了在 Scala 和 Java API 之间保持大致相同的使用体验，在批处理和流处理的标准 API 中省略了一些允许 Scala 高级表达的特性。
如果你想拥有完整的 Scala 体验，可以选择通过隐式转换增强 Scala API 的扩展。
要使用所有可用的扩展，你只需为 DataStream API 添加一个简单的引入
import org.apache.flink.streaming.api.scala.extensions._ 或者，您可以引入单个扩展 a-là-carte 来使用您喜欢的扩展。
Accept partial functions # 通常，DataStream API 不接受匿名模式匹配函数来解构元组、case 类或集合，如下所示：
val data: DataStream[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =&amp;gt; // [...] // The previous line causes the following compilation error: // &amp;#34;The argument types of an anonymous function must be fully known.</description>
    </item>
    
    <item>
      <title>Java Lambda Expressions</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/java_lambdas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/datastream/java_lambdas/</guid>
      <description>Java Lambda 表达式 # Java 8 引入了几种新的语言特性，旨在实现更快、更清晰的编码。作为最重要的特性，即所谓的“Lambda 表达式”，它开启了函数式编程的大门。Lambda 表达式允许以简捷的方式实现和传递函数，而无需声明额外的（匿名）类。
Flink 支持对 Java API 的所有算子使用 Lambda 表达式，但是，当 Lambda 表达式使用 Java 泛型时，你需要 显式 地声明类型信息。 本文档介绍如何使用 Lambda 表达式并描述了其（Lambda 表达式）当前的限制。有关 Flink API 的通用介绍，请参阅 DataStream API 编程指南。
示例和限制 # 下面的这个示例演示了如何实现一个简单的内联 map() 函数，它使用 Lambda 表达式计算输入值的平方。
不需要声明 map() 函数的输入 i 和输出参数的数据类型，因为 Java 编译器会对它们做出推断。
env.fromElements(1, 2, 3) // 返回 i 的平方 .map(i -&amp;gt; i*i) .print(); 由于 OUT 是 Integer 而不是泛型，所以 Flink 可以从方法签名 OUT map(IN value) 的实现中自动提取出结果的类型信息。
不幸的是，像 flatMap() 这样的函数，它的签名 void flatMap(IN value, Collector&amp;lt;OUT&amp;gt; out) 被 Java 编译器编译为 void flatMap(IN value, Collector out)。这样 Flink 就无法自动推断输出的类型信息了。</description>
    </item>
    
    <item>
      <title>Temporal Table Function</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/temporal_table_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/table/concepts/temporal_table_function/</guid>
      <description>Temporal Table Function # A Temporal table function provides access to the version of a temporal table at a specific point in time. In order to access the data in a temporal table, one must pass a time attribute that determines the version of the table that will be returned. Flink uses the SQL syntax of table functions to provide a way to express it.
Unlike a versioned table, temporal table functions can only be defined on top of append-only streams — it does not support changelog inputs.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.10</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.10/</guid>
      <description>Release Notes - Flink 1.10 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.9 and Flink 1.10. Please read these notes carefully if you are planning to upgrade your Flink version to 1.10.
Clusters &amp;amp; Deployment # FileSystems should be loaded via Plugin Architecture # FLINK-11956 # s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through plugins but now seamlessly integrate with all credential providers.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.11</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.11/</guid>
      <description>Release Notes - Flink 1.11 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read these notes carefully if you are planning to upgrade your Flink version to 1.11.
Clusters &amp;amp; Deployment # Support for Application Mode # FLIP-85 # The user can now submit applications and choose to execute their main() method on the cluster rather than the client.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.12</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.12/</guid>
      <description>Release Notes - Flink 1.12 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.11 and Flink 1.12. Please read these notes carefully if you are planning to upgrade your Flink version to 1.12.
Known Issues # Unaligned checkpoint recovery may lead to corrupted data stream # FLINK-20654 # Using unaligned checkpoints in Flink 1.12.0 combined with two/multiple inputs tasks or with union inputs for single input tasks can result in corrupted state.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.13</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.13/</guid>
      <description>Release Notes - Flink 1.13 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.12 and Flink 1.13. Please read these notes carefully if you are planning to upgrade your Flink version to 1.13.
Failover # Remove state.backend.async option. # FLINK-21935 # The state.backend.async option is deprecated. Snapshots are always asynchronous now (as they were by default before) and there is no option to configure a synchronous snapshot any more.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.14</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.14/</guid>
      <description>Release notes - Flink 1.14 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.13 and Flink 1.14. Please read these notes carefully if you are planning to upgrade your Flink version to 1.14.
Known issues # Memory leak with Pulsar connector on Java 11 # Netty, which the Pulsar client uses underneath, allocates memory differently on Java 11 and Java 8.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.5</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.5/</guid>
      <description>Release Notes - Flink 1.5 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.
Update Configuration for Reworked Job Deployment # Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.6</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.6/</guid>
      <description>Release Notes - Flink 1.6 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.5 and Flink 1.6. Please read these notes carefully if you are planning to upgrade your Flink version to 1.6.
Changed Configuration Default Values # The default value of the slot idle timeout slot.idle.timeout is set to the default value of the heartbeat timeout (50 s).
Changed ElasticSearch 5.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.7</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.7/</guid>
      <description>Release Notes - Flink 1.7 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.6 and Flink 1.7. Please read these notes carefully if you are planning to upgrade your Flink version to 1.7.
Scala 2.12 support # When using Scala 2.12 you might have to add explicit type annotations in places where they were not required when using Scala 2.11. This is an excerpt from the TransitiveClosureNaive.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.8</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.8/</guid>
      <description>Release Notes - Flink 1.8 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.7 and Flink 1.8. Please read these notes carefully if you are planning to upgrade your Flink version to 1.8.
State # Continuous incremental cleanup of old Keyed State with TTL # We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (FLINK-9510). This feature allowed to clean up and make inaccessible keyed state entries when accessing them.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.9</title>
      <link>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/release-notes/flink-1.9/</guid>
      <description>Release Notes - Flink 1.9 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.8 and Flink 1.9. It also provides an overview on known shortcoming or limitations with new experimental features introduced in 1.9.
Please read these notes carefully if you are planning to upgrade your Flink version to 1.9.
Known shortcomings or limitations for new features # New Table / SQL Blink planner # Flink 1.</description>
    </item>
    
    <item>
      <title>Versions</title>
      <link>//localhost/flink/flink-docs-master/zh/versions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/versions/</guid>
      <description> Versions # An appendix of hosted documentation for all versions of Apache Flink.
v1.15 v1.14 v1.13 v1.12 v1.11 v1.10 v1.9 v1.8 v1.7 v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 </description>
    </item>
    
  </channel>
</rss>
