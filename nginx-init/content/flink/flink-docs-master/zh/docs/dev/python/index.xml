<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python API on Apache Flink</title>
    <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/</link>
    <description>Recent content in Python API on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//localhost/flink/flink-docs-master/zh/docs/dev/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/overview/</guid>
      <description>Python API # PyFlink 是 Apache Flink 的 Python API，你可以使用它构建可扩展的批处理和流处理任务，例如实时数据处理管道、大规模探索性数据分析、机器学习（ML）管道和 ETL 处理。 如果你对 Python 和 Pandas 等库已经比较熟悉，那么 PyFlink 可以让你更轻松地利用 Flink 生态系统的全部功能。 根据你需要的抽象级别的不同，有两种不同的 API 可以在 PyFlink 中使用：
PyFlink Table API 允许你使用类似于 SQL 或者在 Python 中处理表格数据的方式编写强大的关系查询。 与此同时，PyFlink DataStream API 允许你对 Flink 的核心组件 state 和 time 进行细粒度的控制，以便构建更复杂的流处理应用。 尝试 PyFlink # 如果你有兴趣使用 PyFlink，可以尝试以下教程：
PyFlink DataStream API 介绍 PyFlink Table API 介绍 如果你想了解更多关于 PyFlink 的示例，可以参考 PyFlink 示例 深入 PyFlink # 这些参考文档涵盖了 PyFlink 的所有细节，可以从以下链接入手：
PyFlink DataStream API PyFlink Table API &amp;amp; SQL 获取有关 PyFlink 的帮助 # 如果你遇到困难，请查看我们的社区支持资源，特别是 Apache Flink 的用户邮件列表，Apache Flink 的用户邮件列表一直是所有 Apache 项目中最活跃的项目邮件列表之一，是快速获得帮助的好方法。</description>
    </item>
    
    <item>
      <title>环境安装</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/installation/</guid>
      <description>环境安装 # 环境要求 # 注意 PyFlink 需要 Python 3.6 以上版本（3.6, 3.7 或 3.8）。请运行以下命令，以确保 Python 版本满足要求。
$ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 环境设置 # 你的系统也许安装了好几个版本的 Python。你可以运行下面的 ls 命令来查看当前系统中安装的 Python 版本有哪些:
$ ls /usr/bin/python* 为了满足 Python 版本要求，你可以选择通过软链接的方式将 python 指向 python3 解释器:
ln -s /usr/bin/python3 python 除了软链接的方式，你也可以选择创建一个 Python virtual env（venv）的方式。关于如何创建一个 virtual env，你可以参考准备 Python 虚拟环境。
如果你不想使用软链接的方式改变系统 Python 解释器的路径，你也可以通过配置的方式指定 Python 解释器。 你可以参考配置python.client.executable，了解如何指定编译作业时所使用的 Python 解释器路径， 以及参考配置python.executable，了解如何指定执行 Python UDF 时所使用的 Python 解释器路径。</description>
    </item>
    
    <item>
      <title>Table API 教程</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/table_api_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/table_api_tutorial/</guid>
      <description>Table API 教程 # Apache Flink 提供 Table API 关系型 API 来统一处理流和批，即查询在无边界的实时流或有边界的批处理数据集上以相同的语义执行，并产生相同的结果。 Flink 的 Table API 易于编写，通常能简化数据分析，数据管道和ETL应用的编码。
概要 # 在该教程中，我们会从零开始，介绍如何创建一个 Flink Python 项目及运行 Python Table API 作业。该作业读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
先决条件 # 本练习假定你对 Python 有一定的了解，但是即使你来自其他编程语言，也应该能够继续学习。 它还假定你熟悉基本的关系操作，例如 SELECT 和 GROUP BY 子句。
如何寻求帮助 # 如果你遇到问题，可以访问 社区信息页面。 与此同时，Apache Flink 的用户邮件列表 一直被列为 Apache 项目中最活跃的项目邮件列表之一，也是快速获得帮助的好方法。
继续我们的旅程 # 如果要继续我们的旅程，你需要一台具有以下功能的计算机：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python Table API 需要安装 PyFlink，它已经被发布到 PyPi，你可以通过如下方式安装 PyFlink：</description>
    </item>
    
    <item>
      <title>DataStream API 教程</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/datastream_tutorial/</guid>
      <description>DataStream API 教程 # Apache Flink 提供了 DataStream API，用于构建健壮的、有状态的流式应用程序。它提供了对状态和时间细粒度控制，从而允许实现高级事件驱动系统。 在这篇教程中，你将学习如何使用 PyFlink 和 DataStream API 构建一个简单的流式应用程序。
你要搭建一个什么系统 # 在本教程中，你将学习如何编写一个简单的 Python DataStream 作业。 该程序读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
准备条件 # 本教程假设你对 Python 有一定的了解，但是即使你使用的是其它编程语言，你也应该能够学会。
困难求助 # 如果你有疑惑，可以查阅 社区支持资源。 特别是，Apache Flink 用户邮件列表 一直被评为所有 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。
怎样跟着教程练习 # 首先，你需要在你的电脑上准备以下环境：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python DataStream API 需要安装 PyFlink，PyFlink 发布在 PyPI上，可以通过 pip 快速安装。
$ python -m pip install apache-flink 一旦 PyFlink 安装完成之后，你就可以开始编写 Python DataStream 作业了。</description>
    </item>
    
    <item>
      <title>依赖管理</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/dependency_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/dependency_management/</guid>
      <description>Dependency Management # There are requirements to use dependencies inside the Python API programs. For example, users may need to use third-party Python libraries in Python user-defined functions. In addition, in scenarios such as machine learning prediction, users may want to load a machine learning model inside the Python user-defined functions.
When the PyFlink job is executed locally, users could install the third-party Python libraries into the local Python environment, download the machine learning model to local, etc.</description>
    </item>
    
    <item>
      <title>执行模式</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_execution_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_execution_mode/</guid>
      <description>Execution Mode # The Python API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job. The Python runtime execution mode defines how the Python user-defined functions will be executed.
Prior to release-1.15, there is the only execution mode called PROCESS execution mode. The PROCESS mode means that the Python user-defined functions will be executed in separate Python processes.</description>
    </item>
    
    <item>
      <title>配置</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/python_config/</guid>
      <description>配置 # Depending on the requirements of a Python API program, it might be necessary to adjust certain parameters for optimization.
For Python DataStream API program, the config options could be set as following:
from pyflink.common import Configuration from pyflink.datastream import StreamExecutionEnvironment config = Configuration() config.set_integer(&amp;#34;python.fn-execution.bundle.size&amp;#34;, 1000) env = StreamExecutionEnvironment.get_execution_environment(config) For Python Table API program, all the config options available for Java/Scala Table API program could also be used in the Python Table API program.</description>
    </item>
    
    <item>
      <title>调试</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/debugging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/debugging/</guid>
      <description>调试 # 本页介绍如何在PyFlink进行调试
打印日志信息 # 客户端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 PyFlink 作业中，Python UDF 之外的地方打印上下文和调试信息。 在提交作业时，日志信息会打印在客户端的日志文件中。
from pyflink.table import EnvironmentSettings, TableEnvironment # 创建 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, &amp;#39;Hi&amp;#39;), (2, &amp;#39;Hello&amp;#39;)]) # 使用 logging 模块 import logging logging.warning(table.get_schema()) # 使用 print 函数 print(table.get_schema()) 注意: 客户端缺省的日志级别是 WARNING，因此，只有日志级别在 WARNING 及以上的日志信息才会打印在客户端的日志文件中。
服务器端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 Python UDF 中打印上下文和调试信息。 在作业运行的过程中，日志信息会打印在 TaskManager 的日志文件中。</description>
    </item>
    
    <item>
      <title>环境变量</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/environment_variables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/environment_variables/</guid>
      <description>Environment Variables # These environment variables will affect the behavior of PyFlink:
Environment Variable Description FLINK_HOME PyFlink job will be compiled before submitting and it requires Flink&#39;s distribution to compile the job. PyFlink&#39;s installation package already contains Flink&#39;s distribution and it&#39;s used by default. This environment allows you to specify a custom Flink&#39;s distribution. PYFLINK_CLIENT_EXECUTABLE The path of the Python interpreter used to launch the Python process when submitting the Python jobs via &#34;</description>
    </item>
    
    <item>
      <title>常见问题</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/dev/python/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/dev/python/faq/</guid>
      <description>常见问题 # 本页介绍了针对PyFlink用户的一些常见问题的解决方案。
准备Python虚拟环境 # 您可以下载[便捷脚本]({% link downloads/setup-pyflink-virtual-env.sh %})，以准备可在Mac OS和大多数Linux发行版上使用的Python虚拟环境包(virtual env zip)。 您可以指定PyFlink的版本，来生成对应的PyFlink版本所需的Python虚拟环境，否则将安装最新版本的PyFlink所对应的Python虚拟环境。
$ sh setup-pyflink-virtual-env.sh 集群（Cluster） # $ # 指定Python虚拟环境 $ table_env.add_python_archive(&amp;#34;venv.zip&amp;#34;) $ # 指定用于执行python UDF workers (用户自定义函数工作者) 的python解释器的路径 $ table_env.get_config().set_python_executable(&amp;#34;venv.zip/venv/bin/python&amp;#34;) 如果需要了解add_python_archive和set_python_executable用法的详细信息，请参阅相关文档。
添加Jar文件 # PyFlink作业可能依赖jar文件，比如connector，Java UDF等。 您可以在提交作业时使用以下Python Table API或通过命令行参数来指定依赖项。
# 注意：仅支持本地文件URL（以&amp;#34;file:&amp;#34;开头）。 table_env.get_config().set(&amp;#34;pipeline.jars&amp;#34;, &amp;#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar&amp;#34;) # 注意：路径必须指定协议（例如：文件——&amp;#34;file&amp;#34;），并且用户应确保在客户端和群集上都可以访问这些URL。 table_env.get_config().set(&amp;#34;pipeline.classpaths&amp;#34;, &amp;#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar&amp;#34;) 有关添加Java依赖项的API的详细信息，请参阅相关文档。
添加Python文件 # 您可以使用命令行参数pyfs或TableEnvironment的API add_python_file添加python文件依赖，这些依赖可以是python文件，python软件包或本地目录。 例如，如果您有一个名为myDir的目录，该目录具有以下层次结构：
myDir ├──utils ├──__init__.py ├──my_util.py 您可以将添加目录myDir添加到Python依赖中，如下所示：
table_env.add_python_file(&amp;#39;myDir&amp;#39;) def my_udf(): from utils import my_util 当在 mini cluster 环境执行作业时，显式等待作业执行结束 # 当在 mini cluster 环境执行作业（比如，在IDE中执行作业）且在作业中使用了如下API（比如 Python Table API 的 TableEnvironment.</description>
    </item>
    
  </channel>
</rss>
