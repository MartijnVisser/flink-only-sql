<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Formats on Apache Flink</title>
    <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/</link>
    <description>Recent content in Formats on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Formats</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/overview/</guid>
      <description> Formats # Flink 提供了一套与表连接器（table connector）一起使用的表格式（table format）。表格式是一种存储格式，定义了如何把二进制数据映射到表的列上。
Flink 支持以下格式：
Formats Supported Connectors CSV Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem JSON Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem, Elasticsearch Apache Avro Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem Confluent Avro Apache Kafka, Upsert Kafka Debezium CDC Apache Kafka, Filesystem Canal CDC Apache Kafka, Filesystem Maxwell CDC Apache Kafka, Filesystem OGG CDC Apache Kafka, Filesystem Apache Parquet Filesystem Apache ORC Filesystem Raw Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem </description>
    </item>
    
    <item>
      <title>CSV</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/</guid>
      <description>CSV Format # Format: Serialization Schema Format: Deserialization Schema
CSV Format 允许我们基于 CSV schema 进行解析和生成 CSV 数据。 目前 CSV schema 是基于 table schema 推断而来的。
依赖 # In order to use the CSV format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-csv&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>JSON</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/json/</guid>
      <description>JSON Format # Format: Serialization Schema Format: Deserialization Schema
JSON Format 能读写 JSON 格式的数据。当前，JSON schema 是从 table schema 中自动推导而得的。
依赖 # In order to use the Json format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-json&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Built-in 如何创建一张基于 JSON Format 的表 # 以下是一个利用 Kafka 以及 JSON Format 构建表的例子。</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro/</guid>
      <description>Avro Format # Format: Serialization Schema Format: Deserialization Schema
Apache Avro format 允许基于 Avro schema 读取和写入 Avro 数据。目前，Avro schema 从 table schema 推导而来。
依赖 # In order to use the Avro format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-avro&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Confluent Avro</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro-confluent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/avro-confluent/</guid>
      <description>Confluent Avro Format # Format: Serialization Schema Format: Deserialization Schema
Avro Schema Registry (avro-confluent) 格式能让你读取被 io.confluent.kafka.serializers.KafkaAvroSerializer 序列化的记录，以及可以写入成能被 io.confluent.kafka.serializers.KafkaAvroDeserializer 反序列化的记录。
当以这种格式读取（反序列化）记录时，将根据记录中编码的 schema 版本 id 从配置的 Confluent Schema Registry 中获取 Avro writer schema ，而从 table schema 中推断出 reader schema。
当以这种格式写入（序列化）记录时，Avro schema 是从 table schema 中推断出来的，并会用来检索要与数据一起编码的 schema id。我们会在配置的 Confluent Schema Registry 中配置的 subject 下，检索 schema id。subject 通过 avro-confluent.subject 参数来制定。
Avro Schema Registry 格式只能与 Apache Kafka SQL 连接器或 Upsert Kafka SQL 连接器一起使用。
依赖 # In order to use the Avro Schema Registry format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.</description>
    </item>
    
    <item>
      <title>Protobuf</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/protobuf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/protobuf/</guid>
      <description>Protobuf Format # Format: Serialization Schema Format: Deserialization Schema
The Protocol Buffers Protobuf format allows you to read and write Protobuf data, based on Protobuf generated classes.
Dependencies # In order to use the Protobuf format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-protobuf&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>Debezium</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/debezium/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/debezium/</guid>
      <description>Debezium Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Debezium 是一个 CDC（Changelog Data Capture，变更数据捕获）的工具，可以把来自 MySQL、PostgreSQL、Oracle、Microsoft SQL Server 和许多其他数据库的更改实时流式传输到 Kafka 中。 Debezium 为变更日志提供了统一的格式结构，并支持使用 JSON 和 Apache Avro 序列化消息。
Flink 支持将 Debezium JSON 和 Avro 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Debezium 格式的 JSON 或 Avro 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Debezium 消息。</description>
    </item>
    
    <item>
      <title>Canal</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/canal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/canal/</guid>
      <description>Canal Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Canal 是一个 CDC（ChangeLog Data Capture，变更日志数据捕获）工具，可以实时地将 MySQL 变更传输到其他系统。Canal 为变更日志提供了统一的数据格式，并支持使用 JSON 或 protobuf 序列化消息（Canal 默认使用 protobuf）。
Flink 支持将 Canal 的 JSON 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Canal 格式的 JSON 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Canal 消息。</description>
    </item>
    
    <item>
      <title>Maxwell</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/maxwell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/maxwell/</guid>
      <description>Maxwell Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Maxwell is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into Kafka, Kinesis and other streaming connectors. Maxwell provides a unified format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Maxwell JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as</description>
    </item>
    
    <item>
      <title>Ogg</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/ogg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/ogg/</guid>
      <description>Ogg Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Oracle GoldenGate (a.k.a ogg) 是一个实现异构 IT 环境间数据实时数据集成和复制的综合软件包。 该产品集支持高可用性解决方案、实时数据集成、事务更改数据捕获、运营和分析企业系统之间的数据复制、转换和验证。Ogg 为变更日志提供了统一的格式结构，并支持使用 JSON 序列化消息。
Flink 支持将 Ogg JSON 消息解析为 INSERT/UPDATE/DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等 Flink 还支持将 Flink SQL 中的 INSERT/UPDATE/DELETE 消息编码为 Ogg JSON 格式的消息, 输出到 Kafka 等存储中。 但需要注意, 目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息. 因此, Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Ogg 消息。</description>
    </item>
    
    <item>
      <title>Parquet</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/parquet/</guid>
      <description>Parquet 格式 # Format: Serialization Schema Format: Deserialization Schema
Apache Parquet 格式允许读写 Parquet 数据.
依赖 # In order to use the Parquet format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-parquet&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases. 如何创建基于 Parquet 格式的表 # 以下为用 Filesystem 连接器和 Parquet 格式创建表的示例，</description>
    </item>
    
    <item>
      <title>Orc</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/orc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/orc/</guid>
      <description>Orc Format # Format: Serialization Schema Format: Deserialization Schema
Apache Orc Format 允许读写 ORC 数据。
依赖 # In order to use the ORC format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-orc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases. 如何用 Orc 格式创建一个表格 # 下面是一个用 Filesystem connector 和 Orc format 创建表格的例子</description>
    </item>
    
    <item>
      <title>Raw</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/raw/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/table/formats/raw/</guid>
      <description>Raw Format # Format: Serialization Schema Format: Deserialization Schema
Raw format 允许读写原始（基于字节）值作为单个列。
注意: 这种格式将 null 值编码成 byte[] 类型的 null。这样在 upsert-kafka 中使用时可能会有限制，因为 upsert-kafka 将 null 值视为 墓碑消息（在键上删除）。因此，如果该字段可能具有 null 值，我们建议避免使用 upsert-kafka 连接器和 raw format 作为 value.format。
Raw format 连接器是内置的。
示例 # 例如，你可能在 Kafka 中具有原始日志数据，并希望使用 Flink SQL 读取和分析此类数据。
47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] &amp;#34;GET /?p=1 HTTP/2.0&amp;#34; 200 5316 &amp;#34;https://domain.com/?p=1&amp;#34; &amp;#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36&amp;#34; &amp;#34;2.75&amp;#34; 下面的代码创建了一张表，使用 raw format 以 UTF-8 编码的形式从中读取（也可以写入）底层的 Kafka topic 作为匿名字符串值：</description>
    </item>
    
  </channel>
</rss>
