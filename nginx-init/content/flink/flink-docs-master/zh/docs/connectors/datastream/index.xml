<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DataStream Connectors on Apache Flink</title>
    <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/</link>
    <description>Recent content in DataStream Connectors on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>概览</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/overview/</guid>
      <description>DataStream Connectors # 预定义的 Source 和 Sink # 一些比较基本的 Source 和 Sink 已经内置在 Flink 里。 预定义 data sources 支持从文件、目录、socket，以及 collections 和 iterators 中读取数据。 预定义 data sinks 支持把数据写入文件、标准输出（stdout）、标准错误输出（stderr）和 socket。
附带的连接器 # 连接器可以和多种多样的第三方系统进行交互。目前支持以下系统:
Apache Kafka (source/sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) FileSystem (sink) RabbitMQ (source/sink) Google PubSub (source/sink) Hybrid Source (source) Apache Pulsar (source) JDBC (sink) 请记住，在使用一种连接器时，通常需要额外的第三方组件，比如：数据存储服务器或者消息队列。 要注意这些列举的连接器是 Flink 工程的一部分，包含在发布的源码中，但是不包含在二进制发行版中。 更多说明可以参考对应的子部分。
Apache Bahir 中的连接器 # Flink 还有些一些额外的连接器通过 Apache Bahir 发布, 包括:</description>
    </item>
    
    <item>
      <title>容错保证</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/guarantees/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/guarantees/</guid>
      <description>Data Source 和 Sink 的容错保证 # 当程序出现错误的时候，Flink 的容错机制能恢复并继续运行程序。这种错误包括机器硬件故障、网络故障、瞬态程序故障等等。
只有当 source 参与了快照机制的时候，Flink 才能保证对自定义状态的精确一次更新。下表列举了 Flink 与其自带连接器的状态更新的保证。
请阅读各个连接器的文档来了解容错保证的细节。
Source Guarantees Notes Apache Kafka 精确一次 根据你的版本用恰当的 Kafka 连接器 AWS Kinesis Streams 精确一次 RabbitMQ 至多一次 (v 0.10) / 精确一次 (v 1.0) Google PubSub 至少一次 Collections 精确一次 Files 精确一次 Sockets 至多一次 为了保证端到端精确一次的数据交付（在精确一次的状态语义上更进一步），sink需要参与 checkpointing 机制。下表列举了 Flink 与其自带 sink 的交付保证（假设精确一次状态更新）。
Sink Guarantees Notes Elasticsearch 至少一次 Kafka producer 至少一次 / 精确一次 当使用事务生产者时，保证精确一次 (v 0.11+) Cassandra sink 至少一次 / 精确一次 只有当更新是幂等时，保证精确一次 AWS Kinesis Streams 至少一次 File sinks 精确一次 Socket sinks 至少一次 Standard output 至少一次 Redis sink 至少一次 Back to top</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kafka/</guid>
      <description>Apache Kafka 连接器 # Flink 提供了 Apache Kafka 连接器使用精确一次（Exactly-once）的语义在 Kafka topic 中读取和写入数据。
依赖 # Apache Flink 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。 该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。 当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。 有关 Kafka 兼容性的更多细节，请参考 Kafka 官方文档。
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kafka&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 如果使用 Kafka source，flink-connector-base 也需要包含在依赖中：
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-base&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Flink 目前的流连接器还不是二进制发行版的一部分。 在此处可以了解到如何链接它们，从而在集群中运行。
为了在 PyFlink 作业中使用 Kafka connector ，需要添加下列依赖： Kafka version PyFlink JAR universal Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Cassandra</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/cassandra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/cassandra/</guid>
      <description>Apache Cassandra Connector # This connector provides sinks that writes data into a Apache Cassandra database.
To use this connector, add the following dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-cassandra_2.12&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here.
Installing Apache Cassandra # There are multiple ways to bring up a Cassandra instance on local machine:</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/elasticsearch/</guid>
      <description>Elasticsearch 连接器 # 此连接器提供可以向 Elasticsearch 索引请求文档操作的 sinks。 要使用此连接器，请根据 Elasticsearch 的安装版本将以下依赖之一添加到你的项目中：
Elasticsearch 版本 Maven 依赖 6.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch6&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 7.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch7&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 Elasticsearch connector ，需要添加下列依赖： Elasticsearch version PyFlink JAR 6.x Only available for stable releases. 7.x and later versions Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 请注意，流连接器目前不是二进制发行版的一部分。 有关如何将程序和用于集群执行的库一起打包，参考此文档。
安装 Elasticsearch # Elasticsearch 集群的设置可以参考此文档。</description>
    </item>
    
    <item>
      <title>Firehose</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/firehose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/firehose/</guid>
      <description>Amazon Kinesis Data Firehose Sink # The Firehose sink writes to Amazon Kinesis Data Firehose.
Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to setup a Kinesis Data Firehose delivery stream.
To use the connector, add the following Maven dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-firehose&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 AWS Kinesis Firehose connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 The KinesisFirehoseSink uses AWS v2 SDK for Java to write data from a Flink stream into a Firehose delivery stream.</description>
    </item>
    
    <item>
      <title>Kinesis</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kinesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/kinesis/</guid>
      <description>亚马逊 Kinesis 数据流 SQL 连接器 # Kinesis 连接器提供访问 Amazon Kinesis Data Streams 。
使用此连接器, 取决于您是否读取数据和/或写入数据，增加下面依赖项的一个或多个到您的项目中:
KDS Connectivity Maven Dependency Source &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kinesis&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Sink &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-streams&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 由于许可证问题，以前的版本中 flink-connector-kinesis 工件没有部署到Maven中心库。有关更多信息，请参阅特定版本的文档。
为了在 PyFlink 作业中使用 Kinesis connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 使用亚马逊 Kinesis 流服务 # 遵循 Amazon Kinesis Streams Developer Guide 的指令建立 Kinesis 流。</description>
    </item>
    
    <item>
      <title>文件系统</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/filesystem/</guid>
      <description>文件系统 # 连接器提供了 BATCH 模式和 STREAMING 模式统一的 Source 和 Sink。Flink FileSystem abstraction 支持连接器对文件系统进行（分区）文件读写。文件系统连接器为 BATCH 和 STREAMING 模式提供了相同的保证，而且对 STREAMING 模式执行提供了精确一次（exactly-once）语义保证。
连接器支持对任意（分布式的）文件系统（例如，POSIX、 S3、 HDFS）以某种数据格式 format (例如，Avro、 CSV、 Parquet) 对文件进行写入，或者读取后生成数据流或一组记录。
File Source # File Source 是基于 Source API 同时支持批模式和流模式文件读取的统一 Source。 File Source 分为以下两个部分：SplitEnumerator 和 SourceReader。
SplitEnumerator 负责发现和识别需要读取的文件，并将这些文件分配给 SourceReader 进行读取。 SourceReader 请求需要处理的文件，并从文件系统中读取该文件。 可能需要指定某种 format 与 File Source 联合进行解析 CSV、解码AVRO、或者读取 Parquet 列式文件。
有界流和无界流 # 有界的 File Source（通过 SplitEnumerator）列出所有文件（一个过滤出隐藏文件的递归目录列表）并读取。
无界的 File Source 由配置定期扫描文件的 enumerator 创建。 在无界的情况下，SplitEnumerator 将像有界的 File Source 一样列出所有文件，但是不同的是，经过一个时间间隔之后，重复上述操作。 对于每一次列举操作，SplitEnumerator 会过滤掉之前已经检测过的文件，将新扫描到的文件发送给 SourceReader。</description>
    </item>
    
    <item>
      <title>RabbitMQ</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/rabbitmq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/rabbitmq/</guid>
      <description>RabbitMQ 连接器 # RabbitMQ 连接器的许可证 # Flink 的 RabbitMQ 连接器依赖了 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo;，它基于三种协议下发行：Mozilla Public License 1.1 (&amp;ldquo;MPL&amp;rdquo;)、GNU General Public License version 2 (&amp;ldquo;GPL&amp;rdquo;) 和 Apache License version 2 (&amp;ldquo;ASL&amp;rdquo;)。
Flink 自身既没有复用 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; 的代码，也没有将 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; 打二进制包。
如果用户发布的内容是基于 Flink 的 RabbitMQ 连接器的（进而重新发布了 &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; ），那么一定要注意这可能会受到 Mozilla Public License 1.1 (&amp;ldquo;MPL&amp;rdquo;)、GNU General Public License version 2 (&amp;ldquo;GPL&amp;rdquo;)、Apache License version 2 (&amp;ldquo;ASL&amp;rdquo;) 协议的限制.
RabbitMQ 连接器 # 这个连接器可以访问 RabbitMQ 的数据流。使用这个连接器，需要在工程里添加下面的依赖：</description>
    </item>
    
    <item>
      <title>Google Cloud PubSub</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pubsub/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pubsub/</guid>
      <description>Google Cloud PubSub # 这个连接器可向 Google Cloud PubSub 读取与写入数据。添加下面的依赖来使用此连接器:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-pubsub&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 注意：此连接器最近才加到 Flink 里，还未接受广泛测试。 注意连接器目前还不是二进制发行版的一部分，添加依赖、打包配置以及集群运行信息请参考这里
Consuming or Producing PubSubMessages # 连接器可以接收和发送 Google PubSub 的信息。和 Google PubSub 一样，这个连接器能够保证至少一次的语义。
PubSub SourceFunction # PubSubSource 类的对象由构建类来构建: PubSubSource.newBuilder(...)
有多种可选的方法来创建 PubSubSource，但最低要求是要提供 Google Project、Pubsub 订阅和反序列化 PubSubMessages 的方法。
Example:
Java StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment(); DeserializationSchema&amp;lt;SomeObject&amp;gt; deserializer = (...); SourceFunction&amp;lt;SomeObject&amp;gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializer) .withProjectName(&amp;#34;project&amp;#34;) .withSubscriptionName(&amp;#34;subscription&amp;#34;) .build(); streamExecEnv.addSource(pubsubSource); 当前还不支持 PubSub 的 source functions pulls messages 和 push endpoints。</description>
    </item>
    
    <item>
      <title>Hybrid Source</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/hybridsource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/hybridsource/</guid>
      <description>Hybrid Source # HybridSource is a source that contains a list of concrete sources. It solves the problem of sequentially reading input from heterogeneous sources to produce a single input stream.
For example, a bootstrap use case may need to read several days worth of bounded input from S3 before continuing with the latest unbounded input from Kafka. HybridSource switches from FileSource to KafkaSource when the bounded file input finishes without interrupting the application.</description>
    </item>
    
    <item>
      <title>Pulsar</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pulsar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/pulsar/</guid>
      <description>Apache Pulsar 连接器 # Flink 当前提供 Apache Pulsar Source 和 Sink 连接器，用户可以使用它从 Pulsar 读取数据，并保证每条数据只被处理一次。
添加依赖 # Pulsar Source 当前支持 Pulsar 2.8.1 之后的版本，但是 Pulsar Source 使用到了 Pulsar 的事务机制，建议在 Pulsar 2.9.2 及其之后的版本上使用 Pulsar Source 进行数据读取。
如果想要了解更多关于 Pulsar API 兼容性设计，可以阅读文档 PIP-72。
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-pulsar&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 为了在 PyFlink 作业中使用 Pulsar connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 Flink 的流连接器并不会放到发行文件里面一同发布，阅读此文档，了解如何将连接器添加到集群实例内。
Pulsar Source # Pulsar Source 基于 Flink 最新的批流一体 API 进行开发。 使用示例 # Pulsar Source 提供了 builder 类来构造 PulsarSource 实例。下面的代码实例使用 builder 类创建的实例会从 “persistent://public/default/my-topic” 的数据开始端进行消费。对应的 Pulsar Source 使用了 Exclusive（独占）的订阅方式消费消息，订阅名称为 my-subscription，并把消息体的二进制字节流以 UTF-8 的方式编码为字符串。</description>
    </item>
    
    <item>
      <title>JDBC</title>
      <link>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/zh/docs/connectors/datastream/jdbc/</guid>
      <description>JDBC Connector # 该连接器可以向 JDBC 数据库写入数据。
添加下面的依赖以便使用该连接器（同时添加 JDBC 驱动）：
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-jdbc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 注意该连接器目前还 不是 二进制发行版的一部分，如何在集群中运行请参考 这里。
已创建的 JDBC Sink 能够保证至少一次的语义。 更有效的精确执行一次可以通过 upsert 语句或幂等更新实现。
用法示例： Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .fromElements(...) .addSink(JdbcSink.sink( &amp;#34;insert into books (id, title, author, price, qty) values (?,?,?,?,?)&amp;#34;, (ps, t) -&amp;gt; { ps.setInt(1, t.id); ps.setString(2, t.title); ps.setString(3, t.author); ps.setDouble(4, t.price); ps.setInt(5, t.qty); }, new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withUrl(getDbMetadata().getUrl()) .withDriverName(getDbMetadata().getDriverClass()) .build())); env.execute(); Python env = StreamExecutionEnvironment.</description>
    </item>
    
  </channel>
</rss>
