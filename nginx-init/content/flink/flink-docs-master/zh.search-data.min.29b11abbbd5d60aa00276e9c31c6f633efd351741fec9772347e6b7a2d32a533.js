"use strict";(function(){const t={encode:!1,tokenize:function(e){return e.replace(/[\x00-\x7F]/g,"").split("")}};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/",title:"DataStream Connectors",section:"Connectors",content:""}),e.add({id:1,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/overview/",title:"Formats",section:"Formats",content:` Formats # Flink 提供了一套与表连接器（table connector）一起使用的表格式（table format）。表格式是一种存储格式，定义了如何把二进制数据映射到表的列上。
Flink 支持以下格式：
Formats Supported Connectors CSV Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem JSON Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem, Elasticsearch Apache Avro Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem Confluent Avro Apache Kafka, Upsert Kafka Debezium CDC Apache Kafka, Filesystem Canal CDC Apache Kafka, Filesystem Maxwell CDC Apache Kafka, Filesystem OGG CDC Apache Kafka, Filesystem Apache Parquet Filesystem Apache ORC Filesystem Raw Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem `}),e.add({id:2,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/overview/",title:"Overview",section:"Formats",content:` DataStream Formats # Available Formats # Formats define how information is encoded for storage. Currently these formats are supported:
Avro Azure Table Hadoop Parquet Text files Back to top
`}),e.add({id:3,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/overview/",title:"Overview",section:"Hive",content:` Hive # Apache Hive 已经成为了数据仓库生态系统中的核心。 它不仅仅是一个用于大数据分析和ETL场景的SQL引擎，同样它也是一个数据管理平台，可用于发现，定义，和演化数据。
Flink 与 Hive 的集成包含两个层面。
一是利用了 Hive 的 MetaStore 作为持久化的 Catalog，用户可通过HiveCatalog将不同会话中的 Flink 元数据存储到 Hive Metastore 中。 例如，用户可以使用HiveCatalog将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。
二是利用 Flink 来读写 Hive 的表。
HiveCatalog的设计提供了与 Hive 良好的兼容性，用户可以\u0026quot;开箱即用\u0026quot;的访问其已有的 Hive 数仓。 您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。
支持的Hive版本 # Flink 支持一下的 Hive 版本。
1.0 1.0.0 1.0.1 1.1 1.1.0 1.1.1 1.2 1.2.0 1.2.1 1.2.2 2.0 2.0.0 2.0.1 2.1 2.1.0 2.1.1 2.2 2.2.0 2.3 2.3.0 2.3.1 2.3.2 2.3.3 2.3.4 2.3.5 2.3.6 2.3.7 2.3.8 2.3.9 3.1 3.1.0 3.1.1 3.1.2 3.1.3 请注意，某些功能是否可用取决于您使用的 Hive 版本，这些限制不是由 Flink 所引起的：
Hive 内置函数在使用 Hive-2.3.0 及更高版本时支持。 列约束，也就是 PRIMARY KEY 和 NOT NULL，在使用 Hive-3.1.0 及更高版本时支持。 更改表的统计信息，在使用 Hive-2.3.0 及更高版本时支持。 DATE列统计信息，在使用 Hive-2.3.0 及更高版时支持。 依赖项 # 要与 Hive 集成，您需要在 Flink 下的/lib/目录中添加一些额外的依赖包， 以便通过 Table API 或 SQL Client 与 Hive 进行交互。 或者，您可以将这些依赖项放在专用文件夹中，并分别使用 Table API 程序或 SQL Client 的-C或-l选项将它们添加到 classpath 中。
Apache Hive 是基于 Hadoop 之上构建的, 首先您需要 Hadoop 的依赖，请参考 Providing Hadoop classes:
export HADOOP_CLASSPATH=\`hadoop classpath\` 有两种添加 Hive 依赖项的方法。第一种是使用 Flink 提供的 Hive Jar包。您可以根据使用的 Metastore 的版本来选择对应的 Hive jar。第二个方式是分别添加每个所需的 jar 包。如果您使用的 Hive 版本尚未在此处列出，则第二种方法会更适合。
注意：建议您优先使用 Flink 提供的 Hive jar 包。仅在 Flink 提供的 Hive jar 不满足您的需求时，再考虑使用分开添加 jar 包的方式。
使用 Flink 提供的 Hive jar # 下表列出了所有可用的 Hive jar。您可以选择一个并放在 Flink 发行版的/lib/ 目录中。
Metastore version Maven dependency SQL Client JAR 2.3.0 - 2.3.9 flink-sql-connector-hive-2.3.9 Only available for stable releases 3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Only available for stable releases 用户定义的依赖项 # 您可以在下方找到不同Hive主版本所需要的依赖项。
Hive 2.3.4 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector.Contains flink-hadoop-compatibility and flink-orc jars flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.3.4.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.0.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.0.0.jar hive-exec-1.0.0.jar libfb303-0.9.0.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.1.0.jar hive-exec-1.1.0.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 1.2.1 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-metastore-1.2.1.jar hive-exec-1.2.1.jar libfb303-0.9.2.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3-nohive.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.0.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.0.0.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.1.0.jar // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 2.2.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-2.2.0.jar // Orc dependencies -- required by the ORC vectorized optimizations orc-core-1.4.3.jar aircompressor-0.8.jar // transitive dependency of orc-core // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Hive 3.1.0 /flink-1.16-SNAPSHOT /lib // Flink\u0026#39;s Hive connector flink-connector-hive_2.12-1.16-SNAPSHOT.jar // Hive dependencies hive-exec-3.1.0.jar libfb303-0.9.3.jar // libfb303 is not packed into hive-exec in some versions, need to add it separately // add antlr-runtime if you need to use hive dialect antlr-runtime-3.5.2.jar Maven 依赖 # 如果您在构建自己的应用程序，则需要在 mvn 文件中添加以下依赖项。 您应该在运行时添加以上的这些依赖项，而不要在已生成的 jar 文件中去包含它们。
\u0026lt;!-- Flink Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Hive Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-exec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\${hive.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 连接到Hive # 通过 TableEnvironment 或者 YAML 配置，使用 Catalog 接口 和 HiveCatalog连接到现有的 Hive 集群。
以下是如何连接到 Hive 的示例：
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name = \u0026#34;myhive\u0026#34;; String defaultDatabase = \u0026#34;mydatabase\u0026#34;; String hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34;; HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir); tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive); // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;); Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;myhive\u0026#34; val defaultDatabase = \u0026#34;mydatabase\u0026#34; val hiveConfDir = \u0026#34;/opt/hive-conf\u0026#34; val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir) tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, hive) // set the HiveCatalog as the current catalog of the session tableEnv.useCatalog(\u0026#34;myhive\u0026#34;) Python from pyflink.table import * from pyflink.table.catalog import HiveCatalog settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) catalog_name = \u0026#34;myhive\u0026#34; default_database = \u0026#34;mydatabase\u0026#34; hive_conf_dir = \u0026#34;/opt/hive-conf\u0026#34; hive_catalog = HiveCatalog(catalog_name, default_database, hive_conf_dir) t_env.register_catalog(\u0026#34;myhive\u0026#34;, hive_catalog) # set the HiveCatalog as the current catalog of the session tableEnv.use_catalog(\u0026#34;myhive\u0026#34;) YAML execution: ... current-catalog: myhive # set the HiveCatalog as the current catalog of the session current-database: mydatabase catalogs: - name: myhive type: hive hive-conf-dir: /opt/hive-conf SQL CREATE CATALOG myhive WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;default-database\u0026#39; = \u0026#39;mydatabase\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39; ); -- set the HiveCatalog as the current catalog of the session USE CATALOG myhive; 下表列出了通过 YAML 文件或 DDL 定义 HiveCatalog 时所支持的参数。
参数 必选 默认值 类型 描述 type 是 (无) String Catalog 的类型。 创建 HiveCatalog 时，该参数必须设置为'hive'。 name 是 (无) String Catalog 的名字。仅在使用 YAML file 时需要指定。 hive-conf-dir 否 (无) String 指向包含 hive-site.xml 目录的 URI。 该 URI 必须是 Hadoop 文件系统所支持的类型。 如果指定一个相对 URI，即不包含 scheme，则默认为本地文件系统。如果该参数没有指定，我们会在 class path 下查找hive-site.xml。 default-database 否 default String 当一个catalog被设为当前catalog时，所使用的默认当前database。 hive-version 否 (无) String HiveCatalog 能够自动检测使用的 Hive 版本。我们建议不要手动设置 Hive 版本，除非自动检测机制失败。 hadoop-conf-dir 否 (无) String Hadoop 配置文件目录的路径。目前仅支持本地文件系统路径。我们推荐使用 HADOOP_CONF_DIR 环境变量来指定 Hadoop 配置。因此仅在环境变量不满足您的需求时再考虑使用该参数，例如当您希望为每个 HiveCatalog 单独设置 Hadoop 配置时。 DDL # 在 Flink 中执行 DDL 操作 Hive 的表、视图、分区、函数等元数据时，建议使用 Hive 方言
DML # Flink 支持 DML 写入 Hive 表，请参考读写 Hive 表
`}),e.add({id:4,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/overview/",title:"Overview",section:"Operators",content:` Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
DataStream Transformations # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., mapping, filtering, reducing). Please see operators for an overview of the available transformations in Python DataStream API.
Functions # Transformations accept user-defined functions as input to define the functionality of the transformations. The following section describes different ways of defining Python user-defined functions in Python DataStream API.
Implementing Function Interfaces # Different Function interfaces are provided for different transformations in the Python DataStream API. For example, MapFunction is provided for the map transformation, FilterFunction is provided for the filter transformation, etc. Users can implement the corresponding Function interface according to the type of the transformation. Take MapFunction for instance:
# Implementing MapFunction class MyMapFunction(MapFunction): def map(self, value): return value + 1 data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(MyMapFunction(), output_type=Types.INT()) Lambda Function # As shown in the following example, the transformations can also accept a lambda function to define the functionality of the transformation:
data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(lambda x: x + 1, output_type=Types.INT()) Note ConnectedStream.map() and ConnectedStream.flat_map() do not support lambda function and must accept CoMapFunction and CoFlatMapFunction separately.
Python Function # Users could also use Python function to define the functionality of the transformation:
def my_map_func(value): return value + 1 data_stream = env.from_collection([1, 2, 3, 4, 5], type_info=Types.INT()) mapped_stream = data_stream.map(my_map_func, output_type=Types.INT()) Output Type # Users could specify the output type information of the transformation explicitly in Python DataStream API. If not specified, the output type will be Types.PICKLED_BYTE_ARRAY by default, and the result data will be serialized using pickle serializer. For more details about the pickle serializer, please refer to Pickle Serialization.
Generally, the output type needs to be specified in the following scenarios.
Convert DataStream into Table # from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment def data_stream_api_demo(): env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;number-of-rows\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) ds = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.INT(), Types.STRING()])) def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.TUPLE([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: (i[0] + j[0], i[1])) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_sink ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = t_env.from_data_stream(ds) table_result = table.execute_insert(\u0026#34;my_sink\u0026#34;) # 1）等待作业执行结束，用于local执行，否则可能作业尚未执行结束，该脚本已退出，会导致minicluster过早退出 # 2）当作业通过detach模式往remote集群提交时，比如YARN/Standalone/K8s等，需要移除该方法 table_result.wait() if __name__ == \u0026#39;__main__\u0026#39;: data_stream_api_demo() The output type must be specified for the flat_map operation in the above example which will be used as the output type of the reduce operation implicitly. The reason is that t_env.from_data_stream(ds) requires the output type of ds must be a composite type.
Write DataStream to Sink # from pyflink.common.typeinfo import Types def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds.map(lambda i: (i[0] + 1, i[1]), Types.TUPLE([Types.INT(), Types.STRING()])) \\ .sink_to(...) Generally, the output type needs to be specified for the map operation in the above example if the sink only accepts special kinds of data, e.g. Row, etc.
Operator Chaining # By default, multiple non-shuffle Python functions will be chained together to avoid the serialization and deserialization and improve the performance. There are also cases where you may want to disable the chaining, e.g., there is a flatmap function which will produce a large number of elements for each input element and disabling the chaining allows to process its output in a different parallelism.
Operator chaining could be disabled in one of the following ways:
Disable chaining with following operators by adding a key_by operation, shuffle operation, rescale operation, rebalance operation or partition_custom operation after the current operator. Disable chaining with preceding operators by applying a start_new_chain operation for the current operator. Disable chaining with preceding and following operators by applying a disable_chaining operation for the current operator. Disable chaining of two operators by setting different parallelisms or different slot sharing group for them. You could also disable all the operator chaining via configuration python.operator-chaining.enabled. Bundling Python Functions # To run Python functions in any non-local mode, it is strongly recommended bundling your Python functions definitions using the config option python-files, if your Python functions live outside the file where the main() function is defined. Otherwise, you may run into ModuleNotFoundError: No module named 'my_function' if you define Python functions in a file called my_function.py.
Loading resources in Python Functions # There are scenarios when you want to load some resources in Python functions first, then running computation over and over again, without having to re-load the resources. For example, you may want to load a large deep learning model only once, then run batch prediction against the model multiple times.
Overriding the open method inherited from the base class Function is exactly what you need.
class Predict(MapFunction): def open(self, runtime_context: RuntimeContext): import pickle with open(\u0026#34;resources.zip/resources/model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: self.model = pickle.load(f) def eval(self, x): return self.model.predict(x) `}),e.add({id:5,href:"/flink/flink-docs-master/zh/docs/libs/gelly/overview/",title:"Overview",section:"Graphs",content:` Gelly: Flink Graph API # Gelly is a Graph API for Flink. It contains a set of methods and utilities which aim to simplify the development of graph analysis applications in Flink. In Gelly, graphs can be transformed and modified using high-level functions similar to the ones provided by the batch processing API. Gelly provides methods to create, transform and modify graphs, as well as a library of graph algorithms.
Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators Bipartite Graphs Using Gelly # Gelly is currently part of the libraries Maven project. All relevant classes are located in the org.apache.flink.graph package.
Add the following dependency to your pom.xml to use Gelly.
Java \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-gelly\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Scala \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-gelly-scala_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note that Gelly is not part of the binary distribution. See linking for instructions on packaging Gelly libraries into Flink user programs.
The remaining sections provide a description of available methods and present several examples of how to use Gelly and how to mix it with the Flink DataSet API.
Running Gelly Examples # The Gelly library jars are provided in the Flink distribution in the opt directory (for versions older than Flink 1.2 these can be manually downloaded from Maven Central). To run the Gelly examples the flink-gelly (for Java) or flink-gelly-scala (for Scala) jar must be copied to Flink\u0026rsquo;s lib directory.
cp opt/flink-gelly_*.jar lib/ cp opt/flink-gelly-scala_*.jar lib/ Gelly\u0026rsquo;s examples jar includes drivers for each of the library methods and is provided in the examples directory. After configuring and starting the cluster, list the available algorithm classes:
./bin/start-cluster.sh ./bin/flink run examples/gelly/flink-gelly-examples_*.jar The Gelly drivers can generate graph data or read the edge list from a CSV file (each node in a cluster must have access to the input file). The algorithm description, available inputs and outputs, and configuration are displayed when an algorithm is selected. Print usage for JaccardIndex:
./bin/flink run examples/gelly/flink-gelly-examples_*.jar --algorithm JaccardIndex Display graph metrics for a million vertex graph:
./bin/flink run examples/gelly/flink-gelly-examples_*.jar \\ --algorithm GraphMetrics --order directed \\ --input RMatGraph --type integer --scale 20 --simplify directed \\ --output print The size of the graph is adjusted by the --scale and --edge_factor parameters. The library generator provides access to additional configuration to adjust the power-law skew and random noise.
Sample social network data is provided by the Stanford Network Analysis Project. The com-lj data set is a good starter size. Run a few algorithms and monitor the job progress in Flink\u0026rsquo;s Web UI:
wget -O - http://snap.stanford.edu/data/bigdata/communities/com-lj.ungraph.txt.gz | gunzip -c \u0026gt; com-lj.ungraph.txt ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm GraphMetrics --order undirected \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output print ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm ClusteringCoefficient --order undirected \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output hash ./bin/flink run -q examples/gelly/flink-gelly-examples_*.jar \\ --algorithm JaccardIndex \\ --input CSV --type integer --simplify undirected --input_filename com-lj.ungraph.txt --input_field_delimiter \$\u0026#39;\\t\u0026#39; \\ --output hash Please submit feature requests and report issues on the user mailing list or Flink Jira. We welcome suggestions for new algorithms and features as well as code contributions.
Back to top
`}),e.add({id:6,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/intro_to_datastream_api/",title:"Python DataStream API 简介",section:"DataStream API",content:` Intro to the Python DataStream API # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal).
Python DataStream API is a Python version of DataStream API which allows Python users could write Python DatStream API jobs.
Common Structure of Python DataStream API Programs # The following code example shows the common structure of Python DataStream API programs.
from pyflink.common import WatermarkStrategy, Row from pyflink.common.serialization import Encoder from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig from pyflink.datastream.connectors.number_seq import NumberSequenceSource from pyflink.datastream.functions import RuntimeContext, MapFunction from pyflink.datastream.state import ValueStateDescriptor class MyMapFunction(MapFunction): def open(self, runtime_context: RuntimeContext): state_desc = ValueStateDescriptor(\u0026#39;cnt\u0026#39;, Types.PICKLED_BYTE_ARRAY()) self.cnt_state = runtime_context.get_state(state_desc) def map(self, value): cnt = self.cnt_state.value() if cnt is None or cnt \u0026lt; 2: self.cnt_state.update(1 if cnt is None else cnt + 1) return value[0], value[1] + 1 else: return value[0], value[1] def state_access_demo(): # 1. create a StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() # 2. create source DataStream seq_num_source = NumberSequenceSource(1, 10000) ds = env.from_source( source=seq_num_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#39;seq_num_source\u0026#39;, type_info=Types.LONG()) # 3. define the execution logic ds = ds.map(lambda a: Row(a % 4, 1), output_type=Types.ROW([Types.LONG(), Types.LONG()])) \\ .key_by(lambda a: a[0]) \\ .map(MyMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.LONG()])) # 4. create sink and emit result to sink output_path = \u0026#39;/opt/output/\u0026#39; file_sink = FileSink \\ .for_row_format(output_path, Encoder.simple_string_encoder()) \\ .with_output_file_config(OutputFileConfig.builder().with_part_prefix(\u0026#39;pre\u0026#39;).with_part_suffix(\u0026#39;suf\u0026#39;).build()) \\ .build() ds.sink_to(file_sink) # 5. execute the job env.execute(\u0026#39;state_access_demo\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: state_access_demo() Back to top
Create a StreamExecutionEnvironment # The StreamExecutionEnvironment is a central concept of the DataStream API program. The following code example shows how to create a StreamExecutionEnvironment:
from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() Back to top
Create a DataStream # The DataStream API gets its name from the special DataStream class that is used to represent a collection of data in a Flink program. You can think of them as immutable collections of data that can contain duplicates. This data can either be finite or unbounded, the API that you use to work on them is the same.
A DataStream is similar to a regular Python Collection in terms of usage but is quite different in some key ways. They are immutable, meaning that once they are created you cannot add or remove elements. You can also not simply inspect the elements inside but only work on them using the DataStream API operations, which are also called transformations.
You can create an initial DataStream by adding a source in a Flink program. Then you can derive new streams from this and combine them by using API methods such as map, filter, and so on.
Create from a list object # You can create a DataStream from a list object:
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() ds = env.from_collection( collection=[(1, \u0026#39;aaa|bb\u0026#39;), (2, \u0026#39;bb|a\u0026#39;), (3, \u0026#39;aaa|a\u0026#39;)], type_info=Types.ROW([Types.INT(), Types.STRING()])) The parameter type_info is optional, if not specified, the output type of the returned DataStream will be Types.PICKLED_BYTE_ARRAY().
Create using DataStream connectors # You can also create a DataStream using DataStream connectors with method add_source as following:
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer from pyflink.datastream.format.json import JsonRowDeserializationSchema env = StreamExecutionEnvironment.get_execution_environment() # the sql connector for kafka is used here as it\u0026#39;s a fat jar and could avoid dependency issues env.add_jars(\u0026#34;file:///path/to/flink-sql-connector-kafka.jar\u0026#34;) deserialization_schema = JsonRowDeserializationSchema.builder() \\ .type_info(type_info=Types.ROW([Types.INT(), Types.STRING()])).build() kafka_consumer = FlinkKafkaConsumer( topics=\u0026#39;test_source_topic\u0026#39;, deserialization_schema=deserialization_schema, properties={\u0026#39;bootstrap.servers\u0026#39;: \u0026#39;localhost:9092\u0026#39;, \u0026#39;group.id\u0026#39;: \u0026#39;test_group\u0026#39;}) ds = env.add_source(kafka_consumer) Note It currently only supports FlinkKafkaConsumer to be used as DataStream source connectors with method add_source.
Note The DataStream created using add_source could only be executed in streaming executing mode.
You could also call the from_source method to create a DataStream using unified DataStream source connectors:
from pyflink.common.typeinfo import Types from pyflink.common.watermark_strategy import WatermarkStrategy from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.number_seq import NumberSequenceSource env = StreamExecutionEnvironment.get_execution_environment() seq_num_source = NumberSequenceSource(1, 1000) ds = env.from_source( source=seq_num_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#39;seq_num_source\u0026#39;, type_info=Types.LONG()) Note Currently, it only supports NumberSequenceSource and FileSource as unified DataStream source connectors.
Note The DataStream created using from_source could be executed in both batch and streaming executing mode.
Create using Table \u0026amp; SQL connectors # Table \u0026amp; SQL connectors could also be used to create a DataStream. You could firstly create a Table using Table \u0026amp; SQL connectors and then convert it to a DataStream.
from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;number-of-rows\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) ds = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.INT(), Types.STRING()])) Note The StreamExecutionEnvironment env should be specified when creating the TableEnvironment t_env.
Note As all the Java Table \u0026amp; SQL connectors could be used in PyFlink Table API, this means that all of them could also be used in PyFlink DataStream API.
Back to top
DataStream Transformations # Operators transform one or more DataStream into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
The following example shows a simple example about how to convert a DataStream into another DataStream using map transformation:
ds = ds.map(lambda a: a + 1) Please see operators for an overview of the available DataStream transformations.
Conversion between DataStream and Table # It also supports to convert a DataStream to a Table and vice verse.
# convert a DataStream to a Table table = t_env.from_data_stream(ds, \u0026#39;a, b, c\u0026#39;) # convert a Table to a DataStream ds = t_env.to_append_stream(table, Types.ROW([Types.INT(), Types.STRING()])) # or ds = t_env.to_retract_stream(table, Types.ROW([Types.INT(), Types.STRING()])) Back to top
Emit Results # Print # You can call the print method to print the data of a DataStream to the standard output:
ds.print() Collect results to client # You can call the execute_and_collect method to collect the data of a DataStream to client:
with ds.execute_and_collect() as results: for result in results: print(result) Note The method execute_and_collect will collect the data of the DataStream to the memory of the client and so it\u0026rsquo;s a good practice to limit the number of rows collected.
Emit results to a DataStream sink connector # You can call the add_sink method to emit the data of a DataStream to a DataStream sink connector:
from pyflink.common.typeinfo import Types from pyflink.datastream.connectors.kafka import FlinkKafkaProducer from pyflink.datastream.formats.json import JsonRowSerializationSchema serialization_schema = JsonRowSerializationSchema.builder().with_type_info( type_info=Types.ROW([Types.INT(), Types.STRING()])).build() kafka_producer = FlinkKafkaProducer( topic=\u0026#39;test_sink_topic\u0026#39;, serialization_schema=serialization_schema, producer_config={\u0026#39;bootstrap.servers\u0026#39;: \u0026#39;localhost:9092\u0026#39;, \u0026#39;group.id\u0026#39;: \u0026#39;test_group\u0026#39;}) ds.add_sink(kafka_producer) Note It currently only supports FlinkKafkaProducer and JdbcSink to be used as DataStream sink connectors with method add_sink.
Note The method add_sink could only be used in streaming executing mode.
You could also call the sink_to method to emit the data of a DataStream to a unified DataStream sink connector:
from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig from pyflink.common.serialization import Encoder output_path = \u0026#39;/opt/output/\u0026#39; file_sink = FileSink \\ .for_row_format(output_path, Encoder.simple_string_encoder()) \\ .with_output_file_config(OutputFileConfig.builder().with_part_prefix(\u0026#39;pre\u0026#39;).with_part_suffix(\u0026#39;suf\u0026#39;).build()) \\ .build() ds.sink_to(file_sink) Note It currently only supports FileSink as unified DataStream sink connectors.
Note The method sink_to could be used in both batch and streaming executing mode.
Emit results to a Table \u0026amp; SQL sink connector # Table \u0026amp; SQL connectors could also be used to write out a DataStream. You need firstly convert a DataStream to a Table and then write it to a Table \u0026amp; SQL sink connector.
from pyflink.common import Row from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) # option 1：the result type of ds is Types.ROW def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield Row(s[0], sp) ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.ROW([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: Row(i[0] + j[0], i[1])) # option 1：the result type of ds is Types.TUPLE def split(s): splits = s[1].split(\u0026#34;|\u0026#34;) for sp in splits: yield s[0], sp ds = ds.map(lambda i: (i[0] + 1, i[1])) \\ .flat_map(split, Types.TUPLE([Types.INT(), Types.STRING()])) \\ .key_by(lambda i: i[1]) \\ .reduce(lambda i, j: (i[0] + j[0], i[1])) # emit ds to print sink t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_sink ( a INT, b VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = t_env.from_data_stream(ds) table_result = table.execute_insert(\u0026#34;my_sink\u0026#34;) Note The output type of DataStream ds must be composite type.
Submit Job # Finally, you should call the StreamExecutionEnvironment.execute method to submit the DataStream API job for execution:
env.execute() If you convert the DataStream to a Table and then write it to a Table API \u0026amp; SQL sink connector, it may happen that you need to submit the job using TableEnvironment.execute method.
t_env.execute() `}),e.add({id:7,href:"/flink/flink-docs-master/zh/docs/try-flink/",title:"Try Flink",section:"Docs",content:""}),e.add({id:8,href:"/flink/flink-docs-master/zh/docs/try-flink/local_installation/",title:"本地模式安装",section:"Try Flink",content:` 本地模式安装 # 注意：Apache Flink 社区只发布 Apache Flink 的 release 版本。
由于你当前正在查看的是文档最新的 SNAPSHOT 版本，因此相关内容会被隐藏。请通过左侧菜单底部的版本选择将文档切换到最新的 release 版本。
请按照以下几个步骤下载最新的稳定版本开始使用。
步骤 1：下载 # 为了运行Flink，只需提前安装好 Java 11。你可以通过以下命令来检查 Java 是否已经安装正确。
java -version 下载 release 1.16-SNAPSHOT 并解压。
\$ tar -xzf flink-1.16-SNAPSHOT-bin-scala_2.12.tgz \$ cd flink-1.16-SNAPSHOT-bin-scala_2.12 步骤 2：启动集群 # Flink 附带了一个 bash 脚本，可以用于启动本地集群。
\$ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host. Starting taskexecutor daemon on host. 步骤 3：提交作业（Job） # Flink 的 Releases 附带了许多的示例作业。你可以任意选择一个，快速部署到已运行的集群上。
\$ ./bin/flink run examples/streaming/WordCount.jar \$ tail log/flink-*-taskexecutor-*.out (nymph,1) (in,3) (thy,1) (orisons,1) (be,4) (all,2) (my,1) (sins,1) (remember,1) (d,4) 另外，你可以通过 Flink 的 Web UI 来监视集群的状态和正在运行的作业。
步骤 4：停止集群 # 完成后，你可以快速停止集群和所有正在运行的组件。
\$ ./bin/stop-cluster.sh `}),e.add({id:9,href:"/flink/flink-docs-master/zh/docs/concepts/overview/",title:"概览",section:"概念透析",content:` 概念透析 # 实践练习章节介绍了作为 Flink API 根基的有状态实时流处理的基本概念，并且举例说明了如何在 Flink 应用中使用这些机制。其中 Data Pipelines \u0026amp; ETL 小节介绍了有状态流处理的概念，并且在 Fault Tolerance 小节中进行了深入介绍。Streaming Analytics 小节介绍了实时流处理的概念。
本章将深入分析 Flink 分布式运行时架构如何实现这些概念。
Flink 中的 API # Flink 为流式/批式处理应用程序的开发提供了不同级别的抽象。
Flink API 最底层的抽象为有状态实时流处理。其抽象实现是 Process Function，并且 Process Function 被 Flink 框架集成到了 DataStream API 中来为我们使用。它允许用户在应用程序中自由地处理来自单流或多流的事件（数据），并提供具有全局一致性和容错保障的状态。此外，用户可以在此层抽象中注册事件时间（event time）和处理时间（processing time）回调方法，从而允许程序可以实现复杂计算。
Flink API 第二层抽象是 Core APIs。实际上，许多应用程序不需要使用到上述最底层抽象的 API，而是可以使用 Core APIs 进行编程：其中包含 DataStream API（应用于有界/无界数据流场景）和 DataSet API（应用于有界数据集场景）两部分。Core APIs 提供的流式 API（Fluent API）为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换（transformations）、联接（joins）、聚合（aggregations）、窗口（windows）和状态（state）操作等。此层 API 中处理的数据类型在每种编程语言中都有其对应的类。
Process Function 这类底层抽象和 DataStream API 的相互集成使得用户可以选择使用更底层的抽象 API 来实现自己的需求。DataSet API 还额外提供了一些原语，比如循环/迭代（loop/iteration）操作。
Flink API 第三层抽象是 Table API。Table API 是以表（Table）为中心的声明式编程（DSL）API，例如在流式数据场景下，它可以表示一张正在动态改变的表。Table API 遵循（扩展）关系模型：即表拥有 schema（类似于关系型数据库中的 schema），并且 Table API 也提供了类似于关系模型中的操作，比如 select、project、join、group-by 和 aggregate 等。Table API 程序是以声明的方式定义应执行的逻辑操作，而不是确切地指定程序应该执行的代码。尽管 Table API 使用起来很简洁并且可以由各种类型的用户自定义函数扩展功能，但还是比 Core API 的表达能力差。此外，Table API 程序在执行之前还会使用优化器中的优化规则对用户编写的表达式进行优化。
表和 DataStream/DataSet 可以进行无缝切换，Flink 允许用户在编写应用程序时将 Table API 与 DataStream/DataSet API 混合使用。
Flink API 最顶层抽象是 SQL。这层抽象在语义和程序表达式上都类似于 Table API，但是其程序实现都是 SQL 查询表达式。SQL 抽象与 Table API 抽象之间的关联是非常紧密的，并且 SQL 查询语句可以在 Table API 中定义的表上执行。
`}),e.add({id:10,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/overview/",title:"概览",section:"DataStream Connectors",content:` DataStream Connectors # 预定义的 Source 和 Sink # 一些比较基本的 Source 和 Sink 已经内置在 Flink 里。 预定义 data sources 支持从文件、目录、socket，以及 collections 和 iterators 中读取数据。 预定义 data sinks 支持把数据写入文件、标准输出（stdout）、标准错误输出（stderr）和 socket。
附带的连接器 # 连接器可以和多种多样的第三方系统进行交互。目前支持以下系统:
Apache Kafka (source/sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) FileSystem (sink) RabbitMQ (source/sink) Google PubSub (source/sink) Hybrid Source (source) Apache Pulsar (source) JDBC (sink) 请记住，在使用一种连接器时，通常需要额外的第三方组件，比如：数据存储服务器或者消息队列。 要注意这些列举的连接器是 Flink 工程的一部分，包含在发布的源码中，但是不包含在二进制发行版中。 更多说明可以参考对应的子部分。
Apache Bahir 中的连接器 # Flink 还有些一些额外的连接器通过 Apache Bahir 发布, 包括:
Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) 连接Flink的其他方法 # 异步 I/O # 使用connector并不是唯一可以使数据进入或者流出Flink的方式。 一种常见的模式是从外部数据库或者 Web 服务查询数据得到初始数据流，然后通过 Map 或者 FlatMap 对初始数据流进行丰富和增强。 Flink 提供了异步 I/O API 来让这个过程更加简单、高效和稳定。
可查询状态 # 当 Flink 应用程序需要向外部存储推送大量数据时会导致 I/O 瓶颈问题出现。在这种场景下，如果对数据的读操作远少于写操作，那么让外部应用从 Flink 拉取所需的数据会是一种更好的方式。 可查询状态 接口可以实现这个功能，该接口允许被 Flink 托管的状态可以被按需查询。
Back to top
`}),e.add({id:11,href:"/flink/flink-docs-master/zh/docs/connectors/table/overview/",title:"概览",section:"Table API Connectors",content:` Table \u0026amp; SQL Connectors # Flink\u0026rsquo;s Table API \u0026amp; SQL programs can be connected to other external systems for reading and writing both batch and streaming tables. A table source provides access to data which is stored in external systems (such as a database, key-value store, message queue, or file system). A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, Avro, Parquet, or ORC.
This page describes how to register table sources and table sinks in Flink using the natively supported connectors. After a source or sink has been registered, it can be accessed by Table API \u0026amp; SQL statements.
If you want to implement your own custom table source or sink, have a look at the user-defined sources \u0026amp; sinks page.
Supported Connectors # Flink natively support various connectors. The following tables list all available connectors.
Name Version Source Sink Filesystem Bounded and Unbounded Scan, Lookup Streaming Sink, Batch Sink Elasticsearch 6.x \u0026 7.x Not supported Streaming Sink, Batch Sink Apache Kafka 0.10+ Unbounded Scan Streaming Sink, Batch Sink Amazon Kinesis Data Streams Unbounded Scan Streaming Sink JDBC Bounded Scan, Lookup Streaming Sink, Batch Sink Apache HBase 1.4.x \u0026 2.2.x Bounded Scan, Lookup Streaming Sink, Batch Sink Apache Hive Supported Versions Unbounded Scan, Bounded Scan, Lookup Streaming Sink, Batch Sink Back to top
请查阅配置小节了解如何添加连接器依赖。
How to use connectors # Flink supports using SQL CREATE TABLE statements to register tables. One can define the table name, the table schema, and the table options for connecting to an external system.
See the SQL section for more information about creating a table.
The following code shows a full example of how to connect to Kafka for reading and writing JSON records.
SQL CREATE TABLE MyUserTable ( -- declare the schema of the table \`user\` BIGINT, \`message\` STRING, \`rowtime\` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, -- use a metadata column to access Kafka\u0026#39;s record timestamp \`proctime\` AS PROCTIME(), -- use a computed column to define a proctime attribute WATERMARK FOR \`rowtime\` AS \`rowtime\` - INTERVAL \u0026#39;5\u0026#39; SECOND -- use a WATERMARK statement to define a rowtime attribute ) WITH ( -- declare the external system to connect to \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;topic_name\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; -- declare a format for this system ) The desired connection properties are converted into string-based key-value pairs. Factories will create configured table sources, table sinks, and corresponding formats from the key-value pairs based on factory identifiers (kafka and json in this example). All factories that can be found via Java\u0026rsquo;s Service Provider Interfaces (SPI) are taken into account when searching for exactly one matching factory for each component.
If no factory can be found or multiple factories match for the given properties, an exception will be thrown with additional information about considered factories and supported properties.
Transform table connector/format resources # Flink uses Java\u0026rsquo;s Service Provider Interfaces (SPI) to load the table connector/format factories by their identifiers. Since the SPI resource file named org.apache.flink.table.factories.Factory for every table connector/format is under the same directory META-INF/services, these resource files will override each other when build the uber-jar of the project which uses more than one table connector/format, which will cause Flink to fail to load table connector/format factories.
In this situation, the recommended way is transforming these resource files under the directory META-INF/services by ServicesResourceTransformer of maven shade plugin. Given the pom.xml file content of example that contains connector flink-sql-connector-hive-3.1.2 and format flink-parquet in a project.
\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;myProject\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- other project dependencies ...--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-sql-connector-hive-3.1.2_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-parquet_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;shade\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformers combine.children=\u0026#34;append\u0026#34;\u0026gt; \u0026lt;!-- The service transformer is needed to merge META-INF/services files --\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; After configured the ServicesResourceTransformer, the table connector/format resource files under the directory META-INF/services would be merged rather than overwritten each other when build the uber-jar of above project.
Back to top
Schema Mapping # The body clause of a SQL CREATE TABLE statement defines the names and types of physical columns, constraints and watermarks. Flink doesn\u0026rsquo;t hold the data, thus the schema definition only declares how to map physical columns from an external system to Flink’s representation. The mapping may not be mapped by names, it depends on the implementation of formats and connectors. For example, a MySQL database table is mapped by field names (not case sensitive), and a CSV filesystem is mapped by field order (field names can be arbitrary). This will be explained in every connector.
The following example shows a simple schema without time attributes and one-to-one field mapping of input/output to table columns.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN ) WITH ( ... ) Metadata # Some connectors and formats expose additional metadata fields that can be accessed in metadata columns next to the physical payload columns. See the CREATE TABLE section for more information about metadata columns.
Primary Key # Primary key constraints tell that a column or a set of columns of a table are unique and they do not contain nulls. Primary key uniquely identifies a row in a table.
The primary key of a source table is a metadata information for optimization. The primary key of a sink table is usually used by the sink implementation for upserting.
SQL standard specifies that a constraint can either be ENFORCED or NOT ENFORCED. This controls if the constraint checks are performed on the incoming/outgoing data. Flink does not own the data the only mode we want to support is the NOT ENFORCED mode. Its up to the user to ensure that the query enforces key integrity.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN, PRIMARY KEY (MyField1, MyField2) NOT ENFORCED -- defines a primary key on columns ) WITH ( ... ) Time Attributes # Time attributes are essential when working with unbounded streaming tables. Therefore both proctime and rowtime attributes can be defined as part of the schema.
For more information about time handling in Flink and especially event-time, we recommend the general event-time section.
Proctime Attributes # In order to declare a proctime attribute in the schema, you can use Computed Column syntax to declare a computed column which is generated from PROCTIME() builtin function. The computed column is a virtual column which is not stored in the physical data.
SQL CREATE TABLE MyTable ( MyField1 INT, MyField2 STRING, MyField3 BOOLEAN, MyField4 AS PROCTIME() -- declares a proctime attribute ) WITH ( ... ) Rowtime Attributes # In order to control the event-time behavior for tables, Flink provides predefined timestamp extractors and watermark strategies.
Please refer to CREATE TABLE statements for more information about defining time attributes in DDL.
The following timestamp extractors are supported:
DDL -- use the existing TIMESTAMP(3) field in schema as the rowtime attribute CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ... ) WITH ( ... ) -- use system functions or UDFs or expressions to extract the expected TIMESTAMP(3) rowtime field CREATE TABLE MyTable ( log_ts STRING, ts_field AS TO_TIMESTAMP(log_ts), WATERMARK FOR ts_field AS ... ) WITH ( ... ) The following watermark strategies are supported:
DDL -- Sets a watermark strategy for strictly ascending rowtime attributes. Emits a watermark of the -- maximum observed timestamp so far. Rows that have a timestamp bigger to the max timestamp -- are not late. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field ) WITH ( ... ) -- Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum -- observed timestamp so far minus 1. Rows that have a timestamp bigger or equal to the max timestamp -- are not late. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field - INTERVAL \u0026#39;0.001\u0026#39; SECOND ) WITH ( ... ) -- Sets a watermark strategy for rowtime attributes which are out-of-order by a bounded time interval. -- Emits watermarks which are the maximum observed timestamp minus the specified delay, e.g. 2 seconds. CREATE TABLE MyTable ( ts_field TIMESTAMP(3), WATERMARK FOR ts_field AS ts_field - INTERVAL \u0026#39;2\u0026#39; SECOND ) WITH ( ... ) Make sure to always declare both timestamps and watermarks. Watermarks are required for triggering time-based operations.
SQL Types # Please see the Data Types page about how to declare a type in SQL.
Back to top
`}),e.add({id:12,href:"/flink/flink-docs-master/zh/docs/deployment/ha/overview/",title:"概览",section:"高可用",content:` 高可用 # JobManager 高可用（HA）模式加强了 Flink 集群防止 JobManager 故障的能力。 此特性确保 Flink 集群将始终持续执行你提交的作业。
JobManager 高可用 # JobManager 协调每个 Flink 的部署。它同时负责 调度 和 资源管理。
默认情况下，每个 Flink 集群只有一个 JobManager 实例。这会导致 单点故障（SPOF）：如果 JobManager 崩溃，则不能提交任何新程序，运行中的程序也会失败。
使用 JobManager 高可用模式，你可以从 JobManager 失败中恢复，从而消除单点故障。你可以为每个集群部署配置高可用模式。 有关更多信息，请参阅 高可用服务。
如何启用集群高可用 # JobManager 高可用一般概念是指，在任何时候都有 一个领导者 JobManager，如果领导者出现故障，则有多个备用 JobManager 来接管领导。这保证了 不存在单点故障，只要有备用 JobManager 担任领导者，程序就可以继续运行。
如下是一个使用三个 JobManager 实例的例子：
Flink 的 高可用服务 封装了所需的服务，使一切可以正常工作：
领导者选举：从 n 个候选者中选出一个领导者 服务发现：检索当前领导者的地址 状态持久化：继承程序恢复作业所需的持久化状态（JobGraphs、用户代码jar、已完成的检查点） Back to top
高可用服务 # Flink 提供了两种高可用服务实现：
ZooKeeper：每个 Flink 集群部署都可以使用 ZooKeeper HA 服务。它们需要一个运行的 ZooKeeper 复制组（quorum）。
Kubernetes：Kubernetes HA 服务只能运行在 Kubernetes 上。
Back to top
高可用数据生命周期 # 为了恢复提交的作业，Flink 持久化元数据和 job 组件。高可用数据将一直保存，直到相应的作业执行成功、被取消或最终失败。当这些情况发生时，将删除所有高可用数据，包括存储在高可用服务中的元数据。
Back to top
JobResultStore # The JobResultStore is used to archive the final result of a job that reached a globally-terminal state (i.e. finished, cancelled or failed). The data is stored on a file system (see job-result-store.storage-path). Entries in this store are marked as dirty as long as the corresponding job wasn\u0026rsquo;t cleaned up properly (artifacts are found in the job\u0026rsquo;s subfolder in high-availability.storageDir).
Dirty entries are subject to cleanup, i.e. the corresponding job is either cleaned up by Flink at the moment or will be picked up for cleanup as part of a recovery. The entries will be deleted as soon as the cleanup succeeds. Check the JobResultStore configuration parameters under HA configuration options for further details on how to adapt the behavior.
Back to top
`}),e.add({id:13,href:"/flink/flink-docs-master/zh/docs/deployment/overview/",title:"概览",section:"Deployment",content:` Deployment # Flink is a versatile framework, supporting many different deployment scenarios in a mix and match fashion.
Below, we briefly explain the building blocks of a Flink cluster, their purpose and available implementations. If you just want to start Flink locally, we recommend setting up a Standalone Cluster.
Overview and Reference Architecture # The figure below shows the building blocks of every Flink cluster. There is always somewhere a client running. It takes the code of the Flink applications, transforms it into a JobGraph and submits it to the JobManager.
The JobManager distributes the work onto the TaskManagers, where the actual operators (such as sources, transformations and sinks) are running.
When deploying Flink, there are often multiple options available for each building block. We have listed them in the table below the figure.
Component Purpose Implementations Flink Client Compiles batch or streaming applications into a dataflow graph, which it then submits to the JobManager. Command Line Interface REST Endpoint SQL Client Python REPL JobManager JobManager is the name of the central work coordination component of Flink. It has implementations for different resource providers, which differ on high-availability, resource allocation behavior and supported job submission modes. JobManager modes for job submissions: Application Mode: runs the cluster exclusively for one application. The job's main method (or client) gets executed on the JobManager. Calling \`execute\`/\`executeAsync\` multiple times in an application is supported. Per-Job Mode: runs the cluster exclusively for one job. The job's main method (or client) runs only prior to the cluster creation. Session Mode: one JobManager instance manages multiple jobs sharing the same cluster of TaskManagers Standalone (this is the barebone mode that requires just JVMs to be launched. Deployment with Docker, Docker Swarm / Compose, non-native Kubernetes and other models is possible through manual setup in this mode) Kubernetes YARN TaskManager TaskManagers are the services actually performing the work of a Flink job. External Components (all optional) High Availability Service Provider Flink's JobManager can be run in high availability mode which allows Flink to recover from JobManager faults. In order to failover faster, multiple standby JobManagers can be started to act as backups. Zookeeper Kubernetes HA File Storage and Persistency For checkpointing (recovery mechanism for streaming jobs) Flink relies on external file storage systems See FileSystems page. Resource Provider Flink can be deployed through different Resource Provider Frameworks, such as Kubernetes or YARN. See JobManager implementations above. Metrics Storage Flink components report internal metrics and Flink jobs can report additional, job specific metrics as well. See Metrics Reporter page. Application-level data sources and sinks While application-level data sources and sinks are not technically part of the deployment of Flink cluster components, they should be considered when planning a new Flink production deployment. Colocating frequently used data with Flink can have significant performance benefits For example: Apache Kafka Amazon S3 ElasticSearch Apache Cassandra See Connectors page. Repeatable Resource Cleanup # Once a job has reached a globally terminal state of either finished, failed or cancelled, the external component resources associated with the job are then cleaned up. In the event of a failure when cleaning up a resource, Flink will attempt to retry the cleanup. You can configure the retry strategy used. Reaching the maximum number of retries without succeeding will leave the job in a dirty state. Its artifacts would need to be cleaned up manually (see the High Availability Services / JobResultStore section for further details). Restarting the very same job (i.e. using the same job ID) will result in the cleanup being restarted without running the job again.
There is currently an issue with the cleanup of CompletedCheckpoints that failed to be deleted while subsuming them as part of the usual CompletedCheckpoint management. These artifacts are not covered by the repeatable cleanup, i.e. they have to be deleted manually, still. This is covered by FLINK-26606.
Deployment Modes # Flink can execute applications in one of three ways:
in Application Mode, in a Per-Job Mode, in Session Mode. The above modes differ in:
the cluster lifecycle and resource isolation guarantees whether the application\u0026rsquo;s main() method is executed on the client or on the cluster. Application Mode # In all the other modes, the application\u0026rsquo;s main() method is executed on the client side. This process includes downloading the application\u0026rsquo;s dependencies locally, executing the main() to extract a representation of the application that Flink\u0026rsquo;s runtime can understand (i.e. the JobGraph) and ship the dependencies and the JobGraph(s) to the cluster. This makes the Client a heavy resource consumer as it may need substantial network bandwidth to download dependencies and ship binaries to the cluster, and CPU cycles to execute the main(). This problem can be more pronounced when the Client is shared across users.
Building on this observation, the Application Mode creates a cluster per submitted application, but this time, the main() method of the application is executed on the JobManager. Creating a cluster per application can be seen as creating a session cluster shared only among the jobs of a particular application, and torn down when the application finishes. With this architecture, the Application Mode provides the same resource isolation and load balancing guarantees as the Per-Job mode, but at the granularity of a whole application. Executing the main() on the JobManager allows for saving the CPU cycles required, but also save the bandwidth required for downloading the dependencies locally. Furthermore, it allows for more even spread of the network load for downloading the dependencies of the applications in the cluster, as there is one JobManager per application.
In the Application Mode, the main() is executed on the cluster and not on the client, as in the other modes. This may have implications for your code as, for example, any paths you register in your environment using the registerCachedFile() must be accessible by the JobManager of your application. Compared to the Per-Job mode, the Application Mode allows the submission of applications consisting of multiple jobs. The order of job execution is not affected by the deployment mode but by the call used to launch the job. Using execute(), which is blocking, establishes an order and it will lead to the execution of the \u0026ldquo;next\u0026rdquo; job being postponed until \u0026ldquo;this\u0026rdquo; job finishes. Using executeAsync(), which is non-blocking, will lead to the \u0026ldquo;next\u0026rdquo; job starting before \u0026ldquo;this\u0026rdquo; job finishes.
The Application Mode allows for multi-execute() applications but High-Availability is not supported in these cases. High-Availability in Application Mode is only supported for single-execute() applications.
Additionally, when any of multiple running jobs in Application Mode (submitted for example using executeAsync()) gets cancelled, all jobs will be stopped and the JobManager will shut down. Regular job completions (by the sources shutting down) are supported.
Per-Job Mode # Aiming at providing better resource isolation guarantees, the Per-Job mode uses the available resource provider framework (e.g. YARN, Kubernetes) to spin up a cluster for each submitted job. This cluster is available to that job only. When the job finishes, the cluster is torn down and any lingering resources (files, etc) are cleared up. This provides better resource isolation, as a misbehaving job can only bring down its own TaskManagers. In addition, it spreads the load of book-keeping across multiple JobManagers, as there is one per job. For these reasons, the Per-Job resource allocation model is the preferred mode by many production reasons.
Session Mode # Session mode assumes an already running cluster and uses the resources of that cluster to execute any submitted application. Applications executed in the same (session) cluster use, and consequently compete for, the same resources. This has the advantage that you do not pay the resource overhead of spinning up a full cluster for every submitted job. But, if one of the jobs misbehaves or brings down a TaskManager, then all jobs running on that TaskManager will be affected by the failure. This, apart from a negative impact on the job that caused the failure, implies a potential massive recovery process with all the restarting jobs accessing the filesystem concurrently and making it unavailable to other services. Additionally, having a single cluster running multiple jobs implies more load for the JobManager, who is responsible for the book-keeping of all the jobs in the cluster.
Summary # In Session Mode, the cluster lifecycle is independent of that of any job running on the cluster and the resources are shared across all jobs. The Per-Job mode pays the price of spinning up a cluster for every submitted job, but this comes with better isolation guarantees as the resources are not shared across jobs. In this case, the lifecycle of the cluster is bound to that of the job. Finally, the Application Mode creates a session cluster per application and executes the application\u0026rsquo;s main() method on the cluster.
Vendor Solutions # A number of vendors offer managed or fully hosted Flink solutions. None of these vendors are officially supported or endorsed by the Apache Flink PMC. Please refer to vendor maintained documentation on how to use these products.
AliCloud Realtime Compute # Website
Supported Environments: AliCloud
Amazon EMR # Website
Supported Environments: AWS
Amazon Kinesis Data Analytics for Apache Flink # Website
Supported Environments: AWS
Cloudera DataFlow # Website
Supported Environment: AWS Azure Google On-Premise
Eventador # Website
Supported Environment: AWS
Huawei Cloud Stream Service # Website
Supported Environment: Huawei
Ververica Platform # Website
Supported Environments: AliCloud AWS Azure Google On-Premise
Back to top
`}),e.add({id:14,href:"/flink/flink-docs-master/zh/docs/dev/configuration/overview/",title:"概览",section:"项目配置",content:` 项目配置 # 本节将向你展示如何通过流行的构建工具 (Maven、Gradle) 配置你的项目，必要的依赖项（比如连接器和格式），以及覆盖一些高级配置主题。
每个 Flink 应用程序都依赖于一组 Flink 库。应用程序至少依赖于 Flink API，此外还依赖于某些连接器库（比如 Kafka、Cassandra），以及用户开发的自定义的数据处理逻辑所需要的第三方依赖项。
开始 # 要开始使用 Flink 应用程序，请使用以下命令、脚本和模板来创建 Flink 项目。
Maven 你可以使用如下的 Maven 命令或快速启动脚本，基于原型创建一个项目。
Maven 命令 # \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.16-SNAPSHOT 这允许你命名新建的项目，而且会交互式地询问 groupId、artifactId、package 的名字。
快速启动脚本 # \$ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.16-SNAPSHOT Gradle 你可以创建一个空项目，你需要在其中手动创建 src/main/java 和 src/main/resources 目录并开始在其中编写一些类，使用如下 Gradle 构建脚本或下面提供的快速启动脚本以获得功能齐全的启动项目。
Gradle 构建脚本 # 请在脚本的所在目录执行 gradle 命令来执行这些构建配置脚本。
build.gradle
plugins { id \u0026#39;java\u0026#39; id \u0026#39;application\u0026#39; // shadow plugin to produce fat JARs id \u0026#39;com.github.johnrengelman.shadow\u0026#39; version \u0026#39;7.1.2\u0026#39; } // artifact properties group = \u0026#39;org.quickstart\u0026#39; version = \u0026#39;0.1-SNAPSHOT\u0026#39; mainClassName = \u0026#39;org.quickstart.DataStreamJob\u0026#39; description = \u0026#34;\u0026#34;\u0026#34;Flink Quickstart Job\u0026#34;\u0026#34;\u0026#34; ext { javaVersion = \u0026#39;1.8\u0026#39; flinkVersion = \u0026#39;1.16-SNAPSHOT\u0026#39; scalaBinaryVersion = \u0026#39;_2.12\u0026#39; slf4jVersion = \u0026#39;1.7.32\u0026#39; log4jVersion = \u0026#39;2.17.1\u0026#39; } sourceCompatibility = javaVersion targetCompatibility = javaVersion tasks.withType(JavaCompile) { options.encoding = \u0026#39;UTF-8\u0026#39; } applicationDefaultJvmArgs = [\u0026#34;-Dlog4j.configurationFile=log4j2.properties\u0026#34;] // declare where to find the dependencies of your project repositories { mavenCentral() maven { url \u0026#34;https://repository.apache.org/content/repositories/snapshots\u0026#34; mavenContent { snapshotsOnly() } } } // NOTE: We cannot use \u0026#34;compileOnly\u0026#34; or \u0026#34;shadow\u0026#34; configurations since then we could not run code // in the IDE or with \u0026#34;gradle run\u0026#34;. We also cannot exclude transitive dependencies from the // shadowJar yet (see https://github.com/johnrengelman/shadow/issues/159). // -\u0026gt; Explicitly define the // libraries we want to be included in the \u0026#34;flinkShadowJar\u0026#34; configuration! configurations { flinkShadowJar // dependencies which go into the shadowJar // always exclude these (also from transitive dependencies) since they are provided by Flink flinkShadowJar.exclude group: \u0026#39;org.apache.flink\u0026#39;, module: \u0026#39;force-shading\u0026#39; flinkShadowJar.exclude group: \u0026#39;com.google.code.findbugs\u0026#39;, module: \u0026#39;jsr305\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.slf4j\u0026#39; flinkShadowJar.exclude group: \u0026#39;org.apache.logging.log4j\u0026#39; } // declare the dependencies for your production and test code dependencies { // -------------------------------------------------------------- // Compile-time dependencies that should NOT be part of the // shadow (uber) jar and are provided in the lib folder of Flink // -------------------------------------------------------------- implementation \u0026#34;org.apache.flink:flink-streaming-java:\${flinkVersion}\u0026#34; implementation \u0026#34;org.apache.flink:flink-clients:\${flinkVersion}\u0026#34; // -------------------------------------------------------------- // Dependencies that should be part of the shadow jar, e.g. // connectors. These must be in the flinkShadowJar configuration! // -------------------------------------------------------------- //flinkShadowJar \u0026#34;org.apache.flink:flink-connector-kafka:\${flinkVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-slf4j-impl:\${log4jVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-api:\${log4jVersion}\u0026#34; runtimeOnly \u0026#34;org.apache.logging.log4j:log4j-core:\${log4jVersion}\u0026#34; // Add test dependencies here. // testCompile \u0026#34;junit:junit:4.12\u0026#34; } // make compileOnly dependencies available for tests: sourceSets { main.compileClasspath += configurations.flinkShadowJar main.runtimeClasspath += configurations.flinkShadowJar test.compileClasspath += configurations.flinkShadowJar test.runtimeClasspath += configurations.flinkShadowJar javadoc.classpath += configurations.flinkShadowJar } run.classpath = sourceSets.main.runtimeClasspath jar { manifest { attributes \u0026#39;Built-By\u0026#39;: System.getProperty(\u0026#39;user.name\u0026#39;), \u0026#39;Build-Jdk\u0026#39;: System.getProperty(\u0026#39;java.version\u0026#39;) } } shadowJar { configurations = [project.configurations.flinkShadowJar] } settings.gradle
rootProject.name = \u0026#39;quickstart\u0026#39; 快速启动脚本 # bash -c \u0026#34;\$(curl https://flink.apache.org/q/gradle-quickstart.sh)\u0026#34; -- 1.16-SNAPSHOT _2.12 需要哪些依赖项？ # 要开始一个 Flink 作业，你通常需要如下依赖项：
Flink API，用来开发你的作业 连接器和格式，以将你的作业与外部系统集成 测试实用程序，以测试你的作业 除此之外，若要开发自定义功能，你还要添加必要的第三方依赖项。
Flink API # Flink提供了两大 API：Datastream API 和 Table API \u0026amp; SQL，它们可以单独使用，也可以混合使用，具体取决于你的使用场景：
你要使用的 API 你需要添加的依赖项 DataStream flink-streaming-java DataStream Scala 版 flink-streaming-scala_2.12 Table API flink-table-api-java Table API Scala 版 flink-table-api-scala_2.12 Table API + DataStream flink-table-api-java-bridge Table API + DataStream Scala 版 flink-table-api-scala-bridge_2.12 你只需将它们包含在你的构建工具脚本/描述符中，就可以开发你的作业了！
运行和打包 # 如果你想通过简单地执行主类来运行你的作业，你需要 classpath 里有 flink-runtime。对于 Table API 程序，你还需要 flink-table-runtime 和 flink-table-planner-loader。
根据经验，我们建议将应用程序代码及其所有必需的依赖项打包进一个 fat/uber JAR 中。这包括打包你作业用到的连接器、格式和第三方依赖项。此规则不适用于 Java API、DataStream Scala API 以及前面提到的运行时模块，它们已经由 Flink 本身提供，不应包含在作业的 uber JAR 中。你可以把该作业 JAR 提交到已经运行的 Flink 集群，也可以轻松将其添加到 Flink 应用程序容器镜像中，而无需修改发行版。
下一步是什么？ # 要开发你的作业，请查阅 DataStream API 和 Table API \u0026amp; SQL； 关于如何使用特定的构建工具打包你的作业的更多细节，请查阅如下指南： Maven Gradle 关于项目配置的高级内容，请查阅高级主题部分。 `}),e.add({id:15,href:"/flink/flink-docs-master/zh/docs/dev/dataset/overview/",title:"概览",section:"DataSet API (Legacy)",content:` DataSet API 编程指南 # DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs. The execution can happen in a local JVM, or on clusters of many machines.
Please refer to the DataStream API overview for an introduction to the basic concepts of the Flink API. That overview is for the DataStream API but the basic concepts of the two APIs are the same.
In order to create your own Flink DataSet program, we encourage you to start with the anatomy of a Flink Program and gradually add your own transformations. The remaining sections act as references for additional operations and advanced features.
Starting with Flink 1.12 the DataSet API has been soft deprecated.
We recommend that you use the Table API and SQL to run efficient batch pipelines in a fully unified API. Table API is well integrated with common batch connectors and catalogs.
Alternatively, you can also use the DataStream API with BATCH execution mode. The linked section also outlines cases where it makes sense to use the DataSet API but those cases will become rarer as development progresses and the DataSet API will eventually be removed. Please also see FLIP-131 for background information on this decision.
Example Program # The following program is a complete, working example of WordCount. You can copy \u0026amp; paste the code to run it locally. You only have to include the correct Flink’s library into your project and specify the imports. Then you are ready to go!
Java public class WordCountExample { public static void main(String[] args) throws Exception { final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;String\u0026gt; text = env.fromElements( \u0026#34;Who\u0026#39;s there?\u0026#34;, \u0026#34;I think I hear them. Stand, ho! Who\u0026#39;s there?\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = text .flatMap(new LineSplitter()) .groupBy(0) .sum(1); wordCounts.print(); } public static class LineSplitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String line, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { for (String word : line.split(\u0026#34; \u0026#34;)) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(word, 1)); } } } } Scala import org.apache.flink.api.scala._ object WordCount { def main(args: Array[String]) { val env = ExecutionEnvironment.getExecutionEnvironment val text = env.fromElements( \u0026#34;Who\u0026#39;s there?\u0026#34;, \u0026#34;I think I hear them. Stand, ho! Who\u0026#39;s there?\u0026#34;) val counts = text .flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.print() } } DataSet Transformations # Data transformations transform one or more DataSets into a new DataSet. Programs can combine multiple transformations into sophisticated assemblies.
Map # Takes one element and produces one element.
Java data.map(new MapFunction\u0026lt;String, Integer\u0026gt;() { public Integer map(String value) { return Integer.parseInt(value); } }); Scala data.map { x =\u0026gt; x.toInt } FlatMap # Takes one element and produces zero, one, or more elements.
Java data.flatMap(new FlatMapFunction\u0026lt;String, String\u0026gt;() { public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) { for (String s : value.split(\u0026#34; \u0026#34;)) { out.collect(s); } } }); Scala data.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) } MapPartition # Transforms a parallel partition in a single function call. The function gets the partition as an Iterable stream and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.
Java data.mapPartition(new MapPartitionFunction\u0026lt;String, Long\u0026gt;() { public void mapPartition(Iterable\u0026lt;String\u0026gt; values, Collector\u0026lt;Long\u0026gt; out) { long c = 0; for (String s : values) { c++; } out.collect(c); } }); Scala data.mapPartition { in =\u0026gt; in map { (_, 1) } } Filter # Evaluates a boolean function for each element and retains those for which the function returns true. IMPORTANT: The system assumes that the function does not modify the element on which the predicate is applied. Violating this assumption can lead to incorrect results.
Java data.filter(new FilterFunction\u0026lt;Integer\u0026gt;() { public boolean filter(Integer value) { return value \u0026gt; 1000; } }); Scala data.filter { _ \u0026gt; 1000 } Reduce # Combines a group of elements into a single element by repeatedly combining two elements into one. Reduce may be applied on a full data set or on a grouped data set.
Java data.reduce(new ReduceFunction\u0026lt;Integer\u0026gt; { public Integer reduce(Integer a, Integer b) { return a + b; } }); Scala data.reduce { _ + _ } If the reduce was applied to a grouped data set then you can specify the way that the runtime executes the combine phase of the reduce by supplying a CombineHint to setCombineHint. The hash-based strategy should be faster in most cases, especially if the number of different keys is small compared to the number of input elements (eg. 1/10).
ReduceGroup # Combines a group of elements into one or more elements. ReduceGroup may be applied on a full data set, or on a grouped data set.
Java data.reduceGroup(new GroupReduceFunction\u0026lt;Integer, Integer\u0026gt; { public void reduce(Iterable\u0026lt;Integer\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) { int prefixSum = 0; for (Integer i : values) { prefixSum += i; out.collect(prefixSum); } } }); Scala data.reduceGroup { elements =\u0026gt; elements.sum } Aggregate # Aggregates a group of values into a single value. Aggregation functions can be thought of as built-in reduce functions. Aggregate may be applied on a full data set, or on a grouped data set.
Java Dataset\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input.aggregate(SUM, 0).and(MIN, 2); Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input.aggregate(SUM, 0).aggregate(MIN, 2) Distinct # Returns the distinct elements of a data set. It removes the duplicate entries from the input DataSet, with respect to all fields of the elements, or a subset of fields.
Java data.distinct(); Scala data.distinct() Join # Joins two data sets by creating all pairs of elements that are equal on their keys. Optionally uses a JoinFunction to turn the pair of elements into a single element, or a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.
Java result = input1.join(input2) .where(0) // key of the first input (tuple field 0) .equalTo(1); // key of the second input (tuple field 1) Scala // In this case tuple fields are used as keys. \u0026#34;0\u0026#34; is the join field on the first tuple // \u0026#34;1\u0026#34; is the join field on the second tuple. val result = input1.join(input2).where(0).equalTo(1) You can specify the way that the runtime executes the join via Join Hints. The hints describe whether the join happens through partitioning or broadcasting, and whether it uses a sort-based or a hash-based algorithm. Please refer to the Transformations Guide for a list of possible hints and an example. If no hint is specified, the system will try to make an estimate of the input sizes and pick the best strategy according to those estimates.
Java // This executes a join by broadcasting the first data set // using a hash table for the broadcast data result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1); Scala // This executes a join by broadcasting the first data set // using a hash table for the broadcast data val result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1) Note that the join transformation works only for equi-joins. Other join types need to be expressed using OuterJoin or CoGroup.
OuterJoin # Performs a left, right, or full outer join on two data sets. Outer joins are similar to regular (inner) joins and create all pairs of elements that are equal on their keys. In addition, records of the \u0026ldquo;outer\u0026rdquo; side (left, right, or both in case of full) are preserved if no matching key is found in the other side. Matching pairs of elements (or one element and a null value for the other input) are given to a JoinFunction to turn the pair of elements into a single element, or to a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements. See the keys section to learn how to define join keys.
Java input1.leftOuterJoin(input2) // rightOuterJoin or fullOuterJoin for right or full outer joins .where(0) // key of the first input (tuple field 0) .equalTo(1) // key of the second input (tuple field 1) .with(new JoinFunction\u0026lt;String, String, String\u0026gt;() { public String join(String v1, String v2) { // NOTE: // - v2 might be null for leftOuterJoin // - v1 might be null for rightOuterJoin // - v1 OR v2 might be null for fullOuterJoin } }); Scala val joined = left.leftOuterJoin(right).where(0).equalTo(1) { (left, right) =\u0026gt; val a = if (left == null) \u0026#34;none\u0026#34; else left._1 (a, right) } CoGroup # The two-dimensional variant of the reduce operation. Groups each input on one or more fields and then joins the groups. The transformation function is called per pair of groups. See the keys section to learn how to define coGroup keys.
Java data1.coGroup(data2) .where(0) .equalTo(1) .with(new CoGroupFunction\u0026lt;String, String, String\u0026gt;() { public void coGroup(Iterable\u0026lt;String\u0026gt; in1, Iterable\u0026lt;String\u0026gt; in2, Collector\u0026lt;String\u0026gt; out) { out.collect(...); } }); Scala data1.coGroup(data2).where(0).equalTo(1) Cross # Builds the Cartesian product (cross product) of two inputs, creating all pairs of elements. Optionally uses a CrossFunction to turn the pair of elements into a single element
Java DataSet\u0026lt;Integer\u0026gt; data1 = // [...] DataSet\u0026lt;String\u0026gt; data2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; result = data1.cross(data2); Scala val data1: DataSet[Int] = // [...] val data2: DataSet[String] = // [...] val result: DataSet[(Int, String)] = data1.cross(data2) Cross is potentially a very compute-intensive operation which can challenge even large compute clusters! It is advised to hint the system with the DataSet sizes by using crossWithTiny() and crossWithHuge(). Union # Produces the union of two data sets.
Java data.union(data2); Scala data.union(data2) Rebalance # Evenly rebalances the parallel partitions of a data set to eliminate data skew. Only Map-like transformations may follow a rebalance transformation.
Java DataSet\u0026lt;Int\u0026gt; data1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Int, String\u0026gt;\u0026gt; result = data1.rebalance().map(...); Scala val data1: DataSet[Int] = // [...] val result: DataSet[(Int, String)] = data1.rebalance().map(...) Hash-Partition # Hash-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.partitionByHash(0).mapPartition { ... } Range-Partition # Range-partitions a data set on a given key. Keys can be specified as position keys, expression keys, and key selector functions.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.partitionByRange(0).mapPartition { ... } Custom Partitioning # Assigns records based on a key to a specific partition using a custom Partitioner function. The key can be specified as position key, expression key, and key selector function. Note: This method only works with a single field key.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.partitionCustom(partitioner, key) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in .partitionCustom(partitioner, key).mapPartition { ... } Sort Partitioning # Locally sorts all partitions of a data set on a specified field in a specified order. Fields can be specified as tuple positions or field expressions. Sorting on multiple fields is done by chaining sortPartition() calls.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Integer\u0026gt; result = in.sortPartition(1, Order.ASCENDING) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(Int, String)] = // [...] val result = in.sortPartition(1, Order.ASCENDING).mapPartition { ... } First-N # Returns the first n (arbitrary) elements of a data set. First-n can be applied on a regular data set, a grouped data set, or a grouped-sorted data set. Grouping keys can be specified as key-selector functions or field position keys.
Java DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; in = // [...] // regular data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result1 = in.first(3); // grouped data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result2 = in.groupBy(0) .first(3); // grouped-sorted data set DataSet\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt; result3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); Scala val in: DataSet[(Int, String)] = // [...] // regular data set val result1 = in.first(3) // grouped data set val result2 = in.groupBy(0).first(3) // grouped-sorted data set val result3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) Project # Selects a subset of fields from tuples.
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); Scala This feature is not available in the Scala API MinBy / MaxBy # Selects a tuple from a group of tuples whose values of one or more fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) field values, an arbitrary tuple of these tuples is returned. MinBy (MaxBy) may be applied on a full data set or a grouped data set.
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] // a DataSet with a single tuple with minimum values for the Integer and String fields. DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; out = in.minBy(0, 2); // a DataSet with one tuple for each group with the minimum value for the Double field. DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; out2 = in.groupBy(2) .minBy(1); Scala val in: DataSet[(Int, Double, String)] = // [...] // a data set with a single tuple with minimum values for the Int and String fields. val out: DataSet[(Int, Double, String)] = in.minBy(0, 2) // a data set with one tuple for each group with the minimum value for the Double field. val out2: DataSet[(Int, Double, String)] = in.groupBy(2) .minBy(1) Specifying Keys # Some transformations (join, coGroup, groupBy) require that a key be defined on a collection of elements. Other transformations (Reduce, GroupReduce, Aggregate) allow data being grouped on a key before they are applied.
A DataSet is grouped as
DataSet\u0026lt;...\u0026gt; input = // [...] DataSet\u0026lt;...\u0026gt; reduced = input .groupBy(/*define key here*/) .reduceGroup(/*do something*/); The data model of Flink is not based on key-value pairs. Therefore, you do not need to physically pack the data set types into keys and values. Keys are “virtual”: they are defined as functions over the actual data to guide the grouping operator.
Define keys for Tuples # The simplest case is grouping Tuples on one or more fields of the Tuple:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;\u0026gt; input = // [...] UnsortedGrouping\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;,Tuple\u0026gt; keyed = input.groupBy(0); Scala val input: DataSet[(Int, String, Long)] = // [...] val keyed = input.groupBy(0) Tuples are grouped on the first field (the one of Integer type).
Java DataSet\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;\u0026gt; input = // [...] UnsortedGrouping\u0026lt;Tuple3\u0026lt;Integer,String,Long\u0026gt;,Tuple\u0026gt; keyed = input.groupBy(0,1); Scala val input: DataSet[(Int, String, Long)] = // [...] val grouped = input.groupBy(0,1) Here, we group the tuples on a composite key consisting of the first and the second field.
A note on nested Tuples: If you have a DataSet with a nested tuple, such as:
DataSet\u0026lt;Tuple3\u0026lt;Tuple2\u0026lt;Integer, Float\u0026gt;,String,Long\u0026gt;\u0026gt; ds; Specifying groupBy(0) will cause the system to use the full Tuple2 as a key (with the Integer and Float being the key). If you want to “navigate” into the nested Tuple2, you have to use field expression keys which are explained below.
Define keys using Field Expressions # You can use String-based field expressions to reference nested fields and define keys for grouping, sorting, joining, or coGrouping. Field expressions make it very easy to select fields in (nested) composite types such as Tuple and POJO types.
In the example below, we have a WC POJO with two fields “word” and “count”. To group by the field word, we just pass its name to the groupBy() function.
Java // some ordinary POJO (Plain old Java Object) public class WC { public String word; public int count; } DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words.groupBy(\u0026#34;word\u0026#34;); Scala // some ordinary POJO (Plain old Java Object) class WC(var word: String, var count: Int) { def this() { this(\u0026#34;\u0026#34;, 0L) } } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) // or, as a case class, which is less typing case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;) Field Expression Syntax: # Select POJO fields by their field name. For example \u0026ldquo;user\u0026rdquo; refers to the “user” field of a POJO type.
Select Tuple fields by their 1-offset field name or 0-offset field index. For example \u0026ldquo;_1\u0026rdquo; and \u0026ldquo;5\u0026rdquo; refer to the first and sixth field of a Scala Tuple type, respectively.
You can select nested fields in POJOs and Tuples. For example \u0026ldquo;user.zip\u0026rdquo; refers to the “zip” field of a POJO which is stored in the “user” field of a POJO type. Arbitrary nesting and mixing of POJOs and Tuples is supported such as \u0026ldquo;_2.user.zip\u0026rdquo; or \u0026ldquo;user._4.1.zip\u0026rdquo;.
You can select the full type using the \u0026ldquo;_\u0026rdquo; wildcard expressions. This does also work for types which are not Tuple or POJO types.
Field Expression Example: # Java public static class WC { public ComplexNestedClass complex; //nested POJO private int count; // getter / setter for private field (count) public int getCount() { return count; } public void setCount(int c) { this.count = c; } } public static class ComplexNestedClass { public Integer someNumber; public float someFloat; public Tuple3\u0026lt;Long, Long, String\u0026gt; word; public IntWritable hadoopCitizen; } Scala class WC(var complex: ComplexNestedClass, var count: Int) { def this() { this(null, 0) } } class ComplexNestedClass( var someNumber: Int, someFloat: Float, word: (Long, Long, String), hadoopCitizen: IntWritable) { def this() { this(0, 0, (0, 0, \u0026#34;\u0026#34;), new IntWritable(0)) } } These are valid field expressions for the example code above:
\u0026ldquo;count\u0026rdquo;: The count field in the WC class.
\u0026ldquo;complex\u0026rdquo;: Recursively selects all fields of the field complex of POJO type ComplexNestedClass.
\u0026ldquo;complex.word.f2\u0026rdquo;: Selects the last field of the nested Tuple3.
\u0026ldquo;complex.hadoopCitizen\u0026rdquo;: Selects the Hadoop IntWritable type.
Define keys using Key Selector Functions # An additional way to define keys are “key selector” functions. A key selector function takes a single element as input and returns the key for the element. The key can be of any type and be derived from deterministic computations.
The following example shows a key selector function that simply returns the field of an object:
Java // some ordinary POJO public class WC {public String word; public int count;} DataSet\u0026lt;WC\u0026gt; words = // [...] UnsortedGrouping\u0026lt;WC\u0026gt; keyed = words .groupBy(new KeySelector\u0026lt;WC, String\u0026gt;() { public String getKey(WC wc) { return wc.word; } }); Scala // some ordinary case class case class WC(word: String, count: Int) val words: DataSet[WC] = // [...] val keyed = words.groupBy( _.word ) Data Sources # Data sources create the initial data sets, such as from files or from Java collections. The general mechanism of creating data sets is abstracted behind an InputFormat. Flink comes with several built-in formats to create data sets from common file formats. Many of them have shortcut methods on the ExecutionEnvironment.
File-based:
readTextFile(path) / TextInputFormat - Reads files line wise and returns them as Strings.
readTextFileWithValue(path) / TextValueInputFormat - Reads files line wise and returns them as StringValues. StringValues are mutable strings.
readCsvFile(path) / CsvInputFormat - Parses files of comma (or another char) delimited fields. Returns a DataSet of tuples or POJOs. Supports the basic java types and their Value counterparts as field types.
readFileOfPrimitives(path, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer.
readFileOfPrimitives(path, delimiter, Class) / PrimitiveInputFormat - Parses files of new-line (or another char sequence) delimited primitive data types such as String or Integer using the given delimiter.
Collection-based:
fromCollection(Collection) - Creates a data set from a Java.util.Collection. All elements in the collection must be of the same type.
fromCollection(Iterator, Class) - Creates a data set from an iterator. The class specifies the data type of the elements returned by the iterator.
fromElements(T \u0026hellip;) - Creates a data set from the given sequence of objects. All objects must be of the same type.
fromParallelCollection(SplittableIterator, Class) - Creates a data set from an iterator, in parallel. The class specifies the data type of the elements returned by the iterator.
generateSequence(from, to) - Generates the sequence of numbers in the given interval, in parallel.
Generic:
readFile(inputFormat, path) / FileInputFormat - Accepts a file input format.
createInput(inputFormat) / InputFormat - Accepts a generic input format.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // read text file from local files system DataSet\u0026lt;String\u0026gt; localLines = env.readTextFile(\u0026#34;file:///path/to/my/textfile\u0026#34;); // read text file from an HDFS running at nnHost:nnPort DataSet\u0026lt;String\u0026gt; hdfsLines = env.readTextFile(\u0026#34;hdfs://nnHost:nnPort/path/to/my/textfile\u0026#34;); // read a CSV file with three fields DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .types(Integer.class, String.class, Double.class); // read a CSV file with five fields, taking only two of them DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .includeFields(\u0026#34;10010\u0026#34;) // take the first and the fourth field .types(String.class, Double.class); // read a CSV file with three fields into a POJO (Person.class) with corresponding fields DataSet\u0026lt;Person\u0026gt;\u0026gt; csvInput = env.readCsvFile(\u0026#34;hdfs:///the/CSV/file\u0026#34;) .pojoType(Person.class, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;zipcode\u0026#34;); // read a file from the specified path of type SequenceFileInputFormat DataSet\u0026lt;Tuple2\u0026lt;IntWritable, Text\u0026gt;\u0026gt; tuples = env.createInput(HadoopInputs.readSequenceFile(IntWritable.class, Text.class, \u0026#34;hdfs://nnHost:nnPort/path/to/file\u0026#34;)); // creates a set from some given elements DataSet\u0026lt;String\u0026gt; value = env.fromElements(\u0026#34;Foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;foobar\u0026#34;, \u0026#34;fubar\u0026#34;); // generate a number sequence DataSet\u0026lt;Long\u0026gt; numbers = env.generateSequence(1, 10000000); // Read data from a relational database using the JDBC input format DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt; dbData = env.createInput( JdbcInputFormat.buildJdbcInputFormat() .setDrivername(\u0026#34;org.apache.derby.jdbc.EmbeddedDriver\u0026#34;) .setDBUrl(\u0026#34;jdbc:derby:memory:persons\u0026#34;) .setQuery(\u0026#34;select name, age from persons\u0026#34;) .setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO)) .finish() ); // Note: Flink\u0026#39;s program compiler needs to infer the data types of the data items which are returned // by an InputFormat. If this information cannot be automatically inferred, it is necessary to // manually provide the type information as shown in the examples above. Scala val env = ExecutionEnvironment.getExecutionEnvironment // read text file from local files system val localLines = env.readTextFile(\u0026#34;file:///path/to/my/textfile\u0026#34;) // read text file from an HDFS running at nnHost:nnPort val hdfsLines = env.readTextFile(\u0026#34;hdfs://nnHost:nnPort/path/to/my/textfile\u0026#34;) // read a CSV file with three fields val csvInput = env.readCsvFile[(Int, String, Double)](\u0026#34;hdfs:///the/CSV/file\u0026#34;) // read a CSV file with five fields, taking only two of them val csvInput = env.readCsvFile[(String, Double)]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field // CSV input can also be used with Case Classes case class MyCaseClass(str: String, dbl: Double) val csvInput = env.readCsvFile[MyCaseClass]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, includedFields = Array(0, 3)) // take the first and the fourth field // read a CSV file with three fields into a POJO (Person) with corresponding fields val csvInput = env.readCsvFile[Person]( \u0026#34;hdfs:///the/CSV/file\u0026#34;, pojoFields = Array(\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;zipcode\u0026#34;)) // create a set from some given elements val values = env.fromElements(\u0026#34;Foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;foobar\u0026#34;, \u0026#34;fubar\u0026#34;) // generate a number sequence val numbers = env.generateSequence(1, 10000000) // read a file from the specified path of type SequenceFileInputFormat val tuples = env.createInput(HadoopInputs.readSequenceFile(classOf[IntWritable], classOf[Text], \u0026#34;hdfs://nnHost:nnPort/path/to/file\u0026#34;)) Configuring CSV Parsing # Flink offers a number of configuration options for CSV parsing:
types(Class \u0026hellip; types) specifies the types of the fields to parse. It is mandatory to configure the types of the parsed fields. In case of the type class Boolean.class, “True” (case-insensitive), “False” (case-insensitive), “1” and “0” are treated as booleans.
lineDelimiter(String del) specifies the delimiter of individual records. The default line delimiter is the new-line character \u0026lsquo;\\n\u0026rsquo;.
fieldDelimiter(String del) specifies the delimiter that separates fields of a record. The default field delimiter is the comma character \u0026lsquo;,\u0026rsquo;.
includeFields(boolean \u0026hellip; flag), includeFields(String mask), or includeFields(long bitMask) defines which fields to read from the input file (and which to ignore). By default the first n fields (as defined by the number of types in the types() call) are parsed.
parseQuotedStrings(char quoteChar) enables quoted string parsing. Strings are parsed as quoted strings if the first character of the string field is the quote character (leading or tailing whitespaces are not trimmed). Field delimiters within quoted strings are ignored. Quoted string parsing fails if the last character of a quoted string field is not the quote character or if the quote character appears at some point which is not the start or the end of the quoted string field (unless the quote character is escaped using ‘\u0026rsquo;). If quoted string parsing is enabled and the first character of the field is not the quoting string, the string is parsed as unquoted string. By default, quoted string parsing is disabled.
ignoreComments(String commentPrefix) specifies a comment prefix. All lines that start with the specified comment prefix are not parsed and ignored. By default, no lines are ignored.
ignoreInvalidLines() enables lenient parsing, i.e., lines that cannot be correctly parsed are ignored. By default, lenient parsing is disabled and invalid lines raise an exception.
ignoreFirstLine() configures the InputFormat to ignore the first line of the input file. By default no line is ignored.
Recursive Traversal of the Input Path Directory # For file-based inputs, when the input path is a directory, nested files are not enumerated by default. Instead, only the files inside the base directory are read, while nested files are ignored. Recursive enumeration of nested files can be enabled through the recursive.file.enumeration configuration parameter, like in the following example.
Java // enable recursive enumeration of nested input files ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a configuration object Configuration parameters = new Configuration(); // set the recursive enumeration parameter parameters.setBoolean(\u0026#34;recursive.file.enumeration\u0026#34;, true); // pass the configuration to the data source DataSet\u0026lt;String\u0026gt; logs = env.readTextFile(\u0026#34;file:///path/with.nested/files\u0026#34;) .withParameters(parameters); Scala // enable recursive enumeration of nested input files val env = ExecutionEnvironment.getExecutionEnvironment // create a configuration object val parameters = new Configuration // set the recursive enumeration parameter parameters.setBoolean(\u0026#34;recursive.file.enumeration\u0026#34;, true) // pass the configuration to the data source env.readTextFile(\u0026#34;file:///path/with.nested/files\u0026#34;).withParameters(parameters) Read Compressed Files # Flink currently supports transparent decompression of input files if these are marked with an appropriate file extension. In particular, this means that no further configuration of the input formats is necessary and any FileInputFormat support the compression, including custom input formats. Please notice that compressed files might not be read in parallel, thus impacting job scalability.
The following table lists the currently supported compression methods.
Compressed Method File Extensions Parallelizable DEFLATE .deflate no GZip .gz, .gzip no Bzip2 .bz2 no XZ .xz no ZStandart .zst no Data Sinks # Data sinks consume DataSets and are used to store or return them. Data sink operations are described using an OutputFormat. Flink comes with a variety of built-in output formats that are encapsulated behind operations on the DataSet:
writeAsText() / TextOutputFormat - Writes elements line-wise as Strings. The Strings are obtained by calling the toString() method of each element. writeAsFormattedText() / TextOutputFormat - Write elements line-wise as Strings. The Strings are obtained by calling a user-defined format() method for each element. writeAsCsv(\u0026hellip;) / CsvOutputFormat - Writes tuples as comma-separated value files. Row and field delimiters are configurable. The value for each field comes from the toString() method of the objects. print() / printToErr() / print(String msg) / printToErr(String msg) - Prints the toString() value of each element on the standard out / standard error stream. Optionally, a prefix (msg) can be provided which is prepended to the output. This can help to distinguish between different calls to print. If the parallelism is greater than 1, the output will also be prepended with the identifier of the task which produced the output. write() / FileOutputFormat - Method and base class for custom file outputs. Supports custom object-to-bytes conversion. output()/ OutputFormat - Most generic output method, for data sinks that are not file based (such as storing the result in a database). A DataSet can be input to multiple operations. Programs can write or print a data set and at the same time run additional transformations on them.
Java // text data DataSet\u0026lt;String\u0026gt; textData = // [...] // write DataSet to a file on the local file system textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;); // write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort textData.writeAsText(\u0026#34;hdfs://nnHost:nnPort/my/result/on/localFS\u0026#34;); // write DataSet to a file and overwrite the file if it exists textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;, WriteMode.OVERWRITE); // tuples as lines with pipe as the separator \u0026#34;a|b|c\u0026#34; DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; values = // [...] values.writeAsCsv(\u0026#34;file:///path/to/the/result/file\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;|\u0026#34;); // this writes tuples in the text formatting \u0026#34;(a, b, c)\u0026#34;, rather than as CSV lines values.writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;); // this writes values as strings using a user-defined TextFormatter object values.writeAsFormattedText(\u0026#34;file:///path/to/the/result/file\u0026#34;, new TextFormatter\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt;() { public String format (Tuple2\u0026lt;Integer, Integer\u0026gt; value) { return value.f1 + \u0026#34; - \u0026#34; + value.f0; } }); Scala // text data val textData: DataSet[String] = // [...] // write DataSet to a file on the local file system textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;) // write DataSet to a file on an HDFS with a namenode running at nnHost:nnPort textData.writeAsText(\u0026#34;hdfs://nnHost:nnPort/my/result/on/localFS\u0026#34;) // write DataSet to a file and overwrite the file if it exists textData.writeAsText(\u0026#34;file:///my/result/on/localFS\u0026#34;, WriteMode.OVERWRITE) // tuples as lines with pipe as the separator \u0026#34;a|b|c\u0026#34; val values: DataSet[(String, Int, Double)] = // [...] values.writeAsCsv(\u0026#34;file:///path/to/the/result/file\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;|\u0026#34;) // this writes tuples in the text formatting \u0026#34;(a, b, c)\u0026#34;, rather than as CSV lines values.writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) // this writes values as strings using a user-defined formatting values map { tuple =\u0026gt; tuple._1 + \u0026#34; - \u0026#34; + tuple._2 } .writeAsText(\u0026#34;file:///path/to/the/result/file\u0026#34;) Or with a custom output format:
DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; myResult = [...] // write Tuple DataSet to a relational database myResult.output( // build and configure OutputFormat JdbcOutputFormat.buildJdbcOutputFormat() .setDrivername(\u0026#34;org.apache.derby.jdbc.EmbeddedDriver\u0026#34;) .setDBUrl(\u0026#34;jdbc:derby:memory:persons\u0026#34;) .setQuery(\u0026#34;insert into persons (name, age, height) values (?,?,?)\u0026#34;) .finish() ); Locally Sorted Output # The output of a data sink can be locally sorted on specified fields in specified orders using tuple field positions or field expressions. This works for every output format.
The following examples show how to use this feature:
DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; tData = // [...] DataSet\u0026lt;Tuple2\u0026lt;BookPojo, Double\u0026gt;\u0026gt; pData = // [...] DataSet\u0026lt;String\u0026gt; sData = // [...] // sort output on String field in ascending order tData.sortPartition(1, Order.ASCENDING).print(); // sort output on Double field in descending and Integer field in ascending order tData.sortPartition(2, Order.DESCENDING).sortPartition(0, Order.ASCENDING).print(); // sort output on the \u0026#34;author\u0026#34; field of nested BookPojo in descending order pData.sortPartition(\u0026#34;f0.author\u0026#34;, Order.DESCENDING).writeAsText(...); // sort output on the full tuple in ascending order tData.sortPartition(\u0026#34;*\u0026#34;, Order.ASCENDING).writeAsCsv(...); // sort atomic type (String) output in descending order sData.sortPartition(\u0026#34;*\u0026#34;, Order.DESCENDING).writeAsText(...); Globally sorted output is not supported.
Iteration Operators # Iterations implement loops in Flink programs. The iteration operators encapsulate a part of the program and execute it repeatedly, feeding back the result of one iteration (the partial solution) into the next iteration. There are two types of iterations in Flink: BulkIteration and DeltaIteration.
This section provides quick examples on how to use both operators. Check out the Introduction to Iterations page for a more detailed introduction.
Java Bulk Iterations # To create a BulkIteration call the iterate(int) method of the DataSet the iteration should start at. This will return an IterativeDataSet, which can be transformed with the regular operators. The single argument to the iterate call specifies the maximum number of iterations.
To specify the end of an iteration call the closeWith(DataSet) method on the IterativeDataSet to specify which transformation should be fed back to the next iteration. You can optionally specify a termination criterion with closeWith(DataSet, DataSet), which evaluates the second DataSet and terminates the iteration, if this DataSet is empty. If no termination criterion is specified, the iteration terminates after the given maximum number iterations.
The following example iteratively estimates the number Pi. The goal is to count the number of random points, which fall into the unit circle. In each iteration, a random point is picked. If this point lies inside the unit circle, we increment the count. Pi is then estimated as the resulting count divided by the number of iterations multiplied by 4.
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Create initial IterativeDataSet IterativeDataSet\u0026lt;Integer\u0026gt; initial = env.fromElements(0).iterate(10000); DataSet\u0026lt;Integer\u0026gt; iteration = initial.map(new MapFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public Integer map(Integer i) throws Exception { double x = Math.random(); double y = Math.random(); return i + ((x * x + y * y \u0026lt; 1) ? 1 : 0); } }); // Iteratively transform the IterativeDataSet DataSet\u0026lt;Integer\u0026gt; count = initial.closeWith(iteration); count.map(new MapFunction\u0026lt;Integer, Double\u0026gt;() { @Override public Double map(Integer count) throws Exception { return count / (double) 10000 * 4; } }).print(); env.execute(\u0026#34;Iterative Pi Example\u0026#34;); Delta Iterations # Delta iterations exploit the fact that certain algorithms do not change every data point of the solution in each iteration.
In addition to the partial solution that is fed back (called workset) in every iteration, delta iterations maintain state across iterations (called solution set), which can be updated through deltas. The result of the iterative computation is the state after the last iteration. Please refer to the Introduction to Iterations for an overview of the basic principle of delta iterations.
Defining a DeltaIteration is similar to defining a BulkIteration. For delta iterations, two data sets form the input to each iteration (workset and solution set), and two data sets are produced as the result (new workset, solution set delta) in each iteration.
To create a DeltaIteration call the iterateDelta(DataSet, int, int) (or iterateDelta(DataSet, int, int[]) respectively). This method is called on the initial solution set. The arguments are the initial delta set, the maximum number of iterations and the key positions. The returned DeltaIteration object gives you access to the DataSets representing the workset and solution set via the methods iteration.getWorkset() and iteration.getSolutionSet().
Below is an example for the syntax of a delta iteration
// read the initial data sets DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; initialSolutionSet = // [...] DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; initialDeltaSet = // [...] int maxIterations = 100; int keyPosition = 0; DeltaIteration\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; iteration = initialSolutionSet .iterateDelta(initialDeltaSet, maxIterations, keyPosition); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; candidateUpdates = iteration.getWorkset() .groupBy(1) .reduceGroup(new ComputeCandidateChanges()); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; deltas = candidateUpdates .join(iteration.getSolutionSet()) .where(0) .equalTo(0) .with(new CompareChangesToCurrent()); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; nextWorkset = deltas .filter(new FilterByThreshold()); iteration.closeWith(deltas, nextWorkset) .writeAsCsv(outputPath); Scala Bulk Iterations # To create a BulkIteration call the iterate(int) method of the DataSet the iteration should start at and also specify a step function. The step function gets the input DataSet for the current iteration and must return a new DataSet. The parameter of the iterate call is the maximum number of iterations after which to stop.
There is also the iterateWithTermination(int) function that accepts a step function that returns two DataSets: The result of the iteration step and a termination criterion. The iterations are stopped once the termination criterion DataSet is empty.
The following example iteratively estimates the number Pi. The goal is to count the number of random points, which fall into the unit circle. In each iteration, a random point is picked. If this point lies inside the unit circle, we increment the count. Pi is then estimated as the resulting count divided by the number of iterations multiplied by 4.
val env = ExecutionEnvironment.getExecutionEnvironment() // Create initial DataSet val initial = env.fromElements(0) val count = initial.iterate(10000) { iterationInput: DataSet[Int] =\u0026gt; val result = iterationInput.map { i =\u0026gt; val x = Math.random() val y = Math.random() i + (if (x * x + y * y \u0026lt; 1) 1 else 0) } result } val result = count map { c =\u0026gt; c / 10000.0 * 4 } result.print() env.execute(\u0026#34;Iterative Pi Example\u0026#34;) Delta Iterations # Delta iterations exploit the fact that certain algorithms do not change every data point of the solution in each iteration.
In addition to the partial solution that is fed back (called workset) in every iteration, delta iterations maintain state across iterations (called solution set), which can be updated through deltas. The result of the iterative computation is the state after the last iteration. Please refer to the Introduction to Iterations for an overview of the basic principle of delta iterations.
Defining a DeltaIteration is similar to defining a BulkIteration. For delta iterations, two data sets form the input to each iteration (workset and solution set), and two data sets are produced as the result (new workset, solution set delta) in each iteration.
To create a DeltaIteration call the iterateDelta(initialWorkset, maxIterations, key) on the initial solution set. The step function takes two parameters: (solutionSet, workset), and must return two values: (solutionSetDelta, newWorkset).
Below is an example for the syntax of a delta iteration
// read the initial data sets val initialSolutionSet: DataSet[(Long, Double)] = // [...] val initialWorkset: DataSet[(Long, Double)] = // [...] val maxIterations = 100 val keyPosition = 0 val result = initialSolutionSet.iterateDelta(initialWorkset, maxIterations, Array(keyPosition)) { (solution, workset) =\u0026gt; val candidateUpdates = workset.groupBy(1).reduceGroup(new ComputeCandidateChanges()) val deltas = candidateUpdates.join(solution).where(0).equalTo(0)(new CompareChangesToCurrent()) val nextWorkset = deltas.filter(new FilterByThreshold()) (deltas, nextWorkset) } result.writeAsCsv(outputPath) env.execute() Operating on Data Objects in Functions # Flink’s runtime exchanges data with user functions in form of Java objects. Functions receive input objects from the runtime as method parameters and return output objects as result. Because these objects are accessed by user functions and runtime code, it is very important to understand and follow the rules about how the user code may access, i.e., read and modify, these objects.
User functions receive objects from Flink’s runtime either as regular method parameters (like a MapFunction) or through an Iterable parameter (like a GroupReduceFunction). We refer to objects that the runtime passes to a user function as input objects. User functions can emit objects to the Flink runtime either as a method return value (like a MapFunction) or through a Collector (like a FlatMapFunction). We refer to objects which have been emitted by the user function to the runtime as output objects.
Flink’s DataSet API features two modes that differ in how Flink’s runtime creates or reuses input objects. This behavior affects the guarantees and constraints for how user functions may interact with input and output objects. The following sections define these rules and give coding guidelines to write safe user function code.
Object-Reuse Disabled (DEFAULT) # By default, Flink operates in object-reuse disabled mode. This mode ensures that functions always receive new input objects within a function call. The object-reuse disabled mode gives better guarantees and is safer to use. However, it comes with a certain processing overhead and might cause higher Java garbage collection activity. The following table explains how user functions may access input and output objects in object-reuse disabled mode.
Operation Guarantees and Restrictions Reading Input Objects Within a method call it is guaranteed that the value of an input object does not change. This includes objects served by an Iterable. For example it is safe to collect input objects served by an Iterable in a List or Map. Note that objects may be modified after the method call is left. It is not safe to remember objects across function calls. Modifying Input Objects You may modify input objects. Emitting Input Objects You may emit input objects. The value of an input object may have changed after it was emitted. It is not safe to read an input object after it was emitted. Reading Output Objects An object that was given to a Collector or returned as method result might have changed its value. It is not safe to read an output object. Modifying Output Objects You may modify an object after it was emitted and emit it again. Coding guidelines for the object-reuse disabled (default) mode:
Do not remember the read input objects across method calls. Do not read objects after you emitted them. Object-Reuse Enabled # In object-reuse enabled mode, Flink’s runtime minimizes the number of object instantiations. This can improve the performance and can reduce the Java garbage collection pressure. The object-reuse enabled mode is activated by calling ExecutionConfig.enableObjectReuse(). The following table explains how user functions may access input and output objects in object-reuse enabled mode.
Operation Guarantees and Restrictions Reading input objects received as regular method parameters Input objects received as regular method arguments are not modified within a function call. Objects may be modified after method call is left. It is not safe to remember objects across function calls. Reading input objects received from an Iterable parameter Input objects received from an Iterable are only valid until the next() method is called. An Iterable or Iterator may serve the same object instance multiple times. It is not safe to remember input objects received from an Iterable, e.g., by putting them in a List or Map. Modifying Input Objects You must not modify input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). Emitting Input Objects You must not emit input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). Reading output Objects An object that was given to a Collector or returned as method result might have changed its value. It is not safe to read an output object. Modifying Output Objects You may modify an output object and emit it again. Coding guidelines for object-reuse enabled:
Do not remember input objects received from an Iterable. Do not remember and read input objects across method calls. Do not modify or emit input objects, except for input objects of MapFunction, FlatMapFunction, MapPartitionFunction, GroupReduceFunction, GroupCombineFunction, CoGroupFunction, and InputFormat.next(reuse). To reduce object instantiations, you can always emit a dedicated output object which is repeatedly modified but never read. Debugging # Before running a data analysis program on a large data set in a distributed cluster, it is a good idea to make sure that the implemented algorithm works as desired. Hence, implementing data analysis programs is usually an incremental process of checking results, debugging, and improving.
Flink provides a few nice features to significantly ease the development process of data analysis programs by supporting local debugging from within an IDE, injection of test data, and collection of result data. This section give some hints how to ease the development of Flink programs.
Local Execution Envronment # A LocalEnvironment starts a Flink system within the same JVM process it was created in. If you start the LocalEnvironment from an IDE, you can set breakpoints in your code and easily debug your program.
A LocalEnvironment is created and used as follows:
Java final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); DataSet\u0026lt;String\u0026gt; lines = env.readTextFile(pathToTextFile); // build your program env.execute(); Scala val env = ExecutionEnvironment.createLocalEnvironment() val lines = env.readTextFile(pathToTextFile) // build your program env.execute() Collection Data Sources and Sinks # Providing input for an analysis program and checking its output is cumbersome when done by creating input files and reading output files. Flink features special data sources and sinks which are backed by Java collections to ease testing. Once a program has been tested, the sources and sinks can be easily replaced by sources and sinks that read from / write to external data stores such as HDFS.
Collection data sources can be used as follows:
Java final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); // Create a DataSet from a list of elements DataSet\u0026lt;Integer\u0026gt; myInts = env.fromElements(1, 2, 3, 4, 5); // Create a DataSet from any Java collection List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; data = ...; DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myTuples = env.fromCollection(data); // Create a DataSet from an Iterator Iterator\u0026lt;Long\u0026gt; longIt = ...; DataSet\u0026lt;Long\u0026gt; myLongs = env.fromCollection(longIt, Long.class); Scala val env = ExecutionEnvironment.createLocalEnvironment() // Create a DataSet from a list of elements val myInts = env.fromElements(1, 2, 3, 4, 5) // Create a DataSet from any Collection val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // Create a DataSet from an Iterator val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) Note: Currently, the collection data source requires that data types and iterators implement Serializable. Furthermore, collection data sources can not be executed in parallel ( parallelism = 1).
Broadcast Variables # Broadcast variables allow you to make a data set available to all parallel instances of an operation, in addition to the regular input of the operation. This is useful for auxiliary data sets, or data-dependent parameterization. The data set will then be accessible at the operator as a Collection.
Broadcast: broadcast sets are registered by name via withBroadcastSet(DataSet, String), and Access: accessible via getRuntimeContext().getBroadcastVariable(String) at the target operator. Java Java Scala // 1. The DataSet to be broadcast DataSet\u0026lt;Integer\u0026gt; toBroadcast = env.fromElements(1, 2, 3); DataSet\u0026lt;String\u0026gt; data = env.fromElements(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); data.map(new RichMapFunction\u0026lt;String, String\u0026gt;() { @Override public void open(Configuration parameters) throws Exception { // 3. Access the broadcast DataSet as a Collection Collection\u0026lt;Integer\u0026gt; broadcastSet = getRuntimeContext().getBroadcastVariable(\u0026#34;broadcastSetName\u0026#34;); } @Override public String map(String value) throws Exception { ... } }).withBroadcastSet(toBroadcast, \u0026#34;broadcastSetName\u0026#34;); // 2. Broadcast the DataSet Scala // 1. The DataSet to be broadcast val toBroadcast = env.fromElements(1, 2, 3) val data = env.fromElements(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) data.map(new RichMapFunction[String, String]() { var broadcastSet: Traversable[String] = null override def open(config: Configuration): Unit = { // 3. Access the broadcast DataSet as a Collection broadcastSet = getRuntimeContext().getBroadcastVariable[String](\u0026#34;broadcastSetName\u0026#34;).asScala } def map(in: String): String = { ... } }).withBroadcastSet(toBroadcast, \u0026#34;broadcastSetName\u0026#34;) // 2. Broadcast the DataSet Make sure that the names (broadcastSetName in the previous example) match when registering and accessing broadcast data sets. For a complete example program, have a look at K-Means Algorithm.
Note: As the content of broadcast variables is kept in-memory on each node, it should not become too large. For simpler things like scalar values you can simply make parameters part of the closure of a function, or use the withParameters(\u0026hellip;) method to pass in a configuration.
Distributed Cache # Flink offers a distributed cache, similar to Apache Hadoop, to make files locally accessible to parallel instances of user functions. This functionality can be used to share files that contain static external data such as dictionaries or machine-learned regression models.
The cache works as follows. A program registers a file or directory of a local or remote filesystem such as HDFS or S3 under a specific name in its ExecutionEnvironment as a cached file. When the program is executed, Flink automatically copies the file or directory to the local filesystem of all workers. A user function can look up the file or directory under the specified name and access it from the worker’s local filesystem.
The distributed cache is used as follows:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // register a file from HDFS env.registerCachedFile(\u0026#34;hdfs:///path/to/your/file\u0026#34;, \u0026#34;hdfsFile\u0026#34;); // register a local executable file (script, executable, ...) env.registerCachedFile(\u0026#34;file:///path/to/exec/file\u0026#34;, \u0026#34;localExecFile\u0026#34;, true); // define your program and execute ... DataSet\u0026lt;String\u0026gt; input = ...; DataSet\u0026lt;Integer\u0026gt; result = input.map(new MyMapper()); ... env.execute(); Scala val env = ExecutionEnvironment.getExecutionEnvironment // register a file from HDFS env.registerCachedFile(\u0026#34;hdfs:///path/to/your/file\u0026#34;, \u0026#34;hdfsFile\u0026#34;) // register a local executable file (script, executable, ...) env.registerCachedFile(\u0026#34;file:///path/to/exec/file\u0026#34;, \u0026#34;localExecFile\u0026#34;, true) // define your program and execute ... val input: DataSet[String] = ... val result: DataSet[Integer] = input.map(new MyMapper()) ... env.execute() Access the cached file in a user function (here a MapFunction). The function must extend a RichFunction class because it needs access to the RuntimeContext.
Java // extend a RichFunction to have access to the RuntimeContext public final class MyMapper extends RichMapFunction\u0026lt;String, Integer\u0026gt; { @Override public void open(Configuration config) { // access cached file via RuntimeContext and DistributedCache File myFile = getRuntimeContext().getDistributedCache().getFile(\u0026#34;hdfsFile\u0026#34;); // read the file (or navigate the directory) ... } @Override public Integer map(String value) throws Exception { // use content of cached file ... } } Scala // extend a RichFunction to have access to the RuntimeContext class MyMapper extends RichMapFunction[String, Int] { override def open(config: Configuration): Unit = { // access cached file via RuntimeContext and DistributedCache val myFile: File = getRuntimeContext.getDistributedCache.getFile(\u0026#34;hdfsFile\u0026#34;) // read the file (or navigate the directory) ... } override def map(value: String): Int = { // use content of cached file ... } } Passing Parameters to Functions # Parameters can be passed to functions using either the constructor or the withParameters(Configuration) method. The parameters are serialized as part of the function object and shipped to all parallel task instances.
Via Constructor # Java DataSet\u0026lt;Integer\u0026gt; toFilter = env.fromElements(1, 2, 3); toFilter.filter(new MyFilter(2)); private static class MyFilter implements FilterFunction\u0026lt;Integer\u0026gt; { private final int limit; public MyFilter(int limit) { this.limit = limit; } @Override public boolean filter(Integer value) throws Exception { return value \u0026gt; limit; } } Scala val toFilter = env.fromElements(1, 2, 3) toFilter.filter(new MyFilter(2)) class MyFilter(limit: Int) extends FilterFunction[Int] { override def filter(value: Int): Boolean = { value \u0026gt; limit } } Via withParameters(Configuration) # Java DataSet\u0026lt;Integer\u0026gt; toFilter = env.fromElements(1, 2, 3); Configuration config = new Configuration(); config.setInteger(\u0026#34;limit\u0026#34;, 2); toFilter.filter(new RichFilterFunction\u0026lt;Integer\u0026gt;() { private int limit; @Override public void open(Configuration parameters) throws Exception { limit = parameters.getInteger(\u0026#34;limit\u0026#34;, 0); } @Override public boolean filter(Integer value) throws Exception { return value \u0026gt; limit; } }).withParameters(config); Scala val toFilter = env.fromElements(1, 2, 3) val c = new Configuration() c.setInteger(\u0026#34;limit\u0026#34;, 2) toFilter.filter(new RichFilterFunction[Int]() { var limit = 0 override def open(config: Configuration): Unit = { limit = config.getInteger(\u0026#34;limit\u0026#34;, 0) } def filter(in: Int): Boolean = { in \u0026gt; limit } }).withParameters(c) Globally via the ExecutionConfig # Flink also allows to pass custom configuration values to the ExecutionConfig interface of the environment. Since the execution config is accessible in all (rich) user functions, the custom configuration will be available globally in all functions.
Setting a custom global configuration
Java Configuration conf = new Configuration(); conf.setString(\u0026#34;mykey\u0026#34;,\u0026#34;myvalue\u0026#34;); final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(conf); Scala val env = ExecutionEnvironment.getExecutionEnvironment val conf = new Configuration() conf.setString(\u0026#34;mykey\u0026#34;, \u0026#34;myvalue\u0026#34;) env.getConfig.setGlobalJobParameters(conf) Please note that you can also pass a custom class extending the ExecutionConfig.GlobalJobParameters class as the global job parameters to the execution config. The interface allows to implement the Map\u0026lt;String, String\u0026gt; toMap() method which will in turn show the values from the configuration in the web frontend.
Accessing values from the global configuration
public static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private String mykey; @Override public void open(Configuration parameters) throws Exception { ExecutionConfig.GlobalJobParameters globalParams = getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); Configuration globConf = (Configuration) globalParams; mykey = globConf.getString(\u0026#34;mykey\u0026#34;, null); } `}),e.add({id:16,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/types_serialization/",title:"概览",section:"数据类型以及序列化",content:` 数据类型以及序列化 # Apache Flink 以其独特的方式来处理数据类型以及序列化，这种方式包括它自身的类型描述符、泛型类型提取以及类型序列化框架。 本文档描述了它们背后的概念和基本原理。
Supported Data Types # Flink places some restrictions on the type of elements that can be in a DataStream. The reason for this is that the system analyzes the types to determine efficient execution strategies.
There are seven different categories of data types:
Java Tuples and Scala Case Classes Java POJOs Primitive Types Regular Classes Values Hadoop Writables Special Types Tuples and Case Classes # Java Tuples are composite types that contain a fixed number of fields with various types. The Java API provides classes from Tuple1 up to Tuple25. Every field of a tuple can be an arbitrary Flink type including further tuples, resulting in nested tuples. Fields of a tuple can be accessed directly using the field\u0026rsquo;s name as tuple.f4, or using the generic getter method tuple.getField(int position). The field indices start at 0. Note that this stands in contrast to the Scala tuples, but it is more consistent with Java\u0026rsquo;s general indexing.
DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = env.fromElements( new Tuple2\u0026lt;String, Integer\u0026gt;(\u0026#34;hello\u0026#34;, 1), new Tuple2\u0026lt;String, Integer\u0026gt;(\u0026#34;world\u0026#34;, 2)); wordCounts.map(new MapFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Integer\u0026gt;() { @Override public Integer map(Tuple2\u0026lt;String, Integer\u0026gt; value) throws Exception { return value.f1; } }); wordCounts.keyBy(value -\u0026gt; value.f0); Scala Scala case classes (and Scala tuples which are a special case of case classes), are composite types that contain a fixed number of fields with various types. Tuple fields are addressed by their 1-offset names such as _1 for the first field. Case class fields are accessed by their name.
case class WordCount(word: String, count: Int) val input = env.fromElements( WordCount(\u0026#34;hello\u0026#34;, 1), WordCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set input.keyBy(_.word) val input2 = env.fromElements((\u0026#34;hello\u0026#34;, 1), (\u0026#34;world\u0026#34;, 2)) // Tuple2 Data Set input2.keyBy(value =\u0026gt; (value._1, value._2)) POJOs # Java and Scala classes are treated by Flink as a special POJO data type if they fulfill the following requirements:
The class must be public.
It must have a public constructor without arguments (default constructor).
All fields are either public or must be accessible through getter and setter functions. For a field called foo the getter and setter methods must be named getFoo() and setFoo().
The type of a field must be supported by a registered serializer.
POJOs are generally represented with a PojoTypeInfo and serialized with the PojoSerializer (using Kryo as configurable fallback). The exception is when the POJOs are actually Avro types (Avro Specific Records) or produced as \u0026ldquo;Avro Reflect Types\u0026rdquo;. In that case the POJO\u0026rsquo;s are represented by an AvroTypeInfo and serialized with the AvroSerializer. You can also register your own custom serializer if required; see Serialization for further information.
Flink analyzes the structure of POJO types, i.e., it learns about the fields of a POJO. As a result POJO types are easier to use than general types. Moreover, Flink can process POJOs more efficiently than general types.
You can test whether your class adheres to the POJO requirements via org.apache.flink.types.PojoTestUtils#assertSerializedAsPojo() from the flink-test-utils. If you additionally want to ensure that no field of the POJO will be serialized with Kryo, use assertSerializedAsPojoWithoutKryo() instead.
The following example shows a simple POJO with two public fields.
Java public class WordWithCount { public String word; public int count; public WordWithCount() {} public WordWithCount(String word, int count) { this.word = word; this.count = count; } } DataStream\u0026lt;WordWithCount\u0026gt; wordCounts = env.fromElements( new WordWithCount(\u0026#34;hello\u0026#34;, 1), new WordWithCount(\u0026#34;world\u0026#34;, 2)); wordCounts.keyBy(value -\u0026gt; value.word); Scala class WordWithCount(var word: String, var count: Int) { def this() { this(null, -1) } } val input = env.fromElements( new WordWithCount(\u0026#34;hello\u0026#34;, 1), new WordWithCount(\u0026#34;world\u0026#34;, 2)) // Case Class Data Set input.keyBy(_.word) Primitive Types # Flink supports all Java and Scala primitive types such as Integer, String, and Double.
General Class Types # Flink supports most Java and Scala classes (API and custom). Restrictions apply to classes containing fields that cannot be serialized, like file pointers, I/O streams, or other native resources. Classes that follow the Java Beans conventions work well in general.
All classes that are not identified as POJO types (see POJO requirements above) are handled by Flink as general class types. Flink treats these data types as black boxes and is not able to access their content (e.g., for efficient sorting). General types are de/serialized using the serialization framework Kryo.
Values # Value types describe their serialization and deserialization manually. Instead of going through a general purpose serialization framework, they provide custom code for those operations by means of implementing the org.apache.flink.types.Value interface with the methods read and write. Using a Value type is reasonable when general purpose serialization would be highly inefficient. An example would be a data type that implements a sparse vector of elements as an array. Knowing that the array is mostly zero, one can use a special encoding for the non-zero elements, while the general purpose serialization would simply write all array elements.
The org.apache.flink.types.CopyableValue interface supports manual internal cloning logic in a similar way.
Flink comes with pre-defined Value types that correspond to basic data types. (ByteValue, ShortValue, IntValue, LongValue, FloatValue, DoubleValue, StringValue, CharValue, BooleanValue). These Value types act as mutable variants of the basic data types: Their value can be altered, allowing programmers to reuse objects and take pressure off the garbage collector.
Hadoop Writables # You can use types that implement the org.apache.hadoop.Writable interface. The serialization logic defined in the write()and readFields() methods will be used for serialization.
Special Types # You can use special types, including Scala\u0026rsquo;s Either, Option, and Try. The Java API has its own custom implementation of Either. Similarly to Scala\u0026rsquo;s Either, it represents a value of two possible types, Left or Right. Either can be useful for error handling or operators that need to output two different types of records.
Type Erasure \u0026amp; Type Inference # Note: This Section is only relevant for Java.
The Java compiler throws away much of the generic type information after compilation. This is known as type erasure in Java. It means that at runtime, an instance of an object does not know its generic type any more. For example, instances of DataStream\u0026lt;String\u0026gt; and DataStream\u0026lt;Long\u0026gt; look the same to the JVM.
Flink requires type information at the time when it prepares the program for execution (when the main method of the program is called). The Flink Java API tries to reconstruct the type information that was thrown away in various ways and store it explicitly in the data sets and operators. You can retrieve the type via DataStream.getType(). The method returns an instance of TypeInformation, which is Flink\u0026rsquo;s internal way of representing types.
The type inference has its limits and needs the \u0026ldquo;cooperation\u0026rdquo; of the programmer in some cases. Examples for that are methods that create data sets from collections, such as StreamExecutionEnvironment.fromCollection(), where you can pass an argument that describes the type. But also generic functions like MapFunction\u0026lt;I, O\u0026gt; may need extra type information.
The ResultTypeQueryable interface can be implemented by input formats and functions to tell the API explicitly about their return type. The input types that the functions are invoked with can usually be inferred by the result types of the previous operations.
Back to top
Type handling in Flink # Flink tries to infer a lot of information about the data types that are exchanged and stored during the distributed computation. Think about it like a database that infers the schema of tables. In most cases, Flink infers all necessary information seamlessly by itself. Having the type information allows Flink to do some cool things:
The more Flink knows about data types, the better the serialization and data layout schemes are. That is quite important for the memory usage paradigm in Flink (work on serialized data inside/outside the heap where ever possible and make serialization very cheap).
Finally, it also spares users in the majority of cases from worrying about serialization frameworks and having to register types.
In general, the information about data types is needed during the pre-flight phase - that is, when the program\u0026rsquo;s calls on DataStream are made, and before any call to execute(), print(), count(), or collect().
Most Frequent Issues # The most frequent issues where users need to interact with Flink\u0026rsquo;s data type handling are:
Registering subtypes: If the function signatures describe only the supertypes, but they actually use subtypes of those during execution, it may increase performance a lot to make Flink aware of these subtypes. For that, call .registerType(clazz) on the StreamExecutionEnvironment for each subtype.
Registering custom serializers: Flink falls back to Kryo for the types that it does not handle transparently by itself. Not all types are seamlessly handled by Kryo (and thus by Flink). For example, many Google Guava collection types do not work well by default. The solution is to register additional serializers for the types that cause problems. Call .getConfig().addDefaultKryoSerializer(clazz, serializer) on the StreamExecutionEnvironment. Additional Kryo serializers are available in many libraries. See Custom Serializers for more details on working with custom serializers.
Adding Type Hints: Sometimes, when Flink cannot infer the generic types despite all tricks, a user must pass a type hint. That is generally only necessary in the Java API. The Type Hints Section describes that in more detail.
Manually creating a TypeInformation: This may be necessary for some API calls where it is not possible for Flink to infer the data types due to Java\u0026rsquo;s generic type erasure. See Creating a TypeInformation or TypeSerializer for details.
Flink\u0026rsquo;s TypeInformation class # The class TypeInformation is the base class for all type descriptors. It reveals some basic properties of the type and can generate serializers and, in specializations, comparators for the types. (Note that comparators in Flink do much more than defining an order - they are basically the utility to handle keys)
Internally, Flink makes the following distinctions between types:
Basic types: All Java primitives and their boxed form, plus void, String, Date, BigDecimal, and BigInteger.
Primitive arrays and Object arrays
Composite types
Flink Java Tuples (part of the Flink Java API): max 25 fields, null fields not supported
Scala case classes (including Scala tuples): null fields not supported
Row: tuples with arbitrary number of fields and support for null fields
POJOs: classes that follow a certain bean-like pattern
Auxiliary types (Option, Either, Lists, Maps, \u0026hellip;)
Generic types: These will not be serialized by Flink itself, but by Kryo.
POJOs are of particular interest, because they support the creation of complex types. They are also transparent to the runtime and can be handled very efficiently by Flink.
Rules for POJO types # Flink recognizes a data type as a POJO type (and allows \u0026ldquo;by-name\u0026rdquo; field referencing) if the following conditions are fulfilled:
The class is public and standalone (no non-static inner class) The class has a public no-argument constructor All non-static, non-transient fields in the class (and all superclasses) are either public (and non-final) or have a public getter- and a setter- method that follows the Java beans naming conventions for getters and setters. Note that when a user-defined data type can\u0026rsquo;t be recognized as a POJO type, it must be processed as GenericType and serialized with Kryo.
Creating a TypeInformation or TypeSerializer # To create a TypeInformation object for a type, use the language specific way:
Java Because Java generally erases generic type information, you need to pass the type to the TypeInformation construction:
For non-generic types, you can pass the Class:
TypeInformation\u0026lt;String\u0026gt; info = TypeInformation.of(String.class); For generic types, you need to \u0026ldquo;capture\u0026rdquo; the generic type information via the TypeHint:
TypeInformation\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; info = TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt;(){}); Internally, this creates an anonymous subclass of the TypeHint that captures the generic information to preserve it until runtime.
Scala In Scala, Flink uses macros that runs at compile time and captures all generic type information while it is still available.
// important: this import is needed to access the \u0026#39;createTypeInformation\u0026#39; macro function import org.apache.flink.streaming.api.scala._ val stringInfo: TypeInformation[String] = createTypeInformation[String] val tupleInfo: TypeInformation[(String, Double)] = createTypeInformation[(String, Double)] You can still use the same method as in Java as a fallback.
To create a TypeSerializer, simply call typeInfo.createSerializer(config) on the TypeInformation object.
The config parameter is of type ExecutionConfig and holds the information about the program\u0026rsquo;s registered custom serializers. Where ever possibly, try to pass the programs proper ExecutionConfig. You can usually obtain it from DataStream via calling getExecutionConfig(). Inside functions (like MapFunction), you can get it by making the function a Rich Function and calling getRuntimeContext().getExecutionConfig().
Type Information in the Scala API # Scala has very elaborate concepts for runtime type information though type manifests and class tags. In general, types and methods have access to the types of their generic parameters - thus, Scala programs do not suffer from type erasure as Java programs do.
In addition, Scala allows to run custom code in the Scala Compiler through Scala Macros - that means that some Flink code gets executed whenever you compile a Scala program written against Flink\u0026rsquo;s Scala API.
We use the Macros to look at the parameter types and return types of all user functions during compilation - that is the point in time when certainly all type information is perfectly available. Within the macro, we create a TypeInformation for the function\u0026rsquo;s return types (or parameter types) and make it part of the operation.
No Implicit Value for Evidence Parameter Error # In the case where TypeInformation could not be created, programs fail to compile with an error stating \u0026ldquo;could not find implicit value for evidence parameter of type TypeInformation\u0026rdquo;.
A frequent reason if that the code that generates the TypeInformation has not been imported. Make sure to import the entire flink.api.scala package.
import org.apache.flink.api.scala._ Another common cause are generic methods, which can be fixed as described in the following section.
Generic Methods # Consider the following case below:
def selectFirst[T](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v =\u0026gt; v._1 } } val data : DataStream[(String, Long) = ... val result = selectFirst(data) For such generic methods, the data types of the function parameters and return type may not be the same for every call and are not known at the site where the method is defined. The code above will result in an error that not enough implicit evidence is available.
In such cases, the type information has to be generated at the invocation site and passed to the method. Scala offers implicit parameters for that.
The following code tells Scala to bring a type information for T into the function. The type information will then be generated at the sites where the method is invoked, rather than where the method is defined.
def selectFirst[T : TypeInformation](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v =\u0026gt; v._1 } } Type Information in the Java API # In the general case, Java erases generic type information. Flink tries to reconstruct as much type information as possible via reflection, using the few bits that Java preserves (mainly function signatures and subclass information). This logic also contains some simple type inference for cases where the return type of a function depends on its input type:
public class AppendOne\u0026lt;T\u0026gt; implements MapFunction\u0026lt;T, Tuple2\u0026lt;T, Long\u0026gt;\u0026gt; { public Tuple2\u0026lt;T, Long\u0026gt; map(T value) { return new Tuple2\u0026lt;T, Long\u0026gt;(value, 1L); } } There are cases where Flink cannot reconstruct all generic type information. In that case, a user has to help out via type hints.
Type Hints in the Java API # In cases where Flink cannot reconstruct the erased generic type information, the Java API offers so called type hints. The type hints tell the system the type of the data stream or data set produced by a function:
DataStream\u0026lt;SomeType\u0026gt; result = stream .map(new MyGenericNonInferrableFunction\u0026lt;Long, SomeType\u0026gt;()) .returns(SomeType.class); The returns statement specifies the produced type, in this case via a class. The hints support type definition via
Classes, for non-parameterized types (no generics) TypeHints in the form of returns(new TypeHint\u0026lt;Tuple2\u0026lt;Integer, SomeType\u0026gt;\u0026gt;(){}). The TypeHint class can capture generic type information and preserve it for the runtime (via an anonymous subclass). Type extraction for Java 8 lambdas # Type extraction for Java 8 lambdas works differently than for non-lambdas, because lambdas are not associated with an implementing class that extends the function interface.
Currently, Flink tries to figure out which method implements the lambda and uses Java\u0026rsquo;s generic signatures to determine the parameter types and the return type. However, these signatures are not generated for lambdas by all compilers (as of writing this document only reliably by the Eclipse JDT compiler from 4.5 onwards).
Serialization of POJO types # The PojoTypeInfo is creating serializers for all the fields inside the POJO. Standard types such as int, long, String etc. are handled by serializers we ship with Flink. For all other types, we fall back to Kryo.
If Kryo is not able to handle the type, you can ask the PojoTypeInfo to serialize the POJO using Avro. To do so, you have to call
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceAvro(); Note that Flink is automatically serializing POJOs generated by Avro with the Avro serializer.
If you want your entire POJO Type to be treated by the Kryo serializer, set
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceKryo(); If Kryo is not able to serialize your POJO, you can add a custom serializer to Kryo, using
env.getConfig().addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Class\u0026lt;? extends Serializer\u0026lt;?\u0026gt;\u0026gt; serializerClass); There are different variants of these methods available.
Disabling Kryo Fallback # There are cases when programs may want to explicitly avoid using Kryo as a fallback for generic types. The most common one is wanting to ensure that all types are efficiently serialized either through Flink\u0026rsquo;s own serializers, or via user-defined custom serializers.
The setting below will raise an exception whenever a data type is encountered that would go through Kryo:
env.getConfig().disableGenericTypes(); Defining Type Information using a Factory # A type information factory allows for plugging-in user-defined type information into the Flink type system. You have to implement org.apache.flink.api.common.typeinfo.TypeInfoFactory to return your custom type information. The factory is called during the type extraction phase if the corresponding type has been annotated with the @org.apache.flink.api.common.typeinfo.TypeInfo annotation.
Type information factories can be used in both the Java and Scala API.
In a hierarchy of types the closest factory will be chosen while traversing upwards, however, a built-in factory has highest precedence. A factory has also higher precedence than Flink\u0026rsquo;s built-in types, therefore you should know what you are doing.
The following example shows how to annotate a custom type MyTuple and supply custom type information for it using a factory in Java.
The annotated custom type:
@TypeInfo(MyTupleTypeInfoFactory.class) public class MyTuple\u0026lt;T0, T1\u0026gt; { public T0 myfield0; public T1 myfield1; } The factory supplying custom type information:
public class MyTupleTypeInfoFactory extends TypeInfoFactory\u0026lt;MyTuple\u0026gt; { @Override public TypeInformation\u0026lt;MyTuple\u0026gt; createTypeInfo(Type t, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;\u0026gt; genericParameters) { return new MyTupleTypeInfo(genericParameters.get(\u0026#34;T0\u0026#34;), genericParameters.get(\u0026#34;T1\u0026#34;)); } } The method createTypeInfo(Type, Map\u0026lt;String, TypeInformation\u0026lt;?\u0026gt;\u0026gt;) creates type information for the type the factory is targeted for. The parameters provide additional information about the type itself as well as the type\u0026rsquo;s generic type parameters if available.
If your type contains generic parameters that might need to be derived from the input type of a Flink function, make sure to also implement org.apache.flink.api.common.typeinfo.TypeInformation#getGenericParameters for a bidirectional mapping of generic parameters to type information.
Back to top
`}),e.add({id:17,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/overview/",title:"概览",section:"算子",content:` 算子 # 用户通过算子能将一个或多个 DataStream 转换成新的 DataStream，在应用程序中可以将多个数据转换算子合并成一个复杂的数据流拓扑。
这部分内容将描述 Flink DataStream API 中基本的数据转换 API，数据转换后各种数据分区方式，以及算子的链接策略。
数据流转换 # Map # DataStream → DataStream # 输入一个元素同时输出一个元素。下面是将输入流中元素数值加倍的 map function：
Java DataStream\u0026lt;Integer\u0026gt; dataStream = //... dataStream.map(new MapFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public Integer map(Integer value) throws Exception { return 2 * value; } }); Scala dataStream.map { x =\u0026gt; x * 2 } Python data_stream = env.from_collection(collection=[1, 2, 3, 4, 5]) data_stream.map(lambda x: 2 * x, output_type=Types.INT()) FlatMap # DataStream → DataStream # 输入一个元素同时产生零个、一个或多个元素。下面是将句子拆分为单词的 flatmap function：
Java dataStream.flatMap(new FlatMapFunction\u0026lt;String, String\u0026gt;() { @Override public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) throws Exception { for(String word: value.split(\u0026#34; \u0026#34;)){ out.collect(word); } } }); Scala dataStream.flatMap { str =\u0026gt; str.split(\u0026#34; \u0026#34;) } Python data_stream = env.from_collection(collection=[\u0026#39;hello apache flink\u0026#39;, \u0026#39;streaming compute\u0026#39;]) data_stream.flat_map(lambda x: x.split(\u0026#39; \u0026#39;), output_type=Types.STRING()) Filter # DataStream → DataStream # 为每个元素执行一个布尔 function，并保留那些 function 输出值为 true 的元素。下面是过滤掉零值的 filter：
Java dataStream.filter(new FilterFunction\u0026lt;Integer\u0026gt;() { @Override public boolean filter(Integer value) throws Exception { return value != 0; } }); Scala dataStream.filter { _ != 0 } Python data_stream = env.from_collection(collection=[0, 1, 2, 3, 4, 5]) data_stream.filter(lambda x: x != 0) KeyBy # DataStream → KeyedStream # 在逻辑上将流划分为不相交的分区。具有相同 key 的记录都分配到同一个分区。在内部， keyBy() 是通过哈希分区实现的。有多种指定 key 的方式。
Java dataStream.keyBy(value -\u0026gt; value.getSomeKey()); dataStream.keyBy(value -\u0026gt; value.f0); Scala dataStream.keyBy(_.someKey) dataStream.keyBy(_._1) Python data_stream = env.from_collection(collection=[(1, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;b\u0026#39;)]) data_stream.key_by(lambda x: x[1], key_type=Types.STRING()) // Key by the result of KeySelector 以下情况，一个类不能作为 key：
它是一种 POJO 类，但没有重写 hashCode() 方法而是依赖于 Object.hashCode() 实现。 它是任意类的数组。 Reduce # KeyedStream → DataStream # 在相同 key 的数据流上“滚动”执行 reduce。将当前元素与最后一次 reduce 得到的值组合然后输出新值。
下面是创建局部求和流的 reduce function：
Java keyedStream.reduce(new ReduceFunction\u0026lt;Integer\u0026gt;() { @Override public Integer reduce(Integer value1, Integer value2) throws Exception { return value1 + value2; } }); Scala keyedStream.reduce { _ + _ } Python data_stream = env.from_collection(collection=[(1, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;a\u0026#39;), (4, \u0026#39;b\u0026#39;)], type_info=Types.TUPLE([Types.INT(), Types.STRING()])) data_stream.key_by(lambda x: x[1]).reduce(lambda a, b: (a[0] + b[0], b[1])) Window # KeyedStream → WindowedStream # 可以在已经分区的 KeyedStreams 上定义 Window。Window 根据某些特征（例如，最近 5 秒内到达的数据）对每个 key Stream 中的数据进行分组。请参阅 windows 获取有关 window 的完整说明。
Java dataStream .keyBy(value -\u0026gt; value.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))); Scala dataStream .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) Python Python 中尚不支持此特性。 WindowAll # DataStream → AllWindowedStream # 可以在普通 DataStream 上定义 Window。 Window 根据某些特征（例如，最近 5 秒内到达的数据）对所有流事件进行分组。请参阅windows获取有关 window 的完整说明。
这适用于非并行转换的大多数场景。所有记录都将收集到 windowAll 算子对应的一个任务中。 Java dataStream .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))); Scala dataStream .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) Python Python 中尚不支持此特性。 Window Apply # WindowedStream → DataStream # AllWindowedStream → DataStream # 将通用 function 应用于整个窗口。下面是一个手动对窗口内元素求和的 function。
如果你使用 windowAll 转换，则需要改用 AllWindowFunction。 Java windowedStream.apply(new WindowFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;, Integer, Tuple, Window\u0026gt;() { public void apply (Tuple tuple, Window window, Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) throws Exception { int sum = 0; for (value t: values) { sum += t.f1; } out.collect (new Integer(sum)); } }); // 在 non-keyed 窗口流上应用 AllWindowFunction allWindowedStream.apply (new AllWindowFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;, Integer, Window\u0026gt;() { public void apply (Window window, Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; values, Collector\u0026lt;Integer\u0026gt; out) throws Exception { int sum = 0; for (value t: values) { sum += t.f1; } out.collect (new Integer(sum)); } }); Scala windowedStream.apply { WindowFunction } // 在 non-keyed 窗口流上应用 AllWindowFunction allWindowedStream.apply { AllWindowFunction } Python Python 中尚不支持此特性。 WindowReduce # WindowedStream → DataStream # 对窗口应用 reduce function 并返回 reduce 后的值。
Java windowedStream.reduce (new ReduceFunction\u0026lt;Tuple2\u0026lt;String,Integer\u0026gt;\u0026gt;() { public Tuple2\u0026lt;String, Integer\u0026gt; reduce(Tuple2\u0026lt;String, Integer\u0026gt; value1, Tuple2\u0026lt;String, Integer\u0026gt; value2) throws Exception { return new Tuple2\u0026lt;String,Integer\u0026gt;(value1.f0, value1.f1 + value2.f1); } }); Scala windowedStream.reduce { _ + _ } Python Python 中尚不支持此特性。 Union # DataStream* → DataStream # 将两个或多个数据流联合来创建一个包含所有流中数据的新流。注意：如果一个数据流和自身进行联合，这个流中的每个数据将在合并后的流中出现两次。
Java dataStream.union(otherStream1, otherStream2, ...); Scala dataStream.union(otherStream1, otherStream2, ...) Python data_stream.union(otherStream1, otherStream2, ...) Window Join # DataStream,DataStream → DataStream # 根据指定的 key 和窗口 join 两个数据流。
Java dataStream.join(otherStream) .where(\u0026lt;key selector\u0026gt;).equalTo(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new JoinFunction () {...}); Scala dataStream.join(otherStream) .where(\u0026lt;key selector\u0026gt;).equalTo(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply { ... } Python Python 中尚不支持此特性。 Interval Join # KeyedStream,KeyedStream → DataStream # 根据 key 相等并且满足指定的时间范围内（e1.timestamp + lowerBound \u0026lt;= e2.timestamp \u0026lt;= e1.timestamp + upperBound）的条件将分别属于两个 keyed stream 的元素 e1 和 e2 Join 在一起。
Java // this will join the two streams so that // key1 == key2 \u0026amp;\u0026amp; leftTs - 2 \u0026lt; rightTs \u0026lt; leftTs + 2 keyedStream.intervalJoin(otherKeyedStream) .between(Time.milliseconds(-2), Time.milliseconds(2)) // lower and upper bound .upperBoundExclusive(true) // optional .lowerBoundExclusive(true) // optional .process(new IntervalJoinFunction() {...}); Scala // this will join the two streams so that // key1 == key2 \u0026amp;\u0026amp; leftTs - 2 \u0026lt; rightTs \u0026lt; leftTs + 2 keyedStream.intervalJoin(otherKeyedStream) .between(Time.milliseconds(-2), Time.milliseconds(2)) // lower and upper bound .upperBoundExclusive(true) // optional .lowerBoundExclusive(true) // optional .process(new IntervalJoinFunction() {...}) Python Python 中尚不支持此特性。 Window CoGroup # DataStream,DataStream → DataStream # 根据指定的 key 和窗口将两个数据流组合在一起。
Java dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply (new CoGroupFunction () {...}); Scala dataStream.coGroup(otherStream) .where(0).equalTo(1) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply {} Python Python 中尚不支持此特性。 Connect # DataStream,DataStream → ConnectedStream # “连接” 两个数据流并保留各自的类型。connect 允许在两个流的处理逻辑之间共享状态。
Java DataStream\u0026lt;Integer\u0026gt; someStream = //... DataStream\u0026lt;String\u0026gt; otherStream = //... ConnectedStreams\u0026lt;Integer, String\u0026gt; connectedStreams = someStream.connect(otherStream); Scala someStream : DataStream[Int] = ... otherStream : DataStream[String] = ... val connectedStreams = someStream.connect(otherStream) Python stream_1 = ... stream_2 = ... connected_streams = stream_1.connect(stream_2) CoMap, CoFlatMap # ConnectedStream → DataStream # 类似于在连接的数据流上进行 map 和 flatMap。
Java connectedStreams.map(new CoMapFunction\u0026lt;Integer, String, Boolean\u0026gt;() { @Override public Boolean map1(Integer value) { return true; } @Override public Boolean map2(String value) { return false; } }); connectedStreams.flatMap(new CoFlatMapFunction\u0026lt;Integer, String, String\u0026gt;() { @Override public void flatMap1(Integer value, Collector\u0026lt;String\u0026gt; out) { out.collect(value.toString()); } @Override public void flatMap2(String value, Collector\u0026lt;String\u0026gt; out) { for (String word: value.split(\u0026#34; \u0026#34;)) { out.collect(word); } } }); Scala connectedStreams.map( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false ) connectedStreams.flatMap( (_ : Int) =\u0026gt; true, (_ : String) =\u0026gt; false ) Python class MyCoMapFunction(CoMapFunction): def map1(self, value): return value[0] + 1, value[1] def map2(self, value): return value[0], value[1] + \u0026#39;flink\u0026#39; class MyCoFlatMapFunction(CoFlatMapFunction): def flat_map1(self, value) for i in range(value[0]): yield i def flat_map2(self, value): yield value[0] + 1 connectedStreams.map(MyCoMapFunction()) connectedStreams.flat_map(MyCoFlatMapFunction()) Iterate # DataStream → IterativeStream → ConnectedStream # 通过将一个算子的输出重定向到某个之前的算子来在流中创建“反馈”循环。这对于定义持续更新模型的算法特别有用。下面的代码从一个流开始，并不断地应用迭代自身。大于 0 的元素被发送回反馈通道，其余元素被转发到下游。
Java IterativeStream\u0026lt;Long\u0026gt; iteration = initialStream.iterate(); DataStream\u0026lt;Long\u0026gt; iterationBody = iteration.map (/*do something*/); DataStream\u0026lt;Long\u0026gt; feedback = iterationBody.filter(new FilterFunction\u0026lt;Long\u0026gt;(){ @Override public boolean filter(Long value) throws Exception { return value \u0026gt; 0; } }); iteration.closeWith(feedback); DataStream\u0026lt;Long\u0026gt; output = iterationBody.filter(new FilterFunction\u0026lt;Long\u0026gt;(){ @Override public boolean filter(Long value) throws Exception { return value \u0026lt;= 0; } }); Scala initialStream.iterate { iteration =\u0026gt; { val iterationBody = iteration.map {/*do something*/} (iterationBody.filter(_ \u0026gt; 0), iterationBody.filter(_ \u0026lt;= 0)) } } Python Python 中尚不支持此特性。 Cache # DataStream → CachedDataStream # 把算子的结果缓存起来。目前只支持批执行模式下运行的作业。算子的结果在算子第一次执行的时候会被缓存起来，之后的 作业中会复用该算子缓存的结果。如果算子的结果丢失了，它会被原来的算子重新计算并缓存。
Java DataStream\u0026lt;Integer\u0026gt; dataStream = //... CachedDataStream\u0026lt;Integer\u0026gt; cachedDataStream = dataStream.cache(); cachedDataStream.print(); // Do anything with the cachedDataStream ... env.execute(); // Execute and create cache. cachedDataStream.print(); // Consume cached result. env.execute(); Scala val dataStream : DataStream[Int] = //... val cachedDataStream = dataStream.cache() cachedDataStream.print() // Do anything with the cachedDataStream ... env.execute() // Execute and create cache. cachedDataStream.print() // Consume cached result. env.execute() Python data_stream = ... # DataStream cached_data_stream = data_stream.cache() cached_data_stream.print() # ... env.execute() # Execute and create cache. cached_data_stream.print() # Consume cached result. env.execute() 物理分区 # Flink 也提供以下方法让用户根据需要在数据转换完成后对数据分区进行更细粒度的配置。
自定义分区 # DataStream → DataStream # 使用用户定义的 Partitioner 为每个元素选择目标任务。
Java dataStream.partitionCustom(partitioner, \u0026#34;someKey\u0026#34;); dataStream.partitionCustom(partitioner, 0); Scala dataStream.partitionCustom(partitioner, \u0026#34;someKey\u0026#34;) dataStream.partitionCustom(partitioner, 0) Python data_stream = env.from_collection(collection=[(2, \u0026#39;a\u0026#39;), (2, \u0026#39;a\u0026#39;), (3, \u0026#39;b\u0026#39;)]) data_stream.partition_custom(lambda key, num_partition: key % partition, lambda x: x[0]) 随机分区 # DataStream → DataStream # 将元素随机地均匀划分到分区。
Java dataStream.shuffle(); Scala dataStream.shuffle() Python data_stream.shuffle() Rescaling # DataStream → DataStream # 将元素以 Round-robin 轮询的方式分发到下游算子。如果你想实现数据管道，这将很有用，例如，想将数据源多个并发实例的数据分发到多个下游 map 来实现负载分配，但又不想像 rebalance() 那样引起完全重新平衡。该算子将只会到本地数据传输而不是网络数据传输，这取决于其它配置值，例如 TaskManager 的 slot 数量。
上游算子将元素发往哪些下游的算子实例集合同时取决于上游和下游算子的并行度。例如，如果上游算子并行度为 2，下游算子的并发度为 6， 那么上游算子的其中一个并行实例将数据分发到下游算子的三个并行实例， 另外一个上游算子的并行实例则将数据分发到下游算子的另外三个并行实例中。再如，当下游算子的并行度为2，而上游算子的并行度为 6 的时候，那么上游算子中的三个并行实例将会分发数据至下游算子的其中一个并行实例，而另外三个上游算子的并行实例则将数据分发至另下游算子的另外一个并行实例。
当算子的并行度不是彼此的倍数时，一个或多个下游算子将从上游算子获取到不同数量的输入。
请参阅下图来可视化地感知上述示例中的连接模式：
Java dataStream.rescale(); Scala dataStream.rescale() Python data_stream.rescale() 广播 # DataStream → DataStream # 将元素广播到每个分区 。
Java dataStream.broadcast(); Scala dataStream.broadcast() Python data_stream.broadcast() 算子链和资源组 # 将两个算子链接在一起能使得它们在同一个线程中执行，从而提升性能。Flink 默认会将能链接的算子尽可能地进行链接(例如， 两个 map 转换操作)。此外， Flink 还提供了对链接更细粒度控制的 API 以满足更多需求：
如果想对整个作业禁用算子链，可以调用 StreamExecutionEnvironment.disableOperatorChaining()。下列方法还提供了更细粒度的控制。需要注意的是，这些方法只能在 DataStream 转换操作后才能被调用，因为它们只对前一次数据转换生效。例如，可以 someStream.map(...).startNewChain() 这样调用，而不能 someStream.startNewChain() 这样。
一个资源组对应着 Flink 中的一个 slot 槽，更多细节请看 slots 。 你可以根据需要手动地将各个算子隔离到不同的 slot 中。
创建新链 # 基于当前算子创建一个新的算子链。
后面两个 map 将被链接起来，而 filter 和第一个 map 不会链接在一起。
Java someStream.filter(...).map(...).startNewChain().map(...); Scala someStream.filter(...).map(...).startNewChain().map(...) Python some_stream.filter(...).map(...).start_new_chain().map(...) 禁止链接 # 禁止和 map 算子链接在一起。
Java someStream.map(...).disableChaining(); Scala someStream.map(...).disableChaining() Python some_stream.map(...).disable_chaining() 配置 Slot 共享组 # 为某个算子设置 slot 共享组。Flink 会将同一个 slot 共享组的算子放在同一个 slot 中，而将不在同一 slot 共享组的算子保留在其它 slot 中。这可用于隔离 slot 。如果所有输入算子都属于同一个 slot 共享组，那么 slot 共享组从将继承输入算子所在的 slot。slot 共享组的默认名称是 “default”，可以调用 slotSharingGroup(“default”) 来显式地将算子放入该组。
Java someStream.filter(...).slotSharingGroup(\u0026#34;name\u0026#34;); Scala someStream.filter(...).slotSharingGroup(\u0026#34;name\u0026#34;) Python some_stream.filter(...).slot_sharing_group(\u0026#34;name\u0026#34;) 名字和描述 # Flink里的算子和作业节点会有一个名字和一个描述。名字和描述。名字和描述都是用来介绍一个算子或者节点是在做什么操作，但是他们会被用在不同地方。
名字会用在用户界面、线程名、日志、指标等场景。节点的名字会根据节点中算子的名字来构建。 名字需要尽可能的简洁，避免对外部系统产生大的压力。
描述主要用在执行计划展示，以及用户界面展示。节点的描述同样是根据节点中算子的描述来构建。 描述可以包括详细的算子行为的信息，以便我们在运行时进行debug分析。
Java someStream.filter(...).setName(\u0026#34;filter\u0026#34;).setDescription(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;); Scala someStream.filter(...).setName(\u0026#34;filter\u0026#34;).setDescription(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;) Python some_stream.filter(...).name(\u0026#34;filter\u0026#34;).set_description(\u0026#34;x in (1, 2, 3, 4) and y \u0026gt; 1\u0026#34;) 节点的描述默认是按照一个多行的树形结构来构建的，用户可以通过把pipeline.vertex-description-mode设为CASCADING, 实现将描述改为老版本的单行递归模式。
Flink SQL框架生成的算子默认会有一个由算子的类型以及id构成的名字，以及一个带有详细信息的描述。 用户可以通过将table.optimizer.simplify-operator-name-enabled设为false，将名字改为和以前的版本一样的详细描述。
当一个作业的拓扑很复杂时，用户可以把pipeline.vertex-name-include-index-prefix设为true，在节点的名字前增加一个拓扑序的前缀，这样就可以很容易根据指标以及日志的信息快速找到拓扑图中对应节点。
`}),e.add({id:18,href:"/flink/flink-docs-master/zh/docs/dev/datastream/overview/",title:"概览",section:"DataStream API",content:` Flink DataStream API 编程指南 # Flink 中的 DataStream 程序是对数据流（例如过滤、更新状态、定义窗口、聚合）进行转换的常规程序。数据流的起始是从各种源（例如消息队列、套接字流、文件）创建的。结果通过 sink 返回，例如可以将数据写入文件或标准输出（例如命令行终端）。Flink 程序可以在各种上下文中运行，可以独立运行，也可以嵌入到其它程序中。任务执行可以运行在本地 JVM 中，也可以运行在多台机器的集群上。
为了创建你自己的 Flink DataStream 程序，我们建议你从 Flink 程序剖析开始，然后逐渐添加自己的 stream transformation。其余部分作为附加的算子和高级特性的参考。
DataStream 是什么? # DataStream API 得名于特殊的 DataStream 类，该类用于表示 Flink 程序中的数据集合。你可以认为 它们是可以包含重复项的不可变数据集合。这些数据可以是有界（有限）的，也可以是无界（无限）的，但用于处理它们的API是相同的。
DataStream 在用法上类似于常规的 Java 集合，但在某些关键方面却大不相同。它们是不可变的，这意味着一旦它们被创建，你就不能添加或删除元素。你也不能简单地察看内部元素，而只能使用 DataStream API 操作来处理它们，DataStream API 操作也叫作转换（transformation）。
你可以通过在 Flink 程序中添加 source 创建一个初始的 DataStream。然后，你可以基于 DataStream 派生新的流，并使用 map、filter 等 API 方法把 DataStream 和派生的流连接在一起。
Flink 程序剖析 # Flink 程序看起来像一个转换 DataStream 的常规程序。每个程序由相同的基本部分组成：
获取一个执行环境（execution environment）； 加载/创建初始数据； 指定数据相关的转换； 指定计算结果的存储位置； 触发程序执行。 Java 现在我们将对这些步骤逐一进行概述，更多细节请参考相关章节。请注意，Java DataStream API 的所有核心类都可以在 org.apache.flink.streaming.api 中找到。
StreamExecutionEnvironment 是所有 Flink 程序的基础。你可以使用 StreamExecutionEnvironment 的如下静态方法获取 StreamExecutionEnvironment：
getExecutionEnvironment(); createLocalEnvironment(); createRemoteEnvironment(String host, int port, String... jarFiles); 通常，你只需要使用 getExecutionEnvironment() 即可，因为该方法会根据上下文做正确的处理：如果你在 IDE 中执行你的程序或将其作为一般的 Java 程序执行，那么它将创建一个本地环境，该环境将在你的本地机器上执行你的程序。如果你基于程序创建了一个 JAR 文件，并通过命令行运行它，Flink 集群管理器将执行程序的 main 方法，同时 getExecutionEnvironment() 方法会返回一个执行环境以在集群上执行你的程序。
为了指定 data sources，执行环境提供了一些方法，支持使用各种方法从文件中读取数据：你可以直接逐行读取数据，像读 CSV 文件一样，或使用任何第三方提供的 source。如果你只是将一个文本文件作为一个行的序列来读取，那么可以使用：
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; text = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;); 这将生成一个 DataStream，然后你可以在上面应用转换（transformation）来创建新的派生 DataStream。
你可以调用 DataStream 上具有转换功能的方法来应用转换。例如，一个 map 的转换如下所示：
DataStream\u0026lt;String\u0026gt; input = ...; DataStream\u0026lt;Integer\u0026gt; parsed = input.map(new MapFunction\u0026lt;String, Integer\u0026gt;() { @Override public Integer map(String value) { return Integer.parseInt(value); } }); 这将通过把原始集合中的每一个字符串转换为一个整数来创建一个新的 DataStream。
一旦你有了包含最终结果的 DataStream，你就可以通过创建 sink 把它写到外部系统。下面是一些用于创建 sink 的示例方法：
writeAsText(String path); print(); Scala 现在我们将对这些步骤逐一进行概述，更多细节请参考相关章节。请注意，Java DataStream API 的所有核心类都可以在 org.apache.flink.streaming.api.scala 中找到。
StreamExecutionEnvironment 是所有 Flink 程序的基础。你可以使用 StreamExecutionEnvironment 的如下静态方法获取 StreamExecutionEnvironment：
getExecutionEnvironment() createLocalEnvironment() createRemoteEnvironment(host: String, port: Int, jarFiles: String*) 通常，你只需要使用 getExecutionEnvironment() 即可，因为该方法会根据上下文做正确的处理：如果在 IDE 中执行你的程序或作为常规 Java 程序，它将创建一个本地环境，该环境将在你的本地机器上执行你的程序。如果你基于程序创建了一个 JAR 文件，并通过命令行调用它，Flink 集群管理器将执行程序的 main 方法，同时 getExecutionEnvironment() 方法会返回一个执行环境以在集群上执行你的程序。
为了指定 data sources，执行环境提供了一些方法，支持使用各种方法从文件中读取数据：你可以直接逐行读取数据，像读 CSV 文件一样，或使用任何第三方提供的 source。如果只是将一个文本文件作为一个行的序列来读，你可以使用：
val env = StreamExecutionEnvironment.getExecutionEnvironment() val text: DataStream[String] = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;) 这将为你生成一个 DataStream，然后你可以在上面应用转换来创建新的派生 DataStream。
你可以调用 DataStream 上具有转换功能的方法来应用转换。例如，一个 map 的转换如下所示：
val input: DataSet[String] = ... val mapped = input.map { x =\u0026gt; x.toInt } 这将通过把原始集合中的每一个字符串转换为一个整数来创建一个新的 DataStream。
一旦你有了包含最终结果的 DataStream，你就可以通过创建 sink 把它写到外部系统。下面是一些用于创建 sink 的示例方法：
writeAsText(path: String) print() 一旦指定了完整的程序，需要调用 StreamExecutionEnvironment 的 execute() 方法来触发程序执行。根据 ExecutionEnvironment 的类型，执行会在你的本地机器上触发，或将你的程序提交到某个集群上执行。
execute() 方法将等待作业完成，然后返回一个 JobExecutionResult，其中包含执行时间和累加器结果。
如果不想等待作业完成，可以通过调用 StreamExecutionEnvironment 的 executeAsync() 方法来触发作业异步执行。它会返回一个 JobClient，你可以通过它与刚刚提交的作业进行通信。如下是使用 executeAsync() 实现 execute() 语义的示例。
final JobClient jobClient = env.executeAsync(); final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult().get(); 关于程序执行的最后一部分对于理解何时以及如何执行 Flink 算子是至关重要的。所有 Flink 程序都是延迟执行的：当程序的 main 方法被执行时，数据加载和转换不会直接发生。相反，每个算子都被创建并添加到 dataflow 形成的有向图。当执行被执行环境的 execute() 方法显示地触发时，这些算子才会真正执行。程序是在本地执行还是在集群上执行取决于执行环境的类型。
延迟计算允许你构建复杂的程序，Flink 会将其作为一个整体的计划单元来执行。
Back to top
示例程序 # 如下是一个完整的、可运行的程序示例，它是基于流窗口的单词统计应用程序，计算 5 秒窗口内来自 Web 套接字的单词数。你可以复制并粘贴代码以在本地运行。
Java import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class WindowWordCount { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dataStream = env .socketTextStream(\u0026#34;localhost\u0026#34;, 9999) .flatMap(new Splitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); dataStream.print(); env.execute(\u0026#34;Window WordCount\u0026#34;); } public static class Splitter implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String sentence, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) throws Exception { for (String word: sentence.split(\u0026#34; \u0026#34;)) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(word, 1)); } } } } Scala import org.apache.flink.streaming.api.scala._ import org.apache.flink.streaming.api.windowing.time.Time object WindowWordCount { def main(args: Array[String]) { val env = StreamExecutionEnvironment.getExecutionEnvironment val text = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .keyBy(_._1) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1) counts.print() env.execute(\u0026#34;Window Stream WordCount\u0026#34;) } } 要运行示例程序，首先从终端使用 netcat 启动输入流：
nc -lk 9999 只需输入一些单词，然后按回车键即可传入新单词。这些将作为单词统计程序的输入。如果想查看大于 1 的计数，在 5 秒内重复输入相同的单词即可（如果无法快速输入，则可以将窗口大小从 5 秒增加 ☺）。
Back to top
Data Sources # Java Source 是你的程序从中读取其输入的地方。你可以用 StreamExecutionEnvironment.addSource(sourceFunction) 将一个 source 关联到你的程序。Flink 自带了许多预先实现的 source functions，不过你仍然可以通过实现 SourceFunction 接口编写自定义的非并行 source，也可以通过实现 ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 类编写自定义的并行 sources。
通过 StreamExecutionEnvironment 可以访问多种预定义的 stream source：
基于文件：
readTextFile(path) - 读取文本文件，例如遵守 TextInputFormat 规范的文件，逐行读取并将它们作为字符串返回。
readFile(fileInputFormat, path) - 按照指定的文件输入格式读取（一次）文件。
readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是前两个方法内部调用的方法。它基于给定的 fileInputFormat 读取路径 path 上的文件。根据提供的 watchType 的不同，source 可能定期（每 interval 毫秒）监控路径上的新数据（watchType 为 FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次当前路径中的数据然后退出（watchType 为 FileProcessingMode.PROCESS_ONCE)。使用 pathFilter，用户可以进一步排除正在处理的文件。
实现：
在底层，Flink 将文件读取过程拆分为两个子任务，即 目录监控 和 数据读取。每个子任务都由一个单独的实体实现。监控由单个非并行（并行度 = 1）任务实现，而读取由多个并行运行的任务执行。后者的并行度和作业的并行度相等。单个监控任务的作用是扫描目录（定期或仅扫描一次，取决于 watchType），找到要处理的文件，将它们划分为 分片，并将这些分片分配给下游 reader。Reader 是将实际获取数据的角色。每个分片只能被一个 reader 读取，而一个 reader 可以一个一个地读取多个分片。
重要提示：
如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，当一个文件被修改时，它的内容会被完全重新处理。这可能会打破 \u0026ldquo;精确一次\u0026rdquo; 的语义，因为在文件末尾追加数据将导致重新处理文件的所有内容。
如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，source 扫描一次路径然后退出，无需等待 reader 读完文件内容。当然，reader 会继续读取数据，直到所有文件内容都读完。关闭 source 会导致在那之后不再有检查点。这可能会导致节点故障后恢复速度变慢，因为作业将从最后一个检查点恢复读取。
基于套接字：
socketTextStream - 从套接字读取。元素可以由分隔符分隔。 基于集合：
fromCollection(Collection) - 从 Java Java.util.Collection 创建数据流。集合中的所有元素必须属于同一类型。
fromCollection(Iterator, Class) - 从迭代器创建数据流。class 参数指定迭代器返回元素的数据类型。
fromElements(T ...) - 从给定的对象序列中创建数据流。所有的对象必须属于同一类型。
fromParallelCollection(SplittableIterator, Class) - 从迭代器并行创建数据流。class 参数指定迭代器返回元素的数据类型。
generateSequence(from, to) - 基于给定间隔内的数字序列并行生成数据流。
自定义：
addSource - 关联一个新的 source function。例如，你可以使用 addSource(new FlinkKafkaConsumer\u0026lt;\u0026gt;(...)) 来从 Apache Kafka 获取数据。更多详细信息见连接器。 Scala Source 是你的程序从中读取其输入的地方。你可以用 StreamExecutionEnvironment.addSource(sourceFunction) 将一个 source 关联到你的程序。Flink 自带了许多预先实现的 source functions，不过你仍然可以通过实现 SourceFunction 接口编写自定义的非并行 source，也可以通过实现 ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 类编写自定义的并行 sources。 通过 StreamExecutionEnvironment 可以访问多种预定义的 stream source：
基于文件：
readTextFile(path) - 读取文本文件，例如遵守 TextInputFormat 规范的文件，逐行读取并将它们作为字符串返回。
readFile(fileInputFormat, path) - 按照指定的文件输入格式读取（一次）文件。
readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是前两个方法内部调用的方法。它基于给定的 fileInputFormat 读取路径 path 上的文件。根据提供的 watchType 的不同，source 可能定期（每 interval 毫秒）监控路径上的新数据（watchType 为 FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次当前路径中的数据然后退出（watchType 为 FileProcessingMode.PROCESS_ONCE)。使用 pathFilter，用户可以进一步排除正在处理的文件。
实现：
在底层，Flink 将文件读取过程拆分为两个子任务，即 目录监控 和 数据读取。每个子任务都由一个单独的实体实现。监控由单个非并行（并行度 = 1）任务实现，而读取由多个并行运行的任务执行。后者的并行度和作业的并行度相等。单个监控任务的作用是扫描目录（定期或仅扫描一次，取决于 watchType），找到要处理的文件，将它们划分为 分片，并将这些分片分配给下游 reader。Reader 是将实际获取数据的角色。每个分片只能被一个 reader 读取，而一个 reader 可以一个一个地读取多个分片。
重要提示：
如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，当一个文件被修改时，它的内容会被完全重新处理。这可能会打破 “精确一次” 的语义，因为在文件末尾追加数据将导致重新处理文件的所有内容。
如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，source 扫描一次路径然后退出，无需等待 reader 读完文件内容。当然，reader 会继续读取数据，直到所有文件内容都读完。关闭 source 会导致在那之后不再有检查点。这可能会导致节点故障后恢复速度变慢，因为作业将从最后一个检查点恢复读取。
基于套接字：
socketTextStream - 从套接字读取。元素可以由分隔符分隔。 基于集合：
fromCollection(Collection) - 从 Java Java.util.Collection 创建数据流。集合中的所有元素必须属于同一类型。
fromCollection(Iterator, Class) - 从迭代器创建数据流。class 参数指定迭代器返回元素的数据类型。
fromElements(T ...) - 从给定的对象序列中创建数据流。所有的对象必须属于同一类型。
fromParallelCollection(SplittableIterator, Class) - 从迭代器并行创建数据流。class 参数指定迭代器返回元素的数据类型。
generateSequence(from, to) - 基于给定间隔内的数字序列并行生成数据流。
自定义：
addSource - 关联一个新的 source function。例如，你可以使用 addSource(new FlinkKafkaConsumer\u0026lt;\u0026gt;(...)) 来从 Apache Kafka 获取数据。更多详细信息见连接器。 Back to top
DataStream Transformations # 有关可用 stream 转换（transformation）的概述，请参阅算子。
Back to top
Data Sinks # Java Data sinks 使用 DataStream 并将它们转发到文件、套接字、外部系统或打印它们。Flink 自带了多种内置的输出格式，这些格式相关的实现封装在 DataStreams 的算子里：
writeAsText() / TextOutputFormat - 将元素按行写成字符串。通过调用每个元素的 toString() 方法获得字符串。
writeAsCsv(...) / CsvOutputFormat - 将元组写成逗号分隔值文件。行和字段的分隔符是可配置的。每个字段的值来自对象的 toString() 方法。
print() / printToErr() - 在标准输出/标准错误流上打印每个元素的 toString() 值。 可选地，可以提供一个前缀（msg）附加到输出。这有助于区分不同的 print 调用。如果并行度大于1，输出结果将附带输出任务标识符的前缀。
writeUsingOutputFormat() / FileOutputFormat - 自定义文件输出的方法和基类。支持自定义 object 到 byte 的转换。
writeToSocket - 根据 SerializationSchema 将元素写入套接字。
addSink - 调用自定义 sink function。Flink 捆绑了连接到其他系统（例如 Apache Kafka）的连接器，这些连接器被实现为 sink functions。
Scala Data sinks 使用 DataStream 并将它们转发到文件、套接字、外部系统或打印它们。Flink 自带了多种内置的输出格式，这些格式相关的实现封装在 DataStreams 的算子里：
writeAsText() / TextOutputFormat - 将元素按行写成字符串。通过调用每个元素的 toString() 方法获得字符串。
writeAsCsv(...) / CsvOutputFormat - 将元组写成逗号分隔值文件。行和字段的分隔符是可配置的。每个字段的值来自对象的 toString() 方法。
print() / printToErr() - 在标准输出/标准错误流上打印每个元素的 toString() 值。 可选地，可以提供一个前缀（msg）附加到输出。这有助于区分不同的 print 调用。如果并行度大于1，输出结果将附带输出任务标识符的前缀。
writeUsingOutputFormat() / FileOutputFormat - 自定义文件输出的方法和基类。支持自定义 object 到 byte 的转换。
writeToSocket - 根据 SerializationSchema 将元素写入套接字。
addSink - 调用自定义 sink function。Flink 捆绑了连接到其他系统（例如 Apache Kafka）的连接器，这些连接器被实现为 sink functions。
注意，DataStream 的 write*() 方法主要用于调试目的。它们不参与 Flink 的 checkpointing，这意味着这些函数通常具有至少有一次语义。刷新到目标系统的数据取决于 OutputFormat 的实现。这意味着并非所有发送到 OutputFormat 的元素都会立即显示在目标系统中。此外，在失败的情况下，这些记录可能会丢失。
为了将流可靠地、精准一次地传输到文件系统中，请使用 FileSink。此外，通过 .addSink(...) 方法调用的自定义实现也可以参与 Flink 的 checkpointing，以实现精准一次的语义。
Back to top
Iterations # Java Iterative streaming 程序实现了 setp function 并将其嵌入到 IterativeStream 。由于 DataStream 程序可能永远不会完成，因此没有最大迭代次数。相反，你需要指定流的哪一部分反馈给迭代，哪一部分使用旁路输出或过滤器转发到下游。这里，我们展示了一个使用过滤器的示例。首先，我们定义一个 IterativeStream
IterativeStream\u0026lt;Integer\u0026gt; iteration = input.iterate(); 然后，我们使用一系列转换（这里是一个简单的 map 转换）指定将在循环内执行的逻辑
DataStream\u0026lt;Integer\u0026gt; iterationBody = iteration.map(/* this is executed many times */); 要关闭迭代并定义迭代尾部，请调用 IterativeStream 的 closeWith(feedbackStream) 方法。提供给 closeWith 函数的 DataStream 将反馈给迭代头。一种常见的模式是使用过滤器将反馈的流部分和向前传播的流部分分开。例如，这些过滤器可以定义“终止”逻辑，其中允许元素向下游传播而不是被反馈。
iteration.closeWith(iterationBody.filter(/* one part of the stream */)); DataStream\u0026lt;Integer\u0026gt; output = iterationBody.filter(/* some other part of the stream */); 例如，下面的程序从一系列整数中连续减去 1，直到它们达到零：
DataStream\u0026lt;Long\u0026gt; someIntegers = env.generateSequence(0, 1000); IterativeStream\u0026lt;Long\u0026gt; iteration = someIntegers.iterate(); DataStream\u0026lt;Long\u0026gt; minusOne = iteration.map(new MapFunction\u0026lt;Long, Long\u0026gt;() { @Override public Long map(Long value) throws Exception { return value - 1 ; } }); DataStream\u0026lt;Long\u0026gt; stillGreaterThanZero = minusOne.filter(new FilterFunction\u0026lt;Long\u0026gt;() { @Override public boolean filter(Long value) throws Exception { return (value \u0026gt; 0); } }); iteration.closeWith(stillGreaterThanZero); DataStream\u0026lt;Long\u0026gt; lessThanZero = minusOne.filter(new FilterFunction\u0026lt;Long\u0026gt;() { @Override public boolean filter(Long value) throws Exception { return (value \u0026lt;= 0); } }); Scala Iterative streaming 程序实现了 setp function 并将其嵌入到 IterativeStream 。由于 DataStream 程序可能永远不会完成，因此没有最大迭代次数。相反，你需要指定流的哪一部分反馈给迭代，哪一部分使用旁路输出或过滤器转发到下游。这里，我们展示了一个迭代示例，其中主体（重复计算的部分）是一个简单的映射转换，使用过滤器将反馈的元素和向下游转发的元素进行分离。
val iteratedStream = someDataStream.iterate( iteration =\u0026gt; { val iterationBody = iteration.map(/* this is executed many times */) (iterationBody.filter(/* one part of the stream */), iterationBody.filter(/* some other part of the stream */)) }) 例如，下面的程序从一系列整数中连续减去 1，直到它们达到零：
val someIntegers: DataStream[Long] = env.generateSequence(0, 1000) val iteratedStream = someIntegers.iterate( iteration =\u0026gt; { val minusOne = iteration.map( v =\u0026gt; v - 1) val stillGreaterThanZero = minusOne.filter (_ \u0026gt; 0) val lessThanZero = minusOne.filter(_ \u0026lt;= 0) (stillGreaterThanZero, lessThanZero) } ) Back to top
执行参数 # StreamExecutionEnvironment 包含了 ExecutionConfig，它允许在运行时设置作业特定的配置值。
大多数参数的说明可参考执行配置。这些参数特别适用于 DataStream API：
setAutoWatermarkInterval(long milliseconds)：设置自动发送 watermark 的时间间隔。你可以使用 long getAutoWatermarkInterval() 获取当前配置值。 Back to top
容错 # State \u0026amp; Checkpointing 描述了如何启用和配置 Flink 的 checkpointing 机制。
控制延迟 # 默认情况下，元素不会在网络上一一传输（这会导致不必要的网络传输），而是被缓冲。缓冲区的大小（实际在机器之间传输）可以在 Flink 配置文件中设置。虽然此方法有利于优化吞吐量，但当输入流不够快时，它可能会导致延迟问题。要控制吞吐量和延迟，你可以调用执行环境（或单个算子）的 env.setBufferTimeout(timeoutMillis) 方法来设置缓冲区填满的最长等待时间。超过此时间后，即使缓冲区没有未满，也会被自动发送。超时时间的默认值为 100 毫秒。
用法：
Java LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); env.setBufferTimeout(timeoutMillis); env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis); Scala val env: LocalStreamEnvironment = StreamExecutionEnvironment.createLocalEnvironment env.setBufferTimeout(timeoutMillis) env.generateSequence(1,10).map(myMap).setBufferTimeout(timeoutMillis) 为了最大限度地提高吞吐量，设置 setBufferTimeout(-1) 来删除超时，这样缓冲区仅在它们已满时才会被刷新。要最小化延迟，请将超时设置为接近 0 的值（例如 5 或 10 毫秒）。应避免超时为 0 的缓冲区，因为它会导致严重的性能下降。
Back to top
调试 # 在分布式集群中运行流程序之前，最好确保实现的算法能按预期工作。因此，实现数据分析程序通常是一个检查结果、调试和改进的增量过程。
Flink 通过提供 IDE 内本地调试、注入测试数据和收集结果数据的特性大大简化了数据分析程序的开发过程。本节给出了一些如何简化 Flink 程序开发的提示。
本地执行环境 # LocalStreamEnvironment 在创建它的同一个 JVM 进程中启动 Flink 系统。如果你从 IDE 启动 LocalEnvironment，则可以在代码中设置断点并轻松调试程序。
一个 LocalEnvironment 的创建和使用如下：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); DataStream\u0026lt;String\u0026gt; lines = env.addSource(/* some source */); // 构建你的程序 env.execute(); Scala val env = StreamExecutionEnvironment.createLocalEnvironment() val lines = env.addSource(/* some source */) // 构建你的程序 env.execute() 集合 Data Sources # Flink 提供了由 Java 集合支持的特殊 data sources 以简化测试。一旦程序通过测试，sources 和 sinks 可以很容易地被从外部系统读取/写入到外部系统的 sources 和 sinks 替换。
可以按如下方式使用集合 Data Sources：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); // 从元素列表创建一个 DataStream DataStream\u0026lt;Integer\u0026gt; myInts = env.fromElements(1, 2, 3, 4, 5); // 从任何 Java 集合创建一个 DataStream List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; data = ... DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myTuples = env.fromCollection(data); // 从迭代器创建一个 DataStream Iterator\u0026lt;Long\u0026gt; longIt = ... DataStream\u0026lt;Long\u0026gt; myLongs = env.fromCollection(longIt, Long.class); Scala val env = StreamExecutionEnvironment.createLocalEnvironment() // 从元素列表创建一个 DataStream val myInts = env.fromElements(1, 2, 3, 4, 5) // 从任何 Java 集合创建一个 DataStream val data: Seq[(String, Int)] = ... val myTuples = env.fromCollection(data) // 从迭代器创建一个 DataStream val longIt: Iterator[Long] = ... val myLongs = env.fromCollection(longIt) 注意： 目前，集合 data source 要求数据类型和迭代器实现 Serializable。此外，集合 data sources 不能并行执行（parallelism = 1）。
迭代器 Data Sink # Flink 还提供了一个 sink 来收集 DataStream 的结果，它用于测试和调试目的。可以按以下方式使用。
Java DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myResult = ... Iterator\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; myOutput = myResult.collectAsync(); Scala val myResult: DataStream[(String, Int)] = ... val myOutput: Iterator[(String, Int)] = myResult.collectAsync() 接下来？ # 算子：可用算子的使用指南。 Event Time：Flink 中时间概念的介绍。 状态 \u0026amp; 容错：如何开发有状态应用程序的讲解。 连接器：所有可用输入和输出连接器的描述。 Back to top
`}),e.add({id:19,href:"/flink/flink-docs-master/zh/docs/dev/python/overview/",title:"概览",section:"Python API",content:` Python API # PyFlink 是 Apache Flink 的 Python API，你可以使用它构建可扩展的批处理和流处理任务，例如实时数据处理管道、大规模探索性数据分析、机器学习（ML）管道和 ETL 处理。 如果你对 Python 和 Pandas 等库已经比较熟悉，那么 PyFlink 可以让你更轻松地利用 Flink 生态系统的全部功能。 根据你需要的抽象级别的不同，有两种不同的 API 可以在 PyFlink 中使用：
PyFlink Table API 允许你使用类似于 SQL 或者在 Python 中处理表格数据的方式编写强大的关系查询。 与此同时，PyFlink DataStream API 允许你对 Flink 的核心组件 state 和 time 进行细粒度的控制，以便构建更复杂的流处理应用。 尝试 PyFlink # 如果你有兴趣使用 PyFlink，可以尝试以下教程：
PyFlink DataStream API 介绍 PyFlink Table API 介绍 如果你想了解更多关于 PyFlink 的示例，可以参考 PyFlink 示例 深入 PyFlink # 这些参考文档涵盖了 PyFlink 的所有细节，可以从以下链接入手：
PyFlink DataStream API PyFlink Table API \u0026amp; SQL 获取有关 PyFlink 的帮助 # 如果你遇到困难，请查看我们的社区支持资源，特别是 Apache Flink 的用户邮件列表，Apache Flink 的用户邮件列表一直是所有 Apache 项目中最活跃的项目邮件列表之一，是快速获得帮助的好方法。
`}),e.add({id:20,href:"/flink/flink-docs-master/zh/docs/dev/python/table/udfs/overview/",title:"概览",section:"自定义函数",content:` User-defined Functions # PyFlink Table API empowers users to do data transformations with Python user-defined functions.
Currently, it supports two kinds of Python user-defined functions: the general Python user-defined functions which process data one row at a time and vectorized Python user-defined functions which process data one batch at a time.
打包 UDFs # 如果你在非 local 模式下运行 Python UDFs 和 Pandas UDFs，且 Python UDFs 没有定义在含 main() 入口的 Python 主文件中，强烈建议你通过 python-files 配置项指定 Python UDF 的定义。 否则，如果你将 Python UDFs 定义在名为 my_udf.py 的文件中，你可能会遇到 ModuleNotFoundError: No module named 'my_udf' 这样的报错。
在 UDF 中载入资源 # 有时候，我们想在 UDF 中只载入一次资源，然后反复使用该资源进行计算。例如，你想在 UDF 中首先载入一个巨大的深度学习模型，然后使用该模型多次进行预测。
你要做的是重载 UserDefinedFunction 类的 open 方法。
class Predict(ScalarFunction): def open(self, function_context): import pickle with open(\u0026#34;resources.zip/resources/model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: self.model = pickle.load(f) def eval(self, x): return self.model.predict(x) predict = udf(Predict(), result_type=DataTypes.DOUBLE(), func_type=\u0026#34;pandas\u0026#34;) 测试自定义函数 # 假如你定义了如下 Python 自定义函数：
add = udf(lambda i, j: i + j, result_type=DataTypes.BIGINT()) 如果要对它进行单元测试，首先需要通过 ._func 从 UDF 对象中抽取原来的 Python 函数，然后才能测试：
f = add._func assert f(1, 2) == 3 `}),e.add({id:21,href:"/flink/flink-docs-master/zh/docs/dev/table/overview/",title:"概览",section:"Table API \u0026 SQL",content:` Table API \u0026amp; SQL # Apache Flink 有两种关系型 API 来做流批统一处理：Table API 和 SQL。Table API 是用于 Scala 和 Java 语言的查询API，它可以用一种非常直观的方式来组合使用选取、过滤、join 等关系型算子。Flink SQL 是基于 Apache Calcite 来实现的标准 SQL。无论输入是连续的（流式）还是有界的（批处理），在两个接口中指定的查询都具有相同的语义，并指定相同的结果。
Table API 和 SQL 两种 API 是紧密集成的，以及 DataStream API。你可以在这些 API 之间，以及一些基于这些 API 的库之间轻松的切换。比如，你可以先用 CEP 从 DataStream 中做模式匹配，然后用 Table API 来分析匹配的结果；或者你可以用 SQL 来扫描、过滤、聚合一个批式的表，然后再跑一个 Gelly 图算法 来处理已经预处理好的数据。
Table 程序依赖 # 您需要将 Table API 作为依赖项添加到项目中，以便用 Table API 和 SQL 定义数据管道。
有关如何为 Java 和 Scala 配置这些依赖项的更多细节，请查阅项目配置小节。
如果您使用 Python，请查阅 Python API 文档。
接下来？ # 公共概念和 API: Table API 和 SQL 公共概念以及 API。 数据类型: 内置数据类型以及它们的属性 流式概念: Table API 和 SQL 中流式相关的文档，比如配置时间属性和如何处理更新结果。 连接外部系统: 读写外部系统的连接器和格式。 Table API: Table API 支持的操作。 SQL: SQL 支持的操作和语法。 内置函数: Table API 和 SQL 中的内置函数。 SQL Client: 不用编写代码就可以尝试 Flink SQL，可以直接提交 SQL 任务到集群上。 Back to top
`}),e.add({id:22,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/overview/",title:"概览",section:"SQL",content:` SQL # 本页面描述了 Flink 所支持的 SQL 语言，包括数据定义语言（Data Definition Language，DDL）、数据操纵语言（Data Manipulation Language，DML）以及查询语言。Flink 对 SQL 的支持基于实现了 SQL 标准的 Apache Calcite。
本页面列出了目前 Flink SQL 所支持的所有语句：
SELECT (Queries) CREATE TABLE, CATALOG, DATABASE, VIEW, FUNCTION DROP TABLE, DATABASE, VIEW, FUNCTION ALTER TABLE, DATABASE, FUNCTION ANALYZE TABLE INSERT SQL HINTS DESCRIBE EXPLAIN USE SHOW LOAD UNLOAD 数据类型 # 请参考专门描述该主题的页面 数据类型。
通用类型与（嵌套的）复合类型 （如：POJO、tuples、rows、Scala case 类) 都可以作为行的字段。
复合类型的字段任意的嵌套可被 值访问函数 访问。
通用类型将会被视为一个黑箱，且可以被 用户自定义函数 传递或引用。
对于 DDL 语句而言，我们支持所有在 数据类型 页面中定义的数据类型。
注意： SQL查询不支持部分数据类型（cast 表达式或字符常量值）。如：STRING, BYTES, RAW, TIME(p) WITHOUT TIME ZONE, TIME(p) WITH LOCAL TIME ZONE, TIMESTAMP(p) WITHOUT TIME ZONE, TIMESTAMP(p) WITH LOCAL TIME ZONE, ARRAY, MULTISET, ROW.
Back to top
保留关键字 # 虽然 SQL 的特性并未完全实现，但是一些字符串的组合却已经被预留为关键字以备未来使用。如果你希望使用以下字符串作为你的字段名，请在使用时使用反引号将该字段名包起来（如 \`value\`, \`count\` ）。
A, ABS, ABSOLUTE, ACTION, ADA, ADD, ADMIN, AFTER, ALL, ALLOCATE, ALLOW, ALTER, ALWAYS, AND, ANALYZE, ANY, ARE, ARRAY, AS, ASC, ASENSITIVE, ASSERTION, ASSIGNMENT, ASYMMETRIC, AT, ATOMIC, ATTRIBUTE, ATTRIBUTES, AUTHORIZATION, AVG, BEFORE, BEGIN, BERNOULLI, BETWEEN, BIGINT, BINARY, BIT, BLOB, BOOLEAN, BOTH, BREADTH, BY, BYTES, C, CALL, CALLED, CARDINALITY, CASCADE, CASCADED, CASE, CAST, CATALOG, CATALOG_NAME, CEIL, CEILING, CENTURY, CHAIN, CHAR, CHARACTER, CHARACTERISTICS, CHARACTERS, CHARACTER_LENGTH, CHARACTER_SET_CATALOG, CHARACTER_SET_NAME, CHARACTER_SET_SCHEMA, CHAR_LENGTH, CHECK, CLASS_ORIGIN, CLOB, CLOSE, COALESCE, COBOL, COLLATE, COLLATION, COLLATION_CATALOG, COLLATION_NAME, COLLATION_SCHEMA, COLLECT, COLUMN, COLUMNS, COLUMN_NAME, COMMAND_FUNCTION, COMMAND_FUNCTION_CODE, COMMIT, COMMITTED, CONDITION, CONDITION_NUMBER, CONNECT, CONNECTION, CONNECTION_NAME, CONSTRAINT, CONSTRAINTS, CONSTRAINT_CATALOG, CONSTRAINT_NAME, CONSTRAINT_SCHEMA, CONSTRUCTOR, CONTAINS, CONTINUE, CONVERT, CORR, CORRESPONDING, COUNT, COVAR_POP, COVAR_SAMP, CREATE, CROSS, CUBE, CUME_DIST, CURRENT, CURRENT_CATALOG, CURRENT_DATE, CURRENT_DEFAULT_TRANSFORM_GROUP, CURRENT_PATH, CURRENT_ROLE, CURRENT_SCHEMA, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_TRANSFORM_GROUP_FOR_TYPE, CURRENT_USER, CURSOR, CURSOR_NAME, CYCLE, DATA, DATABASE, DATE, DATETIME_INTERVAL_CODE, DATETIME_INTERVAL_PRECISION, DAY, DEALLOCATE, DEC, DECADE, DECIMAL, DECLARE, DEFAULT, DEFAULTS, DEFERRABLE, DEFERRED, DEFINED, DEFINER, DEGREE, DELETE, DENSE_RANK, DEPTH, DEREF, DERIVED, DESC, DESCRIBE, DESCRIPTION, DESCRIPTOR, DETERMINISTIC, DIAGNOSTICS, DISALLOW, DISCONNECT, DISPATCH, DISTINCT, DOMAIN, DOUBLE, DOW, DOY, DROP, DYNAMIC, DYNAMIC_FUNCTION, DYNAMIC_FUNCTION_CODE, EACH, ELEMENT, ELSE, END, END-EXEC, EPOCH, EQUALS, ESCAPE, EVERY, EXCEPT, EXCEPTION, EXCLUDE, EXCLUDING, EXEC, EXECUTE, EXISTS, EXP, EXPLAIN, EXTEND, EXTERNAL, EXTRACT, FALSE, FETCH, FILTER, FINAL, FIRST, FIRST_VALUE, FLOAT, FLOOR, FOLLOWING, FOR, FOREIGN, FORTRAN, FOUND, FRAC_SECOND, FREE, FROM, FULL, FUNCTION, FUSION, G, GENERAL, GENERATED, GET, GLOBAL, GO, GOTO, GRANT, GRANTED, GROUP, GROUPING, HAVING, HIERARCHY, HOLD, HOUR, IDENTITY, IMMEDIATE, IMPLEMENTATION, IMPORT, IN, INCLUDING, INCREMENT, INDICATOR, INITIALLY, INNER, INOUT, INPUT, INSENSITIVE, INSERT, INSTANCE, INSTANTIABLE, INT, INTEGER, INTERSECT, INTERSECTION, INTERVAL, INTO, INVOKER, IS, ISOLATION, JAVA, JOIN, K, KEY, KEY_MEMBER, KEY_TYPE, LABEL, LANGUAGE, LARGE, LAST, LAST_VALUE, LATERAL, LEADING, LEFT, LENGTH, LEVEL, LIBRARY, LIKE, LIMIT, LN, LOCAL, LOCALTIME, LOCALTIMESTAMP, LOCATOR, LOWER, M, MAP, MATCH, MATCHED, MAX, MAXVALUE, MEMBER, MERGE, MESSAGE_LENGTH, MESSAGE_OCTET_LENGTH, MESSAGE_TEXT, METHOD, MICROSECOND, MILLENNIUM, MIN, MINUTE, MINVALUE, MOD, MODIFIES, MODULE, MODULES, MONTH, MORE, MULTISET, MUMPS, NAME, NAMES, NATIONAL, NATURAL, NCHAR, NCLOB, NESTING, NEW, NEXT, NO, NONE, NORMALIZE, NORMALIZED, NOT, NULL, NULLABLE, NULLIF, NULLS, NUMBER, NUMERIC, OBJECT, OCTETS, OCTET_LENGTH, OF, OFFSET, OLD, ON, ONLY, OPEN, OPTION, OPTIONS, OR, ORDER, ORDERING, ORDINALITY, OTHERS, OUT, OUTER, OUTPUT, OVER, OVERLAPS, OVERLAY, OVERRIDING, PAD, PARAMETER, PARAMETER_MODE, PARAMETER_NAME, PARAMETER_ORDINAL_POSITION, PARAMETER_SPECIFIC_CATALOG, PARAMETER_SPECIFIC_NAME, PARAMETER_SPECIFIC_SCHEMA, PARTIAL, PARTITION, PASCAL, PASSTHROUGH, PATH, PERCENTILE_CONT, PERCENTILE_DISC, PERCENT_RANK, PLACING, PLAN, PLI, POSITION, POWER, PRECEDING, PRECISION, PREPARE, PRESERVE, PRIMARY, PRIOR, PRIVILEGES, PROCEDURE, PUBLIC, QUARTER, RANGE, RANK, RAW, READ, READS, REAL, RECURSIVE, REF, REFERENCES, REFERENCING, REGR_AVGX, REGR_AVGY, REGR_COUNT, REGR_INTERCEPT, REGR_R2, REGR_SLOPE, REGR_SXX, REGR_SXY, REGR_SYY, RELATIVE, RELEASE, REPEATABLE, RESET, RESTART, RESTRICT, RESULT, RETURN, RETURNED_CARDINALITY, RETURNED_LENGTH, RETURNED_OCTET_LENGTH, RETURNED_SQLSTATE, RETURNS, REVOKE, RIGHT, ROLE, ROLLBACK, ROLLUP, ROUTINE, ROUTINE_CATALOG, ROUTINE_NAME, ROUTINE_SCHEMA, ROW, ROWS, ROW_COUNT, ROW_NUMBER, SAVEPOINT, SCALE, SCHEMA, SCHEMA_NAME, SCOPE, SCOPE_CATALOGS, SCOPE_NAME, SCOPE_SCHEMA, SCROLL, SEARCH, SECOND, SECTION, SECURITY, SELECT, SELF, SENSITIVE, SEQUENCE, SERIALIZABLE, SERVER, SERVER_NAME, SESSION, SESSION_USER, SET, SETS, SIMILAR, SIMPLE, SIZE, SMALLINT, SOME, SOURCE, SPACE, SPECIFIC, SPECIFICTYPE, SPECIFIC_NAME, SQL, SQLEXCEPTION, SQLSTATE, SQLWARNING, SQL_TSI_DAY, SQL_TSI_FRAC_SECOND, SQL_TSI_HOUR, SQL_TSI_MICROSECOND, SQL_TSI_MINUTE, SQL_TSI_MONTH, SQL_TSI_QUARTER, SQL_TSI_SECOND, SQL_TSI_WEEK, SQL_TSI_YEAR, SQRT, START, STATE, STATEMENT, STATIC, STDDEV_POP, STDDEV_SAMP, STREAM, STRING, STRUCTURE, STYLE, SUBCLASS_ORIGIN, SUBMULTISET, SUBSTITUTE, SUBSTRING, SUM, SYMMETRIC, SYSTEM, SYSTEM_USER, TABLE, TABLESAMPLE, TABLE_NAME, TEMPORARY, THEN, TIES, TIME, TIMESTAMP, TIMESTAMPADD, TIMESTAMPDIFF, TIMEZONE_HOUR, TIMEZONE_MINUTE, TINYINT, TO, TOP_LEVEL_COUNT, TRAILING, TRANSACTION, TRANSACTIONS_ACTIVE, TRANSACTIONS_COMMITTED, TRANSACTIONS_ROLLED_BACK, TRANSFORM, TRANSFORMS, TRANSLATE, TRANSLATION, TREAT, TRIGGER, TRIGGER_CATALOG, TRIGGER_NAME, TRIGGER_SCHEMA, TRIM, TRUE, TYPE, UESCAPE, UNBOUNDED, UNCOMMITTED, UNDER, UNION, UNIQUE, UNKNOWN, UNNAMED, UNNEST, UPDATE, UPPER, UPSERT, USAGE, USER, USER_DEFINED_TYPE_CATALOG, USER_DEFINED_TYPE_CODE, USER_DEFINED_TYPE_NAME, USER_DEFINED_TYPE_SCHEMA, USING, VALUE, VALUES, VARBINARY, VARCHAR, VARYING, VAR_POP, VAR_SAMP, VERSION, VIEW, WEEK, WHEN, WHENEVER, WHERE, WIDTH_BUCKET, WINDOW, WITH, WITHIN, WITHOUT, WORK, WRAPPER, WRITE, XML, YEAR, ZONE
Back to top
`}),e.add({id:23,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/overview/",title:"概览",section:"Queries 查询",content:" 查询 # SELECT statements and VALUES statements are specified with the sqlQuery() method of the TableEnvironment. The method returns the result of the SELECT statement (or the VALUES statements) as a Table. A Table can be used in subsequent SQL and Table API queries, be converted into a DataStream, or written to a TableSink. SQL and Table API queries can be seamlessly mixed and are holistically optimized and translated into a single program.\nIn order to access a table in a SQL query, it must be registered in the TableEnvironment. A table can be registered from a TableSource, Table, CREATE TABLE statement, DataStream. Alternatively, users can also register catalogs in a TableEnvironment to specify the location of the data sources.\nFor convenience, Table.toString() automatically registers the table under a unique name in its TableEnvironment and returns the name. So, Table objects can be directly inlined into SQL queries as shown in the examples below.\nNote: Queries that include unsupported SQL features cause a TableException. The supported features of SQL on batch and streaming tables are listed in the following sections.\nSpecifying a Query # The following examples show how to specify a SQL queries on registered and inlined tables.\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // ingest a DataStream from an external source DataStream\u0026lt;Tuple3\u0026lt;Long, String, Integer\u0026gt;\u0026gt; ds = env.addSource(...); // SQL query with an inlined (unregistered) table Table table = tableEnv.fromDataStream(ds, $(\u0026#34;user\u0026#34;), $(\u0026#34;product\u0026#34;), $(\u0026#34;amount\u0026#34;)); Table result = tableEnv.sqlQuery( \u0026#34;SELECT SUM(amount) FROM \u0026#34; + table + \u0026#34; WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // SQL query with a registered table // register the DataStream as view \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $(\u0026#34;user\u0026#34;), $(\u0026#34;product\u0026#34;), $(\u0026#34;amount\u0026#34;)); // run a SQL query on the Table and retrieve the result as a new Table Table result2 = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // create and register a TableSink final Schema schema = Schema.newBuilder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build(); final TableDescriptor sinkDescriptor = TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build(); tableEnv.createTemporaryTable(\u0026#34;RubberOrders\u0026#34;, sinkDescriptor); // run an INSERT SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // read a DataStream from an external source val ds: DataStream[(Long, String, Integer)] = env.addSource(...) // SQL query with an inlined (unregistered) table val table = ds.toTable(tableEnv, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) val result = tableEnv.sqlQuery( s\u0026#34;SELECT SUM(amount) FROM $table WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // SQL query with a registered table // register the DataStream under the name \u0026#34;Orders\u0026#34; tableEnv.createTemporaryView(\u0026#34;Orders\u0026#34;, ds, $\u0026#34;user\u0026#34;, $\u0026#34;product\u0026#34;, $\u0026#34;amount\u0026#34;) // run a SQL query on the Table and retrieve the result as a new Table val result2 = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // create and register a TableSink val schema = Schema.newBuilder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build() val sinkDescriptor = TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build() tableEnv.createTemporaryTable(\u0026#34;RubberOrders\u0026#34;, sinkDescriptor) // run an INSERT SQL on the Table and emit the result to the TableSink tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) # SQL query with an inlined (unregistered) table # elements data type: BIGINT, STRING, BIGINT table = table_env.from_elements(..., [\u0026#39;user\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;amount\u0026#39;]) result = table_env \\ .sql_query(\u0026#34;SELECT SUM(amount) FROM %s WHERE product LIKE \u0026#39;%%Rubber%%\u0026#39;\u0026#34; % table) # create and register a TableSink schema = Schema.new_builder() .column(\u0026#34;product\u0026#34;, DataTypes.STRING()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .build() sink_descriptor = TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) .build()) .build() t_env.create_temporary_table(\u0026#34;RubberOrders\u0026#34;, sink_descriptor) # run an INSERT SQL on the Table and emit the result to the TableSink table_env \\ .execute_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Back to top\nExecute a Query # A SELECT statement or a VALUES statement can be executed to collect the content to local through the TableEnvironment.executeSql() method. The method returns the result of the SELECT statement (or the VALUES statement) as a TableResult. Similar to a SELECT statement, a Table object can be executed using the Table.execute() method to collect the content of the query to the local client. TableResult.collect() method returns a closeable row iterator. The select job will not be finished unless all result data has been collected. We should actively close the job to avoid resource leak through the CloseableIterator#close() method. We can also print the select result to client console through the TableResult.print() method. The result data in TableResult can be accessed only once. Thus, collect() and print() must not be called after each other.\nTableResult.collect() and TableResult.print() have slightly different behaviors under different checkpointing settings (to enable checkpointing for a streaming job, see checkpointing config).\nFor batch jobs or streaming jobs without checkpointing, TableResult.collect() and TableResult.print() have neither exactly-once nor at-least-once guarantee. Query results are immediately accessible by the clients once they\u0026rsquo;re produced, but exceptions will be thrown when the job fails and restarts. For streaming jobs with exactly-once checkpointing, TableResult.collect() and TableResult.print() guarantee an end-to-end exactly-once record delivery. A result will be accessible by clients only after its corresponding checkpoint completes. For streaming jobs with at-least-once checkpointing, TableResult.collect() and TableResult.print() guarantee an end-to-end at-least-once record delivery. Query results are immediately accessible by the clients once they\u0026rsquo;re produced, but it is possible for the same result to be delivered multiple times. Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings); tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // execute SELECT statement TableResult tableResult1 = tableEnv.executeSql(\u0026#34;SELECT * FROM Orders\u0026#34;); // use try-with-resources statement to make sure the iterator will be closed automatically try (CloseableIterator\u0026lt;Row\u0026gt; it = tableResult1.collect()) { while(it.hasNext()) { Row row = it.next(); // handle row } } // execute Table TableResult tableResult2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM Orders\u0026#34;).execute(); tableResult2.print(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tableEnv = StreamTableEnvironment.create(env, settings) // enable checkpointing tableEnv.getConfig .set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE, CheckpointingMode.EXACTLY_ONCE) tableEnv.getConfig .set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(10)) tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // execute SELECT statement val tableResult1 = tableEnv.executeSql(\u0026#34;SELECT * FROM Orders\u0026#34;) val it = tableResult1.collect() try while (it.hasNext) { val row = it.next // handle row } finally it.close() // close the iterator to avoid resource leak // execute Table val tableResult2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM Orders\u0026#34;).execute() tableResult2.print() Python env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env, settings) # enable checkpointing table_env.get_config().set(\u0026#34;execution.checkpointing.mode\u0026#34;, \u0026#34;EXACTLY_ONCE\u0026#34;) table_env.get_config().set(\u0026#34;execution.checkpointing.interval\u0026#34;, \u0026#34;10s\u0026#34;) table_env.execute_sql(\u0026#34;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) # execute SELECT statement table_result1 = table_env.execute_sql(\u0026#34;SELECT * FROM Orders\u0026#34;) table_result1.print() # execute Table table_result2 = table_env.sql_query(\u0026#34;SELECT * FROM Orders\u0026#34;).execute() table_result2.print() Back to top\nSyntax # Flink parses SQL using Apache Calcite, which supports standard ANSI SQL.\nThe following BNF-grammar describes the superset of supported SQL features in batch and streaming queries. The Operations section shows examples for the supported features and indicates which features are only supported for batch or streaming queries.\nGrammar ↕ query: values | WITH withItem [ , withItem ]* query | { select | selectWithoutFrom | query UNION [ ALL ] query | query EXCEPT query | query INTERSECT query } [ ORDER BY orderItem [, orderItem ]* ] [ LIMIT { count | ALL } ] [ OFFSET start { ROW | ROWS } ] [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } ONLY] withItem: name [ \u0026#39;(\u0026#39; column [, column ]* \u0026#39;)\u0026#39; ] AS \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; orderItem: expression [ ASC | DESC ] select: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } FROM tableExpression [ WHERE booleanExpression ] [ GROUP BY { groupItem [, groupItem ]* } ] [ HAVING booleanExpression ] [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ] selectWithoutFrom: SELECT [ ALL | DISTINCT ] { * | projectItem [, projectItem ]* } projectItem: expression [ [ AS ] columnAlias ] | tableAlias . * tableExpression: tableReference [, tableReference ]* | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ] joinCondition: ON booleanExpression | USING \u0026#39;(\u0026#39; column [, column ]* \u0026#39;)\u0026#39; tableReference: tablePrimary [ matchRecognize ] [ [ AS ] alias [ \u0026#39;(\u0026#39; columnAlias [, columnAlias ]* \u0026#39;)\u0026#39; ] ] tablePrimary: [ TABLE ] tablePath [ dynamicTableOptions ] [systemTimePeriod] [[AS] correlationName] | LATERAL TABLE \u0026#39;(\u0026#39; functionName \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; \u0026#39;)\u0026#39; | [ LATERAL ] \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; | UNNEST \u0026#39;(\u0026#39; expression \u0026#39;)\u0026#39; tablePath: [ [ catalogName . ] databaseName . ] tableName systemTimePeriod: FOR SYSTEM_TIME AS OF dateTimeExpression dynamicTableOptions: /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral values: VALUES expression [, expression ]* groupItem: expression | \u0026#39;(\u0026#39; \u0026#39;)\u0026#39; | \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | CUBE \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | ROLLUP \u0026#39;(\u0026#39; expression [, expression ]* \u0026#39;)\u0026#39; | GROUPING SETS \u0026#39;(\u0026#39; groupItem [, groupItem ]* \u0026#39;)\u0026#39; windowRef: windowName | windowSpec windowSpec: [ windowName ] \u0026#39;(\u0026#39; [ ORDER BY orderItem [, orderItem ]* ] [ PARTITION BY expression [, expression ]* ] [ RANGE numericOrIntervalExpression {PRECEDING} | ROWS numericExpression {PRECEDING} ] \u0026#39;)\u0026#39; matchRecognize: MATCH_RECOGNIZE \u0026#39;(\u0026#39; [ PARTITION BY expression [, expression ]* ] [ ORDER BY orderItem [, orderItem ]* ] [ MEASURES measureColumn [, measureColumn ]* ] [ ONE ROW PER MATCH ] [ AFTER MATCH ( SKIP TO NEXT ROW | SKIP PAST LAST ROW | SKIP TO FIRST variable | SKIP TO LAST variable | SKIP TO variable ) ] PATTERN \u0026#39;(\u0026#39; pattern \u0026#39;)\u0026#39; [ WITHIN intervalLiteral ] DEFINE variable AS condition [, variable AS condition ]* \u0026#39;)\u0026#39; measureColumn: expression AS alias pattern: patternTerm [ \u0026#39;|\u0026#39; patternTerm ]* patternTerm: patternFactor [ patternFactor ]* patternFactor: variable [ patternQuantifier ] patternQuantifier: \u0026#39;*\u0026#39; | \u0026#39;*?\u0026#39; | \u0026#39;+\u0026#39; | \u0026#39;+?\u0026#39; | \u0026#39;?\u0026#39; | \u0026#39;??\u0026#39; | \u0026#39;{\u0026#39; { [ minRepeat ], [ maxRepeat ] } \u0026#39;}\u0026#39; [\u0026#39;?\u0026#39;] | \u0026#39;{\u0026#39; repeat \u0026#39;}\u0026#39; Flink SQL uses a lexical policy for identifier (table, attribute, function names) similar to Java:\nThe case of identifiers is preserved whether or not they are quoted. After which, identifiers are matched case-sensitively. Unlike Java, back-ticks allow identifiers to contain non-alphanumeric characters (e.g. \u0026ldquo;SELECT a AS my field FROM t\u0026rdquo;). String literals must be enclosed in single quotes (e.g., SELECT 'Hello World'). Duplicate a single quote for escaping (e.g., SELECT 'It''s me').\nFlink SQL\u0026gt; SELECT \u0026#39;Hello World\u0026#39;, \u0026#39;It\u0026#39;\u0026#39;s me\u0026#39;; +-------------+---------+ | EXPR$0 | EXPR$1 | +-------------+---------+ | Hello World | It\u0026#39;s me | +-------------+---------+ 1 row in set Unicode characters are supported in string literals. If explicit unicode code points are required, use the following syntax:\nUse the backslash (\\) as escaping character (default): SELECT U\u0026amp;'\\263A' Use a custom escaping character: SELECT U\u0026amp;'#263A' UESCAPE '#' Back to top\nOperations # WITH clause SELECT \u0026amp; WHERE SELECT DISTINCT Windowing TVF Window Aggregation Group Aggregation Over Aggregation Joins Set Operations ORDER BY clause LIMIT clause Top-N Window Top-N Deduplication Pattern Recognition Back to top\n"}),e.add({id:24,href:"/flink/flink-docs-master/zh/docs/learn-flink/overview/",title:"概览",section:"实践练习",content:` 实践练习 # 本章教程的目标及涵盖范围 # 本章教程对 Apache Flink 的基本概念进行了介绍，虽然省略了许多重要细节，但是如果你掌握了本章内容，就足以实现可扩展并行度的 ETL、数据分析以及事件驱动的流式应用程序。本章重点对 Flink API 中的状态管理和时间进行了介绍，掌握了这些基础知识后，你将能更好地从其他详细参考文档中获取和掌握你所需要的知识。每小节结尾都有链接去引导你了解更多内容。
具体来说，你将在本章学习到以下内容：
如何实现流数据处理管道（pipelines） Flink 如何管理状态以及为何需要管理状态 如何使用事件时间（event time）来一致并准确地进行计算分析 如何在源源不断的数据流上构建事件驱动的应用程序 Flink 如何提供具有精确一次（exactly-once）计算语义的可容错、有状态流处理 本章教程着重介绍四个概念：源源不断的流式数据处理、事件时间、有状态流处理和状态快照。基本概念介绍如下。
每小节教程都有实践练习引导你如何在程序中使用其所述的概念，并在小节结尾都提供了相关实践练习的代码链接。 Back to top
流处理 # 在自然环境中，数据的产生原本就是流式的。无论是来自 Web 服务器的事件数据，证券交易所的交易数据，还是来自工厂车间机器上的传感器数据，其数据都是流式的。但是当你分析数据时，可以围绕 有界流（bounded）或 无界流（unbounded）两种模型来组织处理数据，当然，选择不同的模型，程序的执行和处理方式也都会不同。
批处理是有界数据流处理的范例。在这种模式下，你可以选择在计算结果输出之前输入整个数据集，这也就意味着你可以对整个数据集的数据进行排序、统计或汇总计算后再输出结果。
流处理正相反，其涉及无界数据流。至少理论上来说，它的数据输入永远不会结束，因此程序必须持续不断地对到达的数据进行处理。
在 Flink 中，应用程序由用户自定义算子转换而来的流式 dataflows 所组成。这些流式 dataflows 形成了有向图，以一个或多个源（source）开始，并以一个或多个汇（sink）结束。
通常，程序代码中的 transformation 和 dataflow 中的算子（operator）之间是一一对应的。但有时也会出现一个 transformation 包含多个算子的情况，如上图所示。
Flink 应用程序可以消费来自消息队列或分布式日志这类流式数据源（例如 Apache Kafka 或 Kinesis）的实时数据，也可以从各种的数据源中消费有界的历史数据。同样，Flink 应用程序生成的结果流也可以发送到各种数据汇中。
并行 Dataflows # Flink 程序本质上是分布式并行程序。在程序执行期间，一个流有一个或多个流分区（Stream Partition），每个算子有一个或多个算子子任务（Operator Subtask）。每个子任务彼此独立，并在不同的线程中运行，或在不同的计算机或容器中运行。
算子子任务数就是其对应算子的并行度。在同一程序中，不同算子也可能具有不同的并行度。
Flink 算子之间可以通过一对一（直传）模式或重新分发模式传输数据：
一对一模式（例如上图中的 Source 和 map() 算子之间）可以保留元素的分区和顺序信息。这意味着 map() 算子的 subtask[1] 输入的数据以及其顺序与 Source 算子的 subtask[1] 输出的数据和顺序完全相同，即同一分区的数据只会进入到下游算子的同一分区。
重新分发模式（例如上图中的 map() 和 keyBy/window 之间，以及 keyBy/window 和 Sink 之间）则会更改数据所在的流分区。当你在程序中选择使用不同的 transformation，每个算子子任务也会根据不同的 transformation 将数据发送到不同的目标子任务。例如以下这几种 transformation 和其对应分发数据的模式：keyBy()（通过散列键重新分区）、broadcast()（广播）或 rebalance()（随机重新分发）。在重新分发数据的过程中，元素只有在每对输出和输入子任务之间才能保留其之间的顺序信息（例如，keyBy/window 的 subtask[2] 接收到的 map() 的 subtask[1] 中的元素都是有序的）。因此，上图所示的 keyBy/window 和 Sink 算子之间数据的重新分发时，不同键（key）的聚合结果到达 Sink 的顺序是不确定的。
Back to top
自定义时间流处理 # 对于大多数流数据处理应用程序而言，能够使用处理实时数据的代码重新处理历史数据并产生确定并一致的结果非常有价值。
在处理流式数据时，我们通常更需要关注事件本身发生的顺序而不是事件被传输以及处理的顺序，因为这能够帮助我们推理出一组事件（事件集合）是何时发生以及结束的。例如电子商务交易或金融交易中涉及到的事件集合。
为了满足上述这类的实时流处理场景，我们通常会使用记录在数据流中的事件时间的时间戳，而不是处理数据的机器时钟的时间戳。
Back to top
有状态流处理 # Flink 中的算子可以是有状态的。这意味着如何处理一个事件可能取决于该事件之前所有事件数据的累积结果。Flink 中的状态不仅可以用于简单的场景（例如统计仪表板上每分钟显示的数据），也可以用于复杂的场景（例如训练作弊检测模型）。
Flink 应用程序可以在分布式群集上并行运行，其中每个算子的各个并行实例会在单独的线程中独立运行，并且通常情况下是会在不同的机器上运行。
有状态算子的并行实例组在存储其对应状态时通常是按照键（key）进行分片存储的。每个并行实例算子负责处理一组特定键的事件数据，并且这组键对应的状态会保存在本地。
如下图的 Flink 作业，其前三个算子的并行度为 2，最后一个 sink 算子的并行度为 1，其中第三个算子是有状态的，并且你可以看到第二个算子和第三个算子之间是全互联的（fully-connected），它们之间通过网络进行数据分发。通常情况下，实现这种类型的 Flink 程序是为了通过某些键对数据流进行分区，以便将需要一起处理的事件进行汇合，然后做统一计算处理。
Flink 应用程序的状态访问都在本地进行，因为这有助于其提高吞吐量和降低延迟。通常情况下 Flink 应用程序都是将状态存储在 JVM 堆上，但如果状态太大，我们也可以选择将其以结构化数据格式存储在高速磁盘中。
Back to top
通过状态快照实现的容错 # 通过状态快照和流重放两种方式的组合，Flink 能够提供可容错的，精确一次计算的语义。这些状态快照在执行时会获取并存储分布式 pipeline 中整体的状态，它会将数据源中消费数据的偏移量记录下来，并将整个 job graph 中算子获取到该数据（记录的偏移量对应的数据）时的状态记录并存储下来。当发生故障时，Flink 作业会恢复上次存储的状态，重置数据源从状态中记录的上次消费的偏移量开始重新进行消费处理。而且状态快照在执行时会异步获取状态并存储，并不会阻塞正在进行的数据处理逻辑。
Back to top
`}),e.add({id:25,href:"/flink/flink-docs-master/zh/docs/dev/table/common/",title:"概念与通用 API",section:"Table API \u0026 SQL",content:" 概念与通用 API # Table API 和 SQL 集成在同一套 API 中。 这套 API 的核心概念是Table，用作查询的输入和输出。 本文介绍 Table API 和 SQL 查询程序的通用结构、如何注册 Table 、如何查询 Table 以及如何输出 Table 。\nTable API 和 SQL 程序的结构 # 所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例展示了 Table API 和 SQL 程序的通用结构。\nJava import org.apache.flink.table.api.*; import org.apache.flink.connector.datagen.table.DataGenConnectorOptions; // Create a TableEnvironment for batch or streaming execution. // See the \u0026#34;Create a TableEnvironment\u0026#34; section for details. TableEnvironment tableEnv = TableEnvironment.create(/*…*/); // Create a source table tableEnv.createTemporaryTable(\u0026#34;SourceTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build()); // Create a sink table (using SQL DDL) tableEnv.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;); // Create a Table object from a Table API query Table table1 = tableEnv.from(\u0026#34;SourceTable\u0026#34;); // Create a Table object from a SQL query Table table2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM SourceTable\u0026#34;); // Emit a Table API result Table to a TableSink, same for SQL result TableResult tableResult = table1.insertInto(\u0026#34;SinkTable\u0026#34;).execute(); Scala import org.apache.flink.table.api._ import org.apache.flink.connector.datagen.table.DataGenConnectorOptions // Create a TableEnvironment for batch or streaming execution. // See the \u0026#34;Create a TableEnvironment\u0026#34; section for details. val tableEnv = TableEnvironment.create(/*…*/) // Create a source table tableEnv.createTemporaryTable(\u0026#34;SourceTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build()) // Create a sink table (using SQL DDL) tableEnv.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;) // Create a Table object from a Table API query val table1 = tableEnv.from(\u0026#34;SourceTable\u0026#34;) // Create a Table object from a SQL query val table2 = tableEnv.sqlQuery(\u0026#34;SELECT * FROM SourceTable\u0026#34;) // Emit a Table API result Table to a TableSink, same for SQL result val tableResult = table1.insertInto(\u0026#34;SinkTable\u0026#34;).execute() Python from pyflink.table import * # Create a TableEnvironment for batch or streaming execution table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # Create a source table table_env.executeSql(\u0026#34;\u0026#34;\u0026#34;CREATE TEMPORARY TABLE SourceTable ( f0 STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;rows-per-second\u0026#39; = \u0026#39;100\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # Create a sink table table_env.executeSql(\u0026#34;CREATE TEMPORARY TABLE SinkTable WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE SourceTable (EXCLUDING OPTIONS) \u0026#34;) # Create a Table from a Table API query table1 = table_env.from_path(\u0026#34;SourceTable\u0026#34;).select(...) # Create a Table from a SQL query table2 = table_env.sql_query(\u0026#34;SELECT ... FROM SourceTable ...\u0026#34;) # Emit a Table API result Table to a TableSink, same for SQL result table_result = table1.execute_insert(\u0026#34;SinkTable\u0026#34;) 注意： Table API 和 SQL 查询可以很容易地集成并嵌入到 DataStream 程序中。 请参阅与 DataStream API 集成 章节了解如何将 DataStream 与表之间的相互转化。\nBack to top\n创建 TableEnvironment # TableEnvironment 是 Table API 和 SQL 的核心概念。它负责:\n在内部的 catalog 中注册 Table 注册外部的 catalog 加载可插拔模块 执行 SQL 查询 注册自定义函数 （scalar、table 或 aggregation） DataStream 和 Table 之间的转换(面向 StreamTableEnvironment ) Table 总是与特定的 TableEnvironment 绑定。 不能在同一条查询中使用不同 TableEnvironment 中的表，例如，对它们进行 join 或 union 操作。 TableEnvironment 可以通过静态方法 TableEnvironment.create() 创建。\nJava import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build(); TableEnvironment tEnv = TableEnvironment.create(settings); Scala import org.apache.flink.table.api.{EnvironmentSettings, TableEnvironment} val settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build() val tEnv = TableEnvironment.create(settings) Python from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # create a batch TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) 或者，用户可以从现有的 StreamExecutionEnvironment 创建一个 StreamTableEnvironment 与 DataStream API 互操作。\nJava import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); Scala import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.EnvironmentSettings import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment s_env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(s_env) Back to top\n在 Catalog 中创建表 # TableEnvironment 维护着一个由标识符（identifier）创建的表 catalog 的映射。标识符由三个部分组成：catalog 名称、数据库名称以及对象名称。如果 catalog 或者数据库没有指明，就会使用当前默认值（参见表标识符扩展章节中的例子）。\nTable 可以是虚拟的（视图 VIEWS）也可以是常规的（表 TABLES）。视图 VIEWS可以从已经存在的Table中创建，一般是 Table API 或者 SQL 的查询结果。 表TABLES描述的是外部数据，例如文件、数据库表或者消息队列。\n临时表（Temporary Table）和永久表（Permanent Table） # 表可以是临时的，并与单个 Flink 会话（session）的生命周期相关，也可以是永久的，并且在多个 Flink 会话和群集（cluster）中可见。\n永久表需要 catalog（例如 Hive Metastore）以维护表的元数据。一旦永久表被创建，它将对任何连接到 catalog 的 Flink 会话可见且持续存在，直至被明确删除。\n另一方面，临时表通常保存于内存中并且仅在创建它们的 Flink 会话持续期间存在。这些表对于其它会话是不可见的。它们不与任何 catalog 或者数据库绑定但可以在一个命名空间（namespace）中创建。即使它们对应的数据库被删除，临时表也不会被删除。\n屏蔽（Shadowing） # 可以使用与已存在的永久表相同的标识符去注册临时表。临时表会屏蔽永久表，并且只要临时表存在，永久表就无法访问。所有使用该标识符的查询都将作用于临时表。\n这可能对实验（experimentation）有用。它允许先对一个临时表进行完全相同的查询，例如只有一个子集的数据，或者数据是不确定的。一旦验证了查询的正确性，就可以对实际的生产表进行查询。\n创建表 # 虚拟表 # 在 SQL 的术语中，Table API 的对象对应于视图（虚拟表）。它封装了一个逻辑查询计划。它可以通过以下方法在 catalog 中创建：\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // table is the result of a simple projection query Table projTable = tableEnv.from(\u0026#34;X\u0026#34;).select(...); // register the Table projTable as table \u0026#34;projectedTable\u0026#34; tableEnv.createTemporaryView(\u0026#34;projectedTable\u0026#34;, projTable); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // table is the result of a simple projection query val projTable: Table = tableEnv.from(\u0026#34;X\u0026#34;).select(...) // register the Table projTable as table \u0026#34;projectedTable\u0026#34; tableEnv.createTemporaryView(\u0026#34;projectedTable\u0026#34;, projTable) Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # table is the result of a simple projection query proj_table = table_env.from_path(\u0026#34;X\u0026#34;).select(...) # register the Table projTable as table \u0026#34;projectedTable\u0026#34; table_env.register_table(\u0026#34;projectedTable\u0026#34;, proj_table) 注意： 从传统数据库系统的角度来看，Table 对象与 VIEW 视图非常像。也就是，定义了 Table 的查询是没有被优化的， 而且会被内嵌到另一个引用了这个注册了的 Table的查询中。如果多个查询都引用了同一个注册了的Table，那么它会被内嵌每个查询中并被执行多次， 也就是说注册了的Table的结果不会被共享。\nBack to top\nConnector Tables # 另外一个方式去创建 TABLE 是通过 connector 声明。Connector 描述了存储表数据的外部系统。存储系统例如 Apache Kafka 或者常规的文件系统都可以通过这种方式来声明。\nSuch tables can either be created using the Table API directly, or by switching to SQL DDL.\nJava // Using table descriptors final TableDescriptor sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .schema(Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 100L) .build(); tableEnv.createTable(\u0026#34;SourceTableA\u0026#34;, sourceDescriptor); tableEnv.createTemporaryTable(\u0026#34;SourceTableB\u0026#34;, sourceDescriptor); // Using SQL DDL tableEnv.executeSql(\u0026#34;CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)\u0026#34;); Python # Using table descriptors source_descriptor = TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) \\ .schema(Schema.new_builder() .column(\u0026#34;f0\u0026#34;, DataTypes.STRING()) .build()) \\ .option(\u0026#34;rows-per-second\u0026#34;, \u0026#34;100\u0026#34;) \\ .build() t_env.create_table(\u0026#34;SourceTableA\u0026#34;, source_descriptor) t_env.create_temporary_table(\u0026#34;SourceTableB\u0026#34;, source_descriptor) # Using SQL DDL t_env.execute_sql(\u0026#34;CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)\u0026#34;) 扩展表标识符 # 表总是通过三元标识符注册，包括 catalog 名、数据库名和表名。\n用户可以指定一个 catalog 和数据库作为 \u0026ldquo;当前catalog\u0026rdquo; 和\u0026quot;当前数据库\u0026quot;。有了这些，那么刚刚提到的三元标识符的前两个部分就可以被省略了。如果前两部分的标识符没有指定， 那么会使用当前的 catalog 和当前数据库。用户也可以通过 Table API 或 SQL 切换当前的 catalog 和当前的数据库。\n标识符遵循 SQL 标准，因此使用时需要用反引号（`）进行转义。\nJava TableEnvironment tEnv = ...; tEnv.useCatalog(\u0026#34;custom_catalog\u0026#34;); tEnv.useDatabase(\u0026#34;custom_database\u0026#34;); Table table = ...; // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;exampleView\u0026#34;, table); // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_database.exampleView\u0026#34;, table); // register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;`example.View`\u0026#34;, table); // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table); Scala // get a TableEnvironment val tEnv: TableEnvironment = ... tEnv.useCatalog(\u0026#34;custom_catalog\u0026#34;) tEnv.useDatabase(\u0026#34;custom_database\u0026#34;) val table: Table = ... // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;exampleView\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_database.exampleView\u0026#34;, table) // register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; // in the database named \u0026#39;custom_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;`example.View`\u0026#34;, table) // register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; // in the database named \u0026#39;other_database\u0026#39; tableEnv.createTemporaryView(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table) Python # get a TableEnvironment t_env = TableEnvironment.create(...) t_env.use_catalog(\u0026#34;custom_catalog\u0026#34;) t_env.use_database(\u0026#34;custom_database\u0026#34;) table = ... # register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; # in the database named \u0026#39;custom_database\u0026#39; t_env.create_temporary_view(\u0026#34;other_database.exampleView\u0026#34;, table) # register the view named \u0026#39;example.View\u0026#39; in the catalog named \u0026#39;custom_catalog\u0026#39; # in the database named \u0026#39;custom_database\u0026#39; t_env.create_temporary_view(\u0026#34;`example.View`\u0026#34;, table) # register the view named \u0026#39;exampleView\u0026#39; in the catalog named \u0026#39;other_catalog\u0026#39; # in the database named \u0026#39;other_database\u0026#39; t_env.create_temporary_view(\u0026#34;other_catalog.other_database.exampleView\u0026#34;, table) 查询表 # Table API # Table API 是关于 Scala 和 Java 的集成语言式查询 API。与 SQL 相反，Table API 的查询不是由字符串指定，而是在宿主语言中逐步构建。\nTable API 是基于 Table 类的，该类表示一个表（流或批处理），并提供使用关系操作的方法。这些方法返回一个新的 Table 对象，该对象表示对输入 Table 进行关系操作的结果。 一些关系操作由多个方法调用组成，例如 table.groupBy(...).select()，其中 groupBy(...) 指定 table 的分组，而 select(...) 在 table 分组上的投影。\n文档 Table API 说明了所有流处理和批处理表支持的 Table API 算子。\n以下示例展示了一个简单的 Table API 聚合查询：\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // scan registered Orders table Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); // compute revenue for all customers from France Table revenue = orders .filter($(\u0026#34;cCountry\u0026#34;).isEqual(\u0026#34;FRANCE\u0026#34;)) .groupBy($(\u0026#34;cID\u0026#34;), $(\u0026#34;cName\u0026#34;)) .select($(\u0026#34;cID\u0026#34;), $(\u0026#34;cName\u0026#34;), $(\u0026#34;revenue\u0026#34;).sum().as(\u0026#34;revSum\u0026#34;)); // emit or convert Table // execute query Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // scan registered Orders table val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) // compute revenue for all customers from France val revenue = orders .filter($\u0026#34;cCountry\u0026#34; === \u0026#34;FRANCE\u0026#34;) .groupBy($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;) .select($\u0026#34;cID\u0026#34;, $\u0026#34;cName\u0026#34;, $\u0026#34;revenue\u0026#34;.sum AS \u0026#34;revSum\u0026#34;) // emit or convert Table // execute query Note: The Scala Table API uses Scala String interpolation that starts with a dollar sign ($) to reference the attributes of a Table. The Table API uses Scala implicits. Make sure to import\norg.apache.flink.table.api._ - for implicit expression conversions org.apache.flink.api.scala._ and org.apache.flink.table.api.bridge.scala._ if you want to convert from/to DataStream. Python # get a TableEnvironment table_env = # see \u0026#34;Create a TableEnvironment\u0026#34; section # register Orders table # scan registered Orders table orders = table_env.from_path(\u0026#34;Orders\u0026#34;) # compute revenue for all customers from France revenue = orders \\ .filter(col(\u0026#39;cCountry\u0026#39;) == \u0026#39;FRANCE\u0026#39;) \\ .group_by(col(\u0026#39;cID\u0026#39;), col(\u0026#39;cName\u0026#39;)) \\ .select(col(\u0026#39;cID\u0026#39;), col(\u0026#39;cName\u0026#39;), col(\u0026#39;revenue\u0026#39;).sum.alias(\u0026#39;revSum\u0026#39;)) # emit or convert Table # execute query Back to top\nSQL # Flink SQL 是基于实现了SQL标准的 Apache Calcite 的。SQL 查询由常规字符串指定。\n文档 SQL 描述了Flink对流处理和批处理表的SQL支持。\n下面的示例演示了如何指定查询并将结果作为 Table 对象返回。\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // compute revenue for all customers from France Table revenue = tableEnv.sqlQuery( \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; + \u0026#34;FROM Orders \u0026#34; + \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; + \u0026#34;GROUP BY cID, cName\u0026#34; ); // emit or convert Table // execute query Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register Orders table // compute revenue for all customers from France val revenue = tableEnv.sqlQuery(\u0026#34;\u0026#34;\u0026#34; |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) // emit or convert Table // execute query Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # register Orders table # compute revenue for all customers from France revenue = table_env.sql_query( \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; \u0026#34;FROM Orders \u0026#34; \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; \u0026#34;GROUP BY cID, cName\u0026#34; ) # emit or convert Table # execute query 如下的示例展示了如何指定一个更新查询，将查询的结果插入到已注册的表中。\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // register \u0026#34;Orders\u0026#34; table // register \u0026#34;RevenueFrance\u0026#34; output table // compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; tableEnv.executeSql( \u0026#34;INSERT INTO RevenueFrance \u0026#34; + \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; + \u0026#34;FROM Orders \u0026#34; + \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; + \u0026#34;GROUP BY cID, cName\u0026#34; ); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // register \u0026#34;Orders\u0026#34; table // register \u0026#34;RevenueFrance\u0026#34; output table // compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; tableEnv.executeSql(\u0026#34;\u0026#34;\u0026#34; |INSERT INTO RevenueFrance |SELECT cID, cName, SUM(revenue) AS revSum |FROM Orders |WHERE cCountry = \u0026#39;FRANCE\u0026#39; |GROUP BY cID, cName \u0026#34;\u0026#34;\u0026#34;.stripMargin) Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # register \u0026#34;Orders\u0026#34; table # register \u0026#34;RevenueFrance\u0026#34; output table # compute revenue for all customers from France and emit to \u0026#34;RevenueFrance\u0026#34; table_env.execute_sql( \u0026#34;INSERT INTO RevenueFrance \u0026#34; \u0026#34;SELECT cID, cName, SUM(revenue) AS revSum \u0026#34; \u0026#34;FROM Orders \u0026#34; \u0026#34;WHERE cCountry = \u0026#39;FRANCE\u0026#39; \u0026#34; \u0026#34;GROUP BY cID, cName\u0026#34; ) Back to top\n混用 Table API 和 SQL # Table API 和 SQL 查询的混用非常简单因为它们都返回 Table 对象：\n可以在 SQL 查询返回的 Table 对象上定义 Table API 查询。 在 TableEnvironment 中注册的结果表可以在 SQL 查询的 FROM 子句中引用，通过这种方法就可以在 Table API 查询的结果上定义 SQL 查询。 Back to top\n输出表 # Table 通过写入 TableSink 输出。TableSink 是一个通用接口，用于支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）、存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）或消息队列系统（如 Apache Kafka、RabbitMQ）。\n批处理 Table 只能写入 BatchTableSink，而流处理 Table 需要指定写入 AppendStreamTableSink，RetractStreamTableSink 或者 UpsertStreamTableSink。\n请参考文档 Table Sources \u0026amp; Sinks 以获取更多关于可用 Sink 的信息以及如何自定义 DynamicTableSink。\n方法 Table.insertInto(String tableName) 定义了一个完整的端到端管道将源表中的数据传输到一个被注册的输出表中。 该方法通过名称在 catalog 中查找输出表并确认 Table schema 和输出表 schema 一致。 可以通过方法 TablePipeline.explain() 和 TablePipeline.execute() 分别来解释和执行一个数据流管道。\n下面的示例演示如何输出 Table：\nJava // get a TableEnvironment TableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // create an output Table final Schema schema = Schema.newBuilder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build(); tableEnv.createTemporaryTable(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()); // compute a result Table using Table API operators and/or SQL queries Table result = ...; // Prepare the insert into pipeline TablePipeline pipeline = result.insertInto(\u0026#34;CsvSinkTable\u0026#34;); // Print explain details pipeline.printExplain(); // emit the result Table to the registered TableSink pipeline.execute(); Scala // get a TableEnvironment val tableEnv = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // create an output Table val schema = Schema.newBuilder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build() tableEnv.createTemporaryTable(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.forFormat(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()) // compute a result Table using Table API operators and/or SQL queries val result: Table = ... // Prepare the insert into pipeline val pipeline = result.insertInto(\u0026#34;CsvSinkTable\u0026#34;) // Print explain details pipeline.printExplain() // emit the result Table to the registered TableSink pipeline.execute() Python # get a TableEnvironment table_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section # create a TableSink schema = Schema.new_builder() .column(\u0026#34;a\u0026#34;, DataTypes.INT()) .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) .column(\u0026#34;c\u0026#34;, DataTypes.BIGINT()) .build() table_env.create_temporary_table(\u0026#34;CsvSinkTable\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/path/to/file\u0026#34;) .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;|\u0026#34;) .build()) .build()) # compute a result Table using Table API operators and/or SQL queries result = ... # emit the result Table to the registered TableSink result.execute_insert(\u0026#34;CsvSinkTable\u0026#34;) Back to top\n翻译与执行查询 # 不论输入数据源是流式的还是批式的，Table API 和 SQL 查询都会被转换成 DataStream 程序。 查询在内部表示为逻辑查询计划，并被翻译成两个阶段：\n优化逻辑执行计划 翻译成 DataStream 程序 Table API 或者 SQL 查询在下列情况下会被翻译：\n当 TableEnvironment.executeSql() 被调用时。该方法是用来执行一个 SQL 语句，一旦该方法被调用， SQL 语句立即被翻译。 当 TablePipeline.execute() 被调用时。该方法是用来执行一个源表到输出表的数据流，一旦该方法被调用， TABLE API 程序立即被翻译。 当 Table.execute() 被调用时。该方法是用来将一个表的内容收集到本地，一旦该方法被调用， TABLE API 程序立即被翻译。 当 StatementSet.execute() 被调用时。TablePipeline （通过 StatementSet.add() 输出给某个 Sink）和 INSERT 语句 （通过调用 StatementSet.addInsertSql()）会先被缓存到 StatementSet 中，StatementSet.execute() 方法被调用时，所有的 sink 会被优化成一张有向无环图。 当 Table 被转换成 DataStream 时（参阅与 DataStream 集成）。转换完成后，它就成为一个普通的 DataStream 程序，并会在调用 StreamExecutionEnvironment.execute() 时被执行。 Back to top\n查询优化 # Apache Flink 使用并扩展了 Apache Calcite 来执行复杂的查询优化。 这包括一系列基于规则和成本的优化，例如：\n基于 Apache Calcite 的子查询解相关 投影剪裁 分区剪裁 过滤器下推 子计划消除重复数据以避免重复计算 特殊子查询重写，包括两部分： 将 IN 和 EXISTS 转换为 left semi-joins 将 NOT IN 和 NOT EXISTS 转换为 left anti-join 可选 join 重新排序 通过 table.optimizer.join-reorder-enabled 启用 注意： 当前仅在子查询重写的结合条件下支持 IN / EXISTS / NOT IN / NOT EXISTS。\n优化器不仅基于计划，而且还基于可从数据源获得的丰富统计信息以及每个算子（例如 io，cpu，网络和内存）的细粒度成本来做出明智的决策。\n高级用户可以通过 CalciteConfig 对象提供自定义优化，可以通过调用 TableEnvironment＃getConfig＃setPlannerConfig 将其提供给 TableEnvironment。\n解释表 # Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。 这是通过 Table.explain() 方法或者 StatementSet.explain() 方法来完成的。Table.explain() 返回一个 Table 的计划。StatementSet.explain() 返回多 sink 计划的结果。它返回一个描述三种计划的字符串：\n关系查询的抽象语法树（the Abstract Syntax Tree），即未优化的逻辑查询计划， 优化的逻辑查询计划，以及 物理执行计划。 可以用 TableEnvironment.explainSql() 方法和 TableEnvironment.executeSql() 方法支持执行一个 EXPLAIN 语句获取逻辑和优化查询计划，请参阅 EXPLAIN 页面.\n以下代码展示了一个示例以及对给定 Table 使用 Table.explain() 方法的相应输出：\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); DataStream\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; stream1 = env.fromElements(new Tuple2\u0026lt;\u0026gt;(1, \u0026#34;hello\u0026#34;)); DataStream\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; stream2 = env.fromElements(new Tuple2\u0026lt;\u0026gt;(1, \u0026#34;hello\u0026#34;)); // explain Table API Table table1 = tEnv.fromDataStream(stream1, $(\u0026#34;count\u0026#34;), $(\u0026#34;word\u0026#34;)); Table table2 = tEnv.fromDataStream(stream2, $(\u0026#34;count\u0026#34;), $(\u0026#34;word\u0026#34;)); Table table = table1 .where($(\u0026#34;word\u0026#34;).like(\u0026#34;F%\u0026#34;)) .unionAll(table2); System.out.println(table.explain()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val tEnv = StreamTableEnvironment.create(env) val table1 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table2 = env.fromElements((1, \u0026#34;hello\u0026#34;)).toTable(tEnv, $\u0026#34;count\u0026#34;, $\u0026#34;word\u0026#34;) val table = table1 .where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) .unionAll(table2) println(table.explain()) Python env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) table1 = t_env.from_elements([(1, \u0026#34;hello\u0026#34;)], [\u0026#34;count\u0026#34;, \u0026#34;word\u0026#34;]) table2 = t_env.from_elements([(1, \u0026#34;hello\u0026#34;)], [\u0026#34;count\u0026#34;, \u0026#34;word\u0026#34;]) table = table1 \\ .where(col(\u0026#39;word\u0026#39;).like(\u0026#39;F%\u0026#39;)) \\ .union_all(table2) print(table.explain()) 上述例子的结果是：\n== Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[Unregistered_DataStream_1]]) +- LogicalTableScan(table=[[Unregistered_DataStream_2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word]) +- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word]) == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- DataStreamScan(table=[[Unregistered_DataStream_1]], fields=[count, word]) +- DataStreamScan(table=[[Unregistered_DataStream_2]], fields=[count, word]) 以下代码展示了一个示例以及使用 StatementSet.explain() 的多 sink 计划的相应输出：\nJava EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); final Schema schema = Schema.newBuilder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build(); tEnv.createTemporaryTable(\u0026#34;MySource1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySource2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySink1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); tEnv.createTemporaryTable(\u0026#34;MySink2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()); StatementSet stmtSet = tEnv.createStatementSet(); Table table1 = tEnv.from(\u0026#34;MySource1\u0026#34;).where($(\u0026#34;word\u0026#34;).like(\u0026#34;F%\u0026#34;)); stmtSet.add(table1.insertInto(\u0026#34;MySink1\u0026#34;)); Table table2 = table1.unionAll(tEnv.from(\u0026#34;MySource2\u0026#34;)); stmtSet.add(table2.insertInto(\u0026#34;MySink2\u0026#34;)); String explanation = stmtSet.explain(); System.out.println(explanation); Scala val settings = EnvironmentSettings.inStreamingMode() val tEnv = TableEnvironment.create(settings) val schema = Schema.newBuilder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build() tEnv.createTemporaryTable(\u0026#34;MySource1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySource2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySink1\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) tEnv.createTemporaryTable(\u0026#34;MySink2\u0026#34;, TableDescriptor.forConnector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) val stmtSet = tEnv.createStatementSet() val table1 = tEnv.from(\u0026#34;MySource1\u0026#34;).where($\u0026#34;word\u0026#34;.like(\u0026#34;F%\u0026#34;)) stmtSet.add(table1.insertInto(\u0026#34;MySink1\u0026#34;)) val table2 = table1.unionAll(tEnv.from(\u0026#34;MySource2\u0026#34;)) stmtSet.add(table2.insertInto(\u0026#34;MySink2\u0026#34;)) val explanation = stmtSet.explain() println(explanation) Python settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(environment_settings=settings) schema = Schema.new_builder() .column(\u0026#34;count\u0026#34;, DataTypes.INT()) .column(\u0026#34;word\u0026#34;, DataTypes.STRING()) .build() t_env.create_temporary_table(\u0026#34;MySource1\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySource2\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/source/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySink1\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path1\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) t_env.create_temporary_table(\u0026#34;MySink2\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) .schema(schema) .option(\u0026#34;path\u0026#34;, \u0026#34;/sink/path2\u0026#34;) .format(\u0026#34;csv\u0026#34;) .build()) stmt_set = t_env.create_statement_set() table1 = t_env.from_path(\u0026#34;MySource1\u0026#34;).where(col(\u0026#39;word\u0026#39;).like(\u0026#39;F%\u0026#39;)) stmt_set.add_insert(\u0026#34;MySink1\u0026#34;, table1) table2 = table1.union_all(t_env.from_path(\u0026#34;MySource2\u0026#34;)) stmt_set.add_insert(\u0026#34;MySink2\u0026#34;, table2) explanation = stmt_set.explain() print(explanation) 多 sink 计划的结果是：\n== Abstract Syntax Tree == LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) LogicalLegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]]) == Optimized Physical Plan == LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) == Optimized Execution Plan == Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])(reuse_id=[1]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) LegacySink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[count, word]) +- Reused(reference_id=[1]) LegacySink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[count, word]) +- Union(all=[true], union=[count, word]) :- Reused(reference_id=[1]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]]], fields=[count, word]) Back to top\n"}),e.add({id:26,href:"/flink/flink-docs-master/zh/docs/dev/table/functions/overview/",title:"函数",section:"函数",content:` 函数 # Flink 允许用户在 Table API 和 SQL 中使用函数进行数据的转换。
函数类型 # Flink 中的函数有两个划分标准。
一个划分标准是：系统（内置）函数和 Catalog 函数。系统函数没有名称空间，只能通过其名称来进行引用。 Catalog 函数属于 Catalog 和数据库，因此它们拥有 Catalog 和数据库命名空间。 用户可以通过全/部分限定名（catalog.db.func 或 db.func）或者函数名 来对 Catalog 函数进行引用。
另一个划分标准是：临时函数和持久化函数。 临时函数始终由用户创建，它容易改变并且仅在会话的生命周期内有效。 持久化函数不是由系统提供，就是存储在 Catalog 中，它在会话的整个生命周期内都有效。
这两个划分标准给 Flink 用户提供了 4 种函数：
临时性系统函数 系统函数 临时性 Catalog 函数 Catalog 函数 请注意，系统函数始终优先于 Catalog 函数解析，临时函数始终优先于持久化函数解析， 函数解析优先级如下所述。
函数引用 # 用户在 Flink 中可以通过精确、模糊两种引用方式引用函数。
精确函数引用 # 精确函数引用允许用户跨 Catalog，跨数据库调用 Catalog 函数。 例如：select mycatalog.mydb.myfunc(x) from mytable 和 select mydb.myfunc(x) from mytable。
仅 Flink 1.10 以上版本支持。
模糊函数引用 # 在模糊函数引用中，用户只需在 SQL 查询中指定函数名，例如： select myfunc(x) from mytable。
函数解析顺序 # 当函数名相同，函数类型不同时，函数解析顺序才有意义。 例如：当有三个都名为 \u0026ldquo;myfunc\u0026rdquo; 的临时性 Catalog 函数，Catalog 函数，和系统函数时， 如果没有命名冲突，三个函数将会被解析为一个函数。
精确函数引用 # 由于系统函数没有命名空间，Flink 中的精确函数引用必须 指向临时性 Catalog 函数或 Catalog 函数。
解析顺序如下：
临时性 catalog 函数 Catalog 函数 模糊函数引用 # 解析顺序如下：
临时性系统函数 系统函数 临时性 Catalog 函数, 在会话的当前 Catalog 和当前数据库中 Catalog 函数, 在会话的当前 Catalog 和当前数据库中 `}),e.add({id:27,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/overview/",title:"流式概念",section:"流式概念",content:` 流式概念 # Flink 的 Table API 和 SQL 是流批统一的 API。 这意味着 Table API \u0026amp; SQL 在无论有限的批式输入还是无限的流式输入下，都具有相同的语义。 因为传统的关系代数以及 SQL 最开始都是为了批式处理而设计的， 关系型查询在流式场景下不如在批式场景下容易懂。
下面这些页面包含了概念、实际的限制，以及流式数据处理中的一些特定的配置。
状态管理 # 流模式下运行的表程序利用了 Flink 作为有状态流处理器的所有能力。
事实上，一个表程序（Table program）可以配置一个 state backend 和多个不同的 checkpoint 选项 以处理对不同状态大小和容错需求。这可以对正在运行的 Table API \u0026amp; SQL 管道（pipeline）生成 savepoint，并在这之后用其恢复应用程序的状态。
状态使用 # 由于 Table API \u0026amp; SQL 程序是声明式的，管道内的状态会在哪以及如何被使用并不明确。 Planner 会确认是否需要状态来得到正确的计算结果， 管道会被现有优化规则集优化成尽可能少地使用状态。
从概念上讲， 源表从来不会在状态中被完全保存。 实现者处理的是逻辑表（即动态表）。 它们的状态取决于用到的操作。 形如 SELECT ... FROM ... WHERE 这种只包含字段映射或过滤器的查询的查询语句通常是无状态的管道。 然而诸如 join、 聚合或去重操作需要在 Flink 抽象的容错存储内保存中间结果。
请参考独立的算子文档来获取更多关于状态需求量和限制潜在增长状态大小的信息。 例如对两个表进行 join 操作的普通 SQL 需要算子保存两个表的全部输入。基于正确的 SQL 语义，运行时假设两表会在任意时间点进行匹配。 Flink 提供了 优化窗口和时段 Join 聚合 以利用 watermarks 概念来让保持较小的状态规模。
另一个计算每个会话的点击次数的查询语句的例子如下
SELECT sessionId, COUNT(*) FROM clicks GROUP BY sessionId; sessionId 是用于分组的键，连续查询（Continuous Query）维护了每个观察到的 sessionId 次数。 sessionId 属性随着时间逐步演变， 且 sessionId 的值只活跃到会话结束（即在有限的时间周期内）。然而连续查询无法得知sessionId的这个性质， 并且预期每个 sessionId 值会在任何时间点上出现。这维护了每个可见的 sessionId 值。因此总状态量会随着 sessionId 的发现不断地增长。
空闲状态维持时间 # 空间状态位置时间参数 table.exec.state.ttl 定义了状态的键在被更新后要保持多长时间才被移除。在之前的查询例子中，sessionId 的数目会在配置的时间内未更新时立刻被移除。
通过移除状态的键，连续查询会完全忘记它曾经见过这个键。如果一个状态带有曾被移除状态的键被处理了，这条记录将被认为是 对应键的第一条记录。上述例子中意味着 sessionId 会再次从 0 开始计数。
状态化更新与演化 # 表程序在流模式下执行将被视为标准查询，这意味着它们被定义一次后将被一直视为静态的端到端 (end-to-end) 管道
对于这种状态化的管道，对查询和Flink的Planner的改动都有可能导致完全不同的执行计划。这让表程序的状态化的升级和演化在目前而言 仍具有挑战，社区正致力于改进这一缺点。
例如为了添加过滤谓词，优化器可能决定重排 join 或改变内部算子的 schema。 这会阻碍从 savepoint 的恢复，因为其被改变的拓扑和 算子状态的列布局差异。
查询实现者需要确保改变在优化计划前后是兼容的，在 SQL 中使用 EXPLAIN 或在 Table API 中使用 table.explain() 可获取详情。
由于新的优化器规则正不断地被添加，算子变得更加高效和专用，升级到更新的Flink版本可能造成不兼容的计划。
当前框架无法保证状态可以从 savepoint 映射到新的算子拓扑上。
换言之： Savepoint 只在查询语句和版本保持恒定的情况下被支持。
由于社区拒绝在版本补丁（如 1.13.1 至 1.13.2）上对优化计划和算子拓扑进行修改的贡献，对一个 Table API \u0026amp; SQL 管道 升级到新的 bug fix 发行版应当是安全的。然而主次（major-minor）版本的更新（如 1.12 至 1.13）不被支持。
由于这两个缺点（即修改查询语句和修改Flink版本），我们推荐实现调查升级后的表程序是否可以在切换到实时数据前，被历史数据\u0026quot;暖机\u0026quot; （即被初始化）。Flink社区正致力于 混合源 来让切换变得尽可能方便。
接下来？ # 动态表: 描述了动态表的概念。 时间属性: 解释了时间属性以及它是如何在 Table API \u0026amp; SQL 中使用的。 流上的 Join: 支持的几种流上的 Join。 时态（temporal）表: 描述了时态表的概念。 查询配置: Table API \u0026amp; SQL 特定的配置。 Back to top
`}),e.add({id:28,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/common/",title:"通用配置",section:"File Systems",content:` 通用配置 # Apache Flink 提供了一些对所有文件系统均适用的基本配置。
默认文件系统 # 如果文件路径未明确指定文件系统的 scheme（和 authority），将会使用默认的 scheme（和 authority）：
fs.default-scheme: \u0026lt;default-fs\u0026gt; 例如默认的文件系统配置为 fs.default-scheme: hdfs://localhost:9000/，则文件路径 /user/hugo/in.txt 将被处理为 hdfs://localhost:9000/user/hugo/in.txt。
连接限制 # 如果文件系统不能处理大量并发读/写操作或连接，可以为文件系统同时打开的总连接数设置上限。
例如在一个大型 Flink 任务建立 checkpoint 时，具有少量 RPC handler 的小型 HDFS 集群可能会由于建立了过多的连接而过载。
要限制文件系统的连接数，可将下列配置添加至 Flink 配置中。设置限制的文件系统由其 scheme 指定：
fs.\u0026lt;scheme\u0026gt;.limit.total: (数量，0/-1 表示无限制) fs.\u0026lt;scheme\u0026gt;.limit.input: (数量，0/-1 表示无限制) fs.\u0026lt;scheme\u0026gt;.limit.output: (数量，0/-1 表示无限制) fs.\u0026lt;scheme\u0026gt;.limit.timeout: (毫秒，0 表示无穷) fs.\u0026lt;scheme\u0026gt;.limit.stream-timeout: (毫秒，0 表示无穷) 输入和输出连接（流）的数量可以分别进行限制（fs.\u0026lt;scheme\u0026gt;.limit.input 和 fs.\u0026lt;scheme\u0026gt;.limit.output），也可以限制并发流的总数（fs.\u0026lt;scheme\u0026gt;.limit.total）。如果文件系统尝试打开更多的流，操作将被阻塞直至某些流关闭。如果打开流的时间超过 fs.\u0026lt;scheme\u0026gt;.limit.timeout，则流打开失败。
为避免不活动的流占满整个连接池（阻止新连接的建立），可以在配置中添加无活动超时时间，如果连接至少在 fs.\u0026lt;scheme\u0026gt;.limit.stream-timeout 时间内没有读/写操作，则连接会被强制关闭。
连接数是按每个 TaskManager/文件系统来进行限制的。因为文件系统的创建是按照 scheme 和 authority 进行的，所以不同的 authority 具有独立的连接池，例如 hdfs://myhdfs:50010/ 和 hdfs://anotherhdfs:4399/ 会有单独的连接池。
Back to top
`}),e.add({id:29,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/overview/",title:"文件系统",section:"File Systems",content:` 文件系统 # Apache Flink 使用文件系统来消费和持久化地存储数据，以处理应用结果以及容错与恢复。以下是一些最常用的文件系统：本地存储，hadoop-compatible，Amazon S3，阿里云 OSS 和 Azure Blob Storage。
文件使用的文件系统通过其 URI Scheme 指定。例如 file:///home/user/text.txt 表示一个在本地文件系统中的文件，hdfs://namenode:50010/data/user/text.txt 表示一个在指定 HDFS 集群中的文件。
文件系统在每个进程实例化一次，然后进行缓存/池化，从而避免每次创建流时的配置开销，并强制执行特定的约束，如连接/流的限制。
本地文件系统 # Flink 原生支持本地机器上的文件系统，包括任何挂载到本地文件系统的 NFS 或 SAN 驱动器，默认即可使用，无需额外配置。本地文件可通过 file:// URI Scheme 引用。
外部文件系统 # Apache Flink 支持下列文件系统：
Amazon S3 对象存储由 flink-s3-fs-presto 和 flink-s3-fs-hadoop 两种替代实现提供支持。这两种实现都是独立的，没有依赖项。
阿里云对象存储 由 flink-oss-fs-hadoop 支持，并通过 oss:// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。
Azure Blob Storage 由flink-azure-fs-hadoop 支持，并通过 abfs(s):// 和 wasb(s):// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。
Google Cloud Storage 由gcs-connector 支持，并通过 gs:// URI scheme 使用。该实现基于 Hadoop Project，但其是独立的，没有依赖项。
上述文件系统可以并且需要作为插件使用。
使用外部文件系统时，在启动 Flink 之前需将对应的 JAR 文件从 opt 目录复制到 Flink 发行版 plugin 目录下的某一文件夹中，例如：
mkdir ./plugins/s3-fs-hadoop cp ./opt/flink-s3-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/s3-fs-hadoop/ 注意 文件系统的插件机制在 Flink 版本 1.9 中引入，以支持每个插件专有 Java 类加载器，并避免类隐藏机制。您仍然可以通过旧机制使用文件系统，即将对应的 JAR 文件复制到 lib 目录中，或使用您自己的实现方式，但是从版本 1.10 开始，S3 插件必须通过插件机制加载，因为这些插件不再被隐藏（版本 1.10 之后类不再被重定位），旧机制不再可用。
尽可能通过基于插件的加载机制使用支持的文件系统。未来的 Flink 版本将不再支持通过 lib 目录加载文件系统组件。
添加新的外部文件系统实现 # 文件系统由类 org.apache.flink.core.fs.FileSystem 表示，该类定义了访问与修改文件系统中文件与对象的方法。
要添加一个新的文件系统：
添加文件系统实现，它应是 org.apache.flink.core.fs.FileSystem 的子类。 添加 Factory 类，以实例化该文件系统并声明文件系统所注册的 scheme, 它应是 org.apache.flink.core.fs.FileSystemFactory 的子类。 添加 Service Entry。创建文件 META-INF/services/org.apache.flink.core.fs.FileSystemFactory，文件中包含文件系统 Factory 类的类名。 （更多细节请查看 Java Service Loader docs） 在插件检索时，文件系统 Factory 类会由一个专用的 Java 类加载器加载，从而避免与其他类或 Flink 组件冲突。在文件系统实例化和文件系统调用时，应使用该类加载器。
警告 实际上这表示您的实现应避免使用 Thread.currentThread().getContextClassLoader() 类加载器。
Hadoop 文件系统 (HDFS) 及其其他实现 # 所有 Flink 无法找到直接支持的文件系统均将回退为 Hadoop。 当 flink-runtime 和 Hadoop 类包含在 classpath 中时，所有的 Hadoop 文件系统将自动可用。
因此，Flink 无缝支持所有实现 org.apache.hadoop.fs.FileSystem 接口的 Hadoop 文件系统和所有兼容 Hadoop 的文件系统 (Hadoop-compatible file system, HCFS)：
HDFS （已测试） Google Cloud Storage Connector for Hadoop（已测试） Alluxio（已测试，参见下文的配置详细信息） XtreemFS（已测试） FTP via Hftp（未测试） HAR（未测试） \u0026hellip; Hadoop 配置须在 core-site.xml 文件中包含所需文件系统的实现。可查看 Alluxio 的示例。
除非有其他的需要，建议使用 Flink 内置的文件系统。在某些情况下，如通过配置 Hadoop core-site.xml 中的 fs.defaultFS 属性将文件系统作为 YARN 的资源存储时，可能需要直接使用 Hadoop 文件系统。
Alluxio # 在 core-site.xml 文件中添加以下条目以支持 Alluxio：
\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Back to top
`}),e.add({id:30,href:"/flink/flink-docs-master/zh/docs/dev/configuration/",title:"项目配置",section:"应用开发",content:" "}),e.add({id:31,href:"/flink/flink-docs-master/zh/docs/ops/state/",title:"状态与容错",section:"Operations",content:""}),e.add({id:32,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/s3/",title:"Amazon S3",section:"File Systems",content:` Amazon S3 # Amazon Simple Storage Service (Amazon S3) 提供用于多种场景的云对象存储。S3 可与 Flink 一起使用以读取、写入数据，并可与 流的 State backends 相结合使用。
通过以下格式指定路径，S3 对象可类似于普通文件使用：
s3://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt; Endpoint 可以是一个文件或目录，例如：
// 读取 S3 bucket env.readTextFile(\u0026#34;s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // 写入 S3 bucket stream.writeAsText(\u0026#34;s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // 使用 S3 作为 FsStatebackend env.setStateBackend(new FsStateBackend(\u0026#34;s3://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;)); 注意这些例子并不详尽，S3 同样可以用在其他场景，包括 JobManager 高可用配置 或 RocksDBStateBackend，以及所有 Flink 需要使用文件系统 URI 的位置。
在大部分使用场景下，可使用 flink-s3-fs-hadoop 或 flink-s3-fs-presto 两个独立且易于设置的 S3 文件系统插件。然而在某些情况下，例如使用 S3 作为 YARN 的资源存储目录时，可能需要配置 Hadoop S3 文件系统。
Hadoop/Presto S3 文件系统插件 # 如果您在使用 Flink on EMR，您无需手动对此进行配置。 Flink 提供两种文件系统用来与 S3 交互：flink-s3-fs-presto 和 flink-s3-fs-hadoop。两种实现都是独立的且没有依赖项，因此使用时无需将 Hadoop 添加至 classpath。
flink-s3-fs-presto，通过 s3:// 和 s3p:// 两种 scheme 使用，基于 Presto project。 可以使用和 Presto 文件系统相同的配置项进行配置，方式为将配置添加到 flink-conf.yaml 文件中。如果要在 S3 中使用 checkpoint，推荐使用 Presto S3 文件系统。
flink-s3-fs-hadoop，通过 s3:// 和 s3a:// 两种 scheme 使用, 基于 Hadoop Project。 本文件系统可以使用类似 Hadoop S3A 的配置项进行配置，方式为将配置添加到 flink-conf.yaml 文件中。
例如，Hadoop 有 fs.s3a.connection.maximum 的配置选项。 如果你想在 Flink 程序中改变该配置的值，你需要将配置 s3.connection.maximum: xyz 添加到 flink-conf.yaml 文件中。Flink 会内部将其转换成配置 fs.s3a.connection.maximum。 而无需通过 Hadoop 的 XML 配置文件来传递参数。
另外，它是唯一支持 FileSystem 的 S3 文件系统。
flink-s3-fs-hadoop 和 flink-s3-fs-presto 都为 s3:// scheme 注册了默认的文件系统包装器，flink-s3-fs-hadoop 另外注册了 s3a://，flink-s3-fs-presto 注册了 s3p://，因此二者可以同时使用。 例如某作业使用了 FileSystem，它仅支持 Hadoop，但建立 checkpoint 使用 Presto。在这种情况下，建议明确地使用 s3a:// 作为 sink (Hadoop) 的 scheme，checkpoint (Presto) 使用 s3p://。这一点对于 FileSystem 同样成立。
在启动 Flink 之前，将对应的 JAR 文件从 opt 复制到 Flink 发行版的 plugins 目录下，以使用 flink-s3-fs-hadoop 或 flink-s3-fs-presto。
mkdir ./plugins/s3-fs-presto cp ./opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar ./plugins/s3-fs-presto/ 配置访问凭据 # 在设置好 S3 文件系统包装器后，您需要确认 Flink 具有访问 S3 Bucket 的权限。
Identity and Access Management (IAM)（推荐使用） # 建议通过 Identity and Access Management (IAM) 来配置 AWS 凭据。可使用 IAM 功能为 Flink 实例安全地提供访问 S3 Bucket 所需的凭据。关于配置的细节超出了本文档的范围，请参考 AWS 用户手册中的 IAM Roles 部分。
如果配置正确，则可在 AWS 中管理对 S3 的访问，而无需为 Flink 分发任何访问密钥（Access Key）。
访问密钥（Access Key）（不推荐） # 可以通过**访问密钥对（access and secret key）**授予 S3 访问权限。请注意，根据 Introduction of IAM roles，不推荐使用该方法。
s3.access-key 和 s3.secret-key 均需要在 Flink 的 flink-conf.yaml 中进行配置：
s3.access-key: your-access-key s3.secret-key: your-secret-key 配置非 S3 访问点 # S3 文件系统还支持兼容 S3 的对象存储服务，如 IBM\u0026rsquo;s Cloud Object Storage 和 Minio。可在 flink-conf.yaml 中配置使用的访问点：
s3.endpoint: your-endpoint-hostname 配置路径样式的访问 # 某些兼容 S3 的对象存储服务可能没有默认启用虚拟主机样式的寻址。这种情况下需要在 flink-conf.yaml 中添加配置以启用路径样式的访问：
s3.path.style.access: true S3 文件系统的熵注入 # 内置的 S3 文件系统 (flink-s3-fs-presto and flink-s3-fs-hadoop) 支持熵注入。熵注入是通过在关键字开头附近添加随机字符，以提高 AWS S3 bucket 可扩展性的技术。
如果熵注入被启用，路径中配置好的字串将会被随机字符所替换。例如路径 s3://my-bucket/_entropy_/checkpoints/dashboard-job/ 将会被替换成类似于 s3://my-bucket/gf36ikvg/checkpoints/dashboard-job/ 的路径。 这仅在使用熵注入选项创建文件时启用！ 否则将完全删除文件路径中的 entropy key。更多细节请参见 FileSystem.create(Path, WriteOption)。
目前 Flink 运行时仅对 checkpoint 数据文件使用熵注入选项。所有其他文件包括 checkpoint 元数据与外部 URI 都不使用熵注入，以保证 checkpoint URI 的可预测性。 配置 entropy key 与 entropy length 参数以启用熵注入：
s3.entropy.key: _entropy_ s3.entropy.length: 4 (default) s3.entropy.key 定义了路径中被随机字符替换掉的字符串。不包含 entropy key 路径将保持不变。 如果文件系统操作没有经过 \u0026ldquo;熵注入\u0026rdquo; 写入，entropy key 字串将被直接移除。 s3.entropy.length 定义了用于熵注入的随机字母/数字字符的数量。
Back to top
`}),e.add({id:33,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/csv/",title:"CSV",section:"Formats",content:" CSV Format # Format: Serialization Schema Format: Deserialization Schema\nCSV Format 允许我们基于 CSV schema 进行解析和生成 CSV 数据。 目前 CSV schema 是基于 table schema 推断而来的。\n依赖 # In order to use the CSV format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nMaven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-csv\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in 如何创建使用 CSV 格式的表 # 以下是一个使用 Kafka 连接器和 CSV 格式创建表的示例。\nCREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;csv.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;csv.allow-comments\u0026#39; = \u0026#39;true\u0026#39; ) Format 参数 # 参数 是否必选 默认值 类型 描述 format 必选 (none) String 指定要使用的格式，这里应该是 'csv'。 csv.field-delimiter 可选 , String 字段分隔符 (默认',')，必须为单字符。你可以使用反斜杠字符指定一些特殊字符，例如 '\\t' 代表制表符。 你也可以通过 unicode 编码在纯 SQL 文本中指定一些特殊字符，例如 'csv.field-delimiter' = U\u0026'\\0001' 代表 0x01 字符。 csv.disable-quote-character 可选 false Boolean 是否禁止对引用的值使用引号 (默认是 false). 如果禁止，选项 'csv.quote-character' 不能设置。 csv.quote-character 可选 \" String 用于围住字段值的引号字符 (默认\"). csv.allow-comments 可选 false Boolean 是否允许忽略注释行（默认不允许），注释行以 '#' 作为起始字符。 如果允许注释行，请确保 csv.ignore-parse-errors 也开启了从而允许空行。 csv.ignore-parse-errors 可选 false Boolean 当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null。 csv.array-element-delimiter 可选 ; String 分隔数组和行元素的字符串(默认';'). csv.escape-character 可选 (none) String 转义字符(默认关闭). csv.null-literal 可选 (none) String 是否将 \"null\" 字符串转化为 null 值。 csv.write-bigdecimal-in-scientific-notation 可选 true Boolean 设置将 Bigdecimal 类型的数据表示为科学计数法（默认为true，即需要转为科学计数法），例如一个BigDecimal的值为100000，设置true，结果为 '1E+5'；设置为false，结果为 100000。注意：只有当值不等于0且是10的倍数才会转为科学计数法 数据类型映射 # 目前 CSV 的 schema 都是从 table schema 推断而来的。显式地定义 CSV schema 暂不支持。 Flink 的 CSV Format 数据使用 jackson databind API 去解析 CSV 字符串。\n下面的表格列出了flink数据和CSV数据的对应关系。\nFlink SQL 类型 CSV 类型 CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY string with encoding: base64 DECIMAL number TINYINT number SMALLINT number INT number BIGINT number FLOAT number DOUBLE number DATE string with format: date TIME string with format: time TIMESTAMP string with format: date-time INTERVAL number ARRAY array ROW object "}),e.add({id:34,href:"/flink/flink-docs-master/zh/docs/dev/datastream/",title:"DataStream API",section:"应用开发",content:" "}),e.add({id:35,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/",title:"Formats",section:"DataStream Connectors",content:" "}),e.add({id:36,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/",title:"Formats",section:"Table API Connectors",content:""}),e.add({id:37,href:"/flink/flink-docs-master/zh/docs/libs/gelly/graph_api/",title:"Graph API",section:"Graphs",content:` Graph API # Graph Representation # In Gelly, a Graph is represented by a DataSet of vertices and a DataSet of edges.
The Graph nodes are represented by the Vertex type. A Vertex is defined by a unique ID and a value. Vertex IDs should implement the Comparable interface. Vertices without value can be represented by setting the value type to NullValue.
Java // create a new vertex with a Long ID and a String value Vertex\u0026lt;Long, String\u0026gt; v = new Vertex\u0026lt;Long, String\u0026gt;(1L, \u0026#34;foo\u0026#34;); // create a new vertex with a Long ID and no value Vertex\u0026lt;Long, NullValue\u0026gt; v = new Vertex\u0026lt;Long, NullValue\u0026gt;(1L, NullValue.getInstance()); Scala // create a new vertex with a Long ID and a String value val v = new Vertex(1L, \u0026#34;foo\u0026#34;) // create a new vertex with a Long ID and no value val v = new Vertex(1L, NullValue.getInstance()) The graph edges are represented by the Edge type. An Edge is defined by a source ID (the ID of the source Vertex), a target ID (the ID of the target Vertex) and an optional value. The source and target IDs should be of the same type as the Vertex IDs. Edges with no value have a NullValue value type.
Java Edge\u0026lt;Long, Double\u0026gt; e = new Edge\u0026lt;Long, Double\u0026gt;(1L, 2L, 0.5); // reverse the source and target of this edge Edge\u0026lt;Long, Double\u0026gt; reversed = e.reverse(); Double weight = e.getValue(); // weight = 0.5 Scala val e = new Edge(1L, 2L, 0.5) // reverse the source and target of this edge val reversed = e.reverse val weight = e.getValue // weight = 0.5 In Gelly an Edge is always directed from the source vertex to the target vertex. A Graph may be undirected if for every Edge it contains a matching Edge from the target vertex to the source vertex.
Back to top
Graph Creation # You can create a Graph in the following ways:
from a DataSet of edges and an optional DataSet of vertices: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; vertices = ...; DataSet\u0026lt;Edge\u0026lt;String, Double\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertices: DataSet[Vertex[String, Long]] = ... val edges: DataSet[Edge[String, Double]] = ... val graph = Graph.fromDataSet(vertices, edges, env) from a DataSet of Tuple2 representing the edges. Gelly will convert each Tuple2 to an Edge, where the first field will be the source ID and the second field will be the target ID. Both vertex and edge values will be set to NullValue. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; edges = ... Graph\u0026lt;String, NullValue, NullValue\u0026gt; graph = Graph.fromTuple2DataSet(edges, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val edges: DataSet[(String, String)] = ... val graph = Graph.fromTuple2DataSet(edges, env) from a DataSet of Tuple3 and an optional DataSet of Tuple2. In this case, Gelly will convert each Tuple3 to an Edge, where the first field will be the source ID, the second field will be the target ID and the third field will be the edge value. Equivalently, each Tuple2 will be converted to a Vertex, where the first field will be the vertex ID and the second field will be the vertex value: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; vertexTuples = env.readCsvFile(\u0026#34;path/to/vertex/input\u0026#34;).types(String.class, Long.class); DataSet\u0026lt;Tuple3\u0026lt;String, String, Double\u0026gt;\u0026gt; edgeTuples = env.readCsvFile(\u0026#34;path/to/edge/input\u0026#34;).types(String.class, String.class, Double.class); Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env); from a CSV file of Edge data and an optional CSV file of Vertex data. In this case, Gelly will convert each row from the Edge CSV file to an Edge, where the first field will be the source ID, the second field will be the target ID and the third field (if present) will be the edge value. Equivalently, each row from the optional Vertex CSV file will be converted to a Vertex, where the first field will be the vertex ID and the second field (if present) will be the vertex value. In order to get a Graph from a GraphCsvReader one has to specify the types, using one of the following methods: types(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;VV\u0026gt; vertexValue,Class\u0026lt;EV\u0026gt; edgeValue): both vertex and edge values are present. edgeTypes(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;EV\u0026gt; edgeValue): the Graph has edge values, but no vertex values. vertexTypes(Class\u0026lt;K\u0026gt; vertexKey, Class\u0026lt;VV\u0026gt; vertexValue): the Graph has vertex values, but no edge values. keyType(Class\u0026lt;K\u0026gt; vertexKey): the Graph has no vertex values and no edge values. ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a Graph with String Vertex IDs, Long Vertex values and Double Edge values Graph\u0026lt;String, Long, Double\u0026gt; graph = Graph.fromCsvReader(\u0026#34;path/to/vertex/input\u0026#34;, \u0026#34;path/to/edge/input\u0026#34;, env) .types(String.class, Long.class, Double.class); // create a Graph with neither Vertex nor Edge values Graph\u0026lt;Long, NullValue, NullValue\u0026gt; simpleGraph = Graph.fromCsvReader(\u0026#34;path/to/edge/input\u0026#34;, env).keyType(Long.class); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertexTuples = env.readCsvFile[String, Long](\u0026#34;path/to/vertex/input\u0026#34;) val edgeTuples = env.readCsvFile[String, String, Double](\u0026#34;path/to/edge/input\u0026#34;) val graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env) from a CSV file of Edge data and an optional CSV file of Vertex data. In this case, Gelly will convert each row from the Edge CSV file to an Edge. The first field of the each row will be the source ID, the second field will be the target ID and the third field (if present) will be the edge value. If the edges have no associated value, set the edge value type parameter (3rd type argument) to NullValue. You can also specify that the vertices are initialized with a vertex value. If you provide a path to a CSV file via pathVertices, each row of this file will be converted to a Vertex. The first field of each row will be the vertex ID and the second field will be the vertex value. If you provide a vertex value initializer MapFunction via the vertexValueInitializer parameter, then this function is used to generate the vertex values. The set of vertices will be created automatically from the edges input. If the vertices have no associated value, set the vertex value type parameter (2nd type argument) to NullValue. The vertices will then be automatically created from the edges input with vertex value of type NullValue. val env = ExecutionEnvironment.getExecutionEnvironment // create a Graph with String Vertex IDs, Long Vertex values and Double Edge values val graph = Graph.fromCsvReader[String, Long, Double]( pathVertices = \u0026#34;path/to/vertex/input\u0026#34;, pathEdges = \u0026#34;path/to/edge/input\u0026#34;, env = env) // create a Graph with neither Vertex nor Edge values val simpleGraph = Graph.fromCsvReader[Long, NullValue, NullValue]( pathEdges = \u0026#34;path/to/edge/input\u0026#34;, env = env) // create a Graph with Double Vertex values generated by a vertex value initializer and no Edge values val simpleGraph = Graph.fromCsvReader[Long, Double, NullValue]( pathEdges = \u0026#34;path/to/edge/input\u0026#34;, vertexValueInitializer = new MapFunction[Long, Double]() { def map(id: Long): Double = { id.toDouble } }, env = env) from a Collection of edges and an optional Collection of vertices: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); List\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; vertexList = new ArrayList...; List\u0026lt;Edge\u0026lt;Long, String\u0026gt;\u0026gt; edgeList = new ArrayList...; Graph\u0026lt;Long, Long, String\u0026gt; graph = Graph.fromCollection(vertexList, edgeList, env); If no vertex input is provided during Graph creation, Gelly will automatically produce the Vertex DataSet from the edge input. In this case, the created vertices will have no values. Alternatively, you can provide a MapFunction as an argument to the creation method, in order to initialize the Vertex values:
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // initialize the vertex value to be equal to the vertex ID Graph\u0026lt;Long, Long, String\u0026gt; graph = Graph.fromCollection(edgeList, new MapFunction\u0026lt;Long, Long\u0026gt;() { public Long map(Long value) { return value; } }, env); Scala val env = ExecutionEnvironment.getExecutionEnvironment val vertexList = List(...) val edgeList = List(...) val graph = Graph.fromCollection(vertexList, edgeList, env) If no vertex input is provided during Graph creation, Gelly will automatically produce the Vertex DataSet from the edge input. In this case, the created vertices will have no values. Alternatively, you can provide a MapFunction as an argument to the creation method, in order to initialize the Vertex values:
val env = ExecutionEnvironment.getExecutionEnvironment // initialize the vertex value to be equal to the vertex ID val graph = Graph.fromCollection(edgeList, new MapFunction[Long, Long] { def map(id: Long): Long = id }, env) Back to top
Graph Properties # Gelly includes the following methods for retrieving various Graph properties and metrics:
Java // get the Vertex DataSet DataSet\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; getVertices() // get the Edge DataSet DataSet\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; getEdges() // get the IDs of the vertices as a DataSet DataSet\u0026lt;K\u0026gt; getVertexIds() // get the source-target pairs of the edge IDs as a DataSet DataSet\u0026lt;Tuple2\u0026lt;K, K\u0026gt;\u0026gt; getEdgeIds() // get a DataSet of \u0026lt;vertex ID, in-degree\u0026gt; pairs for all vertices DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; inDegrees() // get a DataSet of \u0026lt;vertex ID, out-degree\u0026gt; pairs for all vertices DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; outDegrees() // get a DataSet of \u0026lt;vertex ID, degree\u0026gt; pairs for all vertices, where degree is the sum of in- and out- degrees DataSet\u0026lt;Tuple2\u0026lt;K, LongValue\u0026gt;\u0026gt; getDegrees() // get the number of vertices long numberOfVertices() // get the number of edges long numberOfEdges() // get a DataSet of Triplets\u0026lt;srcVertex, trgVertex, edge\u0026gt; DataSet\u0026lt;Triplet\u0026lt;K, VV, EV\u0026gt;\u0026gt; getTriplets() Scala // get the Vertex DataSet getVertices: DataSet[Vertex[K, VV]] // get the Edge DataSet getEdges: DataSet[Edge[K, EV]] // get the IDs of the vertices as a DataSet getVertexIds: DataSet[K] // get the source-target pairs of the edge IDs as a DataSet getEdgeIds: DataSet[(K, K)] // get a DataSet of \u0026lt;vertex ID, in-degree\u0026gt; pairs for all vertices inDegrees: DataSet[(K, LongValue)] // get a DataSet of \u0026lt;vertex ID, out-degree\u0026gt; pairs for all vertices outDegrees: DataSet[(K, LongValue)] // get a DataSet of \u0026lt;vertex ID, degree\u0026gt; pairs for all vertices, where degree is the sum of in- and out- degrees getDegrees: DataSet[(K, LongValue)] // get the number of vertices numberOfVertices: Long // get the number of edges numberOfEdges: Long // get a DataSet of Triplets\u0026lt;srcVertex, trgVertex, edge\u0026gt; getTriplets: DataSet[Triplet[K, VV, EV]] Back to top
Graph Transformations # Map: Gelly provides specialized methods for applying a map transformation on the vertex values or edge values. mapVertices and mapEdges return a new Graph, where the IDs of the vertices (or edges) remain unchanged, while the values are transformed according to the provided user-defined map function. The map functions also allow changing the type of the vertex or edge values. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); // increment each vertex value by one Graph\u0026lt;Long, Long, Long\u0026gt; updatedGraph = graph.mapVertices( new MapFunction\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Long\u0026gt;() { public Long map(Vertex\u0026lt;Long, Long\u0026gt; value) { return value.getValue() + 1; } }); Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph = Graph.fromDataSet(vertices, edges, env) // increment each vertex value by one val updatedGraph = graph.mapVertices(v =\u0026gt; v.getValue + 1) Translate: Gelly provides specialized methods for translating the value and/or type of vertex and edge IDs (translateGraphIDs), vertex values (translateVertexValues), or edge values (translateEdgeValues). Translation is performed by the user-defined map function, several of which are provided in the org.apache.flink.graph.asm.translate package. The same MapFunction can be used for all the three translate methods. Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromDataSet(vertices, edges, env); // translate each vertex and edge ID to a String Graph\u0026lt;String, Long, Long\u0026gt; updatedGraph = graph.translateGraphIds( new MapFunction\u0026lt;Long, String\u0026gt;() { public String map(Long id) { return id.toString(); } }); // translate vertex IDs, edge IDs, vertex values, and edge values to LongValue Graph\u0026lt;LongValue, LongValue, LongValue\u0026gt; updatedGraph = graph .translateGraphIds(new LongToLongValue()) .translateVertexValues(new LongToLongValue()) .translateEdgeValues(new LongToLongValue()) Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph = Graph.fromDataSet(vertices, edges, env) // translate each vertex and edge ID to a String val updatedGraph = graph.translateGraphIds(id =\u0026gt; id.toString) Filter: A filter transformation applies a user-defined filter function on the vertices or edges of the Graph. filterOnEdges will create a sub-graph of the original graph, keeping only the edges that satisfy the provided predicate. Note that the vertex dataset will not be modified. Respectively, filterOnVertices applies a filter on the vertices of the graph. Edges whose source and/or target do not satisfy the vertex predicate are removed from the resulting edge dataset. The subgraph method can be used to apply a filter function to the vertices and the edges at the same time. Java Graph\u0026lt;Long, Long, Long\u0026gt; graph = ...; graph.subgraph( new FilterFunction\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;() { public boolean filter(Vertex\u0026lt;Long, Long\u0026gt; vertex) { // keep only vertices with positive values return (vertex.getValue() \u0026gt; 0); } }, new FilterFunction\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt;() { public boolean filter(Edge\u0026lt;Long, Long\u0026gt; edge) { // keep only edges with negative values return (edge.getValue() \u0026lt; 0); } }) Scala val graph: Graph[Long, Long, Long] = ... // keep only vertices with positive values // and only edges with negative values graph.subgraph((vertex =\u0026gt; vertex.getValue \u0026gt; 0), (edge =\u0026gt; edge.getValue \u0026lt; 0)) Join: Gelly provides specialized methods for joining the vertex and edge datasets with other input datasets. joinWithVertices joins the vertices with a Tuple2 input data set. The join is performed using the vertex ID and the first field of the Tuple2 input as the join keys. The method returns a new Graph where the vertex values have been updated according to a provided user-defined transformation function. Similarly, an input dataset can be joined with the edges, using one of three methods. joinWithEdges expects an input DataSet of Tuple3 and joins on the composite key of both source and target vertex IDs. joinWithEdgesOnSource expects a DataSet of Tuple2 and joins on the source key of the edges and the first attribute of the input dataset and joinWithEdgesOnTarget expects a DataSet of Tuple2 and joins on the target key of the edges and the first attribute of the input dataset. All three methods apply a transformation function on the edge and the input data set values. Note that if the input dataset contains a key multiple times, all Gelly join methods will only consider the first value encountered. Java Graph\u0026lt;Long, Double, Double\u0026gt; network = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, LongValue\u0026gt;\u0026gt; vertexOutDegrees = network.outDegrees(); // assign the transition probabilities as the edge weights Graph\u0026lt;Long, Double, Double\u0026gt; networkWithWeights = network.joinWithEdgesOnSource(vertexOutDegrees, new VertexJoinFunction\u0026lt;Double, LongValue\u0026gt;() { public Double vertexJoin(Double vertexValue, LongValue inputValue) { return vertexValue / inputValue.getValue(); } }); Scala val network: Graph[Long, Double, Double] = ... val vertexOutDegrees: DataSet[(Long, LongValue)] = network.outDegrees // assign the transition probabilities as the edge weights val networkWithWeights = network.joinWithEdgesOnSource(vertexOutDegrees, (v1: Double, v2: LongValue) =\u0026gt; v1 / v2.getValue) Reverse: the reverse() method returns a new Graph where the direction of all edges has been reversed.
Undirected: In Gelly, a Graph is always directed. Undirected graphs can be represented by adding all opposite-direction edges to a graph. For this purpose, Gelly provides the getUndirected() method.
Union: Gelly\u0026rsquo;s union() method performs a union operation on the vertex and edge sets of the specified graph and the current graph. Duplicate vertices are removed from the resulting Graph, while if duplicate edges exist, these will be preserved.
Difference: Gelly\u0026rsquo;s difference() method performs a difference on the vertex and edge sets of the current graph and the specified graph.
Intersect: Gelly\u0026rsquo;s intersect() method performs an intersect on the edge sets of the current graph and the specified graph. The result is a new Graph that contains all edges that exist in both input graphs. Two edges are considered equal, if they have the same source identifier, target identifier and edge value. Vertices in the resulting graph have no value. If vertex values are required, one can for example retrieve them from one of the input graphs using the joinWithVertices() method. Depending on the parameter distinct, equal edges are either contained once in the resulting Graph or as often as there are pairs of equal edges in the input graphs.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create first graph from edges {(1, 3, 12) (1, 3, 13), (1, 3, 13)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges1 = ...; Graph\u0026lt;Long, NullValue, Long\u0026gt; graph1 = Graph.fromCollection(edges1, env); // create second graph from edges {(1, 3, 13)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges2 = ...; Graph\u0026lt;Long, NullValue, Long\u0026gt; graph2 = Graph.fromCollection(edges2, env); // Using distinct = true results in {(1,3,13)} Graph\u0026lt;Long, NullValue, Long\u0026gt; intersect1 = graph1.intersect(graph2, true); // Using distinct = false results in {(1,3,13),(1,3,13)} as there is one edge pair Graph\u0026lt;Long, NullValue, Long\u0026gt; intersect2 = graph1.intersect(graph2, false); Scala val env = ExecutionEnvironment.getExecutionEnvironment // create first graph from edges {(1, 3, 12) (1, 3, 13), (1, 3, 13)} val edges1: List[Edge[Long, Long]] = ... val graph1 = Graph.fromCollection(edges1, env) // create second graph from edges {(1, 3, 13)} val edges2: List[Edge[Long, Long]] = ... val graph2 = Graph.fromCollection(edges2, env) // Using distinct = true results in {(1,3,13)} val intersect1 = graph1.intersect(graph2, true) // Using distinct = false results in {(1,3,13),(1,3,13)} as there is one edge pair val intersect2 = graph1.intersect(graph2, false) - Back to top
Graph Mutations # Gelly includes the following methods for adding and removing vertices and edges from an input Graph:
Java // adds a Vertex to the Graph. If the Vertex already exists, it will not be added again. Graph\u0026lt;K, VV, EV\u0026gt; addVertex(final Vertex\u0026lt;K, VV\u0026gt; vertex) // adds a list of vertices to the Graph. If the vertices already exist in the graph, they will not be added once more. Graph\u0026lt;K, VV, EV\u0026gt; addVertices(List\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; verticesToAdd) // adds an Edge to the Graph. If the source and target vertices do not exist in the graph, they will also be added. Graph\u0026lt;K, VV, EV\u0026gt; addEdge(Vertex\u0026lt;K, VV\u0026gt; source, Vertex\u0026lt;K, VV\u0026gt; target, EV edgeValue) // adds a list of edges to the Graph. When adding an edge for a non-existing set of vertices, the edge is considered invalid and ignored. Graph\u0026lt;K, VV, EV\u0026gt; addEdges(List\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; newEdges) // removes the given Vertex and its edges from the Graph. Graph\u0026lt;K, VV, EV\u0026gt; removeVertex(Vertex\u0026lt;K, VV\u0026gt; vertex) // removes the given list of vertices and their edges from the Graph Graph\u0026lt;K, VV, EV\u0026gt; removeVertices(List\u0026lt;Vertex\u0026lt;K, VV\u0026gt;\u0026gt; verticesToBeRemoved) // removes *all* edges that match the given Edge from the Graph. Graph\u0026lt;K, VV, EV\u0026gt; removeEdge(Edge\u0026lt;K, EV\u0026gt; edge) // removes *all* edges that match the edges in the given list Graph\u0026lt;K, VV, EV\u0026gt; removeEdges(List\u0026lt;Edge\u0026lt;K, EV\u0026gt;\u0026gt; edgesToBeRemoved) Scala // adds a Vertex to the Graph. If the Vertex already exists, it will not be added again. addVertex(vertex: Vertex[K, VV]) // adds a list of vertices to the Graph. If the vertices already exist in the graph, they will not be added once more. addVertices(verticesToAdd: List[Vertex[K, VV]]) // adds an Edge to the Graph. If the source and target vertices do not exist in the graph, they will also be added. addEdge(source: Vertex[K, VV], target: Vertex[K, VV], edgeValue: EV) // adds a list of edges to the Graph. When adding an edge for a non-existing set of vertices, the edge is considered invalid and ignored. addEdges(edges: List[Edge[K, EV]]) // removes the given Vertex and its edges from the Graph. removeVertex(vertex: Vertex[K, VV]) // removes the given list of vertices and their edges from the Graph removeVertices(verticesToBeRemoved: List[Vertex[K, VV]]) // removes *all* edges that match the given Edge from the Graph. removeEdge(edge: Edge[K, EV]) // removes *all* edges that match the edges in the given list removeEdges(edgesToBeRemoved: List[Edge[K, EV]]) Neighborhood Methods # Neighborhood methods allow vertices to perform an aggregation on their first-hop neighborhood. reduceOnEdges() can be used to compute an aggregation on the values of the neighboring edges of a vertex and reduceOnNeighbors() can be used to compute an aggregation on the values of the neighboring vertices. These methods assume associative and commutative aggregations and exploit combiners internally, significantly improving performance. The neighborhood scope is defined by the EdgeDirection parameter, which takes the values IN, OUT or ALL. IN will gather all in-coming edges (neighbors) of a vertex, OUT will gather all out-going edges (neighbors), while ALL will gather all edges (neighbors).
For example, assume that you want to select the minimum weight of all out-edges for each vertex in the following graph:
The following code will collect the out-edges for each vertex and apply the SelectMinWeight() user-defined function on each of the resulting neighborhoods:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; minWeights = graph.reduceOnEdges(new SelectMinWeight(), EdgeDirection.OUT); // user-defined function to select the minimum weight static final class SelectMinWeight implements ReduceEdgesFunction\u0026lt;Double\u0026gt; { @Override public Double reduceEdges(Double firstEdgeValue, Double secondEdgeValue) { return Math.min(firstEdgeValue, secondEdgeValue); } } Scala val graph: Graph[Long, Long, Double] = ... val minWeights = graph.reduceOnEdges(new SelectMinWeight, EdgeDirection.OUT) // user-defined function to select the minimum weight final class SelectMinWeight extends ReduceEdgesFunction[Double] { override def reduceEdges(firstEdgeValue: Double, secondEdgeValue: Double): Double = { Math.min(firstEdgeValue, secondEdgeValue) } } Similarly, assume that you would like to compute the sum of the values of all in-coming neighbors, for every vertex. The following code will collect the in-coming neighbors for each vertex and apply the SumValues() user-defined function on each neighborhood:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithSum = graph.reduceOnNeighbors(new SumValues(), EdgeDirection.IN); // user-defined function to sum the neighbor values static final class SumValues implements ReduceNeighborsFunction\u0026lt;Long\u0026gt; { @Override public Long reduceNeighbors(Long firstNeighbor, Long secondNeighbor) { return firstNeighbor + secondNeighbor; } } Scala val graph: Graph[Long, Long, Double] = ... val verticesWithSum = graph.reduceOnNeighbors(new SumValues, EdgeDirection.IN) // user-defined function to sum the neighbor values final class SumValues extends ReduceNeighborsFunction[Long] { override def reduceNeighbors(firstNeighbor: Long, secondNeighbor: Long): Long = { firstNeighbor + secondNeighbor } } When the aggregation function is not associative and commutative or when it is desirable to return more than one values per vertex, one can use the more general groupReduceOnEdges() and groupReduceOnNeighbors() methods. These methods return zero, one or more values per vertex and provide access to the whole neighborhood.
For example, the following code will output all the vertex pairs which are connected with an edge having a weight of 0.5 or more:
Java Graph\u0026lt;Long, Long, Double\u0026gt; graph = ...; DataSet\u0026lt;Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; vertexPairs = graph.groupReduceOnNeighbors(new SelectLargeWeightNeighbors(), EdgeDirection.OUT); // user-defined function to select the neighbors which have edges with weight \u0026gt; 0.5 static final class SelectLargeWeightNeighbors implements NeighborsFunctionWithVertexValue\u0026lt;Long, Long, Double, Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; { @Override public void iterateNeighbors(Vertex\u0026lt;Long, Long\u0026gt; vertex, Iterable\u0026lt;Tuple2\u0026lt;Edge\u0026lt;Long, Double\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; neighbors, Collector\u0026lt;Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; out) { for (Tuple2\u0026lt;Edge\u0026lt;Long, Double\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; neighbor : neighbors) { if (neighbor.f0.f2 \u0026gt; 0.5) { out.collect(new Tuple2\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;, Vertex\u0026lt;Long, Long\u0026gt;\u0026gt;(vertex, neighbor.f1)); } } } } Scala val graph: Graph[Long, Long, Double] = ... val vertexPairs = graph.groupReduceOnNeighbors(new SelectLargeWeightNeighbors, EdgeDirection.OUT) // user-defined function to select the neighbors which have edges with weight \u0026gt; 0.5 final class SelectLargeWeightNeighbors extends NeighborsFunctionWithVertexValue[Long, Long, Double, (Vertex[Long, Long], Vertex[Long, Long])] { override def iterateNeighbors(vertex: Vertex[Long, Long], neighbors: Iterable[(Edge[Long, Double], Vertex[Long, Long])], out: Collector[(Vertex[Long, Long], Vertex[Long, Long])]) = { for (neighbor \u0026lt;- neighbors) { if (neighbor._1.getValue() \u0026gt; 0.5) { out.collect(vertex, neighbor._2) } } } } When the aggregation computation does not require access to the vertex value (for which the aggregation is performed), it is advised to use the more efficient EdgesFunction and NeighborsFunction for the user-defined functions. When access to the vertex value is required, one should use EdgesFunctionWithVertexValue and NeighborsFunctionWithVertexValue instead.
Back to top
Graph Validation # Gelly provides a simple utility for performing validation checks on input graphs. Depending on the application context, a graph may or may not be valid according to certain criteria. For example, a user might need to validate whether their graph contains duplicate edges or whether its structure is bipartite. In order to validate a graph, one can define a custom GraphValidator and implement its validate() method. InvalidVertexIdsValidator is Gelly\u0026rsquo;s pre-defined validator. It checks that the edge set contains valid vertex IDs, i.e. that all edge IDs also exist in the vertex IDs set.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a list of vertices with IDs = {1, 2, 3, 4, 5} List\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; vertices = ...; // create a list of edges with IDs = {(1, 2) (1, 3), (2, 4), (5, 6)} List\u0026lt;Edge\u0026lt;Long, Long\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;Long, Long, Long\u0026gt; graph = Graph.fromCollection(vertices, edges, env); // will return false: 6 is an invalid ID graph.validate(new InvalidVertexIdsValidator\u0026lt;Long, Long, Long\u0026gt;()); Scala val env = ExecutionEnvironment.getExecutionEnvironment // create a list of vertices with IDs = {1, 2, 3, 4, 5} val vertices: List[Vertex[Long, Long]] = ... // create a list of edges with IDs = {(1, 2) (1, 3), (2, 4), (5, 6)} val edges: List[Edge[Long, Long]] = ... val graph = Graph.fromCollection(vertices, edges, env) // will return false: 6 is an invalid ID graph.validate(new InvalidVertexIdsValidator[Long, Long, Long]) Back to top
`}),e.add({id:38,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/hints/",title:"Hints",section:"Queries 查询",content:` Hints # Batch Streaming
SQL hints 是和 SQL 语句一起使用来改变执行计划的。本章介绍如何使用 SQL hints 增强各种方法。
SQL hints 一般可以用于以下：
增强 planner：没有完美的 planner，所以实现 SQL hints 让用户更好地控制执行是非常有意义的； 增加元数据（或者统计信息）：如\u0026quot;已扫描的表索引\u0026quot;和\u0026quot;一些混洗键（shuffle keys）的倾斜信息\u0026quot;的一些统计数据对于查询来说是动态的，用 hints 来配置它们会非常方便，因为我们从 planner 获得的计划元数据通常不那么准确； 算子（Operator）资源约束：在许多情况下，我们会为执行算子提供默认的资源配置，即最小并行度或托管内存（UDF 资源消耗）或特殊资源需求（GPU 或 SSD 磁盘）等，可以使用 SQL hints 非常灵活地为每个查询（非作业）配置资源。 动态表（Dynamic Table）选项 # 动态表选项允许动态地指定或覆盖表选项，不同于用 SQL DDL 或 连接 API 定义的静态表选项，这些选项可以在每个查询的每个表范围内灵活地指定。
因此，它非常适合用于交互式终端中的特定查询，例如，在 SQL-CLI 中，你可以通过添加动态选项/*+ OPTIONS('csv.ignore-parse-errors'='true') */来指定忽略 CSV 源的解析错误。
语法 # 为了不破坏 SQL 兼容性，我们使用 Oracle 风格的 SQL hints 语法：
table_path /*+ OPTIONS(key=val [, key=val]*) */ key: stringLiteral val: stringLiteral 示例 # CREATE TABLE kafka_table1 (id BIGINT, name STRING, age INT) WITH (...); CREATE TABLE kafka_table2 (id BIGINT, name STRING, age INT) WITH (...); -- 覆盖查询语句中源表的选项 select id, name from kafka_table1 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */; -- 覆盖 join 中源表的选项 select * from kafka_table1 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */ t1 join kafka_table2 /*+ OPTIONS(\u0026#39;scan.startup.mode\u0026#39;=\u0026#39;earliest-offset\u0026#39;) */ t2 on t1.id = t2.id; -- 覆盖插入语句中结果表的选项 insert into kafka_table1 /*+ OPTIONS(\u0026#39;sink.partitioner\u0026#39;=\u0026#39;round-robin\u0026#39;) */ select * from kafka_table2; Back to top
`}),e.add({id:39,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_catalog/",title:"Hive Catalog",section:"Hive",content:` Hive Catalog # Hive Metastore has evolved into the de facto metadata hub over the years in Hadoop ecosystem. Many companies have a single Hive Metastore service instance in their production to manage all of their metadata, either Hive metadata or non-Hive metadata, as the source of truth.
For users who have both Hive and Flink deployments, HiveCatalog enables them to use Hive Metastore to manage Flink\u0026rsquo;s metadata.
For users who have just Flink deployment, HiveCatalog is the only persistent catalog provided out-of-box by Flink. Without a persistent catalog, users using Flink SQL CREATE DDL have to repeatedly create meta-objects like a Kafka table in each session, which wastes a lot of time. HiveCatalog fills this gap by empowering users to create tables and other meta-objects only once, and reference and manage them with convenience later on across sessions.
Set up HiveCatalog # Dependencies # Setting up a HiveCatalog in Flink requires the same dependencies as those of an overall Flink-Hive integration.
Configuration # Setting up a HiveCatalog in Flink requires the same configuration as those of an overall Flink-Hive integration.
How to use HiveCatalog # Once configured properly, HiveCatalog should just work out of box. Users can create Flink meta-objects with DDL, and should see them immediately afterwards.
HiveCatalog can be used to handle two kinds of tables: Hive-compatible tables and generic tables. Hive-compatible tables are those stored in a Hive-compatible way, in terms of both metadata and data in the storage layer. Therefore, Hive-compatible tables created via Flink can be queried from Hive side.
Generic tables, on the other hand, are specific to Flink. When creating generic tables with HiveCatalog, we\u0026rsquo;re just using HMS to persist the metadata. While these tables are visible to Hive, it\u0026rsquo;s unlikely Hive is able to understand the metadata. And therefore using such tables in Hive leads to undefined behavior.
It\u0026rsquo;s recommended to switch to Hive dialect to create Hive-compatible tables. If you want to create Hive-compatible tables with default dialect, make sure to set 'connector'='hive' in your table properties, otherwise a table is considered generic by default in HiveCatalog. Note that the connector property is not required if you use Hive dialect.
Example # We will walk through a simple example here.
step 1: set up a Hive Metastore # Have a Hive Metastore running.
Here, we set up a local Hive Metastore and our hive-site.xml file in local path /opt/hive-conf/hive-site.xml. We have some configs like the following:
\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;metadata is stored in a MySQL server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;MySQL JDBC driver class\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;user name for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;...\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;password for connecting to mysql server\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://localhost:9083\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;IP address (or fully-qualified domain name) and port of the metastore host\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Test connection to the HMS with Hive Cli. Running some commands, we can see we have a database named default and there\u0026rsquo;s no table in it.
hive\u0026gt; show databases; OK default Time taken: 0.032 seconds, Fetched: 1 row(s) hive\u0026gt; show tables; OK Time taken: 0.028 seconds, Fetched: 0 row(s) step 2: start SQL Client, and create a Hive catalog with Flink SQL DDL # Add all Hive dependencies to /lib dir in Flink distribution, and create a Hive catalog in Flink SQL CLI as following:
Flink SQL\u0026gt; CREATE CATALOG myhive WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39; ); step 3: set up a Kafka cluster # Bootstrap a local Kafka cluster with a topic named \u0026ldquo;test\u0026rdquo;, and produce some simple data to the topic as tuple of name and age.
localhost\$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test \u0026gt;tom,15 \u0026gt;john,21 These message can be seen by starting a Kafka console consumer.
localhost\$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 step 4: create a Kafka table with Flink SQL DDL # Create a simple Kafka table with Flink SQL DDL, and verify its schema.
Flink SQL\u0026gt; USE CATALOG myhive; Flink SQL\u0026gt; CREATE TABLE mykafka (name String, age Int) WITH ( \u0026#39;connector.type\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;connector.version\u0026#39; = \u0026#39;universal\u0026#39;, \u0026#39;connector.topic\u0026#39; = \u0026#39;test\u0026#39;, \u0026#39;connector.properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format.type\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;update-mode\u0026#39; = \u0026#39;append\u0026#39; ); [INFO] Table has been created. Flink SQL\u0026gt; DESCRIBE mykafka; root |-- name: STRING |-- age: INT Verify the table is also visible to Hive via Hive Cli:
hive\u0026gt; show tables; OK mykafka Time taken: 0.038 seconds, Fetched: 1 row(s) step 5: run Flink SQL to query the Kafka table # Run a simple select query from Flink SQL Client in a Flink cluster, either standalone or yarn-session.
Flink SQL\u0026gt; select * from mykafka; Produce some more messages in the Kafka topic
localhost\$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning tom,15 john,21 kitty,30 amy,24 kaiky,18 You should see results produced by Flink in SQL Client now, as:
SQL Query Result (Table) Refresh: 1 s Page: Last of 1 name age tom 15 john 21 kitty 30 amy 24 kaiky 18 Supported Types # HiveCatalog supports all Flink types for generic tables.
For Hive-compatible tables, HiveCatalog needs to map Flink data types to corresponding Hive types as described in the following table:
Flink Data Type Hive Data Type CHAR(p) CHAR(p) VARCHAR(p) VARCHAR(p) STRING STRING BOOLEAN BOOLEAN TINYINT TINYINT SMALLINT SMALLINT INT INT BIGINT LONG FLOAT FLOAT DOUBLE DOUBLE DECIMAL(p, s) DECIMAL(p, s) DATE DATE TIMESTAMP(9) TIMESTAMP BYTES BINARY ARRAY\u0026lt;T\u0026gt; LIST\u0026lt;T\u0026gt; MAP MAP ROW STRUCT Something to note about the type mapping:
Hive\u0026rsquo;s CHAR(p) has a maximum length of 255 Hive\u0026rsquo;s VARCHAR(p) has a maximum length of 65535 Hive\u0026rsquo;s MAP only supports primitive key types while Flink\u0026rsquo;s MAP can be any data type Hive\u0026rsquo;s UNION type is not supported Hive\u0026rsquo;s TIMESTAMP always has precision 9 and doesn\u0026rsquo;t support other precisions. Hive UDFs, on the other hand, can process TIMESTAMP values with a precision \u0026lt;= 9. Hive doesn\u0026rsquo;t support Flink\u0026rsquo;s TIMESTAMP_WITH_TIME_ZONE, TIMESTAMP_WITH_LOCAL_TIME_ZONE, and MULTISET Flink\u0026rsquo;s INTERVAL type cannot be mapped to Hive INTERVAL type yet `}),e.add({id:40,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/",title:"Resource Providers",section:"Deployment",content:""}),e.add({id:41,href:"/flink/flink-docs-master/zh/docs/deployment/security/security-ssl/",title:"SSL 设置",section:"Security",content:` SSL 设置 # This page provides instructions on how to enable TLS/SSL authentication and encryption for network communication with and between Flink processes. NOTE: TLS/SSL authentication is not enabled by default.
Internal and External Connectivity # When securing network connections between machines processes through authentication and encryption, Apache Flink differentiates between internal and external connectivity. Internal Connectivity refers to all connections made between Flink processes. These connections run Flink custom protocols. Users never connect directly to internal connectivity endpoints. External / REST Connectivity endpoints refers to all connections made from the outside to Flink processes. This includes the web UI and REST commands to start and control running Flink jobs/applications, including the communication of the Flink CLI with the JobManager / Dispatcher.
For more flexibility, security for internal and external connectivity can be enabled and configured separately.
Internal Connectivity # Internal connectivity includes:
Control messages: RPC between JobManager / TaskManager / Dispatcher / ResourceManager The data plane: The connections between TaskManagers to exchange data during shuffles, broadcasts, redistribution, etc. The Blob Service (distribution of libraries and other artifacts). All internal connections are SSL authenticated and encrypted. The connections use mutual authentication, meaning both server and client side of each connection need to present the certificate to each other. The certificate acts effectively as a shared secret when a dedicated CA is used to exclusively sign an internal cert. The certificate for internal communication is not needed by any other party to interact with Flink, and can be simply added to the container images, or attached to the YARN deployment.
The easiest way to realize this setup is by generating a dedicated public/private key pair and self-signed certificate for the Flink deployment. The key- and truststore are identical and contains only that key pair / certificate. An example is shown below.
In an environment where operators are constrained to use firm-wide Internal CA (cannot generated self-signed certificates), the recommendation is to still have a dedicated key pair / certificate for the Flink deployment, signed by that CA. However, the TrustStore must then also contain the CA\u0026rsquo;s public certificate tho accept the deployment\u0026rsquo;s certificate during the SSL handshake (requirement in JDK TrustStore implementation).
NOTE: Because of that, it is critical that you specify the fingerprint of the deployment certificate (security.ssl.internal.cert.fingerprint), when it is not self-signed, to pin that certificate as the only trusted certificate and prevent the TrustStore from trusting all certificates signed by that CA.
Note: Because internal connections are mutually authenticated with shared certificates, Flink can skip hostname verification. This makes container-based setups easier.
External / REST Connectivity # All external connectivity is exposed via an HTTP/REST endpoint, used for example by the web UI and the CLI:
Communication with the Dispatcher to submit jobs (session clusters) Communication with the JobMaster to inspect and modify a running job/application The REST endpoints can be configured to require SSL connections. The server will, however, accept connections from any client by default, meaning the REST endpoint does not authenticate the client.
Simple mutual authentication may be enabled by configuration if authentication of connections to the REST endpoint is required, but we recommend to deploy a \u0026ldquo;side car proxy\u0026rdquo;: Bind the REST endpoint to the loopback interface (or the pod-local interface in Kubernetes) and start a REST proxy that authenticates and forwards the requests to Flink. Examples for proxies that Flink users have deployed are Envoy Proxy or NGINX with MOD_AUTH.
The rationale behind delegating authentication to a proxy is that such proxies offer a wide variety of authentication options and thus better integration into existing infrastructures.
Queryable State # Connections to the queryable state endpoints is currently not authenticated or encrypted.
Configuring SSL # SSL can be enabled separately for internal and external connectivity:
security.ssl.internal.enabled: Enable SSL for all internal connections. security.ssl.rest.enabled: Enable SSL for REST / external connections. Note: For backwards compatibility, the security.ssl.enabled option still exists and enables SSL for both internal and REST endpoints.
For internal connectivity, you can optionally disable security for different connection types separately. When security.ssl.internal.enabled is set to true, you can set the following parameters to false to disable SSL for that particular connection type:
taskmanager.data.ssl.enabled: Data communication between TaskManagers blob.service.ssl.enabled: Transport of BLOBs from JobManager to TaskManager akka.ssl.enabled: Akka-based RPC connections between JobManager / TaskManager / ResourceManager Keystores and Truststores # The SSL configuration requires to configure a keystore and a truststore. The keystore contains the public certificate (public key) and the private key, while the truststore contains the trusted certificates or the trusted authorities. Both stores need to be set up such that the truststore trusts the keystore\u0026rsquo;s certificate.
Internal Connectivity # Because internal communication is mutually authenticated between server and client side, keystore and truststore typically refer to a dedicated certificate that acts as a shared secret. In such a setup, the certificate can use wild card hostnames or addresses. WHen using self-signed certificates, it is even possible to use the same file as keystore and truststore.
security.ssl.internal.keystore: /path/to/file.keystore security.ssl.internal.keystore-password: keystore_password security.ssl.internal.key-password: key_password security.ssl.internal.truststore: /path/to/file.truststore security.ssl.internal.truststore-password: truststore_password When using a certificate that is not self-signed, but signed by a CA, you need to use certificate pinning to allow only a a specific certificate to be trusted when establishing the connectivity.
security.ssl.internal.cert.fingerprint: 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 REST Endpoints (external connectivity) # For REST endpoints, by default the keystore is used by the server endpoint, and the truststore is used by the REST clients (including the CLI client) to accept the server\u0026rsquo;s certificate. In the case where the REST keystore has a self-signed certificate, the truststore must trust that certificate directly. If the REST endpoint uses a certificate that is signed through a proper certification hierarchy, the roots of that hierarchy should be in the trust store.
If mutual authentication is enabled, the keystore and the truststore are used by both, the server endpoint and the REST clients as with internal connectivity.
security.ssl.rest.keystore: /path/to/file.keystore security.ssl.rest.keystore-password: keystore_password security.ssl.rest.key-password: key_password security.ssl.rest.truststore: /path/to/file.truststore security.ssl.rest.truststore-password: truststore_password security.ssl.rest.authentication-enabled: false Cipher suites # The IETF RFC 7525 recommends to use a specific set of cipher suites for strong security. Because these cipher suites were not available on many setups out of the box, Flink\u0026rsquo;s default value is set to a slightly weaker but more compatible cipher suite. We recommend that SSL setups update to the stronger cipher suites, if possible, by adding the below entry to the Flink configuration:
security.ssl.algorithms: TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 If these cipher suites are not supported on your setup, you will see that Flink processes will not be able to connect to each other.
Complete List of SSL Options # Key Default Type Description security.context.factory.classes "org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory";"org.apache.flink.runtime.security.contexts.NoOpSecurityContextFactory" List\u0026lt;String\u0026gt; List of factories that should be used to instantiate a security context. If multiple are configured, Flink will use the first compatible factory. You should have a NoOpSecurityContextFactory in this list as a fallback. security.kerberos.access.hadoopFileSystems (none) List\u0026lt;String\u0026gt; A comma-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens. security.kerberos.fetch.delegation-token true Boolean Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens. security.kerberos.krb5-conf.path (none) String Specify the local location of the krb5.conf file. If defined, this conf would be mounted on the JobManager and TaskManager containers/pods for Kubernetes and Yarn. Note: The KDC defined needs to be visible from inside the containers. security.kerberos.login.contexts (none) String A comma-separated list of login contexts to provide the Kerberos credentials to (for example, \`Client,KafkaClient\` to use the credentials for ZooKeeper authentication and for Kafka authentication) security.kerberos.login.keytab (none) String Absolute path to a Kerberos keytab file that contains the user credentials. security.kerberos.login.principal (none) String Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache true Boolean Indicates whether to read from your Kerberos ticket cache. security.kerberos.relogin.period 1 min Duration The time period when keytab login happens automatically in order to always have a valid TGT. security.kerberos.tokens.renewal.retry.backoff 1 h Duration The time period how long to wait before retrying to obtain new delegation tokens after a failure. security.kerberos.tokens.renewal.time-ratio 0.75 Double Ratio of the tokens's expiration time when new credentials should be re-obtained. security.module.factory.classes "org.apache.flink.runtime.security.modules.HadoopModuleFactory";"org.apache.flink.runtime.security.modules.JaasModuleFactory";"org.apache.flink.runtime.security.modules.ZookeeperModuleFactory" List\u0026lt;String\u0026gt; List of factories that should be used to instantiate security modules. All listed modules will be installed. Keep in mind that the configured security context might rely on some modules being present. security.ssl.algorithms "TLS_RSA_WITH_AES_128_CBC_SHA" String The comma separated list of standard SSL algorithms to be supported. Read more here security.ssl.internal.cert.fingerprint (none) String The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required security.ssl.internal.close-notify-flush-timeout -1 Integer The timeout (in ms) for flushing the \`close_notify\` that was triggered by closing a channel. If the \`close_notify\` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default) security.ssl.internal.enabled false Boolean Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc). security.ssl.internal.handshake-timeout -1 Integer The timeout (in ms) during SSL handshake. (-1 = use system default) security.ssl.internal.key-password (none) String The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.session-cache-size -1 Integer The size of the cache used for storing SSL session objects. According to here, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default). security.ssl.internal.session-timeout -1 Integer The timeout (in ms) for the cached SSL session objects. (-1 = use system default) security.ssl.internal.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore-password (none) String The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.protocol "TLSv1.2" String The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list. security.ssl.provider "JDK" String The SSL engine provider to use for the ssl transport:JDK: default Java-based SSL engineOPENSSL: openSSL-based SSL engine using system librariesOPENSSL is based on netty-tcnative and comes in two flavours:dynamically linked: This will use your system's openSSL libraries (if compatible) and requires opt/flink-shaded-netty-tcnative-dynamic-*.jar to be copied to lib/statically linked: Due to potential licensing issues with openSSL (see LEGAL-393), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into lib/:
git clone https://github.com/apache/flink-shaded.git \u0026amp;\u0026amp; cd flink-shaded \u0026amp;\u0026amp; mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static security.ssl.rest.authentication-enabled false Boolean Turns on mutual SSL authentication for external communication via the REST endpoints. security.ssl.rest.cert.fingerprint (none) String The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA security.ssl.rest.enabled false Boolean Turns on SSL for external communication via the REST endpoints. security.ssl.rest.key-password (none) String The secret to decrypt the key in the keystore for Flink's external REST endpoints. security.ssl.rest.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints. security.ssl.rest.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints. security.ssl.rest.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints. security.ssl.rest.truststore-password (none) String The password to decrypt the truststore for Flink's external REST endpoints. security.ssl.verify-hostname true Boolean Flag to enable peer’s hostname verification during ssl handshake. zookeeper.sasl.disable false Boolean zookeeper.sasl.login-context-name "Client" String zookeeper.sasl.service-name "zookeeper" String Creating and Deploying Keystores and Truststores # Keys, Certificates, and the Keystores and Truststores can be generatedd using the keytool utility. You need to have an appropriate Java Keystore and Truststore accessible from each node in the Flink cluster.
For standalone setups, this means copying the files to each node, or adding them to a shared mounted directory. For container based setups, add the keystore and truststore files to the container images. For Yarn setups, the cluster deployment phase can automatically distribute the keystore and truststore files. For the externally facing REST endpoint, the common name or subject alternative names in the certificate should match the node\u0026rsquo;s hostname and IP address.
Example SSL Setup Standalone and Kubernetes # Internal Connectivity
Execute the following keytool commands to create a key pair in a keystore:
\$ keytool -genkeypair \\ -alias flink.internal \\ -keystore internal.keystore \\ -dname \u0026#34;CN=flink.internal\u0026#34; \\ -storepass internal_store_password \\ -keyalg RSA \\ -keysize 4096 \\ -storetype PKCS12 The single key/certificate in the keystore is used the same way by the server and client endpoints (mutual authentication). The key pair acts as the shared secret for internal security, and we can directly use it as keystore and truststore.
security.ssl.internal.enabled: true security.ssl.internal.keystore: /path/to/flink/conf/internal.keystore security.ssl.internal.truststore: /path/to/flink/conf/internal.keystore security.ssl.internal.keystore-password: internal_store_password security.ssl.internal.truststore-password: internal_store_password security.ssl.internal.key-password: internal_store_password REST Endpoint
The REST endpoint may receive connections from external processes, including tools that are not part of Flink (for example curl request to the REST API). Setting up a proper certificate that is signed though a CA hierarchy may make sense for the REST endpoint.
However, as mentioned above, the REST endpoint does not authenticate clients and thus typically needs to be secured via a proxy anyways.
REST Endpoint (simple self signed certificate)
This example shows how to create a simple keystore / truststore pair. The truststore does not contain the primary key and can be shared with other applications. In this example, myhost.company.org / ip:10.0.2.15 is the node (or service) for the JobManager.
\$ keytool -genkeypair -alias flink.rest -keystore rest.keystore -dname \u0026#34;CN=myhost.company.org\u0026#34; -ext \u0026#34;SAN=dns:myhost.company.org,ip:10.0.2.15\u0026#34; -storepass rest_keystore_password -keyalg RSA -keysize 4096 -storetype PKCS12 \$ keytool -exportcert -keystore rest.keystore -alias flink.rest -storepass rest_keystore_password -file flink.cer \$ keytool -importcert -keystore rest.truststore -alias flink.rest -storepass rest_truststore_password -file flink.cer -noprompt security.ssl.rest.enabled: true security.ssl.rest.keystore: /path/to/flink/conf/rest.keystore security.ssl.rest.truststore: /path/to/flink/conf/rest.truststore security.ssl.rest.keystore-password: rest_keystore_password security.ssl.rest.truststore-password: rest_truststore_password security.ssl.rest.key-password: rest_keystore_password REST Endpoint (with a self signed CA)
Execute the following keytool commands to create a truststore with a self signed CA.
\$ keytool -genkeypair -alias ca -keystore ca.keystore -dname \u0026#34;CN=Sample CA\u0026#34; -storepass ca_keystore_password -keyalg RSA -keysize 4096 -ext \u0026#34;bc=ca:true\u0026#34; -storetype PKCS12 \$ keytool -exportcert -keystore ca.keystore -alias ca -storepass ca_keystore_password -file ca.cer \$ keytool -importcert -keystore ca.truststore -alias ca -storepass ca_truststore_password -file ca.cer -noprompt Now create a keystore for the REST endpoint with a certificate signed by the above CA. Let flink.company.org / ip:10.0.2.15 be the hostname of the JobManager.
\$ keytool -genkeypair -alias flink.rest -keystore rest.signed.keystore -dname \u0026#34;CN=flink.company.org\u0026#34; -ext \u0026#34;SAN=dns:flink.company.org\u0026#34; -storepass rest_keystore_password -keyalg RSA -keysize 4096 -storetype PKCS12 \$ keytool -certreq -alias flink.rest -keystore rest.signed.keystore -storepass rest_keystore_password -file rest.csr \$ keytool -gencert -alias ca -keystore ca.keystore -storepass ca_keystore_password -ext \u0026#34;SAN=dns:flink.company.org,ip:10.0.2.15\u0026#34; -infile rest.csr -outfile rest.cer \$ keytool -importcert -keystore rest.signed.keystore -storepass rest_keystore_password -file ca.cer -alias ca -noprompt \$ keytool -importcert -keystore rest.signed.keystore -storepass rest_keystore_password -file rest.cer -alias flink.rest -noprompt Now add the following configuration to your flink-conf.yaml:
security.ssl.rest.enabled: true security.ssl.rest.keystore: /path/to/flink/conf/rest.signed.keystore security.ssl.rest.truststore: /path/to/flink/conf/ca.truststore security.ssl.rest.keystore-password: rest_keystore_password security.ssl.rest.key-password: rest_keystore_password security.ssl.rest.truststore-password: ca_truststore_password Tips to query REST Endpoint with curl utility
You can convert the keystore into the PEM format using openssl:
\$ openssl pkcs12 -passin pass:rest_keystore_password -in rest.keystore -out rest.pem -nodes Then you can query REST Endpoint with curl:
\$ curl --cacert rest.pem flink_url If mutual SSL is enabled:
\$ curl --cacert rest.pem --cert rest.pem flink_url Tips for YARN Deployment # For YARN, you can use the tools of Yarn to help:
Configuring security for internal communication is exactly the same as in the example above.
To secure the REST endpoint, you need to issue the REST endpoint\u0026rsquo;s certificate such that it is valid for all hosts that the JobManager may get deployed to. This can be done with a wild card DNS name, or by adding multiple DNS names.
The easiest way to deploy keystores and truststore is by YARN client\u0026rsquo;s ship files option (-yt). Copy the keystore and truststore files into a local directory (say deploy-keys/) and start the YARN session as follows: flink run -m yarn-cluster -yt deploy-keys/ flinkapp.jar
When deployed using YARN, Flink\u0026rsquo;s web dashboard is accessible through YARN proxy\u0026rsquo;s Tracking URL. To ensure that the YARN proxy is able to access Flink\u0026rsquo;s HTTPS URL, you need to configure YARN proxy to accept Flink\u0026rsquo;s SSL certificates. For that, add the custom CA certificate into Java\u0026rsquo;s default truststore on the YARN Proxy node.
Back to top
`}),e.add({id:42,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/",title:"Standalone",section:"Resource Providers",content:" "}),e.add({id:43,href:"/flink/flink-docs-master/zh/docs/connectors/table/",title:"Table API Connectors",section:"Connectors",content:""}),e.add({id:44,href:"/flink/flink-docs-master/zh/docs/dev/dataset/transformations/",title:"Transformations",section:"DataSet API (Legacy)",content:` DataSet Transformations # This document gives a deep-dive into the available transformations on DataSets. For a general introduction to the Flink Java API, please refer to the Programming Guide.
For zipping elements in a data set with a dense index, please refer to the Zip Elements Guide.
Map # The Map transformation applies a user-defined map function on each element of a DataSet. It implements a one-to-one mapping, that is, exactly one element must be returned by the function.
The following code transforms a DataSet of Integer pairs into a DataSet of Integers:
Java // MapFunction that adds two integer values public class IntAdder implements MapFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Integer\u0026gt; { @Override public Integer map(Tuple2\u0026lt;Integer, Integer\u0026gt; in) { return in.f0 + in.f1; } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; intPairs = // [...] DataSet\u0026lt;Integer\u0026gt; intSums = intPairs.map(new IntAdder()); Scala val intPairs: DataSet[(Int, Int)] = // [...] val intSums = intPairs.map { pair =\u0026gt; pair._1 + pair._2 } FlatMap # The FlatMap transformation applies a user-defined flat-map function on each element of a DataSet. This variant of a map function can return arbitrary many result elements (including none) for each input element.
The following code transforms a DataSet of text lines into a DataSet of words:
Java // FlatMapFunction that tokenizes a String by whitespace characters and emits all String tokens. public class Tokenizer implements FlatMapFunction\u0026lt;String, String\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;String\u0026gt; out) { for (String token : value.split(\u0026#34;\\\\W\u0026#34;)) { out.collect(token); } } } // [...] DataSet\u0026lt;String\u0026gt; textLines = // [...] DataSet\u0026lt;String\u0026gt; words = textLines.flatMap(new Tokenizer()); Scala val textLines: DataSet[String] = // [...] val words = textLines.flatMap { _.split(\u0026#34; \u0026#34;) } MapPartition # MapPartition transforms a parallel partition in a single function call. The map-partition function gets the partition as Iterable and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.
The following code transforms a DataSet of text lines into a DataSet of counts per partition:
Java public class PartitionCounter implements MapPartitionFunction\u0026lt;String, Long\u0026gt; { public void mapPartition(Iterable\u0026lt;String\u0026gt; values, Collector\u0026lt;Long\u0026gt; out) { long c = 0; for (String s : values) { c++; } out.collect(c); } } // [...] DataSet\u0026lt;String\u0026gt; textLines = // [...] DataSet\u0026lt;Long\u0026gt; counts = textLines.mapPartition(new PartitionCounter()); Scala val textLines: DataSet[String] = // [...] // Some is required because the return value must be a Collection. // There is an implicit conversion from Option to a Collection. val counts = textLines.mapPartition { in =\u0026gt; Some(in.size) } Filter # The Filter transformation applies a user-defined filter function on each element of a DataSet and retains only those elements for which the function returns true.
The following code removes all Integers smaller than zero from a DataSet:
Java // FilterFunction that filters out all Integers smaller than zero. public class NaturalNumberFilter implements FilterFunction\u0026lt;Integer\u0026gt; { @Override public boolean filter(Integer number) { return number \u0026gt;= 0; } } // [...] DataSet\u0026lt;Integer\u0026gt; intNumbers = // [...] DataSet\u0026lt;Integer\u0026gt; naturalNumbers = intNumbers.filter(new NaturalNumberFilter()); Scala val intNumbers: DataSet[Int] = // [...] val naturalNumbers = intNumbers.filter { _ \u0026gt; 0 } IMPORTANT: The system assumes that the function does not modify the elements on which the predicate is applied. Violating this assumption can lead to incorrect results.
Projection of Tuple DataSet # The Project transformation removes or moves Tuple fields of a Tuple DataSet. The project(int...) method selects Tuple fields that should be retained by their index and defines their order in the output Tuple.
Projections do not require the definition of a user function.
The following code shows different ways to apply a Project transformation on a DataSet:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Double, String\u0026gt;\u0026gt; in = // [...] // converts Tuple3\u0026lt;Integer, Double, String\u0026gt; into Tuple2\u0026lt;String, Integer\u0026gt; DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out = in.project(2,0); Projection with Type Hint # Note that the Java compiler cannot infer the return type of project operator. This can cause a problem if you call another operator on a result of project operator such as:
DataSet\u0026lt;Tuple5\u0026lt;String,String,String,String,String\u0026gt;\u0026gt; ds = ....; DataSet\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt; ds2 = ds.project(0).distinct(0); This problem can be overcome by hinting the return type of project operator like this:
DataSet\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt; ds2 = ds.\u0026lt;Tuple1\u0026lt;String\u0026gt;\u0026gt;project(0).distinct(0); Scala Not supported. Transformations on Grouped DataSet # The reduce operations can operate on grouped data sets. Specifying the key to be used for grouping can be done in many ways:
key expressions a key-selector function one or more field position keys (Tuple DataSet only) Case Class fields (Case Classes only) Please look at the reduce examples to see how the grouping keys are specified.
Reduce on Grouped DataSet # A Reduce transformation that is applied on a grouped DataSet reduces each group to a single element using a user-defined reduce function. For each group of input elements, a reduce function successively combines pairs of elements into one element until only a single element for each group remains.
Note that for a ReduceFunction the keyed fields of the returned object should match the input values. This is because reduce is implicitly combinable and objects emitted from the combine operator are again grouped by key when passed to the reduce operator.
Reduce on DataSet Grouped by Key Expression # Key expressions specify one or more fields of each element of a DataSet. Each key expression is either the name of a public field or a getter method. A dot can be used to drill down into objects. The key expression \u0026ldquo;*\u0026rdquo; selects all fields. The following code shows how to group a POJO DataSet using key expressions and to reduce it with a reduce function.
Java // some ordinary POJO public class WC { public String word; public int count; // [...] } // ReduceFunction that sums Integer attributes of a POJO public class WordCounter implements ReduceFunction\u0026lt;WC\u0026gt; { @Override public WC reduce(WC in1, WC in2) { return new WC(in1.word, in1.count + in2.count); } } // [...] DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words // DataSet grouping on field \u0026#34;word\u0026#34; .groupBy(\u0026#34;word\u0026#34;) // apply ReduceFunction on grouped DataSet .reduce(new WordCounter()); Scala // some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy(\u0026#34;word\u0026#34;).reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } Reduce on DataSet Grouped by KeySelector Function # A key-selector function extracts a key value from each element of a DataSet. The extracted key value is used to group the DataSet. The following code shows how to group a POJO DataSet using a key-selector function and to reduce it with a reduce function.
Java // some ordinary POJO public class WC { public String word; public int count; // [...] } // ReduceFunction that sums Integer attributes of a POJO public class WordCounter implements ReduceFunction\u0026lt;WC\u0026gt; { @Override public WC reduce(WC in1, WC in2) { return new WC(in1.word, in1.count + in2.count); } } // [...] DataSet\u0026lt;WC\u0026gt; words = // [...] DataSet\u0026lt;WC\u0026gt; wordCounts = words // DataSet grouping on field \u0026#34;word\u0026#34; .groupBy(new SelectWord()) // apply ReduceFunction on grouped DataSet .reduce(new WordCounter()); public class SelectWord implements KeySelector\u0026lt;WC, String\u0026gt; { @Override public String getKey(Word w) { return w.word; } } Scala // some ordinary POJO class WC(val word: String, val count: Int) { def this() { this(null, -1) } // [...] } val words: DataSet[WC] = // [...] val wordCounts = words.groupBy { _.word } reduce { (w1, w2) =\u0026gt; new WC(w1.word, w1.count + w2.count) } Reduce on DataSet Grouped by Field Position Keys (Tuple DataSets only) # Field position keys specify one or more fields of a Tuple DataSet that are used as grouping keys. The following code shows how to use field position keys and apply a reduce function
Java DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; tuples = // [...] DataSet\u0026lt;Tuple3\u0026lt;String, Integer, Double\u0026gt;\u0026gt; reducedTuples = tuples // group DataSet on first and second field of Tuple .groupBy(0, 1) // apply ReduceFunction on grouped DataSet .reduce(new MyTupleReducer()); Scala val tuples = DataSet[(String, Int, Double)] = // [...] // group on the first and second Tuple field val reducedTuples = tuples.groupBy(0, 1).reduce { ... } Reduce on DataSet grouped by Case Class Fields # When using Case Classes you can also specify the grouping key using the names of the fields:
Java Not supported. Scala case class MyClass(val a: String, b: Int, c: Double) val tuples = DataSet[MyClass] = // [...] // group on the first and second field val reducedTuples = tuples.groupBy(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;).reduce { ... } GroupReduce on Grouped DataSet # A GroupReduce transformation that is applied on a grouped DataSet calls a user-defined group-reduce function for each group. The difference between this and Reduce is that the user defined function gets the whole group at once. The function is invoked with an Iterable over all elements of a group and can return an arbitrary number of result elements.
GroupReduce on DataSet Grouped by Field Position Keys (Tuple DataSets only) # The following code shows how duplicate strings can be removed from a DataSet grouped by Integer.
Java public class DistinctReduce implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; out) { Set\u0026lt;String\u0026gt; uniqStrings = new HashSet\u0026lt;String\u0026gt;(); Integer key = null; // add all strings of the group to the set for (Tuple2\u0026lt;Integer, String\u0026gt; t : in) { key = t.f0; uniqStrings.add(t.f1); } // emit all unique strings. for (String s : uniqStrings) { out.collect(new Tuple2\u0026lt;Integer, String\u0026gt;(key, s)); } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; output = input .groupBy(0) // group DataSet by the first tuple field .reduceGroup(new DistinctReduce()); // apply GroupReduceFunction Scala val input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; in.toSet foreach (out.collect) } GroupReduce on DataSet Grouped by Key Expression, KeySelector Function, or Case Class Fields # Work analogous to key expressions, key-selector functions, and case class fields in Reduce transformations.
GroupReduce on sorted groups # A group-reduce function accesses the elements of a group using an Iterable. Optionally, the Iterable can hand out the elements of a group in a specified order. In many cases this can help to reduce the complexity of a user-defined group-reduce function and improve its efficiency.
The following code shows another example how to remove duplicate Strings in a DataSet grouped by an Integer and sorted by String.
Java // GroupReduceFunction that removes consecutive identical elements public class DistinctReduce implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; out) { Integer key = null; String comp = null; for (Tuple2\u0026lt;Integer, String\u0026gt; t : in) { key = t.f0; String next = t.f1; // check if strings are different if (comp == null || !next.equals(comp)) { out.collect(new Tuple2\u0026lt;Integer, String\u0026gt;(key, next)); comp = next; } } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Double\u0026gt; output = input .groupBy(0) // group DataSet by first field .sortGroup(1, Order.ASCENDING) // sort groups on second tuple field .reduceGroup(new DistinctReduce()); Scala val input: DataSet[(Int, String)] = // [...] val output = input.groupBy(0).sortGroup(1, Order.ASCENDING).reduceGroup { (in, out: Collector[(Int, String)]) =\u0026gt; var prev: (Int, String) = null for (t \u0026lt;- in) { if (prev == null || prev != t) out.collect(t) prev = t } } Note: A GroupSort often comes for free if the grouping is established using a sort-based execution strategy of an operator before the reduce operation.
Combinable GroupReduceFunctions # In contrast to a reduce function, a group-reduce function is not implicitly combinable. In order to make a group-reduce function combinable it must implement the GroupCombineFunction interface.
Important: The generic input and output types of the GroupCombineFunction interface must be equal to the generic input type of the GroupReduceFunction as shown in the following example:
Java // Combinable GroupReduceFunction that computes a sum. public class MyCombinableGroupReducer implements GroupReduceFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, String\u0026gt;, GroupCombineFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void reduce(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in, Collector\u0026lt;String\u0026gt; out) { String key = null; int sum = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; curr : in) { key = curr.f0; sum += curr.f1; } // concat key and sum and emit out.collect(key + \u0026#34;-\u0026#34; + sum); } @Override public void combine(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { String key = null; int sum = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; curr : in) { key = curr.f0; sum += curr.f1; } // emit tuple with key and sum out.collect(new Tuple2\u0026lt;\u0026gt;(key, sum)); } } Scala // Combinable GroupReduceFunction that computes two sums. class MyCombinableGroupReducer extends GroupReduceFunction[(String, Int), String] with GroupCombineFunction[(String, Int), (String, Int)] { override def reduce( in: java.lang.Iterable[(String, Int)], out: Collector[String]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // concat key and sum and emit out.collect (r._1 + \u0026#34;-\u0026#34; + r._2) } override def combine( in: java.lang.Iterable[(String, Int)], out: Collector[(String, Int)]): Unit = { val r: (String, Int) = in.iterator.asScala.reduce( (a,b) =\u0026gt; (a._1, a._2 + b._2) ) // emit tuple with key and sum out.collect(r) } } GroupCombine on a Grouped DataSet # The GroupCombine transformation is the generalized form of the combine step in the combinable GroupReduceFunction. It is generalized in the sense that it allows combining of input type I to an arbitrary output type O. In contrast, the combine step in the GroupReduce only allows combining from input type I to output type I. This is because the reduce step in the GroupReduceFunction expects input type I.
In some applications, it is desirable to combine a DataSet into an intermediate format before performing additional transformations (e.g. to reduce data size). This can be achieved with a CombineGroup transformation with very little costs.
Note: The GroupCombine on a Grouped DataSet is performed in memory with a greedy strategy which may not process all data at once but in multiple steps. It is also performed on the individual partitions without a data exchange like in a GroupReduce transformation. This may lead to partial results.
The following example demonstrates the use of a CombineGroup transformation for an alternative WordCount implementation.
Java DataSet\u0026lt;String\u0026gt; input = [..] // The words received as input DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; combinedWords = input .groupBy(0) // group identical words .combineGroup(new GroupCombineFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;() { public void combine(Iterable\u0026lt;String\u0026gt; words, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;) { // combine String key = null; int count = 0; for (String word : words) { key = word; count++; } // emit tuple with word and count out.collect(new Tuple2(key, count)); } }); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; output = combinedWords .groupBy(0) // group by words again .reduceGroup(new GroupReduceFunction() { // group reduce with full data exchange public void reduce(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;) { String key = null; int count = 0; for (Tuple2\u0026lt;String, Integer\u0026gt; word : words) { key = word; count++; } // emit tuple with word and count out.collect(new Tuple2(key, count)); } }); Scala val input: DataSet[String] = [..] // The words received as input val combinedWords: DataSet[(String, Int)] = input .groupBy(0) .combineGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var count = 0 for (word \u0026lt;- words) { key = word count += 1 } out.collect((key, count)) } val output: DataSet[(String, Int)] = combinedWords .groupBy(0) .reduceGroup { (words, out: Collector[(String, Int)]) =\u0026gt; var key: String = null var sum = 0 for ((word, sum) \u0026lt;- words) { key = word sum += count } out.collect((key, sum)) } The above alternative WordCount implementation demonstrates how the GroupCombine combines words before performing the GroupReduce transformation. The above example is just a proof of concept. Note, how the combine step changes the type of the DataSet which would normally require an additional Map transformation before executing the GroupReduce.
Aggregate on Grouped Tuple DataSet # There are some common aggregation operations that are frequently used. The Aggregate transformation provides the following build-in aggregation functions:
Sum, Min, and Max. The Aggregate transformation can only be applied on a Tuple DataSet and supports only field position keys for grouping.
The following code shows how to apply an Aggregation transformation on a DataSet grouped by field position keys:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .groupBy(1) // group DataSet on second field .aggregate(SUM, 0) // compute sum of the first field .and(MIN, 2); // compute minimum of the third field Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.groupBy(1).aggregate(SUM, 0).and(MIN, 2) To apply multiple aggregations on a DataSet it is necessary to use the .and() function after the first aggregate, that means .aggregate(SUM, 0).and(MIN, 2) produces the sum of field 0 and the minimum of field 2 of the original DataSet. In contrast to that .aggregate(SUM, 0).aggregate(MIN, 2) will apply an aggregation on an aggregation. In the given example it would produce the minimum of field 2 after calculating the sum of field 0 grouped by field 1.
Note: The set of aggregation functions will be extended in the future.
MinBy / MaxBy on Grouped Tuple DataSet # The MinBy (MaxBy) transformation selects a single tuple for each group of tuples. The selected tuple is the tuple whose values of one or more specified fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) fields values, an arbitrary tuple of these tuples is returned.
The following code shows how to select the tuple with the minimum values for the Integer and Double fields for each group of tuples with the same String value from a DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt;:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .groupBy(1) // group DataSet on second field .minBy(0, 2); // select tuple with minimum values for first and third field. Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .groupBy(1) // group DataSet on second field .minBy(0, 2) // select tuple with minimum values for first and third field. Reduce on full DataSet # The Reduce transformation applies a user-defined reduce function to all elements of a DataSet. The reduce function subsequently combines pairs of elements into one element until only a single element remains.
The following code shows how to sum all elements of an Integer DataSet:
Java // ReduceFunction that sums Integers public class IntSummer implements ReduceFunction\u0026lt;Integer\u0026gt; { @Override public Integer reduce(Integer num1, Integer num2) { return num1 + num2; } } // [...] DataSet\u0026lt;Integer\u0026gt; intNumbers = // [...] DataSet\u0026lt;Integer\u0026gt; sum = intNumbers.reduce(new IntSummer()); Scala val intNumbers = env.fromElements(1,2,3) val sum = intNumbers.reduce (_ + _) Reducing a full DataSet using the Reduce transformation implies that the final Reduce operation cannot be done in parallel. However, a reduce function is automatically combinable such that a Reduce transformation does not limit scalability for most use cases.
GroupReduce on full DataSet # The GroupReduce transformation applies a user-defined group-reduce function on all elements of a DataSet. A group-reduce can iterate over all elements of DataSet and return an arbitrary number of result elements.
The following example shows how to apply a GroupReduce transformation on a full DataSet:
Java DataSet\u0026lt;Integer\u0026gt; input = // [...] // apply a (preferably combinable) GroupReduceFunction to a DataSet DataSet\u0026lt;Double\u0026gt; output = input.reduceGroup(new MyGroupReducer()); Scala val input: DataSet[Int] = // [...] val output = input.reduceGroup(new MyGroupReducer()) Note: A GroupReduce transformation on a full DataSet cannot be done in parallel if the group-reduce function is not combinable. Therefore, this can be a very compute intensive operation. See the paragraph on \u0026ldquo;Combinable GroupReduceFunctions\u0026rdquo; above to learn how to implement a combinable group-reduce function.
GroupCombine on a full DataSet # The GroupCombine on a full DataSet works similar to the GroupCombine on a grouped DataSet. The data is partitioned on all nodes and then combined in a greedy fashion (i.e. only data fitting into memory is combined at once).
Aggregate on full Tuple DataSet # There are some common aggregation operations that are frequently used. The Aggregate transformation provides the following build-in aggregation functions:
Sum, Min, and Max. The Aggregate transformation can only be applied on a Tuple DataSet.
The following code shows how to apply an Aggregation transformation on a full DataSet:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; output = input .aggregate(SUM, 0) // compute sum of the first field .and(MIN, 1); // compute minimum of the second field Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.aggregate(SUM, 0).and(MIN, 2) Note: Extending the set of supported aggregation functions is on our roadmap.
MinBy / MaxBy on full Tuple DataSet # The MinBy (MaxBy) transformation selects a single tuple from a DataSet of tuples. The selected tuple is the tuple whose values of one or more specified fields are minimum (maximum). The fields which are used for comparison must be valid key fields, i.e., comparable. If multiple tuples have minimum (maximum) fields values, an arbitrary tuple of these tuples is returned.
The following code shows how to select the tuple with the maximum values for the Integer and Double fields from a DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt;:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, String, Double\u0026gt;\u0026gt; output = input .maxBy(0, 2); // select tuple with maximum values for first and third field. Scala val input: DataSet[(Int, String, Double)] = // [...] val output: DataSet[(Int, String, Double)] = input .maxBy(0, 2) // select tuple with maximum values for first and third field. Distinct # The Distinct transformation computes the DataSet of the distinct elements of the source DataSet. The following code removes all duplicate elements from the DataSet:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; output = input.distinct(); Scala val input: DataSet[(Int, String, Double)] = // [...] val output = input.distinct() It is also possible to change how the distinction of the elements in the DataSet is decided, using:
one or more field position keys (Tuple DataSets only), a key-selector function, or a key expression. Distinct with field position keys # Java DataSet\u0026lt;Tuple2\u0026lt;Integer, Double, String\u0026gt;\u0026gt; input = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double, String\u0026gt;\u0026gt; output = input.distinct(0,2); Scala val input: DataSet[(Int, Double, String)] = // [...] val output = input.distinct(0,2) Distinct with KeySelector function # Java private static class AbsSelector implements KeySelector\u0026lt;Integer, Integer\u0026gt; { private static final long serialVersionUID = 1L; @Override public Integer getKey(Integer t) { return Math.abs(t); } } DataSet\u0026lt;Integer\u0026gt; input = // [...] DataSet\u0026lt;Integer\u0026gt; output = input.distinct(new AbsSelector()); Scala val input: DataSet[Int] = // [...] val output = input.distinct {x =\u0026gt; Math.abs(x)} Distinct with key expression # Java // some ordinary POJO public class CustomType { public String aName; public int aNumber; // [...] } DataSet\u0026lt;CustomType\u0026gt; input = // [...] DataSet\u0026lt;CustomType\u0026gt; output = input.distinct(\u0026#34;aName\u0026#34;, \u0026#34;aNumber\u0026#34;); Scala // some ordinary POJO case class CustomType(aName : String, aNumber : Int) { } val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;aName\u0026#34;, \u0026#34;aNumber\u0026#34;) It is also possible to indicate to use all the fields by the wildcard character:
Java DataSet\u0026lt;CustomType\u0026gt; input = // [...] DataSet\u0026lt;CustomType\u0026gt; output = input.distinct(\u0026#34;*\u0026#34;); Scala // some ordinary POJO val input: DataSet[CustomType] = // [...] val output = input.distinct(\u0026#34;_\u0026#34;) Join # The Join transformation joins two DataSets into one DataSet. The elements of both DataSets are joined on one or more keys which can be specified using
a key expression a key-selector function one or more field position keys (Tuple DataSet only). Case Class Fields There are a few different ways to perform a Join transformation which are shown in the following.
Default Join (Join into Tuple2) # The default Join transformation produces a new Tuple DataSet with two fields. Each tuple holds a joined element of the first input DataSet in the first tuple field and a matching element of the second input DataSet in the second field.
The following code shows a default Join transformation using field position keys:
Java public static class User { public String name; public int zip; } public static class Store { public Manager mgr; public int zip; } DataSet\u0026lt;User\u0026gt; input1 = // [...] DataSet\u0026lt;Store\u0026gt; input2 = // [...] // result dataset is typed as Tuple2 DataSet\u0026lt;Tuple2\u0026lt;User, Store\u0026gt;\u0026gt; result = input1.join(input2) .where(\u0026#34;zip\u0026#34;) // key of the first input (users) .equalTo(\u0026#34;zip\u0026#34;); // key of the second input (stores) Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Double, Int)] = // [...] val result = input1.join(input2).where(0).equalTo(1) Join with Join Function # A Join transformation can also call a user-defined join function to process joining tuples. A join function receives one element of the first input DataSet and one element of the second input DataSet and returns exactly one element.
The following code performs a join of DataSet with custom java objects and a Tuple DataSet using key-selector functions and shows how to use a user-defined join function:
Java // some POJO public class Rating { public String name; public String category; public int points; } // Join function that joins a custom POJO with a Tuple public class PointWeighter implements JoinFunction\u0026lt;Rating, Tuple2\u0026lt;String, Double\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;String, Double\u0026gt; join(Rating rating, Tuple2\u0026lt;String, Double\u0026gt; weight) { // multiply the points and rating and construct a new output tuple return new Tuple2\u0026lt;String, Double\u0026gt;(rating.name, rating.points * weight.f1); } } DataSet\u0026lt;Rating\u0026gt; ratings = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weights = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weightedRatings = ratings.join(weights) // key of the first input .where(\u0026#34;category\u0026#34;) // key of the second input .equalTo(\u0026#34;f0\u0026#34;) // applying the JoinFunction on joining pairs .with(new PointWeighter()); Scala case class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...] val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight) =\u0026gt; (rating.name, rating.points * weight._2) } Join with Flat-Join Function # Analogous to Map and FlatMap, a FlatJoin behaves in the same way as a Join, but instead of returning one element, it can return (collect), zero, one, or more elements.
Java public class PointWeighter implements FlatJoinFunction\u0026lt;Rating, Tuple2\u0026lt;String, Double\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; { @Override public void join(Rating rating, Tuple2\u0026lt;String, Double\u0026gt; weight, Collector\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; out) { if (weight.f1 \u0026gt; 0.1) { out.collect(new Tuple2\u0026lt;String, Double\u0026gt;(rating.name, rating.points * weight.f1)); } } } DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; weightedRatings = ratings.join(weights); Scala case class Rating(name: String, category: String, points: Int) val ratings: DataSet[Ratings] = // [...] val weights: DataSet[(String, Double)] = // [...] val weightedRatings = ratings.join(weights).where(\u0026#34;category\u0026#34;).equalTo(0) { (rating, weight, out: Collector[(String, Double)]) =\u0026gt; if (weight._2 \u0026gt; 0.1) out.collect(rating.name, rating.points * weight._2) } Join with Projection (Java Only) # A Join transformation can construct result tuples using a projection as shown here:
Java DataSet\u0026lt;Tuple3\u0026lt;Integer, Byte, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, String, Double, Byte\u0026gt;\u0026gt; result = input1.join(input2) // key definition on first DataSet using a field position key .where(0) // key definition of second DataSet using a field position key .equalTo(0) // select and reorder fields of matching tuples .projectFirst(0,2).projectSecond(1).projectFirst(1); projectFirst(int...) and projectSecond(int...) select the fields of the first and second joined input that should be assembled into an output Tuple. The order of indexes defines the order of fields in the output tuple. The join projection works also for non-Tuple DataSets. In this case, projectFirst() or projectSecond() must be called without arguments to add a joined element to the output Tuple.
Scala Not supported. Join with DataSet Size Hint # In order to guide the optimizer to pick the right execution strategy, you can hint the size of a DataSet to join as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt;\u0026gt; result1 = // hint that the second DataSet is very small input1.joinWithTiny(input2) .where(0) .equalTo(0); DataSet\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;, Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt;\u0026gt; result2 = // hint that the second DataSet is very large input1.joinWithHuge(input2) .where(0) .equalTo(0); Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...] // hint that the second DataSet is very small val result1 = input1.joinWithTiny(input2).where(0).equalTo(0) // hint that the second DataSet is very large val result1 = input1.joinWithHuge(input2).where(0).equalTo(0) Join Algorithm Hints # The Flink runtime can execute joins in various ways. Each possible way outperforms the others under different circumstances. The system tries to pick a reasonable way automatically, but allows you to manually pick a strategy, in case you want to enforce a specific way of executing the join.
Java DataSet\u0026lt;SomeType\u0026gt; input1 = // [...] DataSet\u0026lt;AnotherType\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); Scala val input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...] // hint that the second DataSet is very small val result1 = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) The following hints are available:
OPTIMIZER_CHOOSES: Equivalent to not giving a hint at all, leaves the choice to the system.
BROADCAST_HASH_FIRST: Broadcasts the first input and builds a hash table from it, which is probed by the second input. A good strategy if the first input is very small.
BROADCAST_HASH_SECOND: Broadcasts the second input and builds a hash table from it, which is probed by the first input. A good strategy if the second input is very small.
REPARTITION_HASH_FIRST: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the first input. This strategy is good if the first input is smaller than the second, but both inputs are still large. Note: This is the default fallback strategy that the system uses if no size estimates can be made and no pre-existing partitions and sort-orders can be re-used.
REPARTITION_HASH_SECOND: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the second input. This strategy is good if the second input is smaller than the first, but both inputs are still large.
REPARTITION_SORT_MERGE: The system partitions (shuffles) each input (unless the input is already partitioned) and sorts each input (unless it is already sorted). The inputs are joined by a streamed merge of the sorted inputs. This strategy is good if one or both of the inputs are already sorted.
OuterJoin # The OuterJoin transformation performs a left, right, or full outer join on two data sets. Outer joins are similar to regular (inner) joins and create all pairs of elements that are equal on their keys. In addition, records of the \u0026ldquo;outer\u0026rdquo; side (left, right, or both in case of full) are preserved if no matching key is found in the other side. Matching pair of elements (or one element and a null value for the other input) are given to a JoinFunction to turn the pair of elements into a single element, or to a FlatJoinFunction to turn the pair of elements into arbitrarily many (including none) elements.
The elements of both DataSets are joined on one or more keys which can be specified using
a key expression a key-selector function one or more field position keys (Tuple DataSet only). Case Class Fields OuterJoins are only supported for the Java and Scala DataSet API.
OuterJoin with Join Function # A OuterJoin transformation calls a user-defined join function to process joining tuples. A join function receives one element of the first input DataSet and one element of the second input DataSet and returns exactly one element. Depending on the type of the outer join (left, right, full) one of both input elements of the join function can be null.
The following code performs a left outer join of DataSet with custom java objects and a Tuple DataSet using key-selector functions and shows how to use a user-defined join function:
Java // some POJO public class Rating { public String name; public String category; public int points; } // Join function that joins a custom POJO with a Tuple public class PointAssigner implements JoinFunction\u0026lt;Tuple2\u0026lt;String, String\u0026gt;, Rating, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;String, Integer\u0026gt; join(Tuple2\u0026lt;String, String\u0026gt; movie, Rating rating) { // Assigns the rating points to the movie. // NOTE: rating might be null return new Tuple2\u0026lt;String, Double\u0026gt;(movie.f0, rating == null ? -1 : rating.points; } } DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; movies = // [...] DataSet\u0026lt;Rating\u0026gt; ratings = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; moviesWithPoints = movies.leftOuterJoin(ratings) // key of the first input .where(\u0026#34;f0\u0026#34;) // key of the second input .equalTo(\u0026#34;name\u0026#34;) // applying the JoinFunction on joining pairs .with(new PointAssigner()); Scala case class Rating(name: String, category: String, points: Int) val movies: DataSet[(String, String)] = // [...] val ratings: DataSet[Ratings] = // [...] val moviesWithPoints = movies.leftOuterJoin(ratings).where(0).equalTo(\u0026#34;name\u0026#34;) { (movie, rating) =\u0026gt; (movie._1, if (rating == null) -1 else rating.points) } OuterJoin with Flat-Join Function # Analogous to Map and FlatMap, an OuterJoin with flat-join function behaves in the same way as an OuterJoin with join function, but instead of returning one element, it can return (collect), zero, one, or more elements.
Java public class PointAssigner implements FlatJoinFunction\u0026lt;Tuple2\u0026lt;String, String\u0026gt;, Rating, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void join(Tuple2\u0026lt;String, String\u0026gt; movie, Rating rating, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { if (rating == null ) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(movie.f0, -1)); } else if (rating.points \u0026lt; 10) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(movie.f0, rating.points)); } else { // do not emit } } DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; moviesWithPoints = movies.leftOuterJoin(ratings) // [...] Scala Not supported. Join Algorithm Hints # The Flink runtime can execute outer joins in various ways. Each possible way outperforms the others under different circumstances. The system tries to pick a reasonable way automatically, but allows you to manually pick a strategy, in case you want to enforce a specific way of executing the outer join.
Java DataSet\u0026lt;SomeType\u0026gt; input1 = // [...] DataSet\u0026lt;AnotherType\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result1 = input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;SomeType, AnotherType\u0026gt; result2 = input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST) .where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;); Scala val input1: DataSet[SomeType] = // [...] val input2: DataSet[AnotherType] = // [...] // hint that the second DataSet is very small val result1 = input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) val result2 = input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST).where(\u0026#34;id\u0026#34;).equalTo(\u0026#34;key\u0026#34;) The following hints are available.
OPTIMIZER_CHOOSES: Equivalent to not giving a hint at all, leaves the choice to the system.
BROADCAST_HASH_FIRST: Broadcasts the first input and builds a hash table from it, which is probed by the second input. A good strategy if the first input is very small.
BROADCAST_HASH_SECOND: Broadcasts the second input and builds a hash table from it, which is probed by the first input. A good strategy if the second input is very small.
REPARTITION_HASH_FIRST: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the first input. This strategy is good if the first input is smaller than the second, but both inputs are still large.
REPARTITION_HASH_SECOND: The system partitions (shuffles) each input (unless the input is already partitioned) and builds a hash table from the second input. This strategy is good if the second input is smaller than the first, but both inputs are still large.
REPARTITION_SORT_MERGE: The system partitions (shuffles) each input (unless the input is already partitioned) and sorts each input (unless it is already sorted). The inputs are joined by a streamed merge of the sorted inputs. This strategy is good if one or both of the inputs are already sorted.
NOTE: Not all execution strategies are supported by every outer join type, yet.
LeftOuterJoin supports:
OPTIMIZER_CHOOSES BROADCAST_HASH_SECOND REPARTITION_HASH_SECOND REPARTITION_SORT_MERGE RightOuterJoin supports:
OPTIMIZER_CHOOSES BROADCAST_HASH_FIRST REPARTITION_HASH_FIRST REPARTITION_SORT_MERGE FullOuterJoin supports:
OPTIMIZER_CHOOSES REPARTITION_SORT_MERGE Cross # The Cross transformation combines two DataSets into one DataSet. It builds all pairwise combinations of the elements of both input DataSets, i.e., it builds a Cartesian product. The Cross transformation either calls a user-defined cross function on each pair of elements or outputs a Tuple2. Both modes are shown in the following.
Note: Cross is potentially a very compute-intensive operation which can challenge even large compute clusters!
Cross with User-Defined Function # A Cross transformation can call a user-defined cross function. A cross function receives one element of the first input and one element of the second input and returns exactly one result element.
The following code shows how to apply a Cross transformation on two DataSets using a cross function:
Java public class Coord { public int id; public int x; public int y; } // CrossFunction computes the Euclidean distance between two Coord objects. public class EuclideanDistComputer implements CrossFunction\u0026lt;Coord, Coord, Tuple3\u0026lt;Integer, Integer, Double\u0026gt;\u0026gt; { @Override public Tuple3\u0026lt;Integer, Integer, Double\u0026gt; cross(Coord c1, Coord c2) { // compute Euclidean distance of coordinates double dist = sqrt(pow(c1.x - c2.x, 2) + pow(c1.y - c2.y, 2)); return new Tuple3\u0026lt;Integer, Integer, Double\u0026gt;(c1.id, c2.id, dist); } } DataSet\u0026lt;Coord\u0026gt; coords1 = // [...] DataSet\u0026lt;Coord\u0026gt; coords2 = // [...] DataSet\u0026lt;Tuple3\u0026lt;Integer, Integer, Double\u0026gt;\u0026gt; distances = coords1.cross(coords2) // apply CrossFunction .with(new EuclideanDistComputer()); Cross with Projection # A Cross transformation can also construct result tuples using a projection as shown here:
DataSet\u0026lt;Tuple3\u0026lt;Integer, Byte, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, Double\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, Byte, Integer, Double\u0026gt; result = input1.cross(input2) // select and reorder fields of matching tuples .projectSecond(0).projectFirst(1,0).projectSecond(1); The field selection in a Cross projection works the same way as in the projection of Join results.
Scala case class Coord(id: Int, x: Int, y: Int) val coords1: DataSet[Coord] = // [...] val coords2: DataSet[Coord] = // [...] val distances = coords1.cross(coords2) { (c1, c2) =\u0026gt; val dist = sqrt(pow(c1.x - c2.x, 2) + pow(c1.y - c2.y, 2)) (c1.id, c2.id, dist) } Cross with DataSet Size Hint # In order to guide the optimizer to pick the right execution strategy, you can hint the size of a DataSet to cross as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;Integer, String\u0026gt;\u0026gt; input2 = // [...] DataSet\u0026lt;Tuple4\u0026lt;Integer, String, Integer, String\u0026gt;\u0026gt; udfResult = // hint that the second DataSet is very small input1.crossWithTiny(input2) // apply any Cross function (or projection) .with(new MyCrosser()); DataSet\u0026lt;Tuple3\u0026lt;Integer, Integer, String\u0026gt;\u0026gt; projectResult = // hint that the second DataSet is very large input1.crossWithHuge(input2) // apply a projection (or any Cross function) .projectFirst(0,1).projectSecond(1); Scala val input1: DataSet[(Int, String)] = // [...] val input2: DataSet[(Int, String)] = // [...] // hint that the second DataSet is very small val result1 = input1.crossWithTiny(input2) // hint that the second DataSet is very large val result1 = input1.crossWithHuge(input2) CoGroup # The CoGroup transformation jointly processes groups of two DataSets. Both DataSets are grouped on a defined key and groups of both DataSets that share the same key are handed together to a user-defined co-group function. If for a specific key only one DataSet has a group, the co-group function is called with this group and an empty group. A co-group function can separately iterate over the elements of both groups and return an arbitrary number of result elements.
Similar to Reduce, GroupReduce, and Join, keys can be defined using the different key-selection methods.
CoGroup on DataSets # Java The example shows how to group by Field Position Keys (Tuple DataSets only). You can do the same with Pojo-types and key expressions.
// Some CoGroupFunction definition class MyCoGrouper implements CoGroupFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, Tuple2\u0026lt;String, Double\u0026gt;, Double\u0026gt; { @Override public void coGroup(Iterable\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; iVals, Iterable\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; dVals, Collector\u0026lt;Double\u0026gt; out) { Set\u0026lt;Integer\u0026gt; ints = new HashSet\u0026lt;Integer\u0026gt;(); // add all Integer values in group to set for (Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; val : iVals) { ints.add(val.f1); } // multiply each Double value with each unique Integer values of group for (Tuple2\u0026lt;String, Double\u0026gt; val : dVals) { for (Integer i : ints) { out.collect(val.f1 * i); } } } } // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; iVals = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; dVals = // [...] DataSet\u0026lt;Double\u0026gt; output = iVals.coGroup(dVals) // group first DataSet on first tuple field .where(0) // group second DataSet on first tuple field .equalTo(0) // apply CoGroup function on each pair of groups .with(new MyCoGrouper()); Scala val iVals: DataSet[(String, Int)] = // [...] val dVals: DataSet[(String, Double)] = // [...] val output = iVals.coGroup(dVals).where(0).equalTo(0) { (iVals, dVals, out: Collector[Double]) =\u0026gt; val ints = iVals map { _._2 } toSet for (dVal \u0026lt;- dVals) { for (i \u0026lt;- ints) { out.collect(dVal._2 * i) } } } Union # Produces the union of two DataSets, which have to be of the same type. A union of more than two DataSets can be implemented with multiple union calls, as shown here:
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals1 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals2 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; vals3 = // [...] DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; unioned = vals1.union(vals2).union(vals3); Scala val vals1: DataSet[(String, Int)] = // [...] val vals2: DataSet[(String, Int)] = // [...] val vals3: DataSet[(String, Int)] = // [...] val unioned = vals1.union(vals2).union(vals3) Rebalance # Evenly rebalances the parallel partitions of a DataSet to eliminate data skew.
Java DataSet\u0026lt;String\u0026gt; in = // [...] // rebalance DataSet and apply a Map transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.rebalance() .map(new Mapper()); Scala val in: DataSet[String] = // [...] // rebalance DataSet and apply a Map transformation. val out = in.rebalance().map { ... } Hash-Partition # Hash-partitions a DataSet on a given key. Keys can be specified as position keys, expression keys, and key selector functions (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // hash-partition DataSet by String value and apply a MapPartition transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // hash-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByHash(0).mapPartition { ... } Range-Partition # Range-partitions a DataSet on a given key. Keys can be specified as position keys, expression keys, and key selector functions (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // range-partition DataSet by String value and apply a MapPartition transformation. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // range-partition DataSet by String value and apply a MapPartition transformation. val out = in.partitionByRange(0).mapPartition { ... } Sort Partition # Locally sorts all partitions of a DataSet on a specified field in a specified order. Fields can be specified as field expressions or field positions (see Reduce examples for how to specify keys). Partitions can be sorted on multiple fields by chaining sortPartition() calls.
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // Locally sort partitions in ascending order on the second String field and // in descending order on the first String field. // Apply a MapPartition transformation on the sorted partitions. DataSet\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; out = in.sortPartition(1, Order.ASCENDING) .sortPartition(0, Order.DESCENDING) .mapPartition(new PartitionMapper()); Scala val in: DataSet[(String, Int)] = // [...] // Locally sort partitions in ascending order on the second String field and // in descending order on the first String field. // Apply a MapPartition transformation on the sorted partitions. val out = in.sortPartition(1, Order.ASCENDING) .sortPartition(0, Order.DESCENDING) .mapPartition { ... } First-n # Returns the first n (arbitrary) elements of a DataSet. First-n can be applied on a regular DataSet, a grouped DataSet, or a grouped-sorted DataSet. Grouping keys can be specified as key-selector functions or field position keys (see Reduce examples for how to specify keys).
Java DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; in = // [...] // Return the first five (arbitrary) elements of the DataSet DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out1 = in.first(5); // Return the first two (arbitrary) elements of each String group DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out2 = in.groupBy(0) .first(2); // Return the first three elements of each String group ordered by the Integer field DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); Scala val in: DataSet[(String, Int)] = // [...] // Return the first five (arbitrary) elements of the DataSet val out1 = in.first(5) // Return the first two (arbitrary) elements of each String group val out2 = in.groupBy(0).first(2) // Return the first three elements of each String group ordered by the Integer field val out3 = in.groupBy(0).sortGroup(1, Order.ASCENDING).first(3) `}),e.add({id:45,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/windows/",title:"Windows",section:"Operators",content:" "}),e.add({id:46,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state/",title:"Working with State",section:"状态与容错",content:` 使用状态 # 本章节您将了解 Flink 用于编写有状态程序的 API。要了解有状态流处理背后的概念，请参阅Stateful Stream Processing。
Keyed DataStream # 如果你希望使用 keyed state，首先需要为DataStream指定 key（主键）。这个主键用于状态分区（也会给数据流中的记录本身分区）。 你可以使用 DataStream 中 Java/Scala API 的 keyBy(KeySelector) 或者是 Python API 的 key_by(KeySelector) 来指定 key。 它将生成 KeyedStream，接下来允许使用 keyed state 操作。
Key selector 函数接收单条记录作为输入，返回这条记录的 key。该 key 可以为任何类型，但是它的计算产生方式必须是具备确定性的。
Flink 的数据模型不基于 key-value 对，因此实际上将数据集在物理上封装成 key 和 value 是没有必要的。 Key 是“虚拟”的。它们定义为基于实际数据的函数，用以操纵分组算子。
下面的例子展示了 key selector 函数。它仅返回了对象当中的字段。
Java // some ordinary POJO public class WC { public String word; public int count; public String getWord() { return word; } } DataStream\u0026lt;WC\u0026gt; words = // [...] KeyedStream\u0026lt;WC\u0026gt; keyed = words .keyBy(WC::getWord); Scala // some ordinary case class case class WC(word: String, count: Int) val words: DataStream[WC] = // [...] val keyed = words.keyBy( _.word ) Python words = # type: DataStream[Row] keyed = words.key_by(lambda row: row[0]) Tuple Keys 和 Expression Keys # Flink 也有两种不同定义 key 的方式：Java/Scala API（Python API 仍未支持） 的 Tuple key（通过字段索引指定的 key）和 Expression key（通过字段名称指定的 key）。 借此你可以通过 tuple 字段索引，或者是选取对象字段的表达式来指定 key。 如今我们不建议这样使用，但你可以参考 DataStream 的 Javadoc 来了解它们。 使用 KeySelector 函数显然是更好的。以几乎可以忽略的额外开销为代价，结合 Java Lambda 表达式，我们可以更方便得使用KeySelector。
Back to top
使用 Keyed State # keyed state 接口提供不同类型状态的访问接口，这些状态都作用于当前输入数据的 key 下。换句话说，这些状态仅可在 KeyedStream 上使用，在Java/Scala API上可以通过 stream.keyBy(...) 得到 KeyedStream，在Python API上可以通过 stream.key_by(...) 得到 KeyedStream。
接下来，我们会介绍不同类型的状态，然后介绍如何使用他们。所有支持的状态类型如下所示：
ValueState\u0026lt;T\u0026gt;: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。
ListState\u0026lt;T\u0026gt;: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上进行检索。可以通过 add(T) 或者 addAll(List\u0026lt;T\u0026gt;) 进行添加元素，通过 Iterable\u0026lt;T\u0026gt; get() 获得整个列表。还可以通过 update(List\u0026lt;T\u0026gt;) 覆盖当前的列表。
ReducingState\u0026lt;T\u0026gt;: 保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的 ReduceFunction 进行聚合。
AggregatingState\u0026lt;IN, OUT\u0026gt;: 保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚合。
MapState\u0026lt;UK, UV\u0026gt;: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得反映当前所有映射的迭代器。使用 put(UK，UV) 或者 putAll(Map\u0026lt;UK，UV\u0026gt;) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、键和值的可迭代视图。你还可以通过 isEmpty() 来判断是否包含任何键值对。
所有类型的状态还有一个clear() 方法，清除当前 key 下的状态数据，也就是当前输入元素的 key。
请牢记，这些状态对象仅用于与状态交互。状态本身不一定存储在内存中，还可能在磁盘或其他位置。 另外需要牢记的是从状态中获取的值取决于输入元素所代表的 key。 因此，在不同 key 上调用同一个接口，可能得到不同的值。
你必须创建一个 StateDescriptor，才能得到对应的状态句柄。 这保存了状态名称（正如我们稍后将看到的，你可以创建多个状态，并且它们必须具有唯一的名称以便可以引用它们）， 状态所持有值的类型，并且可能包含用户指定的函数，例如ReduceFunction。 根据不同的状态类型，可以创建ValueStateDescriptor，ListStateDescriptor， AggregatingStateDescriptor, ReducingStateDescriptor 或 MapStateDescriptor。
状态通过 RuntimeContext 进行访问，因此只能在 rich functions 中使用。请参阅这里获取相关信息， 但是我们很快也会看到一个例子。RichFunction 中 RuntimeContext 提供如下方法：
ValueState\u0026lt;T\u0026gt; getState(ValueStateDescriptor\u0026lt;T\u0026gt;) ReducingState\u0026lt;T\u0026gt; getReducingState(ReducingStateDescriptor\u0026lt;T\u0026gt;) ListState\u0026lt;T\u0026gt; getListState(ListStateDescriptor\u0026lt;T\u0026gt;) AggregatingState\u0026lt;IN, OUT\u0026gt; getAggregatingState(AggregatingStateDescriptor\u0026lt;IN, ACC, OUT\u0026gt;) MapState\u0026lt;UK, UV\u0026gt; getMapState(MapStateDescriptor\u0026lt;UK, UV\u0026gt;) 下面是一个 FlatMapFunction 的例子，展示了如何将这些部分组合起来：
Java public class CountWindowAverage extends RichFlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; sum; @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; input, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) throws Exception { // access the state value Tuple2\u0026lt;Long, Long\u0026gt; currentSum = sum.value(); // update the count currentSum.f0 += 1; // add the second field of the input value currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 \u0026gt;= 2) { out.collect(new Tuple2\u0026lt;\u0026gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {}), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); } } // this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env) env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L)) .keyBy(value -\u0026gt; value.f0) .flatMap(new CountWindowAverage()) .print(); // the printed output will be (1,4) and (1,5) Scala class CountWindowAverage extends RichFlatMapFunction[(Long, Long), (Long, Long)] { private var sum: ValueState[(Long, Long)] = _ override def flatMap(input: (Long, Long), out: Collector[(Long, Long)]): Unit = { // access the state value val tmpCurrentSum = sum.value // If it hasn\u0026#39;t been used before, it will be null val currentSum = if (tmpCurrentSum != null) { tmpCurrentSum } else { (0L, 0L) } // update the count val newSum = (currentSum._1 + 1, currentSum._2 + input._2) // update the state sum.update(newSum) // if the count reaches 2, emit the average and clear the state if (newSum._1 \u0026gt;= 2) { out.collect((input._1, newSum._2 / newSum._1)) sum.clear() } } override def open(parameters: Configuration): Unit = { sum = getRuntimeContext.getState( new ValueStateDescriptor[(Long, Long)](\u0026#34;average\u0026#34;, createTypeInformation[(Long, Long)]) ) } } object ExampleCountWindowAverage extends App { val env = StreamExecutionEnvironment.getExecutionEnvironment env.fromCollection(List( (1L, 3L), (1L, 5L), (1L, 7L), (1L, 4L), (1L, 2L) )).keyBy(_._1) .flatMap(new CountWindowAverage()) .print() // the printed output will be (1,4) and (1,5) env.execute(\u0026#34;ExampleKeyedState\u0026#34;) } Python from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment, FlatMapFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor class CountWindowAverage(FlatMapFunction): def __init__(self): self.sum = None def open(self, runtime_context: RuntimeContext): descriptor = ValueStateDescriptor( \u0026#34;average\u0026#34;, # the state name Types.PICKLED_BYTE_ARRAY() # type information ) self.sum = runtime_context.get_state(descriptor) def flat_map(self, value): # access the state value current_sum = self.sum.value() if current_sum is None: current_sum = (0, 0) # update the count current_sum = (current_sum[0] + 1, current_sum[1] + value[1]) # update the state self.sum.update(current_sum) # if the count reaches 2, emit the average and clear the state if current_sum[0] \u0026gt;= 2: self.sum.clear() yield value[0], int(current_sum[1] / current_sum[0]) env = StreamExecutionEnvironment.get_execution_environment() env.from_collection([(1, 3), (1, 5), (1, 7), (1, 4), (1, 2)]) \\ .key_by(lambda row: row[0]) \\ .flat_map(CountWindowAverage()) \\ .print() env.execute() # the printed output will be (1,4) and (1,5) 这个例子实现了一个简单的计数窗口。 我们把元组的第一个元素当作 key（在示例中都 key 都是 \u0026ldquo;1\u0026rdquo;）。 该函数将出现的次数以及总和存储在 \u0026ldquo;ValueState\u0026rdquo; 中。 一旦出现次数达到 2，则将平均值发送到下游，并清除状态重新开始。 请注意，我们会为每个不同的 key（元组中第一个元素）保存一个单独的值。
状态有效期 (TTL) # 任何类型的 keyed state 都可以有 有效期 (TTL)。如果配置了 TTL 且状态值已过期，则会尽最大可能清除对应的值，这会在后面详述。
所有状态类型都支持单元素的 TTL。 这意味着列表元素和映射元素将独立到期。
在使用状态 TTL 前，需要先构建一个StateTtlConfig 配置对象。 然后把配置传递到 state descriptor 中启用 TTL 功能：
Java import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); ValueStateDescriptor\u0026lt;String\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;text state\u0026#34;, String.class); stateDescriptor.enableTimeToLive(ttlConfig); Scala import org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build val stateDescriptor = new ValueStateDescriptor[String](\u0026#34;text state\u0026#34;, classOf[String]) stateDescriptor.enableTimeToLive(ttlConfig) Python from pyflink.common.time import Time from pyflink.common.typeinfo import Types from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite) \\ .set_state_visibility(StateTtlConfig.StateVisibility.NeverReturnExpired) \\ .build() state_descriptor = ValueStateDescriptor(\u0026#34;text state\u0026#34;, Types.STRING()) state_descriptor.enable_time_to_live(ttl_config) TTL 配置有以下几个选项： newBuilder 的第一个参数表示数据的有效期，是必选项。
TTL 的更新策略（默认是 OnCreateAndWrite）：
StateTtlConfig.UpdateType.OnCreateAndWrite - 仅在创建和写入时更新
StateTtlConfig.UpdateType.OnReadAndWrite - 读取时也更新
(注意: 如果你同时将状态的可见性配置为 StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp， 那么在PyFlink作业中，状态的读缓存将会失效，这将导致一部分的性能损失)
数据在过期但还未被清理时的可见性配置如下（默认为 NeverReturnExpired):
StateTtlConfig.StateVisibility.NeverReturnExpired - 不返回过期数据
(注意: 在PyFlink作业中，状态的读写缓存都将失效，这将导致一部分的性能损失)
StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp - 会返回过期但未清理的数据
NeverReturnExpired 情况下，过期数据就像不存在一样，不管是否被物理删除。这对于不能访问过期数据的场景下非常有用，比如敏感数据。 ReturnExpiredIfNotCleanedUp 在数据被物理删除前都会返回。
注意:
状态上次的修改时间会和数据一起保存在 state backend 中，因此开启该特性会增加状态数据的存储。 Heap state backend 会额外存储一个包括用户状态以及时间戳的 Java 对象，RocksDB state backend 会在每个状态值（list 或者 map 的每个元素）序列化后增加 8 个字节。
暂时只支持基于 processing time 的 TTL。
尝试从 checkpoint/savepoint 进行恢复时，TTL 的状态（是否开启）必须和之前保持一致，否则会遇到 \u0026ldquo;StateMigrationException\u0026rdquo;。
TTL 的配置并不会保存在 checkpoint/savepoint 中，仅对当前 Job 有效。
当前开启 TTL 的 map state 仅在用户值序列化器支持 null 的情况下，才支持用户值为 null。如果用户值序列化器不支持 null， 可以用 NullableSerializer 包装一层。
启用 TTL 配置后，StateDescriptor 中的 defaultValue（已被标记 deprecated）将会失效。这个设计的目的是为了确保语义更加清晰，在此基础上，用户需要手动管理那些实际值为 null 或已过期的状态默认值。
过期数据的清理 # 默认情况下，过期数据会在读取的时候被删除，例如 ValueState#value，同时会有后台线程定期清理（如果 StateBackend 支持的话）。可以通过 StateTtlConfig 配置关闭后台清理：
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground() .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .disable_cleanup_in_background() \\ .build() 可以按照如下所示配置更细粒度的后台清理策略。当前的实现中 HeapStateBackend 依赖增量数据清理，RocksDBStateBackend 利用压缩过滤器进行后台清理。
全量快照时进行清理 # 另外，你可以启用全量快照时进行清理的策略，这可以减少整个快照的大小。当前实现中不会清理本地的状态，但从上次快照恢复时，不会恢复那些已经删除的过期数据。 该策略可以通过 StateTtlConfig 配置进行配置：
Java import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig import org.apache.flink.api.common.time.Time val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_full_snapshot() \\ .build() 这种策略在 RocksDBStateBackend 的增量 checkpoint 模式下无效。
注意:
这种清理方式可以在任何时候通过 StateTtlConfig 启用或者关闭，比如在从 savepoint 恢复时。 增量数据清理 # 另外可以选择增量式清理状态数据，在状态访问或/和处理时进行。如果某个状态开启了该清理策略，则会在存储后端保留一个所有状态的惰性全局迭代器。 每次触发增量清理时，从迭代器中选择已经过期的数进行清理。
该特性可以通过 StateTtlConfig 进行配置：
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_incrementally(10, True) \\ .build() 该策略有两个参数。 第一个是每次清理时检查状态的条目数，在每个状态访问时触发。第二个参数表示是否在处理每条记录时触发清理。 Heap backend 默认会检查 5 条状态，并且关闭在每条记录时触发清理。
注意:
如果没有 state 访问，也没有处理数据，则不会清理过期数据。 增量清理会增加数据处理的耗时。 现在仅 Heap state backend 支持增量清除机制。在 RocksDB state backend 上启用该特性无效。 如果 Heap state backend 使用同步快照方式，则会保存一份所有 key 的拷贝，从而防止并发修改问题，因此会增加内存的使用。但异步快照则没有这个问题。 对已有的作业，这个清理方式可以在任何时候通过 StateTtlConfig 启用或禁用该特性，比如从 savepoint 重启后。 在 RocksDB 压缩时清理 # 如果使用 RocksDB state backend，则会启用 Flink 为 RocksDB 定制的压缩过滤器。RocksDB 会周期性的对数据进行合并压缩从而减少存储空间。 Flink 提供的 RocksDB 压缩过滤器会在压缩时过滤掉已经过期的状态数据。
该特性可以通过 StateTtlConfig 进行配置：
Java import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build(); Scala import org.apache.flink.api.common.state.StateTtlConfig val ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build Python from pyflink.common.time import Time from pyflink.datastream.state import StateTtlConfig ttl_config = StateTtlConfig \\ .new_builder(Time.seconds(1)) \\ .cleanup_in_rocksdb_compact_filter(1000) \\ .build() Flink 处理一定条数的状态数据后，会使用当前时间戳来检测 RocksDB 中的状态是否已经过期， 你可以通过 StateTtlConfig.newBuilder(...).cleanupInRocksdbCompactFilter(long queryTimeAfterNumEntries) 方法指定处理状态的条数。 时间戳更新的越频繁，状态的清理越及时，但由于压缩会有调用 JNI 的开销，因此会影响整体的压缩性能。 RocksDB backend 的默认后台清理策略会每处理 1000 条数据进行一次。
你还可以通过配置开启 RocksDB 过滤器的 debug 日志： log4j.logger.org.rocksdb.FlinkCompactionFilter=DEBUG
注意:
压缩时调用 TTL 过滤器会降低速度。TTL 过滤器需要解析上次访问的时间戳，并对每个将参与压缩的状态进行是否过期检查。 对于集合型状态类型（比如 list 和 map），会对集合中每个元素进行检查。 对于元素序列化后长度不固定的列表状态，TTL 过滤器需要在每次 JNI 调用过程中，额外调用 Flink 的 java 序列化器， 从而确定下一个未过期数据的位置。 对已有的作业，这个清理方式可以在任何时候通过 StateTtlConfig 启用或禁用该特性，比如从 savepoint 重启后。 DataStream 状态相关的 Scala API # 除了上面描述的接口之外，Scala API 还在 KeyedStream 上对 map() 和 flatMap() 访问 ValueState 提供了一个更便捷的接口。 用户函数能够通过 Option 获取当前 ValueState 的值，并且返回即将保存到状态的值。
val stream: DataStream[(String, Int)] = ... val counts: DataStream[(String, Int)] = stream .keyBy(_._1) .mapWithState((in: (String, Int), count: Option[Int]) =\u0026gt; count match { case Some(c) =\u0026gt; ( (in._1, c), Some(c + in._2) ) case None =\u0026gt; ( (in._1, 0), Some(in._2) ) }) 算子状态 (Operator State) # 算子状态（或者非 keyed 状态）是绑定到一个并行算子实例的状态。Kafka Connector 是 Flink 中使用算子状态一个很具有启发性的例子。Kafka consumer 每个并行实例维护了 topic partitions 和偏移量的 map 作为它的算子状态。
当并行度改变的时候，算子状态支持将状态重新分发给各并行算子实例。处理重分发过程有多种不同的方案。
在典型的有状态 Flink 应用中你无需使用算子状态。它大都作为一种特殊类型的状态使用。用于实现 source/sink，以及无法对 state 进行分区而没有主键的这类场景中。
注意： Python DataStream API 仍无法支持算子状态。
广播状态 (Broadcast State) # 广播状态是一种特殊的算子状态。引入它的目的在于支持一个流中的元素需要广播到所有下游任务的使用情形。在这些任务中广播状态用于保持所有子任务状态相同。 该状态接下来可在第二个处理记录的数据流中访问。可以设想包含了一系列用于处理其他流中元素规则的低吞吐量数据流，这个例子自然而然地运用了广播状态。 考虑到上述这类使用情形，广播状态和其他算子状态的不同之处在于：
它具有 map 格式， 它仅在一些特殊的算子中可用。这些算子的输入为一个广播数据流和非广播数据流， 这类算子可以拥有不同命名的多个广播状态 。 注意： Python DataStream API 仍无法支持广播状态。
Back to top
使用 Operator State # 用户可以通过实现 CheckpointedFunction 接口来使用 operator state。
CheckpointedFunction # CheckpointedFunction 接口提供了访问 non-keyed state 的方法，需要实现如下两个方法：
void snapshotState(FunctionSnapshotContext context) throws Exception; void initializeState(FunctionInitializationContext context) throws Exception; 进行 checkpoint 时会调用 snapshotState()。 用户自定义函数初始化时会调用 initializeState()，初始化包括第一次自定义函数初始化和从之前的 checkpoint 恢复。 因此 initializeState() 不仅是定义不同状态类型初始化的地方，也需要包括状态恢复的逻辑。
当前 operator state 以 list 的形式存在。这些状态是一个 可序列化 对象的集合 List，彼此独立，方便在改变并发后进行状态的重新分派。 换句话说，这些对象是重新分配 non-keyed state 的最细粒度。根据状态的不同访问方式，有如下几种重新分配的模式：
Even-split redistribution: 每个算子都保存一个列表形式的状态集合，整个状态由所有的列表拼接而成。当作业恢复或重新分配的时候，整个状态会按照算子的并发度进行均匀分配。 比如说，算子 A 的并发读为 1，包含两个元素 element1 和 element2，当并发读增加为 2 时，element1 会被分到并发 0 上，element2 则会被分到并发 1 上。
Union redistribution: 每个算子保存一个列表形式的状态集合。整个状态由所有的列表拼接而成。当作业恢复或重新分配时，每个算子都将获得所有的状态数据。 Do not use this feature if your list may have high cardinality. Checkpoint metadata will store an offset to each list entry, which could lead to RPC framesize or out-of-memory errors.
下面的例子中的 SinkFunction 在 CheckpointedFunction 中进行数据缓存，然后统一发送到下游，这个例子演示了列表状态数据的 event-split redistribution。
Java public class BufferingSink implements SinkFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;, CheckpointedFunction { private final int threshold; private transient ListState\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; checkpointedState; private List\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; bufferedElements; public BufferingSink(int threshold) { this.threshold = threshold; this.bufferedElements = new ArrayList\u0026lt;\u0026gt;(); } @Override public void invoke(Tuple2\u0026lt;String, Integer\u0026gt; value, Context contex) throws Exception { bufferedElements.add(value); if (bufferedElements.size() \u0026gt;= threshold) { for (Tuple2\u0026lt;String, Integer\u0026gt; element: bufferedElements) { // send it to the sink } bufferedElements.clear(); } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { checkpointedState.clear(); for (Tuple2\u0026lt;String, Integer\u0026gt; element : bufferedElements) { checkpointedState.add(element); } } @Override public void initializeState(FunctionInitializationContext context) throws Exception { ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() {})); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) { for (Tuple2\u0026lt;String, Integer\u0026gt; element : checkpointedState.get()) { bufferedElements.add(element); } } } } Scala class BufferingSink(threshold: Int = 0) extends SinkFunction[(String, Int)] with CheckpointedFunction { @transient private var checkpointedState: ListState[(String, Int)] = _ private val bufferedElements = ListBuffer[(String, Int)]() override def invoke(value: (String, Int), context: Context): Unit = { bufferedElements += value if (bufferedElements.size \u0026gt;= threshold) { for (element \u0026lt;- bufferedElements) { // send it to the sink } bufferedElements.clear() } } override def snapshotState(context: FunctionSnapshotContext): Unit = { checkpointedState.clear() for (element \u0026lt;- bufferedElements) { checkpointedState.add(element) } } override def initializeState(context: FunctionInitializationContext): Unit = { val descriptor = new ListStateDescriptor[(String, Int)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Int)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) if(context.isRestored) { for(element \u0026lt;- checkpointedState.get().asScala) { bufferedElements += element } } } } initializeState 方法接收一个 FunctionInitializationContext 参数，会用来初始化 non-keyed state 的 \u0026ldquo;容器\u0026rdquo;。这些容器是一个 ListState 用于在 checkpoint 时保存 non-keyed state 对象。
注意这些状态是如何初始化的，和 keyed state 类似，StateDescriptor 会包括状态名字、以及状态类型相关信息。
Java ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() {})); checkpointedState = context.getOperatorStateStore().getListState(descriptor); Scala val descriptor = new ListStateDescriptor[(String, Long)]( \u0026#34;buffered-elements\u0026#34;, TypeInformation.of(new TypeHint[(String, Long)]() {}) ) checkpointedState = context.getOperatorStateStore.getListState(descriptor) 调用不同的获取状态对象的接口，会使用不同的状态分配算法。比如 getUnionListState(descriptor) 会使用 union redistribution 算法， 而 getListState(descriptor) 则简单的使用 even-split redistribution 算法。
当初始化好状态对象后，我们通过 isRestored() 方法判断是否从之前的故障中恢复回来，如果该方法返回 true 则表示从故障中进行恢复，会执行接下来的恢复逻辑。
正如代码所示，BufferingSink 中初始化时，恢复回来的 ListState 的所有元素会添加到一个局部变量中，供下次 snapshotState() 时使用。 然后清空 ListState，再把当前局部变量中的所有元素写入到 checkpoint 中。
另外，我们同样可以在 initializeState() 方法中使用 FunctionInitializationContext 初始化 keyed state。
带状态的 Source Function # 带状态的数据源比其他的算子需要注意更多东西。为了保证更新状态以及输出的原子性（用于支持 exactly-once 语义），用户需要在发送数据前获取数据源的全局锁。
Java public static class CounterSource extends RichParallelSourceFunction\u0026lt;Long\u0026gt; implements CheckpointedFunction { /** current offset for exactly once semantics */ private Long offset = 0L; /** flag for job cancellation */ private volatile boolean isRunning = true; /** 存储 state 的变量. */ private ListState\u0026lt;Long\u0026gt; state; @Override public void run(SourceContext\u0026lt;Long\u0026gt; ctx) { final Object lock = ctx.getCheckpointLock(); while (isRunning) { // output and state update are atomic synchronized (lock) { ctx.collect(offset); offset += 1; } } } @Override public void cancel() { isRunning = false; } @Override public void initializeState(FunctionInitializationContext context) throws Exception { state = context.getOperatorStateStore().getListState(new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state\u0026#34;, LongSerializer.INSTANCE)); // 从我们已保存的状态中恢复 offset 到内存中，在进行任务恢复的时候也会调用此初始化状态的方法 for (Long l : state.get()) { offset = l; } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { state.clear(); state.add(offset); } } Scala class CounterSource extends RichParallelSourceFunction[Long] with CheckpointedFunction { @volatile private var isRunning = true private var offset = 0L private var state: ListState[Long] = _ override def run(ctx: SourceFunction.SourceContext[Long]): Unit = { val lock = ctx.getCheckpointLock while (isRunning) { // output and state update are atomic lock.synchronized({ ctx.collect(offset) offset += 1 }) } } override def cancel(): Unit = isRunning = false override def initializeState(context: FunctionInitializationContext): Unit = { state = context.getOperatorStateStore.getListState( new ListStateDescriptor[Long](\u0026#34;state\u0026#34;, classOf[Long])) for (l \u0026lt;- state.get().asScala) { offset = l } } override def snapshotState(context: FunctionSnapshotContext): Unit = { state.clear() state.add(offset) } } 希望订阅 checkpoint 成功消息的算子，可以参考 org.apache.flink.api.common.state.CheckpointListener 接口。
Back to top
`}),e.add({id:47,href:"/flink/flink-docs-master/zh/docs/deployment/ha/zookeeper_ha/",title:"ZooKeeper 高可用服务",section:"高可用",content:` ZooKeeper 高可用服务 # Flink 的 ZooKeeper 高可用模式使用 ZooKeeper 提供高可用服务。
Flink 利用 ZooKeeper 在所有运行的 JobManager 实例之间进行 分布式协调。ZooKeeper 是一个独立于 Flink 的服务，它通过 leader 选举和轻量级的一致性状态存储来提供高可靠的分布式协调。查看 ZooKeeper入门指南，了解更多关于 ZooKeeper 的信息。Flink 包含 启动一个简单的ZooKeeper 的安装脚本。
配置 # 为了启用高可用集群（HA-cluster），你必须设置以下配置项:
high-availability (必要的): high-availability 配置项必须设置为 zookeeper。
high-availability: zookeeper high-availability.storageDir (必要的): JobManager 元数据持久化到文件系统 high-availability.storageDir 配置的路径中，并且在 ZooKeeper 中只能有一个目录指向此位置。
high-availability.storageDir: hdfs:///flink/recovery storageDir 存储要从 JobManager 失败恢复时所需的所有元数据。
high-availability.zookeeper.quorum (必要的): ZooKeeper quorum 是一个提供分布式协调服务的复制组。
high-availability.zookeeper.quorum: address1:2181[,...],addressX:2181 每个 addressX:port 指的是一个 ZooKeeper 服务器，它可以被 Flink 在给定的地址和端口上访问。
high-availability.zookeeper.path.root (推荐的): ZooKeeper 根节点，集群的所有节点都放在该节点下。
high-availability.zookeeper.path.root: /flink high-availability.cluster-id (推荐的): ZooKeeper cluster-id 节点，在该节点下放置集群所需的协调数据。
high-availability.cluster-id: /default_ns # important: customize per cluster 重要: 在 YARN、原生 Kubernetes 或其他集群管理器上运行时，不应该手动设置此值。在这些情况下，将自动生成一个集群 ID。如果在未使用集群管理器的机器上运行多个 Flink 高可用集群，则必须为每个集群手动配置单独的集群 ID（cluster-ids）。
配置示例 # 在 conf/flink-conf.yaml 中配置高可用模式和 ZooKeeper 复制组（quorum）:
high-availability: zookeeper high-availability.zookeeper.quorum: localhost:2181 high-availability.zookeeper.path.root: /flink high-availability.cluster-id: /cluster_one # 重要: 每个集群自定义 high-availability.storageDir: hdfs:///flink/recovery Back to top
ZooKeeper 安全配置 # 如果 ZooKeeper 使用 Kerberos 以安全模式运行，必要时可以在 flink-conf.yaml 中覆盖以下配置:
# 默认配置为 \u0026#34;zookeeper\u0026#34;. 如果 ZooKeeper quorum 配置了不同的服务名称， # 那么可以替换到这里。 zookeeper.sasl.service-name: zookeeper # 默认配置为 \u0026#34;Client\u0026#34;. 该值必须为 \u0026#34;security.kerberos.login.contexts\u0026#34; 项中配置的某一个值。 zookeeper.sasl.login-context-name: Client 有关用于 Kerberos 安全性的 Flink 配置的更多信息，请参阅 Flink 配置页面的安全性部分。你还可以找到关于 Flink 如何在内部设置基于 kerberos 的安全性 的详细信息。
Back to top
Advanced Configuration # Tolerating Suspended ZooKeeper Connections # Per default, Flink\u0026rsquo;s ZooKeeper client treats suspended ZooKeeper connections as an error. This means that Flink will invalidate all leaderships of its components and thereby triggering a failover if a connection is suspended.
This behaviour might be too disruptive in some cases (e.g., unstable network environment). If you are willing to take a more aggressive approach, then you can tolerate suspended ZooKeeper connections and only treat lost connections as an error via high-availability.zookeeper.client.tolerate-suspended-connections. Enabling this feature will make Flink more resilient against temporary connection problems but also increase the risk of running into ZooKeeper timing problems.
For more information take a look at Curator\u0026rsquo;s error handling.
ZooKeeper 版本 # Flink 附带了 3.4 和 3.5 的单独的 ZooKeeper 客户端，其中 3.4 位于发行版的 lib 目录中，为默认使用版本，而 3.5 位于 opt 目录中。
3.5 客户端允许你通过 SSL 保护 ZooKeeper 连接，但 可能 不适用于 3.4 版本的 ZooKeeper 安装。
你可以通过在 lib 目录中放置任意一个 jar 来控制 Flink 使用哪个版本。
Back to top
启动 ZooKeeper # 如果你没有安装 ZooKeeper，可以使用 Flink 附带的帮助脚本。
在 conf/zoo.cfg 文件中有 ZooKeeper 的配置模板。你可以在 server.X 配置项中配置主机来运行 ZooKeeper。其中 X 是每个服务器的唯一 ID:
server.X=addressX:peerPort:leaderPort [...] server.Y=addressY:peerPort:leaderPort 脚本 bin/start-zookeeper-quorum.sh 将在每个配置的主机上启动一个 ZooKeeper 服务。该进程是通过 Flink 包装器来启动的，该包装器从 conf/zoo.cfg 读取配置，并确保设置一些必要的配置值以方便使用。
在生产环境中，建议你自行管理 ZooKeeper 的安装与部署。
Back to top
`}),e.add({id:48,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/windows/",title:"窗口",section:"算子",content:` 窗口 # 窗口（Window）是处理无界流的关键所在。窗口可以将数据流装入大小有限的“桶”中，再对每个“桶”加以处理。 本文的重心将放在 Flink 如何进行窗口操作以及开发者如何尽可能地利用 Flink 所提供的功能。
下面展示了 Flink 窗口在 keyed streams 和 non-keyed streams 上使用的基本结构。 我们可以看到，这两者唯一的区别仅在于：keyed streams 要调用 keyBy(...)后再调用 window(...) ， 而 non-keyed streams 只用直接调用 windowAll(...)。留意这个区别，它能帮我们更好地理解后面的内容。
Keyed Windows
Java/Scala stream .keyBy(...) \u0026lt;- 仅 keyed 窗口需要 .window(...) \u0026lt;- 必填项：\u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选项：\u0026quot;trigger\u0026quot; (省略则使用默认 trigger) [.evictor(...)] \u0026lt;- 可选项：\u0026quot;evictor\u0026quot; (省略则不使用 evictor) [.allowedLateness(...)] \u0026lt;- 可选项：\u0026quot;lateness\u0026quot; (省略则为 0) [.sideOutputLateData(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; (省略则不对迟到数据使用 side output) .reduce/aggregate/apply() \u0026lt;- 必填项：\u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; Python stream .key_by(...) \u0026lt;- 仅 keyed 窗口需要 .window(...) \u0026lt;- 必填项：\u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选项：\u0026quot;trigger\u0026quot; (省略则使用默认 trigger) [.allowed_lateness(...)] \u0026lt;- 可选项：\u0026quot;lateness\u0026quot; (省略则为 0) [.side_output_late_data(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; (省略则不对迟到数据使用 side output) .reduce/aggregate/apply() \u0026lt;- 必填项：\u0026quot;function\u0026quot; [.get_side_output(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; Non-Keyed Windows
Java/Scala stream .windowAll(...) \u0026lt;- 必填项：\u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选项：\u0026quot;trigger\u0026quot; (else default trigger) [.evictor(...)] \u0026lt;- 可选项：\u0026quot;evictor\u0026quot; (else no evictor) [.allowedLateness(...)] \u0026lt;- 可选项：\u0026quot;lateness\u0026quot; (else zero) [.sideOutputLateData(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- 必填项：\u0026quot;function\u0026quot; [.getSideOutput(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; Python stream .window_all(...) \u0026lt;- 必填项：\u0026quot;assigner\u0026quot; [.trigger(...)] \u0026lt;- 可选项：\u0026quot;trigger\u0026quot; (else default trigger) [.allowed_lateness(...)] \u0026lt;- 可选项：\u0026quot;lateness\u0026quot; (else zero) [.side_output_late_data(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; (else no side output for late data) .reduce/aggregate/apply() \u0026lt;- 必填项：\u0026quot;function\u0026quot; [.get_side_output(...)] \u0026lt;- 可选项：\u0026quot;output tag\u0026quot; 上面方括号（[\u0026hellip;]）中的命令是可选的。也就是说，Flink 允许你自定义多样化的窗口操作来满足你的需求。 Note: Evictor 在 Python DataStream API 中还不支持. 窗口的生命周期 # 简单来说，一个窗口在第一个属于它的元素到达时就会被创建，然后在时间（event 或 processing time） 超过窗口的“结束时间戳 + 用户定义的 allowed lateness （详见 Allowed Lateness）”时 被完全删除。Flink 仅保证删除基于时间的窗口，其他类型的窗口不做保证， 比如全局窗口（详见 Window Assigners）。 例如，对于一个基于 event time 且范围互不重合（滚动）的窗口策略， 如果窗口设置的时长为五分钟、可容忍的迟到时间（allowed lateness）为 1 分钟， 那么第一个元素落入 12:00 至 12:05 这个区间时，Flink 就会为这个区间创建一个新的窗口。 当 watermark 越过 12:06 时，这个窗口将被摧毁。
另外，每个窗口会设置自己的 Trigger （详见 Triggers）和 function (ProcessWindowFunction、ReduceFunction、或 AggregateFunction， 详见 Window Functions）。该 function 决定如何计算窗口中的内容， 而 Trigger 决定何时窗口中的数据可以被 function 计算。 Trigger 的触发（fire）条件可能是“当窗口中有多于 4 条数据”或“当 watermark 越过窗口的结束时间”等。 Trigger 还可以在 window 被创建后、删除前的这段时间内定义何时清理（purge）窗口中的数据。 这里的数据仅指窗口内的元素，不包括窗口的 meta data。也就是说，窗口在 purge 后仍然可以加入新的数据。
除此之外，你也可以指定一个 Evictor （详见 Evictors），在 trigger 触发之后，Evictor 可以在窗口函数的前后删除数据。
接下来我们会更详细地介绍上面提到的内容。开头的例子中有必填项和可选项。 我们先从必填项开始（详见 Keyed vs Non-Keyed Windows、 Window Assigners、Window Functions）。
Keyed 和 Non-Keyed Windows # 首先必须要在定义窗口前确定的是你的 stream 是 keyed 还是 non-keyed。 keyBy(...) 会将你的无界 stream 分割为逻辑上的 keyed stream。 如果 keyBy(...) 没有被调用，你的 stream 就不是 keyed。
对于 keyed stream，其中数据的任何属性都可以作为 key （详见此处）。 使用 keyed stream 允许你的窗口计算由多个 task 并行，因为每个逻辑上的 keyed stream 都可以被单独处理。 属于同一个 key 的元素会被发送到同一个 task。
对于 non-keyed stream，原始的 stream 不会被分割为多个逻辑上的 stream， 所以所有的窗口计算会被同一个 task 完成，也就是 parallelism 为 1。
Window Assigners # 指定了你的 stream 是否为 keyed 之后，下一步就是定义 window assigner。
Window assigner 定义了 stream 中的元素如何被分发到各个窗口。 你可以在 window(...)（用于 keyed streams）或 windowAll(...) （用于 non-keyed streams）中指定一个 WindowAssigner。 WindowAssigner 负责将 stream 中的每个数据分发到一个或多个窗口中。 Flink 为最常用的情况提供了一些定义好的 window assigner，也就是 tumbling windows、 sliding windows、 session windows 和 global windows。 你也可以继承 WindowAssigner 类来实现自定义的 window assigner。 所有内置的 window assigner（除了 global window）都是基于时间分发数据的，processing time 或 event time 均可。 请阅读我们对于 event time 的介绍来了解这两者的区别， 以及 timestamp 和 watermark 是如何产生的。
基于时间的窗口用 start timestamp（包含）和 end timestamp（不包含）描述窗口的大小。 在代码中，Flink 处理基于时间的窗口使用的是 TimeWindow， 它有查询开始和结束 timestamp 以及返回窗口所能储存的最大 timestamp 的方法 maxTimestamp()。
接下来我们会说明 Flink 内置的 window assigner 如何工作，以及他们如何用在 DataStream 程序中。 下面的图片展示了每种 assigner 如何工作。 紫色的圆圈代表 stream 中按 key 划分的元素（本例中是按 user 1、user 2 和 user 3 划分）。 x 轴表示时间的进展。
滚动窗口（Tumbling Windows） # 滚动窗口的 assigner 分发元素到指定大小的窗口。滚动窗口的大小是固定的，且各自范围之间不重叠。 比如说，如果你指定了滚动窗口的大小为 5 分钟，那么每 5 分钟就会有一个窗口被计算，且一个新的窗口被创建（如下图所示）。
下面的代码展示了如何使用滚动窗口。
Java DataStream\u0026lt;T\u0026gt; input = ...; // 滚动 event-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 滚动 processing-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 长度为一天的滚动 event-time 窗口， 偏移量为 -8 小时。 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // 滚动 event-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 滚动 processing-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 长度为一天的滚动 event-time 窗口，偏移量为 -8 小时。 input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream # 滚动 event-time 窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 滚动 processing-time 窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 长度为一天的滚动 event-time 窗口，偏移量为 -8 小时。 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 时间间隔可以用 Time.milliseconds(x)、Time.seconds(x)、Time.minutes(x) 等来指定。
如上一个例子所示，滚动窗口的 assigners 也可以传入可选的 offset 参数。这个参数可以用来对齐窗口。 比如说，不设置 offset 时，长度为一小时的滚动窗口会与 linux 的 epoch 对齐。 你会得到如 1:00:00.000 - 1:59:59.999、2:00:00.000 - 2:59:59.999 等。 如果你想改变对齐方式，你可以设置一个 offset。如果设置了 15 分钟的 offset， 你会得到 1:15:00.000 - 2:14:59.999、2:15:00.000 - 3:14:59.999 等。 一个重要的 offset 用例是根据 UTC-0 调整窗口的时差。比如说，在中国你可能会设置 offset 为 Time.hours(-8)。
滑动窗口（Sliding Windows） # 与滚动窗口类似，滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 window size 参数设置。 滑动窗口需要一个额外的滑动距离（window slide）参数来控制生成新窗口的频率。 因此，如果 slide 小于窗口大小，滑动窗口可以允许窗口重叠。这种情况下，一个元素可能会被分发到多个窗口。
比如说，你设置了大小为 10 分钟，滑动距离 5 分钟的窗口，你会在每 5 分钟得到一个新的窗口， 里面包含之前 10 分钟到达的数据（如下图所示）。
下面的代码展示了如何使用滑动窗口。
Java DataStream\u0026lt;T\u0026gt; input = ...; // 滑动 event-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 滑动 processing-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 滑动 processing-time 窗口，偏移量为 -8 小时 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // 滑动 event-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 滑动 processing-time 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 滑动 processing-time 窗口，偏移量为 -8 小时 input .keyBy(\u0026lt;key selector\u0026gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream # 滑动 event-time 窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 滑动 processing-time 窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 滑动 processing-time 窗口，偏移量为 -8 小时 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 时间间隔可以使用 Time.milliseconds(x)、Time.seconds(x)、Time.minutes(x) 等来指定。
如上一个例子所示，滚动窗口的 assigners 也可以传入可选的 offset 参数。这个参数可以用来对齐窗口。 比如说，不设置 offset 时，长度为一小时、滑动距离为 30 分钟的滑动窗口会与 linux 的 epoch 对齐。 你会得到如 1:00:00.000 - 1:59:59.999, 1:30:00.000 - 2:29:59.999 等。 如果你想改变对齐方式，你可以设置一个 offset。 如果设置了 15 分钟的 offset，你会得到 1:15:00.000 - 2:14:59.999、1:45:00.000 - 2:44:59.999 等。 一个重要的 offset 用例是根据 UTC-0 调整窗口的时差。比如说，在中国你可能会设置 offset 为 Time.hours(-8)。
会话窗口（Session Windows） # 会话窗口的 assigner 会把数据按活跃的会话分组。 与滚动窗口和滑动窗口不同，会话窗口不会相互重叠，且没有固定的开始或结束时间。 会话窗口在一段时间没有收到数据之后会关闭，即在一段不活跃的间隔之后。 会话窗口的 assigner 可以设置固定的会话间隔（session gap）或 用 session gap extractor 函数来动态地定义多长时间算作不活跃。 当超出了不活跃的时间段，当前的会话就会关闭，并且将接下来的数据分发到新的会话窗口。
下面的代码展示了如何使用会话窗口。
Java DataStream\u0026lt;T\u0026gt; input = ...; // 设置了固定间隔的 event-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 设置了动态间隔的 event-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withDynamicGap((element) -\u0026gt; { // 决定并返回会话间隔 })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 设置了固定间隔的 processing-time session 窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); // 设置了动态间隔的 processing-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withDynamicGap((element) -\u0026gt; { // 决定并返回会话间隔 })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... // 设置了固定间隔的 event-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 设置了动态间隔的 event-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(EventTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // 决定并返回会话间隔 } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 设置了固定间隔的 processing-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) // 设置了动态间隔的 processing-time 会话窗口 input .keyBy(\u0026lt;key selector\u0026gt;) .window(DynamicProcessingTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] { override def extract(element: String): Long = { // 决定并返回会话间隔 } })) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream class MySessionWindowTimeGapExtractor(SessionWindowTimeGapExtractor): def extract(self, element: tuple) -\u0026gt; int: # 决定并返回会话间隔 # 设置了固定间隔的 event-time 会话窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(EventTimeSessionWindows.with_gap(Time.minutes(10))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 设置了动态间隔的 event-time 会话窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(EventTimeSessionWindows.with_dynamic_gap(MySessionWindowTimeGapExtractor())) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 设置了固定间隔的 processing-time 会话窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(ProcessingTimeSessionWindows.with_gap(Time.minutes(10))) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) # 设置了动态间隔的 processing-time 会话窗口 input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(DynamicProcessingTimeSessionWindows.with_dynamic_gap(MySessionWindowTimeGapExtractor())) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 固定间隔可以使用 Time.milliseconds(x)、Time.seconds(x)、Time.minutes(x) 等来设置。
动态间隔可以通过实现 SessionWindowTimeGapExtractor 接口来指定。
会话窗口并没有固定的开始或结束时间，所以它的计算方法与滑动窗口和滚动窗口不同。在 Flink 内部，会话窗口的算子会为每一条数据创建一个窗口， 然后将距离不超过预设间隔的窗口合并。 想要让窗口可以被合并，会话窗口需要拥有支持合并的 Trigger 和 Window Function， 比如说 ReduceFunction、AggregateFunction 或 ProcessWindowFunction。 全局窗口（Global Windows） # 全局窗口的 assigner 将拥有相同 key 的所有数据分发到一个全局窗口。 这样的窗口模式仅在你指定了自定义的 trigger 时有用。 否则，计算不会发生，因为全局窗口没有天然的终点去触发其中积累的数据。
下面的代码展示了如何使用全局窗口。
Java DataStream\u0026lt;T\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(GlobalWindows.create()) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(GlobalWindows.create()) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(GlobalWindows.create()) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 窗口函数（Window Functions） # 定义了 window assigner 之后，我们需要指定当窗口触发之后，我们如何计算每个窗口中的数据， 这就是 window function 的职责了。关于窗口如何触发，详见 triggers。
窗口函数有三种：ReduceFunction、AggregateFunction 或 ProcessWindowFunction。 前两者执行起来更高效（详见 State Size）因为 Flink 可以在每条数据到达窗口后 进行增量聚合（incrementally aggregate）。 而 ProcessWindowFunction 会得到能够遍历当前窗口内所有数据的 Iterable，以及关于这个窗口的 meta-information。
使用 ProcessWindowFunction 的窗口转换操作没有其他两种函数高效，因为 Flink 在窗口触发前必须缓存里面的所有数据。 ProcessWindowFunction 可以与 ReduceFunction 或 AggregateFunction 合并来提高效率。 这样做既可以增量聚合窗口内的数据，又可以从 ProcessWindowFunction 接收窗口的 metadata。 我们接下来看看每种函数的例子。
ReduceFunction # ReduceFunction 指定两条输入数据如何合并起来产生一条输出数据，输入和输出数据的类型必须相同。 Flink 使用 ReduceFunction 对窗口中的数据进行增量聚合。
ReduceFunction 可以像下面这样定义：
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce(new ReduceFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt;() { public Tuple2\u0026lt;String, Long\u0026gt; reduce(Tuple2\u0026lt;String, Long\u0026gt; v1, Tuple2\u0026lt;String, Long\u0026gt; v2) { return new Tuple2\u0026lt;\u0026gt;(v1.f0, v1.f1 + v2.f1); } }); Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce { (v1, v2) =\u0026gt; (v1._1, v1._2 + v2._2) } Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .reduce(lambda v1, v2: (v1[0], v1[1] + v2[1]), output_type=Types.TUPLE([Types.STRING(), Types.LONG()])) 上面的例子是对窗口内元组的第二个属性求和。
AggregateFunction # ReduceFunction 是 AggregateFunction 的特殊情况。 AggregateFunction 接收三个类型：输入数据的类型(IN)、累加器的类型（ACC）和输出数据的类型（OUT）。 输入数据的类型是输入流的元素类型，AggregateFunction 接口有如下几个方法： 把每一条元素加进累加器、创建初始累加器、合并两个累加器、从累加器中提取输出（OUT 类型）。我们通过下例说明。
与 ReduceFunction 相同，Flink 会在输入数据到达窗口时直接进行增量聚合。
AggregateFunction 可以像下面这样定义：
Java /** * The accumulator is used to keep a running sum and a count. The {@code getResult} method * computes the average. */ private static class AverageAggregate implements AggregateFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Double\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; createAccumulator() { return new Tuple2\u0026lt;\u0026gt;(0L, 0L); } @Override public Tuple2\u0026lt;Long, Long\u0026gt; add(Tuple2\u0026lt;String, Long\u0026gt; value, Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return new Tuple2\u0026lt;\u0026gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); } @Override public Double getResult(Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return ((double) accumulator.f0) / accumulator.f1; } @Override public Tuple2\u0026lt;Long, Long\u0026gt; merge(Tuple2\u0026lt;Long, Long\u0026gt; a, Tuple2\u0026lt;Long, Long\u0026gt; b) { return new Tuple2\u0026lt;\u0026gt;(a.f0 + b.f0, a.f1 + b.f1); } } DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate()); Scala /** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate) Python class AverageAggregate(AggregateFunction): def create_accumulator(self) -\u0026gt; Tuple[int, int]: return 0, 0 def add(self, value: Tuple[str, int], accumulator: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return accumulator[0] + value[1], accumulator[1] + 1 def get_result(self, accumulator: Tuple[int, int]) -\u0026gt; float: return accumulator[0] / accumulator[1] def merge(self, a: Tuple[int, int], b: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return a[0] + b[0], a[1] + b[1] input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .aggregate(AverageAggregate(), accumulator_type=Types.TUPLE([Types.LONG(), Types.LONG()]), output_type=Types.DOUBLE()) 上例计算了窗口内所有元素第二个属性的平均值。
ProcessWindowFunction # ProcessWindowFunction 有能获取包含窗口内所有元素的 Iterable， 以及用来获取时间和状态信息的 Context 对象，比其他窗口函数更加灵活。 ProcessWindowFunction 的灵活性是以性能和资源消耗为代价的， 因为窗口中的数据无法被增量聚合，而需要在窗口触发前缓存所有数据。
ProcessWindowFunction 的签名如下：
Java public abstract class ProcessWindowFunction\u0026lt;IN, OUT, KEY, W extends Window\u0026gt; implements Function { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public abstract void process( KEY key, Context context, Iterable\u0026lt;IN\u0026gt; elements, Collector\u0026lt;OUT\u0026gt; out) throws Exception; /** * Deletes any state in the {@code Context} when the Window expires (the watermark passes its * {@code maxTimestamp} + {@code allowedLateness}). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ public void clear(Context context) throws Exception {} /** * The context holding window metadata. */ public abstract class Context implements java.io.Serializable { /** * Returns the window that is being evaluated. */ public abstract W window(); /** Returns the current processing time. */ public abstract long currentProcessingTime(); /** Returns the current event-time watermark. */ public abstract long currentWatermark(); /** * State accessor for per-key and per-window state. * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;NOTE:\u0026lt;/b\u0026gt;If you use per-window state you have to ensure that you clean it up * by implementing {@link ProcessWindowFunction#clear(Context)}. */ public abstract KeyedStateStore windowState(); /** * State accessor for per-key global state. */ public abstract KeyedStateStore globalState(); } } Scala abstract class ProcessWindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param context The context in which the window is being evaluated. * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def process( key: KEY, context: Context, elements: Iterable[IN], out: Collector[OUT]) /** * Deletes any state in the [[Context]] when the Window expires * (the watermark passes its \`maxTimestamp\` + \`allowedLateness\`). * * @param context The context to which the window is being evaluated * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ @throws[Exception] def clear(context: Context) {} /** * The context holding window metadata */ abstract class Context { /** * Returns the window that is being evaluated. */ def window: W /** * Returns the current processing time. */ def currentProcessingTime: Long /** * Returns the current event-time watermark. */ def currentWatermark: Long /** * State accessor for per-key and per-window state. */ def windowState: KeyedStateStore /** * State accessor for per-key global state. */ def globalState: KeyedStateStore } } Python class ProcessWindowFunction(Function, Generic[IN, OUT, KEY, W]): @abstractmethod def process(self, key: KEY, context: \u0026#39;ProcessWindowFunction.Context\u0026#39;, elements: Iterable[IN]) -\u0026gt; Iterable[OUT]: \u0026#34;\u0026#34;\u0026#34; Evaluates the window and outputs none or several elements. :param key: The key for which this window is evaluated. :param context: The context in which the window is being evaluated. :param elements: The elements in the window being evaluated. :return: The iterable object which produces the elements to emit. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def clear(self, context: \u0026#39;ProcessWindowFunction.Context\u0026#39;) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Deletes any state in the :class:\`Context\` when the Window expires (the watermark passes its max_timestamp + allowed_lateness). :param context: The context to which the window is being evaluated. \u0026#34;\u0026#34;\u0026#34; pass class Context(ABC, Generic[W2]): \u0026#34;\u0026#34;\u0026#34; The context holding window metadata. \u0026#34;\u0026#34;\u0026#34; @abstractmethod def window(self) -\u0026gt; W2: \u0026#34;\u0026#34;\u0026#34; :return: The window that is being evaluated. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def current_processing_time(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; :return: The current processing time. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def current_watermark(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; :return: The current event-time watermark. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def window_state(self) -\u0026gt; KeyedStateStore: \u0026#34;\u0026#34;\u0026#34; State accessor for per-key and per-window state. .. note:: If you use per-window state you have to ensure that you clean it up by implementing :func:\`~ProcessWindowFunction.clear\`. :return: The :class:\`KeyedStateStore\` used to access per-key and per-window states. \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def global_state(self) -\u0026gt; KeyedStateStore: \u0026#34;\u0026#34;\u0026#34; State accessor for per-key global state. \u0026#34;\u0026#34;\u0026#34; pass key 参数由 keyBy() 中指定的 KeySelector 选出。 如果是给出 key 在 tuple 中的 index 或用属性名的字符串形式指定 key，这个 key 的类型将总是 Tuple， 并且你需要手动将它转换为正确大小的 tuple 才能提取 key。
ProcessWindowFunction 可以像下面这样定义：
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(t -\u0026gt; t.f0) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction()); /* ... */ public class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, String, String, TimeWindow\u0026gt; { @Override public void process(String key, Context context, Iterable\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input, Collector\u0026lt;String\u0026gt; out) { long count = 0; for (Tuple2\u0026lt;String, Long\u0026gt; in: input) { count++; } out.collect(\u0026#34;Window: \u0026#34; + context.window() + \u0026#34;count: \u0026#34; + count); } } Scala val input: DataStream[(String, Long)] = ... input .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction()) /* ... */ class MyProcessWindowFunction extends ProcessWindowFunction[(String, Long), String, String, TimeWindow] { def process(key: String, context: Context, input: Iterable[(String, Long)], out: Collector[String]) = { var count = 0L for (in \u0026lt;- input) { count = count + 1 } out.collect(s\u0026#34;Window \${context.window} count: \$count\u0026#34;) } } Python input = ... # type: DataStream input \\ .key_by(lambda v: v[0]) \\ .window(TumblingEventTimeWindows.of(Time.minutes(5))) \\ .process(MyProcessWindowFunction()) # ... class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, elements: Iterable[Tuple[str, int]]) -\u0026gt; Iterable[str]: count = 0 for _ in elements: count += 1 yield \u0026#34;Window: {} count: {}\u0026#34;.format(context.window(), count) 上例使用 ProcessWindowFunction 对窗口中的元素计数，并且将窗口本身的信息一同输出。
注意，使用 ProcessWindowFunction 完成简单的聚合任务是非常低效的。 下一章会说明如何将 ReduceFunction 或 AggregateFunction 与 ProcessWindowFunction 组合成既能 增量聚合又能获得窗口额外信息的窗口函数。 增量聚合的 ProcessWindowFunction # ProcessWindowFunction 可以与 ReduceFunction 或 AggregateFunction 搭配使用， 使其能够在数据到达窗口的时候进行增量聚合。当窗口关闭时，ProcessWindowFunction 将会得到聚合的结果。 这样它就可以增量聚合窗口的元素并且从 ProcessWindowFunction\` 中获得窗口的元数据。
你也可以对过时的 WindowFunction 使用增量聚合。
使用 ReduceFunction 增量聚合 # 下例展示了如何将 ReduceFunction 与 ProcessWindowFunction 组合，返回窗口中的最小元素和窗口的开始时间。
Java DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce(new MyReduceFunction(), new MyProcessWindowFunction()); // Function definitions private static class MyReduceFunction implements ReduceFunction\u0026lt;SensorReading\u0026gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() \u0026gt; r2.value() ? r2 : r1; } } private static class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;SensorReading, Tuple2\u0026lt;Long, SensorReading\u0026gt;, String, TimeWindow\u0026gt; { public void process(String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; minReadings, Collector\u0026lt;Tuple2\u0026lt;Long, SensorReading\u0026gt;\u0026gt; out) { SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2\u0026lt;Long, SensorReading\u0026gt;(context.window().getStart(), min)); } } Scala val input: DataStream[SensorReading] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce( (r1: SensorReading, r2: SensorReading) =\u0026gt; { if (r1.value \u0026gt; r2.value) r2 else r1 }, ( key: String, context: ProcessWindowFunction[_, _, _, TimeWindow]#Context, minReadings: Iterable[SensorReading], out: Collector[(Long, SensorReading)] ) =\u0026gt; { val min = minReadings.iterator.next() out.collect((context.window.getStart, min)) } ) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .reduce(lambda r1, r2: r2 if r1.value \u0026gt; r2.value else r1, window_function=MyProcessWindowFunction(), output_type=Types.TUPLE([Types.STRING(), Types.LONG()])) # Function definition class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, min_readings: Iterable[SensorReading]) -\u0026gt; Iterable[Tuple[int, SensorReading]]: min = next(iter(min_readings)) yield context.window().start, min 使用 AggregateFunction 增量聚合 # 下例展示了如何将 AggregateFunction 与 ProcessWindowFunction 组合，计算平均值并与窗口对应的 key 一同输出。
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction()); // Function definitions /** * The accumulator is used to keep a running sum and a count. The {@code getResult} method * computes the average. */ private static class AverageAggregate implements AggregateFunction\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Double\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; createAccumulator() { return new Tuple2\u0026lt;\u0026gt;(0L, 0L); } @Override public Tuple2\u0026lt;Long, Long\u0026gt; add(Tuple2\u0026lt;String, Long\u0026gt; value, Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return new Tuple2\u0026lt;\u0026gt;(accumulator.f0 + value.f1, accumulator.f1 + 1L); } @Override public Double getResult(Tuple2\u0026lt;Long, Long\u0026gt; accumulator) { return ((double) accumulator.f0) / accumulator.f1; } @Override public Tuple2\u0026lt;Long, Long\u0026gt; merge(Tuple2\u0026lt;Long, Long\u0026gt; a, Tuple2\u0026lt;Long, Long\u0026gt; b) { return new Tuple2\u0026lt;\u0026gt;(a.f0 + b.f0, a.f1 + b.f1); } } private static class MyProcessWindowFunction extends ProcessWindowFunction\u0026lt;Double, Tuple2\u0026lt;String, Double\u0026gt;, String, TimeWindow\u0026gt; { public void process(String key, Context context, Iterable\u0026lt;Double\u0026gt; averages, Collector\u0026lt;Tuple2\u0026lt;String, Double\u0026gt;\u0026gt; out) { Double average = averages.iterator().next(); out.collect(new Tuple2\u0026lt;\u0026gt;(key, average)); } } Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .aggregate(new AverageAggregate(), new MyProcessWindowFunction()) // Function definitions /** * The accumulator is used to keep a running sum and a count. The [getResult] method * computes the average. */ class AverageAggregate extends AggregateFunction[(String, Long), (Long, Long), Double] { override def createAccumulator() = (0L, 0L) override def add(value: (String, Long), accumulator: (Long, Long)) = (accumulator._1 + value._2, accumulator._2 + 1L) override def getResult(accumulator: (Long, Long)) = accumulator._1 / accumulator._2 override def merge(a: (Long, Long), b: (Long, Long)) = (a._1 + b._1, a._2 + b._2) } class MyProcessWindowFunction extends ProcessWindowFunction[Double, (String, Double), String, TimeWindow] { def process(key: String, context: Context, averages: Iterable[Double], out: Collector[(String, Double)]) = { val average = averages.iterator.next() out.collect((key, average)) } } Python input = ... # type: DataStream input .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .aggregate(AverageAggregate(), window_function=MyProcessWindowFunction(), accumulator_type=Types.TUPLE([Types.LONG(), Types.LONG()]), output_type=Types.TUPLE([Types.STRING(), Types.DOUBLE()])) # Function definitions class AverageAggregate(AggregateFunction): \u0026#34;\u0026#34;\u0026#34; The accumulator is used to keep a running sum and a count. The :func:\`get_result\` method computes the average. \u0026#34;\u0026#34;\u0026#34; def create_accumulator(self) -\u0026gt; Tuple[int, int]: return 0, 0 def add(self, value: Tuple[str, int], accumulator: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return accumulator[0] + value[1], accumulator[1] + 1 def get_result(self, accumulator: Tuple[int, int]) -\u0026gt; float: return accumulator[0] / accumulator[1] def merge(self, a: Tuple[int, int], b: Tuple[int, int]) -\u0026gt; Tuple[int, int]: return a[0] + b[0], a[1] + b[1] class MyProcessWindowFunction(ProcessWindowFunction): def process(self, key: str, context: ProcessWindowFunction.Context, averages: Iterable[float]) -\u0026gt; Iterable[Tuple[str, float]]: average = next(iter(averages)) yield key, average 在 ProcessWindowFunction 中使用 per-window state # 除了访问 keyed state （任何富函数都可以），ProcessWindowFunction 还可以使用作用域仅为 “当前正在处理的窗口”的 keyed state。在这种情况下，理解 per-window 中的 window 指的是什么非常重要。 总共有以下几种窗口的理解：
在窗口操作中定义的窗口：比如定义了长一小时的滚动窗口或长两小时、滑动一小时的滑动窗口。 对应某个 key 的窗口实例：比如 以 user-id xyz 为 key，从 12:00 到 13:00 的时间窗口。 具体情况取决于窗口的定义，根据具体的 key 和时间段会产生诸多不同的窗口实例。 Per-window state 作用于后者。也就是说，如果我们处理有 1000 种不同 key 的事件， 并且目前所有事件都处于 [12:00, 13:00) 时间窗口内，那么我们将会得到 1000 个窗口实例， 且每个实例都有自己的 keyed per-window state。
process() 接收到的 Context 对象中有两个方法允许我们访问以下两种 state：
globalState()，访问全局的 keyed state windowState(), 访问作用域仅限于当前窗口的 keyed state 如果你可能将一个 window 触发多次（比如当你的迟到数据会再次触发窗口计算， 或你自定义了根据推测提前触发窗口的 trigger），那么这个功能将非常有用。 这时你可能需要在 per-window state 中储存关于之前触发的信息或触发的总次数。
当使用窗口状态时，一定记得在删除窗口时清除这些状态。他们应该定义在 clear() 方法中。
WindowFunction（已过时） # 在某些可以使用 ProcessWindowFunction 的地方，你也可以使用 WindowFunction。 它是旧版的 ProcessWindowFunction，只能提供更少的环境信息且缺少一些高级的功能，比如 per-window state。 这个接口会在未来被弃用。
WindowFunction 的签名如下：
Java public interface WindowFunction\u0026lt;IN, OUT, KEY, W extends Window\u0026gt; extends Function, Serializable { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ void apply(KEY key, W window, Iterable\u0026lt;IN\u0026gt; input, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } Scala trait WindowFunction[IN, OUT, KEY, W \u0026lt;: Window] extends Function with Serializable { /** * Evaluates the window and outputs none or several elements. * * @param key The key for which this window is evaluated. * @param window The window that is being evaluated. * @param input The elements in the window being evaluated. * @param out A collector for emitting elements. * @throws Exception The function may throw exceptions to fail the program and trigger recovery. */ def apply(key: KEY, window: W, input: Iterable[IN], out: Collector[OUT]) } Python class WindowFunction(Function, Generic[IN, OUT, KEY, W]): @abstractmethod def apply(self, key: KEY, window: W, inputs: Iterable[IN]) -\u0026gt; Iterable[OUT]: \u0026#34;\u0026#34;\u0026#34; Evaluates the window and outputs none or several elements. :param key: The key for which this window is evaluated. :param window: The window that is being evaluated. :param inputs: The elements in the window being evaluated. \u0026#34;\u0026#34;\u0026#34; pass 它可以像下例这样使用：
Java DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .apply(new MyWindowFunction()); Scala val input: DataStream[(String, Long)] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .apply(new MyWindowFunction()) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .apply(MyWindowFunction()) Triggers # Trigger 决定了一个窗口（由 window assigner 定义）何时可以被 window function 处理。 每个 WindowAssigner 都有一个默认的 Trigger。 如果默认 trigger 无法满足你的需要，你可以在 trigger(...) 调用中指定自定义的 trigger。
Trigger 接口提供了五个方法来响应不同的事件：
onElement() 方法在每个元素被加入窗口时调用。 onEventTime() 方法在注册的 event-time timer 触发时调用。 onProcessingTime() 方法在注册的 processing-time timer 触发时调用。 onMerge() 方法与有状态的 trigger 相关。该方法会在两个窗口合并时， 将窗口对应 trigger 的状态进行合并，比如使用会话窗口时。 最后，clear() 方法处理在对应窗口被移除时所需的逻辑。 有两点需要注意：
前三个方法通过返回 TriggerResult 来决定 trigger 如何应对到达窗口的事件。应对方案有以下几种： CONTINUE: 什么也不做 FIRE: 触发计算 PURGE: 清空窗口内的元素 FIRE_AND_PURGE: 触发计算，计算结束后清空窗口内的元素 上面的任意方法都可以用来注册 processing-time 或 event-time timer。 触发（Fire）与清除（Purge） # 当 trigger 认定一个窗口可以被计算时，它就会触发，也就是返回 FIRE 或 FIRE_AND_PURGE。 这是让窗口算子发送当前窗口计算结果的信号。 如果一个窗口指定了 ProcessWindowFunction，所有的元素都会传给 ProcessWindowFunction。 如果是 ReduceFunction 或 AggregateFunction，则直接发送聚合的结果。
当 trigger 触发时，它可以返回 FIRE 或 FIRE_AND_PURGE。 FIRE 会保留被触发的窗口中的内容，而 FIRE_AND_PURGE 会删除这些内容。 Flink 内置的 trigger 默认使用 FIRE，不会清除窗口的状态。
Purge 只会移除窗口的内容， 不会移除关于窗口的 meta-information 和 trigger 的状态。 WindowAssigner 默认的 Triggers # WindowAssigner 默认的 Trigger 足以应付诸多情况。 比如说，所有的 event-time window assigner 都默认使用 EventTimeTrigger。 这个 trigger 会在 watermark 越过窗口结束时间后直接触发。
GlobalWindow 的默认 trigger 是永远不会触发的 NeverTrigger。因此，使用 GlobalWindow 时，你必须自己定义一个 trigger。
当你在 trigger() 中指定了一个 trigger 时， 你实际上覆盖了当前 WindowAssigner 默认的 trigger。 比如说，如果你指定了一个 CountTrigger 给 TumblingEventTimeWindows，你的窗口将不再根据时间触发， 而是根据元素数量触发。如果你希望即响应时间，又响应数量，就需要自定义 trigger 了。 内置 Triggers 和自定义 Triggers # Flink 包含一些内置 trigger。
之前提到过的 EventTimeTrigger 根据 watermark 测量的 event time 触发。 ProcessingTimeTrigger 根据 processing time 触发。 CountTrigger 在窗口中的元素超过预设的限制时触发。 PurgingTrigger 接收另一个 trigger 并将它转换成一个会清理数据的 trigger。 如果你需要实现自定义的 trigger，你应该看看这个抽象类 Trigger 。 请注意，这个 API 仍在发展，所以在之后的 Flink 版本中可能会发生变化。
Evictors # Flink 的窗口模型允许在 WindowAssigner 和 Trigger 之外指定可选的 Evictor。 如本文开篇的代码中所示，通过 evictor(...) 方法传入 Evictor。 Evictor 可以在 trigger 触发后、调用窗口函数之前或之后从窗口中删除元素。 Evictor 接口提供了两个方法实现此功能：
/** * Optionally evicts elements. Called before windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictBefore(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); /** * Optionally evicts elements. Called after windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictAfter(Iterable\u0026lt;TimestampedValue\u0026lt;T\u0026gt;\u0026gt; elements, int size, W window, EvictorContext evictorContext); evictBefore() 包含在调用窗口函数前的逻辑，而 evictAfter() 包含在窗口函数调用之后的逻辑。 在调用窗口函数之前被移除的元素不会被窗口函数计算。
Flink 内置有三个 evictor：
CountEvictor: 仅记录用户指定数量的元素，一旦窗口中的元素超过这个数量，多余的元素会从窗口缓存的开头移除 DeltaEvictor: 接收 DeltaFunction 和 threshold 参数，计算最后一个元素与窗口缓存中所有元素的差值， 并移除差值大于或等于 threshold 的元素。 TimeEvictor: 接收 interval 参数，以毫秒表示。 它会找到窗口中元素的最大 timestamp max_ts 并移除比 max_ts - interval 小的所有元素。 默认情况下，所有内置的 evictor 逻辑都在调用窗口函数前执行。
指定一个 evictor 可以避免预聚合，因为窗口中的所有元素在计算前都必须经过 evictor。 Note: Evictor 在 Python DataStream API 中还不支持. Flink 不对窗口中元素的顺序做任何保证。也就是说，即使 evictor 从窗口缓存的开头移除一个元素，这个元素也不一定是最先或者最后到达窗口的。
Allowed Lateness # 在使用 event-time 窗口时，数据可能会迟到，即 Flink 用来追踪 event-time 进展的 watermark 已经 越过了窗口结束的 timestamp 后，数据才到达。对于 Flink 如何处理 event time， event time 和 late elements 有更详细的探讨。
默认情况下，watermark 一旦越过窗口结束的 timestamp，迟到的数据就会被直接丢弃。 但是 Flink 允许指定窗口算子最大的 allowed lateness。 Allowed lateness 定义了一个元素可以在迟到多长时间的情况下不被丢弃，这个参数默认是 0。 在 watermark 超过窗口末端、到达窗口末端加上 allowed lateness 之前的这段时间内到达的元素， 依旧会被加入窗口。取决于窗口的 trigger，一个迟到但没有被丢弃的元素可能会再次触发窗口，比如 EventTimeTrigger。
为了实现这个功能，Flink 会将窗口状态保存到 allowed lateness 超时才会将窗口及其状态删除 （如 Window Lifecycle 所述）。
默认情况下，allowed lateness 被设为 0。即 watermark 之后到达的元素会被丢弃。
你可以像下面这样指定 allowed lateness：
Java DataStream\u0026lt;T\u0026gt; input = ...; input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); Scala val input: DataStream[T] = ... input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) Python input = ... # type: DataStream input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .allowed_lateness(\u0026lt;time\u0026gt;) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) 使用 GlobalWindows 时，没有数据会被视作迟到，因为全局窗口的结束 timestamp 是 Long.MAX_VALUE。 从旁路输出（side output）获取迟到数据 # 通过 Flink 的 旁路输出 功能，你可以获得迟到数据的数据流。
首先，你需要在开窗后的 stream 上使用 sideOutputLateData(OutputTag) 表明你需要获取迟到数据。 然后，你就可以从窗口操作的结果中获取旁路输出流了。
Java final OutputTag\u0026lt;T\u0026gt; lateOutputTag = new OutputTag\u0026lt;T\u0026gt;(\u0026#34;late-data\u0026#34;){}; DataStream\u0026lt;T\u0026gt; input = ...; SingleOutputStreamOperator\u0026lt;T\u0026gt; result = input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .sideOutputLateData(lateOutputTag) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;); DataStream\u0026lt;T\u0026gt; lateStream = result.getSideOutput(lateOutputTag); Scala val lateOutputTag = OutputTag[T](\u0026#34;late-data\u0026#34;) val input: DataStream[T] = ... val result = input .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .allowedLateness(\u0026lt;time\u0026gt;) .sideOutputLateData(lateOutputTag) .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) val lateStream = result.getSideOutput(lateOutputTag) Python late_output_tag = OutputTag(\u0026#34;late-data\u0026#34;, type_info) input = ... # type: DataStream result = input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(\u0026lt;window assigner\u0026gt;) \\ .allowed_lateness(\u0026lt;time\u0026gt;) \\ .side_output_late_data(late_output_tag) \\ .\u0026lt;windowed transformation\u0026gt;(\u0026lt;window function\u0026gt;) late_stream = result.get_side_output(late_output_tag) 迟到数据的一些考虑 # 当指定了大于 0 的 allowed lateness 时，窗口本身以及其中的内容仍会在 watermark 越过窗口末端后保留。 这时，如果一个迟到但未被丢弃的数据到达，它可能会再次触发这个窗口。 这种触发被称作 late firing，与表示第一次触发窗口的 main firing 相区别。 如果是使用会话窗口的情况，late firing 可能会进一步合并已有的窗口，因为他们可能会连接现有的、未被合并的窗口。
你应该注意：late firing 发出的元素应该被视作对之前计算结果的更新，即你的数据流中会包含一个相同计算任务的多个结果。你的应用需要考虑到这些重复的结果，或去除重复的部分。 Working with window results # 窗口操作的结果会变回 DataStream，并且窗口操作的信息不会保存在输出的元素中。 所以如果你想要保留窗口的 meta-information，你需要在 ProcessWindowFunction 里手动将他们放入输出的元素中。 输出元素中保留的唯一相关的信息是元素的 timestamp。 它被设置为窗口能允许的最大 timestamp，也就是 end timestamp - 1，因为窗口末端的 timestamp 是排他的。 这个情况同时适用于 event-time 窗口和 processing-time 窗口。 也就是说，在窗口操作之后，元素总是会携带一个 event-time 或 processing-time timestamp。 对 Processing-time 窗口来说，这并不意味着什么。 而对于 event-time 窗口来说，“输出携带 timestamp” 以及 “watermark 与窗口的相互作用” 这两者使建立窗口大小相同的连续窗口操作（consecutive windowed operations） 变为可能。我们先看看 watermark 与窗口的相互作用，然后再来讨论它。
Interaction of watermarks and windows # 继续阅读这个章节之前，你可能想要先了解一下我们介绍 event time 和 watermarks 的内容。
当 watermark 到达窗口算子时，它触发了两件事：
这个 watermark 触发了所有最大 timestamp（即 end-timestamp - 1）小于它的窗口 这个 watermark 被原封不动地转发给下游的任务。 通俗来讲，watermark 将当前算子中那些“一旦这个 watermark 被下游任务接收就肯定会就超时”的窗口全部冲走。
Consecutive windowed operations # 如之前提到的，窗口结果的 timestamp 如何计算以及 watermark 如何与窗口相互作用使串联多个窗口操作成为可能。 这提供了一种便利的方法，让你能够有两个连续的窗口，他们即能使用不同的 key， 又能让上游操作中某个窗口的数据出现在下游操作的相同窗口。参考下例：
Java DataStream\u0026lt;Integer\u0026gt; input = ...; DataStream\u0026lt;Integer\u0026gt; resultsPerKey = input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new Summer()); DataStream\u0026lt;Integer\u0026gt; globalResults = resultsPerKey .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new TopKWindowFunction()); Scala val input: DataStream[Int] = ... val resultsPerKey = input .keyBy(\u0026lt;key selector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new Summer()) val globalResults = resultsPerKey .windowAll(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new TopKWindowFunction()) Python input = ... # type: DataStream results_per_key = input \\ .key_by(\u0026lt;key selector\u0026gt;) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(Summer()) global_results = results_per_key \\ .window_all(TumblingProcessingTimeWindows.of(Time.seconds(5))) \\ .process(TopKWindowFunction()) 这个例子中，第一个操作中时间窗口[0, 5) 的结果会出现在下一个窗口操作的 [0, 5) 窗口中。 这就可以让我们先在一个窗口内按 key 求和，再在下一个操作中找出这个窗口中 top-k 的元素。
关于状态大小的考量 # 窗口可以被定义在很长的时间段上（比如几天、几周或几个月）并且积累下很大的状态。 当你估算窗口计算的储存需求时，可以铭记几条规则：
Flink 会为一个元素在它所属的每一个窗口中都创建一个副本。 因此，一个元素在滚动窗口的设置中只会存在一个副本（一个元素仅属于一个窗口，除非它迟到了）。 与之相反，一个元素可能会被拷贝到多个滑动窗口中，就如我们在 Window Assigners 中描述的那样。 因此，设置一个大小为一天、滑动距离为一秒的滑动窗口可能不是个好想法。
ReduceFunction 和 AggregateFunction 可以极大地减少储存需求，因为他们会就地聚合到达的元素， 且每个窗口仅储存一个值。而使用 ProcessWindowFunction 需要累积窗口中所有的元素。
使用 Evictor 可以避免预聚合， 因为窗口中的所有数据必须先经过 evictor 才能进行计算（详见 Evictors）。
Back to top
`}),e.add({id:49,href:"/flink/flink-docs-master/zh/docs/ops/debugging/debugging_event_time/",title:"调试窗口与事件时间",section:"Debugging",content:` 调试窗口与事件时间 # 监控当前事件时间（Event Time） # Flink 的事件时间和 watermark 支持对于处理乱序事件是十分强大的特性。然而，由于是系统内部跟踪时间进度，所以很难了解究竟正在发生什么。
可以通过 Flink web 界面或指标系统访问 task 的 low watermarks。
Flink 中的 task 通过调用 currentInputWatermark 方法暴露一个指标，该指标表示当前 task 所接收到的 the lowest watermark。这个 long 类型值表示“当前事件时间”。该值通过获取上游算子收到的所有 watermarks 的最小值来计算。这意味着用 watermarks 跟踪的事件时间总是由最落后的 source 控制。
使用 web 界面可以访问 low watermark 指标，在指标选项卡中选择一个 task，然后选择 \u0026lt;taskNr\u0026gt;.currentInputWatermark 指标。在新的显示框中，你可以看到此 task 的当前 low watermark。
获取指标的另一种方式是使用指标报告器之一，如指标系统文档所述。对于本地集群设置，我们推荐使用 JMX 指标报告器和类似于 VisualVM 的工具。
处理散乱的事件时间 # 方式 1：延迟的 Watermark（表明完整性），窗口提前触发 方式 2：具有最大延迟启发式的 Watermark，窗口接受迟到的数据 Back to top
`}),e.add({id:50,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/dynamic_tables/",title:"动态表 (Dynamic Table)",section:"流式概念",content:` 动态表 (Dynamic Table) # SQL 和关系代数在设计时并未考虑流数据。因此，在关系代数(和 SQL)之间几乎没有概念上的差异。
本文会讨论这种差异，并介绍 Flink 如何在无界数据集上实现与数据库引擎在有界数据上的处理具有相同的语义。
DataStream 上的关系查询 # 下表比较了传统的关系代数和流处理与输入数据、执行和输出结果的关系。
关系代数 / SQL 流处理 关系(或表)是有界(多)元组集合。 流是一个无限元组序列。 对批数据(例如关系数据库中的表)执行的查询可以访问完整的输入数据。 流式查询在启动时不能访问所有数据，必须“等待”数据流入。 批处理查询在产生固定大小的结果后终止。 流查询不断地根据接收到的记录更新其结果，并且始终不会结束。 尽管存在这些差异，但是使用关系查询和 SQL 处理流并不是不可能的。高级关系数据库系统提供了一个称为 物化视图(Materialized Views) 的特性。物化视图被定义为一条 SQL 查询，就像常规的虚拟视图一样。与虚拟视图相反，物化视图缓存查询的结果，因此在访问视图时不需要对查询进行计算。缓存的一个常见难题是防止缓存为过期的结果提供服务。当其定义查询的基表被修改时，物化视图将过期。 即时视图维护(Eager View Maintenance) 是一种一旦更新了物化视图的基表就立即更新视图的技术。
如果我们考虑以下问题，那么即时视图维护和流上的SQL查询之间的联系就会变得显而易见:
数据库表是 INSERT、UPDATE 和 DELETE DML 语句的 stream 的结果，通常称为 changelog stream 。 物化视图被定义为一条 SQL 查询。为了更新视图，查询不断地处理视图的基本关系的changelog 流。 物化视图是流式 SQL 查询的结果。 了解了这些要点之后，我们将在下一节中介绍 动态表(Dynamic tables) 的概念。
动态表 \u0026amp; 连续查询(Continuous Query) # 动态表 是 Flink 的支持流数据的 Table API 和 SQL 的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。可以像查询静态批处理表一样查询它们。查询动态表将生成一个 连续查询 。一个连续查询永远不会终止，结果会生成一个动态表。查询不断更新其(动态)结果表，以反映其(动态)输入表上的更改。本质上，动态表上的连续查询非常类似于定义物化视图的查询。
需要注意的是，连续查询的结果在语义上总是等价于以批处理模式在输入表快照上执行的相同查询的结果。
下图显示了流、动态表和连续查询之间的关系:
将流转换为动态表。 在动态表上计算一个连续查询，生成一个新的动态表。 生成的动态表被转换回流。 注意： 动态表首先是一个逻辑概念。在查询执行期间不一定(完全)物化动态表。
在下面，我们将解释动态表和连续查询的概念，并使用具有以下模式的单击事件流:
[ user: VARCHAR, // 用户名 cTime: TIMESTAMP, // 访问 URL 的时间 url: VARCHAR // 用户访问的 URL ] 在流上定义表 # 为了使用关系查询处理流，必须将其转换成 Table。从概念上讲，流的每条记录都被解释为对结果表的 INSERT 操作。本质上我们正在从一个 INSERT-only 的 changelog 流构建表。
下图显示了单击事件流(左侧)如何转换为表(右侧)。当插入更多的单击流记录时，结果表将不断增长。
注意： 在流上定义的表在内部没有物化。
连续查询 # 在动态表上计算一个连续查询，并生成一个新的动态表。与批处理查询不同，连续查询从不终止，并根据其输入表上的更新更新其结果表。在任何时候，连续查询的结果在语义上与以批处理模式在输入表快照上执行的相同查询的结果相同。
在接下来的代码中，我们将展示 clicks 表上的两个示例查询，这个表是在点击事件流上定义的。
第一个查询是一个简单的 GROUP-BY COUNT 聚合查询。它基于 user 字段对 clicks 表进行分组，并统计访问的 URL 的数量。下面的图显示了当 clicks 表被附加的行更新时，查询是如何被评估的。
当查询开始，clicks 表(左侧)是空的。当第一行数据被插入到 clicks 表时，查询开始计算结果表。第一行数据 [Mary,./home] 插入后，结果表(右侧，上部)由一行 [Mary, 1] 组成。当第二行 [Bob, ./cart] 插入到 clicks 表时，查询会更新结果表并插入了一行新数据 [Bob, 1]。第三行 [Mary, ./prod?id=1] 将产生已计算的结果行的更新，[Mary, 1] 更新成 [Mary, 2]。最后，当第四行数据加入 clicks 表时，查询将第三行 [Liz, 1] 插入到结果表中。
第二条查询与第一条类似，但是除了用户属性之外，还将 clicks 分组至每小时滚动窗口中，然后计算 url 数量(基于时间的计算，例如基于特定时间属性的窗口，后面会讨论)。同样，该图显示了不同时间点的输入和输出，以可视化动态表的变化特性。
与前面一样，左边显示了输入表 clicks。查询每小时持续计算结果并更新结果表。clicks表包含四行带有时间戳(cTime)的数据，时间戳在 12:00:00 和 12:59:59 之间。查询从这个输入计算出两个结果行(每个 user 一个)，并将它们附加到结果表中。对于 13:00:00 和 13:59:59 之间的下一个窗口，clicks 表包含三行，这将导致另外两行被追加到结果表。随着时间的推移，更多的行被添加到 click 中，结果表将被更新。
更新和追加查询 # 虽然这两个示例查询看起来非常相似(都计算分组计数聚合)，但它们在一个重要方面不同:
第一个查询更新先前输出的结果，即定义结果表的 changelog 流包含 INSERT 和 UPDATE 操作。 第二个查询只附加到结果表，即结果表的 changelog 流只包含 INSERT 操作。 一个查询是产生一个只追加的表还是一个更新的表有一些含义:
产生更新更改的查询通常必须维护更多的状态(请参阅以下部分)。 将 append-only 的表转换为流与将已更新的表转换为流是不同的(参阅表到流的转换章节)。 查询限制 # 许多(但不是全部)语义上有效的查询可以作为流上的连续查询进行评估。有些查询代价太高而无法计算，这可能是由于它们需要维护的状态大小，也可能是由于计算更新代价太高。
状态大小： 连续查询在无界流上计算，通常应该运行数周或数月。因此，连续查询处理的数据总量可能非常大。必须更新先前输出的结果的查询需要维护所有输出的行，以便能够更新它们。例如，第一个查询示例需要存储每个用户的 URL 计数，以便能够增加该计数并在输入表接收新行时发送新结果。如果只跟踪注册用户，则要维护的计数数量可能不会太高。但是，如果未注册的用户分配了一个惟一的用户名，那么要维护的计数数量将随着时间增长，并可能最终导致查询失败。 SELECT user, COUNT(url) FROM clicks GROUP BY user; 计算更新： 有些查询需要重新计算和更新大量已输出的结果行，即使只添加或更新一条输入记录。显然，这样的查询不适合作为连续查询执行。下面的查询就是一个例子，它根据最后一次单击的时间为每个用户计算一个 RANK。一旦 click 表接收到一个新行，用户的 lastAction 就会更新，并必须计算一个新的排名。然而，由于两行不能具有相同的排名，所以所有较低排名的行也需要更新。 SELECT user, RANK() OVER (ORDER BY lastAction) FROM ( SELECT user, MAX(cTime) AS lastAction FROM clicks GROUP BY user ); 查询配置章节讨论了控制连续查询执行的参数。一些参数可以用来在维持状态的大小和获得结果的准确性之间做取舍。
表到流的转换 # 动态表可以像普通数据库表一样通过 INSERT、UPDATE 和 DELETE 来不断修改。它可能是一个只有一行、不断更新的表，也可能是一个 insert-only 的表，没有 UPDATE 和 DELETE 修改，或者介于两者之间的其他表。
在将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的 Table API 和 SQL 支持三种方式来编码一个动态表的变化:
Append-only 流： 仅通过 INSERT 操作修改的动态表可以通过输出插入的行转换为流。
Retract 流： retract 流包含两种类型的 message： add messages 和 retract messages 。通过将INSERT 操作编码为 add message、将 DELETE 操作编码为 retract message、将 UPDATE 操作编码为更新(先前)行的 retract message 和更新(新)行的 add message，将动态表转换为 retract 流。下图显示了将动态表转换为 retract 流的过程。
Upsert 流: upsert 流包含两种类型的 message： upsert messages 和delete messages。转换为 upsert 流的动态表需要(可能是组合的)唯一键。通过将 INSERT 和 UPDATE 操作编码为 upsert message，将 DELETE 操作编码为 delete message ，将具有唯一键的动态表转换为流。消费流的算子需要知道唯一键的属性，以便正确地应用 message。与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。下图显示了将动态表转换为 upsert 流的过程。 在通用概念中讨论了将动态表转换为 DataStream 的 API。请注意，在将动态表转换为 DataStream 时，只支持 append 流和 retract 流。在 TableSources 和 TableSinks 章节讨论向外部系统输出动态表的 TableSink 接口。
Back to top
`}),e.add({id:51,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/overview/",title:"概览",section:"Standalone",content:` Standalone # 本页面提供了关于如何在静态（但可能异构）集群上以完全分布式方式运行 Flink 的说明。
需求 # 软件需求 # Flink 运行在所有类 UNIX 环境下，例如 Linux，Mac OS X 和 Cygwin （Windows），集群由一个 master 节点以及一个或多个 worker 节点构成。在配置系统之前，请确保在每个节点上安装有以下软件：
Java 1.8.x 或更高版本， ssh （必须运行 sshd 以执行用于管理 Flink 各组件的脚本） 如果集群不满足软件要求，那么你需要安装/更新这些软件。
使集群中所有节点使用免密码 SSH 以及拥有相同的目录结构可以让你使用脚本来控制一切。
Back to top
JAVA_HOME 配置 # Flink 需要 master 和所有 worker 节点设置 JAVA_HOME 环境变量，并指向你的 Java 安装目录。
你可以在 conf/flink-conf.yaml 文件中通过 env.java.home 配置项来设置此变量。
Back to top
Flink 设置 # 前往 下载页面 获取可运行的软件包。
在下载完最新的发布版本后，复制压缩文件到 master 节点并解压：
tar xzf flink-*.tgz cd flink-* 配置 Flink # 在解压完文件后，你需要编辑 conf/flink-conf.yaml 文件来为集群配置 Flink。
设置 jobmanager.rpc.address 配置项指向 master 节点。你也应该通过设置 jobmanager.memory.process.size 和 taskmanager.memory.process.size 配置项来定义 Flink 允许在每个节点上分配的最大内存值。
这些值的单位是 MB。如果一些 worker 节点上有你想分配到 Flink 系统的多余内存，你可以在这些特定节点的 conf/flink-conf.yaml 文件中重写 taskmanager.memory.process.size 或 taskmanager.memory.flink.size 的默认值。
最后，你必须提供集群上会被用作为 worker 节点的所有节点列表，也就是运行 TaskManager 的节点。编辑文件 conf/workers 并输入每个 worker 节点的 IP 或主机名。
以下例子展示了三个节点（IP 地址从 10.0.0.1 到 10.0.0.3，主机名为 master、worker1、 woker2）的设置，以及配置文件（在所有机器上都需要在相同路径访问）的内容：
/path/to/flink/conf/
flink-conf.yaml jobmanager.rpc.address: 10.0.0.1 /path/to/flink/
conf/workers 10.0.0.2 10.0.0.3 Flink 目录必须放在所有 worker 节点的相同目录下。你可以使用共享的 NFS 目录，或将 Flink 目录复制到每个 worker 节点上。
请参考 配置参数页面 获取更多细节以及额外的配置项。
特别地，
每个 JobManager 的可用内存值（jobmanager.memory.process.size）， 每个 TaskManager 的可用内存值 （taskmanager.memory.process.size，并检查 内存调优指南）， 每台机器的可用 CPU 数（taskmanager.numberOfTaskSlots）， 集群中所有 CPU 数（parallelism.default）和 临时目录（io.tmp.dirs） 的值都是非常重要的配置项。
Back to top
启动 Flink # 下面的脚本在本地节点启动了一个 JobManager 并通过 SSH 连接到 workers 文件中所有的 worker 节点，在每个节点上启动 TaskManager。现在你的 Flink 系统已经启动并运行着。可以通过配置的 RPC 端口向本地节点上的 JobManager 提交作业。
假定你在 master 节点并且在 Flink 目录下：
bin/start-cluster.sh 为了关闭 Flink，这里同样有一个 stop-cluster.sh 脚本。
Back to top
为集群添加 JobManager/TaskManager 实例 # 你可以使用 bin/jobmanager.sh 和 bin/taskmanager.sh 脚本为正在运行的集群添加 JobManager 和 TaskManager 实例。
添加 JobManager # bin/jobmanager.sh ((start|start-foreground) [args] [webui-port])|stop|stop-all 添加 TaskManager # bin/taskmanager.sh start|start-foreground|stop|stop-all 确保在你想启动/关闭相应实例的主机上执行这些脚本。
High-Availability with Standalone # In order to enable HA for a standalone cluster, you have to use the ZooKeeper HA services.
Additionally, you have to configure your cluster to start multiple JobManagers.
Masters File (masters) # In order to start an HA-cluster configure the masters file in conf/masters:
masters file: The masters file contains all hosts, on which JobManagers are started, and the ports to which the web user interface binds.
jobManagerAddress1:webUIPort1 [...] jobManagerAddressX:webUIPortX By default, the job manager will pick a random port for inter process communication. You can change this via the high-availability.jobmanager.port key. This key accepts single ports (e.g. 50010), ranges (50000-50025), or a combination of both (50010,50011,50020-50025,50050-50075).
Example: Standalone Cluster with 2 JobManagers # Configure high availability mode and ZooKeeper quorum in conf/flink-conf.yaml:
high-availability: zookeeper high-availability.zookeeper.quorum: localhost:2181 high-availability.zookeeper.path.root: /flink high-availability.cluster-id: /cluster_one # important: customize per cluster high-availability.storageDir: hdfs:///flink/recovery Configure masters in conf/masters:
localhost:8081 localhost:8082 Configure ZooKeeper server in conf/zoo.cfg (currently it\u0026rsquo;s only possible to run a single ZooKeeper server per machine):
server.0=localhost:2888:3888 Start ZooKeeper quorum:
\$ bin/start-zookeeper-quorum.sh Starting zookeeper daemon on host localhost. Start an HA-cluster:
\$ bin/start-cluster.sh Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum. Starting standalonesession daemon on host localhost. Starting standalonesession daemon on host localhost. Starting taskexecutor daemon on host localhost. Stop ZooKeeper quorum and cluster:
\$ bin/stop-cluster.sh Stopping taskexecutor daemon (pid: 7647) on localhost. Stopping standalonesession daemon (pid: 7495) on host localhost. Stopping standalonesession daemon (pid: 7349) on host localhost. \$ bin/stop-zookeeper-quorum.sh Stopping zookeeper daemon (pid: 7101) on host localhost. User jars \u0026amp; Classpath # In Standalone mode, the following jars will be recognized as user-jars and included into user classpath:
Session Mode: The JAR file specified in startup command. Application Mode: The JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:52,href:"/flink/flink-docs-master/zh/docs/ops/monitoring/checkpoint_monitoring/",title:"监控 Checkpoint",section:"Monitoring",content:` 监控 Checkpoint # 概览（Overview） # Flink 的 Web 界面提供了选项卡/标签（tab）来监视作业的 checkpoint 信息。作业终止后，这些统计信息仍然可用。有四个不同的选项卡可显示有关 checkpoint 的信息：概览（Overview），历史记录（History），摘要信息（Summary）和配置信息（Configuration）。以下各节将依次介绍这些内容。
监控（Monitoring） # 概览（Overview）选项卡 # 概览选项卡列出了以下统计信息。请注意，这些统计信息在 JobManager 丢失时无法保存，如果 JobManager 发生故障转移，这些统计信息将重置。
Checkpoint Counts Triggered：自作业开始以来触发的 checkpoint 总数。 In Progress：当前正在进行的 checkpoint 数量。 Completed：自作业开始以来成功完成的 checkpoint 总数。 Failed：自作业开始以来失败的 checkpoint 总数。 Restored：自作业开始以来进行的恢复操作的次数。这还表示自 提交以来已重新启动多少次。请注意，带有 savepoint 的初始提交也算作一次恢复，如果 JobManager 在此操作过程中丢失，则该统计将重新计数。 Latest Completed Checkpoint：最新（最近）成功完成的 checkpoint。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Failed Checkpoint：最新失败的 checkpoint。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Savepoint：最新触发的 savepoint 及其外部路径。点击 More details 可以得到 subtask 级别的详细统计信息。 Latest Restore：有两种类型的恢复操作。 Restore from Checkpoint：从 checkpoint 恢复。 Restore from Savepoint：从 savepoint 恢复。 历史记录（History）选项卡 # Checkpoint 历史记录保存有关最近触发的 checkpoint 的统计信息，包括当前正在进行的 checkpoint。
注意，对于失败的 checkpoint，指标会尽最大努力进行更新，但是可能不准确。
ID：已触发 checkpoint 的 ID。每个 checkpoint 的 ID 都会递增，从 1 开始。 Status：Checkpoint 的当前状态，可以是正在进行（In Progress）、已完成（Completed） 或失败（Failed））。如果触发的检查点是一个保存点，你将看到一个 符号。 Acknowledged：已确认完成的子任务数量与总任务数量。 Trigger Time：在 JobManager 上发起 checkpoint 的时间。 Latest Acknowledgement：JobManager 接收到任何 subtask 的最新确认的时间（如果尚未收到确认，则不适用）。 End to End Duration：从触发时间戳到最后一次确认的持续时间（如果还没有收到确认，则不适用）。完整 checkpoint 的端到端持续时间由确认 checkpoint 的最后一个 subtask 确定。这个时间通常大于单个 subtask 实际 checkpoint state 所需的时间。 Checkpointed Data Size: 在此次checkpoint的sync以及async阶段中持久化的数据量。如果启用了增量 checkpoint或者changelog，则此值可能会与全量checkpoint数据量产生区别。 Full Checkpoint Data Size: 所有已确认的 subtask 的 checkpoint 的全量数据大小。 Processed (persisted) in-flight data：在 checkpoint 对齐期间（从接收第一个和最后一个 checkpoint barrier 之间的时间）所有已确认的 subtask 处理/持久化 的大约字节数。如果启用了 unaligned checkpoint，持久化的字节数可能会大于0。 对于 subtask，有两个更详细的统计信息可用。
Sync Duration：Checkpoint 同步部分的持续时间。这包括 operator 的快照状态，并阻塞 subtask 上的所有其他活动（处理记录、触发计时器等）。 Async Duration：Checkpoint 的异步部分的持续时间。这包括将 checkpoint 写入设置的文件系统所需的时间。对于 unaligned checkpoint，这还包括 subtask 必须等待最后一个 checkpoint barrier 到达的时间（checkpoint alignment 持续时间）以及持久化数据所需的时间。 Alignment Duration：处理第一个和最后一个 checkpoint barrier 之间的时间。对于 checkpoint alignment 机制的 checkpoint，在 checkpoint alignment 过程中，已经接收到 checkpoint barrier 的 channel 将阻塞并停止处理后续的数据。 Start Delay：从 checkpoint barrier 创建开始到 subtask 收到第一个 checkpoint barrier 所用的时间。 Unaligned Checkpoint：Checkpoint 完成的时候是否是一个 unaligned checkpoint。在 alignment 超时的时候 aligned checkpoint 可以自动切换成 unaligned checkpoint。 历史记录数量配置 # 你可以通过以下配置键配置历史记录所保存的最近检查点的数量。默认值为 10。
# 保存最近 checkpoint 的个数 web.checkpoints.history: 15 摘要信息（Summary）选项卡 # 摘要计算了所有已完成 checkpoint 的端到端持续时间、增量/全量Checkpoint 数据大小和 checkpoint alignment 期间缓冲的字节数的简单 min/average/maximum 统计信息（有关这些内容的详细信息，请参见 History）。
请注意，这些统计信息不会在 JobManager 丢失后无法保存，如果 JobManager 故障转移，这些统计信息将重新计数。
配置信息（Configuration）选项卡 # 该配置选项卡列出了你指定的配置（streaming configuration）：
Checkpointing Mode：恰好一次（Exactly Once）或者至少一次（At least Once）。 Interval：配置的 checkpoint 触发间隔。在此间隔内触发 checkpoint。 Timeout：超时之后，JobManager 取消 checkpoint 并触发新的 checkpoint。 Minimum Pause Between Checkpoints：Checkpoint 之间所需的最小暂停时间。Checkpoint 成功完成后，我们至少要等这段时间再触发下一个，这可能会延迟正常的间隔。 Maximum Concurrent Checkpoints：可以同时进行的最大 checkpoint 个数。 Persist Checkpoints Externally：启用或禁用持久化 checkpoint 到外部系统。如果启用，还会列出外部化 checkpoint 的清理配置（取消时删除或保留）。 Checkpoint 详细信息 # 当你点击某个 checkpoint 的 More details 链接时，你将获得其所有 operator 的 Minimum/Average/Maximum 摘要信息，以及每个 subtask 单独的详细量化信息。
每个 Operator 的摘要信息 # 所有 Subtask 的统计信息 # Back to top
`}),e.add({id:53,href:"/flink/flink-docs-master/zh/docs/deployment/advanced/external_resources/",title:"扩展资源",section:"Advanced",content:` 扩展资源框架 # 许多计算任务需要使用除了 CPU 与内存外的资源，如用深度学习场景需要使用 GPU 来进行加速。为了支持这种扩展资源，Flink 提供了一个扩展资源框架。 该框架支持从底层资源管理系统（如 Kubernetes）请求各种类型的资源，并向算子提供使用这些资源所需的信息。该框架以插件形式支持不同的资源类型。 目前 Flink 仅内置了支持 GPU 资源的插件，你可以为你想使用的资源类型实现第三方插件。
扩展资源框架做了什么 # 扩展资源（External Resource）框架主要做了以下两件事：
根据你的配置，在 Flink 从底层资源管理系统中申请资源时，设置与扩展资源相关的请求字段
为算子提供使用这些资源所需要的信息
当 Flink 部署在资源管理系统（Kubernetes、Yarn）上时，扩展资源框架将确保分配的 Pod、Container 包含所需的扩展资源。目前，许多资源管理系统都支持扩展资源。 例如，Kubernetes 从 v1.10 开始通过 Device Plugin 机制支持 GPU、FPGA 等资源调度，Yarn 从 2.10 和 3.1 开始支持 GPU 和 FPGA 的调度。 在 Standalone 模式下，由用户负责确保扩展资源的可用性。
扩展资源框架向算子提供扩展资源相关信息，这些信息由你配置的扩展资源 Driver 生成，包含了使用扩展资源所需要的基本属性。
启用扩展资源框架 # 为了启用扩展资源框架来使用扩展资源，你需要：
为该扩展资源准备扩展资源框架的插件
为该扩展资源设置相关的配置
在你的算子中，从 RuntimeContext 来获取扩展资源的信息并使用这些资源
准备插件 # 你需要为使用的扩展资源准备插件，并将其放入 Flink 发行版的 plugins/ 文件夹中, 参看 Flink Plugins。 Flink 提供了第一方的 GPU 资源插件。你同样可以为你所使用的扩展资源实现自定义插件实现自定义插件。
配置项 # 首先，你需要使用分隔符“;”将所有使用的扩展资源类型的资源名称添加到扩展资源列表（配置键“external-resources”）中，例如，“external-resources: gpu;fpga”定义了两个扩展资源“gpu”和“fpga”。 只有此处定义了扩展资源名称（\u0026lt;resource_name\u0026gt;），相应的资源才会在扩展资源框架中生效。
对于每个扩展资源，有以下配置选项。下面的所有配置选项中的 \u0026lt;resource_name\u0026gt; 对应于扩展资源列表中列出的名称：
数量 （external.\u0026lt;resource_name\u0026gt;.amount）：需要从外部系统请求的扩展资源的数量。
Yarn 中的扩展资源配置键 （external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key）：可选配置。如果配置该项，扩展资源框架将把这个键添加到 Yarn 的容器请求的资源配置中，该键对应的值将被设置为external-resource.\u0026lt;resource_name\u0026gt;.amount。
Kubernetes 中的扩展资源配置键 （external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key）：可选配置。 如果配置该项，扩展资源框架将添加 resources.limits.\u0026lt;config-key\u0026gt; 和 resources.requests.\u0026lt;config-key\u0026gt; 到 TaskManager 的主容器配置中，对应的值将被设置为 external-resource.\u0026lt;resource_name\u0026gt;.amount。
Driver 工厂类 （external-resource.\u0026lt;resource_name\u0026gt;.driver-factory.class）：可选配置。定义由 \u0026lt;resource_name\u0026gt; 标识的扩展资源对应的工厂类名。如果配置该项，该工厂类将被用于实例化扩展资源框架中所需要的 drivers。 如果没有配置，扩展资源依然会在其他配置正确时在存在于 TaskManager，只是算子在这种情况下无法从 RuntimeContext 中拿到该资源的信息。
Driver 自定义参数 （external-resource.\u0026lt;resource_name\u0026gt;.param.\u0026lt;param\u0026gt;）：可选配置。由 \u0026lt;resource_name\u0026gt; 标识的扩展资源的自定义配置选项的命名模式。只有遵循此模式的配置才会传递到该扩展资源的工厂类。
示例配置，该配置定义两个扩展资源：
external-resources: gpu;fpga # 定义两个扩展资源，“gpu”和“fpga”。 external-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory # 定义 GPU 资源对应 Driver 的工厂类。 external-resource.gpu.amount: 2 # 定义每个 TaskManager 所需的 GPU 数量。 external-resource.gpu.param.discovery-script.args: --enable-coordination # 自定义参数 discovery-script.args，它将被传递到 GPU 对应的 Driver 中。 external-resource.fpga.driver-factory.class: org.apache.flink.externalresource.fpga.FPGADriverFactory # 定义 FPGA 资源对应 Driver 的工厂类。 external-resource.fpga.amount: 1 # 定义每个 TaskManager 所需的 FPGA 数量。 external-resource.fpga.yarn.config-key: yarn.io/fpga # 定义 FPGA 在 Yarn 中对应的配置键。 使用扩展资源 # 为了使用扩展资源，算子需要从 RuntimeContext 获取 ExternalResourceInfo 集合。 ExternalResourceInfo 包含了使用扩展资源所需的信息，可以使用 getProperty 检索这些信息。 其中具体包含哪些属性以及如何使用这些属性取决于特定的扩展资源插件。
算子可以通过 getExternalResourceInfos(String resourceName) 从 RuntimeContext 或 FunctionContext 中获取特定扩展资源的 ExternalResourceInfo。 此处的 resourceName 应与在扩展资源列表中定义的名称相同。具体用法如下：
Java public class ExternalResourceMapFunction extends RichMapFunction\u0026lt;String, String\u0026gt; { private static final String RESOURCE_NAME = \u0026#34;foo\u0026#34;; @Override public String map(String value) { Set\u0026lt;ExternalResourceInfo\u0026gt; externalResourceInfos = getRuntimeContext().getExternalResourceInfos(RESOURCE_NAME); List\u0026lt;String\u0026gt; addresses = new ArrayList\u0026lt;\u0026gt;(); externalResourceInfos.iterator().forEachRemaining(externalResourceInfo -\u0026gt; addresses.add(externalResourceInfo.getProperty(\u0026#34;address\u0026#34;).get())); // map function with addresses. // ... } } Scala class ExternalResourceMapFunction extends RichMapFunction[(String, String)] { var RESOURCE_NAME = \u0026#34;foo\u0026#34; override def map(value: String): String = { val externalResourceInfos = getRuntimeContext().getExternalResourceInfos(RESOURCE_NAME) val addresses = new util.ArrayList[String] externalResourceInfos.asScala.foreach( externalResourceInfo =\u0026gt; addresses.add(externalResourceInfo.getProperty(\u0026#34;address\u0026#34;).get())) // map function with addresses. // ... } } ExternalResourceInfo 中包含一个或多个键-值对，其键值表示资源的不同维度。你可以通过 ExternalResourceInfo#getKeys 获取所有的键。
提示： 目前，RuntimeContext#getExternalResourceInfos 返回的信息对所有算子都是可用的。 为你所使用的扩展资源实现自定义插件 # 要为你所使用的扩展资源实现自定义插件，你需要：
添加你自定义的扩展资源 Driver ，该 Driver 需要实现 org.apache.flink.api.common.externalresource.ExternalResourceDriver 接口。
添加用来实例化 Driver 的工厂类，该工厂类需要实现 org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory 接口。
添加服务入口。创建 META-INF/services/org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory 文件，其中包含了 Driver 对应工厂类的类名（更多细节请参看 Java Service Loader）。
例如，要为名为“FPGA”的扩展资源实现插件，你首先需要实现 FPGADriver 和 FPGADriverFactory：
Java public class FPGADriver implements ExternalResourceDriver { @Override public Set\u0026lt;FPGAInfo\u0026gt; retrieveResourceInfo(long amount) { // return the information set of \u0026#34;FPGA\u0026#34; } } public class FPGADriverFactory implements ExternalResourceDriverFactory { @Override public ExternalResourceDriver createExternalResourceDriver(Configuration config) { return new FPGADriver(); } } // Also implement FPGAInfo which contains basic properties of \u0026#34;FPGA\u0026#34; resource. public class FPGAInfo implements ExternalResourceInfo { @Override public Optional\u0026lt;String\u0026gt; getProperty(String key) { // return the property with the given key. } @Override public Collection\u0026lt;String\u0026gt; getKeys() { // return all property keys. } } Scala class FPGADriver extends ExternalResourceDriver { override def retrieveResourceInfo(amount: Long): Set[FPGAInfo] = { // return the information set of \u0026#34;FPGA\u0026#34; } } class FPGADriverFactory extends ExternalResourceDriverFactory { override def createExternalResourceDriver(config: Configuration): ExternalResourceDriver = { new FPGADriver() } } // Also implement FPGAInfo which contains basic properties of \u0026#34;FPGA\u0026#34; resource. class FPGAInfo extends ExternalResourceInfo { override def getProperty(key: String): Option[String] = { // return the property with the given key. } override def getKeys(): util.Collection[String] = { // return all property keys. } } 在 META-INF/services/ 中创建名为 org.apache.flink.api.common.externalresource.ExternalResourceDriverFactory 的文件，向其中写入工厂类名，如 your.domain.FPGADriverFactory。
之后，将 FPGADriver，FPGADriverFactory，META-INF/services/ 和所有外部依赖打入 jar 包。在你的 Flink 发行版的 plugins/ 文件夹中创建一个名为“fpga”的文件夹，将打好的 jar 包放入其中。 更多细节请查看 Flink Plugin。
提示： 扩展资源由运行在同一台机器上的所有算子共享。社区可能会在未来的版本中支持外部资源隔离。 已支持的扩展资源插件 # 目前，Flink提供 GPU 资源插件。
GPU 插件 # 我们为 GPU 提供了第一方插件。该插件利用一个脚本来发现 GPU 设备的索引，该索引可通过“index”从 ExternalResourceInfo 中获取。我们提供了一个默认脚本，可以用来发现 NVIDIA GPU。您还可以提供自定义脚本。
我们提供了 一个示例程序 ，展示了如何在 Flink 中使用 GPU 资源来做矩阵-向量乘法。
提示： 目前，对于所有算子，RuntimeContext#getExternalResourceInfos 会返回同样的资源信息。也即，在同一个 TaskManager 中运行的所有算子都可以访问同一组 GPU 设备。扩展资源目前没有算子级别的隔离。 前置准备 # 要使 GPU 资源可访问，根据您的环境，需要满足以下先决条件：
对于 Standalone 模式，集群管理员应确保已安装 NVIDIA 驱动程序，并且集群中所有节点上的 GPU 资源都是可访问的。
对于 Yarn 上部署，管理员需要配置 Yarn 集群使其支持 GPU 调度。 请注意，所需的 Hadoop 版本是 2.10+ 和 3.1+。
对于 Kubernetes 上部署，管理员需要保证 NVIDIA GPU 的 Device Plugin 已在集群上安装。 请注意，所需的 Kubernetes 版本是 1.10+。目前，Kubernetes只支持 NVIDIA GPU 和 AMD GPU。Flink 只提供了 NVIDIA GPU 的脚本，但你可以提供支持 AMD GPU 的自定义脚本，参看 发现脚本。
在计算任务中使用 GPU 资源 # 如启用扩展资源框架中所述，要使用 GPU 资源，还需要执行两项操作：
为 GPU 资源进行相关配置。
在算子中获取 GPU 资源的信息，其中包含键为“index”的 GPU 索引。
配置项 # 对于 GPU 插件，你需要指定的扩展资源框架配置：
external-resources：你需要将 GPU 的扩展资源名称（例如“gpu”）加到该列表中。
external-resource.\u0026lt;resource_name\u0026gt;.amount：每个 TaskManager 中的 GPU 数量。
external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key：对于 Yarn，GPU 的配置键是 yarn.io/gpu。请注意，Yarn 目前只支持 NVIDIA GPU。
external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key：对于 Kubernetes，GPU 的配置键是 \u0026lt;vendor\u0026gt;.com/gpu。 目前，“nvidia”和“amd”是两个支持的 GPU 品牌。请注意，如果你使用 AMD GPU，你需要提供一个自定义的发现脚本。
external-resource.\u0026lt;resource_name\u0026gt;.driver-factory.class：需要设置为 org.apache.flink.externalresource.gpu.GPUDriverFactory。
此外，GPU 插件还有一些专有配置：
external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.path：发现脚本的文件路径。 它既可以是绝对路径，也可以是相对路径，如果定义了“FLINK_HOME”，该路径将相对于“FLINK_HOME”，否则相对于当前目录。如果没有显式配置该项，GPU 插件将使用默认脚本。
external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.args：传递给发现脚本的参数。对于默认的发现脚本，请参见默认脚本以获取可用参数。
GPU 插件示例配置：
external-resources: gpu external-resource.gpu.driver-factory.class: org.apache.flink.externalresource.gpu.GPUDriverFactory # 定义 GPU 资源的工厂类。 external-resource.gpu.amount: 2 # 定义每个 TaskManager 的 GPU 数量。 external-resource.gpu.param.discovery-script.path: plugins/external-resource-gpu/nvidia-gpu-discovery.sh external-resource.gpu.param.discovery-script.args: --enable-coordination # 自定义参数，将被传递到 GPU 的 Driver 中。 external-resource.gpu.yarn.config-key: yarn.io/gpu # for Yarn external-resource.gpu.kubernetes.config-key: nvidia.com/gpu # for Kubernetes 发现脚本 # GPUDriver 利用发现脚本来发现 GPU 资源并生成 GPU 资源信息。
默认脚本 # 我们为 NVIDIA GPU 提供了一个默认脚本，位于 Flink 发行版的 plugins/external-resource-gpu/nvidia-gpu-discovery.sh。 该脚本通过 nvidia-smi 工具获取当前可见 GPU 的索引。它尝试返回一个 GPU 索引列表，其大小由 external-resource.\u0026lt;resource_name\u0026gt;.amount 指定，如果 GPU 数量不足，则以非零退出。
在 Standalone 模式中，多个 TaskManager 可能位于同一台机器上，并且每个 GPU 设备对所有 TaskManager 都是可见的。 默认脚本提供 GPU 协调模式，在这种模式下，脚本利用文件来同步 GPU 的分配情况，并确保每个GPU设备只能由一个TaskManager进程使用。相关参数为：
--enable-coordination-mode：启用 GPU 协调模式。默认情况下不启用。
--coordination-file filePath：用于同步 GPU 资源分配状态的文件路径。默认路径为 /var/tmp/flink-gpu-coordination。
提示： 协调模式只确保一个 GPU 设备不会被同一个 Flink 集群的多个 TaskManager 共享。不同 Flink 集群间（具有不同的协调文件）或非 Flink 应用程序仍然可以使用相同的 GPU 设备。 自定义脚本 # 你可以提供一个自定义的发现脚本来满足你的特殊需求，例如使用 AMD GPU。请确保自定义脚本的的路径正确配置（external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.path）并且 Flink 可以访问。自定义的发现脚本需要：
GPUDriver 将 GPU 数量（由 external-resource.\u0026lt;resource_name\u0026gt;.amount 定义）作为第一个参数传递到脚本中。 external-resource.\u0026lt;resource_name\u0026gt;.param.discovery-script.args 中自定义的参数会被附加在后面。
脚本需返回可用 GPU 索引的列表，用逗号分隔。空白的索引将被忽略。
脚本可以通过以非零退出来表示其未正确执行。在这种情况下，算子将不会得到 GPU 资源相关信息。
`}),e.add({id:54,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup/",title:"配置 Flink 进程的内存",section:"内存配置",content:` 配置 Flink 进程的内存 # Apache Flink 基于 JVM 的高效处理能力，依赖于其对各组件内存用量的细致掌控。 考虑到用户在 Flink 上运行的应用的多样性，尽管社区已经努力为所有配置项提供合理的默认值，仍无法满足所有情况下的需求。 为了给用户生产提供最大化的价值， Flink 允许用户在整体上以及细粒度上对集群的内存分配进行调整。为了优化内存需求，参考网络内存调优指南。
本文接下来介绍的内存配置方法适用于 1.10 及以上版本的 TaskManager 进程和 1.11 及以上版本的 JobManager 进程。 Flink 在 1.10 和 1.11 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
配置总内存 # Flink JVM 进程的*进程总内存（Total Process Memory）*包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 Flink 总内存（Total Flink Memory）包括 JVM 堆内存（Heap Memory）和堆外内存（Off-Heap Memory）。 其中堆外内存包括直接内存（Direct Memory）和本地内存（Native Memory）。
配置 Flink 进程内存最简单的方法是指定以下两个配置项中的任意一个：
配置项 TaskManager 配置参数 JobManager 配置参数 Flink 总内存 taskmanager.memory.flink.size jobmanager.memory.flink.size 进程总内存 taskmanager.memory.process.size jobmanager.memory.process.size 提示 关于本地执行，请分别参考 TaskManager 和 JobManager 的相关文档。
Flink 会根据默认值或其他配置参数自动调整剩余内存部分的大小。 关于各内存部分的更多细节，请分别参考 TaskManager 和 JobManager 的相关文档。
对于独立部署模式（Standalone Deployment），如果你希望指定由 Flink 应用本身使用的内存大小，最好选择配置 Flink 总内存。 Flink 总内存会进一步划分为 JVM 堆内存和堆外内存。 更多详情请参考如何为独立部署模式配置内存。
通过配置进程总内存可以指定由 Flink JVM 进程使用的总内存大小。 对于容器化部署模式（Containerized Deployment），这相当于申请的容器（Container）大小，详情请参考如何配置容器内存（Kubernetes 或 Yarn）。
此外，还可以通过设置 Flink 总内存的特定内部组成部分的方式来进行内存配置。 不同进程需要设置的内存组成部分是不一样的。 详情请分别参考 TaskManager 和 JobManager 的相关文档。
提示 以上三种方式中，用户需要至少选择其中一种进行配置（本地运行除外），否则 Flink 将无法启动。 这意味着，用户需要从以下无默认值的配置参数（或参数组合）中选择一个给出明确的配置：
TaskManager: JobManager: taskmanager.memory.flink.size jobmanager.memory.flink.size taskmanager.memory.process.size jobmanager.memory.process.size taskmanager.memory.task.heap.size 和 taskmanager.memory.managed.size jobmanager.memory.heap.size 提示 不建议同时设置进程总内存和 Flink 总内存。 这可能会造成内存配置冲突，从而导致部署失败。 额外配置其他内存部分时，同样需要注意可能产生的配置冲突。
JVM 参数 # Flink 进程启动时，会根据配置的和自动推导出的各内存部分大小，显式地设置以下 JVM 参数：
JVM 参数 TaskManager 取值 JobManager 取值 -Xmx 和 -Xms 框架堆内存 + 任务堆内存 JVM 堆内存 (*) -XX:MaxDirectMemorySize（TaskManager 始终设置，JobManager 见注释） 框架堆外内存 + 任务堆外内存(**) + 网络内存 堆外内存 (**) (***) -XX:MaxMetaspaceSize JVM Metaspace JVM Metaspace (*) 请记住，根据所使用的 GC 算法，你可能无法使用到全部堆内存。一些 GC 算法会为它们自身分配一定量的堆内存。这会导致堆的指标返回一个不同的最大值。 (**) 请注意，堆外内存也包括了用户代码使用的本地内存（非直接内存）。 (***) 只有在 jobmanager.memory.enable-jvm-direct-memory-limit 设置为 true 时，JobManager 才会设置 JVM 直接内存限制。 相关内存部分的配置方法，请同时参考 TaskManager 和 JobManager 的详细内存模型。
受限的等比内存部分 # 本节介绍下列内存部分的配置方法，它们都可以通过指定在总内存中所占比例的方式进行配置，同时受限于相应的的最大/最小值范围。
JVM 开销：可以配置占用进程总内存的固定比例 网络内存：可以配置占用 Flink 总内存的固定比例（仅针对 TaskManager） 相关内存部分的配置方法，请同时参考 TaskManager 和 JobManager 的详细内存模型。
这些内存部分的大小必须在相应的最大值、最小值范围内，否则 Flink 将无法启动。 最大值、最小值具有默认值，也可以通过相应的配置参数进行设置。 例如，如果仅配置下列参数：
进程总内存 = 1000MB JVM 开销最小值 = 64MB JVM 开销最大值 = 128MB JVM 开销占比 = 0.1 那么 JVM 开销的实际大小将会是 1000MB x 0.1 = 100MB，在 64-128MB 的范围内。
如果将最大值、最小值设置成相同大小，那相当于明确指定了该内存部分的大小。
如果没有明确指定内存部分的大小，Flink 会根据总内存和占比计算出该内存部分的大小。 计算得到的内存大小将受限于相应的最大值、最小值范围。 例如，如果仅配置下列参数：
进程总内存 = 1000MB JVM 开销最小值 = 128MB JVM 开销最大值 = 256MB JVM 开销占比 = 0.1 那么 JVM 开销的实际大小将会是 128MB，因为根据总内存和占比计算得到的内存大小 100MB 小于最小值。
如果配置了总内存和其他内存部分的大小，那么 Flink 也有可能会忽略给定的占比。 这种情况下，受限的等比内存部分的实际大小是总内存减去其他所有内存部分后剩余的部分。 这样推导得出的内存大小必须符合最大值、最小值范围，否则 Flink 将无法启动。 例如，如果仅配置下列参数：
进程总内存 = 1000MB 任务堆内存 = 100MB（或 JobManager 的 JVM 堆内存） JVM 开销最小值 = 64MB JVM 开销最大值 = 256MB JVM 开销占比 = 0.1 进程总内存中所有其他内存部分均有默认大小，包括 TaskManager 的托管内存默认占比或 JobManager 的默认堆外内存。 因此，JVM 开销的实际大小不是根据占比算出的大小（1000MB x 0.1 = 100MB），而是进程总内存中剩余的部分。 这个剩余部分的大小必须在 64-256MB 的范围内，否则将会启动失败。
`}),e.add({id:55,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/guarantees/",title:"容错保证",section:"DataStream Connectors",content:` Data Source 和 Sink 的容错保证 # 当程序出现错误的时候，Flink 的容错机制能恢复并继续运行程序。这种错误包括机器硬件故障、网络故障、瞬态程序故障等等。
只有当 source 参与了快照机制的时候，Flink 才能保证对自定义状态的精确一次更新。下表列举了 Flink 与其自带连接器的状态更新的保证。
请阅读各个连接器的文档来了解容错保证的细节。
Source Guarantees Notes Apache Kafka 精确一次 根据你的版本用恰当的 Kafka 连接器 AWS Kinesis Streams 精确一次 RabbitMQ 至多一次 (v 0.10) / 精确一次 (v 1.0) Google PubSub 至少一次 Collections 精确一次 Files 精确一次 Sockets 至多一次 为了保证端到端精确一次的数据交付（在精确一次的状态语义上更进一步），sink需要参与 checkpointing 机制。下表列举了 Flink 与其自带 sink 的交付保证（假设精确一次状态更新）。
Sink Guarantees Notes Elasticsearch 至少一次 Kafka producer 至少一次 / 精确一次 当使用事务生产者时，保证精确一次 (v 0.11+) Cassandra sink 至少一次 / 精确一次 只有当更新是幂等时，保证精确一次 AWS Kinesis Streams 至少一次 File sinks 精确一次 Socket sinks 至少一次 Standard output 至少一次 Redis sink 至少一次 Back to top
`}),e.add({id:56,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/gettingstarted/",title:"入门",section:"SQL",content:` 入门 # Flink SQL 使得使用标准 SQL 开发流应用程序变的简单。如果你曾经在工作中使用过兼容 ANSI-SQL 2011 的数据库或类似的 SQL 系统，那么就很容易学习 Flink。本教程将帮助你在 Flink SQL 开发环境下快速入门。
先决条件 # 你只需要具备 SQL 的基础知识即可，不需要其他编程经验。
安装 # 安装 Flink 有多种方式。对于实验而言，最常见的选择是下载二进制包并在本地运行。你可以按照本地模式安装中的步骤为本教程的剩余部分设置环境。
完成所有设置后，在安装文件夹中使用以下命令启动本地集群：
./bin/start-cluster.sh 启动完成后，就可以在本地访问 Flink WebUI localhost:8081，通过它，你可以监控不同的作业。
SQL 客户端 # SQL 客户端是一个交互式的客户端，用于向 Flink 提交 SQL 查询并将结果可视化。 在安装文件夹中运行 sql-client 脚本来启动 SQL 客户端。
./bin/sql-client.sh Hello World # SQL 客户端（我们的查询编辑器）启动并运行后，就可以开始编写查询了。 让我们使用以下简单查询打印出 \u0026lsquo;Hello World\u0026rsquo;：
SELECT \u0026#39;Hello World\u0026#39;; 运行 HELP 命令会列出所有支持的 SQL 语句。让我们运行一个 SHOW 命令，来查看 Flink 内置函数的完整列表。
SHOW FUNCTIONS; 这些函数为用户在开发 SQL 查询时提供了一个功能强大的工具箱。 例如，CURRENT_TIMESTAMP 将在执行时打印出机器的当前系统时间。
SELECT CURRENT_TIMESTAMP; Back to top
Source 表 # 与所有 SQL 引擎一样，Flink 查询操作是在表上进行。与传统数据库不同，Flink 不在本地管理静态数据；相反，它的查询在外部表上连续运行。
Flink 数据处理流水线开始于 source 表。source 表产生在查询执行期间可以被操作的行；它们是查询时 FROM 子句中引用的表。这些表可能是 Kafka 的 topics，数据库，文件系统，或者任何其它 Flink 知道如何消费的系统。
可以通过 SQL 客户端或使用环境配置文件来定义表。SQL 客户端支持类似于传统 SQL 的 SQL DDL 命令。标准 SQL DDL 用于创建，修改，删除表。
Flink 支持不同的连接器和格式相结合以定义表。下面是一个示例，定义一个以 CSV 文件作为存储格式的 source 表，其中 emp_id，name，dept_id 作为 CREATE 表语句中的列。
CREATE TABLE employee_information ( emp_id INT, name VARCHAR, dept_id INT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/something.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); 可以从该表中定义一个连续查询，当新行可用时读取并立即输出它们的结果。 例如，我们可以过滤出只在部门 1 中工作的员工。
SELECT * from employee_information WHERE dept_id = 1; Back to top
连续查询 # 虽然最初设计时没有考虑流语义，但 SQL 是用于构建连续数据流水线的强大工具。Flink SQL 与传统数据库查询的不同之处在于，Flink SQL 持续消费到达的行并对其结果进行更新。
一个连续查询永远不会终止，并会产生一个动态表作为结果。动态表是 Flink 中 Table API 和 SQL 对流数据支持的核心概念。
连续流上的聚合需要在查询执行期间不断地存储聚合的结果。例如，假设你需要从传入的数据流中计算每个部门的员工人数。查询需要维护每个部门最新的计算总数，以便在处理新行时及时输出结果。
SELECT dept_id, COUNT(*) as emp_count FROM employee_information GROUP BY dept_id; 这样的查询被认为是 有状态的。Flink 的高级容错机制将维持内部状态和一致性，因此即使遇到硬件故障，查询也始终返回正确结果。
Sink 表 # 当运行此查询时，SQL 客户端实时但是以只读方式提供输出。存储结果，作为报表或仪表板的数据来源，需要写到另一个表。这可以使用 INSERT INTO 语句来实现。本节中引用的表称为 sink 表。INSERT INTO 语句将作为一个独立查询被提交到 Flink 集群中。
INSERT INTO department_counts SELECT dept_id, COUNT(*) as emp_count FROM employee_information; 提交后，它将运行并将结果直接存储到 sink 表中，而不是将结果加载到系统内存中。
Back to top
寻求帮助！ # 如果你有疑惑，可以查阅社区支持资源。 特别是，Apache Flink 的用户邮件列表一直被评为是任何 Apache 项目中最活跃的项目之一，也是快速获得帮助的好方法。
了解更多资源 # SQL：SQL 支持的操作和语法。 SQL 客户端：不用编写代码就可以尝试 Flink SQL，可以直接提交 SQL 任务到集群上。 概念与通用 API：Table API 和 SQL 公共概念以及 API。 流式概念：Table API 和 SQL 中流式相关的文档，比如配置时间属性和如何处理更新结果。 内置函数：Table API 和 SQL 中的内置函数。 连接外部系统：读写外部系统的连接器和格式。 Back to top
`}),e.add({id:57,href:"/flink/flink-docs-master/zh/docs/dev/datastream/event-time/generating_watermarks/",title:"生成 Watermark",section:"事件时间",content:` 生成 Watermark # 在本节中，你将了解 Flink 中用于处理事件时间的时间戳和 watermark 相关的 API。有关事件时间，处理时间和摄取时间的介绍，请参阅事件时间概览小节。
Watermark 策略简介 # 为了使用事件时间语义，Flink 应用程序需要知道事件时间戳对应的字段，意味着数据流中的每个元素都需要拥有可分配的事件时间戳。其通常通过使用 TimestampAssigner API 从元素中的某个字段去访问/提取时间戳。
时间戳的分配与 watermark 的生成是齐头并进的，其可以告诉 Flink 应用程序事件时间的进度。其可以通过指定 WatermarkGenerator 来配置 watermark 的生成方式。
使用 Flink API 时需要设置一个同时包含 TimestampAssigner 和 WatermarkGenerator 的 WatermarkStrategy。WatermarkStrategy 工具类中也提供了许多常用的 watermark 策略，并且用户也可以在某些必要场景下构建自己的 watermark 策略。WatermarkStrategy 接口如下：
public interface WatermarkStrategy\u0026lt;T\u0026gt; extends TimestampAssignerSupplier\u0026lt;T\u0026gt;, WatermarkGeneratorSupplier\u0026lt;T\u0026gt;{ /** * 根据策略实例化一个可分配时间戳的 {@link TimestampAssigner}。 */ @Override TimestampAssigner\u0026lt;T\u0026gt; createTimestampAssigner(TimestampAssignerSupplier.Context context); /** * 根据策略实例化一个 watermark 生成器。 */ @Override WatermarkGenerator\u0026lt;T\u0026gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context); } 如上所述，通常情况下，你不用实现此接口，而是可以使用 WatermarkStrategy 工具类中通用的 watermark 策略，或者可以使用这个工具类将自定义的 TimestampAssigner 与 WatermarkGenerator 进行绑定。例如，你想要要使用有界无序（bounded-out-of-orderness）watermark 生成器和一个 lambda 表达式作为时间戳分配器，那么可以按照如下方式实现：
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -\u0026gt; event.f0); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withTimestampAssigner(new SerializableTimestampAssigner[(Long, String)] { override def extractTimestamp(element: (Long, String), recordTimestamp: Long): Long = element._1 }) Python class FirstElementTimestampAssigner(TimestampAssigner): def extract_timestamp(self, value, record_timestamp): return value[0] WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_timestamp_assigner(FirstElementTimestampAssigner()) 其中 TimestampAssigner 的设置与否是可选的，大多数情况下，可以不用去特别指定。例如，当使用 Kafka 或 Kinesis 数据源时，你可以直接从 Kafka/Kinesis 数据源记录中获取到时间戳。
稍后我们将在自定义 WatermarkGenerator 小节学习 WatermarkGenerator 接口。
注意： 时间戳和 watermark 都是从 1970-01-01T00:00:00Z 起的 Java 纪元开始，并以毫秒为单位。 使用 Watermark 策略 # WatermarkStrategy 可以在 Flink 应用程序中的两处使用，第一种是直接在数据源上使用，第二种是直接在非数据源的操作之后使用。
第一种方式相比会更好，因为数据源可以利用 watermark 生成逻辑中有关分片/分区（shards/partitions/splits）的信息。使用这种方式，数据源通常可以更精准地跟踪 watermark，整体 watermark 生成将更精确。直接在源上指定 WatermarkStrategy 意味着你必须使用特定数据源接口，参阅 Watermark 策略与 Kafka 连接器以了解如何使用 Kafka Connector，以及有关每个分区的 watermark 是如何生成以及工作的。
仅当无法直接在数据源上设置策略时，才应该使用第二种方式（在任意转换操作之后设置 WatermarkStrategy）：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;MyEvent\u0026gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo); DataStream\u0026lt;MyEvent\u0026gt; withTimestampsAndWatermarks = stream .filter( event -\u0026gt; event.severity() == WARNING ) .assignTimestampsAndWatermarks(\u0026lt;watermark strategy\u0026gt;); withTimestampsAndWatermarks .keyBy( (event) -\u0026gt; event.getGroup() ) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce( (a, b) -\u0026gt; a.add(b) ) .addSink(...); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val stream: DataStream[MyEvent] = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter()) val withTimestampsAndWatermarks: DataStream[MyEvent] = stream .filter( _.severity == WARNING ) .assignTimestampsAndWatermarks(\u0026lt;watermark strategy\u0026gt;) withTimestampsAndWatermarks .keyBy( _.getGroup ) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce( (a, b) =\u0026gt; a.add(b) ) .addSink(...) Python env = StreamExecutionEnvironment.get_execution_environment() # currently read_file is not supported in PyFlink stream = env \\ .read_text_file(my_file_path, charset) \\ .map(lambda s: MyEvent.from_string(s)) with_timestamp_and_watermarks = stream \\ .filter(lambda e: e.severity() == WARNING) \\ .assign_timestamp_and_watermarks(\u0026lt;watermark strategy\u0026gt;) with_timestamp_and_watermarks \\ .key_by(lambda e: e.get_group()) \\ .window(TumblingEventTimeWindows.of(Time.seconds(10))) \\ .reduce(lambda a, b: a.add(b)) \\ .add_sink(...) 使用 WatermarkStrategy 去获取流并生成带有时间戳的元素和 watermark 的新流时，如果原始流已经具有时间戳或 watermark，则新指定的时间戳分配器将覆盖原有的时间戳和 watermark。
处理空闲数据源 # 如果数据源中的某一个分区/分片在一段时间内未发送事件数据，则意味着 WatermarkGenerator 也不会获得任何新数据去生成 watermark。我们称这类数据源为空闲输入或空闲源。在这种情况下，当某些其他分区仍然发送事件数据的时候就会出现问题。由于下游算子 watermark 的计算方式是取所有不同的上游并行数据源 watermark 的最小值，则其 watermark 将不会发生变化。
为了解决这个问题，你可以使用 WatermarkStrategy 来检测空闲输入并将其标记为空闲状态。WatermarkStrategy 为此提供了一个工具接口：
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)) Python WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_idleness(Duration.of_minutes(1)) Watermark alignment Beta # In the previous paragraph we discussed a situation when splits/partitions/shards or sources are idle and can stall increasing watermarks. On the other side of the spectrum, a split/partition/shard or source may process records very fast and in turn increase its watermark relatively faster than the others. This on its own is not a problem per se. However, for downstream operators that are using watermarks to emit some data it can actually become a problem.
In this case, contrary to idle sources, the watermark of such downstream operator (like windowed joins on aggregations) can progress. However, such operator might need to buffer excessive amount of data coming from the fast inputs, as the minimal watermark from all of its inputs is held back by the lagging input. All records emitted by the fast input will hence have to be buffered in the said downstream operator state, which can lead into uncontrollable growth of the operator\u0026rsquo;s state.
In order to address the issue, you can enable watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest. You can enable alignment for every source separately:
Java WatermarkStrategy .\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withWatermarkAlignment(\u0026#34;alignment-group-1\u0026#34;, Duration.ofSeconds(20), Duration.ofSeconds(1)); Scala WatermarkStrategy .forBoundedOutOfOrderness[(Long, String)](Duration.ofSeconds(20)) .withWatermarkAlignment(\u0026#34;alignment-group-1\u0026#34;, Duration.ofSeconds(20), Duration.ofSeconds(1)) Python WatermarkStrategy \\ .for_bounded_out_of_orderness(Duration.of_seconds(20)) \\ .with_watermark_alignment(\u0026#34;alignment-group-1\u0026#34;, Duration.of_seconds(20), Duration.of_seconds(1)) Note: You can enable watermark alignment only for FLIP-27 sources. It does not work for legacy or if applied after the source via DataStream#assignTimestampsAndWatermarks. When enabling the alignment, you need to tell Flink, which group should the source belong. You do that by providing a label (e.g. alignment-group-1) which bind together all sources that share it. Moreover, you have to tell the maximal drift from the current minimal watermarks across all sources belonging to that group. The third parameter describes how often the current maximal watermark should be updated. The downside of frequent updates is that there will be more RPC messages travelling between TMs and the JM.
In order to achieve the alignment Flink will pause consuming from the source/task, which generated watermark that is too far into the future. In the meantime it will continue reading records from other sources/tasks which can move the combined watermark forward and that way unblock the faster one.
Note: As of 1.15, Flink supports aligning across tasks of the same source and/or different sources. It does not support aligning splits/partitions/shards in the same task.
In a case where there are e.g. two Kafka partitions that produce watermarks at different pace, that get assigned to the same task watermark might not behave as expected. Fortunately, worst case it should not perform worse than without alignment.
Given the limitation above, we suggest applying watermark alignment in two situations:
You have two different sources (e.g. Kafka and File) that produce watermarks at different speeds You run your source with parallelism equal to the number of splits/shards/partitions, which results in every subtask being assigned a single unit of work. 自定义 WatermarkGenerator # TimestampAssigner 是一个可以从事件数据中提取时间戳字段的简单函数，我们无需详细查看其实现。但是 WatermarkGenerator 的编写相对就要复杂一些了，我们将在接下来的两小节中介绍如何实现此接口。WatermarkGenerator 接口代码如下：
/** * {@code WatermarkGenerator} 可以基于事件或者周期性的生成 watermark。 * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;注意：\u0026lt;/b\u0026gt; WatermarkGenerator 将以前互相独立的 {@code AssignerWithPunctuatedWatermarks} * 和 {@code AssignerWithPeriodicWatermarks} 一同包含了进来。 */ @Public public interface WatermarkGenerator\u0026lt;T\u0026gt; { /** * 每来一条事件数据调用一次，可以检查或者记录事件的时间戳，或者也可以基于事件数据本身去生成 watermark。 */ void onEvent(T event, long eventTimestamp, WatermarkOutput output); /** * 周期性的调用，也许会生成新的 watermark，也许不会。 * * \u0026lt;p\u0026gt;调用此方法生成 watermark 的间隔时间由 {@link ExecutionConfig#getAutoWatermarkInterval()} 决定。 */ void onPeriodicEmit(WatermarkOutput output); } watermark 的生成方式本质上是有两种：周期性生成和标记生成。
周期性生成器通常通过 onEvent() 观察传入的事件数据，然后在框架调用 onPeriodicEmit() 时发出 watermark。
标记生成器将查看 onEvent() 中的事件数据，并等待检查在流中携带 watermark 的特殊标记事件或打点数据。当获取到这些事件数据时，它将立即发出 watermark。通常情况下，标记生成器不会通过 onPeriodicEmit() 发出 watermark。
接下来，我们将学习如何实现上述两类生成器。
自定义周期性 Watermark 生成器 # 周期性生成器会观察流事件数据并定期生成 watermark（其生成可能取决于流数据，或者完全基于处理时间）。
生成 watermark 的时间间隔（每 n 毫秒）可以通过 ExecutionConfig.setAutoWatermarkInterval(...) 指定。每次都会调用生成器的 onPeriodicEmit() 方法，如果返回的 watermark 非空且值大于前一个 watermark，则将发出新的 watermark。
如下是两个使用周期性 watermark 生成器的简单示例。注意：Flink 已经附带了 BoundedOutOfOrdernessWatermarks，它实现了 WatermarkGenerator，其工作原理与下面的 BoundedOutOfOrdernessGenerator 相似。可以在这里参阅如何使用它的内容。
Java /** * 该 watermark 生成器可以覆盖的场景是：数据源在一定程度上乱序。 * 即某个最新到达的时间戳为 t 的元素将在最早到达的时间戳为 t 的元素之后最多 n 毫秒到达。 */ public class BoundedOutOfOrdernessGenerator implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { private final long maxOutOfOrderness = 3500; // 3.5 秒 private long currentMaxTimestamp; @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp); } @Override public void onPeriodicEmit(WatermarkOutput output) { // 发出的 watermark = 当前最大时间戳 - 最大乱序时间 output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)); } } /** * 该生成器生成的 watermark 滞后于处理时间固定量。它假定元素会在有限延迟后到达 Flink。 */ public class TimeLagWatermarkGenerator implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { private final long maxTimeLag = 5000; // 5 秒 @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { // 处理时间场景下不需要实现 } @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)); } } Scala /** * 该 watermark 生成器可以覆盖的场景是：数据源在一定程度上乱序。 * 即某个最新到达的时间戳为 t 的元素将在最早到达的时间戳为 t 的元素之后最多 n 毫秒到达。 */ class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxOutOfOrderness = 3500L // 3.5 秒 var currentMaxTimestamp: Long = _ override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { currentMaxTimestamp = max(eventTimestamp, currentMaxTimestamp) } override def onPeriodicEmit(): Unit = { // 发出的 watermark = 当前最大时间戳 - 最大乱序时间 output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)) } } /** * 该生成器生成的 watermark 滞后于处理时间固定量。它假定元素会在有限延迟后到达 Flink。 */ class TimeLagWatermarkGenerator extends AssignerWithPeriodicWatermarks[MyEvent] { val maxTimeLag = 5000L // 5 秒 override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { // 处理时间场景下不需要实现 } override def onPeriodicEmit(): Unit = { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)) } } Python 目前在python中不支持该api 自定义标记 Watermark 生成器 # 标记 watermark 生成器观察流事件数据并在获取到带有 watermark 信息的特殊事件元素时发出 watermark。
如下是实现标记生成器的方法，当事件带有某个指定标记时，该生成器就会发出 watermark：
Java public class PunctuatedAssigner implements WatermarkGenerator\u0026lt;MyEvent\u0026gt; { @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())); } } @Override public void onPeriodicEmit(WatermarkOutput output) { // onEvent 中已经实现 } } Scala class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks[MyEvent] { override def onEvent(element: MyEvent, eventTimestamp: Long): Unit = { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())) } } override def onPeriodicEmit(): Unit = { // onEvent 中已经实现 } } Python Python API 中尚不支持该特性。 注意： 可以针对每个事件去生成 watermark。但是由于每个 watermark 都会在下游做一些计算，因此过多的 watermark 会降低程序性能。 Watermark 策略与 Kafka 连接器 # 当使用 Apache Kafka 连接器作为数据源时，每个 Kafka 分区可能有一个简单的事件时间模式（递增的时间戳或有界无序）。然而，当使用 Kafka 数据源时，多个分区常常并行使用，因此交错来自各个分区的事件数据就会破坏每个分区的事件时间模式（这是 Kafka 消费客户端所固有的）。
在这种情况下，你可以使用 Flink 中可识别 Kafka 分区的 watermark 生成机制。使用此特性，将在 Kafka 消费端内部针对每个 Kafka 分区生成 watermark，并且不同分区 watermark 的合并方式与在数据流 shuffle 时的合并方式相同。
例如，如果每个 Kafka 分区中的事件时间戳严格递增，则使用单调递增时间戳分配器按分区生成的 watermark 将生成完美的全局 watermark。注意，我们在示例中未使用 TimestampAssigner，而是使用了 Kafka 记录自身的时间戳。
下图展示了如何使用单 kafka 分区 watermark 生成机制，以及在这种情况下 watermark 如何通过 dataflow 传播。
Java FlinkKafkaConsumer\u0026lt;MyType\u0026gt; kafkaSource = new FlinkKafkaConsumer\u0026lt;\u0026gt;(\u0026#34;myTopic\u0026#34;, schema, props); kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20))); DataStream\u0026lt;MyType\u0026gt; stream = env.addSource(kafkaSource); Scala val kafkaSource = new FlinkKafkaConsumer[MyType](\u0026#34;myTopic\u0026#34;, schema, props) kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20))) val stream: DataStream[MyType] = env.addSource(kafkaSource) Python kafka_source = FlinkKafkaConsumer(\u0026#34;timer-stream-source\u0026#34;, schema, props) stream = env .add_source(kafka_source) .assign_timestamps_and_watermarks( WatermarkStrategy .for_bounded_out_of_orderness(Duration.of_seconds(20))) 算子处理 Watermark 的方式 # 一般情况下，在将 watermark 转发到下游之前，需要算子对其进行触发的事件完全进行处理。例如，WindowOperator 将首先计算该 watermark 触发的所有窗口数据，当且仅当由此 watermark 触发计算进而生成的所有数据被转发到下游之后，其才会被发送到下游。换句话说，由于此 watermark 的出现而产生的所有数据元素都将在此 watermark 之前发出。
相同的规则也适用于 TwoInputStreamOperator。但是，在这种情况下，算子当前的 watermark 会取其两个输入的最小值。
详细内容可查看对应算子的实现：OneInputStreamOperator#processWatermark、TwoInputStreamOperator#processWatermark1 和 TwoInputStreamOperator#processWatermark2。
可以弃用 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 了 # 在 Flink 新的 WatermarkStrategy，TimestampAssigner 和 WatermarkGenerator 的抽象接口之前，Flink 使用的是 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks。你仍可以在 API 中看到它们，但建议使用新接口，因为其对时间戳和 watermark 等重点的抽象和分离很清晰，并且还统一了周期性和标记形式的 watermark 生成方式。
Back to top
`}),e.add({id:58,href:"/flink/flink-docs-master/zh/docs/learn-flink/",title:"实践练习",section:"Docs",content:" "}),e.add({id:59,href:"/flink/flink-docs-master/zh/docs/dev/configuration/maven/",title:"使用 Maven",section:"项目配置",content:` 如何使用 Maven 配置您的项目 # 本指南将向您展示如何使用 Maven 配置 Flink 作业项目，Maven是 由 Apache Software Foundation 开源的自动化构建工具，使您能够构建、发布和部署项目。您可以使用它来管理软件项目的整个生命周期。
要求 # Maven 3.0.4 (or higher) Java 11 将项目导入 IDE # 创建项目目录和文件后，我们建议您将此项目导入到 IDE 进行开发和测试。
IntelliJ IDEA 支持开箱即用的 Maven 项目。Eclipse 提供了 m2e 插件 来导入 Maven 项目。
注意： Java 的默认 JVM 堆大小对于 Flink 来说可能太小，您应该手动增加它。在 Eclipse 中，选中 Run Configurations -\u0026gt; Arguments 并在 VM Arguments 框里填上：-Xmx800m。在 IntelliJ IDEA 中，推荐选中 Help | Edit Custom VM Options 菜单修改 JVM 属性。详情请查阅本文。
关于 IntelliJ 的注意事项： 要使应用程序在 IntelliJ IDEA 中运行，需要在运行配置中的 Include dependencies with \u0026ldquo;Provided\u0026rdquo; scope打勾。如果此选项不可用（可能是由于使用了较旧的 IntelliJ IDEA 版本），可创建一个调用应用程序main()\` 方法的测试用例。
构建项目 # 如果您想 构建/打包 您的项目，请转到您的项目目录并运行 \u0026lsquo;mvn clean package\u0026rsquo; 命令。您将 找到一个 JAR 文件，其中包含您的应用程序（还有已作为依赖项添加到应用程序的连接器和库）：target/\u0026lt;artifact-id\u0026gt;-\u0026lt;version\u0026gt;.jar。
注意： 如果您使用不同于 DataStreamJob 的类作为应用程序的主类/入口点，我们建议您对 pom.xml 文件里的 mainClassName 配置进行相应的修改。这样，Flink 可以通过 JAR 文件运行应用程序，而无需额外指定主类。
向项目添加依赖项 # 打开您项目目录的 pom.xml，在 dependencies 标签内添加依赖项。
例如，您可以用如下方式添加 Kafka 连接器依赖：
\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 然后在命令行执行 mvn install。
当您在由 Java Project Template、Scala Project Template 或 Gradle 创建出来的项目里，运行 mvn clean package 会自动将应用程序依赖打包进应用程序 JAR。对于不是通过这些模板创建的项目，我们建议使用 Maven Shade 插件以将所有必需的依赖项打包进应用程序 jar。
重要提示： 请注意，应将所有这些（核心）依赖项的生效范围置为 provided。这意味着需要对它们进行编译，但不应将它们打包进项目生成的应用程序 JAR 文件中。如果不设置为 provided，最好的情况是生成的 JAR 变得过大，因为它还包含所有 Flink 核心依赖项。最坏的情况是添加到应用程序 JAR 文件中的 Flink 核心依赖项与您自己的一些依赖项的版本冲突（通常通过反向类加载来避免）。
要将依赖项正确地打包进应用程序 JAR 中，必须把应用程序依赖项的生效范围设置为 compile 。
打包应用程序 # 在部署应用到 Flink 环境之前，您需要根据使用场景用不同的方式打包 Flink 应用程序。
如果您想为 Flink 作业创建 JAR 并且只使用 Flink 依赖而不使用任何第三方依赖（比如使用 JSON 格式的文件系统连接器），您不需要创建一个 uber/fat JAR 或将任何依赖打进包。
如果您想为 Flink 作业创建 JAR 并使用未内置在 Flink 发行版中的外部依赖项，您可以将它们添加到发行版的类路径中，或者将它们打包进您的 uber/fat 应用程序 JAR 中。
您可以将生成的 uber/fat JAR 提交到本地或远程集群：
bin/flink run -c org.example.MyJob myFatJar.jar 要了解有关如何部署 Flink 作业的更多信息，请查看部署指南。
创建包含依赖项的 uber/fat JAR 的模板 # 为构建一个包含所有必需的连接器、 类库依赖项的应用程序 JAR，您可以使用如下 shade 插件定义：
\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;artifactSet\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;com.google.code.findbugs:jsr305\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/artifactSet\u0026gt; \u0026lt;filters\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --\u0026gt; \u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/filters\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;!-- Replace this with the main class of your job --\u0026gt; \u0026lt;mainClass\u0026gt;my.programs.main.clazz\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; Maven shade 插件 默认会包含所有的生效范围是 \u0026ldquo;runtime\u0026rdquo; 或 \u0026ldquo;compile\u0026rdquo; 的依赖项。
`}),e.add({id:60,href:"/flink/flink-docs-master/zh/docs/libs/cep/",title:"事件处理 (CEP)",section:"Libraries",content:` FlinkCEP - Flink的复杂事件处理 # FlinkCEP是在Flink上层实现的复杂事件处理库。 它可以让你在无限事件流中检测出特定的事件模型，有机会掌握数据中重要的那部分。
本页讲述了Flink CEP中可用的API，我们首先讲述模式API，它可以让你指定想在数据流中检测的模式，然后讲述如何检测匹配的事件序列并进行处理。 再然后我们讲述Flink在按照事件时间处理迟到事件时的假设， 以及如何从旧版本的Flink向1.13之后的版本迁移作业。
开始 # 如果你想现在开始尝试，创建一个 Flink 程序， 添加 FlinkCEP 的依赖到项目的pom.xml文件中。
Java \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-cep\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Scala \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-cep-scala_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! FlinkCEP 不是二进制发布包的一部分。在集群上执行如何链接它可以看这里。 现在可以开始使用Pattern API写你的第一个CEP程序了。
DataStream中的事件，如果你想在上面进行模式匹配的话，必须实现合适的 equals()和hashCode()方法， 因为FlinkCEP使用它们来比较和匹配事件。 Java DataStream\u0026lt;Event\u0026gt; input = ...; Pattern\u0026lt;Event, ?\u0026gt; pattern = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where( new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event event) { return event.getId() == 42; } } ).next(\u0026#34;middle\u0026#34;).subtype(SubEvent.class).where( new SimpleCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent subEvent) { return subEvent.getVolume() \u0026gt;= 10.0; } } ).followedBy(\u0026#34;end\u0026#34;).where( new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event event) { return event.getName().equals(\u0026#34;end\u0026#34;); } } ); PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); DataStream\u0026lt;Alert\u0026gt; result = patternStream.process( new PatternProcessFunction\u0026lt;Event, Alert\u0026gt;() { @Override public void processMatch( Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern, Context ctx, Collector\u0026lt;Alert\u0026gt; out) throws Exception { out.collect(createAlertFrom(pattern)); } }); Scala val input: DataStream[Event] = ... val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;).where(_.getId == 42) .next(\u0026#34;middle\u0026#34;).subtype(classOf[SubEvent]).where(_.getVolume \u0026gt;= 10.0) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;end\u0026#34;) val patternStream = CEP.pattern(input, pattern) val result: DataStream[Alert] = patternStream.process( new PatternProcessFunction[Event, Alert]() { override def processMatch( \`match\`: util.Map[String, util.List[Event]], ctx: PatternProcessFunction.Context, out: Collector[Alert]): Unit = { out.collect(createAlertFrom(pattern)) } }) 模式API # 模式API可以让你定义想从输入流中抽取的复杂模式序列。
每个复杂的模式序列包括多个简单的模式，比如，寻找拥有相同属性事件序列的模式。从现在开始，我们把这些简单的模式称作模式， 把我们在数据流中最终寻找的复杂模式序列称作模式序列，你可以把模式序列看作是这样的模式构成的图， 这些模式基于用户指定的条件从一个转换到另外一个，比如 event.getName().equals(\u0026quot;end\u0026quot;)。 一个匹配是输入事件的一个序列，这些事件通过一系列有效的模式转换，能够访问到复杂模式图中的所有模式。
每个模式必须有一个独一无二的名字，你可以在后面使用它来识别匹配到的事件。 模式的名字不能包含字符\u0026quot;:\u0026quot;. 这一节的剩余部分我们会先讲述如何定义单个模式，然后讲如何将单个模式组合成复杂模式。
单个模式 # 一个模式可以是一个单例或者循环模式。单例模式只接受一个事件，循环模式可以接受多个事件。 在模式匹配表达式中，模式\u0026quot;a b+ c? d\u0026quot;（或者\u0026quot;a\u0026quot;，后面跟着一个或者多个\u0026quot;b\u0026quot;，再往后可选择的跟着一个\u0026quot;c\u0026quot;，最后跟着一个\u0026quot;d\u0026quot;）， a，c?，和 d都是单例模式，b+是一个循环模式。默认情况下，模式都是单例的，你可以通过使用量词把它们转换成循环模式。 每个模式可以有一个或者多个条件来决定它接受哪些事件。
量词 # 在FlinkCEP中，你可以通过这些方法指定循环模式：pattern.oneOrMore()，指定期望一个给定事件出现一次或者多次的模式（例如前面提到的b+模式）； pattern.times(#ofTimes)，指定期望一个给定事件出现特定次数的模式，例如出现4次a； pattern.times(#fromTimes, #toTimes)，指定期望一个给定事件出现次数在一个最小值和最大值中间的模式，比如出现2-4次a。
你可以使用pattern.greedy()方法让循环模式变成贪心的，但现在还不能让模式组贪心。 你可以使用pattern.optional()方法让所有的模式变成可选的，不管是否是循环模式。
对一个命名为start的模式，以下量词是有效的：
Java // 期望出现4次 start.times(4); // 期望出现0或者4次 start.times(4).optional(); // 期望出现2、3或者4次 start.times(2, 4); // 期望出现2、3或者4次，并且尽可能的重复次数多 start.times(2, 4).greedy(); // 期望出现0、2、3或者4次 start.times(2, 4).optional(); // 期望出现0、2、3或者4次，并且尽可能的重复次数多 start.times(2, 4).optional().greedy(); // 期望出现1到多次 start.oneOrMore(); // 期望出现1到多次，并且尽可能的重复次数多 start.oneOrMore().greedy(); // 期望出现0到多次 start.oneOrMore().optional(); // 期望出现0到多次，并且尽可能的重复次数多 start.oneOrMore().optional().greedy(); // 期望出现2到多次 start.timesOrMore(2); // 期望出现2到多次，并且尽可能的重复次数多 start.timesOrMore(2).greedy(); // 期望出现0、2或多次 start.timesOrMore(2).optional(); // 期望出现0、2或多次，并且尽可能的重复次数多 start.timesOrMore(2).optional().greedy(); Scala // 期望出现4次 start.times(4) // 期望出现0或者4次 start.times(4).optional() // 期望出现2、3或者4次 start.times(2, 4) // 期望出现2、3或者4次，并且尽可能的重复次数多 start.times(2, 4).greedy() // 期望出现0、2、3或者4次 start.times(2, 4).optional() // 期望出现0、2、3或者4次，并且尽可能的重复次数多 start.times(2, 4).optional().greedy() // 期望出现1到多次 start.oneOrMore() // 期望出现1到多次，并且尽可能的重复次数多 start.oneOrMore().greedy() // 期望出现0到多次 start.oneOrMore().optional() // 期望出现0到多次，并且尽可能的重复次数多 start.oneOrMore().optional().greedy() // 期望出现2到多次 start.timesOrMore(2) // 期望出现2到多次，并且尽可能的重复次数多 start.timesOrMore(2).greedy() // 期望出现0、2或多次 start.timesOrMore(2).optional() // 期望出现0、2或多次，并且尽可能的重复次数多 start.timesOrMore(2).optional().greedy() 条件 # 对每个模式你可以指定一个条件来决定一个进来的事件是否被接受进入这个模式，例如，它的value字段应该大于5，或者大于前面接受的事件的平均值。 指定判断事件属性的条件可以通过pattern.where()、pattern.or()或者pattern.until()方法。 这些可以是IterativeCondition或者SimpleCondition。
迭代条件: 这是最普遍的条件类型。使用它可以指定一个基于前面已经被接受的事件的属性或者它们的一个子集的统计数据来决定是否接受时间序列的条件。
下面是一个迭代条件的代码，它接受\u0026quot;middle\u0026quot;模式下一个事件的名称开头是\u0026quot;foo\u0026quot;， 并且前面已经匹配到的事件加上这个事件的价格小于5.0。 迭代条件非常强大，尤其是跟循环模式结合使用时。
Java middle.oneOrMore() .subtype(SubEvent.class) .where(new IterativeCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent value, Context\u0026lt;SubEvent\u0026gt; ctx) throws Exception { if (!value.getName().startsWith(\u0026#34;foo\u0026#34;)) { return false; } double sum = value.getPrice(); for (Event event : ctx.getEventsForPattern(\u0026#34;middle\u0026#34;)) { sum += event.getPrice(); } return Double.compare(sum, 5.0) \u0026lt; 0; } }); Scala middle.oneOrMore() .subtype(classOf[SubEvent]) .where( (value, ctx) =\u0026gt; { lazy val sum = ctx.getEventsForPattern(\u0026#34;middle\u0026#34;).map(_.getPrice).sum value.getName.startsWith(\u0026#34;foo\u0026#34;) \u0026amp;\u0026amp; sum + value.getPrice \u0026lt; 5.0 } ) 调用ctx.getEventsForPattern(...)可以获得所有前面已经接受作为可能匹配的事件。 调用这个操作的代价可能很小也可能很大，所以在实现你的条件时，尽量少使用它。 描述的上下文提供了获取事件时间属性的方法。更多细节可以看时间上下文。
简单条件： 这种类型的条件扩展了前面提到的IterativeCondition类，它决定是否接受一个事件只取决于事件自身的属性。
Java start.where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return value.getName().startsWith(\u0026#34;foo\u0026#34;); } }); Scala start.where(event =\u0026gt; event.getName.startsWith(\u0026#34;foo\u0026#34;)) 最后，你可以通过pattern.subtype(subClass)方法限制接受的事件类型是初始事件的子类型。
Java start.subtype(SubEvent.class).where(new SimpleCondition\u0026lt;SubEvent\u0026gt;() { @Override public boolean filter(SubEvent value) { return ...; // 一些判断条件 } }); Scala start.subtype(classOf[SubEvent]).where(subEvent =\u0026gt; ... /* 一些判断条件 */) 组合条件： 如上所示，你可以把subtype条件和其他的条件结合起来使用。这适用于任何条件，你可以通过依次调用where()来组合条件。 最终的结果是每个单一条件的结果的逻辑AND。如果想使用OR来组合条件，你可以像下面这样使用or()方法。
Java pattern.where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return ...; // 一些判断条件 } }).or(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) { return ...; // 一些判断条件 } }); Scala pattern.where(event =\u0026gt; ... /* 一些判断条件 */).or(event =\u0026gt; ... /* 一些判断条件 */) 停止条件： 如果使用循环模式（oneOrMore()和oneOrMore().optional()），你可以指定一个停止条件，例如，接受事件的值大于5直到值的和小于50。
为了更好的理解它，看下面的例子。给定
模式如\u0026quot;(a+ until b)\u0026quot; (一个或者更多的\u0026quot;a\u0026quot;直到\u0026quot;b\u0026quot;)
到来的事件序列\u0026quot;a1\u0026quot; \u0026quot;c\u0026quot; \u0026quot;a2\u0026quot; \u0026quot;b\u0026quot; \u0026quot;a3\u0026quot;
输出结果会是： {a1 a2} {a1} {a2} {a3}.
你可以看到{a1 a2 a3}和{a2 a3}由于停止条件没有被输出。
where(condition) # 为当前模式定义一个条件。为了匹配这个模式，一个事件必须满足某些条件。 多个连续的 where() 语句取与组成判断条件。
Java pattern.where(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ... // 一些判断条件 } }); Scala pattern.where(event =\u0026gt; ... /* 一些判断条件 */) or(condition) # 增加一个新的判断，和当前的判断取或。一个事件只要满足至少一个判断条件就匹配到模式。
Java pattern.where(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // 一些判断条件 } }).or(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // 替代条件 } }); Scala pattern.where(event =\u0026gt; ... /* 一些判断条件 */) .or(event =\u0026gt; ... /* 替代条件 */) until(condition) # 为循环模式指定一个停止条件。意思是满足了给定的条件的事件出现后，就不会再有事件被接受进入模式了。 只适用于和oneOrMore()同时使用。 NOTE: 在基于事件的条件中，它可用于清理对应模式的状态。
Java pattern.oneOrMore().until(new IterativeCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value, Context ctx) throws Exception { return ...; // 替代条件 } }); Scala pattern.oneOrMore().until(event =\u0026gt; ... /* 一些判断条件 */) subtype(subClass) # 为当前模式定义一个子类型条件。一个事件只有是这个子类型的时候才能匹配到模式。
Java pattern.subtype(SubEvent.class); Scala pattern.subtype(classOf[SubEvent]) oneOrMore() # 指定模式期望匹配到的事件至少出现一次。 默认（在子事件间）使用松散的内部连续性。 关于内部连续性的更多信息可以参考连续性。 推荐使用 until()或者 within()来清理状态。
Java pattern.oneOrMore(); Scala pattern.oneOrMore() timesOrMore(#times) # 指定模式期望匹配到的事件至少出现 #times 次。 默认（在子事件间）使用松散的内部连续性。 关于内部连续性的更多信息可以参考连续性。
Java pattern.timesOrMore(2); times(#ofTimes) # 指定模式期望匹配到的事件正好出现的次数。 默认（在子事件间）使用松散的内部连续性。 关于内部连续性的更多信息可以参考连续性。
Java pattern.times(2); Scala pattern.times(2) times(#fromTimes, #toTimes) # 指定模式期望匹配到的事件出现次数在#fromTimes和#toTimes之间。 默认（在子事件间）使用松散的内部连续性。 关于内部连续性的更多信息可以参考连续性。
Java pattern.times(2, 4); Scala pattern.times(2, 4) optional() # 指定这个模式是可选的，也就是说，它可能根本不出现。这对所有之前提到的量词都适用。
Java pattern.oneOrMore().optional(); Scala pattern.oneOrMore().optional() greedy() # 指定这个模式是贪心的，也就是说，它会重复尽可能多的次数。这只对量词适用，现在还不支持模式组。
Java pattern.oneOrMore().greedy(); Scala pattern.oneOrMore().greedy() 组合模式 # 现在你已经看到单个的模式是什么样的了，该去看看如何把它们连接起来组成一个完整的模式序列。
模式序列由一个初始模式作为开头，如下所示：
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;); Scala val start : Pattern[Event, _] = Pattern.begin(\u0026#34;start\u0026#34;) 接下来，你可以增加更多的模式到模式序列中并指定它们之间所需的连续条件。FlinkCEP支持事件之间如下形式的连续策略：
严格连续: 期望所有匹配的事件严格的一个接一个出现，中间没有任何不匹配的事件。
松散连续: 忽略匹配的事件之间的不匹配的事件。
不确定的松散连续: 更进一步的松散连续，允许忽略掉一些匹配事件的附加匹配。
可以使用下面的方法来指定模式之间的连续策略：
next()，指定严格连续， followedBy()，指定松散连续， followedByAny()，指定不确定的松散连续。 或者
notNext()，如果不想后面直接连着一个特定事件 notFollowedBy()，如果不想一个特定事件发生在两个事件之间的任何地方。 如果模式序列没有定义时间约束，则不能以 notFollowedBy() 结尾。 一个 NOT　模式前面不能是可选的模式。 Java // 严格连续 Pattern\u0026lt;Event, ?\u0026gt; strict = start.next(\u0026#34;middle\u0026#34;).where(...); // 松散连续 Pattern\u0026lt;Event, ?\u0026gt; relaxed = start.followedBy(\u0026#34;middle\u0026#34;).where(...); // 不确定的松散连续 Pattern\u0026lt;Event, ?\u0026gt; nonDetermin = start.followedByAny(\u0026#34;middle\u0026#34;).where(...); // 严格连续的NOT模式 Pattern\u0026lt;Event, ?\u0026gt; strictNot = start.notNext(\u0026#34;not\u0026#34;).where(...); // 松散连续的NOT模式 Pattern\u0026lt;Event, ?\u0026gt; relaxedNot = start.notFollowedBy(\u0026#34;not\u0026#34;).where(...); Scala // 严格连续 val strict: Pattern[Event, _] = start.next(\u0026#34;middle\u0026#34;).where(...) // 松散连续 val relaxed: Pattern[Event, _] = start.followedBy(\u0026#34;middle\u0026#34;).where(...) // 不确定的松散连续 val nonDetermin: Pattern[Event, _] = start.followedByAny(\u0026#34;middle\u0026#34;).where(...) // 严格连续的NOT模式 val strictNot: Pattern[Event, _] = start.notNext(\u0026#34;not\u0026#34;).where(...) // 松散连续的NOT模式 val relaxedNot: Pattern[Event, _] = start.notFollowedBy(\u0026#34;not\u0026#34;).where(...) 松散连续意味着跟着的事件中，只有第一个可匹配的事件会被匹配上，而不确定的松散连接情况下，有着同样起始的多个匹配会被输出。 举例来说，模式\u0026quot;a b\u0026quot;，给定事件序列\u0026quot;a\u0026quot;，\u0026quot;c\u0026quot;，\u0026quot;b1\u0026quot;，\u0026quot;b2\u0026quot;，会产生如下的结果：
\u0026quot;a\u0026quot;和\u0026quot;b\u0026quot;之间严格连续： {} （没有匹配），\u0026quot;a\u0026quot;之后的\u0026quot;c\u0026quot;导致\u0026quot;a\u0026quot;被丢弃。
\u0026quot;a\u0026quot;和\u0026quot;b\u0026quot;之间松散连续： {a b1}，松散连续会\u0026quot;跳过不匹配的事件直到匹配上的事件\u0026quot;。
\u0026quot;a\u0026quot;和\u0026quot;b\u0026quot;之间不确定的松散连续： {a b1}, {a b2}，这是最常见的情况。
也可以为模式定义一个有效时间约束。 例如，你可以通过pattern.within()方法指定一个模式应该在10秒内发生。 这种时间模式支持处理时间和事件时间.
一个模式序列只能有一个时间限制。如果限制了多个时间在不同的单个模式上，会使用最小的那个时间限制。 Java next.within(Time.seconds(10)); Scala next.within(Time.seconds(10)) 注意定义过时间约束的模式允许以 notFollowedBy() 结尾。 例如，可以定义如下的模式:
Java Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).notFollowedBy(\u0026#34;end\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }).within(Time.seconds(10)); Scala Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .notFollowedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;b\u0026#34;) .within(Time.seconds(10)) 循环模式中的连续性 # 你可以在循环模式中使用和前面章节讲过的同样的连续性。 连续性会被运用在被接受进入模式的事件之间。 用这个例子来说明上面所说的连续性，一个模式序列\u0026quot;a b+ c\u0026quot;（\u0026quot;a\u0026quot;后面跟着一个或者多个（不确定连续的）\u0026quot;b\u0026quot;，然后跟着一个\u0026quot;c\u0026quot;） 输入为\u0026quot;a\u0026quot;，\u0026quot;b1\u0026quot;，\u0026quot;d1\u0026quot;，\u0026quot;b2\u0026quot;，\u0026quot;d2\u0026quot;，\u0026quot;b3\u0026quot;，\u0026quot;c\u0026quot;，输出结果如下：
严格连续: {a b1 c}, {a b2 c}, {a b3 c} - 没有相邻的 \u0026quot;b\u0026quot; 。
松散连续: {a b1 c}，{a b1 b2 c}，{a b1 b2 b3 c}，{a b2 c}，{a b2 b3 c}，{a b3 c} - \u0026quot;d\u0026quot;都被忽略了。
不确定松散连续: {a b1 c}，{a b1 b2 c}，{a b1 b3 c}，{a b1 b2 b3 c}，{a b2 c}，{a b2 b3 c}，{a b3 c} - 注意{a b1 b3 c}，这是因为\u0026quot;b\u0026quot;之间是不确定松散连续产生的。
对于循环模式（例如oneOrMore()和times())），默认是松散连续。如果想使用严格连续，你需要使用consecutive()方法明确指定， 如果想使用不确定松散连续，你可以使用allowCombinations()方法。
Java 模式操作 描述 consecutive() 与oneOrMore()和times()一起使用， 在匹配的事件之间施加严格的连续性， 也就是说，任何不匹配的事件都会终止匹配（和next()一样）。
如果不使用它，那么就是松散连续（和followedBy()一样）。
\u0026lt;p\u0026gt;例如，一个如下的模式：\u0026lt;/p\u0026gt; Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;c\u0026#34;); } }) .followedBy(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).oneOrMore().consecutive() .followedBy(\u0026#34;end1\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }); \u0026lt;p\u0026gt;输入：C D A1 A2 A3 D A4 B，会产生下面的输出：\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果施加严格连续性： {C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;不施加严格连续性： {C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;allowCombinations()\u0026lt;/strong\u0026gt;\u0026lt;a name=\u0026quot;allow_comb_java\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;p\u0026gt;与\u0026lt;code\u0026gt;oneOrMore()\u0026lt;/code\u0026gt;和\u0026lt;code\u0026gt;times()\u0026lt;/code\u0026gt;一起使用， 在匹配的事件中间施加不确定松散连续性（和\u0026lt;code\u0026gt;followedByAny()\u0026lt;/code\u0026gt;一样）。\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果不使用，就是松散连续（和\u0026lt;code\u0026gt;followedBy()\u0026lt;/code\u0026gt;一样）。\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;例如，一个如下的模式：\u0026lt;/p\u0026gt; Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;c\u0026#34;); } }) .followedBy(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;a\u0026#34;); } }).oneOrMore().allowCombinations() .followedBy(\u0026#34;end1\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;b\u0026#34;); } }); \u0026lt;p\u0026gt;输入：C D A1 A2 A3 D A4 B，会产生如下的输出：\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果使用不确定松散连续： {C A1 B}，{C A1 A2 B}，{C A1 A3 B}，{C A1 A4 B}，{C A1 A2 A3 B}，{C A1 A2 A4 B}，{C A1 A3 A4 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果不使用：{C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; Scala 模式操作 描述 consecutive() 与oneOrMore()和times()一起使用， 在匹配的事件之间施加严格的连续性， 也就是说，任何不匹配的事件都会终止匹配（和next()一样）。
如果不使用它，那么就是松散连续（和followedBy()一样）。
\u0026lt;p\u0026gt;例如，一个如下的模式：\u0026lt;/p\u0026gt; Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().consecutive() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) \u0026lt;p\u0026gt;输入：C D A1 A2 A3 D A4 B，会产生下面的输出：\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果施加严格连续性： {C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;不施加严格连续性： {C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;allowCombinations()\u0026lt;/strong\u0026gt;\u0026lt;a name=\u0026quot;allow_comb_java\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;p\u0026gt;与\u0026lt;code\u0026gt;oneOrMore()\u0026lt;/code\u0026gt;和\u0026lt;code\u0026gt;times()\u0026lt;/code\u0026gt;一起使用， 在匹配的事件中间施加不确定松散连续性（和\u0026lt;code\u0026gt;followedByAny()\u0026lt;/code\u0026gt;一样）。\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果不使用，就是松散连续（和\u0026lt;code\u0026gt;followedBy()\u0026lt;/code\u0026gt;一样）。\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;例如，一个如下的模式：\u0026lt;/p\u0026gt; Pattern.begin(\u0026#34;start\u0026#34;).where(_.getName().equals(\u0026#34;c\u0026#34;)) .followedBy(\u0026#34;middle\u0026#34;).where(_.getName().equals(\u0026#34;a\u0026#34;)) .oneOrMore().allowCombinations() .followedBy(\u0026#34;end1\u0026#34;).where(_.getName().equals(\u0026#34;b\u0026#34;)) \u0026lt;p\u0026gt;输入：C D A1 A2 A3 D A4 B，会产生如下的输出：\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果使用不确定松散连续： {C A1 B}，{C A1 A2 B}，{C A1 A3 B}，{C A1 A4 B}，{C A1 A2 A3 B}，{C A1 A2 A4 B}，{C A1 A3 A4 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;如果不使用：{C A1 B}，{C A1 A2 B}，{C A1 A2 A3 B}，{C A1 A2 A3 A4 B}\u0026lt;/p\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; 模式组 # 也可以定义一个模式序列作为begin，followedBy，followedByAny和next的条件。这个模式序列在逻辑上会被当作匹配的条件， 并且返回一个GroupPattern，可以在GroupPattern上使用oneOrMore()，times(#ofTimes)， times(#fromTimes, #toTimes)，optional()，consecutive()，allowCombinations()。
Java Pattern\u0026lt;Event, ?\u0026gt; start = Pattern.begin( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;start_middle\u0026#34;).where(...) ); // 严格连续 Pattern\u0026lt;Event, ?\u0026gt; strict = start.next( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;next_start\u0026#34;).where(...).followedBy(\u0026#34;next_middle\u0026#34;).where(...) ).times(3); // 松散连续 Pattern\u0026lt;Event, ?\u0026gt; relaxed = start.followedBy( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;followedby_start\u0026#34;).where(...).followedBy(\u0026#34;followedby_middle\u0026#34;).where(...) ).oneOrMore(); // 不确定松散连续 Pattern\u0026lt;Event, ?\u0026gt; nonDetermin = start.followedByAny( Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;followedbyany_start\u0026#34;).where(...).followedBy(\u0026#34;followedbyany_middle\u0026#34;).where(...) ).optional(); Scala val start: Pattern[Event, _] = Pattern.begin( Pattern.begin[Event](\u0026#34;start\u0026#34;).where(...).followedBy(\u0026#34;start_middle\u0026#34;).where(...) ) // 严格连续 val strict: Pattern[Event, _] = start.next( Pattern.begin[Event](\u0026#34;next_start\u0026#34;).where(...).followedBy(\u0026#34;next_middle\u0026#34;).where(...) ).times(3) // 松散连续 val relaxed: Pattern[Event, _] = start.followedBy( Pattern.begin[Event](\u0026#34;followedby_start\u0026#34;).where(...).followedBy(\u0026#34;followedby_middle\u0026#34;).where(...) ).oneOrMore() // 不确定松散连续 val nonDetermin: Pattern[Event, _] = start.followedByAny( Pattern.begin[Event](\u0026#34;followedbyany_start\u0026#34;).where(...).followedBy(\u0026#34;followedbyany_middle\u0026#34;).where(...) ).optional() Java 模式操作 描述 begin(#name) 定义一个开始的模式：
\`\`\`java Pattern start = Pattern.begin("start"); \`\`\` begin(#pattern_sequence) 定义一个开始的模式：
\`\`\`java Pattern start = Pattern.begin( Pattern.begin("start").where(...).followedBy("middle").where(...) ); \`\`\` next(#name) 增加一个新的模式。匹配的事件必须是直接跟在前面匹配到的事件后面（严格连续）：
\`\`\`java Pattern next = start.next("middle"); \`\`\` next(#pattern_sequence) 增加一个新的模式。匹配的事件序列必须是直接跟在前面匹配到的事件后面（严格连续）：
\`\`\`java Pattern next = start.next( Pattern.begin("start").where(...).followedBy("middle").where(...) ); \`\`\` followedBy(#name) 增加一个新的模式。可以有其他事件出现在匹配的事件和之前匹配到的事件中间（松散连续）：
\`\`\`java Pattern followedBy = start.followedBy("middle"); \`\`\` followedBy(#pattern_sequence) 增加一个新的模式。可以有其他事件出现在匹配的事件序列和之前匹配到的事件中间（松散连续）：
\`\`\`java Pattern followedBy = start.followedBy( Pattern.begin("start").where(...).followedBy("middle").where(...) ); \`\`\` followedByAny(#name) 增加一个新的模式。可以有其他事件出现在匹配的事件和之前匹配到的事件中间， 每个可选的匹配事件都会作为可选的匹配结果输出（不确定的松散连续）：
\`\`\`java Pattern followedByAny = start.followedByAny("middle"); \`\`\` followedByAny(#pattern_sequence) 增加一个新的模式。可以有其他事件出现在匹配的事件序列和之前匹配到的事件中间， 每个可选的匹配事件序列都会作为可选的匹配结果输出（不确定的松散连续）：
\`\`\`java Pattern followedByAny = start.followedByAny( Pattern.begin("start").where(...).followedBy("middle").where(...) ); \`\`\` notNext() 增加一个新的否定模式。匹配的（否定）事件必须直接跟在前面匹配到的事件之后（严格连续）来丢弃这些部分匹配：
\`\`\`java Pattern notNext = start.notNext("not"); \`\`\` notFollowedBy() 增加一个新的否定模式。即使有其他事件在匹配的（否定）事件和之前匹配的事件之间发生， 部分匹配的事件序列也会被丢弃（松散连续）：
\`\`\`java Pattern notFollowedBy = start.notFollowedBy("not"); \`\`\` within(time) 定义匹配模式的事件序列出现的最大时间间隔。如果未完成的事件序列超过了这个事件，就会被丢弃：
\`\`\`java pattern.within(Time.seconds(10)); \`\`\` Scala 模式操作 描述 begin(#name) 定一个开始模式：
\`\`\`scala val start = Pattern.begin[Event]("start") \`\`\` begin(#pattern_sequence) 定一个开始模式：
\`\`\`scala val start = Pattern.begin( Pattern.begin[Event]("start").where(...).followedBy("middle").where(...) ) \`\`\` next(#name) 增加一个新的模式，匹配的事件必须是直接跟在前面匹配到的事件后面（严格连续）：
\`\`\`scala val next = start.next("middle") \`\`\` next(#pattern_sequence) 增加一个新的模式。匹配的事件序列必须是直接跟在前面匹配到的事件后面（严格连续）：
\`\`\`scala val next = start.next( Pattern.begin[Event]("start").where(...).followedBy("middle").where(...) ) \`\`\` followedBy(#name) 增加一个新的模式。可以有其他事件出现在匹配的事件和之前匹配到的事件中间（松散连续）：
\`\`\`scala val followedBy = start.followedBy("middle") \`\`\` followedBy(#pattern_sequence) 增加一个新的模式。可以有其他事件出现在匹配的事件和之前匹配到的事件中间（松散连续）：
\`\`\`scala val followedBy = start.followedBy( Pattern.begin[Event]("start").where(...).followedBy("middle").where(...) ) \`\`\` followedByAny(#name) 增加一个新的模式。可以有其他事件出现在匹配的事件和之前匹配到的事件中间， 每个可选的匹配事件都会作为可选的匹配结果输出（不确定的松散连续）：
\`\`\`scala val followedByAny = start.followedByAny("middle") \`\`\` followedByAny(#pattern_sequence) 增加一个新的模式。可以有其他事件出现在匹配的事件序列和之前匹配到的事件中间， 每个可选的匹配事件序列都会作为可选的匹配结果输出（不确定的松散连续）：
\`\`\`scala val followedByAny = start.followedByAny( Pattern.begin[Event]("start").where(...).followedBy("middle").where(...) ) \`\`\` \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;notNext()\u0026lt;/strong\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;p\u0026gt;增加一个新的否定模式。匹配的（否定）事件必须直接跟在前面匹配到的事件之后 （严格连续）来丢弃这些部分匹配：\u0026lt;/p\u0026gt; val notNext = start.notNext(\u0026#34;not\u0026#34;) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;notFollowedBy()\u0026lt;/strong\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;p\u0026gt;增加一个新的否定模式。即使有其他事件在匹配的（否定）事件和之前匹配的事件之间发生， 部分匹配的事件序列也会被丢弃（松散连续）：\u0026lt;/p\u0026gt; val notFollowedBy = start.notFollowedBy(\u0026#34;not\u0026#34;) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;within(time)\u0026lt;/strong\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;p\u0026gt;定义匹配模式的事件序列出现的最大时间间隔。如果未完成的事件序列超过了这个事件，就会被丢弃：\u0026lt;/p\u0026gt; pattern.within(Time.seconds(10)) \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; 匹配后跳过策略 # 对于一个给定的模式，同一个事件可能会分配到多个成功的匹配上。为了控制一个事件会分配到多少个匹配上，你需要指定跳过策略AfterMatchSkipStrategy。 有五种跳过策略，如下：
NO_SKIP: 每个成功的匹配都会被输出。 SKIP_TO_NEXT: 丢弃以相同事件开始的所有部分匹配。 SKIP_PAST_LAST_EVENT: 丢弃起始在这个匹配的开始和结束之间的所有部分匹配。 SKIP_TO_FIRST: 丢弃起始在这个匹配的开始和第一个出现的名称为PatternName事件之间的所有部分匹配。 SKIP_TO_LAST: 丢弃起始在这个匹配的开始和最后一个出现的名称为PatternName事件之间的所有部分匹配。 注意当使用SKIP_TO_FIRST和SKIP_TO_LAST策略时，需要指定一个合法的PatternName.
例如，给定一个模式b+ c和一个数据流b1 b2 b3 c，不同跳过策略之间的不同如下：
跳过策略 结果 描述 NO_SKIP b1 b2 b3 c
b2 b3 c
b3 c
找到匹配b1 b2 b3 c之后，不会丢弃任何结果。 SKIP_TO_NEXT b1 b2 b3 c
b2 b3 c
b3 c
找到匹配b1 b2 b3 c之后，不会丢弃任何结果，因为没有以b1开始的其他匹配。 SKIP_PAST_LAST_EVENT b1 b2 b3 c
找到匹配b1 b2 b3 c之后，会丢弃其他所有的部分匹配。 SKIP_TO_FIRST[b] b1 b2 b3 c
b2 b3 c
b3 c
找到匹配b1 b2 b3 c之后，会尝试丢弃所有在b1之前开始的部分匹配，但没有这样的匹配，所以没有任何匹配被丢弃。 SKIP_TO_LAST[b] b1 b2 b3 c
b3 c
找到匹配b1 b2 b3 c之后，会尝试丢弃所有在b3之前开始的部分匹配，有一个这样的b2 b3 c被丢弃。 在看另外一个例子来说明NO_SKIP和SKIP_TO_FIRST之间的差别： 模式： (a | b | c) (b | c) c+.greedy d，输入：a b c1 c2 c3 d，结果将会是：
跳过策略 结果 描述 NO_SKIP a b c1 c2 c3 d
b c1 c2 c3 d
c1 c2 c3 d
找到匹配a b c1 c2 c3 d之后，不会丢弃任何结果。 SKIP_TO_FIRST[c*] a b c1 c2 c3 d
c1 c2 c3 d
找到匹配a b c1 c2 c3 d之后，会丢弃所有在c1之前开始的部分匹配，有一个这样的b c1 c2 c3 d被丢弃。 为了更好的理解NO_SKIP和SKIP_TO_NEXT之间的差别，看下面的例子： 模式：a b+，输入：a b1 b2 b3，结果将会是：
跳过策略 结果 描述 NO_SKIP a b1
a b1 b2
a b1 b2 b3
找到匹配a b1之后，不会丢弃任何结果。 SKIP_TO_NEXT a b1
找到匹配a b1之后，会丢弃所有以a开始的部分匹配。这意味着不会产生a b1 b2和a b1 b2 b3了。 想指定要使用的跳过策略，只需要调用下面的方法创建AfterMatchSkipStrategy：
方法 描述 AfterMatchSkipStrategy.noSkip() 创建NO_SKIP策略 AfterMatchSkipStrategy.skipToNext() 创建SKIP_TO_NEXT策略 AfterMatchSkipStrategy.skipPastLastEvent() 创建SKIP_PAST_LAST_EVENT策略 AfterMatchSkipStrategy.skipToFirst(patternName) 创建引用模式名称为patternName的SKIP_TO_FIRST策略 AfterMatchSkipStrategy.skipToLast(patternName) 创建引用模式名称为patternName的SKIP_TO_LAST策略 可以通过调用下面方法将跳过策略应用到模式上：
Java AfterMatchSkipStrategy skipStrategy = ...; Pattern.begin(\u0026#34;patternName\u0026#34;, skipStrategy); Scala val skipStrategy = ... Pattern.begin(\u0026#34;patternName\u0026#34;, skipStrategy) 使用SKIP_TO_FIRST/LAST时，有两个选项可以用来处理没有事件可以映射到对应模式名上的情况。 默认情况下会使用NO_SKIP策略，另外一个选项是抛出异常。 可以使用如下的选项： Java AfterMatchSkipStrategy.skipToFirst(patternName).throwExceptionOnMiss(); Scala AfterMatchSkipStrategy.skipToFirst(patternName).throwExceptionOnMiss() 检测模式 # 在指定了要寻找的模式后，该把它们应用到输入流上来发现可能的匹配了。为了在事件流上运行你的模式，需要创建一个PatternStream。 给定一个输入流input，一个模式pattern和一个可选的用来对使用事件时间时有同样时间戳或者同时到达的事件进行排序的比较器comparator， 你可以通过调用如下方法来创建PatternStream：
Java DataStream\u0026lt;Event\u0026gt; input = ...; Pattern\u0026lt;Event, ?\u0026gt; pattern = ...; EventComparator\u0026lt;Event\u0026gt; comparator = ...; // 可选的 PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern, comparator); Scala val input : DataStream[Event] = ... val pattern : Pattern[Event, _] = ... var comparator : EventComparator[Event] = ... // 可选的 val patternStream: PatternStream[Event] = CEP.pattern(input, pattern, comparator) 输入流根据你的使用场景可以是keyed或者non-keyed。
在 non-keyed 流上使用模式将会使你的作业并发度被设为1。 从模式中选取 # 在获得到一个PatternStream之后，你可以应用各种转换来发现事件序列。推荐使用PatternProcessFunction。
PatternProcessFunction有一个processMatch的方法在每找到一个匹配的事件序列时都会被调用。 它按照Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt;的格式接收一个匹配，映射的键是你的模式序列中的每个模式的名称，值是被接受的事件列表（IN是输入事件的类型）。 模式的输入事件按照时间戳进行排序。为每个模式返回一个接受的事件列表的原因是当使用循环模式（比如oneToMany()和times()）时， 对一个模式会有不止一个事件被接受。
class MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); IN endEvent = match.get(\u0026#34;end\u0026#34;).get(0); out.collect(OUT(startEvent, endEvent)); } } PatternProcessFunction可以访问Context对象。有了它之后，你可以访问时间属性，比如currentProcessingTime或者当前匹配的timestamp （最新分配到匹配上的事件的时间戳）。 更多信息可以看时间上下文。 通过这个上下文也可以将结果输出到侧输出.
处理超时的部分匹配 # 当一个模式上通过within加上窗口长度后，部分匹配的事件序列就可能因为超过窗口长度而被丢弃。可以使用TimedOutPartialMatchHandler接口 来处理超时的部分匹配。这个接口可以和其它的混合使用。也就是说你可以在自己的PatternProcessFunction里另外实现这个接口。 TimedOutPartialMatchHandler提供了另外的processTimedOutMatch方法，这个方法对每个超时的部分匹配都会调用。
class MyPatternProcessFunction\u0026lt;IN, OUT\u0026gt; extends PatternProcessFunction\u0026lt;IN, OUT\u0026gt; implements TimedOutPartialMatchHandler\u0026lt;IN\u0026gt; { @Override public void processMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; ... } @Override public void processTimedOutMatch(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; match, Context ctx) throws Exception; IN startEvent = match.get(\u0026#34;start\u0026#34;).get(0); ctx.output(outputTag, T(startEvent)); } } Note processTimedOutMatch不能访问主输出。 但你可以通过Context对象把结果输出到侧输出。
便捷的API # 前面提到的PatternProcessFunction是在Flink 1.8之后引入的，从那之后推荐使用这个接口来处理匹配到的结果。 用户仍然可以使用像select/flatSelect这样旧格式的API，它们会在内部被转换为PatternProcessFunction。
Java PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;ComplexEvent\u0026gt; flatResult = patternStream.flatSelect( outputTag, new PatternFlatTimeoutFunction\u0026lt;Event, TimeoutEvent\u0026gt;() { public void timeout( Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern, long timeoutTimestamp, Collector\u0026lt;TimeoutEvent\u0026gt; out) throws Exception { out.collect(new TimeoutEvent()); } }, new PatternFlatSelectFunction\u0026lt;Event, ComplexEvent\u0026gt;() { public void flatSelect(Map\u0026lt;String, List\u0026lt;IN\u0026gt;\u0026gt; pattern, Collector\u0026lt;OUT\u0026gt; out) throws Exception { out.collect(new ComplexEvent()); } } ); DataStream\u0026lt;TimeoutEvent\u0026gt; timeoutFlatResult = flatResult.getSideOutput(outputTag); Scala val patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream.flatSelect(outputTag){ (pattern: Map[String, Iterable[Event]], timestamp: Long, out: Collector[TimeoutEvent]) =\u0026gt; out.collect(TimeoutEvent()) } { (pattern: mutable.Map[String, Iterable[Event]], out: Collector[ComplexEvent]) =\u0026gt; out.collect(ComplexEvent()) } val timeoutResult: DataStream[TimeoutEvent] = result.getSideOutput(outputTag) CEP库中的时间 # 按照事件时间处理迟到事件 # 在CEP中，事件的处理顺序很重要。在使用事件时间时，为了保证事件按照正确的顺序被处理，一个事件到来后会先被放到一个缓冲区中， 在缓冲区里事件都按照时间戳从小到大排序，当水位线到达后，缓冲区中所有小于水位线的事件被处理。这意味着水位线之间的数据都按照时间戳被顺序处理。
这个库假定按照事件时间时水位线一定是正确的。 为了保证跨水位线的事件按照事件时间处理，Flink CEP库假定水位线一定是正确的，并且把时间戳小于最新水位线的事件看作是晚到的。 晚到的事件不会被处理。你也可以指定一个侧输出标志来收集比最新水位线晚到的事件，你可以这样做：
Java PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(input, pattern); OutputTag\u0026lt;String\u0026gt; lateDataOutputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;late-data\u0026#34;){}; SingleOutputStreamOperator\u0026lt;ComplexEvent\u0026gt; result = patternStream .sideOutputLateData(lateDataOutputTag) .select( new PatternSelectFunction\u0026lt;Event, ComplexEvent\u0026gt;() {...} ); DataStream\u0026lt;String\u0026gt; lateData = result.getSideOutput(lateDataOutputTag); Scala val patternStream: PatternStream[Event] = CEP.pattern(input, pattern) val lateDataOutputTag = OutputTag[String](\u0026#34;late-data\u0026#34;) val result: SingleOutputStreamOperator[ComplexEvent] = patternStream .sideOutputLateData(lateDataOutputTag) .select{ pattern: Map[String, Iterable[ComplexEvent]] =\u0026gt; ComplexEvent() } val lateData: DataStream[String] = result.getSideOutput(lateDataOutputTag) 时间上下文 # 在PatternProcessFunction中，用户可以和IterativeCondition中 一样按照下面的方法使用实现了TimeContext的上下文：
/** * 支持获取事件属性比如当前处理事件或当前正处理的事件的时间。 * 用在{@link PatternProcessFunction}和{@link org.apache.flink.cep.pattern.conditions.IterativeCondition}中 */ @PublicEvolving public interface TimeContext { /** * 当前正处理的事件的时间戳。 * * \u0026lt;p\u0026gt;如果是{@link org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime}，这个值会被设置为事件进入CEP算子的时间。 */ long timestamp(); /** 返回当前的处理时间。 */ long currentProcessingTime(); } 这个上下文让用户可以获取处理的事件（在IterativeCondition时候是进来的记录，在PatternProcessFunction是匹配的结果）的时间属性。 调用TimeContext#currentProcessingTime总是返回当前的处理时间，而且尽量去调用这个函数而不是调用其它的比如说System.currentTimeMillis()。
使用EventTime时，TimeContext#timestamp()返回的值等于分配的时间戳。 使用ProcessingTime时，这个值等于事件进入CEP算子的时间点（在PatternProcessFunction中是匹配产生的时间）。 这意味着多次调用这个方法得到的值是一致的。
可选的参数设置 # 用于配置 Flink CEP 的 SharedBuffer 缓存容量的选项。它可以加快 CEP 算子的处理速度，并限制内存中缓存的元素数量。
Note 仅当 state.backend 设置为 rocksdb 时限制内存使用才有效，这会将超过缓存数量的元素传输到 rocksdb 状态存储而不是内存状态存储。当 state.backend 设置为 rocksdb 时，这些配置项有助于限制内存。相比之下，当 state.backend 设置为非 rocksdb 时，缓存会导致性能下降。与使用 Map 实现的旧缓存相比，状态部分将包含更多从 guava-cache 换出的元素，这将使得 copy on write 时的状态处理增加一些开销。
Key Default Type Description pipeline.cep.sharedbuffer.cache.entry-slots 1024 Integer The Config option to set the maximum element number the entryCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed with state.And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage. pipeline.cep.sharedbuffer.cache.event-slots 1024 Integer The Config option to set the maximum element number the eventsBufferCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage. pipeline.cep.sharedbuffer.cache.statistics-interval 30 min Duration The interval to log the information of cache state statistics in CEP operator. 例子 # 下面的例子在一个分片的Events流上检测模式start, middle(name = \u0026quot;error\u0026quot;) -\u0026gt; end(name = \u0026quot;critical\u0026quot;)。 事件按照id分片，一个有效的模式需要发生在10秒内。
Java StreamExecutionEnvironment env = ...; DataStream\u0026lt;Event\u0026gt; input = ...; DataStream\u0026lt;Event\u0026gt; partitionedInput = input.keyBy(new KeySelector\u0026lt;Event, Integer\u0026gt;() { @Override public Integer getKey(Event value) throws Exception { return value.getId(); } }); Pattern\u0026lt;Event, ?\u0026gt; pattern = Pattern.\u0026lt;Event\u0026gt;begin(\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;error\u0026#34;); } }).followedBy(\u0026#34;end\u0026#34;).where(new SimpleCondition\u0026lt;Event\u0026gt;() { @Override public boolean filter(Event value) throws Exception { return value.getName().equals(\u0026#34;critical\u0026#34;); } }).within(Time.seconds(10)); PatternStream\u0026lt;Event\u0026gt; patternStream = CEP.pattern(partitionedInput, pattern); DataStream\u0026lt;Alert\u0026gt; alerts = patternStream.select(new PatternSelectFunction\u0026lt;Event, Alert\u0026gt;() { @Override public Alert select(Map\u0026lt;String, List\u0026lt;Event\u0026gt;\u0026gt; pattern) throws Exception { return createAlert(pattern); } }); Scala val env : StreamExecutionEnvironment = ... val input : DataStream[Event] = ... val partitionedInput = input.keyBy(event =\u0026gt; event.getId) val pattern = Pattern.begin[Event](\u0026#34;start\u0026#34;) .next(\u0026#34;middle\u0026#34;).where(_.getName == \u0026#34;error\u0026#34;) .followedBy(\u0026#34;end\u0026#34;).where(_.getName == \u0026#34;critical\u0026#34;) .within(Time.seconds(10)) val patternStream = CEP.pattern(partitionedInput, pattern) val alerts = patternStream.select(createAlert(_)) 从旧版本迁移（1.5之前） # Migrating from Flink \u0026lt;= 1.5 # In Flink 1.13 we dropped direct savepoint backward compatibility with Flink \u0026lt;= 1.5. If you want to restore from a savepoint taken from an older version, migrate it first to a newer version (1.6-1.12), take a savepoint and then use that savepoint to restore with Flink \u0026gt;= 1.13.
Back to top
`}),e.add({id:61,href:"/flink/flink-docs-master/zh/docs/concepts/stateful-stream-processing/",title:"有状态流处理",section:"概念透析",content:` 有状态流处理 # What is State? # While many operations in a dataflow simply look at one individual event at a time (for example an event parser), some operations remember information across multiple events (for example window operators). These operations are called stateful.
Some examples of stateful operations:
When an application searches for certain event patterns, the state will store the sequence of events encountered so far. When aggregating events per minute/hour/day, the state holds the pending aggregates. When training a machine learning model over a stream of data points, the state holds the current version of the model parameters. When historic data needs to be managed, the state allows efficient access to events that occurred in the past. Flink needs to be aware of the state in order to make it fault tolerant using checkpoints and savepoints.
Knowledge about the state also allows for rescaling Flink applications, meaning that Flink takes care of redistributing state across parallel instances.
Queryable state allows you to access state from outside of Flink during runtime.
When working with state, it might also be useful to read about Flink\u0026rsquo;s state backends. Flink provides different state backends that specify how and where state is stored.
Back to top
Keyed State # Keyed state is maintained in what can be thought of as an embedded key/value store. The state is partitioned and distributed strictly together with the streams that are read by the stateful operators. Hence, access to the key/value state is only possible on keyed streams, i.e. after a keyed/partitioned data exchange, and is restricted to the values associated with the current event\u0026rsquo;s key. Aligning the keys of streams and state makes sure that all state updates are local operations, guaranteeing consistency without transaction overhead. This alignment also allows Flink to redistribute the state and adjust the stream partitioning transparently.
Keyed State is further organized into so-called Key Groups. Key Groups are the atomic unit by which Flink can redistribute Keyed State; there are exactly as many Key Groups as the defined maximum parallelism. During execution each parallel instance of a keyed operator works with the keys for one or more Key Groups.
State Persistence # Flink implements fault tolerance using a combination of stream replay and checkpointing. A checkpoint marks a specific point in each of the input streams along with the corresponding state for each of the operators. A streaming dataflow can be resumed from a checkpoint while maintaining consistency (exactly-once processing semantics) by restoring the state of the operators and replaying the records from the point of the checkpoint.
The checkpoint interval is a means of trading off the overhead of fault tolerance during execution with the recovery time (the number of records that need to be replayed).
The fault tolerance mechanism continuously draws snapshots of the distributed streaming data flow. For streaming applications with small state, these snapshots are very light-weight and can be drawn frequently without much impact on performance. The state of the streaming applications is stored at a configurable place, usually in a distributed file system.
In case of a program failure (due to machine-, network-, or software failure), Flink stops the distributed streaming dataflow. The system then restarts the operators and resets them to the latest successful checkpoint. The input streams are reset to the point of the state snapshot. Any records that are processed as part of the restarted parallel dataflow are guaranteed to not have affected the previously checkpointed state.
By default, checkpointing is disabled. See Checkpointing for details on how to enable and configure checkpointing. For this mechanism to realize its full guarantees, the data stream source (such as message queue or broker) needs to be able to rewind the stream to a defined recent point. Apache Kafka has this ability and Flink\u0026rsquo;s connector to Kafka exploits this. See Fault Tolerance Guarantees of Data Sources and Sinks for more information about the guarantees provided by Flink\u0026rsquo;s connectors. Because Flink\u0026rsquo;s checkpoints are realized through distributed snapshots, we use the words snapshot and checkpoint interchangeably. Often we also use the term snapshot to mean either checkpoint or savepoint. Checkpointing # The central part of Flink\u0026rsquo;s fault tolerance mechanism is drawing consistent snapshots of the distributed data stream and operator state. These snapshots act as consistent checkpoints to which the system can fall back in case of a failure. Flink\u0026rsquo;s mechanism for drawing these snapshots is described in \u0026ldquo;Lightweight Asynchronous Snapshots for Distributed Dataflows\u0026rdquo;. It is inspired by the standard Chandy-Lamport algorithm for distributed snapshots and is specifically tailored to Flink\u0026rsquo;s execution model.
Keep in mind that everything to do with checkpointing can be done asynchronously. The checkpoint barriers don\u0026rsquo;t travel in lock step and operations can asynchronously snapshot their state.
Since Flink 1.11, checkpoints can be taken with or without alignment. In this section, we describe aligned checkpoints first.
Barriers # A core element in Flink\u0026rsquo;s distributed snapshotting are the stream barriers. These barriers are injected into the data stream and flow with the records as part of the data stream. Barriers never overtake records, they flow strictly in line. A barrier separates the records in the data stream into the set of records that goes into the current snapshot, and the records that go into the next snapshot. Each barrier carries the ID of the snapshot whose records it pushed in front of it. Barriers do not interrupt the flow of the stream and are hence very lightweight. Multiple barriers from different snapshots can be in the stream at the same time, which means that various snapshots may happen concurrently.
Stream barriers are injected into the parallel data flow at the stream sources. The point where the barriers for snapshot n are injected (let\u0026rsquo;s call it Sn) is the position in the source stream up to which the snapshot covers the data. For example, in Apache Kafka, this position would be the last record\u0026rsquo;s offset in the partition. This position Sn is reported to the checkpoint coordinator (Flink\u0026rsquo;s JobManager).
The barriers then flow downstream. When an intermediate operator has received a barrier for snapshot n from all of its input streams, it emits a barrier for snapshot n into all of its outgoing streams. Once a sink operator (the end of a streaming DAG) has received the barrier n from all of its input streams, it acknowledges that snapshot n to the checkpoint coordinator. After all sinks have acknowledged a snapshot, it is considered completed.
Once snapshot n has been completed, the job will never again ask the source for records from before Sn, since at that point these records (and their descendant records) will have passed through the entire data flow topology.
Operators that receive more than one input stream need to align the input streams on the snapshot barriers. The figure above illustrates this:
As soon as the operator receives snapshot barrier n from an incoming stream, it cannot process any further records from that stream until it has received the barrier n from the other inputs as well. Otherwise, it would mix records that belong to snapshot n and with records that belong to snapshot n+1. Once the last stream has received barrier n, the operator emits all pending outgoing records, and then emits snapshot n barriers itself. It snapshots the state and resumes processing records from all input streams, processing records from the input buffers before processing the records from the streams. Finally, the operator writes the state asynchronously to the state backend. Note that the alignment is needed for all operators with multiple inputs and for operators after a shuffle when they consume output streams of multiple upstream subtasks.
Snapshotting Operator State # When operators contain any form of state, this state must be part of the snapshots as well.
Operators snapshot their state at the point in time when they have received all snapshot barriers from their input streams, and before emitting the barriers to their output streams. At that point, all updates to the state from records before the barriers have been made, and no updates that depend on records from after the barriers have been applied. Because the state of a snapshot may be large, it is stored in a configurable state backend. By default, this is the JobManager\u0026rsquo;s memory, but for production use a distributed reliable storage should be configured (such as HDFS). After the state has been stored, the operator acknowledges the checkpoint, emits the snapshot barrier into the output streams, and proceeds.
The resulting snapshot now contains:
For each parallel stream data source, the offset/position in the stream when the snapshot was started For each operator, a pointer to the state that was stored as part of the snapshot Recovery # Recovery under this mechanism is straightforward: Upon a failure, Flink selects the latest completed checkpoint k. The system then re-deploys the entire distributed dataflow, and gives each operator the state that was snapshotted as part of checkpoint k. The sources are set to start reading the stream from position Sk. For example in Apache Kafka, that means telling the consumer to start fetching from offset Sk.
If state was snapshotted incrementally, the operators start with the state of the latest full snapshot and then apply a series of incremental snapshot updates to that state.
See Restart Strategies for more information.
Unaligned Checkpointing # Checkpointing can also be performed unaligned. The basic idea is that checkpoints can overtake all in-flight data as long as the in-flight data becomes part of the operator state.
Note that this approach is actually closer to the Chandy-Lamport algorithm , but Flink still inserts the barrier in the sources to avoid overloading the checkpoint coordinator.
The figure depicts how an operator handles unaligned checkpoint barriers:
The operator reacts on the first barrier that is stored in its input buffers. It immediately forwards the barrier to the downstream operator by adding it to the end of the output buffers. The operator marks all overtaken records to be stored asynchronously and creates a snapshot of its own state. Consequently, the operator only briefly stops the processing of input to mark the buffers, forwards the barrier, and creates the snapshot of the other state.
Unaligned checkpointing ensures that barriers are arriving at the sink as fast as possible. It\u0026rsquo;s especially suited for applications with at least one slow moving data path, where alignment times can reach hours. However, since it\u0026rsquo;s adding additional I/O pressure, it doesn\u0026rsquo;t help when the I/O to the state backends is the bottleneck. See the more in-depth discussion in ops for other limitations.
Note that savepoints will always be aligned.
Unaligned Recovery # Operators first recover the in-flight data before starting processing any data from upstream operators in unaligned checkpointing. Aside from that, it performs the same steps as during recovery of aligned checkpoints.
State Backends # The exact data structures in which the key/values indexes are stored depends on the chosen state backend. One state backend stores data in an in-memory hash map, another state backend uses RocksDB as the key/value store. In addition to defining the data structure that holds the state, the state backends also implement the logic to take a point-in-time snapshot of the key/value state and store that snapshot as part of a checkpoint. State backends can be configured without changing your application logic.
Back to top
Savepoints # All programs that use checkpointing can resume execution from a savepoint. Savepoints allow both updating your programs and your Flink cluster without losing any state.
Savepoints are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend. They rely on the regular checkpointing mechanism for this.
Savepoints are similar to checkpoints except that they are triggered by the user and don\u0026rsquo;t automatically expire when newer checkpoints are completed. 为了正确使用savepoints，了解 checkpoints 与 savepoints 之间的区别非常重要，checkpoints 与 savepoints 中对此进行了描述。
Back to top
Exactly Once vs. At Least Once # The alignment step may add latency to the streaming program. Usually, this extra latency is on the order of a few milliseconds, but we have seen cases where the latency of some outliers increased noticeably. For applications that require consistently super low latencies (few milliseconds) for all records, Flink has a switch to skip the stream alignment during a checkpoint. Checkpoint snapshots are still drawn as soon as an operator has seen the checkpoint barrier from each input.
When the alignment is skipped, an operator keeps processing all inputs, even after some checkpoint barriers for checkpoint n arrived. That way, the operator also processes elements that belong to checkpoint n+1 before the state snapshot for checkpoint n was taken. On a restore, these records will occur as duplicates, because they are both included in the state snapshot of checkpoint n, and will be replayed as part of the data after checkpoint n.
Alignment happens only for operators with multiple predecessors (joins) as well as operators with multiple senders (after a stream repartitioning/shuffle). Because of that, dataflows with only embarrassingly parallel streaming operations (map(), flatMap(), filter(), \u0026hellip;) actually give exactly once guarantees even in at least once mode. Back to top
State and Fault Tolerance in Batch Programs # Flink executes batch programs as a special case of streaming programs, where the streams are bounded (finite number of elements). A DataSet is treated internally as a stream of data. The concepts above thus apply to batch programs in the same way as well as they apply to streaming programs, with minor exceptions:
Fault tolerance for batch programs does not use checkpointing. Recovery happens by fully replaying the streams. That is possible, because inputs are bounded. This pushes the cost more towards the recovery, but makes the regular processing cheaper, because it avoids checkpoints.
Stateful operations in the DataSet API use simplified in-memory/out-of-core data structures, rather than key/value indexes.
The DataSet API introduces special synchronized (superstep-based) iterations, which are only possible on bounded streams. For details, check out the iteration docs.
Back to top
`}),e.add({id:62,href:"/flink/flink-docs-master/zh/docs/dev/datastream/execution_mode/",title:"执行模式（流/批）",section:"DataStream API",content:` 执行模式（流/批） # DataStream API 支持不同的运行时执行模式，你可以根据你的用例需要和作业特点进行选择。
DataStream API 有一种”经典“的执行行为，我们称之为流（STREAMING）执行模式。这种模式适用于需要连续增量处理，而且预计无限期保持在线的无边界作业。
此外，还有一种批式执行模式，我们称之为批（BATCH）执行模式。这种执行作业的方式更容易让人联想到批处理框架，比如 MapReduce。这种执行模式适用于有一个已知的固定输入，而且不会连续运行的有边界作业。
Apache Flink 对流处理和批处理统一方法，意味着无论配置何种执行模式，在有界输入上执行的 DataStream 应用都会产生相同的最终 结果。重要的是要注意最终 在这里是什么意思：一个在流模式执行的作业可能会产生增量更新（想想数据库中的插入（upsert）操作），而批作业只在最后产生一个最终结果。尽管计算方法不同，只要呈现方式得当，最终结果会是相同的。
通过启用批执行，我们允许 Flink 应用只有在我们知道输入是有边界的时侯才会使用到的额外的优化。例如，可以使用不同的关联（join）/ 聚合（aggregation）策略，允许实现更高效的任务调度和故障恢复行为的不同 shuffle。下面我们将介绍一些执行行为的细节。
什么时候可以/应该使用批执行模式？ # 批执行模式只能用于 有边界 的作业/Flink 程序。边界是数据源的一个属性，告诉我们在执行前，来自该数据源的所有输入是否都是已知的，或者是否会有新的数据出现，可能是无限的。而对一个作业来说，如果它的所有源都是有边界的，则它就是有边界的，否则就是无边界的。
而流执行模式，既可用于有边界任务，也可用于无边界任务。
一般来说，在你的程序是有边界的时候，你应该使用批执行模式，因为这样做会更高效。当你的程序是无边界的时候，你必须使用流执行模式，因为只有这种模式足够通用，能够处理连续的数据流。
一个明显的例外是当你想使用一个有边界作业去自展一些作业状态，并将状态使用在之后的无边界作业的时候。例如，通过流模式运行一个有边界作业，取一个 savepoint，然后在一个无边界作业上恢复这个 savepoint。这是一个非常特殊的用例，当我们允许将 savepoint 作为批执行作业的附加输出时，这个用例可能很快就会过时。
另一个你可能会使用流模式运行有边界作业的情况是当你为最终会在无边界数据源写测试代码的时候。对于测试来说，在这些情况下使用有边界数据源可能更自然。
配置批执行模式 # 执行模式可以通过 execute.runtime-mode 设置来配置。有三种可选的值：
STREAMING: 经典 DataStream 执行模式（默认) BATCH: 在 DataStream API 上进行批量式执行 AUTOMATIC: 让系统根据数据源的边界性来决定 这可以通过 bin/flink run ... 的命令行参数进行配置，或者在创建/配置 StreamExecutionEnvironment 时写进程序。
下面是如何通过命令行配置执行模式：
\$ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar 这个例子展示了如何在代码中配置执行模式：
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); 我们不建议用户在程序中设置运行模式，而是在提交应用程序时使用命令行进行设置。保持应用程序代码的免配置可以让程序更加灵活，因为同一个应用程序可能在任何执行模式下执行。 执行行为 # 本节概述了批执行模式的执行行为，并与流执行模式进行了对比。详细内容请参考介绍该功能的 FLIP-134 和 FLIP-140.
任务调度与网络 Shuffle # Flink 作业由不同的操作组成，这些操作在数据流图中连接在一起。系统决定如何在不同的进程/机器（TaskManager）上调度这些操作的执行，以及如何在它们之间 shuffle （发送）数据。
将多个操作/算子链接在一起的功能称为链。Flink 称一个调度单位的一组或多个（链接在一起的）算子为一个 任务。通常，子任务 用来指代在多个 TaskManager 上并行运行的单个任务实例，但我们在这里只使用 任务(task)一词。
任务调度和网络 shuffle 对于批和流执行模式的执行方式不同。这主要是由于在批执行模式中，当知道输入数据是有边界的时候，Flink可以使用更高效的数据结构和算法。
我们将用这个例子来解释任务调度和网络传输的差异。
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource\u0026lt;String\u0026gt; source = env.fromElements(...); source.name(\u0026#34;source\u0026#34;) .map(...).name(\u0026#34;map1\u0026#34;) .map(...).name(\u0026#34;map2\u0026#34;) .rebalance() .map(...).name(\u0026#34;map3\u0026#34;) .map(...).name(\u0026#34;map4\u0026#34;) .keyBy((value) -\u0026gt; value) .map(...).name(\u0026#34;map5\u0026#34;) .map(...).name(\u0026#34;map6\u0026#34;) .sinkTo(...).name(\u0026#34;sink\u0026#34;); 包含 1-to-1 连接模式的操作，比如 map()、 flatMap() 或 filter()，可以直接将数据转发到下一个操作，这使得这些操作可以被链接在一起。这意味着 Flink 一般不会在他们之间插入网络 shuffle。
而像 keyBy() 或者 rebalance() 这样需要在不同的任务并行实例之间进行数据 shuffle 的操作，就会引起网络 shuffle。
对于上面的例子，Flink 会将操作分组为这些任务:
任务1: source、 map1 和 map2 任务2: map3 和 map4 任务3: map5 、 map6 和 sink 我们在任务1到任务2、任务2到任务3之间各有一次网络 shuffle。这是该作业的可视化表示：
流执行模式 # 在流执行模式下，所有任务需要一直在线/运行。这使得 Flink可以通过整个管道立即处理新的记录，以达到我们需要的连续和低延迟的流处理。这同样意味着分配给某个作业的 TaskManagers 需要有足够的资源来同时运行所有的任务。
网络 shuffle 是 流水线 式的，这意味着记录会立即发送给下游任务，在网络层上进行一些缓冲。同样，这也是必须的，因为当处理连续的数据流时，在任务（或任务管道）之间没有可以实体化的自然数据点（时间点）。这与批执行模式形成了鲜明的对比，在批执行模式下，中间的结果可以被实体化，如下所述。
批执行模式 # 在批执行模式下，一个作业的任务可以被分离成可以一个接一个执行的阶段。我们之所以能做到这一点，是因为输入是有边界的，因此 Flink 可以在进入下一个阶段之前完全处理管道中的一个阶段。在上面的例子中，工作会有三个阶段，对应着被 shuffle 界线分开的三个任务。
不同于上文所介绍的流模式立即向下游任务发送记录，分阶段处理要求 Flink 将任务的中间结果实体化到一些非永久存储中，让下游任务在上游任务已经下线后再读取。这将增加处理的延迟，但也会带来其他有趣的特性。其一，这允许 Flink 在故障发生时回溯到最新的可用结果，而不是重新启动整个任务。其二，批作业可以在更少的资源上执行（就 TaskManagers 的可用槽而言），因为系统可以一个接一个地顺序执行任务。
TaskManagers 将至少在下游任务开始消费它们前保留中间结果（从技术上讲，它们将被保留到消费的流水线区域产生它们的输出为止）。在这之后，只要空间允许，它们就会被保留，以便在失败的情况下，可以回溯到前面涉及的结果。
State Backends / State # 在流模式下，Flink 使用 StateBackend 来控制状态的存储方式和检查点的工作方式。
在批模式下，配置的 state backend 被忽略。取而代之的是，keyed 操作的输入按键分组（使用排序），然后我们依次处理一个键的所有记录。这样就可以在同一时间只保留一个键的状态。当进行到下一个键时，一个给定键的状态将被丢弃。
关于这方面的背景信息，请参见 FLIP-140。
处理顺序 # 在批执行和流执行中，算子或用户自定义函数（UDFs）处理记录的顺序可能不同。
在流模式下，用户自定义函数不应该对传入记录的顺序做任何假设。数据一到达就被处理。
在批执行模式下，Flink 通过一些操作确保顺序。排序可以是特定调度任务、网络 shuffle、上文提到的 state backend 或是系统有意识选择的副作用。
我们可以将常见输入类型分为三类：
广播输入（broadcast input)： 从广播流输入（参见 广播状态（Broadcast State）) 常规输入（regular input）： 从广播或 keyed 输入 keyed 输入（keyed input）： 从 KeyedStream 输入 消费多种类型输入的函数或是算子可以使用以下顺序处理：
广播输入第一个处理 常规输入第二个处理 keyed 输入最后处理 对于从多个常规或广播输入进行消费的函数 — 比如 CoProcessFunction — Flink 有权从任一输入以任意顺序处理数据。
对于从多个keyed输入进行消费的函数 — 比如 KeyedCoProcessFunction — Flink 先处理单一键中的所有记录再处理下一个。
事件时间/水印 # 在支持事件时间方面，Flink 的流运行时间建立在一个事件可能是乱序到来的悲观假设上的，即一个时间戳 t 的事件可能会在一个时间戳 t+1 的事件之后出现。因为如此，系统永远无法确定在给定的时间戳 T 下，未来不会再有时间戳 t \u0026lt; T 的元素出现。为了摊平这种失序性对最终结果的影响，同时使系统实用，在流模式下，Flink 使用了一种名为 Watermarks 的启发式方法。一个带有时间戳 T 的水印标志着再没有时间戳 t \u0026lt; T 的元素跟进。
在批模式下，输入的数据集是事先已知的，不需要这样的启发式方法，因为至少可以按照时间戳对元素进行排序，从而按照时间顺序进行处理。对于熟悉流的读者来说，在批中，我们可以假设”完美的 Watermark“。
综上所述，在批模式下，我们只需要在输入的末尾有一个与每个键相关的 MAX_WATERMARK，如果输入流没有键，则在输入的末尾需要一个 MAX_WATERMARK。基于这个方案，所有注册的定时器都会在时间结束时触发，用户定义的 WatermarkAssigners 或 WatermarkStrategies 会被忽略。但细化一个 WatermarkStrategy 仍然是重要的，因为它的 TimestampAssigner 仍然会被用来给记录分配时间戳。
处理时间 # 处理时间是指在处理记录的具体实例上，处理记录的机器上的挂钟时间。根据这个定义，我们知道基于处理时间的计算结果是不可重复的。因为同一条记录被处理两次，会有两个不同的时间戳。
尽管如此，在流模式下处理时间还是很有用的。原因在于因为流式管道从 真实时间 摄取无边界输入，所以事件时间和处理时间之间存在相关性。此外，由于上述原因，在流模式下事件时间的1小时也往往可以几乎是处理时间，或者叫挂钟时间的1小时。所以使用处理时间可以用于早期（不完全）触发，给出预期结果的提示。
在批处理世界中，这种相关性并不存在，因为在批处理世界中，输入的数据集是静态的，是预先知道的。鉴于此，在批模式中，我们允许用户请求当前的处理时间，并注册处理时间计时器，但与事件时间的情况一样，所有的计时器都要在输入结束时触发。
在概念上，我们可以想象，在作业执行过程中，处理时间不会提前，当整个输入处理完毕后，我们会快进到时间结束。
故障恢复 # 在流执行模式下，Flink 使用 checkpoints 进行故障恢复。请参看 checkpointing 文档，了解关于如何实践和配置它。关于通过状态快照进行容错，也有一个比较入门的章节，从更高的层面解释了这些概念。
Checkpointing 用于故障恢复的特点之一是，在发生故障时，Flink 会从 checkpoint 重新启动所有正在运行的任务。这可能比我们在批模式下所要做的事情代价更高（如下文所解释），这也是如果你的任务允许的话应该使用批执行模式的原因之一。
在批执行模式下，Flink 会尝试并回溯到之前的中间结果仍可获取的处理阶段。只有失败的任务（或它们在图中的前辈）才可能需要重新启动。这与从 checkpoint 重新启动所有任务相比，可以提高作业的处理效率和整体处理时间。
重要的考虑因素 # 与经典的流执行模式相比，在批模式下，有些东西可能无法按照预期工作。一些功能的工作方式会略有不同，而其他功能会不支持。
批模式下的行为变化：
“滚动\u0026quot;操作，如 reduce() 或 sum()，会对流模式下每一条新记录发出增量更新。在批模式下，这些操作不是\u0026quot;滚动”。它们只发出最终结果。 批模式下不支持的:
Checkpointing 和任何依赖于 checkpointing 的操作都不支持。 迭代（Iterations） 自定义算子应谨慎执行，否则可能会有不恰当的行为。更多细节请参见下面的补充说明。
Checkpointing # 如上文所述，批处理程序的故障恢复不使用检查点。
重要的是要记住，因为没有 checkpoints，某些功能如 ( CheckpointListener )，以及因此，Kafka 的 精确一次（EXACTLY_ONCE） 模式或 File Sink 的 OnCheckpointRollingPolicy 将无法工作。 如果你需要一个在批模式下工作的事务型 sink，请确保它使用 FLIP-143 中提出的统一 Sink API。
你仍然可以使用所有的 状态原语（state primitives），只是用于故障恢复的机制会有所不同。
编写自定义算子 # 注意： 自定义算子是 Apache Flink 的一种高级使用模式。对于大多数的使用情况，可以考虑使用（keyed-）处理函数来代替。 在编写自定义算子时，记住批执行模式的假设是很重要的。否则，一个在流模式下运行良好的操作符可能会在批模式下产生错误的结果。算子永远不会被限定在一个特定的键上，这意味着他们看到了 Flink 试图利用的批处理的一些属性。
首先你不应该在一个算子内缓存最后看到的 Watermark。在批模式下，我们会逐个键处理记录。因此，Watermark 会在每个键之间从 MAX_VALUE 切换到 MIN_VALUE。你不应该认为 Watermark 在一个算子中总是递增的。出于同样的原因，定时器将首先按键的顺序触发，然后按每个键内的时间戳顺序触发。此外，不支持手动更改键的操作。
`}),e.add({id:63,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/broadcast_state/",title:"Broadcast State 模式",section:"状态与容错",content:` Broadcast State 模式 # 你将在本节中了解到如何实际使用 broadcast state。想了解更多有状态流处理的概念，请参考 Stateful Stream Processing。
提供的 API # 在这里我们使用一个例子来展现 broadcast state 提供的接口。假设存在一个序列，序列中的元素是具有不同颜色与形状的图形，我们希望在序列里相同颜色的图形中寻找满足一定顺序模式的图形对（比如在红色的图形里，有一个长方形跟着一个三角形）。 同时，我们希望寻找的模式也会随着时间而改变。
在这个例子中，我们定义两个流，一个流包含图形（Item），具有颜色和形状两个属性。另一个流包含特定的规则（Rule），代表希望寻找的模式。
在图形流中，我们需要首先使用颜色将流进行进行分区（keyBy），这能确保相同颜色的图形会流转到相同的物理机上。
Java // 将图形使用颜色进行划分 KeyedStream\u0026lt;Item, Color\u0026gt; colorPartitionedStream = itemStream .keyBy(new KeySelector\u0026lt;Item, Color\u0026gt;(){...}); Python # 将图形使用颜色进行划分 color_partitioned_stream = item_stream.key_by(lambda item: ...) 对于规则流，它应该被广播到所有的下游 task 中，下游 task 应当存储这些规则并根据它寻找满足规则的图形对。下面这段代码会完成： i) 将规则广播给所有下游 task； ii) 使用 MapStateDescriptor 来描述并创建 broadcast state 在下游的存储结构
Java // 一个 map descriptor，它描述了用于存储规则名称与规则本身的 map 存储结构 MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); // 广播流，广播规则并且创建 broadcast state BroadcastStream\u0026lt;Rule\u0026gt; ruleBroadcastStream = ruleStream .broadcast(ruleStateDescriptor); Python # 一个 map descriptor，它描述了用于存储规则名称与规则本身的 map 存储结构 rule_state_descriptor = MapStateDescriptor(\u0026#34;RuleBroadcastState\u0026#34;, Types.STRING(), Types.PICKLED_BYTE_ARRAY()) # 广播流，广播规则并且创建 broadcast state rule_broadcast_stream = rule_stream.broadcast(rule_state_descriptor) 最终，为了使用规则来筛选图形序列，我们需要：
将两个流关联起来 完成我们的模式识别逻辑 为了关联一个非广播流（keyed 或者 non-keyed）与一个广播流（BroadcastStream），我们可以调用非广播流的方法 connect()，并将 BroadcastStream 当做参数传入。 这个方法的返回参数是 BroadcastConnectedStream，具有类型方法 process()，传入一个特殊的 CoProcessFunction 来书写我们的模式识别逻辑。 具体传入 process() 的是哪个类型取决于非广播流的类型：
如果流是一个 keyed 流，那就是 KeyedBroadcastProcessFunction 类型； 如果流是一个 non-keyed 流，那就是 BroadcastProcessFunction 类型。 在我们的例子中，图形流是一个 keyed stream，所以我们书写的代码如下：
connect() 方法需要由非广播流来进行调用，BroadcastStream 作为参数传入。 Java DataStream\u0026lt;String\u0026gt; output = colorPartitionedStream .connect(ruleBroadcastStream) .process( // KeyedBroadcastProcessFunction 中的类型参数表示： // 1. key stream 中的 key 类型 // 2. 非广播流中的元素类型 // 3. 广播流中的元素类型 // 4. 结果的类型，在这里是 string new KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // 模式匹配逻辑 } ); Python class MyKeyedBroadcastProcessFunction(KeyedBroadcastProcessFunction): # 模式匹配逻辑 ... output = color_partitioned_stream \\ .connect(rule_broadcast_stream) \\ .process(MyKeyedBroadcastProcessFunction()) BroadcastProcessFunction 和 KeyedBroadcastProcessFunction # 在传入的 BroadcastProcessFunction 或 KeyedBroadcastProcessFunction 中，我们需要实现两个方法。processBroadcastElement() 方法负责处理广播流中的元素，processElement() 负责处理非广播流中的元素。 两个子类型定义如下：
Java public abstract class BroadcastProcessFunction\u0026lt;IN1, IN2, OUT\u0026gt; extends BaseBroadcastProcessFunction { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } public abstract class KeyedBroadcastProcessFunction\u0026lt;KS, IN1, IN2, OUT\u0026gt; { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception; } Python class BroadcastProcessFunction(BaseBroadcastProcessFunction, Generic[IN1, IN2, OUT]): @abstractmethod def process_element(value: IN1, ctx: ReadOnlyContext): pass @abstractmethod def process_broadcast_element(value: IN2, ctx: Context): pass class KeyedBroadcastProcessFunction(BaseBrodcastProcessFunction, Generic[KEY, IN1, IN2, OUT]): @abstractmethod def process_element(value: IN1, ctx: ReadOnlyContext): pass @abstractmethod def process_broadcast_element(value: IN2, ctx: Context): pass def on_timer(timestamp: int, ctx: OnTimerContext): pass 需要注意的是 processBroadcastElement() 负责处理广播流的元素，而 processElement() 负责处理另一个流的元素。两个方法的第二个参数(Context)不同，均有以下方法：
Java 得到广播流的存储状态：ctx.getBroadcastState(MapStateDescriptor\u0026lt;K, V\u0026gt; stateDescriptor) 查询元素的时间戳：ctx.timestamp() 查询目前的Watermark：ctx.currentWatermark() 目前的处理时间(processing time)：ctx.currentProcessingTime() 产生旁路输出：ctx.output(OutputTag\u0026lt;X\u0026gt; outputTag, X value) Python 得到广播流的存储状态：ctx.get_broadcast_state(stateDescriptor: MapStateDescriptor) 查询元素的时间戳：ctx.timestamp() 查询目前的Watermark：ctx.current_watermark() 目前的处理时间(processing time)：ctx.current_processing_time() 产生旁路输出：yield output_tag, value 在 getBroadcastState() 方法中传入的 stateDescriptor 应该与调用 .broadcast(ruleStateDescriptor) 的参数相同。
这两个方法的区别在于对 broadcast state 的访问权限不同。在处理广播流元素这端，是具有读写权限的，而对于处理非广播流元素这端是只读的。 这样做的原因是，Flink 中是不存在跨 task 通讯的。所以为了保证 broadcast state 在所有的并发实例中是一致的，我们在处理广播流元素的时候给予写权限，在所有的 task 中均可以看到这些元素，并且要求对这些元素处理是一致的， 那么最终所有 task 得到的 broadcast state 是一致的。
processBroadcastElement() 的实现必须在所有的并发实例中具有确定性的结果。 同时，KeyedBroadcastProcessFunction 在 Keyed Stream 上工作，所以它提供了一些 BroadcastProcessFunction 没有的功能:
processElement() 的参数 ReadOnlyContext 提供了方法能够访问 Flink 的定时器服务，可以注册事件定时器(event-time timer)或者处理时间的定时器(processing-time timer)。当定时器触发时，会调用 onTimer() 方法， 提供了 OnTimerContext，它具有 ReadOnlyContext 的全部功能，并且提供： 查询当前触发的是一个事件还是处理时间的定时器 查询定时器关联的key processBroadcastElement() 方法中的参数 Context 会提供方法 applyToKeyedState(StateDescriptor\u0026lt;S, VS\u0026gt; stateDescriptor, KeyedStateFunction\u0026lt;KS, S\u0026gt; function)。 这个方法使用一个 KeyedStateFunction 能够对 stateDescriptor 对应的 state 中所有 key 的存储状态进行某些操作。目前 PyFlink 不支持 apply_to_keyed_state。 注册一个定时器只能在 KeyedBroadcastProcessFunction 的 processElement() 方法中进行。 在 processBroadcastElement() 方法中不能注册定时器，因为广播的元素中并没有关联的 key。 回到我们当前的例子中，KeyedBroadcastProcessFunction 应该实现如下：
Java new KeyedBroadcastProcessFunction\u0026lt;Color, Item, Rule, String\u0026gt;() { // 存储部分匹配的结果，即匹配了一个元素，正在等待第二个元素 // 我们用一个数组来存储，因为同时可能有很多第一个元素正在等待 private final MapStateDescriptor\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; mapStateDesc = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;items\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, new ListTypeInfo\u0026lt;\u0026gt;(Item.class)); // 与之前的 ruleStateDescriptor 相同 private final MapStateDescriptor\u0026lt;String, Rule\u0026gt; ruleStateDescriptor = new MapStateDescriptor\u0026lt;\u0026gt;( \u0026#34;RulesBroadcastState\u0026#34;, BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint\u0026lt;Rule\u0026gt;() {})); @Override public void processBroadcastElement(Rule value, Context ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { ctx.getBroadcastState(ruleStateDescriptor).put(value.name, value); } @Override public void processElement(Item value, ReadOnlyContext ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { final MapState\u0026lt;String, List\u0026lt;Item\u0026gt;\u0026gt; state = getRuntimeContext().getMapState(mapStateDesc); final Shape shape = value.getShape(); for (Map.Entry\u0026lt;String, Rule\u0026gt; entry : ctx.getBroadcastState(ruleStateDescriptor).immutableEntries()) { final String ruleName = entry.getKey(); final Rule rule = entry.getValue(); List\u0026lt;Item\u0026gt; stored = state.get(ruleName); if (stored == null) { stored = new ArrayList\u0026lt;\u0026gt;(); } if (shape == rule.second \u0026amp;\u0026amp; !stored.isEmpty()) { for (Item i : stored) { out.collect(\u0026#34;MATCH: \u0026#34; + i + \u0026#34; - \u0026#34; + value); } stored.clear(); } // 不需要额外的 else{} 段来考虑 rule.first == rule.second 的情况 if (shape.equals(rule.first)) { stored.add(value); } if (stored.isEmpty()) { state.remove(ruleName); } else { state.put(ruleName, stored); } } } } Python class MyKeyedBroadcastProcessFunction(KeyedBroadcastProcessFunction): def __init__(self): self._map_state_desc = MapStateDescriptor(\u0026#34;item\u0026#34;, Types.STRING(), Types.LIST(Types.PICKLED_BYTE_ARRAY())) self._rule_state_desc = MapStateDescriptor(\u0026#34;RulesBroadcastState\u0026#34;, Types.STRING(), Types.PICKLED_BYTE_ARRAY()) self._map_state = None def open(self, ctx: RuntimeContext): self._map_state = ctx.get_map_state(self._map_state_desc) def process_broadcast_element(value: Rule, ctx: KeyedBroadcastProcessFunction.Context): ctx.get_broadcast_state(self._rule_state_desc).put(value.name, value) def process_element(value: Item, ctx: KeyedBroadcastProcessFunction.ReadOnlyContext): shape = value.get_shape() for rule_name, rule in ctx.get_broadcast_state(self._rule_state_desc).items(): stored = self._map_state.get(rule_name) if stored is None: stored = [] if shape == rule.second and len(stored) \u0026gt; 0: for i in stored: yield \u0026#34;MATCH: {} - {}\u0026#34;.format(i, value) stored = [] if shape == rule.first: stored.append(value) if len(stored) == 0: self._map_state.remove(rule_name) else: self._map_state.put(rule_name, stored) 重要注意事项 # 这里有一些 broadcast state 的重要注意事项，在使用它时需要时刻清楚：
没有跨 task 通讯：如上所述，这就是为什么只有在 (Keyed)-BroadcastProcessFunction 中处理广播流元素的方法里可以更改 broadcast state 的内容。 同时，用户需要保证所有 task 对于 broadcast state 的处理方式是一致的，否则会造成不同 task 读取 broadcast state 时内容不一致的情况，最终导致结果不一致。
broadcast state 在不同的 task 的事件顺序可能是不同的：虽然广播流中元素的过程能够保证所有的下游 task 全部能够收到，但在不同 task 中元素的到达顺序可能不同。 所以 broadcast state 的更新不能依赖于流中元素到达的顺序。
所有的 task 均会对 broadcast state 进行 checkpoint：虽然所有 task 中的 broadcast state 是一致的，但当 checkpoint 来临时所有 task 均会对 broadcast state 做 checkpoint。 这个设计是为了防止在作业恢复后读文件造成的文件热点。当然这种方式会造成 checkpoint 一定程度的写放大，放大倍数为 p（=并行度）。Flink 会保证在恢复状态/改变并发的时候数据没有重复且没有缺失。 在作业恢复时，如果与之前具有相同或更小的并发度，所有的 task 读取之前已经 checkpoint 过的 state。在增大并发的情况下，task 会读取本身的 state，多出来的并发（p_new - p_old）会使用轮询调度算法读取之前 task 的 state。
不使用 RocksDB state backend: broadcast state 在运行时保存在内存中，需要保证内存充足。这一特性同样适用于所有其他 Operator State。
Back to top
`}),e.add({id:64,href:"/flink/flink-docs-master/zh/docs/dev/table/data_stream_api/",title:"DataStream API Integration",section:"Table API \u0026 SQL",content:" DataStream API Integration # Both Table API and DataStream API are equally important when it comes to defining a data processing pipeline.\nThe DataStream API offers the primitives of stream processing (namely time, state, and dataflow management) in a relatively low-level imperative programming API. The Table API abstracts away many internals and provides a structured and declarative API.\nBoth APIs can work with bounded and unbounded streams.\nBounded streams need to be managed when processing historical data. Unbounded streams occur in real-time processing scenarios that might be initialized with historical data first.\nFor efficient execution, both APIs offer processing bounded streams in an optimized batch execution mode. However, since batch is just a special case of streaming, it is also possible to run pipelines of bounded streams in regular streaming execution mode.\nPipelines in one API can be defined end-to-end without dependencies on the other API. However, it might be useful to mix both APIs for various reasons:\nUse the table ecosystem for accessing catalogs or connecting to external systems easily, before implementing the main pipeline in DataStream API. Access some of the SQL functions for stateless data normalization and cleansing, before implementing the main pipeline in DataStream API. Switch to DataStream API every now and then if a more low-level operation (e.g. custom timer handling) is not present in Table API. Flink provides special bridging functionalities to make the integration with DataStream API as smooth as possible.\nSwitching between DataStream and Table API adds some conversion overhead. For example, internal data structures of the table runtime (i.e. RowData) that partially work on binary data need to be converted to more user-friendly data structures (i.e. Row). Usually, this overhead can be neglected but is mentioned here for completeness. Back to top\nConverting between DataStream and Table # Flink provides a specialized StreamTableEnvironment for integrating with the DataStream API. Those environments extend the regular TableEnvironment with additional methods and take the StreamExecutionEnvironment used in the DataStream API as a parameter.\nThe following code shows an example of how to go back and forth between the two APIs. Column names and types of the Table are automatically derived from the TypeInformation of the DataStream. Since the DataStream API does not support changelog processing natively, the code assumes append-only/insert-only semantics during the stream-to-table and table-to-stream conversion.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a DataStream DataStream\u0026lt;String\u0026gt; dataStream = env.fromElements(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;); // interpret the insert-only DataStream as a Table Table inputTable = tableEnv.fromDataStream(dataStream); // register the Table object as a view and query it tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable); Table resultTable = tableEnv.sqlQuery(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;); // interpret the insert-only Table as a DataStream again DataStream\u0026lt;Row\u0026gt; resultStream = tableEnv.toDataStream(resultTable); // add a printing sink and execute in DataStream API resultStream.print(); env.execute(); // prints: // +I[Alice] // +I[Bob] // +I[John] Scala import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment // create environments of both APIs val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // create a DataStream val dataStream = env.fromElements(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;) // interpret the insert-only DataStream as a Table val inputTable = tableEnv.fromDataStream(dataStream) // register the Table object as a view and query it tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable) val resultTable = tableEnv.sqlQuery(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;) // interpret the insert-only Table as a DataStream again val resultStream = tableEnv.toDataStream(resultTable) // add a printing sink and execute in DataStream API resultStream.print() env.execute() // prints: // +I[Alice] // +I[Bob] // +I[John] Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.common.typeinfo import Types env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create a DataStream ds = env.from_collection([\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;], Types.STRING()) # interpret the insert-only DataStream as a Table t = t_env.from_data_stream(ds) # register the Table object as a view and query it t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, t) res_table = t_env.sql_query(\u0026#34;SELECT UPPER(f0) FROM InputTable\u0026#34;) # interpret the insert-only Table as a DataStream again res_ds = t_env.to_data_stream(res_table) # add a printing sink and execute in DataStream API res_ds.print() env.execute() # prints: # +I[Alice] # +I[Bob] # +I[John] The complete semantics of fromDataStream and toDataStream can be found in the dedicated section below. In particular, the section discusses how to influence the schema derivation with more complex and nested types. It also covers working with event-time and watermarks.\nDepending on the kind of query, in many cases the resulting dynamic table is a pipeline that does not only produce insert-only changes when converting the Table to a DataStream but also produces retractions and other kinds of updates. During table-to-stream conversion, this could lead to an exception similar to\nTable sink \u0026#39;Unregistered_DataStream_Sink_1\u0026#39; doesn\u0026#39;t support consuming update changes [...]. in which case one needs to revise the query again or switch to toChangelogStream.\nThe following example shows how updating tables can be converted. Every result row represents an entry in a changelog with a change flag that can be queried by calling row.getKind() on it. In the example, the second score for Alice creates an update before (-U) and update after (+U) change.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; // create environments of both APIs StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.of(\u0026#34;Alice\u0026#34;, 12), Row.of(\u0026#34;Bob\u0026#34;, 10), Row.of(\u0026#34;Alice\u0026#34;, 100)); // interpret the insert-only DataStream as a Table Table inputTable = tableEnv.fromDataStream(dataStream).as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;); // register the Table object as a view and query it // the query contains an aggregation that produces updates tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable); Table resultTable = tableEnv.sqlQuery( \u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;); // interpret the updating Table as a changelog DataStream DataStream\u0026lt;Row\u0026gt; resultStream = tableEnv.toChangelogStream(resultTable); // add a printing sink and execute in DataStream API resultStream.print(); env.execute(); // prints: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Scala import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment import org.apache.flink.types.Row // create environments of both APIs val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) // create a DataStream val dataStream = env.fromElements( Row.of(\u0026#34;Alice\u0026#34;, Int.box(12)), Row.of(\u0026#34;Bob\u0026#34;, Int.box(10)), Row.of(\u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the insert-only DataStream as a Table val inputTable = tableEnv.fromDataStream(dataStream).as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) // register the Table object as a view and query it // the query contains an aggregation that produces updates tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, inputTable) val resultTable = tableEnv.sqlQuery(\u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;) // interpret the updating Table as a changelog DataStream val resultStream = tableEnv.toChangelogStream(resultTable) // add a printing sink and execute in DataStream API resultStream.print() env.execute() // prints: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.common.typeinfo import Types # create environments of both APIs env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create a DataStream ds = env.from_collection([(\u0026#34;Alice\u0026#34;, 12), (\u0026#34;Bob\u0026#34;, 10), (\u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW_NAMED( [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;], [Types.STRING(), Types.INT()])) input_table = t_env.from_data_stream(ds).alias(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) # register the Table object as a view and query it # the query contains an aggregation that produces updates t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, input_table) res_table = t_env.sql_query(\u0026#34;SELECT name, SUM(score) FROM InputTable GROUP BY name\u0026#34;) # interpret the updating Table as a changelog DataStream res_stream = t_env.to_changelog_stream(res_table) # add a printing sink and execute in DataStream API res_stream.print() env.execute() # prints: # +I[Alice, 12] # +I[Bob, 10] # -U[Alice, 12] # +U[Alice, 112] The complete semantics of fromChangelogStream and toChangelogStream can be found in the dedicated section below. In particular, the section discusses how to influence the schema derivation with more complex and nested types. It covers working with event-time and watermarks. It discusses how to declare a primary key and changelog mode for the input and output streams.\nThe example above shows how the final result is computed incrementally by continuously emitting row-wise updates for each incoming record. However, in cases where the input streams are finite (i.e. bounded), a result can be computed more efficiently by leveraging batch processing principles.\nIn batch processing, operators can be executed in successive stages that consume the entire input table before emitting results. For example, a join operator can sort both bounded inputs before performing the actual joining (i.e. sort-merge join algorithm), or build a hash table from one input before consuming the other (i.e. build/probe phase of the hash join algorithm).\nBoth DataStream API and Table API offer a specialized batch runtime mode.\nThe following example illustrates that the unified pipeline is able to process both batch and streaming data by just switching a flag.\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; // setup DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set the batch runtime mode env.setRuntimeMode(RuntimeExecutionMode.BATCH); // uncomment this for streaming mode // env.setRuntimeMode(RuntimeExecutionMode.STREAMING); // setup Table API // the table environment adopts the runtime mode during initialization StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // define the same pipeline as above // prints in BATCH mode: // +I[Bob, 10] // +I[Alice, 112] // prints in STREAMING mode: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Scala import org.apache.flink.api.common.RuntimeExecutionMode import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment // setup DataStream API val env = StreamExecutionEnvironment.getExecutionEnvironment() // set the batch runtime mode env.setRuntimeMode(RuntimeExecutionMode.BATCH) // uncomment this for streaming mode // env.setRuntimeMode(RuntimeExecutionMode.STREAMING) // setup Table API // the table environment adopts the runtime mode during initialization val tableEnv = StreamTableEnvironment.create(env) // define the same pipeline as above // prints in BATCH mode: // +I[Bob, 10] // +I[Alice, 112] // prints in STREAMING mode: // +I[Alice, 12] // +I[Bob, 10] // -U[Alice, 12] // +U[Alice, 112] Python from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.table import StreamTableEnvironment # setup DataStream API env = StreamExecutionEnvironment.get_execution_environment() # set the batch runtime mode env.set_runtime_mode(RuntimeExecutionMode.BATCH) # uncomment this for streaming mode # env.set_runtime_mode(RuntimeExecutionMode.STREAMING) # setup Table API # the table environment adopts the runtime mode during initialization table_env = StreamTableEnvironment.create(env) # define the same pipeline as above # prints in BATCH mode: # +I[Bob, 10] # +I[Alice, 112] # prints in STREAMING mode: # +I[Alice, 12] # +I[Bob, 10] # -U[Alice, 12] # +U[Alice, 112] Once the changelog is applied to an external system (e.g. a key-value store), one can see that both modes are able to produce exactly the same output table. By consuming all input data before emitting results, the changelog of the batch mode consists solely of insert-only changes. See also the dedicated batch mode section below for more insights.\nDependencies and Imports # Projects that combine Table API with DataStream API need to add one of the following bridging modules. They include transitive dependencies to flink-table-api-java or flink-table-api-scala and the corresponding language-specific DataStream API module.\nJava \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Scala \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-scala-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; The following imports are required to declare common pipelines using either the Java or Scala version of both DataStream API and Table API.\nJava // imports for Java DataStream API import org.apache.flink.streaming.api.*; import org.apache.flink.streaming.api.environment.*; // imports for Table API with bridging to Java DataStream API import org.apache.flink.table.api.*; import org.apache.flink.table.api.bridge.java.*; Scala // imports for Scala DataStream API import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala._ // imports for Table API with bridging to Scala DataStream API import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala._ Python # imports for Python DataStream API from pyflink.datastream import * # imports for Table API to Python DataStream API from pyflink.table import * 请查阅配置小节了解更多细节。\nConfiguration # The TableEnvironment will adopt all configuration options from the passed StreamExecutionEnvironment. However, it cannot be guaranteed that further changes to the configuration of StreamExecutionEnvironment are propagated to the StreamTableEnvironment after its instantiation. The propagation of options from Table API to DataStream API happens during planning.\nWe recommend setting all configuration options in DataStream API early before switching to Table API.\nJava import java.time.ZoneId; import org.apache.flink.streaming.api.CheckpointingMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; // create Java DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // set various configuration early env.setMaxParallelism(256); env.getConfig().addDefaultKryoSerializer(MyCustomType.class, CustomKryoSerializer.class); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // then switch to Java Table API StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // set configuration early tableEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;Europe/Berlin\u0026#34;)); // start defining your pipelines in both APIs... Scala import java.time.ZoneId import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.streaming.api.CheckpointingMode import org.apache.flink.table.api.bridge.scala._ // create Scala DataStream API val env = StreamExecutionEnvironment.getExecutionEnvironment // set various configuration early env.setMaxParallelism(256) env.getConfig.addDefaultKryoSerializer(classOf[MyCustomType], classOf[CustomKryoSerializer]) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // then switch to Scala Table API val tableEnv = StreamTableEnvironment.create(env) // set configuration early tableEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;Europe/Berlin\u0026#34;)) // start defining your pipelines in both APIs... Python from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment from pyflink.datastream.checkpointing_mode import CheckpointingMode # create Python DataStream API env = StreamExecutionEnvironment.get_execution_environment() # set various configuration early env.set_max_parallelism(256) env.get_config().add_default_kryo_serializer(\u0026#34;type_class_name\u0026#34;, \u0026#34;serializer_class_name\u0026#34;) env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE) # then switch to Python Table API t_env = StreamTableEnvironment.create(env) # set configuration early t_env.get_config().set_local_timezone(\u0026#34;Europe/Berlin\u0026#34;) # start defining your pipelines in both APIs... Execution Behavior # Both APIs provide methods to execute pipelines. In other words: if requested, they compile a job graph that will be submitted to the cluster and triggered for execution. Results will be streamed to the declared sinks.\nUsually, both APIs mark such behavior with the term execute in method names. However, the execution behavior is slightly different between Table API and DataStream API.\nDataStream API\nThe DataStream API\u0026rsquo;s StreamExecutionEnvironment uses a builder pattern to construct a complex pipeline. The pipeline possibly splits into multiple branches that might or might not end with a sink. The environment buffers all these defined branches until it comes to job submission.\nStreamExecutionEnvironment.execute() submits the entire constructed pipeline and clears the builder afterward. In other words: no sources and sinks are declared anymore, and a new pipeline can be added to the builder. Thus, every DataStream program usually ends with a call to StreamExecutionEnvironment.execute(). Alternatively, DataStream.executeAndCollect() implicitly defines a sink for streaming the results to the local client.\nTable API\nIn the Table API, branching pipelines are only supported within a StatementSet where each branch must declare a final sink. Both TableEnvironment and also StreamTableEnvironment do not offer a dedicated general execute() method. Instead, they offer methods for submitting a single source-to-sink pipeline or a statement set:\nJava // execute with explicit sink tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable\u0026#34;).execute(); tableEnv.executeSql(\u0026#34;INSERT INTO OutputTable SELECT * FROM InputTable\u0026#34;); tableEnv.createStatementSet() .add(tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable\u0026#34;)) .add(tableEnv.from(\u0026#34;InputTable\u0026#34;).insertInto(\u0026#34;OutputTable2\u0026#34;)) .execute(); tableEnv.createStatementSet() .addInsertSql(\u0026#34;INSERT INTO OutputTable SELECT * FROM InputTable\u0026#34;) .addInsertSql(\u0026#34;INSERT INTO OutputTable2 SELECT * FROM InputTable\u0026#34;) .execute(); // execute with implicit local sink tableEnv.from(\u0026#34;InputTable\u0026#34;).execute().print(); tableEnv.executeSql(\u0026#34;SELECT * FROM InputTable\u0026#34;).print(); Python # execute with explicit sink table_env.from_path(\u0026#34;input_table\u0026#34;).execute_insert(\u0026#34;output_table\u0026#34;) table_env.execute_sql(\u0026#34;INSERT INTO output_table SELECT * FROM input_table\u0026#34;) table_env.create_statement_set() \\ .add_insert(\u0026#34;output_table\u0026#34;, input_table) \\ .add_insert(\u0026#34;output_table2\u0026#34;, input_table) \\ .execute() table_env.create_statement_set() \\ .add_insert_sql(\u0026#34;INSERT INTO output_table SELECT * FROM input_table\u0026#34;) \\ .add_insert_sql(\u0026#34;INSERT INTO output_table2 SELECT * FROM input_table\u0026#34;) \\ .execute() # execute with implicit local sink table_env.from_path(\u0026#34;input_table\u0026#34;).execute().print() table_env.execute_sql(\u0026#34;SELECT * FROM input_table\u0026#34;).print() To combine both execution behaviors, every call to StreamTableEnvironment.toDataStream or StreamTableEnvironment.toChangelogStream will materialize (i.e. compile) the Table API sub-pipeline and insert it into the DataStream API pipeline builder. This means that StreamExecutionEnvironment.execute() or DataStream.executeAndCollect must be called afterwards. An execution in Table API will not trigger these \u0026ldquo;external parts\u0026rdquo;.\nJava // (1) // adds a branch with a printing sink to the StreamExecutionEnvironment tableEnv.toDataStream(table).print(); // (2) // executes a Table API end-to-end pipeline as a Flink job and prints locally, // thus (1) has still not been executed table.execute().print(); // executes the DataStream API pipeline with the sink defined in (1) as a // Flink job, (2) was already running before env.execute(); Python # (1) # adds a branch with a printing sink to the StreamExecutionEnvironment table_env.to_data_stream(table).print() # (2) # executes a Table API end-to-end pipeline as a Flink job and prints locally, # thus (1) has still not been executed table.execute().print() # executes the DataStream API pipeline with the sink defined in (1) as a # Flink job, (2) was already running before env.execute() Back to top\nBatch Runtime Mode # The batch runtime mode is a specialized execution mode for bounded Flink programs.\nGenerally speaking, boundedness is a property of a data source that tells us whether all the records coming from that source are known before execution or whether new data will show up, potentially indefinitely. A job, in turn, is bounded if all its sources are bounded, and unbounded otherwise.\nStreaming runtime mode, on the other hand, can be used for both bounded and unbounded jobs.\nFor more information on the different execution modes, see also the corresponding DataStream API section.\nThe Table API \u0026amp; SQL planner provides a set of specialized optimizer rules and runtime operators for either of the two modes.\nCurrently, the runtime mode is not derived automatically from sources, thus, it must be set explicitly or will be adopted from StreamExecutionEnvironment when instantiating a StreamTableEnvironment:\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.table.api.EnvironmentSettings; // adopt mode from StreamExecutionEnvironment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // or // set mode explicitly for StreamTableEnvironment // it will be propagated to StreamExecutionEnvironment during planning StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, EnvironmentSettings.inBatchMode()); Scala import org.apache.flink.api.common.RuntimeExecutionMode import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment import org.apache.flink.table.api.EnvironmentSettings // adopt mode from StreamExecutionEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment env.setRuntimeMode(RuntimeExecutionMode.BATCH) val tableEnv = StreamTableEnvironment.create(env) // or // set mode explicitly for StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env, EnvironmentSettings.inBatchMode) Python from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.table import EnvironmentSettings, StreamTableEnvironment # adopt mode from StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) table_env = StreamTableEnvironment.create(env) # or # set mode explicitly for StreamTableEnvironment # it will be propagated to StreamExecutionEnvironment during planning env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env, EnvironmentSettings.in_batch_mode()) One must meet the following prerequisites before setting the runtime mode to BATCH:\nAll sources must declare themselves as bounded.\nCurrently, table sources must emit insert-only changes.\nOperators need a sufficient amount of off-heap memory for sorting and other intermediate results.\nAll table operations must be available in batch mode. Currently, some of them are only available in streaming mode. Please check the corresponding Table API \u0026amp; SQL pages.\nA batch execution has the following implications (among others):\nProgressive watermarks are neither generated nor used in operators. However, sources emit a maximum watermark before shutting down.\nExchanges between tasks might be blocking according to the execution.batch-shuffle-mode. This also means potentially less resource requirements compared to executing the same pipeline in streaming mode.\nCheckpointing is disabled. Artificial state backends are inserted.\nTable operations don\u0026rsquo;t produce incremental updates but only a complete final result which converts to an insert-only changelog stream.\nSince batch processing can be considered as a special case of stream processing, we recommend implementing a streaming pipeline first as it is the most general implementation for both bounded and unbounded data.\nIn theory, a streaming pipeline can execute all operators. However, in practice, some operations might not make much sense as they would lead to ever-growing state and are therefore not supported. A global sort would be an example that is only available in batch mode. Simply put: it should be possible to run a working streaming pipeline in batch mode but not necessarily vice versa.\nThe following example shows how to play around with batch mode using the DataGen table source. Many sources offer options that implicitly make the connector bounded, for example, by defining a terminating offset or timestamp. In our example, we limit the number of rows with the number-of-rows option.\nJava import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.TableDescriptor; Table table = tableEnv.from( TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) // make the source bounded .schema( Schema.newBuilder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()); // convert the Table to a DataStream and further transform the pipeline tableEnv.toDataStream(table) .keyBy(r -\u0026gt; r.\u0026lt;Byte\u0026gt;getFieldAs(\u0026#34;uid\u0026#34;)) .map(r -\u0026gt; \u0026#34;My custom operator: \u0026#34; + r.\u0026lt;String\u0026gt;getFieldAs(\u0026#34;payload\u0026#34;)) .executeAndCollect() .forEachRemaining(System.out::println); // prints: // My custom operator: 9660912d30a43c7b035e15bd... // My custom operator: 29f5f706d2144f4a4f9f52a0... // ... Scala import org.apache.flink.api.scala._ import org.apache.flink.table.api._ val table = tableEnv.from( TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) // make the source bounded .schema( Schema.newBuilder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()) // convert the Table to a DataStream and further transform the pipeline tableEnv.toDataStream(table) .keyBy(r =\u0026gt; r.getFieldAs[Byte](\u0026#34;uid\u0026#34;)) .map(r =\u0026gt; \u0026#34;My custom operator: \u0026#34; + r.getFieldAs[String](\u0026#34;payload\u0026#34;)) .executeAndCollect() .foreach(println) // prints: // My custom operator: 9660912d30a43c7b035e15bd... // My custom operator: 29f5f706d2144f4a4f9f52a0... // ... Python from pyflink.table import TableDescriptor, Schema, DataTypes table = table_env.from_descriptor( TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;10\u0026#34;) .schema( Schema.new_builder() .column(\u0026#34;uid\u0026#34;, DataTypes.TINYINT()) .column(\u0026#34;payload\u0026#34;, DataTypes.STRING()) .build()) .build()) # convert the Table to a DataStream and further transform the pipeline collect = table_env.to_data_stream(table) \\ .key_by(lambda r: r[0]) \\ .map(lambda r: \u0026#34;My custom operator: \u0026#34; + r[1]) \\ .execute_and_collect() for c in collect: print(c) # prints: # My custom operator: 9660912d30a43c7b035e15bd... # My custom operator: 29f5f706d2144f4a4f9f52a0... # ... Changelog Unification # In most cases, the pipeline definition itself can remain constant in both Table API and DataStream API when switching from streaming to batch mode and vice versa. However, as mentioned before, the resulting changelog streams might differ due to the avoidance of incremental operations in batch mode.\nTime-based operations that rely on event-time and leverage watermarks as a completeness marker are able to produce an insert-only changelog stream that is independent of the runtime mode.\nThe following Java example illustrates a Flink program that is not only unified on an API level but also in the resulting changelog stream. The example joins two tables in SQL (UserTable and OrderTable) using an interval join based on the time attributes in both tables (ts). It uses DataStream API to implement a custom operator that deduplicates the user name using a KeyedProcessFunction and value state.\nJava import org.apache.flink.api.common.RuntimeExecutionMode; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; import org.apache.flink.util.Collector; // setup DataStream API StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // use BATCH or STREAMING mode env.setRuntimeMode(RuntimeExecutionMode.BATCH); // setup Table API StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a user stream DataStream\u0026lt;Row\u0026gt; userStream = env .fromElements( Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:00:00\u0026#34;), 1, \u0026#34;Alice\u0026#34;), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:05:00\u0026#34;), 2, \u0026#34;Bob\u0026#34;), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:10:00\u0026#34;), 2, \u0026#34;Bob\u0026#34;)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;ts\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;name\u0026#34;}, Types.LOCAL_DATE_TIME, Types.INT, Types.STRING)); // create an order stream DataStream\u0026lt;Row\u0026gt; orderStream = env .fromElements( Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:02:00\u0026#34;), 1, 122), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:07:00\u0026#34;), 2, 239), Row.of(LocalDateTime.parse(\u0026#34;2021-08-21T13:11:00\u0026#34;), 2, 999)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;ts\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;amount\u0026#34;}, Types.LOCAL_DATE_TIME, Types.INT, Types.INT)); // create corresponding tables tableEnv.createTemporaryView( \u0026#34;UserTable\u0026#34;, userStream, Schema.newBuilder() .column(\u0026#34;ts\u0026#34;, DataTypes.TIMESTAMP(3)) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()); tableEnv.createTemporaryView( \u0026#34;OrderTable\u0026#34;, orderStream, Schema.newBuilder() .column(\u0026#34;ts\u0026#34;, DataTypes.TIMESTAMP(3)) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()); // perform interval join Table joinedTable = tableEnv.sqlQuery( \u0026#34;SELECT U.name, O.amount \u0026#34; + \u0026#34;FROM UserTable U, OrderTable O \u0026#34; + \u0026#34;WHERE U.uid = O.uid AND O.ts BETWEEN U.ts AND U.ts + INTERVAL \u0026#39;5\u0026#39; MINUTES\u0026#34;); DataStream\u0026lt;Row\u0026gt; joinedStream = tableEnv.toDataStream(joinedTable); joinedStream.print(); // implement a custom operator using ProcessFunction and value state joinedStream .keyBy(r -\u0026gt; r.\u0026lt;String\u0026gt;getFieldAs(\u0026#34;name\u0026#34;)) .process( new KeyedProcessFunction\u0026lt;String, Row, String\u0026gt;() { ValueState\u0026lt;String\u0026gt; seen; @Override public void open(Configuration parameters) { seen = getRuntimeContext().getState( new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;seen\u0026#34;, String.class)); } @Override public void processElement(Row row, Context ctx, Collector\u0026lt;String\u0026gt; out) throws Exception { String name = row.getFieldAs(\u0026#34;name\u0026#34;); if (seen.value() == null) { seen.update(name); out.collect(name); } } }) .print(); // execute unified pipeline env.execute(); // prints (in both BATCH and STREAMING mode): // +I[Bob, 239] // +I[Alice, 122] // +I[Bob, 999] // // Bob // Alice Python from datetime import datetime from pyflink.common import Row, Types from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode, KeyedProcessFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor from pyflink.table import StreamTableEnvironment, Schema, DataTypes # setup DataStream API env = StreamExecutionEnvironment.get_execution_environment() # use BATCH or STREAMING mode env.set_runtime_mode(RuntimeExecutionMode.BATCH) # setup Table API table_env = StreamTableEnvironment.create(env) # create a user stream t_format = \u0026#34;%Y-%m-%dT%H:%M:%S\u0026#34; user_stream = env.from_collection( [Row(datetime.strptime(\u0026#34;2021-08-21T13:00:00\u0026#34;, t_format), 1, \u0026#34;Alice\u0026#34;), Row(datetime.strptime(\u0026#34;2021-08-21T13:05:00\u0026#34;, t_format), 2, \u0026#34;Bob\u0026#34;), Row(datetime.strptime(\u0026#34;2021-08-21T13:10:00\u0026#34;, t_format), 2, \u0026#34;Bob\u0026#34;)], type_info=Types.ROW_NAMED([\u0026#34;ts1\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;name\u0026#34;], [Types.SQL_TIMESTAMP(), Types.INT(), Types.STRING()])) # create an order stream order_stream = env.from_collection( [Row(datetime.strptime(\u0026#34;2021-08-21T13:02:00\u0026#34;, t_format), 1, 122), Row(datetime.strptime(\u0026#34;2021-08-21T13:07:00\u0026#34;, t_format), 2, 239), Row(datetime.strptime(\u0026#34;2021-08-21T13:11:00\u0026#34;, t_format), 2, 999)], type_info=Types.ROW_NAMED([\u0026#34;ts1\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;amount\u0026#34;], [Types.SQL_TIMESTAMP(), Types.INT(), Types.INT()])) # # create corresponding tables table_env.create_temporary_view( \u0026#34;user_table\u0026#34;, user_stream, Schema.new_builder() .column_by_expression(\u0026#34;ts\u0026#34;, \u0026#34;CAST(ts1 AS TIMESTAMP(3))\u0026#34;) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()) table_env.create_temporary_view( \u0026#34;order_table\u0026#34;, order_stream, Schema.new_builder() .column_by_expression(\u0026#34;ts\u0026#34;, \u0026#34;CAST(ts1 AS TIMESTAMP(3))\u0026#34;) .column(\u0026#34;uid\u0026#34;, DataTypes.INT()) .column(\u0026#34;amount\u0026#34;, DataTypes.INT()) .watermark(\u0026#34;ts\u0026#34;, \u0026#34;ts - INTERVAL \u0026#39;1\u0026#39; SECOND\u0026#34;) .build()) # perform interval join joined_table = table_env.sql_query( \u0026#34;SELECT U.name, O.amount \u0026#34; + \u0026#34;FROM user_table U, order_table O \u0026#34; + \u0026#34;WHERE U.uid = O.uid AND O.ts BETWEEN U.ts AND U.ts + INTERVAL \u0026#39;5\u0026#39; MINUTES\u0026#34;) joined_stream = table_env.to_data_stream(joined_table) joined_stream.print() # implement a custom operator using ProcessFunction and value state class MyProcessFunction(KeyedProcessFunction): def __init__(self): self.seen = None def open(self, runtime_context: RuntimeContext): state_descriptor = ValueStateDescriptor(\u0026#34;seen\u0026#34;, Types.STRING()) self.seen = runtime_context.get_state(state_descriptor) def process_element(self, value, ctx): name = value[0] if self.seen.value() is None: self.seen.update(name) yield name joined_stream \\ .key_by(lambda r: r[0]) \\ .process(MyProcessFunction()) \\ .print() # execute unified pipeline env.execute() # prints (in both BATCH and STREAMING mode): # +I[Bob, 239] # +I[Alice, 122] # +I[Bob, 999] # # Bob # Alice Back to top\nHandling of (Insert-Only) Streams # A StreamTableEnvironment offers the following methods to convert from and to DataStream API:\nfromDataStream(DataStream): Interprets a stream of insert-only changes and arbitrary type as a table. Event-time and watermarks are not propagated by default.\nfromDataStream(DataStream, Schema): Interprets a stream of insert-only changes and arbitrary type as a table. The optional schema allows to enrich column data types and add time attributes, watermarks strategies, other computed columns, or primary keys.\ncreateTemporaryView(String, DataStream): Registers the stream under a name to access it in SQL. It is a shortcut for createTemporaryView(String, fromDataStream(DataStream)).\ncreateTemporaryView(String, DataStream, Schema): Registers the stream under a name to access it in SQL. It is a shortcut for createTemporaryView(String, fromDataStream(DataStream, Schema)).\ntoDataStream(Table): Converts a table into a stream of insert-only changes. The default stream record type is org.apache.flink.types.Row. A single rowtime attribute column is written back into the DataStream API\u0026rsquo;s record. Watermarks are propagated as well.\ntoDataStream(Table, AbstractDataType): Converts a table into a stream of insert-only changes. This method accepts a data type to express the desired stream record type. The planner might insert implicit casts and reorders columns to map columns to fields of the (possibly nested) data type.\ntoDataStream(Table, Class): A shortcut for toDataStream(Table, DataTypes.of(Class)) to quickly create the desired data type reflectively.\nFrom a Table API\u0026rsquo;s perspective, converting from and to DataStream API is similar to reading from or writing to a virtual table connector that has been defined using a CREATE TABLE DDL in SQL.\nThe schema part in the virtual CREATE TABLE name (schema) WITH (options) statement can be automatically derived from the DataStream\u0026rsquo;s type information, enriched, or entirely defined manually using org.apache.flink.table.api.Schema.\nThe virtual DataStream table connector exposes the following metadata for every row:\nKey Data Type Description R/W rowtime TIMESTAMP_LTZ(3) NOT NULL Stream record's timestamp. R/W The virtual DataStream table source implements SupportsSourceWatermark and thus allows calling the SOURCE_WATERMARK() built-in function as a watermark strategy to adopt watermarks from the DataStream API.\nExamples for fromDataStream # The following code shows how to use fromDataStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import java.time.Instant; // some example POJO public static class User { public String name; public Integer score; public Instant event_time; // default constructor for DataStream API public User() {} // fully assigning constructor for Table API public User(String name, Integer score, Instant event_time) { this.name = name; this.score = score; this.event_time = event_time; } } // create a DataStream DataStream\u0026lt;User\u0026gt; dataStream = env.fromElements( new User(\u0026#34;Alice\u0026#34;, 4, Instant.ofEpochMilli(1000)), new User(\u0026#34;Bob\u0026#34;, 6, Instant.ofEpochMilli(1001)), new User(\u0026#34;Alice\u0026#34;, 10, Instant.ofEpochMilli(1002))); // === EXAMPLE 1 === // derive all physical columns automatically Table table = tableEnv.fromDataStream(dataStream); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9) // ) // === EXAMPLE 2 === // derive all physical columns automatically // but add computed columns (in this case for creating a proctime attribute column) Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT NOT NULL, // `event_time` TIMESTAMP_LTZ(9), // `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() //) // === EXAMPLE 3 === // derive all physical columns automatically // but add computed columns (in this case for creating a rowtime attribute column) // and a custom watermark strategy Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND // ) // === EXAMPLE 4 === // derive all physical columns automatically // but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column // also rely on the watermarks generated in the DataStream API // we assume that a watermark strategy has been defined for `dataStream` before // (not part of this example) Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() // ) // === EXAMPLE 5 === // define physical columns manually // in this example, // - we can reduce the default precision of timestamps from 9 to 3 // - we also project the columns and put `event_time` to the beginning Table table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()); table.printSchema(); // prints: // ( // `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, // `name` VARCHAR(200), // `score` INT // ) // note: the watermark strategy is not shown due to the inserted column reordering projection Scala import org.apache.flink.api.scala._ import java.time.Instant // some example case class case class User(name: String, score: java.lang.Integer, event_time: java.time.Instant) // create a DataStream val dataStream = env.fromElements( User(\u0026#34;Alice\u0026#34;, 4, Instant.ofEpochMilli(1000)), User(\u0026#34;Bob\u0026#34;, 6, Instant.ofEpochMilli(1001)), User(\u0026#34;Alice\u0026#34;, 10, Instant.ofEpochMilli(1002))) // === EXAMPLE 1 === // derive all physical columns automatically val table = tableEnv.fromDataStream(dataStream) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9) // ) // === EXAMPLE 2 === // derive all physical columns automatically // but add computed columns (in this case for creating a proctime attribute column) val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT NOT NULL, // `event_time` TIMESTAMP_LTZ(9), // `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() //) // === EXAMPLE 3 === // derive all physical columns automatically // but add computed columns (in this case for creating a rowtime attribute column) // and a custom watermark strategy val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByExpression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND // ) // === EXAMPLE 4 === // derive all physical columns automatically // but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column // also rely on the watermarks generated in the DataStream API // we assume that a watermark strategy has been defined for `dataStream` before // (not part of this example) val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.printSchema() // prints: // ( // `name` STRING, // `score` INT, // `event_time` TIMESTAMP_LTZ(9), // `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, // WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() // ) // === EXAMPLE 5 === // define physical columns manually // in this example, // - we can reduce the default precision of timestamps from 9 to 3 // - we also project the columns and put `event_time` to the beginning val table = tableEnv.fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.printSchema() // prints: // ( // `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, // `name` VARCHAR(200), // `score` INT // ) // note: the watermark strategy is not shown due to the inserted column reordering projection Python from pyflink.common.time import Instant from pyflink.common.types import Row from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) ds = env.from_collection([ Row(\u0026#34;Alice\u0026#34;, 12, Instant.of_epoch_milli(1000)), Row(\u0026#34;Bob\u0026#34;, 5, Instant.of_epoch_milli(1001)), Row(\u0026#34;Alice\u0026#34;, 10, Instant.of_epoch_milli(1002))], type_info=Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;score\u0026#39;, \u0026#39;event_time\u0026#39;], [Types.STRING(), Types.INT(), Types.INSTANT()])) # === EXAMPLE 1 === # derive all physical columns automatically table = t_env.from_data_stream(ds) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9) # ) # === EXAMPLE 2 === # derive all physical columns automatically # but add computed columns (in this case for creating a proctime attribute column) table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_expression(\u0026#34;proc_time\u0026#34;, \u0026#34;PROCTIME()\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `proc_time` TIMESTAMP_LTZ(3) NOT NULL *PROCTIME* AS PROCTIME() # ) # === EXAMPLE 3 === # derive all physical columns automatically # but add computed columns (in this case for creating a rowtime attribute column) # and a custom watermark strategy table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_expression(\u0026#34;rowtime\u0026#34;, \u0026#34;CAST(event_time AS TIMESTAMP_LTZ(3))\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* AS CAST(event_time AS TIMESTAMP_LTZ(3)), # WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS rowtime - INTERVAL \u0026#39;10\u0026#39; SECOND # ) # === EXAMPLE 4 === # derive all physical columns automatically # but access the stream record\u0026#39;s timestamp for creating a rowtime attribute column # also rely on the watermarks generated in the DataStream API # we assume that a watermark strategy has been defined for `dataStream` before # (not part of this example) table = t_env.from_data_stream( ds, Schema.new_builder() .column_by_metadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .watermark(\u0026#34;rowtime\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.print_schema() # prints: # ( # `name` STRING, # `score` INT, # `event_time` TIMESTAMP_LTZ(9), # `rowtime` TIMESTAMP_LTZ(3) *ROWTIME* METADATA, # WATERMARK FOR `rowtime`: TIMESTAMP_LTZ(3) AS SOURCE_WATERMARK() # ) # === EXAMPLE 5 === # define physical columns manually # in this example, # - we can reduce the default precision of timestamps from 9 to 3 # - we also project the columns and put `event_time` to the beginning table = t_env.from_data_stream( ds, Schema.new_builder() .column(\u0026#34;event_time\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .watermark(\u0026#34;event_time\u0026#34;, \u0026#34;SOURCE_WATERMARK()\u0026#34;) .build()) table.print_schema() # prints: # ( # `event_time` TIMESTAMP_LTZ(3) *ROWTIME*, # `name` STRING, # `score` INT # ) # note: the watermark strategy is not shown due to the inserted column reordering projection Example 1 illustrates a simple use case when no time-based operations are needed.\nExample 4 is the most common use case when time-based operations such as windows or interval joins should be part of the pipeline. Example 2 is the most common use case when these time-based operations should work in processing time.\nExample 5 entirely relies on the declaration of the user. This can be useful to replace generic types from the DataStream API (which would be RAW in the Table API) with proper data types.\nSince DataType is richer than TypeInformation, we can easily enable immutable POJOs and other complex data structures. The following example in Java shows what is possible. Check also the Data Types \u0026amp; Serialization page of the DataStream API for more information about the supported types there.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; // the DataStream API does not support immutable POJOs yet, // the class will result in a generic type that is a RAW type in Table API by default public static class User { public final String name; public final Integer score; public User(String name, Integer score) { this.name = name; this.score = score; } } // create a DataStream DataStream\u0026lt;User\u0026gt; dataStream = env.fromElements( new User(\u0026#34;Alice\u0026#34;, 4), new User(\u0026#34;Bob\u0026#34;, 6), new User(\u0026#34;Alice\u0026#34;, 10)); // since fields of a RAW type cannot be accessed, every stream record is treated as an atomic type // leading to a table with a single column `f0` Table table = tableEnv.fromDataStream(dataStream); table.printSchema(); // prints: // ( // `f0` RAW(\u0026#39;User\u0026#39;, \u0026#39;...\u0026#39;) // ) // instead, declare a more useful data type for columns using the Table API\u0026#39;s type system // in a custom schema and rename the columns in a following `as` projection Table table = tableEnv .fromDataStream( dataStream, Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, DataTypes.of(User.class)) .build()) .as(\u0026#34;user\u0026#34;); table.printSchema(); // prints: // ( // `user` *User\u0026lt;`name` STRING,`score` INT\u0026gt;* // ) // data types can be extracted reflectively as above or explicitly defined Table table3 = tableEnv .fromDataStream( dataStream, Schema.newBuilder() .column( \u0026#34;f0\u0026#34;, DataTypes.STRUCTURED( User.class, DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()))) .build()) .as(\u0026#34;user\u0026#34;); table.printSchema(); // prints: // ( // `user` *User\u0026lt;`name` STRING,`score` INT\u0026gt;* // ) Python Custom PoJo Class is unsupported in PyFlink now. Examples for createTemporaryView # A DataStream can be registered directly as a view (possibly enriched with a schema).\nViews created from a DataStream can only be registered as temporary views. Due to their inline/anonymous nature, it is not possible to register them in a permanent catalog. The following code shows how to use createTemporaryView for different scenarios.\nJava import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; // create some DataStream DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; dataStream = env.fromElements( Tuple2.of(12L, \u0026#34;Alice\u0026#34;), Tuple2.of(0L, \u0026#34;Bob\u0026#34;)); // === EXAMPLE 1 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session // all columns are derived automatically tableEnv.createTemporaryView(\u0026#34;MyView\u0026#34;, dataStream); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `f0` BIGINT NOT NULL, // `f1` STRING // ) // === EXAMPLE 2 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, // provide a schema to adjust the columns similar to `fromDataStream` // in this example, the derived NOT NULL information has been removed tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, dataStream, Schema.newBuilder() .column(\u0026#34;f0\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;f1\u0026#34;, \u0026#34;STRING\u0026#34;) .build()); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `f0` BIGINT, // `f1` STRING // ) // === EXAMPLE 3 === // use the Table API before creating the view if it is only about renaming columns tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, tableEnv.fromDataStream(dataStream).as(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)); tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema(); // prints: // ( // `id` BIGINT NOT NULL, // `name` STRING // ) Scala // create some DataStream val dataStream: DataStream[(Long, String)] = env.fromElements( (12L, \u0026#34;Alice\u0026#34;), (0L, \u0026#34;Bob\u0026#34;)) // === EXAMPLE 1 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session // all columns are derived automatically tableEnv.createTemporaryView(\u0026#34;MyView\u0026#34;, dataStream) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `_1` BIGINT NOT NULL, // `_2` STRING // ) // === EXAMPLE 2 === // register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, // provide a schema to adjust the columns similar to `fromDataStream` // in this example, the derived NOT NULL information has been removed tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, dataStream, Schema.newBuilder() .column(\u0026#34;_1\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;_2\u0026#34;, \u0026#34;STRING\u0026#34;) .build()) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `_1` BIGINT, // `_2` STRING // ) // === EXAMPLE 3 === // use the Table API before creating the view if it is only about renaming columns tableEnv.createTemporaryView( \u0026#34;MyView\u0026#34;, tableEnv.fromDataStream(dataStream).as(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)) tableEnv.from(\u0026#34;MyView\u0026#34;).printSchema() // prints: // ( // `id` BIGINT NOT NULL, // `name` STRING // ) Python from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import DataTypes, StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) ds = env.from_collection([(12, \u0026#34;Alice\u0026#34;), (0, \u0026#34;Bob\u0026#34;)], type_info=Types.TUPLE([Types.LONG(), Types.STRING()])) # === EXAMPLE 1 === # register the DataStream as view \u0026#34;MyView\u0026#34; in the current session # all columns are derived automatically t_env.create_temporary_view(\u0026#34;MyView\u0026#34;, ds) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `f0` BIGINT NOT NULL, # `f1` STRING # ) # === EXAMPLE 2 === # register the DataStream as view \u0026#34;MyView\u0026#34; in the current session, # provide a schema to adjust the columns similar to `fromDataStream` # in this example, the derived NOT NULL information has been removed t_env.create_temporary_view( \u0026#34;MyView\u0026#34;, ds, Schema.new_builder() .column(\u0026#34;f0\u0026#34;, \u0026#34;BIGINT\u0026#34;) .column(\u0026#34;f1\u0026#34;, \u0026#34;STRING\u0026#34;) .build()) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `f0` BIGINT, # `f1` STRING # ) # === EXAMPLE 3 === # use the Table API before creating the view if it is only about renaming columns t_env.create_temporary_view( \u0026#34;MyView\u0026#34;, t_env.from_data_stream(ds).alias(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;)) t_env.from_path(\u0026#34;MyView\u0026#34;).print_schema() # prints: # ( # `id` BIGINT NOT NULL, # `name` STRING # ) Back to top\nExamples for toDataStream # The following code shows how to use toDataStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Table; import org.apache.flink.types.Row; import java.time.Instant; // POJO with mutable fields // since no fully assigning constructor is defined, the field order // is alphabetical [event_time, name, score] public static class User { public String name; public Integer score; public Instant event_time; } tableEnv.executeSql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); Table table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;); // === EXAMPLE 1 === // use the default conversion to instances of Row // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toDataStream(table); // === EXAMPLE 2 === // a data type is extracted from class `User`, // the planner reorders fields and inserts implicit casts where possible to convert internal // data structures to the desired structured type // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated DataStream\u0026lt;User\u0026gt; dataStream = tableEnv.toDataStream(table, User.class); // data types can be extracted reflectively as above or explicitly defined DataStream\u0026lt;User\u0026gt; dataStream = tableEnv.toDataStream( table, DataTypes.STRUCTURED( User.class, DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3)))); Scala import org.apache.flink.streaming.api.scala.DataStream import org.apache.flink.table.api.DataTypes case class User(name: String, score: java.lang.Integer, event_time: java.time.Instant) tableEnv.executeSql( \u0026#34;\u0026#34;\u0026#34; CREATE TABLE GeneratedTable ( name STRING, score INT, event_time TIMESTAMP_LTZ(3), WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;) \u0026#34;\u0026#34;\u0026#34; ) val table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;) // === EXAMPLE 1 === // use the default conversion to instances of Row // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated val dataStream: DataStream[Row] = tableEnv.toDataStream(table) // === EXAMPLE 2 === // a data type is extracted from class `User`, // the planner reorders fields and inserts implicit casts where possible to convert internal // data structures to the desired structured type // since `event_time` is a single rowtime attribute, it is inserted into the DataStream // metadata and watermarks are propagated val dataStream: DataStream[User] = tableEnv.toDataStream(table, classOf[User]) // data types can be extracted reflectively as above or explicitly defined val dataStream: DataStream[User] = tableEnv.toDataStream( table, DataTypes.STRUCTURED( classOf[User], DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3)))) Python t_env.execute_sql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); table = t_env.from_path(\u0026#34;GeneratedTable\u0026#34;); # === EXAMPLE 1 === # use the default conversion to instances of Row # since `event_time` is a single rowtime attribute, it is inserted into the DataStream # metadata and watermarks are propagated ds = t_env.to_data_stream(table) Note that only non-updating tables are supported by toDataStream. Usually, time-based operations such as windows, interval joins, or the MATCH_RECOGNIZE clause are a good fit for insert-only pipelines next to simple operations like projections and filters.\nPipelines with operations that produce updates can use toChangelogStream.\nBack to top\nHandling of Changelog Streams # Internally, Flink\u0026rsquo;s table runtime is a changelog processor. The concepts page describes how dynamic tables and streams relate to each other.\nA StreamTableEnvironment offers the following methods to expose these change data capture (CDC) functionalities:\nfromChangelogStream(DataStream): Interprets a stream of changelog entries as a table. The stream record type must be org.apache.flink.types.Row since its RowKind flag is evaluated during runtime. Event-time and watermarks are not propagated by default. This method expects a changelog containing all kinds of changes (enumerated in org.apache.flink.types.RowKind) as the default ChangelogMode.\nfromChangelogStream(DataStream, Schema): Allows to define a schema for the DataStream similar to fromDataStream(DataStream, Schema). Otherwise the semantics are equal to fromChangelogStream(DataStream).\nfromChangelogStream(DataStream, Schema, ChangelogMode): Gives full control about how to interpret a stream as a changelog. The passed ChangelogMode helps the planner to distinguish between insert-only, upsert, or retract behavior.\ntoChangelogStream(Table): Reverse operation of fromChangelogStream(DataStream). It produces a stream with instances of org.apache.flink.types.Row and sets the RowKind flag for every record at runtime. All kinds of updating tables are supported by this method. If the input table contains a single rowtime column, it will be propagated into a stream record\u0026rsquo;s timestamp. Watermarks will be propagated as well.\ntoChangelogStream(Table, Schema): Reverse operation of fromChangelogStream(DataStream, Schema). The method can enrich the produced column data types. The planner might insert implicit casts if necessary. It is possible to write out the rowtime as a metadata column.\ntoChangelogStream(Table, Schema, ChangelogMode): Gives full control about how to convert a table to a changelog stream. The passed ChangelogMode helps the planner to distinguish between insert-only, upsert, or retract behavior.\nFrom a Table API\u0026rsquo;s perspective, converting from and to DataStream API is similar to reading from or writing to a virtual table connector that has been defined using a CREATE TABLE DDL in SQL.\nBecause fromChangelogStream behaves similar to fromDataStream, we recommend reading the previous section before continuing here.\nThis virtual connector also supports reading and writing the rowtime metadata of the stream record.\nThe virtual table source implements SupportsSourceWatermark.\nExamples for fromChangelogStream # The following code shows how to use fromChangelogStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; // === EXAMPLE 1 === // interpret the stream as a retract stream // create a changelog DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)); // interpret the DataStream as a Table Table table = tableEnv.fromChangelogStream(dataStream); // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print(); // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -D | Alice | 12 | // | +I | Alice | 100 | // +----+--------------------------------+-------------+ // === EXAMPLE 2 === // interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) // create a changelog DataStream DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)); // interpret the DataStream as a Table Table table = tableEnv.fromChangelogStream( dataStream, Schema.newBuilder().primaryKey(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()); // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print(); // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -U | Alice | 12 | // | +U | Alice | 100 | // +----+--------------------------------+-------------+ Scala import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.table.api.Schema import org.apache.flink.table.connector.ChangelogMode import org.apache.flink.types.{Row, RowKind} // === EXAMPLE 1 === // interpret the stream as a retract stream // create a changelog DataStream val dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, Int.box(5)), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the DataStream as a Table val table = tableEnv.fromChangelogStream(dataStream) // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table) tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print() // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -D | Alice | 12 | // | +I | Alice | 100 | // +----+--------------------------------+-------------+ // === EXAMPLE 2 === // interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) // create a changelog DataStream val dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, Int.box(12)), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, Int.box(5)), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, Int.box(100)) )(Types.ROW(Types.STRING, Types.INT)) // interpret the DataStream as a Table val table = tableEnv.fromChangelogStream( dataStream, Schema.newBuilder().primaryKey(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()) // register the table under a name and perform an aggregation tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table) tableEnv .executeSql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;) .print() // prints: // +----+--------------------------------+-------------+ // | op | name | score | // +----+--------------------------------+-------------+ // | +I | Bob | 5 | // | +I | Alice | 12 | // | -U | Alice | 12 | // | +U | Alice | 100 | // +----+--------------------------------+-------------+ Python from pyflink.common import Row, RowKind from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.table import DataTypes, StreamTableEnvironment, Schema env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # === EXAMPLE 1 === # create a changelog DataStream ds = env.from_collection([ Row.of_kind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.of_kind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW([Types.STRING(),Types.INT()])) # interpret the DataStream as a Table table = t_env.from_changelog_stream(ds) # register the table under a name and perform an aggregation t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, table) t_env.execute_sql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;).print() # prints: # +----+--------------------------------+-------------+ # | op | name | score | # +----+--------------------------------+-------------+ # | +I | Bob | 5 | # | +I | Alice | 12 | # | -D | Alice | 12 | # | +I | Alice | 100 | # +----+--------------------------------+-------------+ # === EXAMPLE 2 === # interpret the stream as an upsert stream (without a need for UPDATE_BEFORE) # create a changelog DataStream ds = env.from_collection([ Row.of_kind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.of_kind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.of_kind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)], type_info=Types.ROW([Types.STRING(),Types.INT()])) # interpret the DataStream as a Table table = t_env.from_changelog_stream( ds, Schema.new_builder().primary_key(\u0026#34;f0\u0026#34;).build(), ChangelogMode.upsert()) # register the table under a name and perform an aggregation t_env.create_temporary_view(\u0026#34;InputTable\u0026#34;, table) t_env.execute_sql(\u0026#34;SELECT f0 AS name, SUM(f1) AS score FROM InputTable GROUP BY f0\u0026#34;).print() # prints: # +----+--------------------------------+-------------+ # | op | name | score | # +----+--------------------------------+-------------+ # | +I | Bob | 5 | # | +I | Alice | 12 | # | -U | Alice | 12 | # | +U | Alice | 100 | # +----+--------------------------------+-------------+ The default ChangelogMode shown in example 1 should be sufficient for most use cases as it accepts all kinds of changes.\nHowever, example 2 shows how to limit the kinds of incoming changes for efficiency by reducing the number of update messages by 50% using upsert mode. The number of result messages can be reduced by defining a primary key and upsert changelog mode for toChangelogStream.\nExamples for toChangelogStream # The following code shows how to use toChangelogStream for different scenarios.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.functions.ProcessFunction; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.data.StringData; import org.apache.flink.types.Row; import org.apache.flink.util.Collector; import static org.apache.flink.table.api.Expressions.*; // create Table with event-time tableEnv.executeSql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;); Table table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;); // === EXAMPLE 1 === // convert to DataStream in the simplest and most general way possible (no event-time) Table simpleTable = tableEnv .fromValues(row(\u0026#34;Alice\u0026#34;, 12), row(\u0026#34;Alice\u0026#34;, 2), row(\u0026#34;Bob\u0026#34;, 12)) .as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) .groupBy($(\u0026#34;name\u0026#34;)) .select($(\u0026#34;name\u0026#34;), $(\u0026#34;score\u0026#34;).sum()); tableEnv .toChangelogStream(simpleTable) .executeAndCollect() .forEachRemaining(System.out::println); // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] // === EXAMPLE 2 === // convert to DataStream in the simplest and most general way possible (with event-time) DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream(table); // since `event_time` is a single time attribute in the schema, it is set as the // stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row dataStream.process( new ProcessFunction\u0026lt;Row, Void\u0026gt;() { @Override public void processElement(Row row, Context ctx, Collector\u0026lt;Void\u0026gt; out) { // prints: [name, score, event_time] System.out.println(row.getFieldNames(true)); // timestamp exists twice assert ctx.timestamp() == row.\u0026lt;Instant\u0026gt;getFieldAs(\u0026#34;event_time\u0026#34;).toEpochMilli(); } }); env.execute(); // === EXAMPLE 3 === // convert to DataStream but write out the time attribute as a metadata column which means // it is not part of the physical schema anymore DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream( table, Schema.newBuilder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()); // the stream record\u0026#39;s timestamp is defined by the metadata; it is not part of the Row dataStream.process( new ProcessFunction\u0026lt;Row, Void\u0026gt;() { @Override public void processElement(Row row, Context ctx, Collector\u0026lt;Void\u0026gt; out) { // prints: [name, score] System.out.println(row.getFieldNames(true)); // timestamp exists once System.out.println(ctx.timestamp()); } }); env.execute(); // === EXAMPLE 4 === // for advanced users, it is also possible to use more internal data structures for efficiency // note that this is only mentioned here for completeness because using internal data structures // adds complexity and additional type handling // however, converting a TIMESTAMP_LTZ column to `Long` or STRING to `byte[]` might be convenient, // also structured types can be represented as `Row` if needed DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream( table, Schema.newBuilder() .column( \u0026#34;name\u0026#34;, DataTypes.STRING().bridgedTo(StringData.class)) .column( \u0026#34;score\u0026#34;, DataTypes.INT()) .column( \u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3).bridgedTo(Long.class)) .build()); // leads to a stream of Row(name: StringData, score: Integer, event_time: Long) Scala import org.apache.flink.api.scala._ import org.apache.flink.streaming.api.functions.ProcessFunction import org.apache.flink.streaming.api.scala.DataStream import org.apache.flink.table.api._ import org.apache.flink.types.Row import org.apache.flink.util.Collector import java.time.Instant // create Table with event-time tableEnv.executeSql( \u0026#34;\u0026#34;\u0026#34; CREATE TABLE GeneratedTable ( name STRING, score INT, event_time TIMESTAMP_LTZ(3), WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;) \u0026#34;\u0026#34;\u0026#34; ) val table = tableEnv.from(\u0026#34;GeneratedTable\u0026#34;) // === EXAMPLE 1 === // convert to DataStream in the simplest and most general way possible (no event-time) val simpleTable = tableEnv .fromValues(row(\u0026#34;Alice\u0026#34;, 12), row(\u0026#34;Alice\u0026#34;, 2), row(\u0026#34;Bob\u0026#34;, 12)) .as(\u0026#34;name\u0026#34;, \u0026#34;score\u0026#34;) .groupBy($\u0026#34;name\u0026#34;) .select($\u0026#34;name\u0026#34;, $\u0026#34;score\u0026#34;.sum()) tableEnv .toChangelogStream(simpleTable) .executeAndCollect() .foreach(println) // prints: // +I[Bob, 12] // +I[Alice, 12] // -U[Alice, 12] // +U[Alice, 14] // === EXAMPLE 2 === // convert to DataStream in the simplest and most general way possible (with event-time) val dataStream: DataStream[Row] = tableEnv.toChangelogStream(table) // since `event_time` is a single time attribute in the schema, it is set as the // stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row dataStream.process(new ProcessFunction[Row, Unit] { override def processElement( row: Row, ctx: ProcessFunction[Row, Unit]#Context, out: Collector[Unit]): Unit = { // prints: [name, score, event_time] println(row.getFieldNames(true)) // timestamp exists twice assert(ctx.timestamp() == row.getFieldAs[Instant](\u0026#34;event_time\u0026#34;).toEpochMilli) } }) env.execute() // === EXAMPLE 3 === // convert to DataStream but write out the time attribute as a metadata column which means // it is not part of the physical schema anymore val dataStream: DataStream[Row] = tableEnv.toChangelogStream( table, Schema.newBuilder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .columnByMetadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()) // the stream record\u0026#39;s timestamp is defined by the metadata; it is not part of the Row dataStream.process(new ProcessFunction[Row, Unit] { override def processElement( row: Row, ctx: ProcessFunction[Row, Unit]#Context, out: Collector[Unit]): Unit = { // prints: [name, score] println(row.getFieldNames(true)) // timestamp exists once println(ctx.timestamp()) } }) env.execute() // === EXAMPLE 4 === // for advanced users, it is also possible to use more internal data structures for better // efficiency // note that this is only mentioned here for completeness because using internal data structures // adds complexity and additional type handling // however, converting a TIMESTAMP_LTZ column to `Long` or STRING to `byte[]` might be convenient, // also structured types can be represented as `Row` if needed val dataStream: DataStream[Row] = tableEnv.toChangelogStream( table, Schema.newBuilder() .column( \u0026#34;name\u0026#34;, DataTypes.STRING().bridgedTo(classOf[StringData])) .column( \u0026#34;score\u0026#34;, DataTypes.INT()) .column( \u0026#34;event_time\u0026#34;, DataTypes.TIMESTAMP_LTZ(3).bridgedTo(class[Long])) .build()) // leads to a stream of Row(name: StringData, score: Integer, event_time: Long) Python from pyflink.common import Row from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.functions import ProcessFunction from pyflink.table import DataTypes, StreamTableEnvironment, Schema from pyflink.table.expressions import col env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(env) # create Table with event-time t_env.execute_sql( \u0026#34;CREATE TABLE GeneratedTable \u0026#34; + \u0026#34;(\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; score INT,\u0026#34; + \u0026#34; event_time TIMESTAMP_LTZ(3),\u0026#34; + \u0026#34; WATERMARK FOR event_time AS event_time - INTERVAL \u0026#39;10\u0026#39; SECOND\u0026#34; + \u0026#34;)\u0026#34; + \u0026#34;WITH (\u0026#39;connector\u0026#39;=\u0026#39;datagen\u0026#39;)\u0026#34;) table = t_env.from_path(\u0026#34;GeneratedTable\u0026#34;) # === EXAMPLE 1 === # convert to DataStream in the simplest and most general way possible (no event-time) simple_table = t_env.from_elements([Row(\u0026#34;Alice\u0026#34;, 12), Row(\u0026#34;Alice\u0026#34;, 2), Row(\u0026#34;Bob\u0026#34;, 12)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;score\u0026#34;, DataTypes.INT())])) simple_table = simple_table.group_by(col(\u0026#39;name\u0026#39;)).select(col(\u0026#39;name\u0026#39;), col(\u0026#39;score\u0026#39;).sum) t_env.to_changelog_stream(simple_table).print() env.execute() # prints: # +I[Bob, 12] # +I[Alice, 12] # -U[Alice, 12] # +U[Alice, 14] # === EXAMPLE 2 === # convert to DataStream in the simplest and most general way possible (with event-time) ds = t_env.to_changelog_stream(table) # since `event_time` is a single time attribute in the schema, it is set as the # stream record\u0026#39;s timestamp by default; however, at the same time, it remains part of the Row class MyProcessFunction(ProcessFunction): def process_element(self, row, ctx): print(row) assert ctx.timestamp() == row.event_time.to_epoch_milli() ds.process(MyProcessFunction()) env.execute() # === EXAMPLE 3 === # convert to DataStream but write out the time attribute as a metadata column which means # it is not part of the physical schema anymore ds = t_env.to_changelog_stream( table, Schema.new_builder() .column(\u0026#34;name\u0026#34;, \u0026#34;STRING\u0026#34;) .column(\u0026#34;score\u0026#34;, \u0026#34;INT\u0026#34;) .column_by_metadata(\u0026#34;rowtime\u0026#34;, \u0026#34;TIMESTAMP_LTZ(3)\u0026#34;) .build()) class MyProcessFunction(ProcessFunction): def process_element(self, row, ctx): print(row) print(ctx.timestamp()) ds.process(MyProcessFunction()) env.execute() For more information about which conversions are supported for data types in Example 4, see the Table API\u0026rsquo;s Data Types page.\nThe behavior of toChangelogStream(Table).executeAndCollect() is equal to calling Table.execute().collect(). However, toChangelogStream(Table) might be more useful for tests because it allows to access the produced watermarks in a subsequent ProcessFunction in DataStream API.\nBack to top\nAdding Table API Pipelines to DataStream API # A single Flink job can consist of multiple disconnected pipelines that run next to each other.\nSource-to-sink pipelines defined in Table API can be attached as a whole to the StreamExecutionEnvironment and will be submitted when calling one of the execute methods in the DataStream API.\nHowever, a source does not necessarily have to be a table source but can also be another DataStream pipeline that was converted to Table API before. Thus, it is possible to use table sinks for DataStream API programs.\nThe functionality is available through a specialized StreamStatementSet instance created with StreamTableEnvironment.createStatementSet(). By using a statement set, the planner can optimize all added statements together and come up with one or more end-to-end pipelines that are added to the StreamExecutionEnvironment when calling StreamStatementSet.attachAsDataStream().\nThe following example shows how to add table programs to a DataStream API program within one job.\nJava import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.sink.DiscardingSink; import org.apache.flink.table.api.*; import org.apache.flink.table.api.bridge.java.*; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); StreamStatementSet statementSet = tableEnv.createStatementSet(); // create some source TableDescriptor sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) .schema( Schema.newBuilder() .column(\u0026#34;myCol\u0026#34;, DataTypes.INT()) .column(\u0026#34;myOtherCol\u0026#34;, DataTypes.BOOLEAN()) .build()) .build(); // create some sink TableDescriptor sinkDescriptor = TableDescriptor.forConnector(\u0026#34;print\u0026#34;).build(); // add a pure Table API pipeline Table tableFromSource = tableEnv.from(sourceDescriptor); statementSet.add(tableFromSource.insertInto(sinkDescriptor)); // use table sinks for the DataStream API pipeline DataStream\u0026lt;Integer\u0026gt; dataStream = env.fromElements(1, 2, 3); Table tableFromStream = tableEnv.fromDataStream(dataStream); statementSet.add(tableFromStream.insertInto(sinkDescriptor)); // attach both pipelines to StreamExecutionEnvironment // (the statement set will be cleared after calling this method) statementSet.attachAsDataStream(); // define other DataStream API parts env.fromElements(4, 5, 6).addSink(new DiscardingSink\u0026lt;\u0026gt;()); // use DataStream API to submit the pipelines env.execute(); // prints similar to: // +I[1618440447, false] // +I[1259693645, true] // +I[158588930, false] // +I[1] // +I[2] // +I[3] Scala import org.apache.flink.streaming.api.functions.sink.DiscardingSink import org.apache.flink.streaming.api.scala._ import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) val statementSet = tableEnv.createStatementSet() // create some source val sourceDescriptor = TableDescriptor.forConnector(\u0026#34;datagen\u0026#34;) .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) .schema(Schema.newBuilder .column(\u0026#34;myCol\u0026#34;, DataTypes.INT) .column(\u0026#34;myOtherCol\u0026#34;, DataTypes.BOOLEAN).build) .build // create some sink val sinkDescriptor = TableDescriptor.forConnector(\u0026#34;print\u0026#34;).build // add a pure Table API pipeline val tableFromSource = tableEnv.from(sourceDescriptor) statementSet.add(tableFromSource.insertInto(sinkDescriptor)) // use table sinks for the DataStream API pipeline val dataStream = env.fromElements(1, 2, 3) val tableFromStream = tableEnv.fromDataStream(dataStream) statementSet.add(tableFromStream.insertInto(sinkDescriptor)) // attach both pipelines to StreamExecutionEnvironment // (the statement set will be cleared calling this method) statementSet.attachAsDataStream() // define other DataStream API parts env.fromElements(4, 5, 6).addSink(new DiscardingSink[Int]()) // now use DataStream API to submit the pipelines env.execute() // prints similar to: // +I[1618440447, false] // +I[1259693645, true] // +I[158588930, false] // +I[1] // +I[2] // +I[3] Python from pyflink.common import Encoder from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink from pyflink.table import StreamTableEnvironment, TableDescriptor, Schema, DataTypes env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) statement_set = table_env.create_statement_set() # create some source source_descriptor = TableDescriptor.for_connector(\u0026#34;datagen\u0026#34;) \\ .option(\u0026#34;number-of-rows\u0026#34;, \u0026#34;3\u0026#34;) \\ .schema( Schema.new_builder() .column(\u0026#34;my_col\u0026#34;, DataTypes.INT()) .column(\u0026#34;my_other_col\u0026#34;, DataTypes.BOOLEAN()) .build()) \\ .build() # create some sink sink_descriptor = TableDescriptor.for_connector(\u0026#34;print\u0026#34;).build() # add a pure Table API pipeline table_from_source = table_env.from_descriptor(source_descriptor) statement_set.add_insert(sink_descriptor, table_from_source) # use table sinks for the DataStream API pipeline data_stream = env.from_collection([1, 2, 3]) table_from_stream = table_env.from_data_stream(data_stream) statement_set.add_insert(sink_descriptor, table_from_stream) # attach both pipelines to StreamExecutionEnvironment # (the statement set will be cleared after calling this method) statement_set.attach_as_datastream() # define other DataStream API parts env.from_collection([4, 5, 6]) \\ .add_sink(FileSink .for_row_format(\u0026#39;/tmp/output\u0026#39;, Encoder.simple_string_encoder()) .build()) # use DataStream API to submit the pipelines env.execute() # prints similar to: # +I[1618440447, false] # +I[1259693645, true] # +I[158588930, false] # +I[1] # +I[2] # +I[3] Back to top\nImplicit Conversions in Scala # Users of the Scala API can use all the conversion methods above in a more fluent way by leveraging Scala\u0026rsquo;s implicit feature.\nThose implicits are available in the API when importing the package object via org.apache.flink.table.api.bridge.scala._.\nIf enabled, methods such as toTable or toChangelogTable can be called directly on a DataStream object. Similarly, toDataStream and toChangelogStream are available on Table objects. Furthermore, Table objects will be converted to a changelog stream when requesting a DataStream API specific method for DataStream[Row].\nThe use of an implicit conversion should always be a conscious decision. One should pay attention whether the IDE proposes an actual Table API method, or a DataStream API method via implicits.\nFor example, a table.execute().collect() stays in Table API whereas table.executeAndCollect() implicitly uses the DataStream API\u0026rsquo;s executeAndCollect() method and therefore forces an API conversion.\nimport org.apache.flink.streaming.api.scala._ import org.apache.flink.table.api.bridge.scala._ import org.apache.flink.types.Row val env = StreamExecutionEnvironment.getExecutionEnvironment val tableEnv = StreamTableEnvironment.create(env) val dataStream: DataStream[(Int, String)] = env.fromElements((42, \u0026#34;hello\u0026#34;)) // call toChangelogTable() implicitly on the DataStream object val table: Table = dataStream.toChangelogTable(tableEnv) // force implicit conversion val dataStreamAgain1: DataStream[Row] = table // call toChangelogStream() implicitly on the Table object val dataStreamAgain2: DataStream[Row] = table.toChangelogStream Back to top\nMapping between TypeInformation and DataType # The DataStream API uses instances of org.apache.flink.api.common.typeinfo.TypeInformation to describe the record type that travels in the stream. In particular, it defines how to serialize and deserialize records from one DataStream operator to the other. It also helps in serializing state into savepoints and checkpoints.\nThe Table API uses custom data structures to represent records internally and exposes org.apache.flink.table.types.DataType to users for declaring the external format into which the data structures are converted for easier usage in sources, sinks, UDFs, or DataStream API.\nDataType is richer than TypeInformation as it also includes details about the logical SQL type. Therefore, some details will be added implicitly during the conversion.\nColumn names and types of a Table are automatically derived from the TypeInformation of the DataStream. Use DataStream.getType() to check whether the type information has been detected correctly via the DataStream API\u0026rsquo;s reflective type extraction facilities. If the outermost record\u0026rsquo;s TypeInformation is a CompositeType, it will be flattened in the first level when deriving a table\u0026rsquo;s schema.\nThe DataStream API is not always able to extract a more specific TypeInformation based on reflection. This often happens silently and leads to GenericTypeInfo that is backed by the generic Kryo serializer.\nFor example, the Row class cannot be analyzed reflectively and always needs an explicit type information declaration. If no proper type information is declared in DataStream API, the row will show as RAW data type and the Table API is unable to access its fields. Use .map(...).returns(TypeInformation) in Java or .map(...)(TypeInformation) in Scala to declare type information explicitly.\nTypeInformation to DataType # The following rules apply when converting TypeInformation to a DataType:\nAll subclasses of TypeInformation are mapped to logical types, including nullability that is aligned with Flink\u0026rsquo;s built-in serializers.\nSubclasses of TupleTypeInfoBase are translated into a row (for Row) or structured type (for tuples, POJOs, and case classes).\nBigDecimal is converted to DECIMAL(38, 18) by default.\nThe order of PojoTypeInfo fields is determined by a constructor with all fields as its parameters. If that is not found during the conversion, the field order will be alphabetical.\nGenericTypeInfo and other TypeInformation that cannot be represented as one of the listed org.apache.flink.table.api.DataTypes will be treated as a black-box RAW type. The current session configuration is used to materialize the serializer of the raw type. Composite nested fields will not be accessible then.\nSee TypeInfoDataTypeConverter for the full translation logic.\nUse DataTypes.of(TypeInformation) to call the above logic in custom schema declaration or in UDFs.\nDataType to TypeInformation # The table runtime will make sure to properly serialize the output records to the first operator of the DataStream API.\nAfterward, the type information semantics of the DataStream API need to be considered. Back to top\nLegacy Conversion # The following section describes outdated parts of the API that will be removed in future versions.\nIn particular, these parts might not be well integrated into many recent new features and refactorings (e.g. RowKind is not correctly set, type systems don\u0026rsquo;t integrate smoothly).\n将 DataStream 转换成表 # DataStream 可以直接转换为 StreamTableEnvironment 中的 Table。 结果视图的架构取决于注册集合的数据类型。\nJava StreamTableEnvironment tableEnv = ...; DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; stream = ...; Table table2 = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myString\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? val stream: DataStream[(Long, String)] = ??? val table2: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) Python t_env = ... # type: StreamTableEnvironment stream = ... # type: DataStream of Types.TUPLE([Types.LONG(), Types.STRING()]) table2 = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_stream\u0026#39;)) Back to top\n将表转换成 DataStream # Table 可以被转换成 DataStream。 通过这种方式，定制的 DataStream 程序就可以在 Table API 或者 SQL 的查询结果上运行了。\n将 Table 转换为 DataStream 时，你需要指定生成的 DataStream 的数据类型，即，Table 的每行数据要转换成的数据类型。 通常最方便的选择是转换成 Row 。 以下列表概述了不同选项的功能：\nRow: 字段按位置映射，字段数量任意，支持 null 值，无类型安全（type-safe）检查。 POJO: 字段按名称映射（POJO 必须按Table 中字段名称命名），字段数量任意，支持 null 值，无类型安全检查。 Case Class: 字段按位置映射，不支持 null 值，有类型安全检查。 Tuple: 字段按位置映射，字段数量少于 22（Scala）或者 25（Java），不支持 null 值，无类型安全检查。 Atomic Type: Table 必须有一个字段，不支持 null 值，有类型安全检查。 将表转换成 DataStream # 流式查询（streaming query）的结果表会动态更新，即，当新纪录到达查询的输入流时，查询结果会改变。因此，像这样将动态查询结果转换成 DataStream 需要对表的更新方式进行编码。\n将 Table 转换为 DataStream 有两种模式：\nAppend Mode: 仅当动态 Table 仅通过INSERT更改进行修改时，才可以使用此模式，即，它仅是追加操作，并且之前输出的结果永远不会更新。 Retract Mode: 任何情形都可以使用此模式。它使用 boolean 值对 INSERT 和 DELETE 操作的数据进行标记。 Java StreamTableEnvironment tableEnv = ...; Table table = tableEnv.fromValues( DataTypes.Row( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT()), row(\u0026#34;john\u0026#34;, 35), row(\u0026#34;sarah\u0026#34;, 32)); // Convert the Table into an append DataStream of Row by specifying the class DataStream\u0026lt;Row\u0026gt; dsRow = tableEnv.toAppendStream(table, Row.class); // Convert the Table into an append DataStream of Tuple2\u0026lt;String, Integer\u0026gt; with TypeInformation TupleTypeInfo\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; tupleType = new TupleTypeInfo\u0026lt;\u0026gt;(Types.STRING(), Types.INT()); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dsTuple = tableEnv.toAppendStream(table, tupleType); // Convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream\u0026lt;Tuple2\u0026lt;Boolean, X\u0026gt;\u0026gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. DataStream\u0026lt;Tuple2\u0026lt;Boolean, Row\u0026gt;\u0026gt; retractStream = tableEnv.toRetractStream(table, Row.class); Scala val tableEnv: StreamTableEnvironment = ??? // Table with two fields (String name, Integer age) val table: Table = tableEnv.fromValues( DataTypes.Row( DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;age\u0026#34;, DataTypes.INT()), row(\u0026#34;john\u0026#34;, 35), row(\u0026#34;sarah\u0026#34;, 32)) // Convert the Table into an append DataStream of Row by specifying the class val dsRow: DataStream[Row] = tableEnv.toAppendStream[Row](table) // Convert the Table into an append DataStream of (String, Integer) with TypeInformation val dsTuple: DataStream[(String, Int)] dsTuple = tableEnv.toAppendStream[(String, Int)](table) // Convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream\u0026lt;Tuple2\u0026lt;Boolean, X\u0026gt;\u0026gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. val retractStream: DataStream[(Boolean, Row)] = tableEnv.toRetractStream[Row](table) 注意： 文档动态表给出了有关动态表及其属性的详细讨论。\n注意 一旦 Table 被转化为 DataStream，必须使用 StreamExecutionEnvironment 的 execute 方法执行该 DataStream 作业。\nBack to top\n数据类型到 Table Schema 的映射 # Flink 的 DataStream API 支持多样的数据类型。 例如 Tuple（Scala 内置，Flink Java tuple 和 Python tuples）、POJO 类型、Scala case class 类型以及 Flink 的 Row 类型等允许嵌套且有多个可在表的表达式中访问的字段的复合数据类型。其他类型被视为原子类型。下面，我们讨论 Table API 如何将这些数据类型类型转换为内部 row 表示形式，并提供将 DataStream 转换成 Table 的样例。\n数据类型到 table schema 的映射有两种方式：基于字段位置或基于字段名称。\n基于位置映射\n基于位置的映射可在保持字段顺序的同时为字段提供更有意义的名称。这种映射方式可用于具有特定的字段顺序的复合数据类型以及原子类型。如 tuple、row 以及 case class 这些复合数据类型都有这样的字段顺序。然而，POJO 类型的字段则必须通过名称映射（参见下一章）。可以将字段投影出来，但不能使用as(Java 和 Scala) 或者 alias(Python)重命名。\n定义基于位置的映射时，输入数据类型中一定不能存在指定的名称，否则 API 会假定应该基于字段名称进行映射。如果未指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 f0 表示原子类型。\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section; DataStream\u0026lt;Tuple2\u0026lt;Long, Integer\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with field \u0026#34;myLong\u0026#34; only Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;)); // convert DataStream into Table with field names \u0026#34;myLong\u0026#34; and \u0026#34;myInt\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myInt\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with field \u0026#34;myLong\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) // convert DataStream into Table with field names \u0026#34;myLong\u0026#34; and \u0026#34;myInt\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myInt\u0026#34;) Python from pyflink.table.expressions import col # get a TableEnvironment t_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section stream = ... # type: DataStream of Types.Tuple([Types.LONG(), Types.INT()]) # convert DataStream into Table with field \u0026#34;my_long\u0026#34; only table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;)) # convert DataStream into Table with field names \u0026#34;my_long\u0026#34; and \u0026#34;my_int\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_int\u0026#39;)) 基于名称的映射\n基于名称的映射适用于任何数据类型包括 POJO 类型。这是定义 table schema 映射最灵活的方式。映射中的所有字段均按名称引用，并且可以通过 as 重命名。字段可以被重新排序和映射。\n若果没有指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 f0 表示原子类型。\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section DataStream\u0026lt;Tuple2\u0026lt;Long, Integer\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with field \u0026#34;f1\u0026#34; only Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;)); // convert DataStream into Table with swapped fields Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;), $(\u0026#34;f0\u0026#34;)); // convert DataStream into Table with swapped fields and field names \u0026#34;myInt\u0026#34; and \u0026#34;myLong\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;).as(\u0026#34;myInt\u0026#34;), $(\u0026#34;f0\u0026#34;).as(\u0026#34;myLong\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, Int)] = ... // convert DataStream into Table with field \u0026#34;_2\u0026#34; only val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with swapped fields val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with swapped fields and field names \u0026#34;myInt\u0026#34; and \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myInt\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) Python from pyflink.table.expressions import col # get a TableEnvironment t_env = ... # see \u0026#34;Create a TableEnvironment\u0026#34; section stream = ... # type: DataStream of Types.Tuple([Types.LONG(), Types.INT()]) # convert DataStream into Table with field \u0026#34;f1\u0026#34; only table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;)) # convert DataStream into Table with swapped fields table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;), col(\u0026#39;f0\u0026#39;)) # convert DataStream into Table with swapped fields and field names \u0026#34;my_int\u0026#34; and \u0026#34;my_long\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;).alias(\u0026#39;my_int\u0026#39;), col(\u0026#39;f0\u0026#39;).alias(\u0026#39;my_long\u0026#39;)) 原子类型 # Flink 将基础数据类型（Integer、Double、String）或者通用数据类型（不可再拆分的数据类型）视为原子类型。 原子类型的 DataStream 会被转换成只有一条属性的 Table。 属性的数据类型可以由原子类型推断出，还可以重新命名属性。\nJava StreamTableEnvironment tableEnv = ...; DataStream\u0026lt;Long\u0026gt; stream = ...; // Convert DataStream into Table with field name \u0026#34;myLong\u0026#34; Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? val stream: DataStream[Long] = ... // Convert DataStream into Table with default field name \u0026#34;f0\u0026#34; val table: Table = tableEnv.fromDataStream(stream) // Convert DataStream into Table with field name \u0026#34;myLong\u0026#34; val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;) Python from pyflink.table.expressions import col t_env = ... stream = ... # types: DataStream of Types.Long() # Convert DataStream into Table with default field name \u0026#34;f0\u0026#34; table = t_env.from_data_stream(stream) # Convert DataStream into Table with field name \u0026#34;my_long\u0026#34; table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;)) Tuple类型（Scala , Java 和 Python）和 Case Class类型（仅 Scala） # Flink 支持 Scala 的内置 tuple 类型并给 Java 提供自己的 tuple 类型。 两种 tuple 的 DataStream 都能被转换成表。 可以通过提供所有字段名称来重命名字段（基于位置映射）。 如果没有指明任何字段名称，则会使用默认的字段名称。 如果引用了原始字段名称（对于 Flink tuple 为f0、f1 \u0026hellip; \u0026hellip;，对于 Scala tuple 为_1、_2 \u0026hellip; \u0026hellip;），则 API 会假定映射是基于名称的而不是基于位置的。 基于名称的映射可以通过 as 对字段和投影进行重新排序。\nJava Flink 给 Java 提供自己的 tuple 类型。 tuple 的 DataStream 都能被转换成表。 可以通过提供所有字段名称来重命名字段（基于位置映射）。 如果没有指明任何字段名称，则会使用默认的字段名称。 如果引用了原始字段名称（对于 Flink tuple 为f0、f1 \u0026hellip; \u0026hellip;），则 API 会假定映射是基于名称的而不是基于位置的。 基于名称的映射可以通过 as 对字段和投影进行重新排序。\nStreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section DataStream\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; stream = ...; // convert DataStream into Table with renamed field names \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; (position-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myLong\u0026#34;), $(\u0026#34;myString\u0026#34;)); // convert DataStream into Table with reordered fields \u0026#34;f1\u0026#34;, \u0026#34;f0\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;), $(\u0026#34;f0\u0026#34;)); // convert DataStream into Table with projected field \u0026#34;f1\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;)); // convert DataStream into Table with reordered and aliased fields \u0026#34;myString\u0026#34;, \u0026#34;myLong\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;f1\u0026#34;).as(\u0026#34;myString\u0026#34;), $(\u0026#34;f0\u0026#34;).as(\u0026#34;myLong\u0026#34;)); Scala Flink 支持 Scala 的内置 tuple 类型。 tuple 的 DataStream 都能被转换成表。 可以通过提供所有字段名称来重命名字段（基于位置映射）。 如果没有指明任何字段名称，则会使用默认的字段名称。 如果引用了原始字段名称（对于 Scala tuple 为_1、_2 \u0026hellip; \u0026hellip;），则 API 会假定映射是基于名称的而不是基于位置的。 基于名称的映射可以通过 as 对字段和投影进行重新排序。\n// get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section val stream: DataStream[(Long, String)] = ... // convert DataStream into Table with field names \u0026#34;myLong\u0026#34;, \u0026#34;myString\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myLong\u0026#34;, $\u0026#34;myString\u0026#34;) // convert DataStream into Table with reordered fields \u0026#34;_2\u0026#34;, \u0026#34;_1\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;, $\u0026#34;_1\u0026#34;) // convert DataStream into Table with projected field \u0026#34;_2\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myString\u0026#34;, \u0026#34;myLong\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;_2\u0026#34; as \u0026#34;myString\u0026#34;, $\u0026#34;_1\u0026#34; as \u0026#34;myLong\u0026#34;) // define case class case class Person(name: String, age: Int) val streamCC: DataStream[Person] = ... // convert DataStream into Table with field names \u0026#39;myName, \u0026#39;myAge (position-based) val table = tableEnv.fromDataStream(streamCC, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // convert DataStream into Table with reordered and aliased fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python Flink 支持 Python 的内置 tuple 类型。 tuple 的 DataStream 都能被转换成表。 可以通过提供所有字段名称来重命名字段（基于位置映射）。 如果没有指明任何字段名称，则会使用默认的字段名称。 如果引用了原始字段名称（对于 Python tuple 为f0、f1 \u0026hellip; \u0026hellip;），则 API 会假定映射是基于名称的而不是基于位置的。 基于名称的映射可以通过 alias 对字段和投影进行重新排序。\nfrom pyflink.table.expressions import col stream = ... # type: DataStream of Types.TUPLE([Types.LONG(), Types.STRING()]) # convert DataStream into Table with renamed field names \u0026#34;my_long\u0026#34;, \u0026#34;my_string\u0026#34; (position-based) table = t_env.from_data_stream(stream, col(\u0026#39;my_long\u0026#39;), col(\u0026#39;my_string\u0026#39;)) # convert DataStream into Table with reordered fields \u0026#34;f1\u0026#34;, \u0026#34;f0\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;), col(\u0026#39;f0\u0026#39;)) # convert DataStream into Table with projected field \u0026#34;f1\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;)) # convert DataStream into Table with reordered and aliased fields \u0026#34;my_string\u0026#34;, \u0026#34;my_long\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;f1\u0026#39;).alias(\u0026#39;my_string\u0026#39;), col(\u0026#39;f0\u0026#39;).alias(\u0026#39;my_long\u0026#39;)) POJO 类型 （Java 和 Scala） # Flink 支持 POJO 类型作为复合类型。确定 POJO 类型的规则记录在这里.\n在不指定字段名称的情况下将 POJO 类型的 DataStream 转换成 Table 时，将使用原始 POJO 类型字段的名称。名称映射需要原始名称，并且不能按位置进行。字段可以使用别名（带有 as 关键字）来重命名，重新排序和投影。\nJava StreamTableEnvironment tableEnv = ...; // see \u0026#34;Create a TableEnvironment\u0026#34; section // Person is a POJO with fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; DataStream\u0026lt;Person\u0026gt; stream = ...; // convert DataStream into Table with renamed fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;age\u0026#34;).as(\u0026#34;myAge\u0026#34;), $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;)); // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); Scala // get a TableEnvironment val tableEnv: StreamTableEnvironment = ... // see \u0026#34;Create a TableEnvironment\u0026#34; section // Person is a POJO with field names \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; val stream: DataStream[Person] = ... // convert DataStream into Table with renamed fields \u0026#34;myAge\u0026#34;, \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) // convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python PyFlink 暂时还不支持自定义 PoJo 类型 Row类型 # Row 类型支持任意数量的字段以及具有 null 值的字段。字段名称可以通过 RowTypeInfo 指定，也可以在将 Row 的 DataStream 转换为 Table 时指定。 Row 类型的字段映射支持基于名称和基于位置两种方式。 字段可以通过提供所有字段的名称的方式重命名（基于位置映射）或者分别选择进行投影/排序/重命名（基于名称映射）。\nJava StreamTableEnvironment tableEnv = ...; // DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` DataStream\u0026lt;Row\u0026gt; stream = ...; // Convert DataStream into Table with renamed field names \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (position-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;myName\u0026#34;), $(\u0026#34;myAge\u0026#34;)); // Convert DataStream into Table with renamed fields \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;), $(\u0026#34;age\u0026#34;).as(\u0026#34;myAge\u0026#34;)); // Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;)); // Convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) Table table = tableEnv.fromDataStream(stream, $(\u0026#34;name\u0026#34;).as(\u0026#34;myName\u0026#34;)); Scala val tableEnv: StreamTableEnvironment = ??? // DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` val stream: DataStream[Row] = ... // Convert DataStream into Table with renamed field names \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (position-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;myName\u0026#34;, $\u0026#34;myAge\u0026#34;) // Convert DataStream into Table with renamed fields \u0026#34;myName\u0026#34;, \u0026#34;myAge\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;, $\u0026#34;age\u0026#34; as \u0026#34;myAge\u0026#34;) // Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34;) // Convert DataStream into Table with projected and renamed field \u0026#34;myName\u0026#34; (name-based) val table: Table = tableEnv.fromDataStream(stream, $\u0026#34;name\u0026#34; as \u0026#34;myName\u0026#34;) Python from pyflink.table.expressions import col t_env = ...; # DataStream of Row with two fields \u0026#34;name\u0026#34; and \u0026#34;age\u0026#34; specified in `RowTypeInfo` stream = ... # Convert DataStream into Table with renamed field names \u0026#34;my_name\u0026#34;, \u0026#34;my_age\u0026#34; (position-based) table = t_env.from_data_stream(stream, col(\u0026#39;my_name\u0026#39;), col(\u0026#39;my_age\u0026#39;)) # Convert DataStream into Table with renamed fields \u0026#34;my_name\u0026#34;, \u0026#34;my_age\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;).alias(\u0026#39;my_name\u0026#39;), col(\u0026#39;age\u0026#39;).alias(\u0026#39;my_age\u0026#39;)) # Convert DataStream into Table with projected field \u0026#34;name\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;)) # Convert DataStream into Table with projected and renamed field \u0026#34;my_name\u0026#34; (name-based) table = t_env.from_data_stream(stream, col(\u0026#39;name\u0026#39;).alias(\u0026#34;my_name\u0026#34;)) Back to top\n"}),e.add({id:65,href:"/flink/flink-docs-master/zh/docs/learn-flink/datastream_api/",title:"DataStream API 简介",section:"实践练习",content:` DataStream API 简介 # 该练习的重点是充分全面地了解 DataStream API，以便于编写流式应用入门。
什么能被转化成流？ # Flink 的 Java 和 Scala DataStream API 可以将任何可序列化的对象转化为流。Flink 自带的序列化器有
基本类型，即 String、Long、Integer、Boolean、Array 复合类型：Tuples、POJOs 和 Scala case classes 而且 Flink 会交给 Kryo 序列化其他类型。也可以将其他序列化器和 Flink 一起使用。特别是有良好支持的 Avro。
Java tuples 和 POJOs # Flink 的原生序列化器可以高效地操作 tuples 和 POJOs
Tuples # 对于 Java，Flink 自带有 Tuple0 到 Tuple25 类型。
Tuple2\u0026lt;String, Integer\u0026gt; person = Tuple2.of(\u0026#34;Fred\u0026#34;, 35); // zero based index! String name = person.f0; Integer age = person.f1; POJOs # 如果满足以下条件，Flink 将数据类型识别为 POJO 类型（并允许“按名称”字段引用）：
该类是公有且独立的（没有非静态内部类） 该类有公有的无参构造函数 类（及父类）中所有的所有不被 static、transient 修饰的属性要么是公有的（且不被 final 修饰），要么是包含公有的 getter 和 setter 方法，这些方法遵循 Java bean 命名规范。 示例：
public class Person { public String name; public Integer age; public Person() {} public Person(String name, Integer age) { . . . } } Person person = new Person(\u0026#34;Fred Flintstone\u0026#34;, 35); Flink 的序列化器支持的 POJO 类型数据结构升级。
Scala tuples 和 case classes # 如果你了解 Scala，那一定知道 tuple 和 case class。
Back to top
一个完整的示例 # 该示例将关于人的记录流作为输入，并且过滤后只包含成年人。
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.api.common.functions.FilterFunction; public class Example { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Person\u0026gt; flintstones = env.fromElements( new Person(\u0026#34;Fred\u0026#34;, 35), new Person(\u0026#34;Wilma\u0026#34;, 35), new Person(\u0026#34;Pebbles\u0026#34;, 2)); DataStream\u0026lt;Person\u0026gt; adults = flintstones.filter(new FilterFunction\u0026lt;Person\u0026gt;() { @Override public boolean filter(Person person) throws Exception { return person.age \u0026gt;= 18; } }); adults.print(); env.execute(); } public static class Person { public String name; public Integer age; public Person() {} public Person(String name, Integer age) { this.name = name; this.age = age; } public String toString() { return this.name.toString() + \u0026#34;: age \u0026#34; + this.age.toString(); } } } Stream 执行环境 # 每个 Flink 应用都需要有执行环境，在该示例中为 env。流式应用需要用到 StreamExecutionEnvironment。
DataStream API 将你的应用构建为一个 job graph，并附加到 StreamExecutionEnvironment 。当调用 env.execute() 时此 graph 就被打包并发送到 JobManager 上，后者对作业并行处理并将其子任务分发给 Task Manager 来执行。每个作业的并行子任务将在 task slot 中执行。
注意，如果没有调用 execute()，应用就不会运行。
此分布式运行时取决于你的应用是否是可序列化的。它还要求所有依赖对集群中的每个节点均可用。
基本的 stream source # 上述示例用 env.fromElements(...) 方法构造 DataStream\u0026lt;Person\u0026gt; 。这样将简单的流放在一起是为了方便用于原型或测试。StreamExecutionEnvironment 上还有一个 fromCollection(Collection) 方法。因此，你可以这样做：
List\u0026lt;Person\u0026gt; people = new ArrayList\u0026lt;Person\u0026gt;(); people.add(new Person(\u0026#34;Fred\u0026#34;, 35)); people.add(new Person(\u0026#34;Wilma\u0026#34;, 35)); people.add(new Person(\u0026#34;Pebbles\u0026#34;, 2)); DataStream\u0026lt;Person\u0026gt; flintstones = env.fromCollection(people); 另一个获取数据到流中的便捷方法是用 socket
DataStream\u0026lt;String\u0026gt; lines = env.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) 或读取文件
DataStream\u0026lt;String\u0026gt; lines = env.readTextFile(\u0026#34;file:///path\u0026#34;); 在真实的应用中，最常用的数据源是那些支持低延迟，高吞吐并行读取以及重复（高性能和容错能力为先决条件）的数据源，例如 Apache Kafka，Kinesis 和各种文件系统。REST API 和数据库也经常用于增强流处理的能力（stream enrichment）。
基本的 stream sink # 上述示例用 adults.print() 打印其结果到 task manager 的日志中（如果运行在 IDE 中时，将追加到你的 IDE 控制台）。它会对流中的每个元素都调用 toString() 方法。
输出看起来类似于
1\u0026gt; Fred: age 35 2\u0026gt; Wilma: age 35 1\u0026gt; 和 2\u0026gt; 指出输出来自哪个 sub-task（即 thread）
In production, commonly used sinks include various databases and several pub-sub systems.
调试 # 在生产中，应用程序将在远程集群或一组容器中运行。如果集群或容器挂了，这就属于远程失败。JobManager 和 TaskManager 日志对于调试此类故障非常有用，但是更简单的是 Flink 支持在 IDE 内部进行本地调试。你可以设置断点，检查局部变量，并逐行执行代码。如果想了解 Flink 的工作原理和内部细节，查看 Flink 源码也是非常好的方法。
Back to top
动手实践 # 至此，你已经可以开始编写并运行一个简单的 DataStream 应用了。 克隆 flink-training-repo 并在阅读完 README 中的指示后，开始尝试第一个练习吧： Filtering a Stream (Ride Cleansing) 。
Back to top
更多阅读 # Flink Serialization Tuning Vol. 1: Choosing your Serializer — if you can Anatomy of a Flink Program Data Sources Data Sinks DataStream Connectors Back to top
`}),e.add({id:66,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/gcs/",title:"Google Cloud Storage",section:"File Systems",content:` Google Cloud Storage # Google Cloud storage (GCS) provides cloud storage for a variety of use cases. You can use it for reading and writing data, and for checkpoint storage when using FileSystemCheckpointStorage) with the streaming state backends.
You can use GCS objects like regular files by specifying paths in the following format:
gs://\u0026lt;your-bucket\u0026gt;/\u0026lt;endpoint\u0026gt; The endpoint can either be a single file or a directory, for example:
// Read from GSC bucket env.readTextFile(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Write to GCS bucket stream.writeAsText(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); // Use GCS as checkpoint storage env.getCheckpointConfig().setCheckpointStorage(\u0026#34;gs://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt;\u0026#34;); Libraries # You must include the following jars in Flink\u0026rsquo;s lib directory to connect Flink with gcs:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-shaded-hadoop2-uber\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\${flink.shared_hadoop_latest_version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.bigdataoss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gcs-connector\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;hadoop2-2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; We have tested with flink-shared-hadoop2-uber version \u0026gt;= 2.8.5-1.8.3. You can track the latest version of the gcs-connector hadoop 2.
Authentication to access GCS # Most operations on GCS require authentication. Please see the documentation on Google Cloud Storage authentication for more information.
You can use the following method for authentication
Configure via core-site.xml You would need to add the following properties to core-site.xml
\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;google.cloud.auth.service.account.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;google.cloud.auth.service.account.json.keyfile\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;PATH TO GOOGLE AUTHENTICATION JSON\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; You would need to add the following to flink-conf.yaml
flinkConfiguration: fs.hdfs.hadoopconf: \u0026lt;DIRECTORY PATH WHERE core-site.xml IS SAVED\u0026gt; You can provide the necessary key via the GOOGLE_APPLICATION_CREDENTIALS environment variable.
Back to top
`}),e.add({id:67,href:"/flink/flink-docs-master/zh/docs/libs/gelly/",title:"Graphs",section:"Libraries",content:""}),e.add({id:68,href:"/flink/flink-docs-master/zh/docs/deployment/advanced/historyserver/",title:"History Server",section:"Advanced",content:` History Server # Flink 提供了 history server，可以在相应的 Flink 集群关闭之后查询已完成作业的统计信息。
此外，它暴露了一套 REST API，该 API 接受 HTTP 请求并返回 JSON 格式的数据。
概览 # HistoryServer 允许查询 JobManager 存档的已完成作业的状态和统计信息。
在配置 HistoryServer 和 JobManager 之后，你可以使用相应的脚本来启动和停止 HistoryServer：
# 启动或者停止 HistoryServer bin/historyserver.sh (start|start-foreground|stop) 默认情况下，此服务器绑定到 localhost 的 8082 端口。
目前，只能将 HistoryServer 作为独立的进程运行。
配置参数 # 配置项 jobmanager.archive.fs.dir 和 historyserver.archive.fs.refresh-interval 需要根据 作业存档目录 和 刷新作业存档目录的时间间隔 进行调整。
JobManager
已完成作业的存档在 JobManager 上进行，将已存档的作业信息上传到文件系统目录中。你可以在 flink-conf.yaml 文件中通过 jobmanager.archive.fs.dir 设置一个目录存档已完成的作业。
# 上传已完成作业信息的目录 jobmanager.archive.fs.dir: hdfs:///completed-jobs HistoryServer
可以通过 historyserver.archive.fs.dir 设置 HistoryServer 监视以逗号分隔的目录列表。定期轮询已配置的目录以查找新的存档；轮询间隔可以通过 historyserver.archive.fs.refresh-interval 来配置。
# 监视以下目录中已完成的作业 historyserver.archive.fs.dir: hdfs:///completed-jobs # 每 10 秒刷新一次 historyserver.archive.fs.refresh-interval: 10000 所包含的存档被下载缓存在本地文件系统中。本地目录通过 historyserver.web.tmpdir 配置。
请查看配置页面以获取配置选项的完整列表。
日志集成 # Flink 本身并不提供已完成作业的日志收集功能。 但是，如果你已经有了日志收集与浏览服务，可以配置 HistoryServer 与其集成。 （通过historyserver.log.jobmanager.url-pattern 和 historyserver.log.taskmanager.url-pattern）。 如此一来，你可以从 HistoryServer 的 WebUI 直接链接到相关 JobManager / TaskManager 的日志。
# HistoryServer 会将 \u0026lt;jobid\u0026gt; 替换为对应作业的 ID historyserver.log.jobmanager.url-pattern: http://my.log-browsing.url/\u0026lt;jobid\u0026gt; # HistoryServer 会将 \u0026lt;jobid\u0026gt; 和 \u0026lt;tmid\u0026gt; 替换为对应作业和 TaskManager 的 ID historyserver.log.taskmanager.url-pattern: http://my.log-browsing.url/\u0026lt;jobid\u0026gt;/\u0026lt;tmid\u0026gt; 可用的请求 # 以下是可用且带有示例 JSON 响应的请求列表。所有请求格式样例均为 http://hostname:8082/jobs，下面我们仅列出了 URLs 的 path 部分。 尖括号中的值为变量，例如作业 7684be6004e4e955c2a558a9bc463f65 的 http://hostname:port/jobs/\u0026lt;jobid\u0026gt;/exceptions 请求须写为 http://hostname:port/jobs/7684be6004e4e955c2a558a9bc463f65/exceptions。
/config /jobs/overview /jobs/\u0026lt;jobid\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices /jobs/\u0026lt;jobid\u0026gt;/config /jobs/\u0026lt;jobid\u0026gt;/exceptions /jobs/\u0026lt;jobid\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasktimes /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/taskmanagers /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/accumulators /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt;/attempts/\u0026lt;attempt\u0026gt; /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtasknum\u0026gt;/attempts/\u0026lt;attempt\u0026gt;/accumulators /jobs/\u0026lt;jobid\u0026gt;/plan /jobs/\u0026lt;jobid\u0026gt;/jobmanager/config /jobs/\u0026lt;jobid\u0026gt;/jobmanager/environment /jobs/\u0026lt;jobid\u0026gt;/jobmanager/log-url /jobs/\u0026lt;jobid\u0026gt;/taskmanagers/\u0026lt;taskmanagerid\u0026gt;/log-url Back to top
`}),e.add({id:69,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_dialect/",title:"Hive 方言",section:"Hive",content:` Hive 方言 # 从 1.11.0 开始，在使用 Hive 方言时，Flink 允许用户用 Hive 语法来编写 SQL 语句。通过提供与 Hive 语法的兼容性，我们旨在改善与 Hive 的互操作性，并减少用户需要在 Flink 和 Hive 之间切换来执行不同语句的情况。
使用 Hive 方言 # Flink 目前支持两种 SQL 方言: default 和 hive。你需要先切换到 Hive 方言，然后才能使用 Hive 语法编写。下面介绍如何使用 SQL 客户端和 Table API 设置方言。 还要注意，你可以为执行的每个语句动态切换方言。无需重新启动会话即可使用其他方言。
SQL 客户端 # SQL 方言可以通过 table.sql-dialect 属性指定。因此你可以通过 SQL 客户端 yaml 文件中的 configuration 部分来设置初始方言。
execution: type: batch result-mode: table configuration: table.sql-dialect: hive 你同样可以在 SQL 客户端启动后设置方言。
Flink SQL\u0026gt; set table.sql-dialect=hive; -- to use hive dialect [INFO] Session property has been set. Flink SQL\u0026gt; set table.sql-dialect=default; -- to use default dialect [INFO] Session property has been set. Table API # 你可以使用 Table API 为 TableEnvironment 设置方言。
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // to use hive dialect tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE); // to use default dialect tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT); Python from pyflink.table import * settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) # to use hive dialect t_env.get_config().set_sql_dialect(SqlDialect.HIVE) # to use default dialect t_env.get_config().set_sql_dialect(SqlDialect.DEFAULT) DDL # 本章节列出了 Hive 方言支持的 DDL 语句。我们主要关注语法。你可以参考 Hive 文档 了解每个 DDL 语句的语义。
CATALOG # Show # SHOW CURRENT CATALOG; DATABASE # Show # SHOW DATABASES; Create # CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION fs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; Alter # Update Properties # ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); Update Owner # ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; Update Location # ALTER (DATABASE|SCHEMA) database_name SET LOCATION fs_path; Drop # DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; Use # USE database_name; TABLE # Show # SHOW TABLES; Create # CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [column_constraint] [COMMENT col_comment], ... [table_constraint])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [ [ROW FORMAT row_format] [STORED AS file_format] ] [LOCATION fs_path] [TBLPROPERTIES (property_name=property_value, ...)] row_format: : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE | RCFILE | ORC | PARQUET | AVRO | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname column_constraint: : NOT NULL [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]] table_constraint: : [CONSTRAINT constraint_name] PRIMARY KEY (col_name, ...) [[ENABLE|DISABLE] [VALIDATE|NOVALIDATE] [RELY|NORELY]] Alter # Rename # ALTER TABLE table_name RENAME TO new_table_name; Update Properties # ALTER TABLE table_name SET TBLPROPERTIES (property_name = property_value, property_name = property_value, ... ); Update Location # ALTER TABLE table_name [PARTITION partition_spec] SET LOCATION fs_path; 如果指定了 partition_spec，那么必须完整，即具有所有分区列的值。如果指定了，该操作将作用在对应分区上而不是表上。
Update File Format # ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format; 如果指定了 partition_spec，那么必须完整，即具有所有分区列的值。如果指定了，该操作将作用在对应分区上而不是表上。
Update SerDe Properties # ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, ... ) 如果指定了 partition_spec，那么必须完整，即具有所有分区列的值。如果指定了，该操作将作用在对应分区上而不是表上。
Add Partitions # ALTER TABLE table_name ADD [IF NOT EXISTS] (PARTITION partition_spec [LOCATION fs_path])+; Drop Partitions # ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...]; Add/Replace Columns # ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) [CASCADE|RESTRICT] Change Column # ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT]; Drop # DROP TABLE [IF EXISTS] table_name; VIEW # Create # CREATE VIEW [IF NOT EXISTS] view_name [(column_name, ...) ] [COMMENT view_comment] [TBLPROPERTIES (property_name = property_value, ...)] AS SELECT ...; Alter # 注意: 变更视图只在 Table API 中有效，SQL 客户端不支持。
Rename # ALTER VIEW view_name RENAME TO new_view_name; Update Properties # ALTER VIEW view_name SET TBLPROPERTIES (property_name = property_value, ... ); Update As Select # ALTER VIEW view_name AS select_statement; Drop # DROP VIEW [IF EXISTS] view_name; FUNCTION # Show # SHOW FUNCTIONS; Create # CREATE FUNCTION function_name AS class_name; Drop # DROP FUNCTION [IF EXISTS] function_name; DML \u0026amp; DQL Beta # Hive 方言支持常用的 Hive DML 和 DQL 。 下表列出了一些 Hive 方言支持的语法。
SORT/CLUSTER/DISTRIBUTE BY Group By Join Union LATERAL VIEW Window Functions SubQueries CTE INSERT INTO dest schema Implicit type conversions 为了实现更好的语法和语义的兼容，强烈建议使用 HiveModule 并将其放在 Module 列表的首位，以便在函数解析时优先使用 Hive 内置函数。
Hive 方言不再支持 Flink SQL 语法 。 若需使用 Flink 语法，请切换到 default 方言。
以下是一个使用 Hive 方言的示例。
Flink SQL\u0026gt; create catalog myhive with (\u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;/opt/hive-conf\u0026#39;); [INFO] Execute statement succeed. Flink SQL\u0026gt; use catalog myhive; [INFO] Execute statement succeed. Flink SQL\u0026gt; load module hive; [INFO] Execute statement succeed. Flink SQL\u0026gt; use modules hive,core; [INFO] Execute statement succeed. Flink SQL\u0026gt; set table.sql-dialect=hive; [INFO] Session property has been set. Flink SQL\u0026gt; select explode(array(1,2,3)); -- call hive udtf +-----+ | col | +-----+ | 1 | | 2 | | 3 | +-----+ 3 rows in set Flink SQL\u0026gt; create table tbl (key int,value string); [INFO] Execute statement succeed. Flink SQL\u0026gt; insert overwrite table tbl values (5,\u0026#39;e\u0026#39;),(1,\u0026#39;a\u0026#39;),(1,\u0026#39;a\u0026#39;),(3,\u0026#39;c\u0026#39;),(2,\u0026#39;b\u0026#39;),(3,\u0026#39;c\u0026#39;),(3,\u0026#39;c\u0026#39;),(4,\u0026#39;d\u0026#39;); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Flink SQL\u0026gt; select * from tbl cluster by key; -- run cluster by 2021-04-22 16:13:57,005 INFO org.apache.hadoop.mapred.FileInputFormat [] - Total input paths to process : 1 +-----+-------+ | key | value | +-----+-------+ | 1 | a | | 1 | a | | 5 | e | | 2 | b | | 3 | c | | 3 | c | | 3 | c | | 4 | d | +-----+-------+ 8 rows in set 注意 # 以下是使用 Hive 方言的一些注意事项。
Hive 方言只能用于操作 Hive 对象，并要求当前 Catalog 是一个 HiveCatalog 。 Hive 方言只支持 db.table 这种两级的标识符，不支持带有 Catalog 名字的标识符。 虽然所有 Hive 版本支持相同的语法，但是一些特定的功能是否可用仍取决于你使用的Hive 版本。例如，更新数据库位置 只在 Hive-2.4.0 或更高版本支持。 执行 DML 和 DQL 时应该使用 HiveModule 。 从 Flink 1.15版本开始，在使用 Hive 方言抛出以下异常时，请尝试用 opt 目录下的 flink-table-planner_2.12 jar 包来替换 lib 目录下的 flink-table-planner-loader jar 包。具体原因请参考 FLINK-25128。 `}),e.add({id:70,href:"/flink/flink-docs-master/zh/docs/libs/gelly/iterative_graph_processing/",title:"Iterative Graph Processing",section:"Graphs",content:` Iterative Graph Processing # Gelly exploits Flink\u0026rsquo;s efficient iteration operators to support large-scale iterative graph processing. Currently, we provide implementations of the vertex-centric, scatter-gather, and gather-sum-apply models. In the following sections, we describe these abstractions and show how you can use them in Gelly.
Vertex-Centric Iterations # The vertex-centric model, also known as \u0026ldquo;think like a vertex\u0026rdquo; or \u0026ldquo;Pregel\u0026rdquo;, expresses computation from the perspective of a vertex in the graph. The computation proceeds in synchronized iteration steps, called supersteps. In each superstep, each vertex executes one user-defined function. Vertices communicate with other vertices through messages. A vertex can send a message to any other vertex in the graph, as long as it knows its unique ID.
The computational model is shown in the figure below. The dotted boxes correspond to parallelization units. In each superstep, all active vertices execute the same user-defined computation in parallel. Supersteps are executed synchronously, so that messages sent during one superstep are guaranteed to be delivered in the beginning of the next superstep.
To use vertex-centric iterations in Gelly, the user only needs to define the vertex compute function, ComputeFunction. This function and the maximum number of iterations to run are given as parameters to Gelly\u0026rsquo;s runVertexCentricIteration. This method will execute the vertex-centric iteration on the input Graph and return a new Graph, with updated vertex values. An optional message combiner, MessageCombiner, can be defined to reduce communication costs.
Let us consider computing Single-Source-Shortest-Paths with vertex-centric iterations. Initially, each vertex has a value of infinite distance, except from the source vertex, which has a value of zero. During the first superstep, the source propagates distances to its neighbors. During the following supersteps, each vertex checks its received messages and chooses the minimum distance among them. If this distance is smaller than its current value, it updates its state and produces messages for its neighbors. If a vertex does not change its value during a superstep, then it does not produce any messages for its neighbors for the next superstep. The algorithm converges when there are no value updates or the maximum number of supersteps has been reached. In this algorithm, a message combiner can be used to reduce the number of messages sent to a target vertex.
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the vertex-centric iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runVertexCentricIteration( new SSSPComputeFunction(), new SSSPCombiner(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // public static final class SSSPComputeFunction extends ComputeFunction\u0026lt;Long, Double, Double, Double\u0026gt; { public void compute(Vertex\u0026lt;Long, Double\u0026gt; vertex, MessageIterator\u0026lt;Double\u0026gt; messages) { double minDistance = (vertex.getId().equals(srcId)) ? 0d : Double.POSITIVE_INFINITY; for (Double msg : messages) { minDistance = Math.min(minDistance, msg); } if (minDistance \u0026lt; vertex.getValue()) { setNewVertexValue(minDistance); for (Edge\u0026lt;Long, Double\u0026gt; e: getEdges()) { sendMessageTo(e.getTarget(), minDistance + e.getValue()); } } } // message combiner public static final class SSSPCombiner extends MessageCombiner\u0026lt;Long, Double\u0026gt; { public void combineMessages(MessageIterator\u0026lt;Double\u0026gt; messages) { double minMessage = Double.POSITIVE_INFINITY; for (Double msg: messages) { minMessage = Math.min(minMessage, msg); } sendCombinedMessage(minMessage); } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the vertex-centric iteration val result = graph.runVertexCentricIteration(new SSSPComputeFunction, new SSSPCombiner, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // final class SSSPComputeFunction extends ComputeFunction[Long, Double, Double, Double] { override def compute(vertex: Vertex[Long, Double], messages: MessageIterator[Double]) = { var minDistance = if (vertex.getId.equals(srcId)) 0 else Double.MaxValue while (messages.hasNext) { val msg = messages.next if (msg \u0026lt; minDistance) { minDistance = msg } } if (vertex.getValue \u0026gt; minDistance) { setNewVertexValue(minDistance) for (edge: Edge[Long, Double] \u0026lt;- getEdges) { sendMessageTo(edge.getTarget, vertex.getValue + edge.getValue) } } } // message combiner final class SSSPCombiner extends MessageCombiner[Long, Double] { override def combineMessages(messages: MessageIterator[Double]) { var minDistance = Double.MaxValue while (messages.hasNext) { val msg = inMessages.next if (msg \u0026lt; minDistance) { minDistance = msg } } sendCombinedMessage(minMessage) } } Back to top
Configuring a Vertex-Centric Iteration # A vertex-centric iteration can be configured using a VertexCentricConfiguration object. Currently, the following parameters can be specified:
Name: The name for the vertex-centric iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined ComputeFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the ComputeFunction, using the addBroadcastSet() method.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration VertexCentricConfiguration parameters = new VertexCentricConfiguration(); // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;); // set the parallelism parameters.setParallelism(16); // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator()); // run the vertex-centric iteration, also passing the configuration parameters Graph\u0026lt;Long, Long, Double\u0026gt; result = graph.runVertexCentricIteration( new Compute(), null, maxIterations, parameters); // user-defined function public static final class Compute extends ComputeFunction { LongSumAggregator aggregator = new LongSumAggregator(); public void preSuperstep() { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;); } public void compute(Vertex\u0026lt;Long, Long\u0026gt; vertex, MessageIterator inMessages) { //do some computation Long partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue); // update the vertex value setNewVertexValue(...); } } Scala val graph: Graph[Long, Long, Double] = ... val parameters = new VertexCentricConfiguration // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;) // set the parallelism parameters.setParallelism(16) // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator) // run the vertex-centric iteration, also passing the configuration parameters val result = graph.runVertexCentricIteration(new Compute, new Combiner, maxIterations, parameters) // user-defined function final class Compute extends ComputeFunction { var aggregator = new LongSumAggregator override def preSuperstep { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;) } override def compute(vertex: Vertex[Long, Long], inMessages: MessageIterator[Long]) { //do some computation val partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue) // update the vertex value setNewVertexValue(...) } } Back to top
Scatter-Gather Iterations # The scatter-gather model, also known as \u0026ldquo;signal/collect\u0026rdquo; model, expresses computation from the perspective of a vertex in the graph. The computation proceeds in synchronized iteration steps, called supersteps. In each superstep, a vertex produces messages for other vertices and updates its value based on the messages it receives. To use scatter-gather iterations in Gelly, the user only needs to define how a vertex behaves in each superstep:
Scatter: produces the messages that a vertex will send to other vertices. Gather: updates the vertex value using received messages. Gelly provides methods for scatter-gather iterations. The user only needs to implement two functions, corresponding to the scatter and gather phases. The first function is a ScatterFunction, which allows a vertex to send out messages to other vertices. Messages are received during the same superstep as they are sent. The second function is GatherFunction, which defines how a vertex will update its value based on the received messages. These functions and the maximum number of iterations to run are given as parameters to Gelly\u0026rsquo;s runScatterGatherIteration. This method will execute the scatter-gather iteration on the input Graph and return a new Graph, with updated vertex values.
A scatter-gather iteration can be extended with information such as the total number of vertices, the in degree and out degree. Additionally, the neighborhood type (in/out/all) over which to run the scatter-gather iteration can be specified. By default, the updates from the in-neighbors are used to modify the current vertex\u0026rsquo;s state and messages are sent to out-neighbors.
Let us consider computing Single-Source-Shortest-Paths with scatter-gather iterations on the following graph and let vertex 1 be the source. In each superstep, each vertex sends a candidate distance message to all its neighbors. The message value is the sum of the current value of the vertex and the edge weight connecting this vertex with its neighbor. Upon receiving candidate distance messages, each vertex calculates the minimum distance and, if a shorter path has been discovered, it updates its value. If a vertex does not change its value during a superstep, then it does not produce messages for its neighbors for the next superstep. The algorithm converges when there are no value updates.
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the scatter-gather iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new MinDistanceMessenger(), new VertexDistanceUpdater(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // // scatter: messaging public static final class MinDistanceMessenger extends ScatterFunction\u0026lt;Long, Double, Double, Double\u0026gt; { public void sendMessages(Vertex\u0026lt;Long, Double\u0026gt; vertex) { for (Edge\u0026lt;Long, Double\u0026gt; edge : getEdges()) { sendMessageTo(edge.getTarget(), vertex.getValue() + edge.getValue()); } } } // gather: vertex update public static final class VertexDistanceUpdater extends GatherFunction\u0026lt;Long, Double, Double\u0026gt; { public void updateVertex(Vertex\u0026lt;Long, Double\u0026gt; vertex, MessageIterator\u0026lt;Double\u0026gt; inMessages) { Double minDistance = Double.MAX_VALUE; for (double msg : inMessages) { if (msg \u0026lt; minDistance) { minDistance = msg; } } if (vertex.getValue() \u0026gt; minDistance) { setNewVertexValue(minDistance); } } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the scatter-gather iteration val result = graph.runScatterGatherIteration(new MinDistanceMessenger, new VertexDistanceUpdater, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // // messaging final class MinDistanceMessenger extends ScatterFunction[Long, Double, Double, Double] { override def sendMessages(vertex: Vertex[Long, Double]) = { for (edge: Edge[Long, Double] \u0026lt;- getEdges) { sendMessageTo(edge.getTarget, vertex.getValue + edge.getValue) } } } // vertex update final class VertexDistanceUpdater extends GatherFunction[Long, Double, Double] { override def updateVertex(vertex: Vertex[Long, Double], inMessages: MessageIterator[Double]) = { var minDistance = Double.MaxValue while (inMessages.hasNext) { val msg = inMessages.next if (msg \u0026lt; minDistance) { minDistance = msg } } if (vertex.getValue \u0026gt; minDistance) { setNewVertexValue(minDistance) } } } Back to top
Configuring a Scatter-Gather Iteration # A scatter-gather iteration can be configured using a ScatterGatherConfiguration object. Currently, the following parameters can be specified:
Name: The name for the scatter-gather iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined ScatterFunction and GatherFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the ScatterFunction and GatherFunction, using the addBroadcastSetForUpdateFunction() and addBroadcastSetForMessagingFunction() methods, respectively.
Number of Vertices: Accessing the total number of vertices within the iteration. This property can be set using the setOptNumVertices() method. The number of vertices can then be accessed in the vertex update function and in the messaging function using the getNumberOfVertices() method. If the option is not set in the configuration, this method will return -1.
Degrees: Accessing the in/out degree for a vertex within an iteration. This property can be set using the setOptDegrees() method. The in/out degrees can then be accessed in the vertex update function and in the messaging function, per vertex using the getInDegree() and getOutDegree() methods. If the degrees option is not set in the configuration, these methods will return -1.
Messaging Direction: By default, a vertex sends messages to its out-neighbors and updates its value based on messages received from its in-neighbors. This configuration option allows users to change the messaging direction to either EdgeDirection.IN, EdgeDirection.OUT, EdgeDirection.ALL. The messaging direction also dictates the update direction which would be EdgeDirection.OUT, EdgeDirection.IN and EdgeDirection.ALL, respectively. This property can be set using the setDirection() method.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;); // set the parallelism parameters.setParallelism(16); // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator()); // run the scatter-gather iteration, also passing the configuration parameters Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters); // user-defined functions public static final class Messenger extends ScatterFunction {...} public static final class VertexUpdater extends GatherFunction { LongSumAggregator aggregator = new LongSumAggregator(); public void preSuperstep() { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;); } public void updateVertex(Vertex\u0026lt;Long, Long\u0026gt; vertex, MessageIterator inMessages) { //do some computation Long partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue); // update the vertex value setNewVertexValue(...); } } Scala val graph: Graph[Long, Double, Double] = ... val parameters = new ScatterGatherConfiguration // set the iteration name parameters.setName(\u0026#34;Gelly Iteration\u0026#34;) // set the parallelism parameters.setParallelism(16) // register an aggregator parameters.registerAggregator(\u0026#34;sumAggregator\u0026#34;, new LongSumAggregator) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) // user-defined functions final class Messenger extends ScatterFunction {...} final class VertexUpdater extends GatherFunction { var aggregator = new LongSumAggregator override def preSuperstep { // retrieve the Aggregator aggregator = getIterationAggregator(\u0026#34;sumAggregator\u0026#34;) } override def updateVertex(vertex: Vertex[Long, Long], inMessages: MessageIterator[Long]) { //do some computation val partialValue = ... // aggregate the partial value aggregator.aggregate(partialValue) // update the vertex value setNewVertexValue(...) } } The following example illustrates the usage of the degree as well as the number of vertices options.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the number of vertices option to true parameters.setOptNumVertices(true); // set the degree option to true parameters.setOptDegrees(true); // run the scatter-gather iteration, also passing the configuration parameters Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters); // user-defined functions public static final class Messenger extends ScatterFunction { ... // retrieve the vertex out-degree outDegree = getOutDegree(); ... } public static final class VertexUpdater extends GatherFunction { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } Scala val graph: Graph[Long, Double, Double] = ... // configure the iteration val parameters = new ScatterGatherConfiguration // set the number of vertices option to true parameters.setOptNumVertices(true) // set the degree option to true parameters.setOptDegrees(true) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) // user-defined functions final class Messenger extends ScatterFunction { ... // retrieve the vertex out-degree val outDegree = getOutDegree ... } final class VertexUpdater extends GatherFunction { ... // get the number of vertices val numVertices = getNumberOfVertices ... } The following example illustrates the usage of the edge direction option. Vertices update their values to contain a list of all their in-neighbors.
Java Graph\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;, Double\u0026gt; graph = ...; // configure the iteration ScatterGatherConfiguration parameters = new ScatterGatherConfiguration(); // set the messaging direction parameters.setDirection(EdgeDirection.IN); // run the scatter-gather iteration, also passing the configuration parameters DataSet\u0026lt;Vertex\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;\u0026gt;\u0026gt; result = graph.runScatterGatherIteration( new Messenger(), new VertexUpdater(), maxIterations, parameters) .getVertices(); // user-defined functions public static final class Messenger extends GatherFunction {...} public static final class VertexUpdater extends ScatterFunction {...} Scala val graph: Graph[Long, HashSet[Long], Double] = ... // configure the iteration val parameters = new ScatterGatherConfiguration // set the messaging direction parameters.setDirection(EdgeDirection.IN) // run the scatter-gather iteration, also passing the configuration parameters val result = graph.runScatterGatherIteration(new Messenger, new VertexUpdater, maxIterations, parameters) .getVertices // user-defined functions final class Messenger extends ScatterFunction {...} final class VertexUpdater extends GatherFunction {...} Back to top
Gather-Sum-Apply Iterations # Like in the scatter-gather model, Gather-Sum-Apply also proceeds in synchronized iterative steps, called supersteps. Each superstep consists of the following three phases:
Gather: a user-defined function is invoked in parallel on the edges and neighbors of each vertex, producing a partial value. Sum: the partial values produced in the Gather phase are aggregated to a single value, using a user-defined reducer. Apply: each vertex value is updated by applying a function on the current value and the aggregated value produced by the Sum phase. Let us consider computing Single-Source-Shortest-Paths with GSA on the following graph and let vertex 1 be the source. During the Gather phase, we calculate the new candidate distances, by adding each vertex value with the edge weight. In Sum, the candidate distances are grouped by vertex ID and the minimum distance is chosen. In Apply, the newly calculated distance is compared to the current vertex value and the minimum of the two is assigned as the new value of the vertex.
Notice that, if a vertex does not change its value during a superstep, it will not calculate candidate distance during the next superstep. The algorithm converges when no vertex changes value.
To implement this example in Gelly GSA, the user only needs to call the runGatherSumApplyIteration method on the input graph and provide the GatherFunction, SumFunction and ApplyFunction UDFs. Iteration synchronization, grouping, value updates and convergence are handled by the system:
Java // read the input graph Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // define the maximum number of iterations int maxIterations = 10; // Execute the GSA iteration Graph\u0026lt;Long, Double, Double\u0026gt; result = graph.runGatherSumApplyIteration( new CalculateDistances(), new ChooseMinDistance(), new UpdateDistance(), maxIterations); // Extract the vertices as the result DataSet\u0026lt;Vertex\u0026lt;Long, Double\u0026gt;\u0026gt; singleSourceShortestPaths = result.getVertices(); // - - - UDFs - - - // // Gather private static final class CalculateDistances extends GatherFunction\u0026lt;Double, Double, Double\u0026gt; { public Double gather(Neighbor\u0026lt;Double, Double\u0026gt; neighbor) { return neighbor.getNeighborValue() + neighbor.getEdgeValue(); } } // Sum private static final class ChooseMinDistance extends SumFunction\u0026lt;Double, Double, Double\u0026gt; { public Double sum(Double newValue, Double currentValue) { return Math.min(newValue, currentValue); } } // Apply private static final class UpdateDistance extends ApplyFunction\u0026lt;Long, Double, Double\u0026gt; { public void apply(Double newDistance, Double oldDistance) { if (newDistance \u0026lt; oldDistance) { setResult(newDistance); } } } Scala // read the input graph val graph: Graph[Long, Double, Double] = ... // define the maximum number of iterations val maxIterations = 10 // Execute the GSA iteration val result = graph.runGatherSumApplyIteration(new CalculateDistances, new ChooseMinDistance, new UpdateDistance, maxIterations) // Extract the vertices as the result val singleSourceShortestPaths = result.getVertices // - - - UDFs - - - // // Gather final class CalculateDistances extends GatherFunction[Double, Double, Double] { override def gather(neighbor: Neighbor[Double, Double]): Double = { neighbor.getNeighborValue + neighbor.getEdgeValue } } // Sum final class ChooseMinDistance extends SumFunction[Double, Double, Double] { override def sum(newValue: Double, currentValue: Double): Double = { Math.min(newValue, currentValue) } } // Apply final class UpdateDistance extends ApplyFunction[Long, Double, Double] { override def apply(newDistance: Double, oldDistance: Double) = { if (newDistance \u0026lt; oldDistance) { setResult(newDistance) } } } Note that gather takes a Neighbor type as an argument. This is a convenience type which simply wraps a vertex with its neighboring edge.
For more examples of how to implement algorithms with the Gather-Sum-Apply model, check the GSAPageRank and GSAConnectedComponents library methods of Gelly.
Back to top
Configuring a Gather-Sum-Apply Iteration # A GSA iteration can be configured using a GSAConfiguration object. Currently, the following parameters can be specified:
Name: The name for the GSA iteration. The name is displayed in logs and messages and can be specified using the setName() method.
Parallelism: The parallelism for the iteration. It can be set using the setParallelism() method.
Solution set in unmanaged memory: Defines whether the solution set is kept in managed memory (Flink\u0026rsquo;s internal way of keeping objects in serialized form) or as a simple object map. By default, the solution set runs in managed memory. This property can be set using the setSolutionSetUnmanagedMemory() method.
Aggregators: Iteration aggregators can be registered using the registerAggregator() method. An iteration aggregator combines all aggregates globally once per superstep and makes them available in the next superstep. Registered aggregators can be accessed inside the user-defined GatherFunction, SumFunction and ApplyFunction.
Broadcast Variables: DataSets can be added as Broadcast Variables to the GatherFunction, SumFunction and ApplyFunction, using the methods addBroadcastSetForGatherFunction(), addBroadcastSetForSumFunction() and addBroadcastSetForApplyFunction methods, respectively.
Number of Vertices: Accessing the total number of vertices within the iteration. This property can be set using the setOptNumVertices() method. The number of vertices can then be accessed in the gather, sum and/or apply functions by using the getNumberOfVertices() method. If the option is not set in the configuration, this method will return -1.
Neighbor Direction: By default values are gathered from the out neighbors of the Vertex. This can be modified using the setDirection() method.
The following example illustrates the usage of the number of vertices option.
Java Graph\u0026lt;Long, Double, Double\u0026gt; graph = ...; // configure the iteration GSAConfiguration parameters = new GSAConfiguration(); // set the number of vertices option to true parameters.setOptNumVertices(true); // run the gather-sum-apply iteration, also passing the configuration parameters Graph\u0026lt;Long, Long, Long\u0026gt; result = graph.runGatherSumApplyIteration( new Gather(), new Sum(), new Apply(), maxIterations, parameters); // user-defined functions public static final class Gather { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } public static final class Sum { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } public static final class Apply { ... // get the number of vertices long numVertices = getNumberOfVertices(); ... } Scala val graph: Graph[Long, Double, Double] = ... // configure the iteration val parameters = new GSAConfiguration // set the number of vertices option to true parameters.setOptNumVertices(true) // run the gather-sum-apply iteration, also passing the configuration parameters val result = graph.runGatherSumApplyIteration(new Gather, new Sum, new Apply, maxIterations, parameters) // user-defined functions final class Gather { ... // get the number of vertices val numVertices = getNumberOfVertices ... } final class Sum { ... // get the number of vertices val numVertices = getNumberOfVertices ... } final class Apply { ... // get the number of vertices val numVertices = getNumberOfVertices ... } The following example illustrates the usage of the edge direction option. Java Graph\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;, Double\u0026gt; graph = ...; // configure the iteration GSAConfiguration parameters = new GSAConfiguration(); // set the messaging direction parameters.setDirection(EdgeDirection.IN); // run the gather-sum-apply iteration, also passing the configuration parameters DataSet\u0026lt;Vertex\u0026lt;Long, HashSet\u0026lt;Long\u0026gt;\u0026gt;\u0026gt; result = graph.runGatherSumApplyIteration( new Gather(), new Sum(), new Apply(), maxIterations, parameters) .getVertices(); Scala val graph: Graph[Long, HashSet[Long], Double] = ... // configure the iteration val parameters = new GSAConfiguration // set the messaging direction parameters.setDirection(EdgeDirection.IN) // run the gather-sum-apply iteration, also passing the configuration parameters val result = graph.runGatherSumApplyIteration(new Gather, new Sum, new Apply, maxIterations, parameters) .getVertices() Back to top
Iteration Abstractions Comparison # Although the three iteration abstractions in Gelly seem quite similar, understanding their differences can lead to more performant and maintainable programs. Among the three, the vertex-centric model is the most general model and supports arbitrary computation and messaging for each vertex. In the scatter-gather model, the logic of producing messages is decoupled from the logic of updating vertex values. Thus, programs written using scatter-gather are sometimes easier to follow and maintain. Separating the messaging phase from the vertex value update logic not only makes some programs easier to follow but might also have a positive impact on performance. Scatter-gather implementations typically have lower memory requirements, because concurrent access to the inbox (messages received) and outbox (messages to send) data structures is not required. However, this characteristic also limits expressiveness and makes some computation patterns non-intuitive. Naturally, if an algorithm requires a vertex to concurrently access its inbox and outbox, then the expression of this algorithm in scatter-gather might be problematic. Strongly Connected Components and Approximate Maximum Weight Matching are examples of such graph algorithms. A direct consequence of this restriction is that vertices cannot generate messages and update their states in the same phase. Thus, deciding whether to propagate a message based on its content would require storing it in the vertex value, so that the gather phase has access to it, in the following iteration step. Similarly, if the vertex update logic includes computation over the values of the neighboring edges, these have to be included inside a special message passed from the scatter to the gather phase. Such workarounds often lead to higher memory requirements and non-elegant, hard to understand algorithm implementations.
Gather-sum-apply iterations are also quite similar to scatter-gather iterations. In fact, any algorithm which can be expressed as a GSA iteration can also be written in the scatter-gather model. The messaging phase of the scatter-gather model is equivalent to the Gather and Sum steps of GSA: Gather can be seen as the phase where the messages are produced and Sum as the phase where they are routed to the target vertex. Similarly, the value update phase corresponds to the Apply step.
The main difference between the two implementations is that the Gather phase of GSA parallelizes the computation over the edges, while the messaging phase distributes the computation over the vertices. Using the SSSP examples above, we see that in the first superstep of the scatter-gather case, vertices 1, 2 and 3 produce messages in parallel. Vertex 1 produces 3 messages, while vertices 2 and 3 produce one message each. In the GSA case on the other hand, the computation is parallelized over the edges: the three candidate distance values of vertex 1 are produced in parallel. Thus, if the Gather step contains \u0026ldquo;heavy\u0026rdquo; computation, it might be a better idea to use GSA and spread out the computation, instead of burdening a single vertex. Another case when parallelizing over the edges might prove to be more efficient is when the input graph is skewed (some vertices have a lot more neighbors than others).
Another difference between the two implementations is that the scatter-gather implementation uses a coGroup operator internally, while GSA uses a reduce. Therefore, if the function that combines neighbor values (messages) requires the whole group of values for the computation, scatter-gather should be used. If the update function is associative and commutative, then the GSA\u0026rsquo;s reducer is expected to give a more efficient implementation, as it can make use of a combiner.
Another thing to note is that GSA works strictly on neighborhoods, while in the vertex-centric and scatter-gather models, a vertex can send a message to any vertex, given that it knows its vertex ID, regardless of whether it is a neighbor. Finally, in Gelly\u0026rsquo;s scatter-gather implementation, one can choose the messaging direction, i.e. the direction in which updates propagate. GSA does not support this yet, so each vertex will be updated based on the values of its in-neighbors only.
The main differences among the Gelly iteration models are shown in the table below.
Iteration Model Update Function Update Logic Communication Scope Communication Logic Vertex-Centric arbitrary arbitrary any vertex arbitrary Scatter-Gather arbitrary based on received messages any vertex based on vertex state Gather-Sum-Apply associative and commutative based on neighbors' values neighborhood based on vertex state Back to top
`}),e.add({id:71,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/joining/",title:"Joining",section:"算子",content:` Joining # Window Join # Window join 作用在两个流中有相同 key 且处于相同窗口的元素上。这些窗口可以通过 window assigner 定义，并且两个流中的元素都会被用于计算窗口的结果。
两个流中的元素在组合之后，会被传递给用户定义的 JoinFunction 或 FlatJoinFunction，用户可以用它们输出符合 join 要求的结果。
常见的用例可以总结为以下代码：
stream.join(otherStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(\u0026lt;WindowAssigner\u0026gt;) .apply(\u0026lt;JoinFunction\u0026gt;); 语义上有一些值得注意的地方：
从两个流中创建成对的元素与 inner-join 类似，即一个流中的元素在与另一个流中对应的元素完成 join 之前不会被输出。 完成 join 的元素会将他们的 timestamp 设为对应窗口中允许的最大 timestamp。比如一个边界为 [5, 10) 窗口中的元素在 join 之后的 timestamp 为 9。 接下来我们会用例子说明各种 window join 如何运作。
滚动 Window Join # 使用滚动 window join 时，所有 key 相同且共享一个滚动窗口的元素会被组合成对，并传递给 JoinFunction 或 FlatJoinFunction。因为这个行为与 inner join 类似，所以一个流中的元素如果没有与另一个流中的元素组合起来，它就不会被输出！
如图所示，我们定义了一个大小为 2 毫秒的滚动窗口，即形成了边界为 [0,1], [2,3], ... 的窗口。图中展示了如何将每个窗口中的元素组合成对，组合的结果将被传递给 JoinFunction。注意，滚动窗口 [6,7] 将不会输出任何数据，因为绿色流当中没有数据可以与橙色流的 ⑥ 和 ⑦ 配对。
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } 滑动 Window Join # 当使用滑动 window join 时，所有 key 相同且处于同一个滑动窗口的元素将被组合成对，并传递给 JoinFunction 或 FlatJoinFunction。当前滑动窗口内，如果一个流中的元素没有与另一个流中的元素组合起来，它就不会被输出！注意，在某个滑动窗口中被 join 的元素不一定会在其他滑动窗口中被 join。
本例中我们定义了长度为两毫秒，滑动距离为一毫秒的滑动窗口，生成的窗口实例区间为 [-1, 0],[0,1],[1,2],[2,3], …。 X 轴下方是每个滑动窗口中被 join 后传递给 JoinFunction 的元素。图中可以看到橙色 ② 与绿色 ③ 在窗口 [2,3] 中 join，但没有与窗口 [1,2] 中任何元素 join。
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } 会话 Window Join # 使用会话 window join 时，所有 key 相同且组合后符合会话要求的元素将被组合成对，并传递给 JoinFunction 或 FlatJoinFunction。这个操作同样是 inner join，所以如果一个会话窗口中只含有某一个流的元素，这个窗口将不会产生输出！
这里我们定义了一个间隔为至少一毫秒的会话窗口。图中总共有三个会话，前两者中两个流都有元素，它们被 join 并传递给 JoinFunction。而第三个会话中，绿流没有任何元素，所以 ⑧ 和 ⑨ 没有被 join！
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream.join(greenStream) .where(\u0026lt;KeySelector\u0026gt;) .equalTo(\u0026lt;KeySelector\u0026gt;) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply (new JoinFunction\u0026lt;Integer, Integer, String\u0026gt; (){ @Override public String join(Integer first, Integer second) { return first + \u0026#34;,\u0026#34; + second; } }); Scala import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream.join(greenStream) .where(elem =\u0026gt; /* select key */) .equalTo(elem =\u0026gt; /* select key */) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply { (e1, e2) =\u0026gt; e1 + \u0026#34;,\u0026#34; + e2 } Interval Join # Interval join 组合元素的条件为：两个流（我们暂时称为 A 和 B）中 key 相同且 B 中元素的 timestamp 处于 A 中元素 timestamp 的一定范围内。
这个条件可以更加正式地表示为 b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] 或 a.timestamp + lowerBound \u0026lt;= b.timestamp \u0026lt;= a.timestamp + upperBound
这里的 a 和 b 为 A 和 B 中共享相同 key 的元素。上界和下界可正可负，只要下界永远小于等于上界即可。 Interval join 目前仅执行 inner join。
当一对元素被传递给 ProcessJoinFunction，他们的 timestamp 会从两个元素的 timestamp 中取最大值 （timestamp 可以通过 ProcessJoinFunction.Context 访问）。
Interval join 目前仅支持 event time。 上例中，我们 join 了橙色和绿色两个流，join 的条件是：以 -2 毫秒为下界、+1 毫秒为上界。 默认情况下，上下界也被包括在区间内，但 .lowerBoundExclusive() 和 .upperBoundExclusive() 可以将它们排除在外。
图中三角形所表示的条件也可以写成更加正式的表达式：
orangeElem.ts + lowerBound \u0026lt;= greenElem.ts \u0026lt;= orangeElem.ts + upperBound
Java import org.apache.flink.api.java.functions.KeySelector; import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction; import org.apache.flink.streaming.api.windowing.time.Time; ... DataStream\u0026lt;Integer\u0026gt; orangeStream = ...; DataStream\u0026lt;Integer\u0026gt; greenStream = ...; orangeStream .keyBy(\u0026lt;KeySelector\u0026gt;) .intervalJoin(greenStream.keyBy(\u0026lt;KeySelector\u0026gt;)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process (new ProcessJoinFunction\u0026lt;Integer, Integer, String(){ @Override public void processElement(Integer left, Integer right, Context ctx, Collector\u0026lt;String\u0026gt; out) { out.collect(left + \u0026#34;,\u0026#34; + right); } }); Scala import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction import org.apache.flink.streaming.api.windowing.time.Time ... val orangeStream: DataStream[Integer] = ... val greenStream: DataStream[Integer] = ... orangeStream .keyBy(elem =\u0026gt; /* select key */) .intervalJoin(greenStream.keyBy(elem =\u0026gt; /* select key */)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process(new ProcessJoinFunction[Integer, Integer, String] { override def processElement(left: Integer, right: Integer, ctx: ProcessJoinFunction[Integer, Integer, String]#Context, out: Collector[String]): Unit = { out.collect(left + \u0026#34;,\u0026#34; + right) } }) Back to top
`}),e.add({id:72,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/json/",title:"JSON",section:"Formats",content:` JSON Format # Format: Serialization Schema Format: Deserialization Schema
JSON Format 能读写 JSON 格式的数据。当前，JSON schema 是从 table schema 中自动推导而得的。
依赖 # In order to use the Json format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in 如何创建一张基于 JSON Format 的表 # 以下是一个利用 Kafka 以及 JSON Format 构建表的例子。
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) Format 参数 # 参数 是否必须 默认值 类型 描述 format 必选 (none) String 声明使用的格式，这里应为'json'。 json.fail-on-missing-field 可选 false Boolean 当解析字段缺失时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。 json.ignore-parse-errors 可选 false Boolean 当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null。 json.timestamp-format.standard 可选 'SQL' String 声明输入和输出的 TIMESTAMP 和 TIMESTAMP_LTZ 的格式。当前支持的格式为'SQL' 以及 'ISO-8601'： 可选参数 'SQL' 将会以 "yyyy-MM-dd HH:mm:ss.s{precision}" 的格式解析 TIMESTAMP, 例如 "2020-12-30 12:13:14.123"， 以 "yyyy-MM-dd HH:mm:ss.s{precision}'Z'" 的格式解析 TIMESTAMP_LTZ, 例如 "2020-12-30 12:13:14.123Z" 且会以相同的格式输出。 可选参数 'ISO-8601' 将会以 "yyyy-MM-ddTHH:mm:ss.s{precision}" 的格式解析输入 TIMESTAMP, 例如 "2020-12-30T12:13:14.123" ， 以 "yyyy-MM-ddTHH:mm:ss.s{precision}'Z'" 的格式解析 TIMESTAMP_LTZ, 例如 "2020-12-30T12:13:14.123Z" 且会以相同的格式输出。 json.map-null-key.mode 选填 'FAIL' String 指定处理 Map 中 key 值为空的方法. 当前支持的值有 'FAIL', 'DROP' 和 'LITERAL': Option 'FAIL' 将抛出异常，如果遇到 Map 中 key 值为空的数据。 Option 'DROP' 将丢弃 Map 中 key 值为空的数据项。 Option 'LITERAL' 将使用字符串常量来替换 Map 中的空 key 值。字符串常量的值由 'json.map-null-key.literal' 定义。 json.map-null-key.literal 选填 'null' String 当 'json.map-null-key.mode' 是 LITERAL 的时候，指定字符串常量替换 Map 中的空 key 值。 json.encode.decimal-as-plain-number 选填 false Boolean 将所有 DECIMAL 类型的数据保持原状，不使用科学计数法表示。例：0.000000027 默认会表示为 2.7E-8。当此选项设为 true 时，则会表示为 0.000000027。 数据类型映射关系 # 当前，JSON schema 将会自动从 table schema 之中自动推导得到。不支持显式地定义 JSON schema。
在 Flink 中，JSON Format 使用 jackson databind API 去解析和生成 JSON。
下表列出了 Flink 中的数据类型与 JSON 中的数据类型的映射关系。
Flink SQL 类型 JSON 类型 CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY string with encoding: base64 DECIMAL number TINYINT number SMALLINT number INT number BIGINT number FLOAT number DOUBLE number DATE string with format: date TIME string with format: time TIMESTAMP string with format: date-time TIMESTAMP_WITH_LOCAL_TIME_ZONE string with format: date-time (with UTC time zone) INTERVAL number ARRAY array MAP / MULTISET object ROW object `}),e.add({id:73,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/kafka/",title:"Kafka",section:"DataStream Connectors",content:` Apache Kafka 连接器 # Flink 提供了 Apache Kafka 连接器使用精确一次（Exactly-once）的语义在 Kafka topic 中读取和写入数据。
依赖 # Apache Flink 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。 该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。 当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。 有关 Kafka 兼容性的更多细节，请参考 Kafka 官方文档。
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 如果使用 Kafka source，flink-connector-base 也需要包含在依赖中：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-base\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Flink 目前的流连接器还不是二进制发行版的一部分。 在此处可以了解到如何链接它们，从而在集群中运行。
为了在 PyFlink 作业中使用 Kafka connector ，需要添加下列依赖： Kafka version PyFlink JAR universal Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 Kafka Source # 该文档描述的是基于新数据源 API 的 Kafka Source。 使用方法 # Kafka Source 提供了构建类来创建 KafkaSource 的实例。以下代码片段展示了如何构建 KafkaSource 来消费 “input-topic” 最早位点的数据， 使用消费组 “my-group”，并且将 Kafka 消息体反序列化为字符串：
Java KafkaSource\u0026lt;String\u0026gt; source = KafkaSource.\u0026lt;String\u0026gt;builder() .setBootstrapServers(brokers) .setTopics(\u0026#34;input-topic\u0026#34;) .setGroupId(\u0026#34;my-group\u0026#34;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .build(); env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Kafka Source\u0026#34;); Python source = KafkaSource.builder() \\ .set_bootstrap_servers(brokers) \\ .set_topics(\u0026#34;input-topic\u0026#34;) \\ .set_group_id(\u0026#34;my-group\u0026#34;) \\ .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\ .set_value_only_deserializer(SimpleStringSchema()) \\ .build() env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;Kafka Source\u0026#34;) 以下属性在构建 KafkaSource 时是必须指定的：
Bootstrap server，通过 setBootstrapServers(String) 方法配置 消费者组 ID，通过 setGroupId(String) 配置 要订阅的 Topic / Partition，请参阅 Topic / Partition 订阅一节 用于解析 Kafka 消息的反序列化器（Deserializer），请参阅消息解析一节 Topic / Partition 订阅 # Kafka Source 提供了 3 种 Topic / Partition 的订阅方式：
Topic 列表，订阅 Topic 列表中所有 Partition 的消息： Java KafkaSource.builder().setTopics(\u0026#34;topic-a\u0026#34;, \u0026#34;topic-b\u0026#34;); Python KafkaSource.builder().set_topics(\u0026#34;topic-a\u0026#34;, \u0026#34;topic-b\u0026#34;) 正则表达式匹配，订阅与正则表达式所匹配的 Topic 下的所有 Partition： Java KafkaSource.builder().setTopicPattern(\u0026#34;topic.*\u0026#34;); Python KafkaSource.builder().set_topic_pattern(\u0026#34;topic.*\u0026#34;) Partition 列表，订阅指定的 Partition： Java final HashSet\u0026lt;TopicPartition\u0026gt; partitionSet = new HashSet\u0026lt;\u0026gt;(Arrays.asList( new TopicPartition(\u0026#34;topic-a\u0026#34;, 0), // Partition 0 of topic \u0026#34;topic-a\u0026#34; new TopicPartition(\u0026#34;topic-b\u0026#34;, 5))); // Partition 5 of topic \u0026#34;topic-b\u0026#34; KafkaSource.builder().setPartitions(partitionSet); Python partition_set = { KafkaTopicPartition(\u0026#34;topic-a\u0026#34;, 0), KafkaTopicPartition(\u0026#34;topic-b\u0026#34;, 5) } KafkaSource.builder().set_partitions(partition_set) 消息解析 # 代码中需要提供一个反序列化器（Deserializer）来对 Kafka 的消息进行解析。 反序列化器通过 setDeserializer(KafkaRecordDeserializationSchema) 来指定，其中 KafkaRecordDeserializationSchema 定义了如何解析 Kafka 的 ConsumerRecord。
如果只需要 Kafka 消息中的消息体（value）部分的数据，可以使用 KafkaSource 构建类中的 setValueOnlyDeserializer(DeserializationSchema) 方法，其中 DeserializationSchema 定义了如何解析 Kafka 消息体中的二进制数据。
也可使用 Kafka 提供的解析器 来解析 Kafka 消息体。例如使用 StringDeserializer 来将 Kafka 消息体解析成字符串：
import org.apache.kafka.common.serialization.StringDeserializer; KafkaSource.\u0026lt;String\u0026gt;builder() .setDeserializer(KafkaRecordDeserializationSchema.valueOnly(StringDeserializer.class)); 目前 PyFlink 只支持 set_value_only_deserializer 来自定义 Kafka 消息中值的反序列化.
KafkaSource.builder().set_value_only_deserializer(SimpleStringSchema()) 起始消费位点 # Kafka source 能够通过位点初始化器（OffsetsInitializer）来指定从不同的偏移量开始消费 。内置的位点初始化器包括：
Java KafkaSource.builder() // 从消费组提交的位点开始消费，不指定位点重置策略 .setStartingOffsets(OffsetsInitializer.committedOffsets()) // 从消费组提交的位点开始消费，如果提交位点不存在，使用最早位点 .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST)) // 从时间戳大于等于指定时间戳（毫秒）的数据开始消费 .setStartingOffsets(OffsetsInitializer.timestamp(1657256176000L)) // 从最早位点开始消费 .setStartingOffsets(OffsetsInitializer.earliest()) // 从最末尾位点开始消费 .setStartingOffsets(OffsetsInitializer.latest()); Python KafkaSource.builder() # 从消费组提交的位点开始消费，不指定位点重置策略 .set_starting_offsets(KafkaOffsetsInitializer.committed_offsets()) \\ # 从消费组提交的位点开始消费，如果提交位点不存在，使用最早位点 .set_starting_offsets(KafkaOffsetsInitializer.committed_offsets(KafkaOffsetResetStrategy.EARLIEST)) \\ # 从时间戳大于等于指定时间戳（毫秒）的数据开始消费 .set_starting_offsets(KafkaOffsetsInitializer.timestamp(1657256176000)) \\ # 从最早位点开始消费 .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\ # 从最末尾位点开始消费 .set_starting_offsets(KafkaOffsetsInitializer.latest()) 如果内置的初始化器不能满足需求，也可以实现自定义的位点初始化器（OffsetsInitializer）。（ PyFlink 不支持）
如果未指定位点初始化器，将默认使用 OffsetsInitializer.earliest()。
有界 / 无界模式 # Kafka Source 支持流式和批式两种运行模式。默认情况下，KafkaSource 设置为以流模式运行，因此作业永远不会停止，直到 Flink 作业失败或被取消。 可以使用 setBounded(OffsetsInitializer) 指定停止偏移量使 Kafka Source 以批处理模式运行。当所有分区都达到其停止偏移量时，Kafka Source 会退出运行。
流模式下运行通过使用 setUnbounded(OffsetsInitializer) 也可以指定停止消费位点，当所有分区达到其指定的停止偏移量时，Kafka Source 会退出运行。
其他属性 # 除了上述属性之外，您还可以使用 setProperties(Properties) 和 setProperty(String, String) 为 Kafka Source 和 Kafka Consumer 设置任意属性。KafkaSource 有以下配置项：
client.id.prefix，指定用于 Kafka Consumer 的客户端 ID 前缀 partition.discovery.interval.ms，定义 Kafka Source 检查新分区的时间间隔。 请参阅下面的动态分区检查一节 register.consumer.metrics 指定是否在 Flink 中注册 Kafka Consumer 的指标 commit.offsets.on.checkpoint 指定是否在进行 checkpoint 时将消费位点提交至 Kafka broker Kafka consumer 的配置可以参考 Apache Kafka 文档。
请注意，即使指定了以下配置项，构建器也会将其覆盖：
key.deserializer 始终设置为 ByteArrayDeserializer value.deserializer 始终设置为 ByteArrayDeserializer auto.offset.reset.strategy 被 OffsetsInitializer#getAutoOffsetResetStrategy() 覆盖 partition.discovery.interval.ms 会在批模式下被覆盖为 -1 动态分区检查 # 为了在不重启 Flink 作业的情况下处理 Topic 扩容或新建 Topic 等场景，可以将 Kafka Source 配置为在提供的 Topic / Partition 订阅模式下定期检查新分区。要启用动态分区检查，请将 partition.discovery.interval.ms 设置为非负值：
Java KafkaSource.builder() .setProperty(\u0026#34;partition.discovery.interval.ms\u0026#34;, \u0026#34;10000\u0026#34;); // 每 10 秒检查一次新分区 Python KafkaSource.builder() \\ .set_property(\u0026#34;partition.discovery.interval.ms\u0026#34;, \u0026#34;10000\u0026#34;) # 每 10 秒检查一次新分区 分区检查功能默认不开启。需要显式地设置分区检查间隔才能启用此功能。 事件时间和水印 # 默认情况下，Kafka Source 使用 Kafka 消息中的时间戳作为事件时间。您可以定义自己的水印策略（Watermark Strategy） 以从消息中提取事件时间，并向下游发送水印：
env.fromSource(kafkaSource, new CustomWatermarkStrategy(), \u0026#34;Kafka Source With Custom Watermark Strategy\u0026#34;); 这篇文档描述了如何自定义水印策略（WatermarkStrategy）。（ PyFlink 不支持）
空闲 # 如果并行度高于分区数，Kafka Source 不会自动进入空闲状态。您将需要降低并行度或向水印策略添加空闲超时。如果在这段时间内没有记录在流的分区中流动，则该分区被视为“空闲”并且不会阻止下游操作符中水印的进度。 这篇文档 描述了有关如何定义 WatermarkStrategy#withIdleness 的详细信息.
消费位点提交 # Kafka source 在 checkpoint 完成时提交当前的消费位点 ，以保证 Flink 的 checkpoint 状态和 Kafka broker 上的提交位点一致。如果未开启 checkpoint，Kafka source 依赖于 Kafka consumer 内部的位点定时自动提交逻辑，自动提交功能由 enable.auto.commit 和 auto.commit.interval.ms 两个 Kafka consumer 配置项进行配置。
注意：Kafka source 不依赖于 broker 上提交的位点来恢复失败的作业。提交位点只是为了上报 Kafka consumer 和消费组的消费进度，以在 broker 端进行监控。
监控 # Kafka source 会在不同的范围 (Scope)中汇报下列指标。
指标范围 # 范围 指标 用户变量 描述 类型 算子 currentEmitEventTimeLag n/a 数据的事件时间与数据离开 Source 时的间隔¹：currentEmitEventTimeLag = EmitTime - EventTime Gauge watermarkLag n/a 水印时间滞后于当前时间的时长：watermarkLag = CurrentTime - Watermark Gauge sourceIdleTime n/a Source 闲置时长：sourceIdleTime = CurrentTime - LastRecordProcessTime Gauge pendingRecords n/a 尚未被 Source 拉取的数据数量，即 Kafka partition 当前消费位点之后的数据数量。 Gauge KafkaSourceReader.commitsSucceeded n/a 位点成功提交至 Kafka 的总次数，在开启了位点提交功能时适用。 Counter KafkaSourceReader.commitsFailed n/a 位点未能成功提交至 Kafka 的总次数，在开启了位点提交功能时适用。注意位点提交仅是为了向 Kafka 上报消费进度，因此提交失败并不影响 Flink checkpoint 中存储的位点信息的完整性。 Counter KafkaSourceReader.committedOffsets topic, partition 每个 partition 最近一次成功提交至 Kafka 的位点。各个 partition 的指标可以通过指定 topic 名称和 partition ID 获取。 Gauge KafkaSourceReader.currentOffsets topic, partition 每个 partition 当前读取的位点。各个 partition 的指标可以通过指定 topic 名称和 partition ID 获取。 Gauge ¹ 该指标反映了最后一条数据的瞬时值。之所以提供瞬时值是因为统计延迟直方图会消耗更多资源，瞬时值通常足以很好地反映延迟。
Kafka Consumer 指标 # Kafka consumer 的所有指标都注册在指标组 KafkaSourceReader.KafkaConsumer 下。例如 Kafka consumer 的指标 records-consumed-total 将在该 Flink 指标中汇报： \u0026lt;some_parent_groups\u0026gt;.operator.KafkaSourceReader.KafkaConsumer.records-consumed-total。
您可以使用配置项 register.consumer.metrics 配置是否注册 Kafka consumer 的指标 。默认此选项设置为 true。
关于 Kafka consumer 的指标，您可以参考 Apache Kafka 文档 了解更多详细信息。
安全 # 要启用加密和认证相关的安全配置，只需将安全配置作为其他属性配置在 Kafka source 上即可。下面的代码片段展示了如何配置 Kafka source 以使用 PLAIN 作为 SASL 机制并提供 JAAS 配置：
Java KafkaSource.builder() .setProperty(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_PLAINTEXT\u0026#34;) .setProperty(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;PLAIN\u0026#34;) .setProperty(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;); Python KafkaSource.builder() \\ .set_property(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_PLAINTEXT\u0026#34;) \\ .set_property(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;PLAIN\u0026#34;) \\ .set_property(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;) 另一个更复杂的例子，使用 SASL_SSL 作为安全协议并使用 SCRAM-SHA-256 作为 SASL 机制：
Java KafkaSource.builder() .setProperty(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_SSL\u0026#34;) // SSL 配置 // 配置服务端提供的 truststore (CA 证书) 的路径 .setProperty(\u0026#34;ssl.truststore.location\u0026#34;, \u0026#34;/path/to/kafka.client.truststore.jks\u0026#34;) .setProperty(\u0026#34;ssl.truststore.password\u0026#34;, \u0026#34;test1234\u0026#34;) // 如果要求客户端认证，则需要配置 keystore (私钥) 的路径 .setProperty(\u0026#34;ssl.keystore.location\u0026#34;, \u0026#34;/path/to/kafka.client.keystore.jks\u0026#34;) .setProperty(\u0026#34;ssl.keystore.password\u0026#34;, \u0026#34;test1234\u0026#34;) // SASL 配置 // 将 SASL 机制配置为 as SCRAM-SHA-256 .setProperty(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;SCRAM-SHA-256\u0026#34;) // 配置 JAAS .setProperty(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;); Python KafkaSource.builder() \\ .set_property(\u0026#34;security.protocol\u0026#34;, \u0026#34;SASL_SSL\u0026#34;) \\ # SSL 配置 # 配置服务端提供的 truststore (CA 证书) 的路径 .set_property(\u0026#34;ssl.truststore.location\u0026#34;, \u0026#34;/path/to/kafka.client.truststore.jks\u0026#34;) \\ .set_property(\u0026#34;ssl.truststore.password\u0026#34;, \u0026#34;test1234\u0026#34;) \\ # 如果要求客户端认证，则需要配置 keystore (私钥) 的路径 .set_property(\u0026#34;ssl.keystore.location\u0026#34;, \u0026#34;/path/to/kafka.client.keystore.jks\u0026#34;) \\ .set_property(\u0026#34;ssl.keystore.password\u0026#34;, \u0026#34;test1234\u0026#34;) \\ # SASL 配置 # 将 SASL 机制配置为 SCRAM-SHA-256 .set_property(\u0026#34;sasl.mechanism\u0026#34;, \u0026#34;SCRAM-SHA-256\u0026#34;) \\ # 配置 JAAS .set_property(\u0026#34;sasl.jaas.config\u0026#34;, \u0026#34;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#34;) 如果在作业 JAR 中 Kafka 客户端依赖的类路径被重置了（relocate class），登录模块（login module）的类路径可能会不同，因此请根据登录模块在 JAR 中实际的类路径来改写以上配置。
关于安全配置的详细描述，请参阅 Apache Kafka 文档中的\u0026quot;安全\u0026quot;一节。
实现细节 # 如果你对 Kafka source 在新的 Source API 中的设计感兴趣，可阅读该部分作为参考。关于新 Source API 的细节，Source API 文档和 FLIP-27 提供了更详细的描述。 在新 Source API 的抽象中，Kafka source 由以下几个部分组成：
数据源分片（Source Split） # Kafka source 的数据源分片（source split）表示 Kafka topic 中的一个 partition。Kafka 的数据源分片包括：
该分片表示的 topic 和 partition 该 partition 的起始位点 该 partition 的停止位点，当 source 运行在批模式时适用 Kafka source 分片的状态同时存储该 partition 的当前消费位点，该分片状态将会在 Kafka 源读取器（source reader）进行快照（snapshot） 时将当前消费位点保存为起始消费位点以将分片状态转换成不可变更的分片。
可查看 KafkaPartitionSplit 和 KafkaPartitionSplitState 类来了解细节。
分片枚举器（Split Enumerator） # Kafka source 的分片枚举器负责检查在当前的 topic / partition 订阅模式下的新分片（partition），并将分片轮流均匀地分配给源读取器（source reader）。 注意 Kafka source 的分片枚举器会将分片主动推送给源读取器，因此它无需处理来自源读取器的分片请求。
源读取器（Source Reader） # Kafka source 的源读取器扩展了 SourceReaderBase，并使用单线程复用（single thread multiplex）的线程模型，使用一个由分片读取器 （split reader）驱动的 KafkaConsumer 来处理多个分片（partition）。消息会在从 Kafka 拉取下来后在分片读取器中立刻被解析。分片的状态 即当前的消息消费进度会在 KafkaRecordEmitter 中更新，同时会在数据发送至下游时指定事件时间。
Kafka SourceFunction # FlinkKafkaConsumer 已被弃用并将在 Flink 1.15 中移除，请改用 KafkaSource。 如需参考，请参阅 Flink 1.13 文档。
Kafka Sink # KafkaSink 可将数据流写入一个或多个 Kafka topic。
使用方法 # Kafka sink 提供了构建类来创建 KafkaSink 的实例。以下代码片段展示了如何将字符串数据按照至少一次（at lease once）的语义保证写入 Kafka topic：
Java DataStream\u0026lt;String\u0026gt; stream = ...; KafkaSink\u0026lt;String\u0026gt; sink = KafkaSink.\u0026lt;String\u0026gt;builder() .setBootstrapServers(brokers) .setRecordSerializer(KafkaRecordSerializationSchema.builder() .setTopic(\u0026#34;topic-name\u0026#34;) .setValueSerializationSchema(new SimpleStringSchema()) .build() ) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build(); stream.sinkTo(sink); Python sink = KafkaSink.builder() \\ .set_bootstrap_servers(brokers) \\ .set_record_serializer( KafkaRecordSerializationSchema.builder() .set_topic(\u0026#34;topic-name\u0026#34;) .set_value_serialization_schema(SimpleStringSchema()) .build() ) \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .build() stream.sink_to(sink) 以下属性在构建 KafkaSink 时是必须指定的：
Bootstrap servers, setBootstrapServers(String) 消息序列化器（Serializer）, setRecordSerializer(KafkaRecordSerializationSchema) 如果使用DeliveryGuarantee.EXACTLY_ONCE 的语义保证，则需要使用 setTransactionalIdPrefix(String) 序列化器 # 构建时需要提供 KafkaRecordSerializationSchema 来将输入数据转换为 Kafka 的 ProducerRecord。Flink 提供了 schema 构建器 以提供一些通用的组件，例如消息键（key）/消息体（value）序列化、topic 选择、消息分区，同样也可以通过实现对应的接口来进行更丰富的控制。
Java KafkaRecordSerializationSchema.builder() .setTopicSelector((element) -\u0026gt; {\u0026lt;your-topic-selection-logic\u0026gt;}) .setValueSerializationSchema(new SimpleStringSchema()) .setKeySerializationSchema(new SimpleStringSchema()) .setPartitioner(new FlinkFixedPartitioner()) .build(); Python KafkaRecordSerializationSchema.builder() \\ .set_topic_selector(lambda element: \u0026lt;your-topic-selection-logic\u0026gt;) \\ .set_value_serialization_schema(SimpleStringSchema()) \\ .set_key_serialization_schema(SimpleStringSchema()) \\ # set partitioner is not supported in PyFlink .build() 其中消息体（value）序列化方法和 topic 的选择方法是必须指定的，此外也可以通过 setKafkaKeySerializer(Serializer) 或 setKafkaValueSerializer(Serializer) 来使用 Kafka 提供而非 Flink 提供的序列化器。
容错 # KafkaSink 总共支持三种不同的语义保证（DeliveryGuarantee）。对于 DeliveryGuarantee.AT_LEAST_ONCE 和 DeliveryGuarantee.EXACTLY_ONCE，Flink checkpoint 必须启用。默认情况下 KafkaSink 使用 DeliveryGuarantee.NONE。 以下是对不同语义保证的解释：
DeliveryGuarantee.NONE 不提供任何保证：消息有可能会因 Kafka broker 的原因发生丢失或因 Flink 的故障发生重复。 DeliveryGuarantee.AT_LEAST_ONCE: sink 在 checkpoint 时会等待 Kafka 缓冲区中的数据全部被 Kafka producer 确认。消息不会因 Kafka broker 端发生的事件而丢失，但可能会在 Flink 重启时重复，因为 Flink 会重新处理旧数据。 DeliveryGuarantee.EXACTLY_ONCE: 该模式下，Kafka sink 会将所有数据通过在 checkpoint 时提交的事务写入。因此，如果 consumer 只读取已提交的数据（参见 Kafka consumer 配置 isolation.level），在 Flink 发生重启时不会发生数据重复。然而这会使数据在 checkpoint 完成时才会可见，因此请按需调整 checkpoint 的间隔。请确认事务 ID 的前缀（transactionIdPrefix）对不同的应用是唯一的，以保证不同作业的事务 不会互相影响！此外，强烈建议将 Kafka 的事务超时时间调整至远大于 checkpoint 最大间隔 + 最大重启时间，否则 Kafka 对未提交事务的过期处理会导致数据丢失。 监控 # Kafka sink 会在不同的范围（Scope）中汇报下列指标。
范围 指标 用户变量 描述 类型 算子 currentSendTime n/a 发送最近一条数据的耗时。该指标反映最后一条数据的瞬时值。 Gauge Kafka Producer # FlinkKafkaProducer 已被弃用并将在 Flink 1.15 中移除，请改用 KafkaSink。 如需参考，请参阅 Flink 1.13 文档。
Kafka 连接器指标 # Flink 的 Kafka 连接器通过 Flink 的指标系统提供一些指标来帮助分析 connector 的行为。 各个版本的 Kafka producer 和 consumer 会通过 Flink 的指标系统汇报 Kafka 内部的指标。 该 Kafka 文档列出了所有汇报的指标。
同样也可通过将 Kafka source 在该章节描述的 register.consumer.metrics，或 Kafka sink 的 register.producer.metrics 配置设置为 false 来关闭 Kafka 指标的注册。
启用 Kerberos 身份验证 # Flink 通过 Kafka 连接器提供了一流的支持，可以对 Kerberos 配置的 Kafka 安装进行身份验证。只需在 flink-conf.yaml 中配置 Flink。像这样为 Kafka 启用 Kerberos 身份验证：
通过设置以下内容配置 Kerberos 票据 security.kerberos.login.use-ticket-cache：默认情况下，这个值是 true，Flink 将尝试在 kinit 管理的票据缓存中使用 Kerberos 票据。注意！在 YARN 上部署的 Flink jobs 中使用 Kafka 连接器时，使用票据缓存的 Kerberos 授权将不起作用。 security.kerberos.login.keytab 和 security.kerberos.login.principal：要使用 Kerberos keytabs，需为这两个属性设置值。 将 KafkaClient 追加到 security.kerberos.login.contexts：这告诉 Flink 将配置的 Kerberos 票据提供给 Kafka 登录上下文以用于 Kafka 身份验证。 一旦启用了基于 Kerberos 的 Flink 安全性后，只需在提供的属性配置中包含以下两个设置（通过传递给内部 Kafka 客户端），即可使用 Flink Kafka Consumer 或 Producer 向 Kafk a进行身份验证：
将 security.protocol 设置为 SASL_PLAINTEXT（默认为 NONE）：用于与 Kafka broker 进行通信的协议。使用独立 Flink 部署时，也可以使用 SASL_SSL；请在此处查看如何为 SSL 配置 Kafka 客户端。 将 sasl.kerberos.service.name 设置为 kafka（默认为 kafka）：此值应与用于 Kafka broker 配置的 sasl.kerberos.service.name 相匹配。客户端和服务器配置之间的服务名称不匹配将导致身份验证失败。 有关 Kerberos 安全性 Flink 配置的更多信息，请参见这里。你也可以在这里进一步了解 Flink 如何在内部设置基于 kerberos 的安全性。
升级到最近的连接器版本 # 通用的升级步骤概述见 升级 Jobs 和 Flink 版本指南。对于 Kafka，你还需要遵循这些步骤：
不要同时升级 Flink 和 Kafka 连接器 确保你对 Consumer 设置了 group.id 在 Consumer 上设置 setCommitOffsetsOnCheckpoints(true)，以便读 offset 提交到 Kafka。务必在停止和恢复 savepoint 前执行此操作。你可能需要在旧的连接器版本上进行停止/重启循环来启用此设置。 在 Consumer 上设置 setStartFromGroupOffsets(true)，以便我们从 Kafka 获取读 offset。这只会在 Flink 状态中没有读 offset 时生效，这也是为什么下一步非要重要的原因。 修改 source/sink 分配到的 uid。这会确保新的 source/sink 不会从旧的 sink/source 算子中读取状态。 使用 --allow-non-restored-state 参数启动新 job，因为我们在 savepoint 中仍然有先前连接器版本的状态。 问题排查 # 如果你在使用 Flink 时对 Kafka 有问题，请记住，Flink 只封装 KafkaConsumer 或 KafkaProducer，你的问题可能独立于 Flink，有时可以通过升级 Kafka broker 程序、重新配置 Kafka broker 程序或在 Flink 中重新配置 KafkaConsumer 或 KafkaProducer 来解决。下面列出了一些常见问题的示例。 数据丢失 # 根据你的 Kafka 配置，即使在 Kafka 确认写入后，你仍然可能会遇到数据丢失。特别要记住在 Kafka 的配置中设置以下属性：
acks log.flush.interval.messages log.flush.interval.ms log.flush.* 上述选项的默认值是很容易导致数据丢失的。请参考 Kafka 文档以获得更多的解释。
UnknownTopicOrPartitionException # 导致此错误的一个可能原因是正在进行新的 leader 选举，例如在重新启动 Kafka broker 之后或期间。这是一个可重试的异常，因此 Flink job 应该能够重启并恢复正常运行。也可以通过更改 producer 设置中的 retries 属性来规避。但是，这可能会导致重新排序消息，反过来可以通过将 max.in.flight.requests.per.connection 设置为 1 来避免不需要的消息。
ProducerFencedException # 这个错误是由于 FlinkKafkaProducer 所生成的 transactional.id 与其他应用所使用的的产生了冲突。多数情况下，由于 FlinkKafkaProducer 产生的 ID 都是以 taskName + \u0026quot;-\u0026quot; + operatorUid 为前缀的，这些产生冲突的应用也是使用了相同 Job Graph 的 Flink Job。 我们可以使用 setTransactionalIdPrefix() 方法来覆盖默认的行为，为每个不同的 Job 分配不同的 transactional.id 前缀来解决这个问题。
Back to top
`}),e.add({id:74,href:"/flink/flink-docs-master/zh/docs/connectors/table/kafka/",title:"Kafka",section:"Table API Connectors",content:" Apache Kafka SQL 连接器 # Scan Source: Unbounded Sink: Streaming Append Mode\nKafka 连接器提供从 Kafka topic 中消费和写入数据的能力。\n依赖 # In order to use the Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nKafka version Maven dependency SQL Client JAR universal \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Kafka 连接器目前并不包含在 Flink 的二进制发行版中，请查阅这里了解如何在集群运行中引用 Kafka 连接器。\n如何创建 Kafka 表 # 以下示例展示了如何创建 Kafka 表：\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ) 可用的元数据 # 以下的连接器元数据可以在表定义中通过元数据列的形式获取。\nR/W 列定义了一个元数据是可读的（R）还是可写的（W）。 只读列必须声明为 VIRTUAL 以在 INSERT INTO 操作中排除它们。\n键 数据类型 描述 R/W topic STRING NOT NULL Kafka 记录的 Topic 名。 R partition INT NOT NULL Kafka 记录的 partition ID。 R headers MAP NOT NULL 二进制 Map 类型的 Kafka 记录头（Header）。 R/W leader-epoch INT NULL Kafka 记录的 Leader epoch（如果可用）。 R offset BIGINT NOT NULL Kafka 记录在 partition 中的 offset。 R timestamp TIMESTAMP_LTZ(3) NOT NULL Kafka 记录的时间戳。 R/W timestamp-type STRING NOT NULL Kafka 记录的时间戳类型。可能的类型有 \"NoTimestampType\"， \"CreateTime\"（会在写入元数据时设置），或 \"LogAppendTime\"。 R 以下扩展的 CREATE TABLE 示例展示了使用这些元数据字段的语法：\nCREATE TABLE KafkaTable ( `event_time` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `partition` BIGINT METADATA VIRTUAL, `offset` BIGINT METADATA VIRTUAL, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); 格式元信息\n连接器可以读出消息格式的元数据。格式元数据的配置键以 'value.' 作为前缀。\n以下示例展示了如何获取 Kafka 和 Debezium 的元数据字段：\nCREATE TABLE KafkaTable ( `event_time` TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, -- from Debezium format `origin_table` STRING METADATA FROM \u0026#39;value.source.table\u0026#39; VIRTUAL, -- from Debezium format `partition_id` BIGINT METADATA FROM \u0026#39;partition\u0026#39; VIRTUAL, -- from Kafka connector `offset` BIGINT METADATA VIRTUAL, -- from Kafka connector `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39; ); 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必选 （无） String 指定使用的连接器，Kafka 连接器使用 'kafka'。 topic required for sink （无） String 当表用作 source 时读取数据的 topic 名。亦支持用分号间隔的 topic 列表，如 'topic-1;topic-2'。注意，对 source 表而言，'topic' 和 'topic-pattern' 两个选项只能使用其中一个。当表被用作 sink 时，该配置表示写入的 topic 名。注意 sink 表不支持 topic 列表。 topic-pattern 可选 （无） String 匹配读取 topic 名称的正则表达式。在作业开始运行时，所有匹配该正则表达式的 topic 都将被 Kafka consumer 订阅。注意，对 source 表而言，'topic' 和 'topic-pattern' 两个选项只能使用其中一个。 properties.bootstrap.servers 必选 （无） String 逗号分隔的 Kafka broker 列表。 properties.group.id 对 source 可选，不适用于 sink （无） String Kafka source 的消费组 id。如果未指定消费组 ID，则会使用自动生成的 \"KafkaSource-{tableIdentifier}\" 作为消费组 ID。 properties.* 可选 （无） String 可以设置和传递任意 Kafka 的配置项。后缀名必须匹配在 Kafka 配置文档 中定义的配置键。Flink 将移除 \"properties.\" 配置键前缀并将变换后的配置键和值传入底层的 Kafka 客户端。例如，你可以通过 'properties.allow.auto.create.topics' = 'false' 来禁用 topic 的自动创建。但是某些配置项不支持进行配置，因为 Flink 会覆盖这些配置，例如 'key.deserializer' 和 'value.deserializer'。 format 必选 （无） String 用来序列化或反序列化 Kafka 消息的格式。 请参阅 格式 页面以获取更多关于格式的细节和相关配置项。 注意：该配置项和 'value.format' 二者必需其一。 key.format 可选 （无） String 用来序列化和反序列化 Kafka 消息键（Key）的格式。 请参阅 格式 页面以获取更多关于格式的细节和相关配置项。 注意：如果定义了键格式，则配置项 'key.fields' 也是必需的。 否则 Kafka 记录将使用空值作为键。 key.fields 可选 [] List\u0026lt;String\u0026gt; 表结构中用来配置消息键（Key）格式数据类型的字段列表。默认情况下该列表为空，因此消息键没有定义。 列表格式为 'field1;field2'。 key.fields-prefix 可选 （无） String 为所有消息键（Key）格式字段指定自定义前缀，以避免与消息体（Value）格式字段重名。默认情况下前缀为空。 如果定义了前缀，表结构和配置项 'key.fields' 都需要使用带前缀的名称。 当构建消息键格式字段时，前缀会被移除，消息键格式将会使用无前缀的名称。 请注意该配置项要求必须将 'value.fields-include' 配置为 'EXCEPT_KEY'。 value.format 必选 （无） String 序列化和反序列化 Kafka 消息体时使用的格式。 请参阅 格式 页面以获取更多关于格式的细节和相关配置项。 注意：该配置项和 'format' 二者必需其一。 value.fields-include 可选 ALL 枚举类型\n可选值：[ALL, EXCEPT_KEY] 定义消息体（Value）格式如何处理消息键（Key）字段的策略。 默认情况下，表结构中 'ALL' 即所有的字段都会包含在消息体格式中，即消息键字段在消息键和消息体格式中都会出现。 scan.startup.mode 可选 group-offsets String Kafka consumer 的启动模式。有效值为：'earliest-offset'，'latest-offset'，'group-offsets'，'timestamp' 和 'specific-offsets'。 请参阅下方 起始消费位点 以获取更多细节。 scan.startup.specific-offsets 可选 （无） String 在使用 'specific-offsets' 启动模式时为每个 partition 指定 offset，例如 'partition:0,offset:42;partition:1,offset:300'。 scan.startup.timestamp-millis 可选 （无） Long 在使用 'timestamp' 启动模式时指定启动的时间戳（单位毫秒）。 scan.topic-partition-discovery.interval 可选 （无） Duration Consumer 定期探测动态创建的 Kafka topic 和 partition 的时间间隔。 sink.partitioner 可选 'default' String Flink partition 到 Kafka partition 的分区映射关系，可选值有： default：使用 Kafka 默认的分区器对消息进行分区。 fixed：每个 Flink partition 最终对应最多一个 Kafka partition。 round-robin：Flink partition 按轮循（round-robin）的模式对应到 Kafka partition。只有当未指定消息的消息键时生效。 自定义 FlinkKafkaPartitioner 的子类：例如 'org.mycompany.MyPartitioner'。 请参阅下方 Sink 分区 以获取更多细节。 sink.semantic 可选 at-least-once String 定义 Kafka sink 的语义。有效值为 'at-least-once'，'exactly-once' 和 'none'。请参阅 一致性保证 以获取更多细节。 sink.parallelism 可选 （无） Integer 定义 Kafka sink 算子的并行度。默认情况下，并行度由框架定义为与上游串联的算子相同。 特性 # 消息键（Key）与消息体（Value）的格式 # Kafka 消息的消息键和消息体部分都可以使用某种 格式 来序列化或反序列化成二进制数据。\n消息体格式\n由于 Kafka 消息中消息键是可选的，以下语句将使用消息体格式读取和写入消息，但不使用消息键格式。 'format' 选项与 'value.format' 意义相同。 所有的格式配置使用格式识别符作为前缀。\nCREATE TABLE KafkaTable ( `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) 消息体格式将配置为以下的数据类型：\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT, `behavior` STRING\u0026gt; 消息键和消息体格式\n以下示例展示了如何配置和使用消息键和消息体格式。 格式配置使用 'key' 或 'value' 加上格式识别符作为前缀。\nCREATE TABLE KafkaTable ( `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;user_id;item_id\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;ALL\u0026#39; ) 消息键格式包含了在 'key.fields' 中列出的字段（使用 ';' 分隔）和字段顺序。 因此将配置为以下的数据类型：\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT\u0026gt; 由于消息体格式配置为 'value.fields-include' = 'ALL'，所以消息键字段也会出现在消息体格式的数据类型中：\nROW\u0026lt;`user_id` BIGINT, `item_id` BIGINT, `behavior` STRING\u0026gt; 重名的格式字段\n如果消息键字段和消息体字段重名，连接器无法根据表结构信息将这些列区分开。 'key.fields-prefix' 配置项可以在表结构中为消息键字段指定一个唯一名称，并在配置消息键格式的时候保留原名。\n以下示例展示了在消息键和消息体中同时包含 version 字段的情况：\nCREATE TABLE KafkaTable ( `k_version` INT, `k_user_id` BIGINT, `k_item_id` BIGINT, `version` INT, `behavior` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;k_\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;k_version;k_user_id;k_item_id\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) 消息体格式必须配置为 'EXCEPT_KEY' 模式。格式将被配置为以下的数据类型：\n消息键格式： ROW\u0026lt;`version` INT, `user_id` BIGINT, `item_id` BIGINT\u0026gt; 消息体格式： ROW\u0026lt;`version` INT, `behavior` STRING\u0026gt; Topic 和 Partition 的探测 # topic 和 topic-pattern 配置项决定了 source 消费的 topic 或 topic 的匹配规则。topic 配置项可接受使用分号间隔的 topic 列表，例如 topic-1;topic-2。 topic-pattern 配置项使用正则表达式来探测匹配的 topic。例如 topic-pattern 设置为 test-topic-[0-9]，则在作业启动时，所有匹配该正则表达式的 topic（以 test-topic- 开头，以一位数字结尾）都将被 consumer 订阅。\n为允许 consumer 在作业启动之后探测到动态创建的 topic，请将 scan.topic-partition-discovery.interval 配置为一个非负值。这将使 consumer 能够探测匹配名称规则的 topic 中新的 partition。\n请参阅 Kafka DataStream 连接器文档 以获取更多关于 topic 和 partition 探测的信息。\n注意 topic 列表和 topic 匹配规则只适用于 source。对于 sink 端，Flink 目前只支持单一 topic。\n起始消费位点 # scan.startup.mode 配置项决定了 Kafka consumer 的启动模式。有效值为：\n`group-offsets`：从 Zookeeper/Kafka 中某个指定的消费组已提交的偏移量开始。 `earliest-offset`：从可能的最早偏移量开始。 `latest-offset`：从最末尾偏移量开始。 `timestamp`：从用户为每个 partition 指定的时间戳开始。 `specific-offsets`：从用户为每个 partition 指定的偏移量开始。 默认值 group-offsets 表示从 Zookeeper/Kafka 中最近一次已提交的偏移量开始消费。\n如果使用了 timestamp，必须使用另外一个配置项 scan.startup.timestamp-millis 来指定一个从格林尼治标准时间 1970 年 1 月 1 日 00:00:00.000 开始计算的毫秒单位时间戳作为起始时间。\n如果使用了 specific-offsets，必须使用另外一个配置项 scan.startup.specific-offsets 来为每个 partition 指定起始偏移量， 例如，选项值 partition:0,offset:42;partition:1,offset:300 表示 partition 0 从偏移量 42 开始，partition 1 从偏移量 300 开始。\nCDC 变更日志（Changelog） Source # Flink 原生支持使用 Kafka 作为 CDC 变更日志（changelog） source。如果 Kafka topic 中的消息是通过变更数据捕获（CDC）工具从其他数据库捕获的变更事件，则你可以使用 CDC 格式将消息解析为 Flink SQL 系统中的插入（INSERT）、更新（UPDATE）、删除（DELETE）消息。\n在许多情况下，变更日志（changelog） source 都是非常有用的功能，例如将数据库中的增量数据同步到其他系统，审核日志，数据库的物化视图，时态表关联数据库表的更改历史等。\nFlink 提供了几种 CDC 格式：\ndebezium canal maxwell Sink 分区 # 配置项 sink.partitioner 指定了从 Flink 分区到 Kafka 分区的映射关系。 默认情况下，Flink 使用 Kafka 默认分区器 来对消息分区。默认分区器对没有消息键的消息使用 粘性分区策略（sticky partition strategy） 进行分区，对含有消息键的消息使用 murmur2 哈希算法计算分区。\n为了控制数据行到分区的路由，也可以提供一个自定义的 sink 分区器。\u0026lsquo;fixed\u0026rsquo; 分区器会将同一个 Flink 分区中的消息写入同一个 Kafka 分区，从而减少网络连接的开销。\n一致性保证 # 默认情况下，如果查询在 启用 checkpoint 模式下执行时，Kafka sink 按照至少一次（at-lease-once）语义保证将数据写入到 Kafka topic 中。\n当 Flink checkpoint 启用时，kafka 连接器可以提供精确一次（exactly-once）的语义保证。\n除了启用 Flink checkpoint，还可以通过传入对应的 sink.semantic 选项来选择三种不同的运行模式：\nnone：Flink 不保证任何语义。已经写出的记录可能会丢失或重复。 at-least-once (默认设置)：保证没有记录会丢失（但可能会重复）。 exactly-once：使用 Kafka 事务提供精确一次（exactly-once）语义。当使用事务向 Kafka 写入数据时，请将所有从 Kafka 中消费记录的应用中的 isolation.level 配置项设置成实际所需的值（read_committed 或 read_uncommitted，后者为默认值）。 请参阅 Kafka 文档 以获取更多关于语义保证的信息。\nSource 按分区 Watermark # Flink 对于 Kafka 支持发送按分区的 watermark。Watermark 在 Kafka consumer 中生成。 按分区 watermark 的合并方式和在流 shuffle 时合并 Watermark 的方式一致。 Source 输出的 watermark 由读取的分区中最小的 watermark 决定。 如果 topic 中的某些分区闲置，watermark 生成器将不会向前推进。 你可以在表配置中设置 'table.exec.source.idle-timeout' 选项来避免上述问题。\n请参阅 Kafka watermark 策略 以获取更多细节。\n安全 # 要启用加密和认证相关的安全配置，只需将安全配置加上 \u0026ldquo;properties.\u0026rdquo; 前缀配置在 Kafka 表上即可。下面的代码片段展示了如何配置 Kafka 表以使用 PLAIN 作为 SASL 机制并提供 JAAS 配置：\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_PLAINTEXT\u0026#39;, \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;PLAIN\u0026#39;, \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#39; ) 另一个更复杂的例子，使用 SASL_SSL 作为安全协议并使用 SCRAM-SHA-256 作为 SASL 机制：\nCREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... \u0026#39;properties.security.protocol\u0026#39; = \u0026#39;SASL_SSL\u0026#39;, /* SSL 配置 */ /* 配置服务端提供的 truststore (CA 证书) 的路径 */ \u0026#39;properties.ssl.truststore.location\u0026#39; = \u0026#39;/path/to/kafka.client.truststore.jks\u0026#39;, \u0026#39;properties.ssl.truststore.password\u0026#39; = \u0026#39;test1234\u0026#39;, /* 如果要求客户端认证，则需要配置 keystore (私钥) 的路径 */ \u0026#39;properties.ssl.keystore.location\u0026#39; = \u0026#39;/path/to/kafka.client.keystore.jks\u0026#39;, \u0026#39;properties.ssl.keystore.password\u0026#39; = \u0026#39;test1234\u0026#39;, /* SASL 配置 */ /* 将 SASL 机制配置为 as SCRAM-SHA-256 */ \u0026#39;properties.sasl.mechanism\u0026#39; = \u0026#39;SCRAM-SHA-256\u0026#39;, /* 配置 JAAS */ \u0026#39;properties.sasl.jaas.config\u0026#39; = \u0026#39;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\u0026#34;username\\\u0026#34; password=\\\u0026#34;password\\\u0026#34;;\u0026#39; ) 如果在作业 JAR 中 Kafka 客户端依赖的类路径被重置了（relocate class），登录模块（login module）的类路径可能会不同，因此请根据登录模块在 JAR 中实际的类路径来改写以上配置。例如在 SQL client JAR 中，Kafka client 依赖被重置在了 org.apache.flink.kafka.shaded.org.apache.kafka 路径下， 因此 plain 登录模块的类路径应写为 org.apache.flink.kafka.shaded.org.apache.kafka.common.security.plain.PlainLoginModule。\n关于安全配置的详细描述，请参阅 Apache Kafka 文档中的\u0026quot;安全\u0026quot;一节。\n数据类型映射 # Kafka 将消息键值以二进制进行存储，因此 Kafka 并不存在 schema 或数据类型。Kafka 消息使用格式配置进行序列化和反序列化，例如 csv，json，avro。 因此，数据类型映射取决于使用的格式。请参阅 格式 页面以获取更多细节。\nBack to top\n"}),e.add({id:75,href:"/flink/flink-docs-master/zh/docs/deployment/security/security-kerberos/",title:"Kerberos",section:"Security",content:` Kerberos 身份认证设置和配置 # 本文简要描述了 Flink 如何在各种部署机制（Standalone, native Kubernetes, YARN）、文件系统、connector 以及 state backend 的上下文中安全工作。
目标 # Flink Kerberos 安全框架的主要目标如下：
在集群内使用 connector（例如 Kafka）时确保作业安全地访问数据； 对 zookeeper 进行身份认证（如果配置了 SASL）； 对 Hadoop 组件进行身份认证（例如 HDFS，HBASE）。 生产部署场景中，流式作业通常会运行很长一段时间（天、周、月级别的时间段），并且需要在作业的整个生命周期中对其进行身份认证以保护数据源。与 Hadoop delegation token 和 ticket 缓存项不同，Kerberos keytab 不会在该时间段内过期。
当前的实现支持使用可配置的 keytab credential 或 Hadoop delegation token 来运行 Flink 集群（JobManager / TaskManager / 作业）。
请注意，所有作业都能共享为指定集群配置的凭据。如果想为一个作业使用不同的 keytab，只需单独启动一个具有不同配置的 Flink 集群。多个 Flink 集群可以在 Kubernetes 或 YARN 环境中并行运行。
Flink Security 如何工作 # 理论上，Flink 程序可以使用自己的或第三方的 connector（Kafka、HDFS、Cassandra、Flume、Kinesis 等），同时需要支持任意的认证方式（Kerberos、SSL/TLS、用户名/密码等）。满足所有 connector 的安全需求还在进行中，不过 Flink 提供了针对 Kerberos 身份认证的一流支持。Kerberos 身份认证支持以下服务和 connector：
Kafka (0.9+) HDFS HBase ZooKeeper 请注意，你可以单独为每个服务或 connector 启用 Kerberos。例如，用户可以启用 Hadoop security，而无需为 ZooKeeper 开启 Kerberos，反之亦然。Kerberos 凭证是组件之间共享的配置，每个组件会显式地使用它。
Flink 安全内部架构是建立在安全模块（实现 org.apache.flink.runtime.security.modules.SecurityModule）上的，安全模块在 Flink 启动过程中被安装。后面部分描述了每个安全模块。
Hadoop Security 模块 # 该模块使用 Hadoop UserGroupInformation（UGI）类来建立进程范围的 登录用户 上下文。然后，登录用户用于与 Hadoop 组件的所有交互，包括 HDFS、HBase 和 YARN。
如果启用了 Hadoop security（在 core-site.xml 中），登录用户将拥有所有配置的 Kerberos 凭据。否则，登录用户仅继承启动集群的操作系统帐户的用户身份。
JAAS Security 模块 # 该模块为集群提供动态 JAAS 配置，使已配置的 Kerberos 凭证对 ZooKeeper、Kafka 和其他依赖 JAAS 的组件可用。
请注意，用户还可以使用 Java SE 文档中描述的机制提供静态 JAAS 配置文件。静态配置项会覆盖此模块提供的任何动态配置项。
ZooKeeper Security 模块 # 该模块配置某些进程范围内 ZooKeeper 安全相关的设置，即 ZooKeeper 服务名称（默认为：zookeeper）和 JAAS 登录上下文名称（默认为：Client）。
部署模式 # 以下是针对每种部署模式的一些信息。
Standalone 模式 # 在 standalone 模式或集群模式下运行安全 Flink 集群的步骤如下：
将与安全相关的配置选项添加到 Flink 配置文件（在所有集群节点上执行）（详见此处）。 确保 keytab 文件存在于每个群集节点通过 security.kerberos.login.keytab 指定的路径上。 正常部署 Flink 集群。 原生 Kubernetes 和 YARN 模式 # 在原生 Kubernetes 或 YARN 模式下运行安全 Flink 集群的步骤如下：
在客户端的 Flink 配置文件中添加安全相关的配置选项（详见此处）。 确保 keytab 文件存在于客户端通过 security.kerberos.login.keytab 指定的路径上。 正常部署 Flink 集群。 在 YARN 和 原生 Kubernetes 模式下，keytab 文件会被自动从客户端拷贝到 Flink 容器中。
要启用 Kerberos 身份认证，还需要 Kerberos 配置文件。该文件可以从集群环境中获取，也可以由 Flink 上传。针对后者，你需要配置 security.kerberos.krb5-conf.path 来指定 Kerberos 配置文件的路径，Flink 会将此文件复制到相应容器或 pod。
请参阅 YARN security 文档获取更多相关信息。
使用 kinit (仅限 YARN) # 在 YARN 模式下，可以不需要 keytab 而只使用 ticket 缓存（由 kinit 管理）来部署一个安全的 Flink 集群。这避免了生成 keytab 的复杂性，同时避免了将其委托给集群管理器。在这种情况下，使用 Flink CLI 获取 Hadoop delegation token（用于 HDFS 和 HBase）。主要缺点是集群必须是短暂的，因为生成的 delegation token 将会过期（通常在一周内）。
使用 kinit 运行安全 Flink 集群的步骤如下：
在客户端的 Flink 配置文件中添加安全相关的配置选项（详见此处）。 使用 kinit 命令登录。 正常部署 Flink 集群。 更多细节 # Ticket 更新 # 使用 Kerberos 的每个组件都独立负责更新 Kerberos ticket-granting-ticket（TGT）。Hadoop、ZooKeeper 和 Kafka 都在提供 keytab 时自动更新 TGT。在 delegation token 场景中，YARN 本身会更新 token（更新至其最大生命周期）。
Back to top
`}),e.add({id:76,href:"/flink/flink-docs-master/zh/docs/deployment/ha/kubernetes_ha/",title:"Kubernetes 高可用服务",section:"高可用",content:` Kubernetes 高可用服务 # Flink 的 Kubernetes 高可用模式使用 Kubernetes 提供高可用服务。
Kubernetes 高可用服务只能在部署到 Kubernetes 时使用。因此，当使用 在 Kubernetes 上单节点部署 Flink 或 Flink 原生 Kubernetes 集成 两种模式时，可以对它们进行配置。
准备 # 为了使用 Flink 的 Kubernetes 高可用服务，你必须满足以下先决条件:
Kubernetes \u0026gt;= 1.9. 具有创建、编辑、删除 ConfigMaps 权限的服务帐户。想了解更多信息，请查看如何在 Flink 原生 Kubernetes 集成 和 在 Kubernetes 上单节点部署 Flink 两种模式中配置服务帐户。 配置 # 为了启用高可用集群（HA-cluster），你必须设置以下配置项:
high-availability (必要的): high-availability 选项必须设置为 KubernetesHaServicesFactory. high-availability: kubernetes high-availability.storageDir (必要的): JobManager 元数据持久化到文件系统 high-availability.storageDir 配置的路径中，并且在 Kubernetes 中只能有一个目录指向此位置。 high-availability.storageDir: s3://flink/recovery storageDir 存储要从 JobManager 失败恢复时所需的所有元数据。
kubernetes.cluster-id (必要的): 为了识别 Flink 集群，你必须指定 kubernetes.cluster-id。 kubernetes.cluster-id: cluster1337 配置示例 # 在 conf/flink-conf.yaml 中配置高可用模式:
kubernetes.cluster-id: \u0026lt;cluster-id\u0026gt; high-availability: kubernetes high-availability.storageDir: hdfs:///flink/recovery Back to top
高可用数据清理 # 要在重新启动 Flink 集群时保留高可用数据，只需删除部署（通过 kubectl delete deployment \u0026lt;cluster-id\u0026gt;）。所有与 Flink 集群相关的资源将被删除（例如：JobManager Deployment、TaskManager pods、services、Flink conf ConfigMap）。高可用相关的 ConfigMaps 将被保留，因为它们没有设置所有者引用。当重新启动集群时，所有以前运行的作业将从最近成功的检查点恢复并重新启动。
Back to top
`}),e.add({id:77,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/native_kubernetes/",title:"Native Kubernetes",section:"Resource Providers",content:` Native Kubernetes # This page describes how to deploy Flink natively on Kubernetes.
Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on Kubernetes.
Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink\u0026rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.
Preparation # The Getting Started section assumes a running Kubernetes cluster fulfilling the following requirements:
Kubernetes \u0026gt;= 1.9. KubeConfig, which has access to list, create, delete pods and services, configurable via ~/.kube/config. You can verify permissions by running kubectl auth can-i \u0026lt;list|create|edit|delete\u0026gt; pods. Enabled Kubernetes DNS. default service account with RBAC permissions to create, delete pods. If you have problems setting up a Kubernetes cluster, then take a look at how to setup a Kubernetes cluster.
Starting a Flink Session on Kubernetes # Once you have your Kubernetes cluster running and kubectl is configured to point to it, you can launch a Flink cluster in Session Mode via
# (1) Start Kubernetes session \$ ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=my-first-flink-cluster # (2) Submit example job \$ ./bin/flink run \\ --target kubernetes-session \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ ./examples/streaming/TopSpeedWindowing.jar # (3) Stop Kubernetes session by deleting cluster deployment \$ kubectl delete deployment/my-first-flink-cluster In default, Flink’s Web UI and REST endpoint are exposed as ClusterIP service. To access the service, please refer to Accessing Flink’s Web UI for instructions. Congratulations! You have successfully run a Flink application by deploying Flink on Kubernetes.
Back to top
Deployment Modes # For production use, we recommend deploying Flink Applications in the Application Mode, as these modes provide a better isolation for the Applications.
Application Mode # The Application Mode requires that the user code is bundled together with the Flink image because it runs the user code\u0026rsquo;s main() method on the cluster. The Application Mode makes sure that all Flink components are properly cleaned up after the termination of the application.
The Flink community provides a base Docker image which can be used to bundle the user code:
FROM flink RUN mkdir -p \$FLINK_HOME/usrlib COPY /path/of/my-flink-job.jar \$FLINK_HOME/usrlib/my-flink-job.jar After creating and publishing the Docker image under custom-image-name, you can start an Application cluster with the following command:
\$ ./bin/flink run-application \\ --target kubernetes-application \\ -Dkubernetes.cluster-id=my-first-application-cluster \\ -Dkubernetes.container.image=custom-image-name \\ local:///opt/flink/usrlib/my-flink-job.jar Note local is the only supported scheme in Application Mode.
The kubernetes.cluster-id option specifies the cluster name and must be unique. If you do not specify this option, then Flink will generate a random name.
The kubernetes.container.image option specifies the image to start the pods with.
Once the application cluster is deployed you can interact with it:
# List running job on the cluster \$ ./bin/flink list --target kubernetes-application -Dkubernetes.cluster-id=my-first-application-cluster # Cancel running job \$ ./bin/flink cancel --target kubernetes-application -Dkubernetes.cluster-id=my-first-application-cluster \u0026lt;jobId\u0026gt; You can override configurations set in conf/flink-conf.yaml by passing key-value pairs -Dkey=value to bin/flink.
Per-Job Cluster Mode # Flink on Kubernetes does not support Per-Job Cluster Mode.
Session Mode # You have seen the deployment of a Session cluster in the Getting Started guide at the top of this page.
The Session Mode can be executed in two modes:
detached mode (default): The kubernetes-session.sh deploys the Flink cluster on Kubernetes and then terminates.
attached mode (-Dexecution.attached=true): The kubernetes-session.sh stays alive and allows entering commands to control the running Flink cluster. For example, stop stops the running Session cluster. Type help to list all supported commands.
In order to re-attach to a running Session cluster with the cluster id my-first-flink-cluster use the following command:
\$ ./bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ -Dexecution.attached=true You can override configurations set in conf/flink-conf.yaml by passing key-value pairs -Dkey=value to bin/kubernetes-session.sh.
Stop a Running Session Cluster # In order to stop a running Session Cluster with cluster id my-first-flink-cluster you can either delete the Flink deployment or use:
\$ echo \u0026#39;stop\u0026#39; | ./bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=my-first-flink-cluster \\ -Dexecution.attached=true Back to top
Flink on Kubernetes Reference # Configuring Flink on Kubernetes # The Kubernetes-specific configuration options are listed on the configuration page.
Flink uses Fabric8 Kubernetes client to communicate with Kubernetes APIServer to create/delete Kubernetes resources(e.g. Deployment, Pod, ConfigMap, Service, etc.), as well as watch the Pods and ConfigMaps. Except for the above Flink config options, some expert options of Fabric8 Kubernetes client could be configured via system properties or environment variables.
For example, users could use the following Flink config options to set the concurrent max requests.
containerized.master.env.KUBERNETES_MAX_CONCURRENT_REQUESTS: 200 env.java.opts.jobmanager: \u0026#34;-Dkubernetes.max.concurrent.requests=200\u0026#34; Accessing Flink\u0026rsquo;s Web UI # Flink\u0026rsquo;s Web UI and REST endpoint can be exposed in several ways via the kubernetes.rest-service.exposed.type configuration option.
ClusterIP: Exposes the service on a cluster-internal IP. The Service is only reachable within the cluster. If you want to access the JobManager UI or submit job to the existing session, you need to start a local proxy. You can then use localhost:8081 to submit a Flink job to the session or view the dashboard. \$ kubectl port-forward service/\u0026lt;ServiceName\u0026gt; 8081 NodePort: Exposes the service on each Node’s IP at a static port (the NodePort). \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt; can be used to contact the JobManager service.
LoadBalancer: Exposes the service externally using a cloud provider’s load balancer. Since the cloud provider and Kubernetes needs some time to prepare the load balancer, you may get a NodePort JobManager Web Interface in the client log. You can use kubectl get services/\u0026lt;cluster-id\u0026gt;-rest to get EXTERNAL-IP and construct the load balancer JobManager Web Interface manually http://\u0026lt;EXTERNAL-IP\u0026gt;:8081.
Please refer to the official documentation on publishing services in Kubernetes for more information.
Depending on your environment, starting a Flink cluster with LoadBalancer REST service exposed type might make the cluster accessible publicly (usually with the ability to execute arbitrary code). Logging # The Kubernetes integration exposes conf/log4j-console.properties and conf/logback-console.xml as a ConfigMap to the pods. Changes to these files will be visible to a newly started cluster.
Accessing the Logs # By default, the JobManager and TaskManager will output the logs to the console and /opt/flink/log in each pod simultaneously. The STDOUT and STDERR output will only be redirected to the console. You can access them via
\$ kubectl logs \u0026lt;pod-name\u0026gt; If the pod is running, you can also use kubectl exec -it \u0026lt;pod-name\u0026gt; bash to tunnel in and view the logs or debug the process.
Accessing the Logs of the TaskManagers # Flink will automatically de-allocate idling TaskManagers in order to not waste resources. This behaviour can make it harder to access the logs of the respective pods. You can increase the time before idling TaskManagers are released by configuring resourcemanager.taskmanager-timeout so that you have more time to inspect the log files.
Changing the Log Level Dynamically # If you have configured your logger to detect configuration changes automatically, then you can dynamically adapt the log level by changing the respective ConfigMap (assuming that the cluster id is my-first-flink-cluster):
\$ kubectl edit cm flink-config-my-first-flink-cluster Using Plugins # In order to use plugins, you must copy them to the correct location in the Flink JobManager/TaskManager pod. You can use the built-in plugins without mounting a volume or building a custom Docker image. For example, use the following command to enable the S3 plugin for your Flink session cluster.
\$ ./bin/kubernetes-session.sh -Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.16-SNAPSHOT.jar \\ -Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.16-SNAPSHOT.jar Custom Docker Image # If you want to use a custom Docker image, then you can specify it via the configuration option kubernetes.container.image. The Flink community provides a rich Flink Docker image which can be a good starting point. See how to customize Flink\u0026rsquo;s Docker image for how to enable plugins, add dependencies and other options.
Using Secrets # Kubernetes Secrets is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a pod specification or in an image. Flink on Kubernetes can use Secrets in two ways:
Using Secrets as files from a pod;
Using Secrets as environment variables;
Using Secrets as Files From a Pod # The following command will mount the secret mysecret under the path /path/to/secret in the started pods:
\$ ./bin/kubernetes-session.sh -Dkubernetes.secrets=mysecret:/path/to/secret The username and password of the secret mysecret can then be found stored in the files /path/to/secret/username and /path/to/secret/password. For more details see the official Kubernetes documentation.
Using Secrets as Environment Variables # The following command will expose the secret mysecret as environment variable in the started pods:
\$ ./bin/kubernetes-session.sh -Dkubernetes.env.secretKeyRef=\\ env:SECRET_USERNAME,secret:mysecret,key:username;\\ env:SECRET_PASSWORD,secret:mysecret,key:password The env variable SECRET_USERNAME contains the username and the env variable SECRET_PASSWORD contains the password of the secret mysecret. For more details see the official Kubernetes documentation.
High-Availability on Kubernetes # For high availability on Kubernetes, you can use the existing high availability services.
Configure the value of kubernetes.jobmanager.replicas to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers.
Manual Resource Cleanup # Flink uses Kubernetes OwnerReference\u0026rsquo;s to clean up all cluster components. All the Flink created resources, including ConfigMap, Service, and Pod, have the OwnerReference being set to deployment/\u0026lt;cluster-id\u0026gt;. When the deployment is deleted, all related resources will be deleted automatically.
\$ kubectl delete deployment/\u0026lt;cluster-id\u0026gt; Supported Kubernetes Versions # Currently, all Kubernetes versions \u0026gt;= 1.9 are supported.
Namespaces # Namespaces in Kubernetes divide cluster resources between multiple users via resource quotas. Flink on Kubernetes can use namespaces to launch Flink clusters. The namespace can be configured via kubernetes.namespace.
RBAC # Role-based access control (RBAC) is a method of regulating access to compute or network resources based on the roles of individual users within an enterprise. Users can configure RBAC roles and service accounts used by JobManager to access the Kubernetes API server within the Kubernetes cluster.
Every namespace has a default service account. However, the default service account may not have the permission to create or delete pods within the Kubernetes cluster. Users may need to update the permission of the default service account or specify another service account that has the right role bound.
\$ kubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=default:default If you do not want to use the default service account, use the following command to create a new flink-service-account service account and set the role binding. Then use the config option -Dkubernetes.service-account=flink-service-account to make the JobManager pod use the flink-service-account service account to create/delete TaskManager pods and leader ConfigMaps. Also this will allow the TaskManager to watch leader ConfigMaps to retrieve the address of JobManager and ResourceManager.
\$ kubectl create serviceaccount flink-service-account \$ kubectl create clusterrolebinding flink-role-binding-flink --clusterrole=edit --serviceaccount=default:flink-service-account Please refer to the official Kubernetes documentation on RBAC Authorization for more information.
Pod Template # Flink allows users to define the JobManager and TaskManager pods via template files. This allows to support advanced features that are not supported by Flink Kubernetes config options directly. Use kubernetes.pod-template-file to specify a local file that contains the pod definition. It will be used to initialize the JobManager and TaskManager. The main container should be defined with name flink-main-container. Please refer to the pod template example for more information.
Fields Overwritten by Flink # Some fields of the pod template will be overwritten by Flink. The mechanism for resolving effective field values can be categorized as follows:
Defined by Flink: User cannot configure it.
Defined by the user: User can freely specify this value. Flink framework won\u0026rsquo;t set any additional values and the effective value derives from the config option and the template.
Precedence order: First an explicit config option value is taken, then the value in pod template and at last the default value of a config option if nothing is specified.
Merged with Flink: Flink will merge values for a setting with a user defined value (see precedence order for \u0026ldquo;Defined by the user\u0026rdquo;). Flink values have precedence in case of same name fields.
Refer to the following tables for the full list of pod fields that will be overwritten. All the fields defined in the pod template that are not listed in the tables will be unaffected.
Pod Metadata
Key Category Related Config Options Description name Defined by Flink The JobManager pod name will be overwritten with the deployment which is defined by kubernetes.cluster-id. The TaskManager pod names will be overwritten with the pattern \u0026lt;clusterID\u0026gt;-\u0026lt;attempt\u0026gt;-\u0026lt;index\u0026gt; which is generated by Flink ResourceManager. namespace Defined by the user kubernetes.namespace Both the JobManager deployment and TaskManager pods will be created in the user specified namespace. ownerReferences Defined by Flink The owner reference of JobManager and TaskManager pods will always be set to the JobManager deployment. Please use kubernetes.jobmanager.owner.reference to control when the deployment is deleted. annotations Defined by the user kubernetes.jobmanager.annotations kubernetes.taskmanager.annotations Flink will add additional annotations specified by the Flink configuration options. labels Merged with Flink kubernetes.jobmanager.labels kubernetes.taskmanager.labels Flink will add some internal labels to the user defined values. Pod Spec
Key Category Related Config Options Description imagePullSecrets Defined by the user kubernetes.container.image.pull-secrets Flink will add additional pull secrets specified by the Flink configuration options. nodeSelector Defined by the user kubernetes.jobmanager.node-selector kubernetes.taskmanager.node-selector Flink will add additional node selectors specified by the Flink configuration options. tolerations Defined by the user kubernetes.jobmanager.tolerations kubernetes.taskmanager.tolerations Flink will add additional tolerations specified by the Flink configuration options. restartPolicy Defined by Flink "always" for JobManager pod and "never" for TaskManager pod. The JobManager pod will always be restarted by deployment. And the TaskManager pod should not be restarted. serviceAccount Defined by the user kubernetes.service-account The JobManager and TaskManager pods will be created with the user defined service account. volumes Merged with Flink Flink will add some internal ConfigMap volumes(e.g. flink-config-volume, hadoop-config-volume) which is necessary for shipping the Flink configuration and hadoop configuration. Main Container Spec
Key Category Related Config Options Description env Merged with Flink containerized.master.env.{ENV_NAME} containerized.taskmanager.env.{ENV_NAME} Flink will add some internal environment variables to the user defined values. image Defined by the user kubernetes.container.image The container image will be resolved with respect to the defined precedence order for user defined values. imagePullPolicy Defined by the user kubernetes.container.image.pull-policy The container image pull policy will be resolved with respect to the defined precedence order for user defined values. name Defined by Flink The container name will be overwritten by Flink with "flink-main-container". resources Defined by the user Memory: jobmanager.memory.process.size taskmanager.memory.process.size CPU: kubernetes.jobmanager.cpu kubernetes.taskmanager.cpu The memory and cpu resources(including requests and limits) will be overwritten by Flink configuration options. All other resources(e.g. ephemeral-storage) will be retained. containerPorts Merged with Flink Flink will add some internal container ports(e.g. rest, jobmanager-rpc, blob, taskmanager-rpc). volumeMounts Merged with Flink Flink will add some internal volume mounts(e.g. flink-config-volume, hadoop-config-volume) which is necessary for shipping the Flink configuration and hadoop configuration. Example of Pod Template # pod-template.yaml
apiVersion: v1 kind: Pod metadata: name: jobmanager-pod-template spec: initContainers: - name: artifacts-fetcher image: busybox:latest # Use wget or other tools to get user jars from remote storage command: [ \u0026#39;wget\u0026#39;, \u0026#39;https://path/of/StateMachineExample.jar\u0026#39;, \u0026#39;-O\u0026#39;, \u0026#39;/flink-artifact/myjob.jar\u0026#39; ] volumeMounts: - mountPath: /flink-artifact name: flink-artifact containers: # Do not change the main container name - name: flink-main-container resources: requests: ephemeral-storage: 2048Mi limits: ephemeral-storage: 2048Mi volumeMounts: - mountPath: /opt/flink/volumes/hostpath name: flink-volume-hostpath - mountPath: /opt/flink/artifacts name: flink-artifact - mountPath: /opt/flink/log name: flink-logs # Use sidecar container to push logs to remote storage or do some other debugging things - name: sidecar-log-collector image: sidecar-log-collector:latest command: [ \u0026#39;command-to-upload\u0026#39;, \u0026#39;/remote/path/of/flink-logs/\u0026#39; ] volumeMounts: - mountPath: /flink-logs name: flink-logs volumes: - name: flink-volume-hostpath hostPath: path: /tmp type: Directory - name: flink-artifact emptyDir: { } - name: flink-logs emptyDir: { } User jars \u0026amp; Classpath # When deploying Flink natively on Kubernetes, the following jars will be recognized as user-jars and included into user classpath:
Session Mode: The JAR file specified in startup command. Application Mode: The JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:78,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/",title:"Queries 查询",section:"SQL",content:""}),e.add({id:79,href:"/flink/flink-docs-master/zh/docs/libs/state_processor_api/",title:"State Processor API",section:"Libraries",content:` State Processor API # Apache Flink\u0026rsquo;s State Processor API provides powerful functionality to reading, writing, and modifying savepoints and checkpoints using Flink’s DataStream API under BATCH execution. Due to the interoperability of DataStream and Table API, you can even use relational Table API or SQL queries to analyze and process state data.
For example, you can take a savepoint of a running stream processing application and analyze it with a DataStream batch program to verify that the application behaves correctly. Or you can read a batch of data from any store, preprocess it, and write the result to a savepoint that you use to bootstrap the state of a streaming application. It is also possible to fix inconsistent state entries. Finally, the State Processor API opens up many ways to evolve a stateful application that was previously blocked by parameter and design choices that could not be changed without losing all the state of the application after it was started. For example, you can now arbitrarily modify the data types of states, adjust the maximum parallelism of operators, split or merge operator state, re-assign operator UIDs, and so on.
To get started with the state processor api, include the following library in your application.
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-state-processor-api\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Mapping Application State to DataSets # The State Processor API maps the state of a streaming application to one or more data sets that can be processed separately. In order to be able to use the API, you need to understand how this mapping works.
But let us first have a look at what a stateful Flink job looks like. A Flink job is composed of operators; typically one or more source operators, a few operators for the actual processing, and one or more sink operators. Each operator runs in parallel in one or more tasks and can work with different types of state. An operator can have zero, one, or more “operator states” which are organized as lists that are scoped to the operator\u0026rsquo;s tasks. If the operator is applied on a keyed stream, it can also have zero, one, or more “keyed states” which are scoped to a key that is extracted from each processed record. You can think of keyed state as a distributed key-value map.
The following figure shows the application “MyApp” which consists of three operators called “Src”, “Proc”, and “Snk”. Src has one operator state (os1), Proc has one operator state (os2) and two keyed states (ks1, ks2) and Snk is stateless.
A savepoint or checkpoint of MyApp consists of the data of all states, organized in a way that the states of each task can be restored. When processing the data of a savepoint (or checkpoint) with a batch job, we need a mental model that maps the data of the individual tasks\u0026rsquo; states into data sets or tables. In fact, we can think of a savepoint as a database. Every operator (identified by its UID) represents a namespace. Each operator state of an operator is mapped to a dedicated table in the namespace with a single column that holds the state\u0026rsquo;s data of all tasks. All keyed states of an operator are mapped to a single table consisting of a column for the key, and one column for each keyed state. The following figure shows how a savepoint of MyApp is mapped to a database.
The figure shows how the values of Src\u0026rsquo;s operator state are mapped to a table with one column and five rows, one row for each of the list entries across all parallel tasks of Src. Operator state os2 of the operator “Proc” is similarly mapped to an individual table. The keyed states ks1 and ks2 are combined to a single table with three columns, one for the key, one for ks1 and one for ks2. The keyed table holds one row for each distinct key of both keyed states. Since the operator “Snk” does not have any state, its namespace is empty.
Reading State # Reading state begins by specifying the path to a valid savepoint or checkpoint along with the StateBackend that should be used to restore the data. The compatibility guarantees for restoring state are identical to those when restoring a DataStream application.
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SavepointReader savepoint = SavepointReader.read(env, \u0026#34;hdfs://path/\u0026#34;, new HashMapStateBackend()); Operator State # Operator state is any non-keyed state in Flink. This includes, but is not limited to, any use of CheckpointedFunction or BroadcastState within an application. When reading operator state, users specify the operator uid, the state name, and the type information.
Operator List State # Operator state stored in a CheckpointedFunction using getListState can be read using ExistingSavepoint#readListState. The state name and type information should match those used to define the ListStateDescriptor that declared this state in the DataStream application.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readListState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;list-state\u0026#34;, Types.INT); Operator Union List State # Operator state stored in a CheckpointedFunction using getUnionListState can be read using ExistingSavepoint#readUnionState. The state name and type information should match those used to define the ListStateDescriptor that declared this state in the DataStream application. The framework will return a single copy of the state, equivalent to restoring a DataStream with parallelism 1.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readUnionState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;union-state\u0026#34;, Types.INT); Broadcast State # BroadcastState can be read using ExistingSavepoint#readBroadcastState. The state name and type information should match those used to define the MapStateDescriptor that declared this state in the DataStream application. The framework will return a single copy of the state, equivalent to restoring a DataStream with parallelism 1.
DataStream\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; broadcastState = savepoint.readBroadcastState\u0026lt;\u0026gt;( \u0026#34;my-uid\u0026#34;, \u0026#34;broadcast-state\u0026#34;, Types.INT, Types.INT); Using Custom Serializers # Each of the operator state readers support using custom TypeSerializers if one was used to define the StateDescriptor that wrote out the state.
DataStream\u0026lt;Integer\u0026gt; listState = savepoint.readListState\u0026lt;\u0026gt;( \u0026#34;uid\u0026#34;, \u0026#34;list-state\u0026#34;, Types.INT, new MyCustomIntSerializer()); Keyed State # Keyed state, or partitioned state, is any state that is partitioned relative to a key. When reading a keyed state, users specify the operator id and a KeyedStateReaderFunction\u0026lt;KeyType, OutputType\u0026gt;.
The KeyedStateReaderFunction allows users to read arbitrary columns and complex state types such as ListState, MapState, and AggregatingState. This means if an operator contains a stateful process function such as:
public class StatefulFunctionWithTime extends KeyedProcessFunction\u0026lt;Integer, Integer, Void\u0026gt; { ValueState\u0026lt;Integer\u0026gt; state; ListState\u0026lt;Long\u0026gt; updateTimes; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Integer\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT); state = getRuntimeContext().getState(stateDescriptor); ListStateDescriptor\u0026lt;Long\u0026gt; updateDescriptor = new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;times\u0026#34;, Types.LONG); updateTimes = getRuntimeContext().getListState(updateDescriptor); } @Override public void processElement(Integer value, Context ctx, Collector\u0026lt;Void\u0026gt; out) throws Exception { state.update(value + 1); updateTimes.add(System.currentTimeMillis()); } } Then it can read by defining an output type and corresponding KeyedStateReaderFunction.
DataStream\u0026lt;KeyedState\u0026gt; keyedState = savepoint.readKeyedState(\u0026#34;my-uid\u0026#34;, new ReaderFunction()); public class KeyedState { public int key; public int value; public List\u0026lt;Long\u0026gt; times; } public class ReaderFunction extends KeyedStateReaderFunction\u0026lt;Integer, KeyedState\u0026gt; { ValueState\u0026lt;Integer\u0026gt; state; ListState\u0026lt;Long\u0026gt; updateTimes; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Integer\u0026gt; stateDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT); state = getRuntimeContext().getState(stateDescriptor); ListStateDescriptor\u0026lt;Long\u0026gt; updateDescriptor = new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;times\u0026#34;, Types.LONG); updateTimes = getRuntimeContext().getListState(updateDescriptor); } @Override public void readKey( Integer key, Context ctx, Collector\u0026lt;KeyedState\u0026gt; out) throws Exception { KeyedState data = new KeyedState(); data.key = key; data.value = state.value(); data.times = StreamSupport .stream(updateTimes.get().spliterator(), false) .collect(Collectors.toList()); out.collect(data); } } Along with reading registered state values, each key has access to a Context with metadata such as registered event time and processing time timers.
Note: When using a KeyedStateReaderFunction, all state descriptors must be registered eagerly inside of open. Any attempt to call a RuntimeContext#get*State will result in a RuntimeException.
Window State # The state processor api supports reading state from a window operator. When reading a window state, users specify the operator id, window assigner, and aggregation type.
Additionally, a WindowReaderFunction can be specified to enrich each read with additional information similar to a WindowFunction or ProcessWindowFunction.
Suppose a DataStream application that counts the number of clicks per user per minute.
class Click { public String userId; public LocalDateTime time; } class ClickCounter implements AggregateFunction\u0026lt;Click, Integer, Integer\u0026gt; { @Override public Integer createAccumulator() { return 0; } @Override public Integer add(Click value, Integer accumulator) { return 1 + accumulator; } @Override public Integer getResult(Integer accumulator) { return accumulator; } @Override public Integer merge(Integer a, Integer b) { return a + b; } } DataStream\u0026lt;Click\u0026gt; clicks = ...; clicks .keyBy(click -\u0026gt; click.userId) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .aggregate(new ClickCounter()) .uid(\u0026#34;click-window\u0026#34;) .addSink(new Sink()); This state can be read using the code below.
class ClickState { public String userId; public int count; public TimeWindow window; public Set\u0026lt;Long\u0026gt; triggerTimers; } class ClickReader extends WindowReaderFunction\u0026lt;Integer, ClickState, String, TimeWindow\u0026gt; { @Override public void readWindow( String key, Context\u0026lt;TimeWindow\u0026gt; context, Iterable\u0026lt;Integer\u0026gt; elements, Collector\u0026lt;ClickState\u0026gt; out) { ClickState state = new ClickState(); state.userId = key; state.count = elements.iterator().next(); state.window = context.window(); state.triggerTimers = context.registeredEventTimeTimers(); out.collect(state); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SavepointReader savepoint = SavepointReader.read(env, \u0026#34;hdfs://checkpoint-dir\u0026#34;, new HashMapStateBackend()); savepoint .window(TumblingEventTimeWindows.of(Time.minutes(1))) .aggregate(\u0026#34;click-window\u0026#34;, new ClickCounter(), new ClickReader(), Types.String, Types.INT, Types.INT) .print(); Additionally, trigger state - from CountTriggers or custom triggers - can be read using the method Context#triggerState inside the WindowReaderFunction.
Writing New Savepoints # Savepoint\u0026rsquo;s may also be written, which allows such use cases as bootstrapping state based on historical data. Each savepoint is made up of one or more StateBootstrapTransformation\u0026rsquo;s (explained below), each of which defines the state for an individual operator.
When using the SavepointWriter, your application must be executed under BATCH execution. Note The state processor api does not currently provide a Scala API. As a result it will always auto-derive serializers using the Java type stack. To bootstrap a savepoint for the Scala DataStream API please manually pass in all type information. int maxParallelism = 128; SavepointWriter .newSavepoint(new HashMapStateBackend(), maxParallelism) .withOperator(\u0026#34;uid1\u0026#34;, transformation1) .withOperator(\u0026#34;uid2\u0026#34;, transformation2) .write(savepointPath); The UIDs associated with each operator must match one to one with the UIDs assigned to the operators in your DataStream application; these are how Flink knows what state maps to which operator.
Operator State # Simple operator state, using CheckpointedFunction, can be created using the StateBootstrapFunction.
public class SimpleBootstrapFunction extends StateBootstrapFunction\u0026lt;Integer\u0026gt; { private ListState\u0026lt;Integer\u0026gt; state; @Override public void processElement(Integer value, Context ctx) throws Exception { state.add(value); } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { } @Override public void initializeState(FunctionInitializationContext context) throws Exception { state = context.getOperatorState().getListState(new ListStateDescriptor\u0026lt;\u0026gt;(\u0026#34;state\u0026#34;, Types.INT)); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Integer\u0026gt; data = env.fromElements(1, 2, 3); StateBootstrapTransformation transformation = OperatorTransformation .bootstrapWith(data) .transform(new SimpleBootstrapFunction()); Broadcast State # BroadcastState can be written using a BroadcastStateBootstrapFunction. Similar to broadcast state in the DataStream API, the full state must fit in memory.
public class CurrencyRate { public String currency; public Double rate; } public class CurrencyBootstrapFunction extends BroadcastStateBootstrapFunction\u0026lt;CurrencyRate\u0026gt; { public static final MapStateDescriptor\u0026lt;String, Double\u0026gt; descriptor = new MapStateDescriptor\u0026lt;\u0026gt;(\u0026#34;currency-rates\u0026#34;, Types.STRING, Types.DOUBLE); @Override public void processElement(CurrencyRate value, Context ctx) throws Exception { ctx.getBroadcastState(descriptor).put(value.currency, value.rate); } } DataStream\u0026lt;CurrencyRate\u0026gt; currencyDataSet = env.fromCollection( new CurrencyRate(\u0026#34;USD\u0026#34;, 1.0), new CurrencyRate(\u0026#34;EUR\u0026#34;, 1.3)); StateBootstrapTransformation\u0026lt;CurrencyRate\u0026gt; broadcastTransformation = OperatorTransformation .bootstrapWith(currencyDataSet) .transform(new CurrencyBootstrapFunction()); Keyed State # Keyed state for ProcessFunction\u0026rsquo;s and other RichFunction types can be written using a KeyedStateBootstrapFunction.
public class Account { public int id; public double amount;	public long timestamp; } public class AccountBootstrapper extends KeyedStateBootstrapFunction\u0026lt;Integer, Account\u0026gt; { ValueState\u0026lt;Double\u0026gt; state; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Double\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;total\u0026#34;,Types.DOUBLE); state = getRuntimeContext().getState(descriptor); } @Override public void processElement(Account value, Context ctx) throws Exception { state.update(value.amount); } } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Account\u0026gt; accountDataSet = env.fromCollection(accounts); StateBootstrapTransformation\u0026lt;Account\u0026gt; transformation = OperatorTransformation .bootstrapWith(accountDataSet) .keyBy(acc -\u0026gt; acc.id) .transform(new AccountBootstrapper()); The KeyedStateBootstrapFunction supports setting event time and processing time timers. The timers will not fire inside the bootstrap function and only become active once restored within a DataStream application. If a processing time timer is set but the state is not restored until after that time has passed, the timer will fire immediately upon start.
Attention If your bootstrap function creates timers, the state can only be restored using one of the process type functions.
Window State # The state processor api supports writing state for the window operator. When writing window state, users specify the operator id, window assigner, evictor, optional trigger, and aggregation type. It is important the configurations on the bootstrap transformation match the configurations on the DataStream window.
public class Account { public int id; public double amount;	public long timestamp; } StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Account\u0026gt; accountDataSet = env.fromCollection(accounts); StateBootstrapTransformation\u0026lt;Account\u0026gt; transformation = OperatorTransformation .bootstrapWith(accountDataSet) .keyBy(acc -\u0026gt; acc.id) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .reduce((left, right) -\u0026gt; left + right); Modifying Savepoints # Besides creating a savepoint from scratch, you can base one off an existing savepoint such as when bootstrapping a single new operator for an existing job.
SavepointWriter .fromExistingSavepoint(oldPath, new HashMapStateBackend()) .withOperator(\u0026#34;uid\u0026#34;, transformation) .write(newPath); `}),e.add({id:80,href:"/flink/flink-docs-master/zh/docs/dev/table/",title:"Table API \u0026 SQL",section:"应用开发",content:" "}),e.add({id:81,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/with/",title:"WITH 子句",section:"Queries 查询",content:` WITH 子句 # Batch Streaming
WITH 子句提供了一种用于更大查询而编写辅助语句的方法。这些编写的语句通常被称为公用表表达式，表达式可以理解为仅针对某个查询而存在的临时视图。
WITH 子句的语法
WITH \u0026lt;with_item_definition\u0026gt; [ , ... ] SELECT ... FROM ...; \u0026lt;with_item_defintion\u0026gt;: with_item_name (column_name[, ...n]) AS ( \u0026lt;select_query\u0026gt; ) 下面的示例中定义了一个公用表表达式 orders_with_total ，并在一个 GROUP BY 查询中使用它。
WITH orders_with_total AS ( SELECT order_id, price + tax AS total FROM Orders ) SELECT order_id, SUM(total) FROM orders_with_total GROUP BY order_id; Back to top
`}),e.add({id:82,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/working_directory/",title:"Working Directory",section:"Standalone",content:` Working Directory # Flink supports to configure a working directory (FLIP-198) for Flink processes (JobManager and TaskManager). The working directory is used by the processes to store information that can be recovered upon a process restart. The requirement for this to work is that the process is started with the same identity and has access to the volume on which the working directory is stored.
Configuring the Working Directory # The working directories for the Flink processes are:
JobManager working directory: \u0026lt;WORKING_DIR_BASE\u0026gt;/jm_\u0026lt;JM_RESOURCE_ID\u0026gt; TaskManager working directory: \u0026lt;WORKING_DIR_BASE\u0026gt;/tm_\u0026lt;TM_RESOURCE_ID\u0026gt; with \u0026lt;WORKING_DIR_BASE\u0026gt; being the working directory base, \u0026lt;JM_RESOURCE_ID\u0026gt; being the resource id of the JobManager process and \u0026lt;TM_RESOURCE_ID\u0026gt; being the resource id of the TaskManager process.
The \u0026lt;WORKING_DIR_BASE\u0026gt; can be configured by process.working-dir. It needs to point to a local directory. If not explicitly configured, then it defaults to a randomly picked directory from io.tmp.dirs.
It is also possible to configure a JobManager and TaskManager specific \u0026lt;WORKING_DIR_BASE\u0026gt; via process.jobmanager.working-dir and process.taskmanager.working-dir respectively.
The JobManager resource id can be configured via jobmanager.resource-id. If not explicitly configured, then it will be a random UUID.
Similarly, the TaskManager resource id can be configured via taskmanager.resource-id. If not explicitly configured, then it will be a random value containing the host and port of the running process.
Artifacts Stored in the Working Directory # Flink processes will use the working directory to store the following artifacts:
Blobs stored by the BlobServer and BlobCache Local state if state.backend.local-recovery is enabled RocksDB\u0026rsquo;s working directory Local Recovery Across Process Restarts # The working directory can be used to enable local recovery across process restarts (FLIP-201). This means that Flink does not have to recover state information from remote storage.
In order to use this feature, local recovery has to be enabled via state.backend.local-recovery. Moreover, the TaskManager processes need to get a deterministic resource id assigned via taskmanager.resource-id. Last but not least, a failed TaskManager process needs to be restarted with the same working directory.
process.working-dir: /path/to/working/dir/base state.backend.local-recovery: true taskmanager.resource-id: TaskManager_1 # important: Change for every TaskManager process Back to top
`}),e.add({id:83,href:"/flink/flink-docs-master/zh/docs/dev/dataset/zip_elements_guide/",title:"Zipping Elements",section:"DataSet API (Legacy)",content:` 给 DataSet 中的元素编号 # 在一些算法中，可能需要为数据集元素分配唯一标识符。 本文档阐述了如何将 DataSetUtils 用于此目的。
以密集索引编号 # zipWithIndex 为元素分配连续的标签，接收数据集作为输入并返回一个新的 (unique id, initial value) 二元组的数据集。 这个过程需要分为两个（子）过程，首先是计数，然后是标记元素，由于计数操作的同步性，这个过程不能被 pipelined（流水线化）。
可供备选的 zipWithUniqueId 是以 pipelined 的方式进行工作的。当唯一标签足够时，首选 zipWithUniqueId 。 例如，下面的代码：
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataSet\u0026lt;String\u0026gt; in = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; result = DataSetUtils.zipWithIndex(in); result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;); env.execute(); Scala import org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithIndex result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() Python from flink.plan.Environment import get_environment env = get_environment() env.set_parallelism(2) input = env.from_elements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) result = input.zip_with_index() result.write_text(result_path) env.execute() 可能会生成这些元组：(0,G)，(1,H)，(2,A)，(3,B)，(4,C)，(5,D)，(6,E)，(7,F)
Back to top
以唯一标识符编号 # 在许多情况下，可能不需要指定连续的标签。 zipWithUniqueId 以 pipelined 的方式工作，加快了标签分配的过程。该方法接收一个数据集作为输入，并返回一个新的 (unique id, initial value) 二元组的数据集。 例如，下面的代码：
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataSet\u0026lt;String\u0026gt; in = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Long, String\u0026gt;\u0026gt; result = DataSetUtils.zipWithUniqueId(in); result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;); env.execute(); Scala import org.apache.flink.api.scala._ val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment env.setParallelism(2) val input: DataSet[String] = env.fromElements(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;G\u0026#34;, \u0026#34;H\u0026#34;) val result: DataSet[(Long, String)] = input.zipWithUniqueId result.writeAsCsv(resultPath, \u0026#34;\\n\u0026#34;, \u0026#34;,\u0026#34;) env.execute() 可能会产生这些元组：(0,G)，(1,A)，(2,H)，(3,B)，(5,C)，(7,D)，(9,E)，(11,F)
Back to top
`}),e.add({id:84,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/oss/",title:"阿里云 OSS",section:"File Systems",content:` 阿里云对象存储服务 (OSS) # OSS：对象存储服务 # 阿里云对象存储服务 (Aliyun OSS) 使用广泛，尤其在中国云用户中十分流行，能提供多种应用场景下的云对象存储。OSS 可与 Flink 一起使用以读取与存储数据，以及与流 State Backend 结合使用。
通过以下格式指定路径，OSS 对象可类似于普通文件使用：
oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt; 以下代码展示了如何在 Flink 作业中使用 OSS：
// 读取 OSS bucket env.readTextFile(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;); // 写入 OSS bucket stream.writeAsText(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;); // 将 OSS 用作 FsStatebackend env.setStateBackend(new FsStateBackend(\u0026#34;oss://\u0026lt;your-bucket\u0026gt;/\u0026lt;object-name\u0026gt;\u0026#34;)); Shaded Hadoop OSS 文件系统 # 为使用 flink-oss-fs-hadoop，在启动 Flink 之前，将对应的 JAR 文件从 opt 目录复制到 Flink 发行版中的 plugin 目录下的一个文件夹中，例如：
mkdir ./plugins/oss-fs-hadoop cp ./opt/flink-oss-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/oss-fs-hadoop/ flink-oss-fs-hadoop 为使用 oss:// scheme 的 URI 注册了默认的文件系统包装器。
配置设置 # 在设置好 OSS 文件系统包装器之后，需要添加一些配置以保证 Flink 有权限访问 OSS buckets。
为了简单使用，可直接在 flink-conf.yaml 中使用与 Hadoop core-site.xml 相同的配置关键字。
可在 Hadoop OSS 文档 中查看配置关键字。
一些配置必须添加至 flink-conf.yaml （在 Hadoop OSS 文档中定义的其它配置为用作性能调优的高级配置）：
fs.oss.endpoint: 连接的 Aliyun OSS endpoint fs.oss.accessKeyId: Aliyun access key ID fs.oss.accessKeySecret: Aliyun access key secret 备选的 CredentialsProvider 也可在 flink-conf.yaml 中配置，例如：
# 从 OSS_ACCESS_KEY_ID 和 OSS_ACCESS_KEY_SECRET 读取凭据 (Credentials) fs.oss.credentials.provider: com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider 其余的凭据提供者（credential providers）可在这里中找到。
Back to top
`}),e.add({id:85,href:"/flink/flink-docs-master/zh/docs/ops/debugging/debugging_classloading/",title:"调试类加载",section:"Debugging",content:` 调试类加载 # Flink中的类加载概述 # Flink应用程序运行时，JVM会随着时间不断加载各种不同的类。 根据起源不同这些类可以分为三组类型：
Java Classpath: Java共有的classpath类加载路径，包括JDK库和Flink的/lib目录（Apache Flink及相关依赖的类）中的代码。它们通过AppClassLoader进行加载。
Flink插件类组件：存放于Flink的 /plugins 目录中的插件代码。Flink的插件机制确保在启动时对它们进行动态加载。
动态用户代码：动态提交job（通过REST、命令行或者web UI方式）时存在JAR文件中的类。job运行时它们通过FlinkUserCodeClassLoader进行动态加载或卸载。
作为通用规则，每当Flink进程先启动，之后job提交时，job相关的类都是动态加载的。 如果Flink进程与job或应用程序一起启动，或者应用代码启动Flink组件（JobManager, TaskManager等），这时所有job的类存在于Java的classpath下。
每个插件中的组件代码会由一个专用的类加载器进行动态加载。
下面是不同部署模式的一些细节信息：
Session模式(Standalone/Yarn/Kubernetes)
当Flink Session集群启动时，JobManager和TaskManager由Java classpath中的Flink框架类（Flink framework classes）进行启动加载。而通过session提交（REST或命令行方式）的job或应用程序由FlinkUserCodeClassLoader进行加载。
Per-Job模式（已弃用）（Yarn）
当前只有Yarn支持Per-Job模式。默认情况下，Flink集群运行在Per-Job模式下时会将用户的jar文件包含在系统的classpath中。 这种模式可以由yarn.classpath.include-user-jar 参数控制。 当该参数设定为DISABLED时，Flink会将用户jar文件含在用户的classpath中，并由FlinkUserCodeClassLoader进行动态加载。
详细信息参见Flink on Yarn。
Application模式（Standalone/Yarn/Kubernetes）
当Application模式的Flink集群基于Standalone或Kubernetes方式运行时，用户jar文件（启动命令指定的jar文件和Flink的usrlib目录中的jar包）会由FlinkUserCodeClassLoader进行动态加载。
当Flink集群以Application模式运行时，用户jar文件（启动命令指定的jar文件和Flink的usrlib目录中的jar包）默认情况下会包含在系统classpath（AppClassLoader）。与Per-Job模式相同，当yarn.classpath.include-user-jar设置为DISABLED时，Flink会将用户jar文件含在用户的classpath中，并由FlinkUserCodeClassLoader进行动态加载。
倒置类加载（Inverted Class Loading）和ClassLoader解析顺序 # 涉及到动态类加载的层次结构涉及两种ClassLoader： （1）Java的application classloader，包含classpath中的所有类； （2）动态的plugin/user code classloader，用来加载插件代码或用户代码的jar文件。动态的ClassLoader将应用程序classloader作为parent。
默认情况下Flink会倒置类加载顺序，首先Flink会查找动态类加载器，如果该类不属于动态加载的代码时才会去查找其parent（application classloader）。
倒置类加载的好处在于插件和job可以使用与Flink核心不同的库版本，尤其在使用不同版本的库从而出现不兼容的情况下。这种机制可以帮助避免常见的类似 IllegalAccessError 或NoSuchMethodError的依赖冲突错误。代码的不同部分会有独立的拷贝（Flink内核及它的不同依赖包可使用与用户代码或插件代码不同的拷贝），多数情况下这种方式可以正常运行，并且不需要用户进行额外配置。
然而有些情况下，倒置类加载可能会引起一些问题，参见下面的\u0026ldquo;X cannot be cast to X\u0026rdquo;。
对于用户代码的类加载，您可以通过调整Flink的classloader.resolve-order配置将ClassLoader解析顺序还原至Java的默认模式（从Flink默认的child-first调整为parent-first）。
请注意由于有些类在Flink内核与插件或用户代码间共享，它们总是以parent-first方式进行解析的。这些类相关的包通过classloader.parent-first-patterns-default和classloader.parent-first-patterns-additional进行配置。如果需要新添加parent-first 方式的包，请调整classloader.parent-first-patterns-additional 配置选项。
避免用户代码的动态类加载 # Flink的组件（JobManager, TaskManager, Client, ApplicationMaster等）在启动时会在日志开头的环境信息部分记录classpath的设定。
当JobManager和TaskManager的运行模式为指定一个job时，可以通过将用户代码的JAR文件放置在/lib目录下，从而包含在classpath路径中，以保证它们不会被动态加载。
通常情况下将job的JAR文件放置在/lib目录下可以正常运行。JAR文件会同时作为classpath（AppClassLoader）和动态类加载器（FlinkUserCodeClassLoader）的一部分。 由于AppClassLoader是FlinkUserCodeClassLoader的父类（Java默认情况下以parent-first方式加载），这样类只会加载一次。
当job相关的JAR文件不能全部放在/lib目录下（如多个job共用的一个session）时，可以通过将相对公共的类库放在/lib目录下，从而避免这些类的动态加载。
手动进行用户代码的类加载 # 某些情况下，transformation、source或者sink需要进行手动类加载（通过反射动态实现），这需要通过能访问到job相关类的类加载器进行实现。
在这种情况下，可以把函数（或sources和sinks）实现为RichFunction（如RichMapFunction 或者 RichWindowFunction），然后通过getRuntimeContext().getUserCodeClassLoader()访问用户代码的类加载器。
X cannot be cast to X 异常 # 当进行动态类加载时，您可能会遇到类似com.foo.X cannot be cast to com.foo.X类型的异常。 出现这种异常代表不同的类加载器加载了不同版本的com.foo.X类，并且它们互相之间尝试进行类型指定转换。
发生这种情况的通常原因是这个库与Flink的倒置类加载（inverted classloading）方式不兼容造成的。您可以通过关闭倒置类加载（inverted classloading）来进行验证（在Flink设置中调整classloader.resolve-order: parent-first），或者将库排除在inverted classloading之外（通过设置classloader.parent-first-patterns-additional）。
另一种原因可能是由缓存的对象实例引起的，比如类似Apache Avro或者Guava的Interners类型的对象。 解决办法是设置没有任何动态类加载，或者确保相应的库完全是动态加载代码的一部分。后者意味着库不能添加到Flink的/lib目录下，但必须作为应用程序的fat-jar或uber-jar的一部分。
卸载用户代码中动态加载的类 # 所有涉及动态用户代码类加载（会话）的场景都依赖于再次卸载的类。
类卸载指垃圾回收器发现一个类的对象不再被引用，这时会对该类（相关代码、静态变量、元数据等）进行移除。
当TaskManager启动或重启任务时会加载指定任务的代码，除非这些类可以卸载，否则就有可能引起内存泄露，因为更新新版本的类可能会随着时间不断的被加载积累。这种现象经常会引起OutOfMemoryError: Metaspace这种典型异常。
类泄漏的常见原因和建议的修复方式：
Lingering Threads: 确保应用代码的函数/sources/sink关闭了所有线程。延迟关闭的线程不仅自身消耗资源，同时会因为占据对象引用，从而阻止垃圾回收和类的卸载。
Interners: 避免缓存超出function/sources/sinks生命周期的特殊结构中的对象。比如Guava的Interner，或是Avro的序列化器中的类或对象。
JDBC: JDBC驱动会在用户类加载器之外泄漏引用。为了确保这些类只被加载一次，您可以将驱动JAR包放在Flink的lib/目录下，或者将驱动类通过classloader.parent-first-patterns-additional加到父级优先加载类的列表中。
释放用户代码类加载器的钩子（hook）可以帮助卸载动态加载的类。这种钩子在类加载器卸载前执行。通常情况下最好把关闭和卸载资源作为正常函数生命周期操作的一部分（比如典型的close()方法）。有些情况下（比如静态字段）最好确定类加载器不再需要后就立即卸载。
释放类加载器的钩子可以通过RuntimeContext.registerUserCodeClassLoaderReleaseHookIfAbsent()方法进行注册。
通过maven-shade-plugin解决与Flink的依赖冲突 # 从应用开发者的角度可以通过shading them away的方式公开依赖关系来解决依赖冲突。
Apache Maven提供了maven-shade-plugin，通过插件可以允许在编译后调整类相关的包。举例来说，假如您的用户代码jar文件中包含aws的sdk中的com.amazonaws包，shade plugin会将它们重定位到org.myorg.shaded.com.amazonaws，这样代码就会正确调用您的aws sdk的版本。
这个文档页面解释了relocating classes using the shade plugin。
对于大部分的Flink依赖如guava, netty, jackson等，这些已经由Flink的维护者进行处理，普通用户通常情况下无需再对其进行关注。
Back to top
`}),e.add({id:86,href:"/flink/flink-docs-master/zh/docs/dev/dataset/iterations/",title:"迭代",section:"DataSet API (Legacy)",content:` 迭代 # Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. Such algorithms are crucial in order to realize the promise of Big Data to extract meaningful information out of your data. With increasing interest to run these kinds of algorithms on very large data sets, there is a need to execute iterations in a massively parallel fashion.
Flink programs implement iterative algorithms by defining a step function and embedding it into a special iteration operator. There are two variants of this operator: Iterate and Delta Iterate. Both operators repeatedly invoke the step function on the current iteration state until a certain termination condition is reached.
Here, we provide background on both operator variants and outline their usage. The programming guide explains how to implement the operators in both Scala and Java. We also support both vertex-centric and gather-sum-apply iterations through Flink\u0026rsquo;s graph processing API, Gelly.
The following table provides an overview of both operators:
Iterate Delta Iterate Iteration Input Partial Solution Workset and Solution Set Step Function Arbitrary Data Flows State Update Next partial solution Next workset Changes to solution set Iteration Result Last partial solution Solution set state after last iteration Termination Maximum number of iterations (default) Custom aggregator convergence Maximum number of iterations or empty workset (default) Custom aggregator convergence Iterate Operator # The iterate operator covers the simple form of iterations: in each iteration, the step function consumes the entire input (the result of the previous iteration, or the initial data set), and computes the next version of the partial solution (e.g. map, reduce, join, etc.).
Iteration Input: Initial input for the first iteration from a data source or previous operators. Step Function: The step function will be executed in each iteration. It is an arbitrary data flow consisting of operators like map, reduce, join, etc. and depends on your specific task at hand. Next Partial Solution: In each iteration, the output of the step function will be fed back into the next iteration. Iteration Result: Output of the last iteration is written to a data sink or used as input to the following operators. There are multiple options to specify termination conditions for an iteration:
Maximum number of iterations: Without any further conditions, the iteration will be executed this many times. Custom aggregator convergence: Iterations allow to specify custom aggregators and convergence criteria like sum aggregate the number of emitted records (aggregator) and terminate if this number is zero (convergence criterion). You can also think about the iterate operator in pseudo-code:
IterationState state = getInitialState(); while (!terminationCriterion()) { state = step(state); } setFinalState(state); See the Programming Guide for details and code examples. Example: Incrementing Numbers # In the following example, we iteratively increment a set numbers:
Iteration Input: The initial input is read from a data source and consists of five single-field records (integers 1 to 5). Step function: The step function is a single map operator, which increments the integer field from i to i+1. It will be applied to every record of the input. Next Partial Solution: The output of the step function will be the output of the map operator, i.e. records with incremented integers. Iteration Result: After ten iterations, the initial numbers will have been incremented ten times, resulting in integers 11 to 15. // 1st 2nd 10th map(1) -\u0026gt; 2 map(2) -\u0026gt; 3 ... map(10) -\u0026gt; 11 map(2) -\u0026gt; 3 map(3) -\u0026gt; 4 ... map(11) -\u0026gt; 12 map(3) -\u0026gt; 4 map(4) -\u0026gt; 5 ... map(12) -\u0026gt; 13 map(4) -\u0026gt; 5 map(5) -\u0026gt; 6 ... map(13) -\u0026gt; 14 map(5) -\u0026gt; 6 map(6) -\u0026gt; 7 ... map(14) -\u0026gt; 15 Note that 1, 2, and 4 can be arbitrary data flows.
Delta Iterate Operator # The delta iterate operator covers the case of incremental iterations. Incremental iterations selectively modify elements of their solution and evolve the solution rather than fully recompute it.
Where applicable, this leads to more efficient algorithms, because not every element in the solution set changes in each iteration. This allows to focus on the hot parts of the solution and leave the cold parts untouched. Frequently, the majority of the solution cools down comparatively fast and the later iterations operate only on a small subset of the data.
Iteration Input: The initial workset and solution set are read from data sources or previous operators as input to the first iteration. Step Function: The step function will be executed in each iteration. It is an arbitrary data flow consisting of operators like map, reduce, join, etc. and depends on your specific task at hand. Next Workset/Update Solution Set: The next workset drives the iterative computation and will be fed back into the next iteration. Furthermore, the solution set will be updated and implicitly forwarded (it is not required to be rebuild). Both data sets can be updated by different operators of the step function. Iteration Result: After the last iteration, the solution set is written to a data sink or used as input to the following operators. The default termination condition for delta iterations is specified by the empty workset convergence criterion and a maximum number of iterations. The iteration will terminate when a produced next workset is empty or when the maximum number of iterations is reached. It is also possible to specify a custom aggregator and convergence criterion.
You can also think about the iterate operator in pseudo-code:
IterationState workset = getInitialState(); IterationState solution = getInitialSolution(); while (!terminationCriterion()) { (delta, workset) = step(workset, solution); solution.update(delta); } setFinalState(solution); See the Programming Guide for details and code examples. Example: Propagate Minimum in Graph # In the following example, every vertex has an ID and a coloring. Each vertex will propagate its vertex ID to neighboring vertices. The goal is to assign the minimum ID to every vertex in a subgraph. If a received ID is smaller then the current one, it changes to the color of the vertex with the received ID. One application of this can be found in community analysis or connected components computation.
The initial input is set as both workset and solution set. In the above figure, the colors visualize the evolution of the solution set. With each iteration, the color of the minimum ID is spreading in the respective subgraph. At the same time, the amount of work (exchanged and compared vertex IDs) decreases with each iteration. This corresponds to the decreasing size of the workset, which goes from all seven vertices to zero after three iterations, at which time the iteration terminates. The important observation is that the lower subgraph converges before the upper half does and the delta iteration is able to capture this with the workset abstraction.
In the upper subgraph ID 1 (orange) is the minimum ID. In the first iteration, it will get propagated to vertex 2, which will subsequently change its color to orange. Vertices 3 and 4 will receive ID 2 (in yellow) as their current minimum ID and change to yellow. Because the color of vertex 1 didn\u0026rsquo;t change in the first iteration, it can be skipped it in the next workset.
In the lower subgraph ID 5 (cyan) is the minimum ID. All vertices of the lower subgraph will receive it in the first iteration. Again, we can skip the unchanged vertices (vertex 5) for the next workset.
In the 2nd iteration, the workset size has already decreased from seven to five elements (vertices 2, 3, 4, 6, and 7). These are part of the iteration and further propagate their current minimum IDs. After this iteration, the lower subgraph has already converged (cold part of the graph), as it has no elements in the workset, whereas the upper half needs a further iteration (hot part of the graph) for the two remaining workset elements (vertices 3 and 4).
The iteration terminates, when the workset is empty after the 3rd iteration.
Superstep Synchronization # We referred to each execution of the step function of an iteration operator as a single iteration. In parallel setups, multiple instances of the step function are evaluated in parallel on different partitions of the iteration state. In many settings, one evaluation of the step function on all parallel instances forms a so called superstep, which is also the granularity of synchronization. Therefore, all parallel tasks of an iteration need to complete the superstep, before a next superstep will be initialized. Termination criteria will also be evaluated at superstep barriers.
`}),e.add({id:87,href:"/flink/flink-docs-master/zh/docs/concepts/",title:"概念透析",section:"Docs",content:""}),e.add({id:88,href:"/flink/flink-docs-master/zh/docs/ops/debugging/flame_graphs/",title:"火焰图",section:"Debugging",content:` Flame Graphs # Flame Graphs are a visualization that effectively surfaces answers to questions like:
Which methods are currently consuming CPU resources? How does consumption by one method compare to the others? Which series of calls on the stack led to executing a particular method? Flame Graph Flame Graphs are constructed by sampling stack traces a number of times. Each method call is presented by a bar, where the length of the bar is proportional to the number of times it is present in the samples.
Starting with Flink 1.13, Flame Graphs are natively supported in Flink. In order to produce a Flame Graph, navigate to the job graph of a running job, select an operator of interest and in the menu to the right click on the Flame Graph tab:
Operator\u0026rsquo;s On-CPU Flame Graph Any measurement process in and of itself inevitably affects the subject of measurement (see the double-split experiment). Sampling CPU stack traces is no exception. In order to prevent unintended impacts on production environments, Flame Graphs are currently available as an opt-in feature. To enable it, you\u0026rsquo;ll need to set rest.flamegraph.enabled: true in conf/flink-conf.yaml. We recommend enabling it in development and pre-production environments, but you should treat it as an experimental feature in production. Apart from the On-CPU Flame Graphs, Off-CPU and Mixed visualizations are available and can be switched between by using the selector at the top of the pane:
The Off-CPU Flame Graph visualizes blocking calls found in the samples. A distinction is made as follows:
On-CPU: Thread.State in [RUNNABLE, NEW] Off-CPU: Thread.State in [TIMED_WAITING, WAITING, BLOCKED] Off-CPU Flame Graph Mixed mode Flame Graphs are constructed from stack traces of threads in all possible states.
Flame Graph in Mixed Mode Sampling process # The collection of stack traces is done purely within the JVM, so only method calls within the Java runtime are visible (no system calls).
Flame Graph construction is performed at the level of an individual operator, i.e. all task threads of that operator are sampled in parallel and their stack traces are combined.
Note: Stack trace samples from all threads of an operator are combined together. If a method call consumes 100% of the resources in one of the parallel tasks but none in the others, the bottleneck might be obscured by being averaged out.
There are plans to address this limitation in the future by providing \u0026ldquo;drill down\u0026rdquo; visualizations to the task level.
`}),e.add({id:89,href:"/flink/flink-docs-master/zh/docs/try-flink/datastream/",title:"基于 DataStream API 实现欺诈检测",section:"Try Flink",content:` 基于 DataStream API 实现欺诈检测 # Apache Flink 提供了 DataStream API 来实现稳定可靠的、有状态的流处理应用程序。 Flink 支持对状态和时间的细粒度控制，以此来实现复杂的事件驱动数据处理系统。 这个入门指导手册讲述了如何通过 Flink DataStream API 来实现一个有状态流处理程序。
你要搭建一个什么系统 # 在当今数字时代，信用卡欺诈行为越来越被重视。 罪犯可以通过诈骗或者入侵安全级别较低系统来盗窃信用卡卡号。 用盗得的信用卡进行很小额度的例如一美元或者更小额度的消费进行测试。 如果测试消费成功，那么他们就会用这个信用卡进行大笔消费，来购买一些他们希望得到的，或者可以倒卖的财物。
在这个教程中，你将会建立一个针对可疑信用卡交易行为的反欺诈检测系统。 通过使用一组简单的规则，你将了解到 Flink 如何为我们实现复杂业务逻辑并实时执行。
准备条件 # 这个代码练习假定你对 Java 或 Scala 有一定的了解，当然，如果你之前使用的是其他开发语言，你也应该能够跟随本教程进行学习。
在 IDE 中运行 # 在 IDE 中运行该项目可能会遇到 java.langNoClassDefFoundError 的异常。这很可能是因为运行所需要的 Flink 的依赖库没有默认被全部加载到类路径（classpath）里。
IntelliJ IDE：前往 运行 \u0026gt; 编辑配置 \u0026gt; 修改选项 \u0026gt; 选中 将带有 \u0026ldquo;provided\u0026rdquo; 范围的依赖项添加到类路径。这样的话，运行配置将会包含所有在 IDE 中运行所必须的类。
困难求助 # 如果遇到困难，可以参考 社区支持资源。 当然也可以在邮件列表提问，Flink 的 用户邮件列表 一直被评为所有Apache项目中最活跃的一个，这也是快速获得帮助的好方法。
怎样跟着教程练习 # 首先，你需要在你的电脑上准备以下环境：
Java 11 Maven 一个准备好的 Flink Maven Archetype 能够快速创建一个包含了必要依赖的 Flink 程序骨架，基于此，你可以把精力集中在编写业务逻辑上即可。 这些已包含的依赖包括 flink-streaming-java、flink-walkthrough-common 等，他们分别是 Flink 应用程序的核心依赖项和这个代码练习需要的数据生成器，当然还包括其他本代码练习所依赖的类。
说明： 为简洁起见，本练习中的代码块中可能不包含完整的类路径。完整的类路径可以在文档底部 链接 中找到。 Java \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-walkthrough-datastream-java \\ -DarchetypeVersion=1.16-SNAPSHOT \\ -DgroupId=frauddetection \\ -DartifactId=frauddetection \\ -Dversion=0.1 \\ -Dpackage=spendreport \\ -DinteractiveMode=false Scala \$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-walkthrough-datastream-scala \\ -DarchetypeVersion=1.16-SNAPSHOT \\ -DgroupId=frauddetection \\ -DartifactId=frauddetection \\ -Dversion=0.1 \\ -Dpackage=spendreport \\ -DinteractiveMode=false Maven 3.0 及更高版本，不再支持通过命令行指定仓库（-DarchetypeCatalog）。有关这个改动的详细信息， 请参阅 Maven 官方文档 如果你希望使用快照仓库，则需要在 settings.xml 文件中添加一个仓库条目。例如：
\u0026lt;settings\u0026gt; \u0026lt;activeProfiles\u0026gt; \u0026lt;activeProfile\u0026gt;apache\u0026lt;/activeProfile\u0026gt; \u0026lt;/activeProfiles\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;apache\u0026lt;/id\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;apache-snapshots\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://repository.apache.org/content/repositories/snapshots/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;/settings\u0026gt; 你可以根据自己的情况修改 groupId、 artifactId 和 package。通过这三个参数， Maven 将会创建一个名为 frauddetection 的文件夹，包含了所有依赖的整个工程项目将会位于该文件夹下。 将工程目录导入到你的开发环境之后，你可以找到 FraudDetectionJob.java （或 FraudDetectionJob.scala） 代码文件，文件中的代码如下所示。你可以在 IDE 中直接运行这个文件。 同时，你可以试着在数据流中设置一些断点或者以 DEBUG 模式来运行程序，体验 Flink 是如何运行的。
Java FraudDetectionJob.java # package spendreport; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.walkthrough.common.sink.AlertSink; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; import org.apache.flink.walkthrough.common.source.TransactionSource; public class FraudDetectionJob { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Transaction\u0026gt; transactions = env .addSource(new TransactionSource()) .name(\u0026#34;transactions\u0026#34;); DataStream\u0026lt;Alert\u0026gt; alerts = transactions .keyBy(Transaction::getAccountId) .process(new FraudDetector()) .name(\u0026#34;fraud-detector\u0026#34;); alerts .addSink(new AlertSink()) .name(\u0026#34;send-alerts\u0026#34;); env.execute(\u0026#34;Fraud Detection\u0026#34;); } } FraudDetector.java # package spendreport; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } } Scala FraudDetectionJob.scala # package spendreport import org.apache.flink.streaming.api.scala._ import org.apache.flink.walkthrough.common.sink.AlertSink import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction import org.apache.flink.walkthrough.common.source.TransactionSource object FraudDetectionJob { @throws[Exception] def main(args: Array[String]): Unit = { val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment val transactions: DataStream[Transaction] = env .addSource(new TransactionSource) .name(\u0026#34;transactions\u0026#34;) val alerts: DataStream[Alert] = transactions .keyBy(transaction =\u0026gt; transaction.getAccountId) .process(new FraudDetector) .name(\u0026#34;fraud-detector\u0026#34;) alerts .addSink(new AlertSink) .name(\u0026#34;send-alerts\u0026#34;) env.execute(\u0026#34;Fraud Detection\u0026#34;) } } FraudDetector.scala # package spendreport import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @throws[Exception] def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } } 代码分析 # 让我们一步步地来分析一下这两个代码文件。FraudDetectionJob 类定义了程序的数据流，而 FraudDetector 类定义了欺诈交易检测的业务逻辑。
下面我们开始讲解整个 Job 是如何组装到 FraudDetectionJob 类的 main 函数中的。
执行环境 # 第一行的 StreamExecutionEnvironment 用于设置你的执行环境。 任务执行环境用于定义任务的属性、创建数据源以及最终启动任务的执行。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Scala val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment 创建数据源 # 数据源从外部系统例如 Apache Kafka、Rabbit MQ 或者 Apache Pulsar 接收数据，然后将数据送到 Flink 程序中。 这个代码练习使用的是一个能够无限循环生成信用卡模拟交易数据的数据源。 每条交易数据包括了信用卡 ID （accountId），交易发生的时间 （timestamp） 以及交易的金额（amount）。 绑定到数据源上的 name 属性是为了调试方便，如果发生一些异常，我们能够通过它快速定位问题发生在哪里。
Java DataStream\u0026lt;Transaction\u0026gt; transactions = env .addSource(new TransactionSource()) .name(\u0026#34;transactions\u0026#34;); Scala val transactions: DataStream[Transaction] = env .addSource(new TransactionSource) .name(\u0026#34;transactions\u0026#34;) 对事件分区 \u0026amp; 欺诈检测 # transactions 这个数据流包含了大量的用户交易数据，需要被划分到多个并发上进行欺诈检测处理。由于欺诈行为的发生是基于某一个账户的，所以，必须要要保证同一个账户的所有交易行为数据要被同一个并发的 task 进行处理。
为了保证同一个 task 处理同一个 key 的所有数据，你可以使用 DataStream#keyBy 对流进行分区。 process() 函数对流绑定了一个操作，这个操作将会对流上的每一个消息调用所定义好的函数。 通常，一个操作会紧跟着 keyBy 被调用，在这个例子中，这个操作是FraudDetector，该操作是在一个 keyed context 上执行的。
Java DataStream\u0026lt;Alert\u0026gt; alerts = transactions .keyBy(Transaction::getAccountId) .process(new FraudDetector()) .name(\u0026#34;fraud-detector\u0026#34;); Scala val alerts: DataStream[Alert] = transactions .keyBy(transaction =\u0026gt; transaction.getAccountId) .process(new FraudDetector) .name(\u0026#34;fraud-detector\u0026#34;) 输出结果 # sink 会将 DataStream 写出到外部系统，例如 Apache Kafka、Cassandra 或者 AWS Kinesis 等。 AlertSink 使用 INFO 的日志级别打印每一个 Alert 的数据记录，而不是将其写入持久存储，以便你可以方便地查看结果。
Java alerts.addSink(new AlertSink()); Scala alerts.addSink(new AlertSink) 运行作业 # Flink 程序是懒加载的，并且只有在完全搭建好之后，才能够发布到集群上执行。 调用 StreamExecutionEnvironment#execute 时给任务传递一个任务名参数，就可以开始运行任务。
Java env.execute(\u0026#34;Fraud Detection\u0026#34;); Scala env.execute(\u0026#34;Fraud Detection\u0026#34;) 欺诈检测器 # 欺诈检查类 FraudDetector 是 KeyedProcessFunction 接口的一个实现。 他的方法 KeyedProcessFunction#processElement 将会在每个交易事件上被调用。 这个程序里边会对每笔交易发出警报，有人可能会说这做报过于保守了。
本教程的后续步骤将指导你对这个欺诈检测器进行更有意义的业务逻辑扩展。
Java public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } } Scala object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @throws[Exception] def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } } 实现一个真正的应用程序 # 我们先实现第一版报警程序，对于一个账户，如果出现小于 \$1 美元的交易后紧跟着一个大于 \$500 的交易，就输出一个报警信息。
假设你的欺诈检测器所处理的交易数据如下：
交易 3 和交易 4 应该被标记为欺诈行为，因为交易 3 是一个 \$0.09 的小额交易，而紧随着的交易 4 是一个 \$510 的大额交易。 另外，交易 7、8 和 交易 9 就不属于欺诈交易了，因为在交易 7 这个 \$0.02 的小额交易之后，并没有跟随一个大额交易，而是一个金额适中的交易，这使得交易 7 到 交易 9 不属于欺诈行为。
欺诈检测器需要在多个交易事件之间记住一些信息。仅当一个大额的交易紧随一个小额交易的情况发生时，这个大额交易才被认为是欺诈交易。 在多个事件之间存储信息就需要使用到 状态，这也是我们选择使用 KeyedProcessFunction 的原因。 它能够同时提供对状态和时间的细粒度操作，这使得我们能够在接下来的代码练习中实现更复杂的算法。
最直接的实现方式是使用一个 boolean 型的标记状态来表示是否刚处理过一个小额交易。 当处理到该账户的一个大额交易时，你只需要检查这个标记状态来确认上一个交易是是否小额交易即可。
然而，仅使用一个标记作为 FraudDetector 的类成员来记录账户的上一个交易状态是不准确的。 Flink 会在同一个 FraudDetector 的并发实例中处理多个账户的交易数据，假设，当账户 A 和账户 B 的数据被分发的同一个并发实例上处理时，账户 A 的小额交易行为可能会将标记状态设置为真，随后账户 B 的大额交易可能会被误判为欺诈交易。 当然，我们可以使用如 Map 这样的数据结构来保存每一个账户的状态，但是常规的类成员变量是无法做到容错处理的，当任务失败重启后，之前的状态信息将会丢失。 这样的话，如果程序曾出现过失败重启的情况，将会漏掉一些欺诈报警。
为了应对这个问题，Flink 提供了一套支持容错状态的原语，这些原语几乎与常规成员变量一样易于使用。
Flink 中最基础的状态类型是 ValueState，这是一种能够为被其封装的变量添加容错能力的类型。 ValueState 是一种 keyed state，也就是说它只能被用于 keyed context 提供的 operator 中，即所有能够紧随 DataStream#keyBy 之后被调用的operator。 一个 operator 中的 keyed state 的作用域默认是属于它所属的 key 的。 这个例子中，key 就是当前正在处理的交易行为所属的信用卡账户（key 传入 keyBy() 函数调用），而 FraudDetector 维护了每个帐户的标记状态。 ValueState 需要使用 ValueStateDescriptor 来创建，ValueStateDescriptor 包含了 Flink 如何管理变量的一些元数据信息。状态在使用之前需要先被注册。 状态需要使用 open() 函数来注册状态。
Java public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private transient ValueState\u0026lt;Boolean\u0026gt; flagState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); } Scala @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) } ValueState 是一个包装类，类似于 Java 标准库里边的 AtomicReference 和 AtomicLong。 它提供了三个用于交互的方法。update 用于更新状态，value 用于获取状态值，还有 clear 用于清空状态。 如果一个 key 还没有状态，例如当程序刚启动或者调用过 ValueState#clear 方法时，ValueState#value 将会返回 null。 如果需要更新状态，需要调用 ValueState#update 方法，直接更改 ValueState#value 的返回值可能不会被系统识别。 容错处理将在 Flink 后台自动管理，你可以像与常规变量那样与状态变量进行交互。
下边的示例，说明了如何使用标记状态来追踪可能的欺诈交易行为。
Java @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { // Get the current state for the current key Boolean lastTransactionWasSmall = flagState.value(); // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount() \u0026gt; LARGE_AMOUNT) { // Output an alert downstream Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } // Clean up our state flagState.clear(); } if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // Set the flag to true flagState.update(true); } } Scala override def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { // Get the current state for the current key val lastTransactionWasSmall = flagState.value // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount \u0026gt; FraudDetector.LARGE_AMOUNT) { // Output an alert downstream val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } // Clean up our state flagState.clear() } if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) } } 对于每笔交易，欺诈检测器都会检查该帐户的标记状态。 请记住，ValueState 的作用域始终限于当前的 key，即信用卡帐户。 如果标记状态不为空，则该帐户的上一笔交易是小额的，因此，如果当前这笔交易的金额很大，那么检测程序将输出报警信息。
在检查之后，不论是什么状态，都需要被清空。 不管是当前交易触发了欺诈报警而造成模式的结束，还是当前交易没有触发报警而造成模式的中断，都需要重新开始新的模式检测。
最后，检查当前交易的金额是否属于小额交易。 如果是，那么需要设置标记状态，以便可以在下一个事件中对其进行检查。 注意，ValueState\u0026lt;Boolean\u0026gt; 实际上有 3 种状态：unset (null)，true，和 false，ValueState 是允许空值的。 我们的程序只使用了 unset (null) 和 true 两种来判断标记状态被设置了与否。
欺诈检测器 v2：状态 + 时间 = ❤️ # 骗子们在小额交易后不会等很久就进行大额消费，这样可以降低小额测试交易被发现的几率。 比如，假设你为欺诈检测器设置了一分钟的超时，对于上边的例子，交易 3 和 交易 4 只有间隔在一分钟之内才被认为是欺诈交易。 Flink 中的 KeyedProcessFunction 允许您设置计时器，该计时器在将来的某个时间点执行回调函数。
让我们看看如何修改程序以符合我们的新要求：
当标记状态被设置为 true 时，设置一个在当前时间一分钟后触发的定时器。 当定时器被触发时，重置标记状态。 当标记状态被重置时，删除定时器。 要删除一个定时器，你需要记录这个定时器的触发时间，这同样需要状态来实现，所以你需要在标记状态后也创建一个记录定时器时间的状态。
Java private transient ValueState\u0026lt;Boolean\u0026gt; flagState; private transient ValueState\u0026lt;Long\u0026gt; timerState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); ValueStateDescriptor\u0026lt;Long\u0026gt; timerDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;timer-state\u0026#34;, Types.LONG); timerState = getRuntimeContext().getState(timerDescriptor); } Scala @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @transient private var timerState: ValueState[java.lang.Long] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) val timerDescriptor = new ValueStateDescriptor(\u0026#34;timer-state\u0026#34;, Types.LONG) timerState = getRuntimeContext.getState(timerDescriptor) } KeyedProcessFunction#processElement 需要使用提供了定时器服务的 Context 来调用。 定时器服务可以用于查询当前时间、注册定时器和删除定时器。 使用它，你可以在标记状态被设置时，也设置一个当前时间一分钟后触发的定时器，同时，将触发时间保存到 timerState 状态中。
Java if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // set the flag to true flagState.update(true); // set the timer and timer state long timer = context.timerService().currentProcessingTime() + ONE_MINUTE; context.timerService().registerProcessingTimeTimer(timer); timerState.update(timer); } Scala if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) // set the timer and timer state val timer = context.timerService.currentProcessingTime + FraudDetector.ONE_MINUTE context.timerService.registerProcessingTimeTimer(timer) timerState.update(timer) } 处理时间是本地时钟时间，这是由运行任务的服务器的系统时间来决定的。
当定时器触发时，将会调用 KeyedProcessFunction#onTimer 方法。 通过重写这个方法来实现一个你自己的重置状态的回调逻辑。
Java @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;Alert\u0026gt; out) { // remove flag after 1 minute timerState.clear(); flagState.clear(); } Scala override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Long, Transaction, Alert]#OnTimerContext, out: Collector[Alert]): Unit = { // remove flag after 1 minute timerState.clear() flagState.clear() } 最后，如果要取消定时器，你需要删除已经注册的定时器，并同时清空保存定时器的状态。 你可以把这些逻辑封装到一个助手函数中，而不是直接调用 flagState.clear()。
Java private void cleanUp(Context ctx) throws Exception { // delete timer Long timer = timerState.value(); ctx.timerService().deleteProcessingTimeTimer(timer); // clean up all state timerState.clear(); flagState.clear(); } Scala @throws[Exception] private def cleanUp(ctx: KeyedProcessFunction[Long, Transaction, Alert]#Context): Unit = { // delete timer val timer = timerState.value ctx.timerService.deleteProcessingTimeTimer(timer) // clean up all states timerState.clear() flagState.clear() } 这就是一个功能完备的，有状态的分布式流处理程序了。
完整的程序 # Java package spendreport; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.walkthrough.common.entity.Alert; import org.apache.flink.walkthrough.common.entity.Transaction; public class FraudDetector extends KeyedProcessFunction\u0026lt;Long, Transaction, Alert\u0026gt; { private static final long serialVersionUID = 1L; private static final double SMALL_AMOUNT = 1.00; private static final double LARGE_AMOUNT = 500.00; private static final long ONE_MINUTE = 60 * 1000; private transient ValueState\u0026lt;Boolean\u0026gt; flagState; private transient ValueState\u0026lt;Long\u0026gt; timerState; @Override public void open(Configuration parameters) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; flagDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;flag\u0026#34;, Types.BOOLEAN); flagState = getRuntimeContext().getState(flagDescriptor); ValueStateDescriptor\u0026lt;Long\u0026gt; timerDescriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;timer-state\u0026#34;, Types.LONG); timerState = getRuntimeContext().getState(timerDescriptor); } @Override public void processElement( Transaction transaction, Context context, Collector\u0026lt;Alert\u0026gt; collector) throws Exception { // Get the current state for the current key Boolean lastTransactionWasSmall = flagState.value(); // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount() \u0026gt; LARGE_AMOUNT) { //Output an alert downstream Alert alert = new Alert(); alert.setId(transaction.getAccountId()); collector.collect(alert); } // Clean up our state cleanUp(context); } if (transaction.getAmount() \u0026lt; SMALL_AMOUNT) { // set the flag to true flagState.update(true); long timer = context.timerService().currentProcessingTime() + ONE_MINUTE; context.timerService().registerProcessingTimeTimer(timer); timerState.update(timer); } } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;Alert\u0026gt; out) { // remove flag after 1 minute timerState.clear(); flagState.clear(); } private void cleanUp(Context ctx) throws Exception { // delete timer Long timer = timerState.value(); ctx.timerService().deleteProcessingTimeTimer(timer); // clean up all state timerState.clear(); flagState.clear(); } } Scala package spendreport import org.apache.flink.api.common.state.{ValueState, ValueStateDescriptor} import org.apache.flink.api.scala.typeutils.Types import org.apache.flink.configuration.Configuration import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector import org.apache.flink.walkthrough.common.entity.Alert import org.apache.flink.walkthrough.common.entity.Transaction object FraudDetector { val SMALL_AMOUNT: Double = 1.00 val LARGE_AMOUNT: Double = 500.00 val ONE_MINUTE: Long = 60 * 1000L } @SerialVersionUID(1L) class FraudDetector extends KeyedProcessFunction[Long, Transaction, Alert] { @transient private var flagState: ValueState[java.lang.Boolean] = _ @transient private var timerState: ValueState[java.lang.Long] = _ @throws[Exception] override def open(parameters: Configuration): Unit = { val flagDescriptor = new ValueStateDescriptor(\u0026#34;flag\u0026#34;, Types.BOOLEAN) flagState = getRuntimeContext.getState(flagDescriptor) val timerDescriptor = new ValueStateDescriptor(\u0026#34;timer-state\u0026#34;, Types.LONG) timerState = getRuntimeContext.getState(timerDescriptor) } override def processElement( transaction: Transaction, context: KeyedProcessFunction[Long, Transaction, Alert]#Context, collector: Collector[Alert]): Unit = { // Get the current state for the current key val lastTransactionWasSmall = flagState.value // Check if the flag is set if (lastTransactionWasSmall != null) { if (transaction.getAmount \u0026gt; FraudDetector.LARGE_AMOUNT) { // Output an alert downstream val alert = new Alert alert.setId(transaction.getAccountId) collector.collect(alert) } // Clean up our state cleanUp(context) } if (transaction.getAmount \u0026lt; FraudDetector.SMALL_AMOUNT) { // set the flag to true flagState.update(true) val timer = context.timerService.currentProcessingTime + FraudDetector.ONE_MINUTE context.timerService.registerProcessingTimeTimer(timer) timerState.update(timer) } } override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Long, Transaction, Alert]#OnTimerContext, out: Collector[Alert]): Unit = { // remove flag after 1 minute timerState.clear() flagState.clear() } @throws[Exception] private def cleanUp(ctx: KeyedProcessFunction[Long, Transaction, Alert]#Context): Unit = { // delete timer val timer = timerState.value ctx.timerService.deleteProcessingTimeTimer(timer) // clean up all states timerState.clear() flagState.clear() } } 期望的结果 # 使用已准备好的 TransactionSource 数据源运行这个代码，将会检测到账户 3 的欺诈行为，并输出报警信息。 你将能够在你的 task manager 的日志中看到下边输出：
2019-08-19 14:22:06,220 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:11,383 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:16,551 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:21,723 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} 2019-08-19 14:22:26,896 INFO org.apache.flink.walkthrough.common.sink.AlertSink - Alert{id=3} `}),e.add({id:90,href:"/flink/flink-docs-master/zh/docs/concepts/time/",title:"及时流处理",section:"概念透析",content:` 及时流处理 # Introduction # Timely stream processing is an extension of stateful stream processing in which time plays some role in the computation. Among other things, this is the case when you do time series analysis, when doing aggregations based on certain time periods (typically called windows), or when you do event processing where the time when an event occurred is important.
In the following sections we will highlight some of the topics that you should consider when working with timely Flink Applications.
Back to top
Notions of Time: Event Time and Processing Time # When referring to time in a streaming program (for example to define windows), one can refer to different notions of time:
Processing time: Processing time refers to the system time of the machine that is executing the respective operation.
When a streaming program runs on processing time, all time-based operations (like time windows) will use the system clock of the machines that run the respective operator. An hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour. For example, if an application begins running at 9:15am, the first hourly processing time window will include events processed between 9:15am and 10:00am, the next window will include events processed between 10:00am and 11:00am, and so on.
Processing time is the simplest notion of time and requires no coordination between streams and machines. It provides the best performance and the lowest latency. However, in distributed and asynchronous environments processing time does not provide determinism, because it is susceptible to the speed at which records arrive in the system (for example from the message queue), to the speed at which the records flow between operators inside the system, and to outages (scheduled, or otherwise).
Event time: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink, and that event timestamp can be extracted from each record. In event time, the progress of time depends on the data, not on any wall clocks. Event time programs must specify how to generate Event Time Watermarks, which is the mechanism that signals progress in event time. This watermarking mechanism is described in a later section, below.
In a perfect world, event time processing would yield completely consistent and deterministic results, regardless of when events arrive, or their ordering. However, unless the events are known to arrive in-order (by timestamp), event time processing incurs some latency while waiting for out-of-order events. As it is only possible to wait for a finite period of time, this places a limit on how deterministic event time applications can be.
Assuming all of the data has arrived, event time operations will behave as expected, and produce correct and consistent results even when working with out-of-order or late events, or when reprocessing historic data. For example, an hourly event time window will contain all records that carry an event timestamp that falls into that hour, regardless of the order in which they arrive, or when they are processed. (See the section on lateness for more information.)
Note that sometimes when event time programs are processing live data in real-time, they will use some processing time operations in order to guarantee that they are progressing in a timely fashion.
Back to top
Event Time and Watermarks # Note: Flink implements many techniques from the Dataflow Model. For a good introduction to event time and watermarks, have a look at the articles below.
Streaming 101 by Tyler Akidau The Dataflow Model paper A stream processor that supports event time needs a way to measure the progress of event time. For example, a window operator that builds hourly windows needs to be notified when event time has passed beyond the end of an hour, so that the operator can close the window in progress.
Event time can progress independently of processing time (measured by wall clocks). For example, in one program the current event time of an operator may trail slightly behind the processing time (accounting for a delay in receiving the events), while both proceed at the same speed. On the other hand, another streaming program might progress through weeks of event time with only a few seconds of processing, by fast-forwarding through some historic data already buffered in a Kafka topic (or another message queue).
The mechanism in Flink to measure progress in event time is watermarks. Watermarks flow as part of the data stream and carry a timestamp t. A Watermark(t) declares that event time has reached time t in that stream, meaning that there should be no more elements from the stream with a timestamp t\u0026rsquo; \u0026lt;= t (i.e. events with timestamps older or equal to the watermark).
The figure below shows a stream of events with (logical) timestamps, and watermarks flowing inline. In this example the events are in order (with respect to their timestamps), meaning that the watermarks are simply periodic markers in the stream.
Watermarks are crucial for out-of-order streams, as illustrated below, where the events are not ordered by their timestamps. In general a watermark is a declaration that by that point in the stream, all events up to a certain timestamp should have arrived. Once a watermark reaches an operator, the operator can advance its internal event time clock to the value of the watermark.
Note that event time is inherited by a freshly created stream element (or elements) from either the event that produced them or from watermark that triggered creation of those elements.
Watermarks in Parallel Streams # Watermarks are generated at, or directly after, source functions. Each parallel subtask of a source function usually generates its watermarks independently. These watermarks define the event time at that particular parallel source.
As the watermarks flow through the streaming program, they advance the event time at the operators where they arrive. Whenever an operator advances its event time, it generates a new watermark downstream for its successor operators.
Some operators consume multiple input streams; a union, for example, or operators following a keyBy(\u0026hellip;) or partition(\u0026hellip;) function. Such an operator\u0026rsquo;s current event time is the minimum of its input streams\u0026rsquo; event times. As its input streams update their event times, so does the operator.
The figure below shows an example of events and watermarks flowing through parallel streams, and operators tracking event time.
Lateness # It is possible that certain elements will violate the watermark condition, meaning that even after the Watermark(t) has occurred, more elements with timestamp t\u0026rsquo; \u0026lt;= t will occur. In fact, in many real world setups, certain elements can be arbitrarily delayed, making it impossible to specify a time by which all elements of a certain event timestamp will have occurred. Furthermore, even if the lateness can be bounded, delaying the watermarks by too much is often not desirable, because it causes too much delay in the evaluation of event time windows.
For this reason, streaming programs may explicitly expect some late elements. Late elements are elements that arrive after the system\u0026rsquo;s event time clock (as signaled by the watermarks) has already passed the time of the late element\u0026rsquo;s timestamp. See Allowed Lateness for more information on how to work with late elements in event time windows.
Windowing # Aggregating events (e.g., counts, sums) works differently on streams than in batch processing. For example, it is impossible to count all elements in a stream, because streams are in general infinite (unbounded). Instead, aggregates on streams (counts, sums, etc), are scoped by windows, such as \u0026ldquo;count over the last 5 minutes\u0026rdquo;, or \u0026ldquo;sum of the last 100 elements\u0026rdquo;.
Windows can be time driven (example: every 30 seconds) or data driven (example: every 100 elements). One typically distinguishes different types of windows, such as tumbling windows (no overlap), sliding windows (with overlap), and session windows (punctuated by a gap of inactivity).
Please check out this blog post for additional examples of windows or take a look a window documentation of the DataStream API.
Back to top
`}),e.add({id:91,href:"/flink/flink-docs-master/zh/docs/ops/monitoring/back_pressure/",title:"监控反压",section:"Monitoring",content:` 监控反压 # Flink Web 界面提供了一个选项卡来监控正在运行 jobs 的反压行为。
反压 # 如果你看到一个 task 发生 反压警告（例如： High），意味着它生产数据的速率比下游 task 消费数据的速率要快。 在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。
以一个简单的 Source -\u0026gt; Sink job 为例。如果看到 Source 发生了警告，意味着 Sink 消费数据的速率比 Source 生产数据的速率要慢。 Sink 正在向上游的 Source 算子产生反压。
Task 性能指标 # Task（SubTask）的每个并行实例都可以用三个一组的指标评价：
backPressureTimeMsPerSecond，subtask 被反压的时间 idleTimeMsPerSecond，subtask 等待某类处理的时间 busyTimeMsPerSecond，subtask 实际工作时间 在任何时间点，这三个指标相加都约等于1000ms。 这些指标每两秒更新一次，上报的值表示 subtask 在最近两秒被反压（或闲或忙）的平均时长。 当你的工作负荷是变化的时需要尤其引起注意。比如，一个以恒定50%负载工作的 subtask 和另一个每秒钟在满负载和闲置切换的 subtask 的busyTimeMsPerSecond值相同，都是500ms。
在内部，反压根据输出 buffers 的可用性来进行判断的。 如果一个 task 没有可用的输出 buffers，那么这个 task 就被认定是在被反压。 相反，如果有可用的输入，则可认定为闲置，
示例 # WebUI 集合了所有 subTasks 的反压和繁忙指标的最大值，并在 JobGraph 中将集合的值进行显示。除了显示原始的数值，tasks 也用颜色进行了标记，使检查更加容易。
闲置的 tasks 为蓝色，完全被反压的 tasks 为黑色，完全繁忙的 tasks 被标记为红色。 中间的所有值都表示为这三种颜色之间的过渡色。
反压状态 # 在 Job Overview 旁的 Back Pressure 选项卡中，你可以找到更多细节指标。
如果你看到 subtasks 的状态为 OK 表示没有反压。HIGH 表示这个 subtask 被反压。状态用如下定义：
OK: 0% \u0026lt;= 反压比例 \u0026lt;= 10% LOW: 10% \u0026lt; 反压比例 \u0026lt;= 50% HIGH: 50% \u0026lt; 反压比例 \u0026lt;= 100% 除此之外，你还可以找到每一个 subtask 被反压、闲置或是繁忙的时间百分比。
Back to top
`}),e.add({id:92,href:"/flink/flink-docs-master/zh/docs/dev/datastream/event-time/built_in/",title:"内置 Watermark 生成器",section:"事件时间",content:` 内置 Watermark 生成器 # 如生成 Watermark 小节中所述，Flink 提供的抽象方法可以允许用户自己去定义时间戳分配方式和 watermark 生成的方式。你可以通过实现 WatermarkGenerator 接口来实现上述功能。
为了进一步简化此类任务的编程工作，Flink 框架预设了一些时间戳分配器。本节后续内容有举例。除了开箱即用的已有实现外，其还可以作为自定义实现的示例以供参考。
单调递增时间戳分配器 # 周期性 watermark 生成方式的一个最简单特例就是你给定的数据源中数据的时间戳升序出现。在这种情况下，当前时间戳就可以充当 watermark，因为后续到达数据的时间戳不会比当前的小。
注意：在 Flink 应用程序中，如果是并行数据源，则只要求并行数据源中的每个单分区数据源任务时间戳递增。例如，设置每一个并行数据源实例都只读取一个 Kafka 分区，则时间戳只需在每个 Kafka 分区内递增即可。Flink 的 watermark 合并机制会在并行数据流进行分发（shuffle）、联合（union）、连接（connect）或合并（merge）时生成正确的 watermark。
Java WatermarkStrategy.forMonotonousTimestamps(); Scala WatermarkStrategy.forMonotonousTimestamps() Python WatermarkStrategy.for_monotonous_timestamps() 数据之间存在最大固定延迟的时间戳分配器 # 另一个周期性 watermark 生成的典型例子是，watermark 滞后于数据流中最大（事件时间）时间戳一个固定的时间量。该示例可以覆盖的场景是你预先知道数据流中的数据可能遇到的最大延迟，例如，在测试场景下创建了一个自定义数据源，并且这个数据源的产生的数据的时间戳在一个固定范围之内。Flink 针对上述场景提供了 boundedOutfordernessWatermarks 生成器，该生成器将 maxOutOfOrderness 作为参数，该参数代表在计算给定窗口的结果时，允许元素被忽略计算之前延迟到达的最长时间。其中延迟时长就等于 t - t_w ，其中 t 代表元素的（事件时间）时间戳，t_w 代表前一个 watermark 对应的（事件时间）时间戳。如果 lateness \u0026gt; 0，则认为该元素迟到了，并且在计算相应窗口的结果时默认会被忽略。有关使用延迟元素的详细内容，请参阅有关允许延迟的文档。
Java WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); Scala WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(10)) Python WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(10)) Back to top
`}),e.add({id:93,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_tm/",title:"配置 TaskManager 内存",section:"内存配置",content:` 配置 TaskManager 内存 # Flink 的 TaskManager 负责执行用户代码。 根据实际需求为 TaskManager 配置内存将有助于减少 Flink 的资源占用，增强作业运行的稳定性。
本文接下来介绍的内存配置方法适用于 1.10 及以上版本。 Flink 在 1.10 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
提示 本篇内存配置文档仅针对 TaskManager！ 与 JobManager 相比，TaskManager 具有相似但更加复杂的内存模型。
配置总内存 # Flink JVM 进程的*进程总内存（Total Process Memory）*包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 其中，*Flink 总内存（Total Flink Memory）*包括 JVM 堆内存（Heap Memory）、*托管内存（Managed Memory）*以及其他直接内存（Direct Memory）或本地内存（Native Memory）。
如果你是在本地运行 Flink（例如在 IDE 中）而非创建一个集群，那么本文介绍的配置并非所有都是适用的，详情请参考本地执行。
其他情况下，配置 Flink 内存最简单的方法就是配置总内存。 此外，Flink 也支持更细粒度的内存配置方式。
Flink 会根据默认值或其他配置参数自动调整剩余内存部分的大小。 接下来的章节将介绍关于各内存部分的更多细节。
配置堆内存和托管内存 # 如配置总内存中所述，另一种配置 Flink 内存的方式是同时设置任务堆内存和托管内存。 通过这种方式，用户可以更好地掌控用于 Flink 任务的 JVM 堆内存及 Flink 的托管内存大小。
Flink 会根据默认值或其他配置参数自动调整剩余内存部分的大小。 关于各内存部分的更多细节，请参考相关文档。
提示 如果已经明确设置了任务堆内存和托管内存，建议不要再设置进程总内存或 Flink 总内存，否则可能会造成内存配置冲突。
任务（算子）堆内存 # 如果希望确保指定大小的 JVM 堆内存给用户代码使用，可以明确指定任务堆内存（taskmanager.memory.task.heap.size）。 指定的内存将被包含在总的 JVM 堆空间中，专门用于 Flink 算子及用户代码的执行。
托管内存 # 托管内存是由 Flink 负责分配和管理的本地（堆外）内存。 以下场景需要使用托管内存：
流处理作业中用于 RocksDB State Backend。 流处理和批处理作业中用于排序、哈希表及缓存中间结果。 流处理和批处理作业中用于在 Python 进程中执行用户自定义函数。 可以通过以下两种范式指定托管内存的大小：
通过 taskmanager.memory.managed.size 明确指定其大小。 通过 taskmanager.memory.managed.fraction 指定在Flink 总内存中的占比。 当同时指定二者时，会优先采用指定的大小（Size）。 若二者均未指定，会根据默认占比进行计算。
请同时参考如何配置 State Backend 内存以及如何配置批处理作业内存。
消费者权重 # 对于包含不同种类的托管内存消费者的作业，可以进一步控制托管内存如何在消费者之间分配。 通过 taskmanager.memory.managed.consumer-weights 可以为每一种类型的消费者指定一个权重，Flink 会按照权重的比例进行内存分配。 目前支持的消费者类型包括：
OPERATOR: 用于内置算法。 STATE_BACKEND: 用于流处理中的 RocksDB State Backend。 PYTHON：用户 Python 进程。 例如，一个流处理作业同时使用到了 RocksDB State Backend 和 Python UDF，消费者权重设置为 STATE_BACKEND:70,PYTHON:30，那么 Flink 会将 70% 的托管内存用于 RocksDB State Backend，30% 留给 Python 进程。
提示 只有作业中包含某种类型的消费者时，Flink 才会为该类型分配托管内存。 例如，一个流处理作业使用 Heap State Backend 和 Python UDF，消费者权重设置为 STATE_BACKEND:70,PYTHON:30，那么 Flink 会将全部托管内存用于 Python 进程，因为 Heap State Backend 不使用托管内存。
提示 对于未出现在消费者权重中的类型，Flink 将不会为其分配托管内存。 如果缺失的类型是作业运行所必须的，则会引发内存分配失败。 默认情况下，消费者权重中包含了所有可能的消费者类型。 上述问题仅可能出现在用户显式地配置了消费者权重的情况下。
配置堆外内存（直接内存或本地内存） # 用户代码中分配的堆外内存被归为任务堆外内存（Task Off-heap Memory），可以通过 taskmanager.memory.task.off-heap.size 指定。
提示 你也可以调整框架堆外内存（Framework Off-heap Memory）。 这是一个进阶配置，建议仅在确定 Flink 框架需要更多的内存时调整该配置。
Flink 将框架堆外内存和任务堆外内存都计算在 JVM 的直接内存限制中，请参考 JVM 参数。
提示 本地内存（非直接内存）也可以被归在框架堆外内存或任务堆外内存中，在这种情况下 JVM 的直接内存限制可能会高于实际需求。
提示 网络内存（Network Memory）同样被计算在 JVM 直接内存中。 Flink 会负责管理网络内存，保证其实际用量不会超过配置大小。 因此，调整网络内存的大小不会对其他堆外内存有实质上的影响。
请参考内存模型详解。
内存模型详解 # 如上图所示，下表中列出了 Flink TaskManager 内存模型的所有组成部分，以及影响其大小的相关配置参数。
组成部分 配置参数 描述 框架堆内存（Framework Heap Memory） taskmanager.memory.framework.heap.size 用于 Flink 框架的 JVM 堆内存（进阶配置）。 任务堆内存（Task Heap Memory） taskmanager.memory.task.heap.size 用于 Flink 应用的算子及用户代码的 JVM 堆内存。 托管内存（Managed memory） taskmanager.memory.managed.size taskmanager.memory.managed.fraction 由 Flink 管理的用于排序、哈希表、缓存中间结果及 RocksDB State Backend 的本地内存。 框架堆外内存（Framework Off-heap Memory） taskmanager.memory.framework.off-heap.size 用于 Flink 框架的堆外内存（直接内存或本地内存）（进阶配置）。 任务堆外内存（Task Off-heap Memory） taskmanager.memory.task.off-heap.size 用于 Flink 应用的算子及用户代码的堆外内存（直接内存或本地内存）。 网络内存（Network Memory） taskmanager.memory.network.min taskmanager.memory.network.max taskmanager.memory.network.fraction 用于任务之间数据传输的直接内存（例如网络传输缓冲）。该内存部分为基于 Flink 总内存的受限的等比内存部分。这块内存被用于分配网络缓冲 JVM Metaspace taskmanager.memory.jvm-metaspace.size Flink JVM 进程的 Metaspace。 JVM 开销 taskmanager.memory.jvm-overhead.min taskmanager.memory.jvm-overhead.max taskmanager.memory.jvm-overhead.fraction 用于其他 JVM 开销的本地内存，例如栈空间、垃圾回收空间等。该内存部分为基于进程总内存的受限的等比内存部分。 我们可以看到，有些内存部分的大小可以直接通过一个配置参数进行设置，有些则需要根据多个参数进行调整。
框架内存 # 通常情况下，不建议对框架堆内存和框架堆外内存进行调整。 除非你非常肯定 Flink 的内部数据结构及操作需要更多的内存。 这可能与具体的部署环境及作业结构有关，例如非常高的并发度。 此外，Flink 的部分依赖（例如 Hadoop）在某些特定的情况下也可能会需要更多的直接内存或本地内存。
提示 不管是堆内存还是堆外内存，Flink 中的框架内存和任务内存之间目前是没有隔离的。 对框架和任务内存的区分，主要是为了在后续版本中做进一步优化。
本地执行 # 如果你是将 Flink 作为一个单独的 Java 程序运行在你的电脑本地而非创建一个集群（例如在 IDE 中），那么只有下列配置会生效，其他配置参数则不会起到任何效果：
组成部分 配置参数 本地执行时的默认值 任务堆内存 taskmanager.memory.task.heap.size 无穷大 任务堆外内存 taskmanager.memory.task.off-heap.size 无穷大 托管内存 taskmanager.memory.managed.size 128MB 网络内存 taskmanager.memory.network.min taskmanager.memory.network.max 64MB 本地执行模式下，上面列出的所有内存部分均可以但不是必须进行配置。 如果未配置，则会采用默认值。 其中，任务堆内存和任务堆外内存的默认值无穷大（Long.MAX_VALUE 字节），以及托管内存的默认值 128MB 均只针对本地执行模式。
提示 这种情况下，任务堆内存的大小与实际的堆空间大小无关。 该配置参数可能与后续版本中的进一步优化相关。 本地执行模式下，JVM 堆空间的实际大小不受 Flink 掌控，而是取决于本地执行进程是如何启动的。 如果希望控制 JVM 的堆空间大小，可以在启动进程时明确地指定相关的 JVM 参数，即 -Xmx 和 -Xms。
`}),e.add({id:94,href:"/flink/flink-docs-master/zh/docs/deployment/config/",title:"配置参数",section:"Deployment",content:` 配置参数 # All configuration is done in conf/flink-conf.yaml, which is expected to be a flat collection of YAML key value pairs with format key: value.
The configuration is parsed and evaluated when the Flink processes are started. Changes to the configuration file require restarting the relevant processes.
The out of the box configuration will use your default Java installation. You can manually set the environment variable JAVA_HOME or the configuration key env.java.home in conf/flink-conf.yaml if you want to manually override the Java runtime to use.
You can specify a different configuration directory location by defining the FLINK_CONF_DIR environment variable. For resource providers which provide non-session deployments, you can specify per-job configurations this way. Make a copy of the conf directory from the Flink distribution and modify the settings on a per-job basis. Note that this is not supported in Docker or standalone Kubernetes deployments. On Docker-based deployments, you can use the FLINK_PROPERTIES environment variable for passing configuration values.
On session clusters, the provided configuration will only be used for configuring execution parameters, e.g. configuration parameters affecting the job, not the underlying cluster.
Basic Setup # The default configuration supports starting a single-node Flink session cluster without any changes. The options in this section are the ones most commonly needed for a basic distributed Flink setup.
Hostnames / Ports
These options are only necessary for standalone application- or session deployments (simple standalone or Kubernetes).
If you use Flink with Yarn or the active Kubernetes integration, the hostnames and ports are automatically discovered.
rest.address, rest.port: These are used by the client to connect to Flink. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes) service in front of the JobManager\u0026rsquo;s REST interface.
The jobmanager.rpc.address (defaults to \u0026ldquo;localhost\u0026rdquo;) and jobmanager.rpc.port (defaults to 6123) config entries are used by the TaskManager to connect to the JobManager/ResourceManager. Set this to the hostname where the JobManager runs, or to the hostname of the (Kubernetes internal) service for the JobManager. This option is ignored on setups with high-availability where the leader election mechanism is used to discover this automatically.
Memory Sizes
The default memory sizes support simple streaming/batch applications, but are too low to yield good performance for more complex applications.
jobmanager.memory.process.size: Total size of the JobManager (JobMaster / ResourceManager / Dispatcher) process. taskmanager.memory.process.size: Total size of the TaskManager process. The total sizes include everything. Flink will subtract some memory for the JVM\u0026rsquo;s own memory requirements (metaspace and others), and divide and configure the rest automatically between its components (JVM Heap, Off-Heap, for Task Managers also network, managed memory etc.).
These value are configured as memory sizes, for example 1536m or 2g.
Parallelism
taskmanager.numberOfTaskSlots: The number of slots that a TaskManager offers (default: 1). Each slot can take one task or pipeline. Having multiple slots in a TaskManager can help amortize certain constant overheads (of the JVM, application libraries, or network connections) across parallel tasks or pipelines. See the Task Slots and Resources concepts section for details.
Running more smaller TaskManagers with one slot each is a good starting point and leads to the best isolation between tasks. Dedicating the same resources to fewer larger TaskManagers with more slots can help to increase resource utilization, at the cost of weaker isolation between the tasks (more tasks share the same JVM).
parallelism.default: The default parallelism used when no parallelism is specified anywhere (default: 1).
Checkpointing
You can configure checkpointing directly in code within your Flink job or application. Putting these values here in the configuration defines them as defaults in case the application does not configure anything.
state.backend: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are filesystem or rocksdb. state.checkpoints.dir: The directory to write checkpoints to. This takes a path URI like s3://mybucket/flink-app/checkpoints or hdfs://namenode:port/flink/checkpoints. state.savepoints.dir: The default directory for savepoints. Takes a path URI, similar to state.checkpoints.dir. execution.checkpointing.interval: The base interval setting. To enable checkpointing, you need to set this value larger than 0. Web UI
web.submit.enable: Enables uploading and starting jobs through the Flink UI (true by default). Please note that even when this is disabled, session clusters still accept jobs through REST requests (HTTP calls). This flag only guards the feature to upload jobs in the UI. web.cancel.enable: Enables canceling jobs through the Flink UI (true by default). Please note that even when this is disabled, session clusters still cancel jobs through REST requests (HTTP calls). This flag only guards the feature to cancel jobs in the UI. web.upload.dir: The directory where to store uploaded jobs. Only used when web.submit.enable is true. Other
io.tmp.dirs: The directories where Flink puts local data, defaults to the system temp directory (java.io.tmpdir property). If a list of directories is configured, Flink will rotate files across the directories.
The data put in these directories include by default the files created by RocksDB, spilled intermediate results (batch algorithms), and cached jar files.
This data is NOT relied upon for persistence/recovery, but if this data gets deleted, it typically causes a heavyweight recovery operation. It is hence recommended to set this to a directory that is not automatically periodically purged.
Yarn and Kubernetes setups automatically configure this value to the local working directories by default.
Common Setup Options # Common options to configure your Flink application or cluster.
Hosts and Ports # Options to configure hostnames and ports for the different Flink components.
The JobManager hostname and port are only relevant for standalone setups without high-availability. In that setup, the config values are used by the TaskManagers to find (and connect to) the JobManager. In all highly-available setups, the TaskManagers discover the JobManager via the High-Availability-Service (for example ZooKeeper).
Setups using resource orchestration frameworks (K8s, Yarn) typically use the framework\u0026rsquo;s service discovery facilities.
You do not need to configure any TaskManager hosts and ports, unless the setup requires the use of specific port ranges or specific network interfaces to bind to.
Key Default Type Description jobmanager.rpc.address (none) String The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.rpc.port 6123 Integer The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. metrics.internal.query-service.port "0" String The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. rest.address (none) String The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE. rest.bind-address (none) String The address that the server binds itself. rest.bind-port "8081" String The port that the server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine. rest.port 8081 Integer The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE. taskmanager.data.port 0 Integer The task manager’s external port used for data exchange operations. taskmanager.host (none) String The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. taskmanager.rpc.port "0" String The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. Fault Tolerance # These configuration options control Flink\u0026rsquo;s restart behaviour in case of failures during the execution. By configuring these options in your flink-conf.yaml, you define the cluster\u0026rsquo;s default restart strategy.
The default restart strategy will only take effect if no job specific restart strategy has been configured via the ExecutionConfig.
Key Default Type Description restart-strategy (none) String Defines the restart strategy to use in case of job failures.
Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy. More details can be found here.failurerate, failure-rate: Failure rate restart strategy. More details can be found here.exponentialdelay, exponential-delay: Exponential delay restart strategy. More details can be found here.If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay. Fixed Delay Restart Strategy
Key Default Type Description restart-strategy.fixed-delay.attempts 1 Integer The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay. restart-strategy.fixed-delay.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s" Failure Rate Restart Strategy
Key Default Type Description restart-strategy.failure-rate.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.failure-rate-interval 1 min Duration Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.max-failures-per-interval 1 Integer Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate. Retryable Cleanup # After jobs reach a globally-terminal state, a cleanup of all related resources is performed. This cleanup can be retried in case of failure. Different retry strategies can be configured to change this behavior:
Key Default Type Description cleanup-strategy "exponential-delay" String Defines the cleanup strategy to use in case of cleanup failures.
Accepted values are:none, disable, off: Cleanup is only performed once. No retry will be initiated in case of failure. The job artifacts (and the job's JobResultStore entry) have to be cleaned up manually in case of a failure.fixed-delay, fixeddelay: Cleanup attempts will be separated by a fixed interval up to the point where the cleanup is considered successful or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.exponential-delay, exponentialdelay: Exponential delay restart strategy triggers the cleanup with an exponentially increasing delay up to the point where the cleanup succeeded or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.The default configuration relies on an exponentially delayed retry strategy with the given default values. Fixed-Delay Cleanup Retry Strategy
Key Default Type Description cleanup-strategy.fixed-delay.attempts infinite Integer The number of times that Flink retries the cleanup before giving up if cleanup-strategy has been set to fixed-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. cleanup-strategy.fixed-delay.delay 1 min Duration Amount of time that Flink waits before re-triggering the cleanup after a failed attempt if the cleanup-strategy is set to fixed-delay. It can be specified using the following notation: "1 min", "20 s" Exponential-Delay Cleanup Retry Strategy
Key Default Type Description cleanup-strategy.exponential-delay.attempts infinite Integer The number of times a failed cleanup is retried if cleanup-strategy has been set to exponential-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually. cleanup-strategy.exponential-delay.initial-backoff 1 s Duration Starting duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s" cleanup-strategy.exponential-delay.max-backoff 1 h Duration The highest possible duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s" Checkpoints and State Backends # These options control the basic setup of state backends and checkpointing behavior.
The options are only relevant for jobs/applications executing in a continuous streaming fashion. Jobs/applications executing in a batch fashion do not use state backends and checkpoints, but different internal data structures that are optimized for batch processing.
Key Default Type Description state.backend (none) String The state backend to be used to store state.
The implementation can be specified either via their shortcut name, or via the class name of a StateBackendFactory. If a factory is specified it is instantiated via its zero argument constructor and its StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'hashmap' and 'rocksdb'. state.checkpoint-storage (none) String The checkpoint storage implementation to be used to checkpoint state.
The implementation can be specified either via their shortcut name, or via the class name of a CheckpointStorageFactory. If a factory is specified it is instantiated via its zero argument constructor and its CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'jobmanager' and 'filesystem'. state.checkpoints.dir (none) String The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). state.savepoints.dir (none) String The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). state.backend.incremental false Boolean Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. state.backend.local-recovery false Boolean This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, the MemoryStateBackend does not support local recovery and ignores this option. state.checkpoints.num-retained 1 Integer The maximum number of completed checkpoints to retain. taskmanager.state.local.root-dirs (none) String The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignores this option. If not configured it will default to \u0026lt;WORKING_DIR\u0026gt;/localState. The \u0026lt;WORKING_DIR\u0026gt; can be configured via process.taskmanager.working-dir High Availability # High-availability here refers to the ability of the JobManager process to recover from failures.
The JobManager ensures consistency during recovery across TaskManagers. For the JobManager itself to recover consistently, an external service must store a minimal amount of recovery metadata (like \u0026ldquo;ID of last committed checkpoint\u0026rdquo;), as well as help to elect and lock which JobManager is the leader (to avoid split-brain situations).
Key Default Type Description high-availability "NONE" String Defines high-availability mode used for cluster execution. To enable high-availability, set this mode to "ZOOKEEPER", "KUBERNETES", or specify the fully qualified name of the factory class. high-availability.cluster-id "/default" String The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN. high-availability.storageDir (none) String File system path (URI) where Flink persists metadata in high-availability setups. Options for the JobResultStore in high-availability setups
Key Default Type Description job-result-store.delete-on-commit true Boolean Determines whether job results should be automatically removed from the underlying job result store when the corresponding entity transitions into a clean state. If false, the cleaned job results are, instead, marked as clean to indicate their state. In this case, Flink no longer has ownership and the resources need to be cleaned up by the user. job-result-store.storage-path (none) String Defines where job results should be stored. This should be an underlying file-system that provides read-after-write consistency. By default, this is {high-availability.storageDir}/job-result-store/{high-availability.cluster-id}. Options for high-availability setups with ZooKeeper
Key Default Type Description high-availability.zookeeper.path.root "/flink" String The root path under which Flink stores its entries in ZooKeeper. high-availability.zookeeper.quorum (none) String The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper. Memory Configuration # These configuration values control the way that TaskManagers and JobManagers use memory.
Flink tries to shield users as much as possible from the complexity of configuring the JVM for data-intensive processing. In most cases, users should only need to set the values taskmanager.memory.process.size or taskmanager.memory.flink.size (depending on how the setup), and possibly adjusting the ratio of JVM heap and Managed Memory via taskmanager.memory.managed.fraction. The other options below can be used for performance tuning and fixing memory related errors.
For a detailed explanation of how these options interact, see the documentation on TaskManager and JobManager memory configurations.
Key Default Type Description jobmanager.memory.enable-jvm-direct-memory-limit false Boolean Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option. jobmanager.memory.flink.size (none) MemorySize Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration. jobmanager.memory.heap.size (none) MemorySize JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes). jobmanager.memory.jvm-metaspace.size 256 mb MemorySize JVM Metaspace Size for the JobManager. jobmanager.memory.jvm-overhead.fraction 0.1 Float Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.jvm-overhead.max 1 gb MemorySize Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.jvm-overhead.min 192 mb MemorySize Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value. jobmanager.memory.off-heap.size 128 mb MemorySize Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'. jobmanager.memory.process.size (none) MemorySize Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration. taskmanager.memory.flink.size (none) MemorySize Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration. taskmanager.memory.framework.heap.size 128 mb MemorySize Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots. taskmanager.memory.framework.off-heap.batch-shuffle.size 64 mb MemorySize Size of memory used by blocking shuffle for shuffle data read (currently only used by sort-shuffle and hybrid shuffle). Notes: 1) The memory is cut from 'taskmanager.memory.framework.off-heap.size' so must be smaller than that, which means you may also need to increase 'taskmanager.memory.framework.off-heap.size' after you increase this config value; 2) This memory size can influence the shuffle performance and you can increase this config value for large-scale batch jobs (for example, to 128M or 256M). taskmanager.memory.framework.off-heap.size 128 mb MemorySize Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. taskmanager.memory.jvm-metaspace.size 256 mb MemorySize JVM Metaspace Size for the TaskExecutors. taskmanager.memory.jvm-overhead.fraction 0.1 Float Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.jvm-overhead.max 1 gb MemorySize Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.jvm-overhead.min 192 mb MemorySize Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.managed.consumer-weights OPERATOR:70,STATE_BACKEND:70,PYTHON:30 Map Managed memory weights for different kinds of consumers. A slot’s managed memory is shared by all kinds of consumers it contains, proportionally to the kinds’ weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are OPERATOR (for built-in algorithms), STATE_BACKEND (for RocksDB state backend) and PYTHON (for Python processes). taskmanager.memory.managed.fraction 0.4 Float Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified. taskmanager.memory.managed.size (none) MemorySize Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory. taskmanager.memory.network.fraction 0.1 Float Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value. taskmanager.memory.network.max 1 gb MemorySize Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. taskmanager.memory.network.min 64 mb MemorySize Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value. taskmanager.memory.process.size (none) MemorySize Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration. taskmanager.memory.task.heap.size (none) MemorySize Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory. taskmanager.memory.task.off-heap.size 0 bytes MemorySize Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter. Miscellaneous Options # Key Default Type Description fs.allowed-fallback-filesystems (none) String A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb) fs.default-scheme (none) String The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode. io.tmp.dirs 'LOCAL_DIRS' on Yarn. System.getProperty("java.io.tmpdir") in standalone. String Directories for temporary files, separated by",", "|", or the system's java.io.File.pathSeparator. Security # Options for configuring Flink\u0026rsquo;s security and secure interaction with external systems.
SSL # Flink\u0026rsquo;s network connections can be secured via SSL. Please refer to the SSL Setup Docs for detailed setup guide and background.
Key Default Type Description security.ssl.algorithms "TLS_RSA_WITH_AES_128_CBC_SHA" String The comma separated list of standard SSL algorithms to be supported. Read more here security.ssl.internal.cert.fingerprint (none) String The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required security.ssl.internal.enabled false Boolean Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc). security.ssl.internal.key-password (none) String The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.internal.truststore-password (none) String The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server). security.ssl.protocol "TLSv1.2" String The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list. security.ssl.rest.authentication-enabled false Boolean Turns on mutual SSL authentication for external communication via the REST endpoints. security.ssl.rest.cert.fingerprint (none) String The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA security.ssl.rest.enabled false Boolean Turns on SSL for external communication via the REST endpoints. security.ssl.rest.key-password (none) String The secret to decrypt the key in the keystore for Flink's external REST endpoints. security.ssl.rest.keystore (none) String The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints. security.ssl.rest.keystore-password (none) String The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints. security.ssl.rest.truststore (none) String The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints. security.ssl.rest.truststore-password (none) String The password to decrypt the truststore for Flink's external REST endpoints. security.ssl.verify-hostname true Boolean Flag to enable peer’s hostname verification during ssl handshake. Auth with External Systems # ZooKeeper Authentication / Authorization
These options are necessary when connecting to a secured ZooKeeper quorum.
Key Default Type Description zookeeper.sasl.disable false Boolean zookeeper.sasl.login-context-name "Client" String zookeeper.sasl.service-name "zookeeper" String Kerberos-based Authentication / Authorization
Please refer to the Flink and Kerberos Docs for a setup guide and a list of external system to which Flink can authenticate itself via Kerberos.
Key Default Type Description security.kerberos.access.hadoopFileSystems (none) List\u0026lt;String\u0026gt; A comma-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens. security.kerberos.fetch.delegation-token true Boolean Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens. security.kerberos.login.contexts (none) String A comma-separated list of login contexts to provide the Kerberos credentials to (for example, \`Client,KafkaClient\` to use the credentials for ZooKeeper authentication and for Kafka authentication) security.kerberos.login.keytab (none) String Absolute path to a Kerberos keytab file that contains the user credentials. security.kerberos.login.principal (none) String Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache true Boolean Indicates whether to read from your Kerberos ticket cache. security.kerberos.relogin.period 1 min Duration The time period when keytab login happens automatically in order to always have a valid TGT. security.kerberos.tokens.renewal.retry.backoff 1 h Duration The time period how long to wait before retrying to obtain new delegation tokens after a failure. security.kerberos.tokens.renewal.time-ratio 0.75 Double Ratio of the tokens's expiration time when new credentials should be re-obtained. Resource Orchestration Frameworks # This section contains options related to integrating Flink with resource orchestration frameworks, like Kubernetes, Yarn, etc.
Note that is not always necessary to integrate Flink with the resource orchestration framework. For example, you can easily deploy Flink applications on Kubernetes without Flink knowing that it runs on Kubernetes (and without specifying any of the Kubernetes config options here.) See this setup guide for an example.
The options in this section are necessary for setups where Flink itself actively requests and releases resources from the orchestrators.
YARN # Key Default Type Description external-resource.\u0026lt;resource_name\u0026gt;.yarn.config-key (none) String If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount. flink.hadoop.\u0026lt;key\u0026gt; (none) String A general option to probe Hadoop configuration through prefix 'flink.hadoop.'. Flink will remove the prefix to get \u0026lt;key\u0026gt; (from core-default.xml and hdfs-default.xml) then set the \u0026lt;key\u0026gt; and value to Hadoop configuration. For example, flink.hadoop.dfs.replication=5 in Flink configuration and convert to dfs.replication=5 in Hadoop configuration. flink.yarn.\u0026lt;key\u0026gt; (none) String A general option to probe Yarn configuration through prefix 'flink.yarn.'. Flink will remove the prefix 'flink.' to get yarn.\u0026lt;key\u0026gt; (from yarn-default.xml) then set the yarn.\u0026lt;key\u0026gt; and value to Yarn configuration. For example, flink.yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Flink configuration and convert to yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Yarn configuration. yarn.application-attempt-failures-validity-interval 10000 Long Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See here for more information. yarn.application-attempts (none) String Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via yarn.resourcemanager.am.max-attempts). Note that that the entire Flink cluster will restart and the YARN Client will lose the connection. yarn.application-master.port "0" String With this configuration option, users can specify a port, a range of ports or a list of ports for the Application Master (and JobManager) RPC port. By default we recommend using the default value (0) to let the operating system choose an appropriate port. In particular when multiple AMs are running on the same physical host, fixed port assignments prevent the AM from starting. For example when running Flink on YARN on an environment with a restrictive firewall, this option allows specifying a range of allowed ports. yarn.application.id (none) String The YARN application id of the running yarn cluster. This is the YARN cluster where the pipeline is going to be executed. yarn.application.name (none) String A custom name for your YARN application. yarn.application.node-label (none) String Specify YARN node label for the YARN application. yarn.application.priority -1 Integer A non-negative integer indicating the priority for submitting a Flink YARN application. It will only take effect if YARN priority scheduling setting is enabled. Larger integer corresponds with higher priority. If priority is negative or set to '-1'(default), Flink will unset yarn priority setting and use cluster default priority. Please refer to YARN's official documentation for specific settings required to enable priority scheduling for the targeted YARN version. yarn.application.queue (none) String The YARN queue on which to put the current pipeline. yarn.application.type (none) String A custom type for your YARN application.. yarn.appmaster.vcores 1 Integer The number of virtual cores (vcores) used by YARN application master. yarn.classpath.include-user-jar ORDER Enum
Defines whether user-jars are included in the system class path as well as their positioning in the path.
Possible values:"DISABLED": Exclude user jars from the system class path"FIRST": Position at the beginning"LAST": Position at the end"ORDER": Position based on the name of the jar yarn.containers.vcores -1 Integer The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler. yarn.file-replication -1 Integer Number of file replication of each local resource file. If it is not configured, Flink will use the default replication value in hadoop configuration. yarn.flink-dist-jar (none) String The location of the Flink dist jar. yarn.heartbeat.container-request-interval 500 Integer Time between heartbeats with the ResourceManager in milliseconds if Flink requests containers:The lower this value is, the faster Flink will get notified about container allocations since requests and allocations are transmitted via heartbeats.The lower this value is, the more excessive containers might get allocated which will eventually be released but put pressure on Yarn.If you observe too many container allocations on the ResourceManager, then it is recommended to increase this value. See this link for more information. yarn.heartbeat.interval 5 Integer Time between heartbeats with the ResourceManager in seconds. yarn.properties-file.location (none) String When a Flink job is submitted to YARN, the JobManager’s host and the number of available processing slots is written into a properties file, so that the Flink client is able to pick those details up. This configuration parameter allows changing the default location of that file (for example for environments sharing a Flink installation between users). yarn.provided.lib.dirs (none) List\u0026lt;String\u0026gt; A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://\$namenode_address/path/of/flink/lib yarn.provided.usrlib.dir (none) String The provided usrlib directory in remote. It should be pre-uploaded and world-readable. Flink will use it to exclude the local usrlib directory(i.e. usrlib/ under the parent directory of FLINK_LIB_DIR). Unlike yarn.provided.lib.dirs, YARN will not cache it on the nodes as it is for each application. An example could be hdfs://\$namenode_address/path/of/flink/usrlib yarn.security.kerberos.localized-keytab-path "krb5.keytab" String Local (on NodeManager) path where kerberos keytab file will be localized to. If yarn.security.kerberos.ship-local-keytab set to true, Flink willl ship the keytab file as a YARN local resource. In this case, the path is relative to the local resource directory. If set to false, Flink will try to directly locate the keytab from the path itself. yarn.security.kerberos.ship-local-keytab true Boolean When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource. yarn.ship-archives (none) List\u0026lt;String\u0026gt; A semicolon-separated list of archives to be shipped to the YARN cluster. These archives will be un-packed when localizing and they can be any of the following types: ".tar.gz", ".tar", ".tgz", ".dst", ".jar", ".zip". yarn.ship-files (none) List\u0026lt;String\u0026gt; A semicolon-separated list of files and/or directories to be shipped to the YARN cluster. yarn.staging-directory (none) String Staging directory used to store YARN files while submitting applications. Per default, it uses the home directory of the configured file system. yarn.tags (none) String A comma-separated list of tags to apply to the Flink YARN application. yarn.taskmanager.node-label (none) String Specify YARN node label for the Flink TaskManagers, it will override the yarn.application.node-label for TaskManagers if both are set. Kubernetes # Key Default Type Description external-resource.\u0026lt;resource_name\u0026gt;.kubernetes.config-key (none) String If configured, Flink will add "resources.limits.\u0026lt;config-key\u0026gt;" and "resources.requests.\u0026lt;config-key\u0026gt;" to the main container of TaskExecutor and set the value to the value of external-resource.\u0026lt;resource_name\u0026gt;.amount. kubernetes.client.io-pool.size 4 Integer The size of the IO executor pool used by the Kubernetes client to execute blocking IO operations (e.g. start/stop TaskManager pods, update leader related ConfigMaps, etc.). Increasing the pool size allows to run more IO operations concurrently. kubernetes.client.user-agent "flink" String The user agent to be used for contacting with Kubernetes APIServer. kubernetes.cluster-id (none) String The cluster-id, which should be no more than 45 characters, is used for identifying a unique Flink cluster. The id must only contain lowercase alphanumeric characters and "-". The required format is [a-z]([-a-z0-9]*[a-z0-9]). If not set, the client will automatically generate it with a random ID. kubernetes.config.file (none) String The kubernetes config file will be used to create the client. The default is located at ~/.kube/config kubernetes.container.image The default value depends on the actually running version. In general it looks like "flink:\u0026lt;FLINK_VERSION\u0026gt;-scala_\u0026lt;SCALA_VERSION\u0026gt;" String Image to use for Flink containers. The specified image must be based upon the same Apache Flink and Scala versions as used by the application. Visit here for the official docker images provided by the Flink project. The Flink project also publishes docker images to apache/flink DockerHub repository. kubernetes.container.image.pull-policy IfNotPresent Enum
The Kubernetes container image pull policy. The default policy is IfNotPresent to avoid putting pressure to image repository.
Possible values:"IfNotPresent""Always""Never" kubernetes.container.image.pull-secrets (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the Kubernetes secrets used to access private image registries. kubernetes.context (none) String The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts. kubernetes.entry.path "/docker-entrypoint.sh" String The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container. kubernetes.env.secretKeyRef (none) List\u0026lt;Map\u0026gt; The user-specified secrets to set env variables in Flink container. The value should be in the form of env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key. kubernetes.flink.conf.dir "/opt/flink/conf" String The flink conf directory that will be mounted in pod. The flink-conf.yaml, log4j.properties, logback.xml in this path will be overwritten from config map. kubernetes.flink.log.dir (none) String The directory that logs of jobmanager and taskmanager be saved in the pod. The default value is \$FLINK_HOME/log. kubernetes.hadoop.conf.config-map.name (none) String Specify the name of an existing ConfigMap that contains custom Hadoop configuration to be mounted on the JobManager(s) and TaskManagers. kubernetes.hostnetwork.enabled false Boolean Whether to enable HostNetwork mode. The HostNetwork allows the pod could use the node network namespace instead of the individual pod network namespace. Please note that the JobManager service account should have the permission to update Kubernetes service. kubernetes.jobmanager.annotations (none) Map The user-specified annotations that are set to the JobManager pod. The value could be in the form of a1:v1,a2:v2 kubernetes.jobmanager.cpu 1.0 Double The number of cpu used by job manager kubernetes.jobmanager.cpu.limit-factor 1.0 Double The limit factor of cpu used by job manager. The resources limit cpu will be set to cpu * limit-factor. kubernetes.jobmanager.labels (none) Map The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. kubernetes.jobmanager.memory.limit-factor 1.0 Double The limit factor of memory used by job manager. The resources limit memory will be set to memory * limit-factor. kubernetes.jobmanager.node-selector (none) Map The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. kubernetes.jobmanager.owner.reference (none) List\u0026lt;Map\u0026gt; The user-specified Owner References to be set to the JobManager Deployment. When all the owner resources are deleted, the JobManager Deployment will be deleted automatically, which also deletes all the resources created by this Flink cluster. The value should be formatted as a semicolon-separated list of owner references, where each owner reference is a comma-separated list of \`key:value\` pairs. E.g., apiVersion:v1,blockOwnerDeletion:true,controller:true,kind:FlinkApplication,name:flink-app-name,uid:flink-app-uid;apiVersion:v1,kind:Deployment,name:deploy-name,uid:deploy-uid kubernetes.jobmanager.replicas 1 Integer Specify how many JobManager pods will be started simultaneously. Configure the value to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers. kubernetes.jobmanager.service-account "default" String Service account that is used by jobmanager within kubernetes cluster. The job manager uses this service account when requesting taskmanager pods from the API server. If not explicitly configured, config option 'kubernetes.service-account' will be used. kubernetes.jobmanager.tolerations (none) List\u0026lt;Map\u0026gt; The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 kubernetes.namespace "default" String The namespace that will be used for running the jobmanager and taskmanager pods. kubernetes.pod-template-file (none) String Specify a local file that contains the pod template definition. It will be used to initialize the jobmanager and taskmanager pod. The main container should be defined with name 'flink-main-container'. Notice that this can be overwritten by config options 'kubernetes.pod-template-file.jobmanager' and 'kubernetes.pod-template-file.taskmanager' for jobmanager and taskmanager respectively. kubernetes.pod-template-file.jobmanager (none) String Specify a local file that contains the jobmanager pod template definition. It will be used to initialize the jobmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used. kubernetes.pod-template-file.taskmanager (none) String Specify a local file that contains the taskmanager pod template definition. It will be used to initialize the taskmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used. kubernetes.rest-service.annotations (none) Map The user-specified annotations that are set to the rest Service. The value should be in the form of a1:v1,a2:v2 kubernetes.rest-service.exposed.node-port-address-type InternalIP Enum
The user-specified address type that is used for filtering node IPs when constructing a node port connection string. This option is only considered when 'kubernetes.rest-service.exposed.type' is set to 'NodePort'.
Possible values:"InternalIP""ExternalIP" kubernetes.rest-service.exposed.type ClusterIP Enum
The exposed type of the rest service. The exposed rest service could be used to access the Flink’s Web UI and REST endpoint.
Possible values:"ClusterIP""NodePort""LoadBalancer""Headless_ClusterIP" kubernetes.secrets (none) Map The user-specified secrets that will be mounted into Flink container. The value should be in the form of foo:/opt/secrets-foo,bar:/opt/secrets-bar. kubernetes.service-account "default" String Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively. kubernetes.taskmanager.annotations (none) Map The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2 kubernetes.taskmanager.cpu -1.0 Double The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager kubernetes.taskmanager.cpu.limit-factor 1.0 Double The limit factor of cpu used by task manager. The resources limit cpu will be set to cpu * limit-factor. kubernetes.taskmanager.labels (none) Map The labels to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test. kubernetes.taskmanager.memory.limit-factor 1.0 Double The limit factor of memory used by task manager. The resources limit memory will be set to memory * limit-factor. kubernetes.taskmanager.node-selector (none) Map The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd. kubernetes.taskmanager.service-account "default" String Service account that is used by taskmanager within kubernetes cluster. The task manager uses this service account when watching config maps on the API server to retrieve leader address of jobmanager and resourcemanager. If not explicitly configured, config option 'kubernetes.service-account' will be used. kubernetes.taskmanager.tolerations (none) List\u0026lt;Map\u0026gt; The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000 kubernetes.transactional-operation.max-retries 5 Integer Defines the number of Kubernetes transactional operation retries before the client gives up. For example, FlinkKubeClient#checkAndUpdateConfigMap. State Backends # Please refer to the State Backend Documentation for background on State Backends.
RocksDB State Backend # These are the options commonly needed to configure the RocksDB state backend. See the Advanced RocksDB Backend Section for options necessary for advanced low level configurations and trouble-shooting.
Key Default Type Description state.backend.rocksdb.memory.fixed-per-slot (none) MemorySize The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. If neither this option, nor the 'state.backend.rocksdb.memory.managed' optionare set, then each RocksDB column family state has its own memory caches (as controlled by the column family options). state.backend.rocksdb.memory.high-prio-pool-ratio 0.1 Double The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.memory.managed true Boolean If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped. state.backend.rocksdb.memory.partitioned-index-filters false Boolean With partitioning, the index/filter block of an SST file is partitioned into smaller blocks with an additional top-level index on them. When reading an index/filter, only top-level index is loaded into memory. The partitioned index/filter then uses the top-level index to load on demand into the block cache the partitions that are required to perform the index/filter query. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.memory.write-buffer-ratio 0.5 Double The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured. state.backend.rocksdb.timer-service.factory ROCKSDB Enum
This determines the factory for timer service state implementation.
Possible values:"HEAP": Heap-based"ROCKSDB": Implementation based on RocksDB Metrics # Please refer to the metrics system documentation for background on Flink\u0026rsquo;s metrics infrastructure.
Key Default Type Description metrics.fetcher.update-interval 10000 Long Update interval for the metric fetcher used by the web UI in milliseconds. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely. metrics.internal.query-service.port "0" String The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. metrics.internal.query-service.thread-priority 1 Integer The thread priority used for Flink's internal metric query service. The thread is created by Akka's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down. metrics.job.status.enable CURRENT_TIME List\u0026lt;Enum\u0026gt;
The selection of job status metrics that should be reported.
Possible values:"STATE": For a given state, return 1 if the job is currently in that state, otherwise return 0."CURRENT_TIME": For a given state, if the job is currently in that state, return the time since the job transitioned into that state, otherwise return 0."TOTAL_TIME": For a given state, return how much time the job has spent in that state in total. metrics.latency.granularity "operator" String Defines the granularity of latency metrics. Accepted values are:single - Track latency without differentiating between sources and subtasks.operator - Track latency while differentiating between sources, but not subtasks.subtask - Track latency while differentiating between sources and subtasks. metrics.latency.history-size 128 Integer Defines the number of measured latencies to maintain at each operator. metrics.latency.interval 0 Long Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster. metrics.reporter.\u0026lt;name\u0026gt;.\u0026lt;parameter\u0026gt; (none) String Configures the parameter \u0026lt;parameter\u0026gt; for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.factory.class (none) String The reporter factory class to use for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.filter.excludes List\u0026lt;String\u0026gt; The metrics that should be excluded for the reporter named \u0026lt;name\u0026gt;. The format is identical to filter.includes
metrics.reporter.\u0026lt;name\u0026gt;.filter.includes "*:*:*" List\u0026lt;String\u0026gt; The metrics that should be included for the reporter named \u0026lt;name\u0026gt;. Filters are specified as a list, with each filter following this format:
\u0026lt;scope\u0026gt;[:\u0026lt;name\u0026gt;[,\u0026lt;name\u0026gt;][:\u0026lt;type\u0026gt;[,\u0026lt;type\u0026gt;]]]
A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.
scope: Filters based on the logical scope.
Specified as a pattern where * matches any sequence of characters and . separates scope components.
For example:
"jobmanager.job" matches any job-related metrics on the JobManager,
"*.job" matches all job-related metrics and
"*.job.*" matches all metrics below the job-level (i.e., task/operator metrics etc.).
name: Filters based on the metric name.
Specified as a comma-separate list of patterns where * matches any sequence of characters.
For example, "*Records*,*Bytes*" matches any metrics where the name contains "Records" or "Bytes".
type: Filters based on the metric type. Specified as a comma-separated list of metric types: [counter, meter, gauge, histogram]Examples:"*:numRecords*" Matches metrics like numRecordsIn."*.job.task.operator:numRecords*" Matches metrics like numRecordsIn on the operator level."*.job.task.operator:numRecords*:meter" Matches meter metrics like numRecordsInPerSecond on the operator level."*:numRecords*,numBytes*:counter,meter" Matches all counter/meter metrics like or numRecordsInPerSecond. metrics.reporter.\u0026lt;name\u0026gt;.interval 10 s Duration The reporter interval to use for the reporter named \u0026lt;name\u0026gt;. Only applicable to push-based reporters. metrics.reporter.\u0026lt;name\u0026gt;.scope.delimiter "." String The delimiter used to assemble the metric identifier for the reporter named \u0026lt;name\u0026gt;. metrics.reporter.\u0026lt;name\u0026gt;.scope.variables.additional Map The map of additional variables that should be included for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. metrics.reporter.\u0026lt;name\u0026gt;.scope.variables.excludes "." String The set of variables that should be excluded for the reporter named \u0026lt;name\u0026gt;. Only applicable to tag-based reporters. metrics.reporters (none) String An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started. metrics.scope.delimiter "." String Delimiter used to assemble the metric identifier. metrics.scope.jm "\u0026lt;host\u0026gt;.jobmanager" String Defines the scope format string that is applied to all metrics scoped to a JobManager. Only effective when a identifier-based reporter is configured. metrics.scope.jm.job "\u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a job on a JobManager. Only effective when a identifier-based reporter is configured metrics.scope.operator "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to an operator. Only effective when a identifier-based reporter is configured metrics.scope.task "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;subtask_index\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a task. Only effective when a identifier-based reporter is configured metrics.scope.tm "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a TaskManager. Only effective when a identifier-based reporter is configured metrics.scope.tm.job "\u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;" String Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager. Only effective when a identifier-based reporter is configured metrics.system-resource false Boolean Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage. metrics.system-resource-probing-interval 5000 Long Interval between probing of system resource metrics specified in milliseconds. Has an effect only when 'metrics.system-resource' is enabled. RocksDB Native Metrics # Flink can report metrics from RocksDB\u0026rsquo;s native code, for applications using the RocksDB state backend. The metrics here are scoped to the operators with unsigned longs and have two kinds of types：
RocksDB property-based metrics, which is broken down by column family, e.g. number of currently running compactions of one specific column family. RocksDB statistics-based metrics, which holds at the database level, e.g. total block cache hit count within the DB. Enabling RocksDB\u0026rsquo;s native metrics may cause degraded performance and should be set carefully. Key Default Type Description state.backend.rocksdb.metrics.actual-delayed-write-rate false Boolean Monitor the current actual delayed write rate. 0 means no delay. state.backend.rocksdb.metrics.background-errors false Boolean Monitor the number of background errors in RocksDB. state.backend.rocksdb.metrics.block-cache-capacity false Boolean Monitor block cache capacity. state.backend.rocksdb.metrics.block-cache-hit false Boolean Monitor the total count of block cache hit in RocksDB (BLOCK_CACHE_HIT == BLOCK_CACHE_INDEX_HIT + BLOCK_CACHE_FILTER_HIT + BLOCK_CACHE_DATA_HIT). state.backend.rocksdb.metrics.block-cache-miss false Boolean Monitor the total count of block cache misses in RocksDB (BLOCK_CACHE_MISS == BLOCK_CACHE_INDEX_MISS + BLOCK_CACHE_FILTER_MISS + BLOCK_CACHE_DATA_MISS). state.backend.rocksdb.metrics.block-cache-pinned-usage false Boolean Monitor the memory size for the entries being pinned in block cache. state.backend.rocksdb.metrics.block-cache-usage false Boolean Monitor the memory size for the entries residing in block cache. state.backend.rocksdb.metrics.bytes-read false Boolean Monitor the number of uncompressed bytes read (from memtables/cache/sst) from Get() operation in RocksDB. state.backend.rocksdb.metrics.bytes-written false Boolean Monitor the number of uncompressed bytes written by DB::{Put(), Delete(), Merge(), Write()} operations, which does not include the compaction written bytes, in RocksDB. state.backend.rocksdb.metrics.column-family-as-variable false Boolean Whether to expose the column family as a variable for RocksDB property based metrics. state.backend.rocksdb.metrics.compaction-pending false Boolean Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise. state.backend.rocksdb.metrics.compaction-read-bytes false Boolean Monitor the bytes read during compaction in RocksDB. state.backend.rocksdb.metrics.compaction-write-bytes false Boolean Monitor the bytes written during compaction in RocksDB. state.backend.rocksdb.metrics.cur-size-active-mem-table false Boolean Monitor the approximate size of the active memtable in bytes. state.backend.rocksdb.metrics.cur-size-all-mem-tables false Boolean Monitor the approximate size of the active and unflushed immutable memtables in bytes. state.backend.rocksdb.metrics.estimate-live-data-size false Boolean Estimate of the amount of live data in bytes (usually smaller than sst files size due to space amplification). state.backend.rocksdb.metrics.estimate-num-keys false Boolean Estimate the number of keys in RocksDB. state.backend.rocksdb.metrics.estimate-pending-compaction-bytes false Boolean Estimated total number of bytes compaction needs to rewrite to get all levels down to under target size. Not valid for other compactions than level-based. state.backend.rocksdb.metrics.estimate-table-readers-mem false Boolean Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g.,filter and index blocks) in bytes. state.backend.rocksdb.metrics.is-write-stopped false Boolean Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise. state.backend.rocksdb.metrics.iter-bytes-read false Boolean Monitor the number of uncompressed bytes read (from memtables/cache/sst) from an iterator operation in RocksDB. state.backend.rocksdb.metrics.live-sst-files-size false Boolean Monitor the total size (bytes) of all SST files belonging to the latest version.WARNING: may slow down online queries if there are too many files. state.backend.rocksdb.metrics.mem-table-flush-pending false Boolean Monitor the number of pending memtable flushes in RocksDB. state.backend.rocksdb.metrics.num-deletes-active-mem-table false Boolean Monitor the total number of delete entries in the active memtable. state.backend.rocksdb.metrics.num-deletes-imm-mem-tables false Boolean Monitor the total number of delete entries in the unflushed immutable memtables. state.backend.rocksdb.metrics.num-entries-active-mem-table false Boolean Monitor the total number of entries in the active memtable. state.backend.rocksdb.metrics.num-entries-imm-mem-tables false Boolean Monitor the total number of entries in the unflushed immutable memtables. state.backend.rocksdb.metrics.num-immutable-mem-table false Boolean Monitor the number of immutable memtables in RocksDB. state.backend.rocksdb.metrics.num-live-versions false Boolean Monitor number of live versions. Version is an internal data structure. See RocksDB file version_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions. state.backend.rocksdb.metrics.num-running-compactions false Boolean Monitor the number of currently running compactions. state.backend.rocksdb.metrics.num-running-flushes false Boolean Monitor the number of currently running flushes. state.backend.rocksdb.metrics.num-snapshots false Boolean Monitor the number of unreleased snapshots of the database. state.backend.rocksdb.metrics.size-all-mem-tables false Boolean Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes. state.backend.rocksdb.metrics.stall-micros false Boolean Monitor the duration of writer requiring to wait for compaction or flush to finish in RocksDB. state.backend.rocksdb.metrics.total-sst-files-size false Boolean Monitor the total size (bytes) of all SST files of all versions.WARNING: may slow down online queries if there are too many files. History Server # The history server keeps the information of completed jobs (graphs, runtimes, statistics). To enable it, you have to enable \u0026ldquo;job archiving\u0026rdquo; in the JobManager (jobmanager.archive.fs.dir).
See the History Server Docs for details.
Key Default Type Description historyserver.archive.clean-expired-jobs false Boolean Whether HistoryServer should cleanup jobs that are no longer present \`historyserver.archive.fs.dir\`. historyserver.archive.fs.dir (none) String Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via \`jobmanager.archive.fs.dir\`. historyserver.archive.fs.refresh-interval 10000 Long Interval in milliseconds for refreshing the archived job directories. historyserver.archive.retained-jobs -1 Integer The maximum number of jobs to retain in each archive directory defined by \`historyserver.archive.fs.dir\`. If set to \`-1\`(default), there is no limit to the number of archives. If set to \`0\` or less than \`-1\` HistoryServer will throw an IllegalConfigurationException. historyserver.log.jobmanager.url-pattern (none) String Pattern of the log URL of JobManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`\u0026lt;jobid\u0026gt;\`, to the id of job. Only http / https schemes are supported. historyserver.log.taskmanager.url-pattern (none) String Pattern of the log URL of TaskManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, \`\u0026lt;jobid\u0026gt;\` and \`\u0026lt;tmid\u0026gt;\`, to the id of job and TaskManager respectively. Only http / https schemes are supported. historyserver.web.address (none) String Address of the HistoryServer's web interface. historyserver.web.port 8082 Integer Port of the HistoryServers's web interface. historyserver.web.refresh-interval 10000 Long The refresh interval for the HistoryServer web-frontend in milliseconds. historyserver.web.ssl.enabled false Boolean Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true. historyserver.web.tmpdir (none) String Local directory that is used by the history server REST API for temporary files. Experimental # Options for experimental features in Flink.
Queryable State # Queryable State is an experimental features that gives lets you access Flink\u0026rsquo;s internal state like a key/value store. See the Queryable State Docs for details.
Key Default Type Description queryable-state.client.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state client. queryable-state.enable false Boolean Option whether the queryable state proxy and server should be enabled where possible and configurable. queryable-state.proxy.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state proxy. queryable-state.proxy.ports "9069" String The port range of the queryable state proxy. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". queryable-state.proxy.query-threads 0 Integer Number of query Threads for queryable state proxy. Uses the number of slots if set to 0. queryable-state.server.network-threads 0 Integer Number of network (Netty's event loop) Threads for queryable state server. queryable-state.server.ports "9067" String The port range of the queryable state server. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". queryable-state.server.query-threads 0 Integer Number of query Threads for queryable state server. Uses the number of slots if set to 0. Client # Key Default Type Description client.retry-period 2 s Duration The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec). client.timeout 1 min Duration Timeout on the client side. Execution # Key Default Type Description execution.allow-client-job-configurations true Boolean Determines whether configurations in the user program are allowed. Depending on your deployment mode failing the job might have different affects. Either your client that is trying to submit the job to an external cluster (session cluster deployment) throws the exception or the Job manager (application mode deployment). execution.attached false Boolean Specifies if the pipeline is submitted in attached or detached mode. execution.job-listeners (none) List\u0026lt;String\u0026gt; Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments. execution.shutdown-on-application-finish true Boolean Whether a Flink Application cluster should shut down automatically after its application finishes (either successfully or as result of a failure). Has no effect for other deployment modes. execution.shutdown-on-attached-exit false Boolean If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. execution.submit-failed-job-on-application-error false Boolean If a failed job should be submitted (in the application mode) when there is an error in the application driver before an actual job submission. This is intended for providing a clean way of reporting failures back to the user and is especially useful in combination with 'execution.shutdown-on-application-finish'. This option only works when the single job submission is enforced ('high-availability' is enabled). Please note that this is an experimental option and may be changed in the future. execution.target (none) String The deployment target for the execution. This can take one of the following values when calling bin/flink run:remotelocalyarn-per-job (deprecated)yarn-sessionkubernetes-sessionAnd one of the following values when calling bin/flink run-application:yarn-applicationkubernetes-application Key Default Type Description execution.savepoint-restore-mode NO_CLAIM Enum
Describes the mode how Flink should restore from the given savepoint or retained checkpoint.
Possible values:"CLAIM": Flink will take ownership of the given snapshot. It will clean the snapshot once it is subsumed by newer ones."NO_CLAIM": Flink will not claim ownership of the snapshot files. However it will make sure it does not depend on any artefacts from the restored snapshot. In order to do that, Flink will take the first checkpoint as a full one, which means it might reupload/duplicate files that are part of the restored checkpoint."LEGACY": This is the mode in which Flink worked so far. It will not claim ownership of the snapshot and will not delete the files. However, it can directly depend on the existence of the files of the restored checkpoint. It might not be safe to delete checkpoints that were restored in legacy mode execution.savepoint.ignore-unclaimed-state false Boolean Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered. execution.savepoint.path (none) String Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). Key Default Type Description execution.batch-shuffle-mode ALL_EXCHANGES_BLOCKING Enum
Defines how data is exchanged between tasks in batch 'execution.runtime-mode' if the shuffling behavior has not been set explicitly for an individual exchange.
With pipelined exchanges, upstream and downstream tasks run simultaneously. In order to achieve lower latency, a result record is immediately sent to and processed by the downstream task. Thus, the receiver back-pressures the sender. The streaming mode always uses this exchange.
With blocking exchanges, upstream and downstream tasks run in stages. Records are persisted to some storage between stages. Downstream tasks then fetch these records after the upstream tasks finished. Such an exchange reduces the resources required to execute the job as it does not need to run upstream and downstream tasks simultaneously.
With hybrid exchanges (experimental), downstream tasks can run anytime as long as upstream tasks start running. When given sufficient resources, it can reduce the overall job execution time by running tasks simultaneously. Otherwise, it also allows jobs to be executed with very little resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies.
Possible values:"ALL_EXCHANGES_PIPELINED": Upstream and downstream tasks run simultaneously. This leads to lower latency and more evenly distributed (but higher) resource usage across tasks."ALL_EXCHANGES_BLOCKING": Upstream and downstream tasks run subsequently. This reduces the resource usage as downstream tasks are started after upstream tasks finished."ALL_EXCHANGES_HYBRID_FULL": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will spill all data to disk to support re-consume."ALL_EXCHANGES_HYBRID_SELECTIVE": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will selective spilling data to reduce disk writes as much as possible. execution.buffer-timeout 100 ms Duration The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes:A positive value triggers flushing periodically by that interval0 triggers flushing after every record thus minimizing latency-1 ms triggers flushing only when the output buffer is full thus maximizing throughput execution.checkpointing.snapshot-compression false Boolean Tells if we should use compression for the state snapshot data or not execution.runtime-mode STREAMING Enum
Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics.
Possible values:"STREAMING""BATCH""AUTOMATIC" Pipeline # Key Default Type Description pipeline.auto-generate-uids true Boolean When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications.
It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state. pipeline.auto-type-registration true Boolean Controls whether Flink is automatically registering all types in the user programs with Kryo. pipeline.auto-watermark-interval 0 ms Duration The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing. pipeline.cached-files (none) List\u0026lt;String\u0026gt; Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.
Example:
name:file1,path:\`file:///tmp/file1\`;name:file2,path:\`hdfs:///tmp/file2\` pipeline.classpaths (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs. pipeline.closure-cleaner-level RECURSIVE Enum
Configures the mode in which the closure cleaner works.
Possible values:"NONE": Disables the closure cleaner completely."TOP_LEVEL": Cleans only the top-level class without recursing into fields."RECURSIVE": Cleans all fields recursively. pipeline.default-kryo-serializers (none) List\u0026lt;String\u0026gt; Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializers
Example:
class:org.example.ExampleClass,serializer:org.example.ExampleSerializer1; class:org.example.ExampleClass2,serializer:org.example.ExampleSerializer2 pipeline.force-avro false Boolean Forces Flink to use the Apache Avro serializer for POJOs.
Important: Make sure to include the flink-avro module. pipeline.force-kryo false Boolean If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO. pipeline.generic-types true Boolean If the use of generic types is disabled, Flink will throw an UnsupportedOperationException whenever it encounters a data type that would go through Kryo for serialization.
Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used.
We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail. pipeline.global-job-parameters (none) Map Register a custom, serializable user configuration object. The configuration can be accessed in operators pipeline.jars (none) List\u0026lt;String\u0026gt; A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths. pipeline.max-parallelism -1 Integer The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state. pipeline.name (none) String The job name used for printing and logging. pipeline.object-reuse false Boolean When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour. pipeline.operator-chaining true Boolean Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization. pipeline.registered-kryo-types (none) List\u0026lt;String\u0026gt; Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written. pipeline.registered-pojo-types (none) List\u0026lt;String\u0026gt; Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written. pipeline.vertex-description-mode TREE Enum
The mode how we organize description of a job vertex.
Possible values:"TREE""CASCADING" pipeline.vertex-name-include-index-prefix false Boolean Whether name of vertex includes topological index or not. When it is true, the name will have a prefix of index of the vertex, like '[vertex-0]Source: source'. It is false by default Key Default Type Description pipeline.time-characteristic ProcessingTime Enum
The time characteristic for all created streams, e.g., processingtime, event time, or ingestion time.
If you set the characteristic to IngestionTime or EventTime this will set a default watermark update interval of 200 ms. If this is not applicable for your application you should change it using pipeline.auto-watermark-interval.
Possible values:"ProcessingTime""IngestionTime""EventTime" Checkpointing # Key Default Type Description execution.checkpointing.aligned-checkpoint-timeout 0 ms Duration Only relevant if execution.checkpointing.unaligned is enabled.
If timeout is 0, checkpoints will always start unaligned.
If timeout has a positive value, checkpoints will start aligned. If during checkpointing, checkpoint start delay exceeds this timeout, alignment will timeout and checkpoint barrier will start working as unaligned checkpoint. execution.checkpointing.checkpoints-after-tasks-finish.enabled true Boolean Feature toggle for enabling checkpointing even if some of tasks have finished. Before you enable it, please take a look at the important considerations execution.checkpointing.externalized-checkpoint-retention NO_EXTERNALIZED_CHECKPOINTS Enum
Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status JobStatus#FAILED or JobStatus#SUSPENDED). In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state.
The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status JobStatus#CANCELED).
The target directory for externalized checkpoints is configured via state.checkpoints.dir.
Possible values:"DELETE_ON_CANCELLATION": Checkpoint state is only kept when the owning job fails. It is deleted if the job is cancelled."RETAIN_ON_CANCELLATION": Checkpoint state is kept when the owning job is cancelled or fails."NO_EXTERNALIZED_CHECKPOINTS": Externalized checkpoints are disabled. execution.checkpointing.interval (none) Duration Gets the interval in which checkpoints are periodically scheduled.
This setting defines the base interval. Checkpoint triggering may be delayed by the settings execution.checkpointing.max-concurrent-checkpoints and execution.checkpointing.min-pause execution.checkpointing.max-concurrent-checkpoints 1 Integer The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire. execution.checkpointing.min-pause 0 ms Duration The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see execution.checkpointing.max-concurrent-checkpoints).
If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all. execution.checkpointing.mode EXACTLY_ONCE Enum
The checkpointing mode (exactly-once vs. at-least-once).
Possible values:"EXACTLY_ONCE""AT_LEAST_ONCE" execution.checkpointing.recover-without-channel-state.checkpoint-id -1 Long Checkpoint id for which in-flight data should be ignored in case of the recovery from this checkpoint.
It is better to keep this value empty until there is explicit needs to restore from the specific checkpoint without in-flight data.
execution.checkpointing.timeout 10 min Duration The maximum time that a checkpoint may take before being discarded. execution.checkpointing.tolerable-failed-checkpoints (none) Integer The tolerable checkpoint consecutive failure number. If set to 0, that means we do not tolerance any checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored. execution.checkpointing.unaligned false Boolean Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure.
Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.
Unaligned checkpoints can only be enabled if execution.checkpointing.mode is EXACTLY_ONCE and if execution.checkpointing.max-concurrent-checkpoints is 1 execution.checkpointing.unaligned.forced false Boolean Forces unaligned checkpoints, particularly allowing them for iterative jobs. Debugging \u0026amp; Expert Tuning # The options below here are meant for expert users and for fixing/debugging problems. Most setups should not need to configure these options. Class Loading # Flink dynamically loads the code for jobs submitted to a session cluster. In addition, Flink tries to hide many dependencies in the classpath from the application. This helps to reduce dependency conflicts between the application code and the dependencies in the classpath.
Please refer to the Debugging Classloading Docs for details.
Key Default Type Description classloader.check-leaked-classloader true Boolean Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running. classloader.fail-on-metaspace-oom-error true Boolean Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class. classloader.parent-first-patterns.additional List\u0026lt;String\u0026gt; A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to "classloader.parent-first-patterns.default". classloader.parent-first-patterns.default "java.";"scala.";"org.apache.flink.";"com.esotericsoftware.kryo";"org.apache.hadoop.";"javax.annotation.";"org.xml";"javax.xml";"org.apache.xerces";"org.w3c";"org.rocksdb.";"org.slf4j";"org.apache.log4j";"org.apache.logging";"org.apache.commons.logging";"ch.qos.logback" List\u0026lt;String\u0026gt; A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use "classloader.parent-first-patterns.additional" instead. classloader.resolve-order "child-first" String Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar ("child-first") or the application classpath ("parent-first"). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively). Advanced Options for the debugging # Key Default Type Description jmx.server.port (none) String The port range for the JMX server to start the registry. The port config can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". This option overrides metrics.reporter.*.port option. Advanced State Backends Options # Key Default Type Description state.storage.fs.memory-threshold 20 kb MemorySize The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. state.storage.fs.write-buffer-size 4096 Integer The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.storage.fs.memory-threshold'. State Backends Latency Tracking Options # Key Default Type Description state.backend.latency-track.history-size 128 Integer Defines the number of measured latencies to maintain at each state access operation. state.backend.latency-track.keyed-state-enabled false Boolean Whether to track latency of keyed state operations, e.g value state put/get/clear. state.backend.latency-track.sample-interval 100 Integer The sample interval of latency track once 'state.backend.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests. state.backend.latency-track.state-name-as-variable true Boolean Whether to expose state name as a variable if tracking latency. Advanced RocksDB State Backends Options # Advanced options to tune RocksDB and RocksDB checkpoints.
Key Default Type Description state.backend.rocksdb.checkpoint.transfer.thread.num 4 Integer The number of threads (per stateful operator) used to transfer (download and upload) files in RocksDBStateBackend. state.backend.rocksdb.localdir (none) String The local directory (on the TaskManager) where RocksDB puts its files. Per default, it will be \u0026lt;WORKING_DIR\u0026gt;/tmp. See process.taskmanager.working-dir for more details. state.backend.rocksdb.options-factory (none) String The options factory class for users to add customized options in DBOptions and ColumnFamilyOptions for RocksDB. If set, the RocksDB state backend will load the class and apply configs to DBOptions and ColumnFamilyOptions after loading ones from 'RocksDBConfigurableOptions' and pre-defined options. state.backend.rocksdb.predefined-options "DEFAULT" String The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING_DISK_OPTIMIZED, SPINNING_DISK_OPTIMIZED_HIGH_MEM or FLASH_SSD_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones. State Changelog Options # Please refer to State Backends for information on using State Changelog. The feature is in experimental status. Key Default Type Description state.backend.changelog.enabled false Boolean Whether to enable state backend to write state changes to StateChangelog. If this config is not set explicitly, it means no preference for enabling the change log, and the value in lower config level will take effect. The default value 'false' here means if no value set (job or cluster), the change log will not be enabled. state.backend.changelog.max-failures-allowed 3 Integer Max number of consecutive materialization failures allowed. state.backend.changelog.periodic-materialize.interval 10 min Duration Defines the interval in milliseconds to perform periodic materialization for state backend. The periodic materialization will be disabled when the value is negative state.backend.changelog.storage "memory" String The storage to be used to store state changelog.
The implementation can be specified via their shortcut name.
The list of recognized shortcut names currently includes 'memory' and 'filesystem'. FileSystem-based Changelog options # These settings take effect when the state.backend.changelog.storage is set to filesystem (see above). Key Default Type Description dstl.dfs.base-path (none) String Base path to store changelog files. dstl.dfs.batch.persist-delay 10 ms Duration Delay before persisting changelog after receiving persist request (on checkpoint). Minimizes the number of files and requests if multiple operators (backends) or sub-tasks are using the same store. Correspondingly increases checkpoint time (async phase). dstl.dfs.batch.persist-size-threshold 10 mb MemorySize Size threshold for state changes that were requested to be persisted but are waiting for dstl.dfs.batch.persist-delay (from all operators). . Once reached, accumulated changes are persisted immediately. This is different from dstl.dfs.preemptive-persist-threshold as it happens AFTER the checkpoint and potentially for state changes of multiple operators. Must not exceed in-flight data limit (see below) dstl.dfs.compression.enabled false Boolean Whether to enable compression when serializing changelog. dstl.dfs.discard.num-threads 1 Integer Number of threads to use to discard changelog (e.g. pre-emptively uploaded unused state). dstl.dfs.download.local-cache.idle-timeout-ms 10 min Duration Maximum idle time for cache files of distributed changelog file, after which the cache files will be deleted. dstl.dfs.preemptive-persist-threshold 5 mb MemorySize Size threshold for state changes of a single operator beyond which they are persisted pre-emptively without waiting for a checkpoint. Improves checkpointing time by allowing quasi-continuous uploading of state changes (as opposed to uploading all accumulated changes on checkpoint). dstl.dfs.upload.buffer-size 1 mb MemorySize Buffer size used when uploading change sets dstl.dfs.upload.max-attempts 3 Integer Maximum number of attempts (including the initial one) to perform a particular upload. Only takes effect if dstl.dfs.upload.retry-policy is fixed. dstl.dfs.upload.max-in-flight 100 mb MemorySize Max amount of data allowed to be in-flight. Upon reaching this limit the task will be back-pressured. I.e., snapshotting will block; normal processing will block if dstl.dfs.preemptive-persist-threshold is set and reached. The limit is applied to the total size of in-flight changes if multiple operators/backends are using the same changelog storage. Must be greater than or equal to dstl.dfs.batch.persist-size-threshold dstl.dfs.upload.next-attempt-delay 500 ms Duration Delay before the next attempt (if the failure was not caused by a timeout). dstl.dfs.upload.num-threads 5 Integer Number of threads to use for upload. dstl.dfs.upload.retry-policy "fixed" String Retry policy for the failed uploads (in particular, timed out). Valid values: none, fixed. dstl.dfs.upload.timeout 1 s Duration Time threshold beyond which an upload is considered timed out. If a new attempt is made but this upload succeeds earlier then this upload result will be used. May improve upload times if tail latencies of upload requests are significantly high. Only takes effect if dstl.dfs.upload.retry-policy is fixed. Please note that timeout * max_attempts should be less than execution.checkpointing.timeout RocksDB Configurable Options
These options give fine-grained control over the behavior and resources of ColumnFamilies. With the introduction of state.backend.rocksdb.memory.managed and state.backend.rocksdb.memory.fixed-per-slot (Apache Flink 1.10), it should be only necessary to use the options here for advanced performance tuning. These options here can also be specified in the application program via RocksDBStateBackend.setRocksDBOptions(RocksDBOptionsFactory).
Key Default Type Description state.backend.rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'. state.backend.rocksdb.block.cache-size 8 mb MemorySize The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'. state.backend.rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'. state.backend.rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0. state.backend.rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'. state.backend.rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'. state.backend.rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'. state.backend.rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc. state.backend.rocksdb.compaction.style LEVEL Enum
The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.
Possible values:"LEVEL""UNIVERSAL""FIFO""NONE" state.backend.rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'. state.backend.rocksdb.log.dir (none) String The directory for RocksDB's information logging files. If empty (Flink default setting), log files will be in the same directory as the Flink log. If non-empty, this directory will be used and the data directory's absolute path will be used as the prefix of the log file name. If setting this option as a non-existing location, e.g '/dev/null', RocksDB will then create the log under its own database folder as before. state.backend.rocksdb.log.file-num 4 Integer The maximum number of files RocksDB should keep for information logging (Default setting: 4). state.backend.rocksdb.log.level INFO_LEVEL Enum
The specified information logging level for RocksDB. If unset, Flink will use INFO_LEVEL.
Note: RocksDB info logs will not be written to the TaskManager logs and there is no rolling strategy, unless you configure state.backend.rocksdb.log.dir, state.backend.rocksdb.log.max-file-size, and state.backend.rocksdb.log.file-num accordingly. Without a rolling strategy, long-running tasks may lead to uncontrolled disk space usage if configured with increased log levels!
There is no need to modify the RocksDB log level, unless for troubleshooting RocksDB.
Possible values:"DEBUG_LEVEL""INFO_LEVEL""WARN_LEVEL""ERROR_LEVEL""FATAL_LEVEL""HEADER_LEVEL""NUM_INFO_LOG_LEVELS" state.backend.rocksdb.log.max-file-size 25 mb MemorySize The maximum size of RocksDB's file used for information logging. If the log files becomes larger than this, a new file will be created. If 0, all logs will be written to one log file. The default maximum file size is '25MB'. state.backend.rocksdb.restore-overlap-fraction-threshold 0.0 Double The threshold of overlap fraction between the handle's key-group range and target key-group range. When restore base DB, only the handle which overlap fraction greater than or equal to threshold has a chance to be an initial handle. The default value is 0.0, there is always a handle will be selected for initialization. state.backend.rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'. state.backend.rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default. state.backend.rocksdb.write-batch-size 2 mb MemorySize The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0. state.backend.rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'. state.backend.rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'. state.backend.rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'. Advanced Fault Tolerance Options # These parameters can help with problems related to failover and to components erroneously considering each other as failed.
Key Default Type Description cluster.io-pool.size (none) Integer The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently. cluster.registration.error-delay 10000 Long The pause made after an registration attempt caused an exception (other than timeout) in milliseconds. cluster.registration.initial-timeout 100 Long Initial registration timeout between cluster components in milliseconds. cluster.registration.max-timeout 30000 Long Maximum registration timeout between cluster components in milliseconds. cluster.registration.refused-registration-delay 30000 Long The pause made after the registration attempt was refused in milliseconds. cluster.services.shutdown-timeout 30000 Long The shutdown timeout for cluster services like executors in milliseconds. heartbeat.interval 10000 Long Time interval between heartbeat RPC requests from the sender to the receiver side. heartbeat.rpc-failure-threshold 2 Integer The number of consecutive failed heartbeat RPCs until a heartbeat target is marked as unreachable. Failed heartbeat RPCs can be used to detect dead targets faster because they no longer receive the RPCs. The detection time is heartbeat.interval * heartbeat.rpc-failure-threshold. In environments with a flaky network, setting this value too low can produce false positives. In this case, we recommend to increase this value, but not higher than heartbeat.timeout / heartbeat.interval. The mechanism can be disabled by setting this option to -1 heartbeat.timeout 50000 Long Timeout for requesting and receiving heartbeats for both sender and receiver sides. jobmanager.execution.failover-strategy "region" String This option specifies how the job computation recovers from task failures. Accepted values are:'full': Restarts all tasks to recover the job.'region': Restarts all tasks that could be affected by the task failure. More details can be found here. Advanced Cluster Options # Key Default Type Description cluster.intercept-user-system-exit DISABLED Enum
Flag to check user code exiting system by terminating JVM (e.g., System.exit()). Note that this configuration option can interfere with cluster.processes.halt-on-fatal-error: In intercepted user-code, a call to System.exit() will not cause the JVM to halt, when THROW is configured.
Possible values:"DISABLED": Flink is not monitoring or intercepting calls to System.exit()"LOG": Log exit attempt with stack trace but still allowing exit to be performed"THROW": Throw exception when exit is attempted disallowing JVM termination cluster.processes.halt-on-fatal-error false Boolean Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See FLINK-16510 for details. cluster.thread-dump.stacktrace-max-depth 8 Integer The maximum stacktrace depth of TaskManager and JobManager's thread dump web-frontend displayed. cluster.uncaught-exception-handling LOG Enum
Defines whether cluster will handle any uncaught exceptions by just logging them (LOG mode), or by failing job (FAIL mode)
Possible values:"LOG""FAIL" process.jobmanager.working-dir (none) String Working directory for Flink JobManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir. process.taskmanager.working-dir (none) String Working directory for Flink TaskManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir. process.working-dir io.tmp.dirs String Local working directory for Flink processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to a randomly picked temporary directory defined via io.tmp.dirs. Advanced JobManager Options # Key Default Type Description jobmanager.future-pool.size (none) Integer The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.io-pool.size (none) Integer The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. Advanced Scheduling Options # These parameters can help with fine-tuning scheduling for specific situations.
Key Default Type Description cluster.evenly-spread-out-slots false Boolean Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors. cluster.fine-grained-resource-management.enabled false Boolean Defines whether the cluster uses fine-grained resource management. fine-grained.shuffle-mode.all-blocking false Boolean Whether to convert all PIPELINE edges to BLOCKING when apply fine-grained resource management in batch jobs. jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task 1 gb MemorySize The average size of data volume to expect each task instance to process if jobmanager.scheduler has been set to AdaptiveBatch. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs or the decided parallelism reaches the jobmanager.adaptive-batch-scheduler.max-parallelism (due to too much data), the data actually processed by some tasks may far exceed this value. jobmanager.adaptive-batch-scheduler.default-source-parallelism 1 Integer The default parallelism of source vertices if jobmanager.scheduler has been set to AdaptiveBatch jobmanager.adaptive-batch-scheduler.max-parallelism 128 Integer The upper bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded down to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.min-parallelism 1 Integer The lower bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded up to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 1 min Duration Controls how long an detected slow node should be blocked for. jobmanager.adaptive-batch-scheduler.speculative.enabled false Boolean Controls whether to enable speculative execution. jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions 2 Integer Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones. jobmanager.adaptive-scheduler.min-parallelism-increase 1 Integer Configure the minimum increase in parallelism for a job to scale up. jobmanager.adaptive-scheduler.resource-stabilization-timeout 10 s Duration The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.
If scheduler-mode is configured to REACTIVE, this configuration value will default to 0, so that jobs are starting immediately with the available resources. jobmanager.adaptive-scheduler.resource-wait-timeout 5 min Duration The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.
Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).
Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.
If scheduler-mode is configured to REACTIVE, this configuration value will default to a negative value to disable the resource timeout. jobmanager.scheduler Default Enum
Determines which scheduler implementation is used to schedule tasks. Accepted values are:'Default': Default scheduler'Adaptive': Adaptive scheduler. More details can be found here.'AdaptiveBatch': Adaptive batch scheduler. More details can be found here.
Possible values:"Default""Adaptive""AdaptiveBatch" scheduler-mode (none) Enum
Determines the mode of the scheduler. Note that scheduler-mode=REACTIVE is only supported by standalone application deployments, not by active resource managers (YARN, Kubernetes) or session clusters.
Possible values:"REACTIVE" slot.idle.timeout 50000 Long The timeout in milliseconds for a idle slot in Slot Pool. slot.request.timeout 300000 Long The timeout in milliseconds for requesting a slot from Slot Pool. slotmanager.max-total-resource.cpu (none) Double Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.max-total-resource.memory (none) MemorySize Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.number-of-slots.max 2147483647 Integer Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. slow-task-detector.check-interval 1 s Duration The interval to check slow tasks. slow-task-detector.execution-time.baseline-lower-bound 1 min Duration The lower bound of slow task detection baseline. slow-task-detector.execution-time.baseline-multiplier 1.5 Double The multiplier to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. slow-task-detector.execution-time.baseline-ratio 0.75 Double The finished execution ratio threshold to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline. Advanced High-availability Options # Key Default Type Description high-availability.jobmanager.port "0" String The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery. Advanced High-availability ZooKeeper Options # Key Default Type Description high-availability.zookeeper.client.acl "open" String Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos). high-availability.zookeeper.client.connection-timeout 15000 Integer Defines the connection timeout for ZooKeeper in ms. high-availability.zookeeper.client.max-retry-attempts 3 Integer Defines the number of connection retries before the client gives up. high-availability.zookeeper.client.retry-wait 5000 Integer Defines the pause between consecutive retries in ms. high-availability.zookeeper.client.session-timeout 60000 Integer Defines the session timeout for the ZooKeeper session in ms. high-availability.zookeeper.client.tolerate-suspended-connections false Boolean Defines whether a suspended ZooKeeper connection will be treated as an error that causes the leader information to be invalidated or not. In case you set this option to true, Flink will wait until a ZooKeeper connection is marked as lost before it revokes the leadership of components. This has the effect that Flink is more resilient against temporary connection instabilities at the cost of running more likely into timing issues with ZooKeeper. high-availability.zookeeper.path.jobgraphs "/jobgraphs" String ZooKeeper root path (ZNode) for job graphs high-availability.zookeeper.path.running-registry "/running_job_registry/" String Advanced High-availability Kubernetes Options # Key Default Type Description high-availability.kubernetes.leader-election.lease-duration 15 s Duration Define the lease duration for the Kubernetes leader election. The leader will continuously renew its lease time to indicate its existence. And the followers will do a lease checking against the current time. "renewTime + leaseDuration \u0026gt; now" means the leader is alive. high-availability.kubernetes.leader-election.renew-deadline 15 s Duration Defines the deadline duration when the leader tries to renew the lease. The leader will give up its leadership if it cannot successfully renew the lease in the given time. high-availability.kubernetes.leader-election.retry-period 5 s Duration Defines the pause duration between consecutive retries. All the contenders, including the current leader and all other followers, periodically try to acquire/renew the leadership if possible at this interval. Advanced SSL Security Options # Key Default Type Description security.ssl.internal.close-notify-flush-timeout -1 Integer The timeout (in ms) for flushing the \`close_notify\` that was triggered by closing a channel. If the \`close_notify\` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default) security.ssl.internal.handshake-timeout -1 Integer The timeout (in ms) during SSL handshake. (-1 = use system default) security.ssl.internal.session-cache-size -1 Integer The size of the cache used for storing SSL session objects. According to here, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default). security.ssl.internal.session-timeout -1 Integer The timeout (in ms) for the cached SSL session objects. (-1 = use system default) security.ssl.provider "JDK" String The SSL engine provider to use for the ssl transport:JDK: default Java-based SSL engineOPENSSL: openSSL-based SSL engine using system librariesOPENSSL is based on netty-tcnative and comes in two flavours:dynamically linked: This will use your system's openSSL libraries (if compatible) and requires opt/flink-shaded-netty-tcnative-dynamic-*.jar to be copied to lib/statically linked: Due to potential licensing issues with openSSL (see LEGAL-393), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into lib/:
git clone https://github.com/apache/flink-shaded.git \u0026amp;\u0026amp; cd flink-shaded \u0026amp;\u0026amp; mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static Advanced Options for the REST endpoint and Client # Key Default Type Description rest.async.store-duration 5 min Duration Maximum duration that the result of an async operation is stored. Once elapsed the result of the operation can no longer be retrieved. rest.await-leader-timeout 30000 Long The time in ms that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint rest.client.max-content-length 104857600 Integer The maximum content length in bytes that the client will handle. rest.connection-timeout 15000 Long The maximum time in ms for the client to establish a TCP connection. rest.flamegraph.cleanup-interval 10 min Duration Time after which cached stats are cleaned up if not accessed. It can be specified using notation: "100 s", "10 m". rest.flamegraph.delay-between-samples 50 ms Duration Delay between individual stack trace samples taken for building a FlameGraph. It can be specified using notation: "100 ms", "1 s". rest.flamegraph.enabled false Boolean Enables the experimental flame graph feature. rest.flamegraph.num-samples 100 Integer Number of samples to take to build a FlameGraph. rest.flamegraph.refresh-interval 1 min Duration Time after which available stats are deprecated and need to be refreshed (by resampling). It can be specified using notation: "30 s", "1 m". rest.flamegraph.stack-depth 100 Integer Maximum depth of stack traces used to create FlameGraphs. rest.idleness-timeout 300000 Long The maximum time in ms for a connection to stay idle before failing. rest.retry.delay 3000 Long The time in ms that the client waits between retries (See also \`rest.retry.max-attempts\`). rest.retry.max-attempts 20 Integer The number of retries the client will attempt if a retryable operations fails. rest.server.max-content-length 104857600 Integer The maximum content length in bytes that the server will handle. rest.server.numThreads 4 Integer The number of threads for the asynchronous processing of requests. rest.server.thread-priority 5 Integer Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing. Advanced Options for Flink Web UI # Key Default Type Description web.access-control-allow-origin "*" String Access-Control-Allow-Origin header for all responses from the web-frontend. web.cancel.enable true Boolean Flag indicating whether jobs can be canceled from the web-frontend. web.checkpoints.history 10 Integer Number of checkpoints to remember for recent history. web.exception-history-size 16 Integer The maximum number of failures collected by the exception history per job. web.history 5 Integer Number of archived jobs for the JobManager. web.log.path (none) String Path to the log file (may be in /log for standalone but under log directory when using YARN). web.refresh-interval 3000 Long Refresh interval for the web-frontend in milliseconds. web.submit.enable true Boolean Flag indicating whether jobs can be uploaded and run from the web-frontend. web.timeout 600000 Long Timeout for asynchronous operations by the web monitor in milliseconds. web.tmpdir System.getProperty("java.io.tmpdir") String Local directory that is used by the REST API for temporary files. web.upload.dir (none) String Local directory that is used by the REST API for storing uploaded jars. If not specified a dynamic directory will be created under web.tmpdir. Full JobManager Options # JobManager
Key Default Type Description jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task 1 gb MemorySize The average size of data volume to expect each task instance to process if jobmanager.scheduler has been set to AdaptiveBatch. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs or the decided parallelism reaches the jobmanager.adaptive-batch-scheduler.max-parallelism (due to too much data), the data actually processed by some tasks may far exceed this value. jobmanager.adaptive-batch-scheduler.default-source-parallelism 1 Integer The default parallelism of source vertices if jobmanager.scheduler has been set to AdaptiveBatch jobmanager.adaptive-batch-scheduler.max-parallelism 128 Integer The upper bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded down to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.min-parallelism 1 Integer The lower bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded up to a power of 2 automatically. jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 1 min Duration Controls how long an detected slow node should be blocked for. jobmanager.adaptive-batch-scheduler.speculative.enabled false Boolean Controls whether to enable speculative execution. jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions 2 Integer Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones. jobmanager.adaptive-scheduler.min-parallelism-increase 1 Integer Configure the minimum increase in parallelism for a job to scale up. jobmanager.adaptive-scheduler.resource-stabilization-timeout 10 s Duration The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.
If scheduler-mode is configured to REACTIVE, this configuration value will default to 0, so that jobs are starting immediately with the available resources. jobmanager.adaptive-scheduler.resource-wait-timeout 5 min Duration The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.
Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).
Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.
If scheduler-mode is configured to REACTIVE, this configuration value will default to a negative value to disable the resource timeout. jobmanager.archive.fs.dir (none) String Dictionary for JobManager to store the archives of completed jobs. jobmanager.execution.attempts-history-size 16 Integer The maximum number of historical execution attempts kept in history. jobmanager.execution.failover-strategy "region" String This option specifies how the job computation recovers from task failures. Accepted values are:'full': Restarts all tasks to recover the job.'region': Restarts all tasks that could be affected by the task failure. More details can be found here. jobmanager.future-pool.size (none) Integer The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.io-pool.size (none) Integer The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.resource-id (none) String The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly. jobmanager.retrieve-taskmanager-hostname true Boolean Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to "false", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted. jobmanager.rpc.address (none) String The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.rpc.port 6123 Integer The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers. jobmanager.scheduler Default Enum
Determines which scheduler implementation is used to schedule tasks. Accepted values are:'Default': Default scheduler'Adaptive': Adaptive scheduler. More details can be found here.'AdaptiveBatch': Adaptive batch scheduler. More details can be found here.
Possible values:"Default""Adaptive""AdaptiveBatch" jobstore.cache-size 52428800 Long The job store cache size in bytes which is used to keep completed jobs in memory. jobstore.expiration-time 3600 Long The time in seconds after which a completed job expires and is purged from the job store. jobstore.max-capacity 2147483647 Integer The max number of completed jobs that can be kept in the job store. NOTICE: if memory store keeps too many jobs in session cluster, it may cause FullGC or OOM in jm. jobstore.type File Enum
Determines which job store implementation is used in session cluster. Accepted values are:'File': the file job store keeps the archived execution graphs in files'Memory': the memory job store keeps the archived execution graphs in memory. You may need to limit the jobstore.max-capacity to mitigate FullGC or OOM when there are too many graphs
Possible values:"File""Memory" web.exception-history-size 16 Integer The maximum number of failures collected by the exception history per job. Blob Server
The Blob Server is a component in the JobManager. It is used for distribution of objects that are too large to be attached to a RPC message and that benefit from caching (like Jar files or large serialized code objects).
Key Default Type Description blob.client.connect.timeout 0 Integer The connection timeout in milliseconds for the blob client. blob.client.socket.timeout 300000 Integer The socket timeout in milliseconds for the blob client. blob.fetch.backlog 1000 Integer The config parameter defining the desired backlog of BLOB fetches on the JobManager.Note that the operating system usually enforces an upper limit on the backlog size based on the SOMAXCONN setting. blob.fetch.num-concurrent 50 Integer The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves. blob.fetch.retries 5 Integer The config parameter defining number of retires for failed BLOB fetches. blob.offload.minsize 1048576 Integer The minimum size for messages to be offloaded to the BlobServer. blob.server.port "0" String The config parameter defining the server port of the blob service. blob.service.cleanup.interval 3600 Long Cleanup interval of the blob caches at the task managers (in seconds). blob.service.ssl.enabled true Boolean Flag to override ssl support for the blob service transport. blob.storage.directory (none) String The config parameter defining the local storage directory to be used by the blob server. If not configured, then it will default to \u0026lt;WORKING_DIR\u0026gt;/blobStorage. ResourceManager
These configuration keys control basic Resource Manager behavior, independent of the used resource orchestration management framework (YARN, etc.)
Key Default Type Description resourcemanager.job.timeout "5 minutes" String Timeout for jobs which don't have a job manager as leader assigned. resourcemanager.previous-worker.recovery.timeout 0 ms Duration Timeout for resource manager to recover all the previous attempts workers. If exceeded, resource manager will handle new resource requests by requesting new workers. If you would like to reuse the previous workers as much as possible, you should configure a longer timeout time to wait for previous workers to register. resourcemanager.rpc.port 0 Integer Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges. resourcemanager.standalone.start-up-time -1 Long Time in milliseconds of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, slot.request.timeout will be used by default. resourcemanager.start-worker.max-failure-rate 10.0 Double The maximum number of start worker failures (Native Kubernetes / Yarn) per minute before pausing requesting new workers. Once the threshold is reached, subsequent worker requests will be postponed to after a configured retry interval ('resourcemanager.start-worker.retry-interval'). resourcemanager.start-worker.retry-interval 3 s Duration The time to wait before requesting new workers (Native Kubernetes / Yarn) once the max failure rate of starting workers ('resourcemanager.start-worker.max-failure-rate') is reached. resourcemanager.taskmanager-registration.timeout 5 min Duration Timeout for TaskManagers to register at the active resource managers. If exceeded, active resource manager will release and try to re-request the resource for the worker. If not configured, fallback to 'taskmanager.registration.timeout'. resourcemanager.taskmanager-timeout 30000 Long The timeout for an idle task manager to be released. slotmanager.max-total-resource.cpu (none) Double Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.max-total-resource.memory (none) MemorySize Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'. slotmanager.number-of-slots.max 2147483647 Integer Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink. slotmanager.redundant-taskmanager-num 0 Integer The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn). Full TaskManagerOptions # Please refer to the network memory tuning guide for details on how to use the taskmanager.network.memory.buffer-debloat.* configuration.
Key Default Type Description task.cancellation.interval 30000 Long Time interval between two successive task cancellation attempts in milliseconds. task.cancellation.timeout 180000 Long Timeout in milliseconds after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog. Notice that a task cancellation is different from both a task failure and a clean shutdown. Task cancellation timeout only applies to task cancellation and does not apply to task closing/clean-up caused by a task failure or a clean shutdown. task.cancellation.timers.timeout 7500 Long Time we wait for the timers in milliseconds to finish all pending timer threads when the stream task is cancelled. taskmanager.data.port 0 Integer The task manager’s external port used for data exchange operations. taskmanager.data.ssl.enabled true Boolean Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true taskmanager.debug.memory.log false Boolean Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM. taskmanager.debug.memory.log-interval 5000 Long The interval (in ms) for the log thread to log the current memory usage. taskmanager.host (none) String The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file. taskmanager.jvm-exit-on-oom false Boolean Whether to kill the TaskManager when the task thread throws an OutOfMemoryError. taskmanager.memory.min-segment-size 256 bytes MemorySize Minimum possible size of memory buffers used by the network stack and the memory manager. ex. can be used for automatic buffer size adjustment. taskmanager.memory.segment-size 32 kb MemorySize Size of memory buffers used by the network stack and the memory manager. taskmanager.network.bind-policy "ip" String The automatic address binding policy used by the TaskManager if "taskmanager.host" is not set. The value should be one of the following: "name" - uses hostname as binding address"ip" - uses host's ip address as binding address taskmanager.numberOfTaskSlots 1 Integer The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores). taskmanager.registration.timeout 5 min Duration Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates. taskmanager.resource-id (none) String The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the "RpcAddress:RpcPort" and a 6-character random string. Notice that this option is not valid in Yarn and Native Kubernetes mode. taskmanager.rpc.port "0" String The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine. taskmanager.slot.timeout 10 s Duration Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no value is configured, then it will fall back to akka.ask.timeout. Data Transport Network Stack
These options are for the network stack that handles the streaming and batch data exchanges between TaskManagers.
Key Default Type Description taskmanager.network.batch-shuffle.compression.enabled true Boolean Boolean flag indicating whether the shuffle data will be compressed for batch shuffle mode. Note that data is compressed per buffer and compression can incur extra CPU overhead, so it is more effective for IO bounded scenario when compression ratio is high. taskmanager.network.blocking-shuffle.type "file" String The blocking shuffle type, either "mmap" or "file". The "auto" means selecting the property type automatically based on system memory architecture (64 bit for mmap and 32 bit for file). Note that the memory usage of mmap is not accounted by configured memory limits, but some resource frameworks like yarn would track this memory usage and kill the container once memory exceeding some threshold. Also note that this option is experimental and might be changed future. taskmanager.network.compression.codec "LZ4" String The codec to be used when compressing shuffle data, only "LZ4", "LZO" and "ZSTD" are supported now. Through tpc-ds test of these three algorithms, the results show that "LZ4" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. "ZSTD" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future. taskmanager.network.detailed-metrics false Boolean Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths. taskmanager.network.max-num-tcp-connections 1 Integer The maximum number of tpc connections between taskmanagers for data communication. taskmanager.network.memory.buffer-debloat.enabled false Boolean The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput. taskmanager.network.memory.buffer-debloat.period 200 ms Duration The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance. taskmanager.network.memory.buffer-debloat.samples 20 Integer The number of the last buffer size values that will be taken for the correct calculation of the new one. taskmanager.network.memory.buffer-debloat.target 1 s Duration The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data. taskmanager.network.memory.buffer-debloat.threshold-percentages 25 Integer The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments. taskmanager.network.memory.buffers-per-channel 2 Integer Number of exclusive network buffers to use for each outgoing/incoming channel (subpartition/input channel) in the credit-based flow control model. It should be configured at least 2 for good performance. 1 buffer is for receiving in-flight data in the subpartition and 1 buffer is for parallel serialization. The minimum valid value that can be configured is 0. When 0 buffers-per-channel is configured, the exclusive network buffers used per downstream incoming channel will be 0, but for each upstream outgoing channel, max(1, configured value) will be used. In other words we ensure that, for performance reasons, there is at least one buffer per outgoing channel regardless of the configuration. taskmanager.network.memory.floating-buffers-per-gate 8 Integer Number of extra network buffers to use for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the input channels. The floating buffers are distributed based on backlog (real-time output buffers in the subpartition) feedback, and can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. This value should be increased in case of higher round trip times between nodes and/or larger number of machines in the cluster. taskmanager.network.memory.max-buffers-per-channel 10 Integer Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data. taskmanager.network.memory.max-overdraft-buffers-per-gate 5 Integer Number of max overdraft network buffers to use for each ResultPartition. The overdraft buffers will be used when the subtask cannot apply to the normal buffers due to back pressure, while subtask is performing an action that can not be interrupted in the middle, like serializing a large record, flatMap operator producing multiple records for one single input record or processing time timer producing large output. In situations like that system will allow subtask to request overdraft buffers, so that the subtask can finish such uninterruptible action, without blocking unaligned checkpoints for long period of time. Overdraft buffers are provided on best effort basis only if the system has some unused buffers available. Subtask that has used overdraft buffers won't be allowed to process any more records until the overdraft buffers are returned to the pool. taskmanager.network.netty.client.connectTimeoutSec 120 Integer The Netty client connection timeout. taskmanager.network.netty.client.numThreads -1 Integer The number of Netty client threads. taskmanager.network.netty.num-arenas -1 Integer The number of Netty arenas. taskmanager.network.netty.sendReceiveBufferSize 0 Integer The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux. taskmanager.network.netty.server.backlog 0 Integer The netty server connection backlog. taskmanager.network.netty.server.numThreads -1 Integer The number of Netty server threads. taskmanager.network.netty.transport "auto" String The Netty transport type, either "nio" or "epoll". The "auto" means selecting the property mode automatically based on the platform. Note that the "epoll" mode can get better performance, less GC and have more advanced features which are only available on modern Linux. taskmanager.network.request-backoff.initial 100 Integer Minimum backoff in milliseconds for partition requests of input channels. taskmanager.network.request-backoff.max 10000 Integer Maximum backoff in milliseconds for partition requests of input channels. taskmanager.network.retries 0 Integer The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections taskmanager.network.sort-shuffle.min-buffers 512 Integer Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value. taskmanager.network.sort-shuffle.min-parallelism 1 Integer Parallelism threshold to switch between sort-based blocking shuffle and hash-based blocking shuffle, which means for batch jobs of smaller parallelism, hash-shuffle will be used and for batch jobs of larger or equal parallelism, sort-shuffle will be used. The value 1 means that sort-shuffle is the default option. Note: For production usage, you may also need to tune 'taskmanager.network.sort-shuffle.min-buffers' and 'taskmanager.memory.framework.off-heap.batch-shuffle.size' for better performance. taskmanager.network.tcp-connection.enable-reuse-across-jobs true Boolean Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. Note that to avoid connection leak, you must set taskmanager.network.max-num-tcp-connections to a smaller value before you enable tcp connection reuse. RPC / Akka # Flink uses Akka for RPC between components (JobManager/TaskManager/ResourceManager). Flink does not use Akka for data transport.
Key Default Type Description akka.ask.callstack true Boolean If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint. akka.ask.timeout 10 s Duration Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d). akka.client-socket-worker-pool.pool-size-factor 1.0 Double The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. akka.client-socket-worker-pool.pool-size-max 2 Integer Max number of threads to cap factor-based number to. akka.client-socket-worker-pool.pool-size-min 1 Integer Min number of threads to cap factor-based number to. akka.fork-join-executor.parallelism-factor 2.0 Double The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values. akka.fork-join-executor.parallelism-max 64 Integer Max number of threads to cap factor-based parallelism number to. akka.fork-join-executor.parallelism-min 8 Integer Min number of threads to cap factor-based parallelism number to. akka.framesize "10485760b" String Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier. akka.jvm-exit-on-fatal-error true Boolean Exit JVM on fatal Akka errors. akka.log.lifecycle.events false Boolean Turns on the Akka’s remote logging of events. Set this value to 'true' in case of debugging. akka.lookup.timeout 10 s Duration Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d). akka.retry-gate-closed-for 50 Long Milliseconds a gate should be closed for after a remote connection was disconnected. akka.server-socket-worker-pool.pool-size-factor 1.0 Double The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. akka.server-socket-worker-pool.pool-size-max 2 Integer Max number of threads to cap factor-based number to. akka.server-socket-worker-pool.pool-size-min 1 Integer Min number of threads to cap factor-based number to. akka.ssl.enabled true Boolean Turns on SSL for Akka’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true. akka.startup-timeout (none) String Timeout after which the startup of a remote component is considered being failed. akka.tcp.timeout "20 s" String Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value. akka.throughput 15 Integer Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness. JVM and Logging Options # Key Default Type Description env.hadoop.conf.dir (none) String Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable. env.hbase.conf.dir (none) String Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable. env.java.opts (none) String Java options to start the JVM of all Flink processes with. env.java.opts.client (none) String Java options to start the JVM of the Flink Client with. env.java.opts.historyserver (none) String Java options to start the JVM of the HistoryServer with. env.java.opts.jobmanager (none) String Java options to start the JVM of the JobManager with. env.java.opts.taskmanager (none) String Java options to start the JVM of the TaskManager with. env.log.dir (none) String Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink’s home) env.log.max 5 Integer The maximum number of old log files to keep. env.pid.dir "/tmp" String Defines the directory where the flink-\u0026lt;host\u0026gt;-\u0026lt;process\u0026gt;.pid files are saved. env.ssh.opts (none) String Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh). env.yarn.conf.dir (none) String Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable. Forwarding Environment Variables # You can configure environment variables to be set on the JobManager and TaskManager processes started on Yarn.
containerized.master.env.: Prefix for passing custom environment variables to Flink\u0026rsquo;s JobManager process. For example for passing LD_LIBRARY_PATH as an env variable to the JobManager, set containerized.master.env.LD_LIBRARY_PATH: \u0026ldquo;/usr/lib/native\u0026rdquo; in the flink-conf.yaml.
containerized.taskmanager.env.: Similar to the above, this configuration prefix allows setting custom environment variables for the workers (TaskManagers).
Deprecated Options # These options relate to parts of Flink that are not actively developed any more. These options may be removed in a future release.
DataSet API Optimizer
Key Default Type Description compiler.delimited-informat.max-line-samples 10 Integer The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters. compiler.delimited-informat.max-sample-len 2097152 Integer The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format’s parameters. compiler.delimited-informat.min-line-samples 2 Integer The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters DataSet API Runtime Algorithms
Key Default Type Description taskmanager.runtime.hashjoin-bloom-filters false Boolean Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles. taskmanager.runtime.large-record-handler false Boolean Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging. taskmanager.runtime.max-fan 128 Integer The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small. taskmanager.runtime.sort-spilling-threshold 0.8 Float A sort operation starts spilling when this fraction of its memory budget is full. DataSet File Sinks
Key Default Type Description fs.output.always-create-directory false Boolean File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to "true", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to "false", the writer will directly create the file directly at the output path, without creating a containing directory. fs.overwrite-files false Boolean Specifies whether file output writers should overwrite existing files by default. Set to "true" to overwrite by default,"false" otherwise. Back to top
`}),e.add({id:95,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/time_attributes/",title:"时间属性",section:"流式概念",content:" 时间属性 # Flink 可以基于几种不同的 时间 概念来处理数据。\n处理时间 指的是执行具体操作时的机器时间（大家熟知的绝对时间, 例如 Java的 System.currentTimeMillis()) ） 事件时间 指的是数据本身携带的时间。这个时间是在事件产生时的时间。 摄入时间 指的是数据进入 Flink 的时间；在系统内部，会把它当做事件时间来处理。 对于时间相关的更多信息，可以参考 事件时间和Watermark。\n本页面说明了如何在 Flink Table API \u0026amp; SQL 里面定义时间以及相关的操作。\n时间属性介绍 # 像窗口（在 Table API 和 SQL ）这种基于时间的操作，需要有时间信息。因此，Table API 中的表就需要提供逻辑时间属性来表示时间，以及支持时间相关的操作。\n每种类型的表都可以有时间属性，可以在用CREATE TABLE DDL创建表的时候指定、也可以在 DataStream 中指定、也可以在定义 TableSource 时指定。一旦时间属性定义好，它就可以像普通列一样使用，也可以在时间相关的操作中使用。\n只要时间属性没有被修改，而是简单地从一个表传递到另一个表，它就仍然是一个有效的时间属性。时间属性可以像普通的时间戳的列一样被使用和计算。一旦时间属性被用在了计算中，它就会被物化，进而变成一个普通的时间戳。普通的时间戳是无法跟 Flink 的时间以及watermark等一起使用的，所以普通的时间戳就无法用在时间相关的操作中。\nTable API 程序需要在 streaming environment 中指定时间属性：\nJava final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // default // 或者: // env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) // default // 或者: // env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime) // env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime) # default # 或者: # env.set_stream_time_characteristic(TimeCharacteristic.IngestionTime) # env.set_stream_time_characteristic(TimeCharacteristic.EventTime) 处理时间 # 处理时间是基于机器的本地时间来处理数据，它是最简单的一种时间概念，但是它不能提供确定性。它既不需要从数据里获取时间，也不需要生成 watermark。\n共有三种方法可以定义处理时间。\n在创建表的 DDL 中定义 # 处理时间属性可以在创建表的 DDL 中用计算列的方式定义，用 PROCTIME() 就可以定义处理时间，函数 PROCTIME() 的返回类型是 TIMESTAMP_LTZ 。关于计算列，更多信息可以参考：CREATE TABLE DDL\nCREATE TABLE user_actions ( user_name STRING, data STRING, user_action_time AS PROCTIME() -- 声明一个额外的列作为处理时间属性 ) WITH ( ... ); SELECT TUMBLE_START(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE); 在 DataStream 到 Table 转换时定义 # 处理时间属性可以在 schema 定义的时候用 .proctime 后缀来定义。时间属性一定不能定义在一个已有字段上，所以它只能定义在 schema 定义的最后。\nJava DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = ...; // 声明一个额外的字段作为时间属性字段 Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;), $(\u0026#34;user_action_time\u0026#34;).proctime()); WindowedTable windowedTable = table.window( Tumble.over(lit(10).minutes()) .on($(\u0026#34;user_action_time\u0026#34;)) .as(\u0026#34;userActionWindow\u0026#34;)); Scala val stream: DataStream[(String, String)] = ... // 声明一个额外的字段作为时间属性字段 val table = tEnv.fromDataStream(stream, $\u0026#34;UserActionTimestamp\u0026#34;, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.proctime) val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 使用 TableSource 定义 # 处理时间属性可以在实现了 DefinedProctimeAttribute 的 TableSource 中定义。逻辑的时间属性会放在 TableSource 已有物理字段的最后\nJava // 定义一个由处理时间属性的 table source public class UserActionSource implements StreamTableSource\u0026lt;Row\u0026gt;, DefinedProctimeAttribute { @Override public TypeInformation\u0026lt;Row\u0026gt; getReturnType() { String[] names = new String[] {\u0026#34;user_name\u0026#34; , \u0026#34;data\u0026#34;}; TypeInformation[] types = new TypeInformation[] {Types.STRING(), Types.STRING()}; return Types.ROW(names, types); } @Override public DataStream\u0026lt;Row\u0026gt; getDataStream(StreamExecutionEnvironment execEnv) { // create stream DataStream\u0026lt;Row\u0026gt; stream = ...; return stream; } @Override public String getProctimeAttribute() { // 这个名字的列会被追加到最后，作为第三列 return \u0026#34;user_action_time\u0026#34;; } } // register table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource()); WindowedTable windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble .over(lit(10).minutes()) .on($(\u0026#34;user_action_time\u0026#34;)) .as(\u0026#34;userActionWindow\u0026#34;)); Scala // 定义一个由处理时间属性的 table source class UserActionSource extends StreamTableSource[Row] with DefinedProctimeAttribute { override def getReturnType = { val names = Array[String](\u0026#34;user_name\u0026#34; , \u0026#34;data\u0026#34;) val types = Array[TypeInformation[_]](Types.STRING, Types.STRING) Types.ROW(names, types) } override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = { // create stream val stream = ... stream } override def getProctimeAttribute = { // 这个名字的列会被追加到最后，作为第三列 \u0026#34;user_action_time\u0026#34; } } // register table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource) val windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 事件时间 # 事件时间允许程序按照数据中包含的时间来处理，这样可以在有乱序或者晚到的数据的情况下产生一致的处理结果。它可以保证从外部存储读取数据后产生可以复现（replayable）的结果。\n除此之外，事件时间可以让程序在流式和批式作业中使用同样的语法。在流式程序中的事件时间属性，在批式程序中就是一个正常的时间字段。\n为了能够处理乱序的事件，并且区分正常到达和晚到的事件，Flink 需要从事件中获取事件时间并且产生 watermark（watermarks）。\n事件时间属性也有类似于处理时间的三种定义方式：在DDL中定义、在 DataStream 到 Table 转换时定义、用 TableSource 定义。\n在 DDL 中定义 # 事件时间属性可以用 WATERMARK 语句在 CREATE TABLE DDL 中进行定义。WATERMARK 语句在一个已有字段上定义一个 watermark 生成表达式，同时标记这个已有字段为时间属性字段。更多信息可以参考：CREATE TABLE DDL\nFlink 支持和在 TIMESTAMP 列和 TIMESTAMP_LTZ 列上定义事件时间。如果源数据中的时间戳数据表示为年-月-日-时-分-秒，则通常为不带时区信息的字符串值，例如 2020-04-15 20:13:40.564，建议将事件时间属性定义在 TIMESTAMP 列上:\nCREATE TABLE user_actions ( user_name STRING, data STRING, user_action_time TIMESTAMP(3), -- 声明 user_action_time 是事件时间属性，并且用 延迟 5 秒的策略来生成 watermark WATERMARK FOR user_action_time AS user_action_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( ... ); SELECT TUMBLE_START(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(user_action_time, INTERVAL \u0026#39;10\u0026#39; MINUTE); 源数据中的时间戳数据表示为一个纪元 (epoch) 时间，通常是一个 long 值，例如 1618989564564，建议将事件时间属性定义在 TIMESTAMP_LTZ 列上：\nCREATE TABLE user_actions ( user_name STRING, data STRING, ts BIGINT, time_ltz AS TO_TIMESTAMP_LTZ(ts, 3), -- declare time_ltz as event time attribute and use 5 seconds delayed watermark strategy WATERMARK FOR time_ltz AS time_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( ... ); SELECT TUMBLE_START(time_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTE), COUNT(DISTINCT user_name) FROM user_actions GROUP BY TUMBLE(time_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTE); 在 DataStream 到 Table 转换时定义 # 事件时间属性可以用 .rowtime 后缀在定义 DataStream schema 的时候来定义。时间戳和 watermark 在这之前一定是在 DataStream 上已经定义好了。 在从 DataStream 转换到 Table 时，由于 DataStream 没有时区概念，因此 Flink 总是将 rowtime 属性解析成 TIMESTAMP WITHOUT TIME ZONE 类型，并且将所有事件时间的值都视为 UTC 时区的值。\n在从 DataStream 到 Table 转换时定义事件时间属性有两种方式。取决于用 .rowtime 后缀修饰的字段名字是否是已有字段，事件时间字段可以是：\n在 schema 的结尾追加一个新的字段 替换一个已经存在的字段。 不管在哪种情况下，事件时间字段都表示 DataStream 中定义的事件的时间戳。\nJava // Option 1: // 基于 stream 中的事件产生时间戳和 watermark DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = inputStream.assignTimestampsAndWatermarks(...); // 声明一个额外的逻辑字段作为事件时间属性 Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;), $(\u0026#34;user_action_time\u0026#34;).rowtime()); // Option 2: // 从第一个字段获取事件时间，并且产生 watermark DataStream\u0026lt;Tuple3\u0026lt;Long, String, String\u0026gt;\u0026gt; stream = inputStream.assignTimestampsAndWatermarks(...); // 第一个字段已经用作事件时间抽取了，不用再用一个新字段来表示事件时间了 Table table = tEnv.fromDataStream(stream, $(\u0026#34;user_action_time\u0026#34;).rowtime(), $(\u0026#34;user_name\u0026#34;), $(\u0026#34;data\u0026#34;)); // Usage: WindowedTable windowedTable = table.window(Tumble .over(lit(10).minutes()) .on($(\u0026#34;user_action_time\u0026#34;)) .as(\u0026#34;userActionWindow\u0026#34;)); Scala // Option 1: // 基于 stream 中的事件产生时间戳和 watermark val stream: DataStream[(String, String)] = inputStream.assignTimestampsAndWatermarks(...) // 声明一个额外的逻辑字段作为事件时间属性 val table = tEnv.fromDataStream(stream, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;, $\u0026#34;user_action_time\u0026#34;.rowtime) // Option 2: // 从第一个字段获取事件时间，并且产生 watermark val stream: DataStream[(Long, String, String)] = inputStream.assignTimestampsAndWatermarks(...) // 第一个字段已经用作事件时间抽取了，不用再用一个新字段来表示事件时间了 val table = tEnv.fromDataStream(stream, $\u0026#34;user_action_time\u0026#34;.rowtime, $\u0026#34;user_name\u0026#34;, $\u0026#34;data\u0026#34;) // Usage: val windowedTable = table.window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) 使用 TableSource 定义 # 事件时间属性可以在实现了 DefinedRowTimeAttributes 的 TableSource 中定义。getRowtimeAttributeDescriptors() 方法返回 RowtimeAttributeDescriptor 的列表，包含了描述事件时间属性的字段名字、如何计算事件时间、以及 watermark 生成策略等信息。\n同时需要确保 getDataStream 返回的 DataStream 已经定义好了时间属性。 只有在定义了 StreamRecordTimestamp 时间戳分配器的时候，才认为 DataStream 是有时间戳信息的。 只有定义了 PreserveWatermarks watermark 生成策略的 DataStream 的 watermark 才会被保留。反之，则只有时间字段的值是生效的。\nJava // 定义一个有事件时间属性的 table source public class UserActionSource implements StreamTableSource\u0026lt;Row\u0026gt;, DefinedRowtimeAttributes { @Override public TypeInformation\u0026lt;Row\u0026gt; getReturnType() { String[] names = new String[] {\u0026#34;user_name\u0026#34;, \u0026#34;data\u0026#34;, \u0026#34;user_action_time\u0026#34;}; TypeInformation[] types = new TypeInformation[] {Types.STRING(), Types.STRING(), Types.LONG()}; return Types.ROW(names, types); } @Override public DataStream\u0026lt;Row\u0026gt; getDataStream(StreamExecutionEnvironment execEnv) { // 构造 DataStream // ... // 基于 \u0026#34;user_action_time\u0026#34; 定义 watermark DataStream\u0026lt;Row\u0026gt; stream = inputStream.assignTimestampsAndWatermarks(...); return stream; } @Override public List\u0026lt;RowtimeAttributeDescriptor\u0026gt; getRowtimeAttributeDescriptors() { // 标记 \u0026#34;user_action_time\u0026#34; 字段是事件时间字段 // 给 \u0026#34;user_action_time\u0026#34; 构造一个时间属性描述符 RowtimeAttributeDescriptor rowtimeAttrDescr = new RowtimeAttributeDescriptor( \u0026#34;user_action_time\u0026#34;, new ExistingField(\u0026#34;user_action_time\u0026#34;), new AscendingTimestamps()); List\u0026lt;RowtimeAttributeDescriptor\u0026gt; listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr); return listRowtimeAttrDescr; } } // register the table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource()); WindowedTable windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble.over(lit(10).minutes()).on($(\u0026#34;user_action_time\u0026#34;)).as(\u0026#34;userActionWindow\u0026#34;)); Scala // 定义一个有事件时间属性的 table source class UserActionSource extends StreamTableSource[Row] with DefinedRowtimeAttributes { override def getReturnType = { val names = Array[String](\u0026#34;user_name\u0026#34; , \u0026#34;data\u0026#34;, \u0026#34;user_action_time\u0026#34;) val types = Array[TypeInformation[_]](Types.STRING, Types.STRING, Types.LONG) Types.ROW(names, types) } override def getDataStream(execEnv: StreamExecutionEnvironment): DataStream[Row] = { // 构造 DataStream // ... // 基于 \u0026#34;user_action_time\u0026#34; 定义 watermark val stream = inputStream.assignTimestampsAndWatermarks(...) stream } override def getRowtimeAttributeDescriptors: util.List[RowtimeAttributeDescriptor] = { // 标记 \u0026#34;user_action_time\u0026#34; 字段是事件时间字段 // 给 \u0026#34;user_action_time\u0026#34; 构造一个时间属性描述符 val rowtimeAttrDescr = new RowtimeAttributeDescriptor( \u0026#34;user_action_time\u0026#34;, new ExistingField(\u0026#34;user_action_time\u0026#34;), new AscendingTimestamps) val listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr) listRowtimeAttrDescr } } // register the table source tEnv.registerTableSource(\u0026#34;user_actions\u0026#34;, new UserActionSource) val windowedTable = tEnv .from(\u0026#34;user_actions\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;user_action_time\u0026#34; as \u0026#34;userActionWindow\u0026#34;) Back to top\n"}),e.add({id:96,href:"/flink/flink-docs-master/zh/docs/dev/configuration/gradle/",title:"使用 Gradle",section:"项目配置",content:` 如何使用 Gradle 配置您的项目 # 您可能需要一个构建工具来配置您的 Flink 项目，本指南将向您展示如何使用 Gradle 执行此操作。Gradle 是一个开源的通用构建工具，可用于在开发过程中自动化执行任务。
要求 # Gradle 7.x Java 11 将项目导入 IDE # 创建项目目录和文件后，我们建议您将此项目导入到 IDE 进行开发和测试。
IntelliJ IDEA 通过 Gradle 插件支持 Gradle 项目。
Eclipse 通过 Eclipse Buildship 插件执行此操作（确保在导入向导的最后一步中指定 Gradle 版本 \u0026gt;= 3.0，shadow 插件会用到它）。您还可以使用 Gradle 的 IDE 集成 来使用 Gradle 创建项目文件。
注意： Java 的默认 JVM 堆大小对于 Flink 来说可能太小，您应该手动增加它。在 Eclipse 中，选中 Run Configurations -\u0026gt; Arguments 并在 VM Arguments 框里填上：-Xmx800m。在 IntelliJ IDEA 中，推荐选中 Help | Edit Custom VM Options 菜单修改 JVM 属性。详情请查阅本文。
关于 IntelliJ 的注意事项： 要使应用程序在 IntelliJ IDEA 中运行，需要在运行配置中的 Include dependencies with \u0026quot;Provided\u0026quot; scope 打勾。如果此选项不可用（可能是由于使用了较旧的 IntelliJ IDEA 版本），可创建一个调用应用程序 main() 方法的测试用例。
构建项目 # 如果您想 构建/打包 您的项目，请转到您的项目目录并运行 \u0026lsquo;gradle clean shadowJar\u0026rsquo; 命令。您将 找到一个 JAR 文件，其中包含您的应用程序，还有已作为依赖项添加到应用程序的连接器和库：build/libs/\u0026lt;project-name\u0026gt;-\u0026lt;version\u0026gt;-all.jar。
注意： 如果您使用不同于 StreamingJob 的类作为应用程序的主类/入口点，我们建议您对 build.gradle 文件里的 mainClassName 配置进行相应的修改。这样，Flink 可以通过 JAR 文件运行应用程序，而无需额外指定主类。
向项目添加依赖项 # 在 build.gradle 文件的 dependencies 块中配置依赖项
例如，如果您使用我们的 Gradle 构建脚本或快速启动脚本创建了项目，如下所示，可以将 Kafka 连接器添加为依赖项：
build.gradle
... dependencies { ... flinkShadowJar \u0026#34;org.apache.flink:flink-connector-kafka:\${flinkVersion}\u0026#34; ... } ... 重要提示： 请注意，应将所有这些（核心）依赖项的生效范围置为 provided。这意味着需要对它们进行编译，但不应将它们打包进项目生成的应用程序 JAR 文件中。如果不设置为 provided，最好的情况是生成的 JAR 变得过大，因为它还包含所有 Flink 核心依赖项。最坏的情况是添加到应用程序 JAR 文件中的 Flink 核心依赖项与您自己的一些依赖项的版本冲突（通常通过反向类加载来避免）。
要将依赖项正确地打包进应用程序 JAR 中，必须把应用程序依赖项的生效范围设置为 compile 。
打包应用程序 # 在部署应用到 Flink 环境之前，您需要根据使用场景用不同的方式打包 Flink 应用程序。
如果您想为 Flink 作业创建 JAR 并且只使用 Flink 依赖而不使用任何第三方依赖（比如使用 JSON 格式的文件系统连接器），您不需要创建一个 uber/fat JAR 或将任何依赖打进包。
您可以使用 gradle clean installDist 命令，如果您使用的是 Gradle Wrapper ，则用 ./gradlew clean installDist。
如果您想为 Flink 作业创建 JAR 并使用未内置在 Flink 发行版中的外部依赖项，您可以将它们添加到发行版的类路径中，或者将它们打包进您的 uber/fat 应用程序 JAR 中。
您可以使用该命令 gradle clean installShadowDist，该命令将在 /build/install/yourProject/lib 目录生成一个 fat JAR。如果您使用的是 Gradle Wrapper ，则用 ./gradlew clean installShadowDist。
您可以将生成的 uber/fat JAR 提交到本地或远程集群：
bin/flink run -c org.example.MyJob myFatJar.jar 要了解有关如何部署 Flink 作业的更多信息，请查看部署指南。
`}),e.add({id:97,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/avro/",title:"Avro",section:"Formats",content:` Avro format # Flink 内置支持 Apache Avro 格式。在 Flink 中将更容易地读写基于 Avro schema 的 Avro 数据。 Flink 的序列化框架可以处理基于 Avro schemas 生成的类。为了能够使用 Avro format，需要在自动构建工具（例如 Maven 或 SBT）中添加如下依赖到项目中。
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 为了在 PyFlink 作业中使用 Avro format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 如果读取 Avro 文件数据，你必须指定 AvroInputFormat。
示例：
AvroInputFormat\u0026lt;User\u0026gt; users = new AvroInputFormat\u0026lt;User\u0026gt;(in, User.class); DataStream\u0026lt;User\u0026gt; usersDS = env.createInput(users); 注意，User 是一个通过 Avro schema生成的 POJO 类。Flink 还允许选择 POJO 中字符串类型的键。例如：
usersDS.keyBy(\u0026#34;name\u0026#34;); 注意，在 Flink 中可以使用 GenericData.Record 类型，但是不推荐使用。因为该类型的记录中包含了完整的 schema，导致数据非常密集，使用起来可能很慢。
Flink 的 POJO 字段选择也适用于从 Avro schema 生成的 POJO 类。但是，只有将字段类型正确写入生成的类时，才能使用。如果字段是 Object 类型，则不能将该字段用作 join 键或 grouping 键。 在 Avro 中如 {\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot;}, 这样指定字段是可行的，但是如 ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;double\u0026quot;]},) 这样指定包含一个字段的复合类型就会生成 Object 类型的字段。注意，如 ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;null\u0026quot;, \u0026quot;double\u0026quot;]},) 这样指定 nullable 类型字段也是可能产生 Object 类型的!
在 Python 作业中读取 Avro 文件，需要先定义 Avro schema，产生的 DataStream 元素为原生的 Python 对象 Generic。例如：
schema = AvroSchema.parse_string(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;int\u0026#34;, \u0026#34;null\u0026#34;]}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteColor\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;string\u0026#34;, \u0026#34;null\u0026#34;]} ] } \u0026#34;\u0026#34;\u0026#34;) env = StreamExecutionEnvironment.get_execution_environment() ds = env.create_input(AvroInputFormat(AVRO_FILE_PATH, schema)) def json_dumps(record): import json return json.dumps(record) ds.map(json_dumps).print() `}),e.add({id:98,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/avro/",title:"Avro",section:"Formats",content:` Avro Format # Format: Serialization Schema Format: Deserialization Schema
Apache Avro format 允许基于 Avro schema 读取和写入 Avro 数据。目前，Avro schema 从 table schema 推导而来。
依赖 # In order to use the Avro format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 如何使用 Avro format 创建表 # 这是使用 Kafka 连接器和 Avro format 创建表的示例。
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; ) Format 参数 # 参数 是否必选 默认值 类型 描述 format 必要 (none) String 指定使用什么 format，这里应该是 'avro'。 avro.codec 可选 (none) String 仅用于 filesystem，avro 压缩编解码器。默认 snappy 压缩。目前支持：null, deflate、snappy、bzip2、xz。 数据类型映射 # 目前，Avro schema 通常是从 table schema 中推导而来。尚不支持显式定义 Avro schema。因此，下表列出了从 Flink 类型到 Avro 类型的类型映射。
Flink SQL 类型 Avro 类型 Avro 逻辑类型 CHAR / VARCHAR / STRING string BOOLEAN boolean BINARY / VARBINARY bytes DECIMAL fixed decimal TINYINT int SMALLINT int INT int BIGINT long FLOAT float DOUBLE double DATE int date TIME int time-millis TIMESTAMP long timestamp-millis ARRAY array MAP
(key 必须是 string/char/varchar 类型) map MULTISET
(元素必须是 string/char/varchar 类型) map ROW record 除了上面列出的类型，Flink 支持读取/写入 nullable 的类型。Flink 将 nullable 的类型映射到 Avro union(something, null)，其中 something 是从 Flink 类型转换的 Avro 类型。
您可以参考 Avro 规范 获取更多有关 Avro 类型的信息。
`}),e.add({id:99,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/azure/",title:"Azure Blob 存储",section:"File Systems",content:` Azure Blob 存储 # Azure Blob 存储 是一项由 Microsoft 管理的服务，能提供多种应用场景下的云存储。 Azure Blob 存储可与 Flink 一起使用以读取和写入数据，以及与流 State Backend 结合使用。
Flink 支持使用 wasb:// 或 abfs:// 访问 Azure Blob 存储。
Azure 建议使用 abfs:// 访问 ADLS Gen2 存储帐户，尽管 wasb:// 通过向后兼容也可以工作。 abfs:// 只能用于访问 ADLS Gen2 存储帐户。 请访问Azure文档，了解如何识别 ADLS Gen2 存储帐户。 通过以下格式指定路径，Azure Blob 存储对象可类似于普通文件使用：
// WASB unencrypted access wasb://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt; // WASB SSL encrypted access wasbs://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt; // ABFS unecrypted access abfs://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.dfs.core.windows.net/\u0026lt;object-path\u0026gt; // ABFS SSL encrypted access abfss://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.dfs.core.windows.net/\u0026lt;object-path\u0026gt; 参见以下代码了解如何在 Flink 作业中使用 Azure Blob 存储：
// 读取 Azure Blob 存储 env.readTextFile(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\$\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;); // 写入 Azure Blob 存储 stream.writeAsText(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;); // 将 Azure Blob 存储用作 FsStatebackend env.setStateBackend(new FsStateBackend(\u0026#34;wasb://\u0026lt;your-container\u0026gt;@\`\u0026lt;your-azure-account\u0026gt;.blob.core.windows.net/\u0026lt;object-path\u0026gt;\u0026#34;)); Shaded Hadoop Azure Blob 存储文件系统 # 为使用 flink-azure-fs-hadoop，在启动 Flink 之前，将对应的 JAR 文件从 opt 目录复制到 Flink 发行版中的 plugin 目录下的一个文件夹中，例如：
mkdir ./plugins/azure-fs-hadoop cp ./opt/flink-azure-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/azure-fs-hadoop/ flink-azure-fs-hadoop 为使用 wasb:// 和 wasbs:// (SSL 加密访问) 的 URI 注册了默认的文件系统包装器。
凭据配置 # WASB # Hadoop 的 WASB Azure 文件系统支持通过 Hadoop 配置来配置凭据，如 Hadoop Azure Blob Storage 文档 所述。 为方便起见，Flink 将所有的 Flink 配置添加 fs.azure 键前缀后转发至文件系统的 Hadoop 配置中。因此，可通过以下方法在 flink-conf.yaml 中配置 Azure Blob 存储密钥：
fs.azure.account.key.\u0026lt;account_name\u0026gt;.blob.core.windows.net: \u0026lt;azure_storage_key\u0026gt; 或者通过在 flink-conf.yaml 中设置以下配置键，将文件系统配置为从环境变量 AZURE_STORAGE_KEY 读取 Azure Blob 存储密钥：
fs.azure.account.keyprovider.\u0026lt;account_name\u0026gt;.blob.core.windows.net: org.apache.flink.fs.azurefs.EnvironmentVariableKeyProvider ABFS # Hadoop 的 ABFS Azure 文件系统支持多种配置身份验证的方法。关于如何配置，请访问Hadoop ABFS文档。
Azure 推荐使用 Azure 托管身份来使用 abfs 访问 ADLS Gen2 存储帐户。关于如何做到这一点的细节超出了本文档的范围，更多细节请参阅 Azure 文档。 使用存储密钥访问ABFS(不鼓励) # Azure blob 存储密钥可以通过以下方式在 flink-conf.yaml 中配置：
fs.azure.account.key.\u0026lt;account_name\u0026gt;.dfs.core.windows.net: \u0026lt;azure_storage_key\u0026gt; Back to top
`}),e.add({id:100,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/azure_table_storage/",title:"Azure Table storage",section:"Formats",content:` Azure Table Storage # 本例使用 HadoopInputFormat 包装器来使用现有的 Hadoop input format 实现访问 Azure\u0026rsquo;s Table Storage.
下载并编译 azure-tables-hadoop 项目。该项目开发的 input format 在 Maven 中心尚不存在，因此，我们必须自己构建该项目。 执行如下命令： git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install 使用 quickstarts 创建一个新的 Flink 项目： curl https://flink.apache.org/q/quickstart.sh | bash 在你的 pom.xml 文件 \u0026lt;dependencies\u0026gt; 部分添加如下依赖： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;microsoft-hadoop-azure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; flink-hadoop-compatibility 是一个提供 Hadoop input format 包装器的 Flink 包。 microsoft-hadoop-azure 可以将之前构建的部分添加到项目中。
现在可以开始进行项目的编码。我们建议将项目导入 IDE，例如 IntelliJ。你应该将其作为 Maven 项目导入。 跳转到文件 Job.java。这是 Flink 作业的初始框架。
粘贴如下代码：
import java.util.Map; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataStream; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import com.microsoft.hadoop.azure.AzureTableConfiguration; import com.microsoft.hadoop.azure.AzureTableInputFormat; import com.microsoft.hadoop.azure.WritableEntity; import com.microsoft.windowsazure.storage.table.EntityProperty; public class AzureTableExample { public static void main(String[] args) throws Exception { // 安装 execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); // 使用 Hadoop input format 包装器创建 AzureTableInputFormat HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt; hdIf = new HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt;(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job()); // 设置 Account URI，如 https://apacheflink.table.core.windows.net hdIf.getConfiguration().set(azuretableconfiguration.Keys.ACCOUNT_URI.getKey(), \u0026#34;TODO\u0026#34;); // 设置存储密钥 hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), \u0026#34;TODO\u0026#34;); // 在此处设置表名 hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), \u0026#34;TODO\u0026#34;); DataStream\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt; input = env.createInput(hdIf); // 如何在 map 中使用数据的简单示例。 DataStream\u0026lt;String\u0026gt; fin = input.map(new MapFunction\u0026lt;Tuple2\u0026lt;Text,WritableEntity\u0026gt;, String\u0026gt;() { @Override public String map(Tuple2\u0026lt;Text, WritableEntity\u0026gt; arg0) throws Exception { System.err.println(\u0026#34;--------------------------------\\nKey = \u0026#34;+arg0.f0); WritableEntity we = arg0.f1; for(Map.Entry\u0026lt;String, EntityProperty\u0026gt; prop : we.getProperties().entrySet()) { System.err.println(\u0026#34;key=\u0026#34;+prop.getKey() + \u0026#34; ; value (asString)=\u0026#34;+prop.getValue().getValueAsString()); } return arg0.f0.toString(); } }); // 发送结果（这仅在本地模式有效） fin.print(); // 执行程序 env.execute(\u0026#34;Azure Example\u0026#34;); } } 该示例展示了如何访问 Azure 表和如何将数据转换为 Flink 的 DataStream（更具体地说，集合的类型是 DataStream\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt;）。你可以将所有已知的 transformations 应用到 DataStream 实例。
Back to top
`}),e.add({id:101,href:"/flink/flink-docs-master/zh/docs/ops/batch/batch_shuffle/",title:"Batch Shuffle",section:"Batch",content:` Batch Shuffle # 总览 # Flink DataStream API 和 Table / SQL 都支持通过批处理执行模式处理有界输入。 在批处理模式下，Flink 提供了两种网络交换模式: Blocking Shuffle 和 Hybrid Shuffle.
Blocking Shuffle 是批处理的默认数据交换模式。它会持久化所有的中间数据，只有当数据产出完全后才能被消费。 Hybrid Shuffle 是下一代的批处理数据交换模式. 他会更加智能地持久化数据, 并且允许在数据生产的同时进行消费. 该特性目前仍处于实验阶段并且存在一些已知的 限制. Blocking Shuffle # 与流式应用使用管道 shuffle 交换数据的方式不同，blocking 交换持久化数据到存储中，然后下游任务通过网络获取这些值。这种交换减少了执行作业所需的资源，因为它不需要同时运行上游和下游任务。
总的来说，Flink 提供了两种不同类型的 blocking shuffles：Hash shuffle 和 Sort shuffle。
Hash Shuffle # 对于 1.14 以及更低的版本，Hash Shuffle 是 blocking shuffle 的默认实现，它为每个下游任务将每个上游任务的结果以单独文件的方式保存在 TaskManager 本地磁盘上。当下游任务运行时会向上游的 TaskManager 请求分片，TaskManager 读取文件之后通过网络传输（给下游任务）。
Hash Shuffle 为读写文件提供了不同的机制:
file: 通过标准文件 IO 写文件，读取和传输文件需要通过 Netty 的 FileRegion。FileRegion 依靠系统调用 sendfile 来减少数据拷贝和内存消耗。 mmap: 通过系统调用 mmap 来读写文件。 auto: 通过标准文件 IO 写文件，对于文件读取，在 32 位机器上降级到 file 选项并且在 64 位机器上使用 mmap 。这是为了避免在 32 位机器上 java 实现 mmap 的文件大小限制。 可通过设置 TaskManager 参数 选择不同的机制。
这个选项是实验性的，将来或许会有改动。 如果开启 SSL，file 机制不能使用 FileRegion 而是在传输之前使用非池化的缓存去缓存数据。这可能会 导致 direct memory OOM。此外，因为同步读取文件有时会造成 netty 线程阻塞，SSL handshake timeout 配置需要调大以防 connection reset 异常。 mmap使用的内存不计算进已有配置的内存限制中，但是一些资源管理框架比如 YARN 将追踪这块内存使用，并且如果容器使用内存超过阈值会被杀掉。 Hash Shuffle 在小规模运行在固态硬盘的任务情况下效果显著，但是依旧有一些问题:
如果任务的规模庞大将会创建很多文件，并且要求同时对这些文件进行大量的写操作。 在机械硬盘情况下，当大量的下游任务同时读取数据，可能会导致随机读写问题。 Sort Shuffle # Sort Shuffle 是 1.13 版中引入的另一种 blocking shuffle 实现，它在 1.15 版本成为默认。不同于 Hash Shuffle，Sort Shuffle 将每个分区结果写入到一个文件。当多个下游任务同时读取结果分片，数据文件只会被打开一次并共享给所有的读请求。因此，集群使用更少的资源。例如：节点和文件描述符以提升稳定性。此外，通过写更少的文件和尽可能线性的读取文件，尤其是在使用机械硬盘情况下 Sort Shuffle 可以获得比 Hash Shuffle 更好的性能。另外，Sort Shuffle 使用额外管理的内存作为读数据缓存并不依赖 sendfile 或 mmap 机制，因此也适用于 SSL。关于 Sort Shuffle 的更多细节请参考 FLINK-19582 和 FLINK-19614。
当使用sort blocking shuffle的时候有些配置需要适配:
taskmanager.network.sort-shuffle.min-buffers: 配置该选项以控制数据写缓存大小。对于大规模的任务而言，你可能需要调大这个值，正常几百兆内存就足够了。因为这部分内存是从网络内存分配的，所以想要增大这个配置值，你可能还需要通过调整 taskmanager.memory.network.fraction，taskmanager.memory.network.min 和 taskmanager.memory.network.max 这几个参数来增大总的网络内存大小以避免出现 \u0026ldquo;Insufficient number of network buffers\u0026rdquo; 的错误。 taskmanager.memory.framework.off-heap.batch-shuffle.size: 配置该选项以控制数据读取缓存大小。对于大规模的任务而言，你可能需要调大这个值，正常几百兆内存就足够了。因为这部分内存是从框架堆外内存中切分出来的，所以想要增大这个配置值，你还需要通过调整 taskmanager.memory.framework.off-heap.size 来增大框架堆外内存以避免出现直接内存溢出的错误。 目前 Sort Shuffle 只通过分区索引来排序而不是记录本身，也就是说 sort 只是被当成数据聚类算法使用。 如何选择 Blocking Shuffle # 总的来说，
对于在固态硬盘上运行的小规模任务而言，两者都可以。 对于在机械硬盘上运行的大规模任务而言，Sort Shuffle 更为合适。 要在 Sort Shuffle 和 Hash Shuffle 间切换，你需要配置这个参数：taskmanager.network.sort-shuffle.min-parallelism。这个参数根据消费者Task的并发选择当前Task使用Hash Shuffle 或 Sort Shuffle，如果并发小于配置值则使用 Hash Shuffle，否则使用 Sort Shuffle。对于 1.15 以下版本，它的默认值是 Integer.MAX_VALUE，这意味着 Hash Shuffle 是默认实现。从 1.15 起，它的默认值是 1，这意味着 Sort Shuffle 是默认实现。
Hybrid Shuffle # This feature is still experimental and has some known limitations. Hybrid shuffle is the next generation of batch data exchanges. It combines the advantages of blocking shuffle and pipelined shuffle (in streaming mode).
Like blocking shuffle, it does not require upstream and downstream tasks to run simultaneously, which allows executing a job with little resources. Like pipelined shuffle, it does not require downstream tasks to be executed after upstream tasks finish, which reduces the overall execution time of the job when given sufficient resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies. Spilling Strategy # Hybrid shuffle provides two spilling strategies:
Selective Spilling Strategy persists data only if they are not consumed by downstream tasks timely. This reduces the amount of data to persist, at the price that in case of failures upstream tasks need to be restarted to reproduce the complete intermediate results. Full Spilling Strategy persists all data, no matter they are consumed by downstream tasks or not. In case of failures, the persisted complete intermediate result can be re-consumed, without having to restart upstream tasks. Usage # To use hybrid shuffle mode, you need to configure the execution.batch-shuffle-mode to ALL_EXCHANGES_HYBRID_FULL (full spilling strategy) or ALL_EXCHANGES_HYBRID_SELECTIVE (selective spilling strategy).
Limitations # Hybrid shuffle mode is still experimental and has some known limitations, which the Flink community is still working on eliminating.
No support for Slot Sharing. In hybrid shuffle mode, Flink currently forces each task to be executed in a dedicated slot exclusively. If slot sharing is explicitly specified, an error will occur. No support for Adaptive Batch Scheduler and Speculative Execution. If adaptive batch scheduler is used in hybrid shuffle mode, an error will occur. 性能调优 # 下面这些建议可以帮助你实现更好的性能，这些对于大规模批作业尤其重要：
Blocking Shuffle 如果你使用机械硬盘作为存储设备，请总是使用 Sort Shuffle，因为这可以极大的提升稳定性和性能。从 1.15 开始，Sort Shuffle 已经成为默认实现，对于 1.14 以及更低版本，你需要通过将 taskmanager.network.sort-shuffle.min-parallelism 配置为 1 以手动开启 Sort Shuffle。 对于 Sort Shuffle 和 Hash Shuffle 两种实现，你都可以考虑开启 数据压缩 除非数据本身无法压缩。从 1.15 开启，数据压缩是默认开启的，对于 1.14 以及更低版本你需要手动开启。 当使用 Sort Shuffle 时，减少 独占网络缓冲区 并增加 流动网络缓冲区 有利于性能提升。对于 1.14 以及更高版本，建议将 taskmanager.network.memory.buffers-per-channel 设为 0 并且将 taskmanager.network.memory.floating-buffers-per-gate 设为一个较大的值 (比如，4096)。这有两个主要的好处：1) 首先这解耦了并发与网络内存使用量，对于大规模作业，这降低了遇到 \u0026ldquo;Insufficient number of network buffers\u0026rdquo; 错误的可能性；2) 网络缓冲区可以根据需求在不同的数据通道间共享流动，这可以提高了网络缓冲区的利用率，进而可以提高性能。 增大总的网络内存。目前网络内存的大小是比较保守的。对于大规模作业，为了实现更好的性能，建议将 网络内存比例 增加至至少 0.2。为了使调整生效，你可能需要同时调整 网络内存大小下界 以及 网络内存大小上界。要获取更多信息，你可以参考这个 内存配置文档。 增大数据写出内存。像上面提到的那样，对于大规模作业，如果有充足的空闲内存，建议增大 数据写出内存 大小到至少 (2 * 并发数)。注意：在你增大这个配置后，为避免出现 \u0026ldquo;Insufficient number of network buffers\u0026rdquo; 错误，你可能还需要增大总的网络内存大小。 增大数据读取内存。像上面提到的那样，对于大规模作业，建议增大 数据读取内存 到一个较大的值 (比如，256M 或 512M)。因为这个内存是从框架的堆外内存切分出来的，因此你必须增加相同的内存大小到 taskmanager.memory.framework.off-heap.size 以避免出现直接内存溢出错误。 Hybrid Shuffle 增大总的网络内存。目前网络内存的大小是比较保守的。对于大规模作业，为了实现更好的性能，建议将 网络内存比例 增加至至少 0.2。为了使调整生效，你可能需要同时调整 网络内存大小下界 以及 网络内存大小上界。要获取更多信息，你可以参考这个 内存配置文档。 增大数据写出内存。对于大规模作业, 建议增大总内存大小，用于数据写入的内存越大, 下游越有机会直接从内存读取数据. 你需要保证每个 Result Partition 至少能够分配到 numSubpartition + 1 个buffer, 否则可能会遇到 \u0026ldquo;Insufficient number of network buffers\u0026rdquo; 错误。 增大数据读取内存。对于大规模作业，建议增大 数据读取内存 到一个较大的值 (比如，256M 或 512M)。因为这个内存是从框架的堆外内存切分出来的，因此你必须增加相同的内存大小到 taskmanager.memory.framework.off-heap.size 以避免出现直接内存溢出错误。 Trouble Shooting # 尽管十分罕见，下面列举了一些你可能会碰到的异常情况以及对应的处理策略：
Blocking Shuffle 异常情况 处理策略 Insufficient number of network buffers 这意味着网络内存大小不足以支撑作业运行，你需要增加总的网络内存大小。注意：从 1.15 开始，Sort Shuffle 已经成为默认实现，对于一些场景，Sort Shuffle 可能比 Hash Shuffle 需要更多的网络内存，因此当你的批作业升级到 1.15 以后可能会遇到这个网络内存不足的问题。这种情况下，你只需要增大总的网络内存大小即可。 Too many open files 这意味着文件句柄不够用了。如果你使用的是 Hash Shuffle，请切换到 Sort Shuffle。如果你已经在使用 Sort Shuffle，请考虑增大操作系统文件句柄上限并且检查是否是作业代码占用了过多的文件句柄。 Connection reset by peer 这通常意味着网络不太稳定或者压力较大。其他一些原因，比如上面提到的 SSL 握手超时等也可能会导致这一问题。如果你使用的是 Hash Shuffle，请切换到 Sort Shuffle。如果你已经在使用 Sort Shuffle，增大 网络连接 backlog 可能会有所帮助。 Network connection timeout 这通常意味着网络不太稳定或者压力较大。增大 网络连接超时时间 或者开启 网络连接重试 可能会有所帮助。 Socket read/write timeout 这通常意味着网络传输速度较慢或者压力较大。增大 网络收发缓冲区 大小可能会有所帮助。如果作业运行在 Kubernetes 环境，使用 host network 可能会有所帮助。 Read buffer request timeout 这个问题只会出现在 Sort Shuffle，它意味着对数据读取缓冲区的激烈竞争。要解决这一问题，你可以增大 taskmanager.memory.framework.off-heap.batch-shuffle.size 和 taskmanager.memory.framework.off-heap.size。 No space left on device 这通常意味着磁盘存储空间或者 inodes 被耗尽。你可以考虑扩展磁盘存储空间或者做一些数据清理。 Out of memory error 如果你使用的是 Hash Shuffle，请切换到 Sort Shuffle。如果你已经在使用 Sort Shuffle 并且遵循了上面章节的建议，你可以考虑增大相应的内存大小。对于堆上内存，你可以增大 taskmanager.memory.task.heap.size，对于直接内存，你可以增大 taskmanager.memory.task.off-heap.size。 Container killed by external resource manger 多种原因可能会导致容器被杀，比如，杀掉一个低优先级容器以释放资源启动高优先级容器，或者容器占用了过多的资源，比如内存、磁盘空间等。像上面章节所提到的那样，Hash Shuffle 可能会使用过多的内存而被 YARN 杀掉。所以，如果你使用的是 Hash Shuffle，请切换到 Sort Shuffle。如果你已经在使用 Sort Shuffle，你可能需要同时检查 Flink 日志以及资源管理框架的日志以找出容器被杀的根因，并且做出相应的修复。 Hybrid Shuffle 异常情况 处理策略 Insufficient number of network buffers 这意味着网络内存量不足以支撑作业运行，你需要增加总的内存大小。 Connection reset by peer 这通常意味着网络不太稳定或者压力较大。其他一些原因，如SSL握手超时等也可能会导致这一问题。 增大 网络连接 backlog 可能会有所帮助。 Network connection timeout 这通常意味着网络不太稳定或者压力较大。增大 网络连接超时时间 或者开启 网络连接重试 可能会有所帮助。 Socket read/write timeout 这通常意味着网络传输速度较慢或者压力较大。增大 网络收发缓冲区 大小可能会有所帮助。如果作业运行在 Kubernetes 环境，使用 host network 可能会有所帮助。 Read buffer request timeout 这意味着对数据读取缓冲区的激烈竞争。要解决这一问题，你可以增大 taskmanager.memory.framework.off-heap.batch-shuffle.size 和 taskmanager.memory.framework.off-heap.size。 No space left on device 这通常意味着磁盘存储空间或者 inodes 被耗尽。你可以考虑扩展磁盘存储空间或者做一些数据清理。 `}),e.add({id:102,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/cassandra/",title:"Cassandra",section:"DataStream Connectors",content:` Apache Cassandra Connector # This connector provides sinks that writes data into a Apache Cassandra database.
To use this connector, add the following dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-cassandra_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here.
Installing Apache Cassandra # There are multiple ways to bring up a Cassandra instance on local machine:
Follow the instructions from Cassandra Getting Started page. Launch a container running Cassandra from Official Docker Repository Cassandra Sinks # Configurations # Flink\u0026rsquo;s Cassandra sink are created by using the static CassandraSink.addSink(DataStream input) method. This method returns a CassandraSinkBuilder, which offers methods to further configure the sink, and finally build() the sink instance.
The following configuration methods can be used:
setQuery(String query) Sets the upsert query that is executed for every record the sink receives. The query is internally treated as CQL statement. DO set the upsert query for processing Tuple data type. DO NOT set the query for processing POJO data types. setClusterBuilder(ClusterBuilder clusterBuilder) Sets the cluster builder that is used to configure the connection to cassandra with more sophisticated settings such as consistency level, retry policy and etc. setHost(String host[, int port]) Simple version of setClusterBuilder() with host/port information to connect to Cassandra instances setMapperOptions(MapperOptions options) Sets the mapper options that are used to configure the DataStax ObjectMapper. Only applies when processing POJO data types. setMaxConcurrentRequests(int maxConcurrentRequests, Duration timeout) Sets the maximum allowed number of concurrent requests with a timeout for acquiring permits to execute. Only applies when enableWriteAheadLog() is not configured. enableWriteAheadLog([CheckpointCommitter committer]) An optional setting Allows exactly-once processing for non-deterministic algorithms. setFailureHandler([CassandraFailureHandler failureHandler]) An optional setting Sets the custom failure handler. setDefaultKeyspace(String keyspace) Sets the default keyspace to be used. enableIgnoreNullFields() Enables ignoring null values, treats null values as unset and avoids writing null fields and creating tombstones. build() Finalizes the configuration and constructs the CassandraSink instance. Write-ahead Log # A checkpoint committer stores additional information about completed checkpoints in some resource. This information is used to prevent a full replay of the last completed checkpoint in case of a failure. You can use a CassandraCommitter to store these in a separate table in cassandra. Note that this table will NOT be cleaned up by Flink.
Flink can provide exactly-once guarantees if the query is idempotent (meaning it can be applied multiple times without changing the result) and checkpointing is enabled. In case of a failure the failed checkpoint will be replayed completely.
Furthermore, for non-deterministic programs the write-ahead log has to be enabled. For such a program the replayed checkpoint may be completely different than the previous attempt, which may leave the database in an inconsistent state since part of the first attempt may already be written. The write-ahead log guarantees that the replayed checkpoint is identical to the first attempt. Note that that enabling this feature will have an adverse impact on latency.
Note: The write-ahead log functionality is currently experimental. In many cases it is sufficient to use the connector without enabling it. Please report problems to the development mailing list.
Checkpointing and Fault Tolerance # With checkpointing enabled, Cassandra Sink guarantees at-least-once delivery of action requests to C* instance.
More details on checkpoints docs and fault tolerance guarantee docs
Examples # The Cassandra sink currently supports both Tuple and POJO data types, and Flink automatically detects which type of input is used. For general use of those streaming data types, please refer to Supported Data Types. We show two implementations based on SocketWindowWordCount , for POJO and Tuple data types respectively.
In all these examples, we assumed the associated Keyspace example and Table wordcount have been created.
CQL CREATE KEYSPACE IF NOT EXISTS example WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;: \u0026#39;1\u0026#39;}; CREATE TABLE IF NOT EXISTS example.wordcount ( word text, count bigint, PRIMARY KEY(word) ); Cassandra Sink Example for Streaming Tuple Data Type # While storing the result with Java/Scala Tuple data type to a Cassandra sink, it is required to set a CQL upsert statement (via setQuery(\u0026lsquo;stmt\u0026rsquo;)) to persist each record back to the database. With the upsert query cached as PreparedStatement, each Tuple element is converted to parameters of the statement.
For details about PreparedStatement and BoundStatement, please visit DataStax Java Driver manual
Java // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream\u0026lt;String\u0026gt; text = env.socketTextStream(hostname, port, \u0026#34;\\n\u0026#34;); // parse the data, group it, window it, and aggregate the counts DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; result = text .flatMap(new FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Long\u0026gt;\u0026gt;() { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) { // normalize and split the line String[] words = value.toLowerCase().split(\u0026#34;\\\\s\u0026#34;); // emit the pairs for (String word : words) { //Do not accept empty word, since word is defined as primary key in C* table if (!word.isEmpty()) { out.collect(new Tuple2\u0026lt;String, Long\u0026gt;(word, 1L)); } } } }) .keyBy(value -\u0026gt; value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); CassandraSink.addSink(result) .setQuery(\u0026#34;INSERT INTO example.wordcount(word, count) values (?, ?);\u0026#34;) .setHost(\u0026#34;127.0.0.1\u0026#34;) .build(); Scala val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment // get input data by connecting to the socket val text: DataStream[String] = env.socketTextStream(hostname, port, \u0026#39;\\n\u0026#39;) // parse the data, group it, window it, and aggregate the counts val result: DataStream[(String, Long)] = text // split up the lines in pairs (2-tuples) containing: (word,1) .flatMap(_.toLowerCase.split(\u0026#34;\\\\s\u0026#34;)) .filter(_.nonEmpty) .map((_, 1L)) // group by the tuple field \u0026#34;0\u0026#34; and sum up tuple field \u0026#34;1\u0026#34; .keyBy(_._1) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1) CassandraSink.addSink(result) .setQuery(\u0026#34;INSERT INTO example.wordcount(word, count) values (?, ?);\u0026#34;) .setHost(\u0026#34;127.0.0.1\u0026#34;) .build() result.print().setParallelism(1) Cassandra Sink Example for Streaming POJO Data Type # An example of streaming a POJO data type and store the same POJO entity back to Cassandra. In addition, this POJO implementation needs to follow DataStax Java Driver Manual to annotate the class as each field of this entity is mapped to an associated column of the designated table using the DataStax Java Driver com.datastax.driver.mapping.Mapper class.
The mapping of each table column can be defined through annotations placed on a field declaration in the Pojo class. For details of the mapping, please refer to CQL documentation on Definition of Mapped Classes and CQL Data types
Java // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream\u0026lt;String\u0026gt; text = env.socketTextStream(hostname, port, \u0026#34;\\n\u0026#34;); // parse the data, group it, window it, and aggregate the counts DataStream\u0026lt;WordCount\u0026gt; result = text .flatMap(new FlatMapFunction\u0026lt;String, WordCount\u0026gt;() { public void flatMap(String value, Collector\u0026lt;WordCount\u0026gt; out) { // normalize and split the line String[] words = value.toLowerCase().split(\u0026#34;\\\\s\u0026#34;); // emit the pairs for (String word : words) { if (!word.isEmpty()) { //Do not accept empty word, since word is defined as primary key in C* table out.collect(new WordCount(word, 1L)); } } } }) .keyBy(WordCount::getWord) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .reduce(new ReduceFunction\u0026lt;WordCount\u0026gt;() { @Override public WordCount reduce(WordCount a, WordCount b) { return new WordCount(a.getWord(), a.getCount() + b.getCount()); } }); CassandraSink.addSink(result) .setHost(\u0026#34;127.0.0.1\u0026#34;) .setMapperOptions(() -\u0026gt; new Mapper.Option[]{Mapper.Option.saveNullFields(true)}) .build(); @Table(keyspace = \u0026#34;example\u0026#34;, name = \u0026#34;wordcount\u0026#34;) public class WordCount { @Column(name = \u0026#34;word\u0026#34;) private String word = \u0026#34;\u0026#34;; @Column(name = \u0026#34;count\u0026#34;) private long count = 0; public WordCount() {} public WordCount(String word, long count) { this.setWord(word); this.setCount(count); } public String getWord() { return word; } public void setWord(String word) { this.word = word; } public long getCount() { return count; } public void setCount(long count) { this.count = count; } @Override public String toString() { return getWord() + \u0026#34; : \u0026#34; + getCount(); } } Back to top
`}),e.add({id:103,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/checkpointing/",title:"Checkpointing",section:"状态与容错",content:` Checkpointing # Flink 中的每个方法或算子都能够是有状态的（阅读 working with state 了解更多）。 状态化的方法在处理单个 元素/事件 的时候存储数据，让状态成为使各个类型的算子更加精细的重要部分。 为了让状态容错，Flink 需要为状态添加 checkpoint（检查点）。Checkpoint 使得 Flink 能够恢复状态和在流中的位置，从而向应用提供和无故障执行时一样的语义。
容错文档 中介绍了 Flink 流计算容错机制内部的技术原理。
前提条件 # Flink 的 checkpoint 机制会和持久化存储进行交互，读写流与状态。一般需要：
一个能够回放一段时间内数据的持久化数据源，例如持久化消息队列（例如 Apache Kafka、RabbitMQ、 Amazon Kinesis、 Google PubSub 等）或文件系统（例如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。 存放状态的持久化存储，通常为分布式文件系统（比如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。 开启与配置 Checkpoint # 默认情况下 checkpoint 是禁用的。通过调用 StreamExecutionEnvironment 的 enableCheckpointing(n) 来启用 checkpoint，里面的 n 是进行 checkpoint 的间隔，单位毫秒。
Checkpoint 其他的属性包括：
精确一次（exactly-once）对比至少一次（at-least-once）：你可以选择向 enableCheckpointing(long interval, CheckpointingMode mode) 方法中传入一个模式来选择使用两种保证等级中的哪一种。 对于大多数应用来说，精确一次是较好的选择。至少一次可能与某些延迟超低（始终只有几毫秒）的应用的关联较大。
checkpoint 超时：如果 checkpoint 执行的时间超过了该配置的阈值，还在进行中的 checkpoint 操作就会被抛弃。
checkpoints 之间的最小时间：该属性定义在 checkpoint 之间需要多久的时间，以确保流应用在 checkpoint 之间有足够的进展。如果值设置为了 5000， 无论 checkpoint 持续时间与间隔是多久，在前一个 checkpoint 完成时的至少五秒后会才开始下一个 checkpoint。
往往使用“checkpoints 之间的最小时间”来配置应用会比 checkpoint 间隔容易很多，因为“checkpoints 之间的最小时间”在 checkpoint 的执行时间超过平均值时不会受到影响（例如如果目标的存储系统忽然变得很慢）。
注意这个值也意味着并发 checkpoint 的数目是一。
checkpoint 可容忍连续失败次数：该属性定义可容忍多少次连续的 checkpoint 失败。超过这个阈值之后会触发作业错误 fail over。 默认次数为“0”，这意味着不容忍 checkpoint 失败，作业将在第一次 checkpoint 失败时fail over。 可容忍的checkpoint失败仅适用于下列情形：Job Manager的IOException，TaskManager做checkpoint时异步部分的失败， checkpoint超时等。TaskManager做checkpoint时同步部分的失败会直接触发作业fail over。其它的checkpoint失败（如一个checkpoint被另一个checkpoint包含）会被忽略掉。
并发 checkpoint 的数目: 默认情况下，在上一个 checkpoint 未完成（失败或者成功）的情况下，系统不会触发另一个 checkpoint。这确保了拓扑不会在 checkpoint 上花费太多时间，从而影响正常的处理流程。 不过允许多个 checkpoint 并行进行是可行的，对于有确定的处理延迟（例如某方法所调用比较耗时的外部服务），但是仍然想进行频繁的 checkpoint 去最小化故障后重跑的 pipelines 来说，是有意义的。
该选项不能和 \u0026ldquo;checkpoints 间的最小时间\u0026quot;同时使用。
externalized checkpoints: 你可以配置周期存储 checkpoint 到外部系统中。Externalized checkpoints 将他们的元数据写到持久化存储上并且在 job 失败的时候不会被自动删除。 这种方式下，如果你的 job 失败，你将会有一个现有的 checkpoint 去恢复。更多的细节请看 Externalized checkpoints 的部署文档。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 每 1000ms 开始一次 checkpoint env.enableCheckpointing(1000); // 高级选项： // 设置模式为精确一次 (这是默认值) env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // 确认 checkpoints 之间的时间会进行 500 ms env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500); // Checkpoint 必须在一分钟内完成，否则就会被抛弃 env.getCheckpointConfig().setCheckpointTimeout(60000); // 允许两个连续的 checkpoint 错误 env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2); // 同一时间只允许一个 checkpoint 进行 env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 使用 externalized checkpoints，这样 checkpoint 在作业取消后仍就会被保留 env.getCheckpointConfig().setExternalizedCheckpointCleanup( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); // 开启实验性的 unaligned checkpoints env.getCheckpointConfig().enableUnalignedCheckpoints(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() // 每 1000ms 开始一次 checkpoint env.enableCheckpointing(1000) // 高级选项： // 设置模式为精确一次 (这是默认值) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) // 确认 checkpoints 之间的时间会进行 500 ms env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500) // Checkpoint 必须在一分钟内完成，否则就会被抛弃 env.getCheckpointConfig.setCheckpointTimeout(60000) // 允许两个连续的 checkpoint 错误 env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2) // 同一时间只允许一个 checkpoint 进行 env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 使用 externalized checkpoints，这样 checkpoint 在作业取消后仍就会被保留 env.getCheckpointConfig().setExternalizedCheckpointCleanup( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) // 开启实验性的 unaligned checkpoints env.getCheckpointConfig.enableUnalignedCheckpoints() Python env = StreamExecutionEnvironment.get_execution_environment() # 每 1000ms 开始一次 checkpoint env.enable_checkpointing(1000) # 高级选项： # 设置模式为精确一次 (这是默认值) env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE) # 确认 checkpoints 之间的时间会进行 500 ms env.get_checkpoint_config().set_min_pause_between_checkpoints(500) # Checkpoint 必须在一分钟内完成，否则就会被抛弃 env.get_checkpoint_config().set_checkpoint_timeout(60000) # 允许两个连续的 checkpoint 错误 env.get_checkpoint_config().set_tolerable_checkpoint_failure_number(2) # 同一时间只允许一个 checkpoint 进行 env.get_checkpoint_config().set_max_concurrent_checkpoints(1) # 使用 externalized checkpoints，这样 checkpoint 在作业取消后仍就会被保留 env.get_checkpoint_config().enable_externalized_checkpoints( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) # 开启实验性的 unaligned checkpoints env.get_checkpoint_config().enable_unaligned_checkpoints() 相关的配置选项 # 更多的属性与默认值能在 conf/flink-conf.yaml 中设置（完整教程请阅读 配置）。
Key Default Type Description state.backend.incremental false Boolean Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option. state.backend.local-recovery false Boolean This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, the MemoryStateBackend does not support local recovery and ignores this option. state.checkpoint-storage (none) String The checkpoint storage implementation to be used to checkpoint state.
The implementation can be specified either via their shortcut name, or via the class name of a CheckpointStorageFactory. If a factory is specified it is instantiated via its zero argument constructor and its CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.
Recognized shortcut names are 'jobmanager' and 'filesystem'. state.checkpoints.dir (none) String The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers). state.checkpoints.num-retained 1 Integer The maximum number of completed checkpoints to retain. state.savepoints.dir (none) String The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend). state.storage.fs.memory-threshold 20 kb MemorySize The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB. state.storage.fs.write-buffer-size 4096 Integer The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.storage.fs.memory-threshold'. taskmanager.state.local.root-dirs (none) String The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignores this option. If not configured it will default to \u0026lt;WORKING_DIR\u0026gt;/localState. The \u0026lt;WORKING_DIR\u0026gt; can be configured via process.taskmanager.working-dir Back to top
选择一个 State Backend # Flink 的 checkpointing 机制 会将 timer 以及 stateful 的 operator 进行快照，然后存储下来， 包括连接器（connectors），窗口（windows）以及任何用户自定义的状态。 Checkpoint 存储在哪里取决于所配置的 State Backend（比如 JobManager memory、 file system、 database）。
默认情况下，状态是保持在 TaskManagers 的内存中，checkpoint 保存在 JobManager 的内存中。为了合适地持久化大体量状态， Flink 支持各种各样的途径去存储 checkpoint 状态到其他的 state backends 上。通过 StreamExecutionEnvironment.setStateBackend(…) 来配置所选的 state backends。
阅读 state backends 来查看在 job 范围和集群范围上可用的 state backends 与选项的更多细节。
迭代作业中的状态和 checkpoint # Flink 现在为没有迭代（iterations）的作业提供一致性的处理保证。在迭代作业上开启 checkpoint 会导致异常。为了在迭代程序中强制进行 checkpoint，用户需要在开启 checkpoint 时设置一个特殊的标志： env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true)。
请注意在环形边上游走的记录（以及与之相关的状态变化）在故障时会丢失。
部分任务结束后的 Checkpoint # 从版本 1.14 开始 Flink 支持在部分任务结束后继续进行Checkpoint。 如果一部分数据源是有限数据集，那么就可以出现这种情况。 从版本 1.15 开始，这一特性被默认打开。如果想要关闭这一功能，可以执行：
Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, false); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config); 在这种情况下，结束的任务不会参与 Checkpoint 的过程。在实现自定义的算子或者 UDF （用户自定义函数）时需要考虑这一点。
为了支持部分任务结束后的 Checkpoint 操作，我们调整了 任务的生命周期 并且引入了 StreamOperator#finish 方法。 在这一方法中，用户需要写出所有缓冲区中的数据。在 finish 方法调用后的 checkpoint 中，这一任务一定不能再有缓冲区中的数据，因为在 finish() 后没有办法来输出这些数据。 在大部分情况下，finish() 后这一任务的状态为空，唯一的例外是如果其中某些算子中包含外部系统事务的句柄（例如为了实现恰好一次语义）， 在这种情况下，在 finish() 后进行的 checkpoint 操作应该保留这些句柄，并且在结束 checkpoint（即任务退出前所等待的 checkpoint）时提交。 一个可以参考的例子是满足恰好一次语义的 sink 接口与 TwoPhaseCommitSinkFunction。
对 operator state 的影响 # 在部分 Task 结束后的checkpoint中，Flink 对 UnionListState 进行了特殊的处理。 UnionListState 一般用于实现对外部系统读取位置的一个全局视图（例如，用于记录所有 Kafka 分区的读取偏移）。 如果我们在算子的某个并发调用 close() 方法后丢弃它的状态，我们就会丢失它所分配的分区的偏移量信息。 为了解决这一问题，对于使用 UnionListState 的算子我们只允许在它的并发都在运行或都已结束的时候才能进行 checkpoint 操作。
ListState 一般不会用于类似的场景，但是用户仍然需要注意在调用 close() 方法后进行的 checkpoint 会丢弃算子的状态并且 这些状态在算子重启后不可用。
任何支持并发修改操作的算子也可以支持部分并发实例结束后的恢复操作。从这种类型的快照中恢复等价于将算子的并发改为正在运行的并发实例数。
任务结束前等待最后一次 Checkpoint # 为了保证使用两阶段提交的算子可以提交所有的数据，任务会在所有算子都调用 finish() 方法后等待下一次 checkpoint 成功后退出。 需要注意的是，这一行为可能会延长任务运行的时间，如果 checkpoint 周期比较大，这一延迟会非常明显。 极端情况下，如果 checkpoint 的周期被设置为 Long.MAX_VALUE，那么任务永远不会结束，因为下一次 checkpoint 不会进行。
Back to top
`}),e.add({id:104,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/avro-confluent/",title:"Confluent Avro",section:"Formats",content:` Confluent Avro Format # Format: Serialization Schema Format: Deserialization Schema
Avro Schema Registry (avro-confluent) 格式能让你读取被 io.confluent.kafka.serializers.KafkaAvroSerializer 序列化的记录，以及可以写入成能被 io.confluent.kafka.serializers.KafkaAvroDeserializer 反序列化的记录。
当以这种格式读取（反序列化）记录时，将根据记录中编码的 schema 版本 id 从配置的 Confluent Schema Registry 中获取 Avro writer schema ，而从 table schema 中推断出 reader schema。
当以这种格式写入（序列化）记录时，Avro schema 是从 table schema 中推断出来的，并会用来检索要与数据一起编码的 schema id。我们会在配置的 Confluent Schema Registry 中配置的 subject 下，检索 schema id。subject 通过 avro-confluent.subject 参数来制定。
Avro Schema Registry 格式只能与 Apache Kafka SQL 连接器或 Upsert Kafka SQL 连接器一起使用。
依赖 # In order to use the Avro Schema Registry format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-confluent-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. For Maven, SBT, Gradle, or other build automation tools, please also ensure that Confluent\u0026rsquo;s maven repository at https://packages.confluent.io/maven/ is configured in your project\u0026rsquo;s build files.
如何创建使用 Avro-Confluent 格式的表 # 以下是一个使用 Kafka 连接器和 Confluent Avro 格式创建表的示例。
SQL 使用原始的 UTF-8 字符串作为 Kafka 的 key，Schema Registry 中注册的 Avro 记录作为 Kafka 的 values 的表的示例：
CREATE TABLE user_created ( -- 该列映射到 Kafka 原始的 UTF-8 key the_kafka_key STRING, -- 映射到 Kafka value 中的 Avro 字段的一些列 id STRING, name STRING, email STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example1\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- UTF-8 字符串作为 Kafka 的 keys，使用表中的 \u0026#39;the_kafka_key\u0026#39; 列 \u0026#39;key.format\u0026#39; = \u0026#39;raw\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;the_kafka_key\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) 我们可以像下面这样将数据写入到 kafka 表中：
INSERT INTO user_created SELECT -- 将 user id 复制至映射到 kafka key 的列中 id as the_kafka_key, -- 所有的 values id, name, email FROM some_table Kafka 的 key 和 value 在 Schema Registry 中都注册为 Avro 记录的表的示例：
CREATE TABLE user_created ( -- 该列映射到 Kafka key 中的 Avro 字段 \u0026#39;id\u0026#39; kafka_key_id STRING, -- 映射到 Kafka value 中的 Avro 字段的一些列 id STRING, name STRING, email STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example2\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- 注意：由于哈希分区，在 Kafka key 的上下文中，schema 升级几乎从不向后也不向前兼容。 \u0026#39;key.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;key.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;key.fields\u0026#39; = \u0026#39;kafka_key_id\u0026#39;, -- 在本例中，我们希望 Kafka 的 key 和 value 的 Avro 类型都包含 \u0026#39;id\u0026#39; 字段 -- =\u0026gt; 给表中与 Kafka key 字段关联的列添加一个前缀来避免冲突 \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;kafka_key_\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39;, -- 自 Flink 1.13 起，subjects 具有一个默认值, 但是可以被覆盖： \u0026#39;key.avro-confluent.subject\u0026#39; = \u0026#39;user_events_example2-key2\u0026#39;, \u0026#39;value.avro-confluent.subject\u0026#39; = \u0026#39;user_events_example2-value2\u0026#39; ) 使用 upsert-kafka 连接器，Kafka 的 value 在 Schema Registry 中注册为 Avro 记录的表的示例：
CREATE TABLE user_created ( -- 该列映射到 Kafka 原始的 UTF-8 key kafka_key_id STRING, -- 映射到 Kafka value 中的 Avro 字段的一些列 id STRING, name STRING, email STRING, -- upsert-kafka 连接器需要一个主键来定义 upsert 行为 PRIMARY KEY (kafka_key_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_events_example3\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, -- UTF-8 字符串作为 Kafka 的 keys -- 在本例中我们不指定 \u0026#39;key.fields\u0026#39;，因为它由表的主键决定 \u0026#39;key.format\u0026#39; = \u0026#39;raw\u0026#39;, -- 在本例中，我们希望 Kafka 的 key 和 value 的 Avro 类型都包含 \u0026#39;id\u0026#39; 字段 -- =\u0026gt; 给表中与 Kafka key 字段关联的列添加一个前缀来避免冲突 \u0026#39;key.fields-prefix\u0026#39; = \u0026#39;kafka_key_\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro-confluent\u0026#39;, \u0026#39;value.avro-confluent.url\u0026#39; = \u0026#39;http://localhost:8082\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) Format 参数 # 参数 是否必选 默认值 类型 描述 format required (none) String Specify what format to use, here should be 'avro-confluent'. avro-confluent.basic-auth.credentials-source optional (none) String Basic auth credentials source for Schema Registry avro-confluent.basic-auth.user-info optional (none) String Basic auth user info for schema registry avro-confluent.bearer-auth.credentials-source optional (none) String Bearer auth credentials source for Schema Registry avro-confluent.bearer-auth.token optional (none) String Bearer auth token for Schema Registry avro-confluent.properties optional (none) Map Properties map that is forwarded to the underlying Schema Registry. This is useful for options that are not officially exposed via Flink config options. However, note that Flink options have higher precedence. avro-confluent.ssl.keystore.location optional (none) String Location / File of SSL keystore avro-confluent.ssl.keystore.password optional (none) String Password for SSL keystore avro-confluent.ssl.truststore.location optional (none) String Location / File of SSL truststore avro-confluent.ssl.truststore.password optional (none) String Password for SSL truststore avro-confluent.subject optional (none) String The Confluent Schema Registry subject under which to register the schema used by this format during serialization. By default, 'kafka' and 'upsert-kafka' connectors use '\u0026lt;topic_name\u0026gt;-value' or '\u0026lt;topic_name\u0026gt;-key' as the default subject name if this format is used as the value or key format. But for other connectors (e.g. 'filesystem'), the subject option is required when used as sink. avro-confluent.url required (none) String The URL of the Confluent Schema Registry to fetch/register schemas. 数据类型映射 # 目前 Apache Flink 都是从 table schema 去推断反序列化期间的 Avro reader schema 和序列化期间的 Avro writer schema。显式地定义 Avro schema 暂不支持。 Apache Avro Format中描述了 Flink 数据类型和 Avro 类型的对应关系。
除了此处列出的类型之外，Flink 还支持读取/写入可为空（nullable）的类型。 Flink 将可为空的类型映射到 Avro union(something, null), 其中 something 是从 Flink 类型转换的 Avro 类型。
您可以参考 Avro Specification 以获取有关 Avro 类型的更多信息。
`}),e.add({id:105,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/create/",title:"CREATE 语句",section:"SQL",content:` CREATE 语句 # CREATE 语句用于向当前或指定的 Catalog 中注册表、视图或函数。注册后的表、视图和函数可以在 SQL 查询中使用。
目前 Flink SQL 支持下列 CREATE 语句：
CREATE TABLE CREATE CATALOG CREATE DATABASE CREATE VIEW CREATE FUNCTION 执行 CREATE 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，execute_sql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。
SQL CLI 可以在 SQL CLI 中执行 CREATE 语句。
以下的例子展示了如何在 SQL CLI 中执行一个 CREATE 语句。
Java EnvironmentSettings settings = EnvironmentSettings.newInstance()... TableEnvironment tableEnv = TableEnvironment.create(settings); // 对已注册的表进行 SQL 查询 // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // 在表上执行 SQL 查询，并把得到的结果作为一个新的表 Table result = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // 对已注册的表进行 INSERT 操作 // 注册 TableSink tableEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;); // 在表上执行 INSERT 语句并向 TableSink 发出结果 tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); Scala val tableEnv = TableEnvironment.create(...) // 对已注册的表进行 SQL 查询 // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // 在表上执行 SQL 查询，并把得到的结果作为一个新的表 val result = tableEnv.sqlQuery( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // 对已注册的表进行 INSERT 操作 // 注册 TableSink tableEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (\u0026#39;connector.path\u0026#39;=\u0026#39;/path/to/file\u0026#39; ...)\u0026#34;) // 在表上执行 INSERT 语句并向 TableSink 发出结果 tableEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) Python table_env = TableEnvironment.create(...) # 对已经注册的表进行 SQL 查询 # 注册名为 “Orders” 的表 table_env.execute_sql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); # 在表上执行 SQL 查询，并把得到的结果作为一个新的表 result = table_env.sql_query( \u0026#34;SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); # 对已注册的表进行 INSERT 操作 # 注册 TableSink table_env.execute_sql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) # 在表上执行 INSERT 语句并向 TableSink 发出结果 table_env \\ .execute_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders (product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... Back to top
CREATE TABLE # CREATE TABLE [IF NOT EXISTS] [catalog_name.][db_name.]table_name ( { \u0026lt;physical_column_definition\u0026gt; | \u0026lt;metadata_column_definition\u0026gt; | \u0026lt;computed_column_definition\u0026gt; }[ , ...n] [ \u0026lt;watermark_definition\u0026gt; ] [ \u0026lt;table_constraint\u0026gt; ][ , ...n] ) [COMMENT table_comment] [PARTITIONED BY (partition_column_name1, partition_column_name2, ...)] WITH (key1=val1, key2=val2, ...) [ LIKE source_table [( \u0026lt;like_options\u0026gt; )] ] \u0026lt;physical_column_definition\u0026gt;: column_name column_type [ \u0026lt;column_constraint\u0026gt; ] [COMMENT column_comment] \u0026lt;column_constraint\u0026gt;: [CONSTRAINT constraint_name] PRIMARY KEY NOT ENFORCED \u0026lt;table_constraint\u0026gt;: [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED \u0026lt;metadata_column_definition\u0026gt;: column_name column_type METADATA [ FROM metadata_key ] [ VIRTUAL ] \u0026lt;computed_column_definition\u0026gt;: column_name AS computed_column_expression [COMMENT column_comment] \u0026lt;watermark_definition\u0026gt;: WATERMARK FOR rowtime_column_name AS watermark_strategy_expression \u0026lt;source_table\u0026gt;: [catalog_name.][db_name.]table_name \u0026lt;like_options\u0026gt;: { { INCLUDING | EXCLUDING } { ALL | CONSTRAINTS | PARTITIONS } | { INCLUDING | EXCLUDING | OVERWRITING } { GENERATED | OPTIONS | WATERMARKS } }[, ...] 根据指定的表名创建一个表，如果同名表已经在 catalog 中存在了，则无法注册。
Columns # Physical / Regular Columns
Physical columns are regular columns known from databases. They define the names, the types, and the order of fields in the physical data. Thus, physical columns represent the payload that is read from and written to an external system. Connectors and formats use these columns (in the defined order) to configure themselves. Other kinds of columns can be declared between physical columns but will not influence the final physical schema.
The following statement creates a table with only regular columns:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING ) WITH ( ... ); Metadata Columns
Metadata columns are an extension to the SQL standard and allow to access connector and/or format specific fields for every row of a table. A metadata column is indicated by the METADATA keyword. For example, a metadata column can be be used to read and write the timestamp from and to Kafka records for time-based operations. The connector and format documentation lists the available metadata fields for every component. However, declaring a metadata column in a table\u0026rsquo;s schema is optional.
The following statement creates a table with an additional metadata column that references the metadata field timestamp:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`record_time\` TIMESTAMP_LTZ(3) METADATA FROM \u0026#39;timestamp\u0026#39; -- reads and writes a Kafka record\u0026#39;s timestamp ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); Every metadata field is identified by a string-based key and has a documented data type. For example, the Kafka connector exposes a metadata field with key timestamp and data type TIMESTAMP_LTZ(3) that can be used for both reading and writing records.
In the example above, the metadata column record_time becomes part of the table\u0026rsquo;s schema and can be transformed and stored like a regular column:
INSERT INTO MyTable SELECT user_id, name, record_time + INTERVAL \u0026#39;1\u0026#39; SECOND FROM MyTable; For convenience, the FROM clause can be omitted if the column name should be used as the identifying metadata key:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`timestamp\` TIMESTAMP_LTZ(3) METADATA -- use column name as metadata key ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); For convenience, the runtime will perform an explicit cast if the data type of the column differs from the data type of the metadata field. Of course, this requires that the two data types are compatible.
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`name\` STRING, \`timestamp\` BIGINT METADATA -- cast the timestamp as BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); By default, the planner assumes that a metadata column can be used for both reading and writing. However, in many cases an external system provides more read-only metadata fields than writable fields. Therefore, it is possible to exclude metadata columns from persisting using the VIRTUAL keyword.
CREATE TABLE MyTable ( \`timestamp\` BIGINT METADATA, -- part of the query-to-sink schema \`offset\` BIGINT METADATA VIRTUAL, -- not part of the query-to-sink schema \`user_id\` BIGINT, \`name\` STRING, ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); In the example above, the offset is a read-only metadata column and excluded from the query-to-sink schema. Thus, source-to-query schema (for SELECT) and query-to-sink (for INSERT INTO) schema differ:
source-to-query schema: MyTable(\`timestamp\` BIGINT, \`offset\` BIGINT, \`user_id\` BIGINT, \`name\` STRING) query-to-sink schema: MyTable(\`timestamp\` BIGINT, \`user_id\` BIGINT, \`name\` STRING) Computed Columns
Computed columns are virtual columns that are generated using the syntax column_name AS computed_column_expression.
A computed column evaluates an expression that can reference other columns declared in the same table. Both physical columns and metadata columns can be accessed. The column itself is not physically stored within the table. The column\u0026rsquo;s data type is derived automatically from the given expression and does not have to be declared manually.
The planner will transform computed columns into a regular projection after the source. For optimization or watermark strategy push down, the evaluation might be spread across operators, performed multiple times, or skipped if not needed for the given query.
For example, a computed column could be defined as:
CREATE TABLE MyTable ( \`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE, \`cost\` AS price * quanitity, -- evaluate expression and supply the result to queries ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39; ... ); The expression may contain any combination of columns, constants, or functions. The expression cannot contain a subquery.
Computed columns are commonly used in Flink for defining time attributes in CREATE TABLE statements.
A processing time attribute can be defined easily via proc AS PROCTIME() using the system\u0026rsquo;s PROCTIME() function. An event time attribute timestamp can be pre-processed before the WATERMARK declaration. For example, the computed column can be used if the original field is not TIMESTAMP(3) type or is nested in a JSON string. Similar to virtual metadata columns, computed columns are excluded from persisting. Therefore, a computed column cannot be the target of an INSERT INTO statement. Thus, source-to-query schema (for SELECT) and query-to-sink (for INSERT INTO) schema differ:
source-to-query schema: MyTable(\`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE, \`cost\` DOUBLE) query-to-sink schema: MyTable(\`user_id\` BIGINT, \`price\` DOUBLE, \`quantity\` DOUBLE) WATERMARK # WATERMARK 定义了表的事件时间属性，其形式为 WATERMARK FOR rowtime_column_name AS watermark_strategy_expression 。
rowtime_column_name 把一个现有的列定义为一个为表标记事件时间的属性。该列的类型必须为 TIMESTAMP(3)，且是 schema 中的顶层列，它也可以是一个计算列。
watermark_strategy_expression 定义了 watermark 的生成策略。它允许使用包括计算列在内的任意非查询表达式来计算 watermark ；表达式的返回类型必须是 TIMESTAMP(3)，表示了从 Epoch 以来的经过的时间。 返回的 watermark 只有当其不为空且其值大于之前发出的本地 watermark 时才会被发出（以保证 watermark 递增）。每条记录的 watermark 生成表达式计算都会由框架完成。 框架会定期发出所生成的最大的 watermark ，如果当前 watermark 仍然与前一个 watermark 相同、为空、或返回的 watermark 的值小于最后一个发出的 watermark ，则新的 watermark 不会被发出。 Watermark 根据 pipeline.auto-watermark-interval 中所配置的间隔发出。 若 watermark 的间隔是 0ms ，那么每条记录都会产生一个 watermark，且 watermark 会在不为空并大于上一个发出的 watermark 时发出。
使用事件时间语义时，表必须包含事件时间属性和 watermark 策略。
Flink 提供了几种常用的 watermark 策略。
严格递增时间戳： WATERMARK FOR rowtime_column AS rowtime_column。
发出到目前为止已观察到的最大时间戳的 watermark ，时间戳大于最大时间戳的行被认为没有迟到。
递增时间戳： WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND。
发出到目前为止已观察到的最大时间戳减 1 的 watermark ，时间戳大于或等于最大时间戳的行被认为没有迟到。
有界乱序时间戳： WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL 'string' timeUnit。
发出到目前为止已观察到的最大时间戳减去指定延迟的 watermark ，例如， WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '5' SECOND 是一个 5 秒延迟的 watermark 策略。
CREATE TABLE Orders ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( . . . ); PRIMARY KEY # 主键用作 Flink 优化的一种提示信息。主键限制表明一张表或视图的某个（些）列是唯一的并且不包含 Null 值。 主键声明的列都是非 nullable 的。因此主键可以被用作表行级别的唯一标识。
主键可以和列的定义一起声明，也可以独立声明为表的限制属性，不管是哪种方式，主键都不可以重复定义，否则 Flink 会报错。
有效性检查
SQL 标准主键限制可以有两种模式：ENFORCED 或者 NOT ENFORCED。 它申明了是否输入/出数据会做合法性检查（是否唯一）。Flink 不存储数据因此只支持 NOT ENFORCED 模式，即不做检查，用户需要自己保证唯一性。
Flink 假设声明了主键的列都是不包含 Null 值的，Connector 在处理数据时需要自己保证语义正确。
Notes: 在 CREATE TABLE 语句中，创建主键会修改列的 nullable 属性，主键声明的列默认都是非 Nullable 的。
PARTITIONED BY # 根据指定的列对已经创建的表进行分区。若表使用 filesystem sink ，则将会为每个分区创建一个目录。
WITH Options # 表属性用于创建 table source/sink ，一般用于寻找和创建底层的连接器。
表达式 key1=val1 的键和值必须为字符串文本常量。请参考 连接外部系统 了解不同连接器所支持的属性。
注意： 表名可以为以下三种格式 1. catalog_name.db_name.table_name 2. db_name.table_name 3. table_name。使用catalog_name.db_name.table_name 的表将会与名为 \u0026ldquo;catalog_name\u0026rdquo; 的 catalog 和名为 \u0026ldquo;db_name\u0026rdquo; 的数据库一起注册到 metastore 中。使用 db_name.table_name 的表将会被注册到当前执行的 table environment 中的 catalog 且数据库会被命名为 \u0026ldquo;db_name\u0026rdquo;；对于 table_name, 数据表将会被注册到当前正在运行的catalog和数据库中。
注意： 使用 CREATE TABLE 语句注册的表均可用作 table source 和 table sink。 在被 DML 语句引用前，我们无法决定其实际用于 source 抑或是 sink。
LIKE # LIKE 子句来源于两种 SQL 特性的变体/组合（Feature T171，“表定义中的 LIKE 语法” 和 Feature T173，“表定义中的 LIKE 语法扩展”）。LIKE 子句可以基于现有表的定义去创建新表，并且可以扩展或排除原始表中的某些部分。与 SQL 标准相反，LIKE 子句必须在 CREATE 语句中定义，并且是基于 CREATE 语句的更上层定义，这是因为 LIKE 子句可以用于定义表的多个部分，而不仅仅是 schema 部分。
你可以使用该子句，重用（或改写）指定的连接器配置属性或者可以向外部表添加 watermark 定义，例如可以向 Apache Hive 中定义的表添加 watermark 定义。
示例如下：
CREATE TABLE Orders ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39; ); CREATE TABLE Orders_with_watermark ( -- 添加 watermark 定义 WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( -- 改写 startup-mode 属性 \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39; ) LIKE Orders; 结果表 Orders_with_watermark 等效于使用以下语句创建的表：
CREATE TABLE Orders_with_watermark ( \`user\` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39; ); 表属性的合并逻辑可以用 like options 来控制。
可以控制合并的表属性如下：
CONSTRAINTS - 主键和唯一键约束 GENERATED - 计算列 OPTIONS - 连接器信息、格式化方式等配置项 PARTITIONS - 表分区信息 WATERMARKS - watermark 定义 并且有三种不同的表属性合并策略：
INCLUDING - 新表包含源表（source table）所有的表属性，如果和源表的表属性重复则会直接失败，例如新表和源表存在相同 key 的属性。 EXCLUDING - 新表不包含源表指定的任何表属性。 OVERWRITING - 新表包含源表的表属性，但如果出现重复项，则会用新表的表属性覆盖源表中的重复表属性，例如，两个表中都存在相同 key 的属性，则会使用当前语句中定义的 key 的属性值。 并且你可以使用 INCLUDING/EXCLUDING ALL 这种声明方式来指定使用怎样的合并策略，例如使用 EXCLUDING ALL INCLUDING WATERMARKS，那么代表只有源表的 WATERMARKS 属性才会被包含进新表。
示例如下：
-- 存储在文件系统的源表 CREATE TABLE Orders_in_file ( \`user\` BIGINT, product STRING, order_time_string STRING, order_time AS to_timestamp(order_time) ) PARTITIONED BY (\`user\`) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;...\u0026#39; ); -- 对应存储在 kafka 的源表 CREATE TABLE Orders_in_kafka ( -- 添加 watermark 定义 WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, ... ) LIKE Orders_in_file ( -- 排除需要生成 watermark 的计算列之外的所有内容。 -- 去除不适用于 kafka 的所有分区和文件系统的相关属性。 EXCLUDING ALL INCLUDING GENERATED ); 如果未提供 like 配置项（like options），默认将使用 INCLUDING ALL OVERWRITING OPTIONS 的合并策略。
注意： 您无法选择物理列的合并策略，当物理列进行合并时就如使用了 INCLUDING 策略。
注意： 源表 source_table 可以是一个组合 ID。您可以指定不同 catalog 或者 DB 的表作为源表: 例如，my_catalog.my_db.MyTable 指定了源表 MyTable 来源于名为 MyCatalog 的 catalog 和名为 my_db 的 DB ，my_db.MyTable 指定了源表 MyTable 来源于当前 catalog 和名为 my_db 的 DB。
Back to top
CREATE CATALOG # CREATE CATALOG catalog_name WITH (key1=val1, key2=val2, ...) Create a catalog with the given catalog properties. If a catalog with the same name already exists, an exception is thrown.
WITH OPTIONS
Catalog properties used to store extra information related to this catalog. The key and value of expression key1=val1 should both be string literal.
Check out more details at Catalogs.
Back to top
CREATE DATABASE # CREATE DATABASE [IF NOT EXISTS] [catalog_name.]db_name [COMMENT database_comment] WITH (key1=val1, key2=val2, ...) 根据给定的表属性创建数据库。若数据库中已存在同名表会抛出异常。
IF NOT EXISTS
若数据库已经存在，则不会进行任何操作。
WITH OPTIONS
数据库属性一般用于存储关于这个数据库额外的信息。 表达式 key1=val1 中的键和值都需要是字符串文本常量。
Back to top
CREATE VIEW # CREATE [TEMPORARY] VIEW [IF NOT EXISTS] [catalog_name.][db_name.]view_name [{columnName [, columnName ]* }] [COMMENT view_comment] AS query_expression 根据给定的 query 语句创建一个视图。若数据库中已经存在同名视图会抛出异常.
TEMPORARY
创建一个有 catalog 和数据库命名空间的临时视图，并覆盖原有的视图。
IF NOT EXISTS
若该视图已经存在，则不会进行任何操作。
Back to top
CREATE FUNCTION # CREATE [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF NOT EXISTS] [[catalog_name.]db_name.]function_name AS identifier [LANGUAGE JAVA|SCALA|PYTHON] 创建一个有 catalog 和数据库命名空间的 catalog function ，需要指定一个 identifier ，可指定 language tag 。 若 catalog 中，已经有同名的函数注册了，则无法注册。
如果 language tag 是 JAVA 或者 SCALA ，则 identifier 是 UDF 实现类的全限定名。关于 JAVA/SCALA UDF 的实现，请参考 自定义函数。
如果 language tag 是 PYTHON，则 identifier 是 UDF 对象的全限定名，例如 pyflink.table.tests.test_udf.add。关于 PYTHON UDF 的实现，请参考 Python UDFs。
如果 language tag 是 PYTHON，而当前程序是 Java／Scala 程序或者纯 SQL 程序，则需要配置 Python 相关的依赖。
TEMPORARY
创建一个有 catalog 和数据库命名空间的临时 catalog function ，并覆盖原有的 catalog function 。
TEMPORARY SYSTEM
创建一个没有数据库命名空间的临时系统 catalog function ，并覆盖系统内置的函数。
IF NOT EXISTS
若该函数已经存在，则不会进行任何操作。
LANGUAGE JAVA|SCALA|PYTHON
Language tag 用于指定 Flink runtime 如何执行这个函数。目前，只支持 JAVA, SCALA 和 PYTHON，且函数的默认语言为 JAVA。
`}),e.add({id:106,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/csv/",title:"CSV",section:"Formats",content:` CSV format # To use the CSV format you need to add the Flink CSV dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-csv\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading CSV files using CsvReaderFormat. The reader utilizes Jackson library and allows passing the corresponding configuration for the CSV schema and parsing options.
CsvReaderFormat can be initialized and used like this:
CsvReaderFormat\u0026lt;SomePojo\u0026gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class); FileSource\u0026lt;SomePojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); The schema for CSV parsing, in this case, is automatically derived based on the fields of the SomePojo class using the Jackson library.
Note: you might need to add @JsonPropertyOrder({field1, field2, ...}) annotation to your class definition with the fields order exactly matching those of the CSV file columns. Advanced configuration # If you need more fine-grained control over the CSV schema or the parsing options, use the more low-level forSchema static factory method of CsvReaderFormat:
CsvReaderFormat\u0026lt;T\u0026gt; forSchema(Supplier\u0026lt;CsvMapper\u0026gt; mapperFactory, Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator, TypeInformation\u0026lt;T\u0026gt; typeInformation) Below is an example of reading a POJO with a custom columns\u0026rsquo; separator:
//Has to match the exact order of columns in the CSV file @JsonPropertyOrder({\u0026#34;city\u0026#34;,\u0026#34;lat\u0026#34;,\u0026#34;lng\u0026#34;,\u0026#34;country\u0026#34;,\u0026#34;iso2\u0026#34;, \u0026#34;adminName\u0026#34;,\u0026#34;capital\u0026#34;,\u0026#34;population\u0026#34;}) public static class CityPojo { public String city; public BigDecimal lat; public BigDecimal lng; public String country; public String iso2; public String adminName; public String capital; public long population; } Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator = mapper -\u0026gt; mapper.schemaFor(CityPojo.class).withoutQuoteChar().withColumnSeparator(\u0026#39;|\u0026#39;); CsvReaderFormat\u0026lt;CityPojo\u0026gt; csvFormat = CsvReaderFormat.forSchema(() -\u0026gt; new CsvMapper(), schemaGenerator, TypeInformation.of(CityPojo.class)); FileSource\u0026lt;CityPojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); The corresponding CSV file:
Berlin|52.5167|13.3833|Germany|DE|Berlin|primary|3644826 San Francisco|37.7562|-122.443|United States|US|California||3592294 Beijing|39.905|116.3914|China|CN|Beijing|primary|19433000 It is also possible to read more complex data types using fine-grained Jackson settings:
public static class ComplexPojo { private long id; private int[] array; } CsvReaderFormat\u0026lt;ComplexPojo\u0026gt; csvFormat = CsvReaderFormat.forSchema( CsvSchema.builder() .addColumn( new CsvSchema.Column(0, \u0026#34;id\u0026#34;, CsvSchema.ColumnType.NUMBER)) .addColumn( new CsvSchema.Column(4, \u0026#34;array\u0026#34;, CsvSchema.ColumnType.ARRAY) .withArrayElementSeparator(\u0026#34;#\u0026#34;)) .build(), TypeInformation.of(ComplexPojo.class)); For PyFlink users, a csv schema can be defined by manually adding columns, and the output type of the csv source will be a Row with each column mapped to a field.
schema = CsvSchema.builder() \\ .add_number_column(\u0026#39;id\u0026#39;, number_type=DataTypes.BIGINT()) \\ .add_array_column(\u0026#39;array\u0026#39;, separator=\u0026#39;#\u0026#39;, element_type=DataTypes.INT()) \\ .set_column_separator(\u0026#39;,\u0026#39;) \\ .build() source = FileSource.for_record_stream_format( CsvReaderFormat.for_schema(schema), CSV_FILE_PATH).build() # the type of record will be Types.ROW_NAMED([\u0026#39;id\u0026#39;, \u0026#39;array\u0026#39;], [Types.LONG(), Types.LIST(Types.INT())]) ds = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#39;csv-source\u0026#39;) The corresponding CSV file:
0,1#2#3 1, 2,1 Similarly to the TextLineInputFormat, CsvReaderFormat can be used in both continues and batch modes (see TextLineInputFormat for examples).
For PyFlink users, CsvBulkWriters could be used to create BulkWriterFactory to write records to files in CSV format.
schema = CsvSchema.builder() \\ .add_number_column(\u0026#39;id\u0026#39;, number_type=DataTypes.BIGINT()) \\ .add_array_column(\u0026#39;array\u0026#39;, separator=\u0026#39;#\u0026#39;, element_type=DataTypes.INT()) \\ .set_column_separator(\u0026#39;,\u0026#39;) \\ .build() sink = FileSink.for_bulk_format( OUTPUT_DIR, CsvBulkWriters.for_schema(schema)).build() ds.sink_to(sink) `}),e.add({id:107,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/docker/",title:"Docker 设置",section:"Standalone",content:` Docker Setup # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate containers) of a Flink cluster using Docker containers.
Introduction # Docker is a popular container runtime. There are official Docker images for Apache Flink available on Docker Hub. You can use the Docker images to deploy a Session or Application cluster on Docker. This page focuses on the setup of Flink on Docker, Docker Swarm and Docker Compose.
Deployment into managed containerized environments, such as standalone Kubernetes or native Kubernetes, are described on separate pages.
Starting a Session Cluster on Docker # A Flink Session cluster can be used to run multiple jobs. Each job needs to be submitted to the cluster after the cluster has been deployed. To deploy a Flink Session cluster with Docker, you need to start a JobManager container. To enable communication between the containers, we first set a required Flink configuration property and create a network:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: jobmanager\u0026#34; \$ docker network create flink-network Then we launch the JobManager:
\$ docker run \\ --rm \\ --name=jobmanager \\ --network flink-network \\ --publish 8081:8081 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest jobmanager and one or more TaskManager containers:
\$ docker run \\ --rm \\ --name=taskmanager \\ --network flink-network \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest taskmanager The web interface is now available at localhost:8081.
Submission of a job is now possible like this (assuming you have a local distribution of Flink available):
\$ ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar To shut down the cluster, either terminate (e.g. with CTRL-C) the JobManager and TaskManager processes, or use docker ps to identify and docker stop to terminate the containers.
Deployment Modes # The Flink image contains a regular Flink distribution with its default configuration and a standard entry point script. You can run its entry point in the following modes:
JobManager for a Session cluster JobManager for a Application cluster TaskManager for any cluster This allows you to deploy a standalone cluster (Session or Application Mode) in any containerised environment, for example:
manually in a local Docker setup, in a Kubernetes cluster, with Docker Compose, with Docker swarm. Note The native Kubernetes also runs the same image by default and deploys TaskManagers on demand so that you do not have to do it manually.
The next chapters describe how to start a single Flink Docker container for various purposes.
Once you\u0026rsquo;ve started Flink on Docker, you can access the Flink Webfrontend on localhost:8081 or submit jobs like this ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar.
We recommend using Docker Compose or Docker Swarm for deploying Flink in Session Mode to ease system configuration.
Application Mode on Docker # A Flink Application cluster is a dedicated cluster which runs a single job. In this case, you deploy the cluster with the job as one step, thus, there is no extra job submission needed.
The job artifacts are included into the class path of Flink\u0026rsquo;s JVM process within the container and consist of:
your job jar, which you would normally submit to a Session cluster and all other necessary dependencies or resources, not included into Flink. To deploy a cluster for a single job with Docker, you need to
make job artifacts available locally in all containers under /opt/flink/usrlib, start a JobManager container in the Application cluster mode start the required number of TaskManager containers. To make the job artifacts available locally in the container, you can
either mount a volume (or multiple volumes) with the artifacts to /opt/flink/usrlib when you start the JobManager and TaskManagers:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: jobmanager\u0026#34; \$ docker network create flink-network \$ docker run \\ --mount type=bind,src=/host/path/to/job/artifacts1,target=/opt/flink/usrlib/artifacts1 \\ --mount type=bind,src=/host/path/to/job/artifacts2,target=/opt/flink/usrlib/artifacts2 \\ --rm \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --name=jobmanager \\ --network flink-network \\ flink:latest standalone-job \\ --job-classname com.job.ClassName \\ [--job-id \u0026lt;job id\u0026gt;] \\ [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] \\ [job arguments] \$ docker run \\ --mount type=bind,src=/host/path/to/job/artifacts1,target=/opt/flink/usrlib/artifacts1 \\ --mount type=bind,src=/host/path/to/job/artifacts2,target=/opt/flink/usrlib/artifacts2 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ flink:latest taskmanager or extend the Flink image by writing a custom Dockerfile, build it and use it for starting the JobManager and TaskManagers:
FROM flink ADD /host/path/to/job/artifacts/1 /opt/flink/usrlib/artifacts/1 ADD /host/path/to/job/artifacts/2 /opt/flink/usrlib/artifacts/2 \$ docker build --tag flink_with_job_artifacts . \$ docker run \\ flink_with_job_artifacts standalone-job \\ --job-classname com.job.ClassName \\ [--job-id \u0026lt;job id\u0026gt;] \\ [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] \\ [job arguments] \$ docker run flink_with_job_artifacts taskmanager The standalone-job argument starts a JobManager container in the Application Mode.
JobManager additional command line arguments # You can provide the following additional command line arguments to the cluster entrypoint:
--job-classname \u0026lt;job class name\u0026gt;: Class name of the job to run.
By default, Flink scans its class path for a JAR with a Main-Class or program-class manifest entry and chooses it as the job class. Use this command line argument to manually set the job class. This argument is required in case that no or more than one JAR with such a manifest entry is available on the class path.
--job-id \u0026lt;job id\u0026gt; (optional): Manually set a Flink job ID for the job (default: 00000000000000000000000000000000)
--fromSavepoint /path/to/savepoint (optional): Restore from a savepoint
In order to resume from a savepoint, you also need to pass the savepoint path. Note that /path/to/savepoint needs to be accessible in all Docker containers of the cluster (e.g., storing it on a DFS or from the mounted volume or adding it to the image).
--allowNonRestoredState (optional): Skip broken savepoint state
Additionally you can specify this argument to allow that savepoint state is skipped which cannot be restored.
If the main function of the user job main class accepts arguments, you can also pass them at the end of the docker run command.
Per-Job Mode on Docker # Per-Job Mode is not supported by Flink on Docker.
Session Mode on Docker # Local deployment in the Session Mode has already been described in the Getting Started section above.
Back to top
Flink on Docker Reference # Image hosting # There are two distribution channels for the Flink Docker images:
Official Flink images on Docker Hub (reviewed and build by Docker) Flink images on Docker Hub apache/flink (managed by the Flink developers) We recommend using the official images on Docker Hub, as they are reviewed by Docker. The images on apache/flink are provided in case of delays in the review process by Docker.
Launching an image named flink:latest will pull the latest image from Docker Hub. In order to use the images hosted in apache/flink, replace flink by apache/flink. Any of the image tags (starting from Flink 1.11.3) are available on apache/flink as well.
Image tags # The Flink Docker repository is hosted on Docker Hub and serves images of Flink version 1.2.1 and later. The source for these images can be found in the Apache flink-docker repository.
Images for each supported combination of Flink and Scala versions are available, and tag aliases are provided for convenience.
For example, you can use the following aliases:
flink:latest → flink:\u0026lt;latest-flink\u0026gt;-scala_\u0026lt;latest-scala\u0026gt; flink:1.11 → flink:1.11.\u0026lt;latest-flink-1.11\u0026gt;-scala_2.12 Note It is recommended to always use an explicit version tag of the docker image that specifies both the needed Flink and Scala versions (for example flink:1.11-scala_2.12). This will avoid some class conflicts that can occur if the Flink and/or Scala versions used in the application are different from the versions provided by the docker image.
Note Prior to Flink 1.5 version, Hadoop dependencies were always bundled with Flink. You can see that certain tags include the version of Hadoop, e.g. (e.g. -hadoop28). Beginning with Flink 1.5, image tags that omit the Hadoop version correspond to Hadoop-free releases of Flink that do not include a bundled Hadoop distribution.
Passing configuration via via dynamic properties # \$ docker run flink:latest \\ \u0026lt;jobmanager|standalone-job|taskmanager|historyserver\u0026gt; \\ -D jobmanager.rpc.address=host \\ -D taskmanager.numberOfTaskSlots=3 \\ -D blob.server.port=6124 Options set via dynamic properties overwrite the options from flink-conf.yaml.
Passing configuration via environment variables # When you run Flink image, you can also change its configuration options by setting the environment variable FLINK_PROPERTIES:
\$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: host taskmanager.numberOfTaskSlots: 3 blob.server.port: 6124 \u0026#34; \$ docker run --env FLINK_PROPERTIES=\${FLINK_PROPERTIES} flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; The jobmanager.rpc.address option must be configured, others are optional to set.
The environment variable FLINK_PROPERTIES should contain a list of Flink cluster configuration options separated by new line, the same way as in the flink-conf.yaml. FLINK_PROPERTIES takes precedence over configurations in flink-conf.yaml.
Provide custom configuration # The configuration files (flink-conf.yaml, logging, hosts etc) are located in the /opt/flink/conf directory in the Flink image. To provide a custom location for the Flink configuration files, you can
either mount a volume with the custom configuration files to this path /opt/flink/conf when you run the Flink image:
\$ docker run \\ --mount type=bind,src=/host/path/to/custom/conf,target=/opt/flink/conf \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; or add them to your custom Flink image, build and run it:
FROM flink ADD /host/path/to/flink-conf.yaml /opt/flink/conf/flink-conf.yaml ADD /host/path/to/log4j.properties /opt/flink/conf/log4j.properties The mounted volume must contain all necessary configuration files. The flink-conf.yaml file must have write permission so that the Docker entry point script can modify it in certain cases. Using filesystem plugins # As described in the plugins documentation page: In order to use plugins they must be copied to the correct location in the Flink installation in the Docker container for them to work.
If you want to enable plugins provided with Flink (in the opt/ directory of the Flink distribution), you can pass the environment variable ENABLE_BUILT_IN_PLUGINS when you run the Flink image. The ENABLE_BUILT_IN_PLUGINS should contain a list of plugin jar file names separated by ;. A valid plugin name is for example flink-s3-fs-hadoop-1.16-SNAPSHOT.jar
\$ docker run \\ --env ENABLE_BUILT_IN_PLUGINS=flink-plugin1.jar;flink-plugin2.jar \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; There are also more advanced ways for customizing the Flink image.
Enabling Python # To build a custom image which has Python and PyFlink prepared, you can refer to the following Dockerfile:
FROM flink:latest # install python3: it has updated Python to 3.9 in Debian 11 and so install Python 3.7 from source # it currently only supports Python 3.6, 3.7 and 3.8 in PyFlink officially. RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev libffi-dev \u0026amp;\u0026amp; \\ wget https://www.python.org/ftp/python/3.7.9/Python-3.7.9.tgz \u0026amp;\u0026amp; \\ tar -xvf Python-3.7.9.tgz \u0026amp;\u0026amp; \\ cd Python-3.7.9 \u0026amp;\u0026amp; \\ ./configure --without-tests --enable-shared \u0026amp;\u0026amp; \\ make -j6 \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ ldconfig /usr/local/lib \u0026amp;\u0026amp; \\ cd .. \u0026amp;\u0026amp; rm -f Python-3.7.9.tgz \u0026amp;\u0026amp; rm -rf Python-3.7.9 \u0026amp;\u0026amp; \\ ln -s /usr/local/bin/python3 /usr/local/bin/python \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* # install PyFlink COPY apache-flink*.tar.gz / RUN pip3 install /apache-flink-libraries*.tar.gz \u0026amp;\u0026amp; pip3 install /apache-flink*.tar.gz Note For Debian 10 and below, Python 3 could also be installed alternatively as following:
RUN apt-get update -y \u0026amp;\u0026amp; \\ apt-get install -y python3.7 python3-pip python3.7-dev \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN ln -s /usr/bin/python3 /usr/bin/python Note PyFlink packages could be built-in according to development guide Build the image named as pyflink:latest:
\$ docker build --tag pyflink:latest . Switch memory allocator # Flink introduced jemalloc as default memory allocator to resolve memory fragmentation problem (please refer to FLINK-19125).
You could switch back to use glibc as the memory allocator to restore the old behavior or if any unexpected memory consumption or problem observed (and please report the issue via JIRA or mailing list if you found any), by setting environment variable DISABLE_JEMALLOC as true:
\$ docker run \\ --env DISABLE_JEMALLOC=true \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; For users that are still using glibc memory allocator, the glibc bug can easily be reproduced, especially while savepoints or full checkpoints with RocksDBStateBackend are created. Setting the environment variable MALLOC_ARENA_MAX can avoid unlimited memory growth:
\$ docker run \\ --env MALLOC_ARENA_MAX=1 \\ flink:latest \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; Advanced customization # There are several ways in which you can further customize the Flink image:
install custom software (e.g. python) enable (symlink) optional libraries or plugins from /opt/flink/opt into /opt/flink/lib or /opt/flink/plugins add other libraries to /opt/flink/lib (e.g. Hadoop) add other plugins to /opt/flink/plugins You can customize the Flink image in several ways:
override the container entry point with a custom script where you can run any bootstrap actions. At the end you can call the standard /docker-entrypoint.sh script of the Flink image with the same arguments as described in supported deployment modes.
The following example creates a custom entry point script which enables more libraries and plugins. The custom script, custom library and plugin are provided from a mounted volume. Then it runs the standard entry point script of the Flink image:
# create custom_lib.jar # create custom_plugin.jar \$ echo \u0026#34; # enable an optional library ln -fs /opt/flink/opt/flink-queryable-state-runtime-*.jar /opt/flink/lib/ # enable a custom library ln -fs /mnt/custom_lib.jar /opt/flink/lib/ mkdir -p /opt/flink/plugins/flink-s3-fs-hadoop # enable an optional plugin ln -fs /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/flink-s3-fs-hadoop/ mkdir -p /opt/flink/plugins/custom_plugin # enable a custom plugin ln -fs /mnt/custom_plugin.jar /opt/flink/plugins/custom_plugin/ /docker-entrypoint.sh \u0026lt;jobmanager|standalone-job|taskmanager\u0026gt; \u0026#34; \u0026gt; custom_entry_point_script.sh \$ chmod 755 custom_entry_point_script.sh \$ docker run \\ --mount type=bind,src=\$(pwd),target=/mnt flink:latest /mnt/custom_entry_point_script.sh extend the Flink image by writing a custom Dockerfile and build a custom image:
FROM flink RUN set -ex; apt-get update; apt-get -y install python ADD /host/path/to/flink-conf.yaml /container/local/path/to/custom/conf/flink-conf.yaml ADD /host/path/to/log4j.properties /container/local/path/to/custom/conf/log4j.properties RUN ln -fs /opt/flink/opt/flink-queryable-state-runtime-*.jar /opt/flink/lib/. RUN mkdir -p /opt/flink/plugins/flink-s3-fs-hadoop RUN ln -fs /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/flink-s3-fs-hadoop/. ENV VAR_NAME value Commands for building:
\$ docker build --tag custom_flink_image . # optional push to your docker image registry if you have it, # e.g. to distribute the custom image to your cluster \$ docker push custom_flink_image Flink with Docker Compose # Docker Compose is a way to run a group of Docker containers locally. The next sections show examples of configuration files to run Flink.
Usage # Create the yaml files with the container configuration, check examples for:
Application cluster Session cluster See also the Flink Docker image tags and how to customize the Flink Docker image for usage in the configuration files.
Launch a cluster in the foreground (use -d for background)
\$ docker-compose up Scale the cluster up or down to N TaskManagers
\$ docker-compose scale taskmanager=\u0026lt;N\u0026gt; Access the JobManager container
\$ docker exec -it \$(docker ps --filter name=jobmanager --format={{.ID}}) /bin/sh Kill the cluster
\$ docker-compose kill Access Web UI
When the cluster is running, you can visit the web UI at http://localhost:8081. You can also use the web UI to submit a job to a Session cluster.
To submit a job to a Session cluster via the command line, you can either
use Flink CLI on the host if it is installed:
\$ ./bin/flink run --detached --class \${JOB_CLASS_NAME} /job.jar or copy the JAR to the JobManager container and submit the job using the CLI from there, for example:
\$ JOB_CLASS_NAME=\u0026#34;com.job.ClassName\u0026#34; \$ JM_CONTAINER=\$(docker ps --filter name=jobmanager --format={{.ID}})) \$ docker cp path/to/jar \u0026#34;\${JM_CONTAINER}\u0026#34;:/job.jar \$ docker exec -t -i \u0026#34;\${JM_CONTAINER}\u0026#34; flink run -d -c \${JOB_CLASS_NAME} /job.jar Here, we provide the docker-compose.yml for Application Cluster.
Note: For the Application Mode cluster, the artifacts must be available in the Flink containers, check details here. See also how to specify the JobManager arguments in the command for the jobmanager service.
version: \u0026#34;2.2\u0026#34; services: jobmanager: image: flink:latest ports: - \u0026#34;8081:8081\u0026#34; command: standalone-job --job-classname com.job.ClassName [--job-id \u0026lt;job id\u0026gt;] [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] [job arguments] volumes: - /host/path/to/job/artifacts:/opt/flink/usrlib environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager parallelism.default: 2 taskmanager: image: flink:latest depends_on: - jobmanager command: taskmanager scale: 1 volumes: - /host/path/to/job/artifacts:/opt/flink/usrlib environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 parallelism.default: 2 As well as the configuration file for Session Cluster:
version: \u0026#34;2.2\u0026#34; services: jobmanager: image: flink:latest ports: - \u0026#34;8081:8081\u0026#34; command: jobmanager environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager: image: flink:latest depends_on: - jobmanager command: taskmanager scale: 1 environment: - | FLINK_PROPERTIES= jobmanager.rpc.address: jobmanager taskmanager.numberOfTaskSlots: 2 Flink with Docker Swarm # The Docker swarm is a container orchestration tool, that allows you to manage multiple containers deployed across multiple host machines.
The following chapters contain examples of how to configure and start JobManager and TaskManager containers. You can adjust them accordingly to start a cluster. See also the Flink Docker image tags and how to customize the Flink Docker image for usage in the provided scripts.
The port 8081 is exposed for the Flink Web UI access. If you run the swarm locally, you can visit the web UI at http://localhost:8081 after starting the cluster.
Session Cluster with Docker Swarm # \$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: flink-session-jobmanager taskmanager.numberOfTaskSlots: 2 \u0026#34; # Create overlay network \$ docker network create -d overlay flink-session # Create the JobManager service \$ docker service create \\ --name flink-session-jobmanager \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --publish 8081:8081 \\ --network flink-session \\ flink:latest \\ jobmanager # Create the TaskManager service (scale this out as needed) \$ docker service create \\ --name flink-session-taskmanager \\ --replicas 2 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --network flink-session \\ flink:latest \\ taskmanager Application Cluster with Docker Swarm # \$ FLINK_PROPERTIES=\u0026#34;jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 \u0026#34; # Create overlay network \$ docker network create -d overlay flink-job # Create the JobManager service \$ docker service create \\ --name flink-jobmanager \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --mount type=bind,source=/host/path/to/job/artifacts,target=/opt/flink/usrlib \\ --publish 8081:8081 \\ --network flink-job \\ flink:latest \\ standalone-job \\ --job-classname com.job.ClassName \\ [--job-id \u0026lt;job id\u0026gt;] \\ [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] \\ [job arguments] # Create the TaskManager service (scale this out as needed) \$ docker service create \\ --name flink-job-taskmanager \\ --replicas 2 \\ --env FLINK_PROPERTIES=\u0026#34;\${FLINK_PROPERTIES}\u0026#34; \\ --mount type=bind,source=/host/path/to/job/artifacts,target=/opt/flink/usrlib \\ --network flink-job \\ flink:latest \\ taskmanager The job artifacts must be available in the JobManager container, as outlined here. See also how to specify the JobManager arguments to pass them to the flink-jobmanager container.
The example assumes that you run the swarm locally and expects the job artifacts to be in /host/path/to/job/artifacts. It also mounts the host path with the artifacts as a volume to the container\u0026rsquo;s path /opt/flink/usrlib.
Back to top
`}),e.add({id:108,href:"/flink/flink-docs-master/zh/docs/concepts/flink-architecture/",title:"Flink 架构",section:"概念透析",content:` Flink 架构 # Flink 是一个分布式系统，需要有效分配和管理计算资源才能执行流应用程序。它集成了所有常见的集群资源管理器，例如Hadoop YARN，但也可以设置作为独立集群甚至库运行。
本节概述了 Flink 架构，并且描述了其主要组件如何交互以执行应用程序和从故障中恢复。
Flink 集群剖析 # Flink 运行时由两种类型的进程组成：一个 JobManager 和一个或者多个 TaskManager。
Client 不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。之后，客户端可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。客户端可以作为触发执行 Java/Scala 程序的一部分运行，也可以在命令行进程./bin/flink run ...中运行。
可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。
JobManager # JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：
ResourceManager
ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位（请参考TaskManagers）。Flink 为不同的环境和资源提供者（例如 YARN、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。
Dispatcher
Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。
JobMaster
JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。
始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby（请参考 高可用（HA））。
TaskManagers # TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。
必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子（请参考Tasks 和算子链）。
Back to top
Tasks 和算子链 # 对于分布式执行，Flink 将算子的 subtasks 链接成 tasks。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。链行为是可以配置的；请参考链文档以获取详细信息。
下图中样例数据流用 5 个 subtask 执行，因此有 5 个并行线程。
Back to top
Task Slots 和资源 # 每个 worker（TaskManager）都是一个 JVM 进程，可以在单独的线程中执行一个或多个 subtask。为了控制一个 TaskManager 中接受多少个 task，就有了所谓的 task slots（至少一个）。
每个 task slot 代表 TaskManager 中资源的固定子集。例如，具有 3 个 slot 的 TaskManager，会将其托管内存 1/3 用于每个 slot。分配资源意味着 subtask 不会与其他作业的 subtask 竞争托管内存，而是具有一定数量的保留托管内存。注意此处没有 CPU 隔离；当前 slot 仅分离 task 的托管内存。
通过调整 task slot 的数量，用户可以定义 subtask 如何互相隔离。每个 TaskManager 有一个 slot，这意味着每个 task 组都在单独的 JVM 中运行（例如，可以在单独的容器中启动）。具有多个 slot 意味着更多 subtask 共享同一 JVM。同一 JVM 中的 task 共享 TCP 连接（通过多路复用）和心跳信息。它们还可以共享数据集和数据结构，从而减少了每个 task 的开销。
默认情况下，Flink 允许 subtask 共享 slot，即便它们是不同的 task 的 subtask，只要是来自于同一作业即可。结果就是一个 slot 可以持有整个作业管道。允许 slot 共享有两个主要优点：
Flink 集群所需的 task slot 和作业中使用的最大并行度恰好一样。无需计算程序总共包含多少个 task（具有不同并行度）。
容易获得更好的资源利用。如果没有 slot 共享，非密集 subtask（source/map()）将阻塞和密集型 subtask（window） 一样多的资源。通过 slot 共享，我们示例中的基本并行度从 2 增加到 6，可以充分利用分配的资源，同时确保繁重的 subtask 在 TaskManager 之间公平分配。
Flink 应用程序执行 # Flink 应用程序 是从其 main() 方法产生的一个或多个 Flink 作业的任何用户程序。这些作业的执行可以在本地 JVM（LocalEnvironment）中进行，或具有多台机器的集群的远程设置（RemoteEnvironment）中进行。对于每个程序，ExecutionEnvironment 提供了一些方法来控制作业执行（例如设置并行度）并与外界交互（请参考 Flink 程序剖析 ）。
Flink 应用程序的作业可以被提交到长期运行的 Flink Session 集群、专用的 Flink Job 集群 或 Flink Application 集群。这些选项之间的差异主要与集群的生命周期和资源隔离保证有关。
Flink Session 集群 # 集群生命周期：在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群，该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。因此，Flink Session 集群的寿命不受任何 Flink 作业寿命的约束。
资源隔离：TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争 — 例如提交工作阶段的网络带宽。此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；类似的，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。
其他注意事项：拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响 — 就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。
以前，Flink Session 集群也被称为 session 模式下的 Flink 集群。 Flink Job 集群 # 集群生命周期：在 Flink Job 集群中，可用的集群管理器（例如 YARN）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。在这里，客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。一旦作业完成，Flink Job 集群将被拆除。
资源隔离：JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。
其他注意事项：由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业。
以前，Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。 Kubernetes 不支持 Flink Job 集群。 请参考 Standalone Kubernetes 和 Native Kubernetes。 Flink Application 集群 # 集群生命周期：Flink Application 集群是专用的 Flink 集群，仅从 Flink 应用程序执行作业，并且 main()方法在集群上而不是客户端上运行。提交作业是一个单步骤过程：无需先启动 Flink 集群，然后将作业提交到现有的 session 集群；相反，将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，并且集群入口（ApplicationClusterEntryPoint）负责调用 main()方法来提取 JobGraph。例如，这允许你像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。
资源隔离：在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。
Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。 Back to top
`}),e.add({id:109,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/hadoop/",title:"Hadoop",section:"Formats",content:` Hadoop formats # Project Configuration # 对 Hadoop 的支持位于 flink-hadoop-compatibility Maven 模块中。
将以下依赖添加到 pom.xml 中使用 hadoop
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 如果你想在本地运行你的 Flink 应用（例如在 IDE 中），你需要按照如下所示将 hadoop-client 依赖也添加到 pom.xml：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Using Hadoop InputFormats # 在 Flink 中使用 Hadoop InputFormats，必须首先使用 HadoopInputs 工具类的 readHadoopFile 或 createHadoopInput 包装 Input Format。 前者用于从 FileInputFormat 派生的 Input Format，而后者必须用于通用的 Input Format。 生成的 InputFormat 可通过使用 ExecutionEnvironment#createInput 创建数据源。
生成的 DataStream 包含 2 元组，其中第一个字段是键，第二个字段是从 Hadoop InputFormat 接收的值。
下面的示例展示了如何使用 Hadoop 的 TextInputFormat。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath)); // 对数据进行一些处理。 [...] Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val input: DataStream[(LongWritable, Text)] = env.createInput(HadoopInputs.readHadoopFile( new TextInputFormat, classOf[LongWritable], classOf[Text], textPath)) // 对数据进行一些处理。 [...] Using Hadoop OutputFormats # Flink 为 Hadoop OutputFormats 提供了一个兼容性包装器。支持任何实现 org.apache.hadoop.mapred.OutputFormat 或扩展 org.apache.hadoop.mapreduce.OutputFormat 的类。 OutputFormat 包装器期望其输入数据是包含键和值的 2-元组的 DataSet。这些将由 Hadoop OutputFormat 处理。
下面的示例展示了如何使用 Hadoop 的 TextOutputFormat。
Java // 获取我们希望发送的结果 DataStream\u0026lt;Tuple2\u0026lt;Text, IntWritable\u0026gt;\u0026gt; hadoopResult = [...]; // 设置 the Hadoop TextOutputFormat。 HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt; hadoopOF = // 创建 Flink wrapper. new HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt;( // 设置 Hadoop OutputFormat 并指定 job。 new TextOutputFormat\u0026lt;Text, IntWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // 使用 Hadoop TextOutputFormat 发送数据。 hadoopResult.output(hadoopOF); Scala // 获取我们希望发送的结果 val hadoopResult: DataStream[(Text, IntWritable)] = [...] val hadoopOF = new HadoopOutputFormat[Text,IntWritable]( new TextOutputFormat[Text, IntWritable], new JobConf) hadoopOF.getJobConf.set(\u0026#34;mapred.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;) FileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) hadoopResult.output(hadoopOF) Back to top
`}),e.add({id:110,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_read_write/",title:"Hive Read \u0026 Write",section:"Hive",content:` Hive 读 \u0026amp; 写 # 通过使用 HiveCatalog，Apache Flink 可以对 Apache Hive 表做统一的批和流处理。这意味着 Flink 可以成为 Hive 批处理引擎的一个性能更好的选择，或者连续读写 Hive 表中的数据以支持实时数据仓库应用。
读 # Flink 支持以批和流两种模式从 Hive 表中读取数据。批读的时候，Flink 会基于执行查询时表的状态进行查询。流读时将持续监控表，并在表中新数据可用时进行增量获取，默认情况下，Flink 将以批模式读取数据。
流读支持消费分区表和非分区表。对于分区表，Flink 会监控新分区的生成，并且在数据可用的情况下增量获取数据。对于非分区表，Flink 将监控文件夹中新文件的生成，并增量地读取新文件。
键 默认值 类型 描述 streaming-source.enable false Boolean 是否启动流读。注意：请确保每个分区/文件都应该原子地写入，否则读取不到完整的数据。 streaming-source.partition.include all String 选择读取的分区，可选项为 \`all\` 和 \`latest\`，\`all\` 读取所有分区；\`latest\` 读取按照 'streaming-source.partition.order' 排序后的最新分区，\`latest\` 仅在流模式的 Hive 源表作为时态表时有效。默认的选项是 \`all\`。在开启 'streaming-source.enable' 并设置 'streaming-source.partition.include' 为 'latest' 时，Flink 支持 temporal join 最新的 Hive 分区，同时，用户可以通过配置分区相关的选项来配置分区比较顺序和数据更新时间间隔。 streaming-source.monitor-interval None Duration 连续监控分区/文件的时间间隔。 注意: 默认情况下，流式读 Hive 的间隔为 '1 min'，但流读 Hive 的 temporal join 的默认时间间隔是 '60 min'，这是因为当前流读 Hive 的 temporal join 实现上有一个框架限制，即每个 TM 都要访问 Hive metastore，这可能会对 metastore 产生压力，这个问题将在未来得到改善。 streaming-source.partition-order partition-name String streaming source 分区排序，支持 create-time， partition-time 和 partition-name。 create-time 比较分区/文件创建时间， 这不是 Hive metastore 中创建分区的时间，而是文件夹/文件在文件系统的修改时间，如果分区文件夹以某种方式更新，比如添加在文件夹里新增了一个文件，它会影响到数据的使用。partition-time 从分区名称中抽取时间进行比较。partition-name 会比较分区名称的字典顺序。对于非分区的表，总是会比较 'create-time'。对于分区表默认值是 'partition-name'。该选项与已经弃用的 'streaming-source.consume-order' 的选项相同 streaming-source.consume-start-offset None String 流模式起始消费偏移量。如何解析和比较偏移量取决于你指定的顺序。对于 create-time 和 partition-time，会比较时间戳 (yyyy-[m]m-[d]d [hh:mm:ss])。对于 partition-time，将使用分区时间提取器从分区名字中提取的时间。 对于 partition-name，是字符串类型的分区名称(比如 pt_year=2020/pt_mon=10/pt_day=01)。 SQL Hints 可以在不改变 Hive metastore 的情况下配置 Hive table 的属性。
SELECT * FROM hive_table /*+ OPTIONS(\u0026#39;streaming-source.enable\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;streaming-source.consume-start-offset\u0026#39;=\u0026#39;2020-05-20\u0026#39;) */; 注意
监控策略是扫描当前位置路径中的所有目录/文件，分区太多可能导致性能下降。 流读非分区表时要求每个文件应原子地写入目标目录。 流读分区表要求每个分区应该被原子地添加进 Hive metastore 中。如果不是的话，只有添加到现有分区的新数据会被消费。 流读 Hive 表不支持 Flink DDL 的 watermark 语法。这些表不能被用于窗口算子。 读取 Hive Views # Flink 能够读取 Hive 中已经定义的视图。但是也有一些限制：
Hive catalog 必须设置成当前的 catalog 才能查询视图。在 Table API 中使用 tableEnv.useCatalog(...)，或者在 SQL 客户端使用 USE CATALOG ... 来改变当前 catalog。
Hive 和 Flink SQL 的语法不同, 比如不同的关键字和字面值。请确保对视图的查询语法与 Flink 语法兼容。
读取时的向量化优化 # 当满足以下条件时，Flink 会自动对 Hive 表进行向量化读取:
格式：ORC 或者 Parquet。 没有复杂类型的列，比如 Hive 列类型：List、Map、Struct、Union。 该特性默认开启，可以使用以下配置禁用它。
table.exec.hive.fallback-mapred-reader=true Source 并发推断 # 默认情况下，Flink 会基于文件的数量，以及每个文件中块的数量推断出读取 Hive 的最佳并行度。
Flink 允许你灵活的配置并发推断策略。你可以在 TableConfig 中配置以下参数(注意这些参数会影响当前作业的所有 source)：
键 默认值 类型 描述 table.exec.hive.infer-source-parallelism true Boolean 如果是 true，会根据 split 的数量推断 source 的并发度。如果是 false，source 的并发度由配置决定。 table.exec.hive.infer-source-parallelism.max 1000 Integer 设置 source operator 推断的最大并发度。 读 Hive 表时调整数据分片（Split） 大小 # 读 Hive 表时, 数据文件将会被切分为若干个分片（split）, 每一个分片是要读取的数据的一部分。 分片是 Flink 进行任务分配和数据并行读取的基本粒度。 用户可以通过下面的参数来调整每个分片的大小来做一定的读性能调优。
Key Default Type Description table.exec.hive.split-max-size 128mb MemorySize 读 Hive 表时，每个分片最大可以包含的字节数 (默认是 128MB) table.exec.hive.file-open-cost 4mb MemorySize 打开一个文件预估的开销，以字节为单位，默认是 4MB。 如果这个值比较大，Flink 则将会倾向于将 Hive 表切分为更少的分片，这在 Hive 表中包含大量小文件的时候很有用。 反之，Flink 将会倾向于将 Hive 表切分为更多的分片，这有利于提升数据读取的并行度。 注意： 目前上述参数仅适用于 ORC 格式的 Hive 表。
加载分区切片 # Flink 使用多个线程并发将 Hive 分区切分成多个 split 进行读取。你可以使用 table.exec.hive.load-partition-splits.thread-num 去配置线程数。默认值是3，你配置的值应该大于0。
读取带有子目录的分区 # 在某些情况下，你或许会创建一个引用其他表的外部表，但是该表的分区列是另一张表分区字段的子集。 比如，你创建了一个分区表 fact_tz，分区字段是 day 和 hour：
CREATE TABLE fact_tz(x int) PARTITIONED BY (day STRING, hour STRING); 然后你基于 fact_tz 表创建了一个外部表 fact_daily，并使用了一个粗粒度的分区字段 day：
CREATE EXTERNAL TABLE fact_daily(x int) PARTITIONED BY (ds STRING) LOCATION \u0026#39;/path/to/fact_tz\u0026#39;; 当读取外部表 fact_daily 时，该表的分区目录下存在子目录(hour=1 到 hour=24)。
默认情况下，可以将带有子目录的分区添加到外部表中。Flink SQL 会递归扫描所有的子目录，并获取所有子目录中数据。
ALTER TABLE fact_daily ADD PARTITION (ds=\u0026#39;2022-07-07\u0026#39;) location \u0026#39;/path/to/fact_tz/ds=2022-07-07\u0026#39;; 你可以设置作业属性 table.exec.hive.read-partition-with-subdirectory.enabled (默认为 true) 为 false 以禁止 Flink 读取子目录。 如果你设置成 false 并且分区目录下不包含任何子目录，Flink 会抛出 java.io.IOException: Not a file: /path/to/data/* 异常。
时态表 Join # 你可以使用 Hive 表作为时态表，然后一个数据流就可以使用 temporal join 关联 Hive 表。 请参照 temporal join 获取更多关于 temporal join 的信息。
Flink 支持 processing-time temporal join Hive 表，processing-time temporal join 总是关联最新版本的时态表。 Flink 支持 temporal join Hive 的分区表和非分区表，对于分区表，Flink 支持自动跟踪 Hive 表的最新分区。
注意: Flink 还不支持 event-time temporal join Hive 表。
Temporal Join 最新的分区 # 对于随时变化的分区表，我们可以把它看作是一个无界流进行读取，如果每个分区包含完整数据，则分区可以作为时态表的一个版本，时态表的版本保存分区的数据。
Flink 支持在使用 processing time temporal join 时自动追踪最新的分区（版本），通过 streaming-source.partition-order 定义最新的分区（版本）。 用户最常使用的案例就是在 Flink 流作业中使用 Hive 表作为维度表。
注意: 该特性仅支持 Flink 流模式。
下面的案例演示了经典的业务 pipeline，使用 Hive 中的表作为维度表，它们由每天一次的批任务或者 Flink 任务来更新。 Kafka 数据流来自实时在线业务数据或者日志，该流需要关联维度表以丰富数据流。
-- 假设 Hive 表中的数据每天更新, 每天包含最新和完整的维度数据 SET table.sql-dialect=hive; CREATE TABLE dimension_table ( product_id STRING, product_name STRING, unit_price DECIMAL(10, 4), pv_count BIGINT, like_count BIGINT, comment_count BIGINT, update_time TIMESTAMP(3), update_user STRING, ... ) PARTITIONED BY (pt_year STRING, pt_month STRING, pt_day STRING) TBLPROPERTIES ( -- using default partition-name order to load the latest partition every 12h (the most recommended and convenient way) \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;partition-name\u0026#39;, -- 有默认的配置项，可以不填。 -- using partition file create-time order to load the latest partition every 12h \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;create-time\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39; -- using partition-time order to load the latest partition every 12h \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;latest\u0026#39;, \u0026#39;streaming-source.monitor-interval\u0026#39; = \u0026#39;12 h\u0026#39;, \u0026#39;streaming-source.partition-order\u0026#39; = \u0026#39;partition-time\u0026#39;, \u0026#39;partition.time-extractor.kind\u0026#39; = \u0026#39;default\u0026#39;, \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39; = \u0026#39;\$pt_year-\$pt_month-\$pt_day 00:00:00\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE orders_table ( order_id STRING, order_amount DOUBLE, product_id STRING, log_ts TIMESTAMP(3), proctime as PROCTIME() ) WITH (...); -- streaming sql, kafka temporal join Hive 维度表. Flink 将在 \u0026#39;streaming-source.monitor-interval\u0026#39; 的间隔内自动加载最新分区的数据。 SELECT * FROM orders_table AS o JOIN dimension_table FOR SYSTEM_TIME AS OF o.proctime AS dim ON o.product_id = dim.product_id; Temporal Join 最新的表 # 对于 Hive 表，我们可以把它看作是一个无界流进行读取，在这个案例中，当我们查询时只能去追踪最新的版本。 最新版本的表保留了 Hive 表的所有数据。
当 temporal join 最新的 Hive 表，Hive 表会缓存到 Slot 内存中，并且数据流中的每条记录通过 key 去关联表找到对应的匹配项。 使用最新的 Hive 表作为时态表不需要额外的配置。作为可选项，您可以使用以下配置项配置 Hive 表缓存的 TTL。当缓存失效，Hive 表会重新扫描并加载最新的数据。
键 默认值 类型 描述 lookup.join.cache.ttl 60 min Duration 在 lookup join 时构建表缓存的 TTL (例如 10min)。默认的 TTL 是60分钟。注意: 该选项仅在 lookup 表为有界的 Hive 表时有效，如果你使用流式的 Hive 表作为时态表，请使用 'streaming-source.monitor-interval' 去配置数据更新的间隔。 下面的案例演示加载 Hive 表的所有数据作为时态表。
-- 假设 Hive 表中的数据被批处理 pipeline 覆盖。 SET table.sql-dialect=hive; CREATE TABLE dimension_table ( product_id STRING, product_name STRING, unit_price DECIMAL(10, 4), pv_count BIGINT, like_count BIGINT, comment_count BIGINT, update_time TIMESTAMP(3), update_user STRING, ... ) TBLPROPERTIES ( \u0026#39;streaming-source.enable\u0026#39; = \u0026#39;false\u0026#39;, -- 有默认的配置项，可以不填。 \u0026#39;streaming-source.partition.include\u0026#39; = \u0026#39;all\u0026#39;, -- 有默认的配置项，可以不填。 \u0026#39;lookup.join.cache.ttl\u0026#39; = \u0026#39;12 h\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE orders_table ( order_id STRING, order_amount DOUBLE, product_id STRING, log_ts TIMESTAMP(3), proctime as PROCTIME() ) WITH (...); -- streaming sql, kafka join Hive 维度表. 当缓存失效时 Flink 会加载维度表的所有数据。 SELECT * FROM orders_table AS o JOIN dimension_table FOR SYSTEM_TIME AS OF o.proctime AS dim ON o.product_id = dim.product_id; 注意:
每个参与 join 的 subtask 需要在他们的缓存中保留 Hive 表。请确保 Hive 表可以放到 TM task slot 中。 建议把这两个选项配置成较大的值 streaming-source.monitor-interval(最新的分区作为时态表) 和 lookup.join.cache.ttl(所有的分区作为时态表)。否则，任务会频繁更新和加载表，容易出现性能问题。 目前，缓存刷新的时候会重新加载整个 Hive 表，所以没有办法区分数据是新数据还是旧数据。 写 # Flink 支持批和流两种模式往 Hive 中写入数据，当作为批程序，只有当作业完成时，Flink 写入 Hive 表的数据才能被看见。批模式写入支持追加到现有的表或者覆盖现有的表。
# ------ INSERT INTO 将追加到表或者分区，保证数据的完整性 ------ Flink SQL\u0026gt; INSERT INTO mytable SELECT \u0026#39;Tom\u0026#39;, 25; # ------ INSERT OVERWRITE 将覆盖表或者分区中所有已经存在的数据 ------ Flink SQL\u0026gt; INSERT OVERWRITE mytable SELECT \u0026#39;Tom\u0026#39;, 25; 还可以将数据插入到特定的分区中。
# ------ 插入静态分区 ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;, my_date=\u0026#39;2019-08-08\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25; # ------ 插入动态分区 ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;type_1\u0026#39;, \u0026#39;2019-08-08\u0026#39;; # ------ 插入静态(my_type)和动态(my_date)分区 ------ Flink SQL\u0026gt; INSERT OVERWRITE myparttable PARTITION (my_type=\u0026#39;type_1\u0026#39;) SELECT \u0026#39;Tom\u0026#39;, 25, \u0026#39;2019-08-08\u0026#39;; 流写会不断的往 Hive 中添加新数据，提交记录使它们可见。用户可以通过几个属性控制如何触发提交。流写不支持 Insert overwrite 。
下面的案例演示如何流式地从 Kafka 写入 Hive 表并执行分区提交，然后运行一个批处理查询将数据读出来。
请参阅 streaming sink 获取可用配置的完整列表。
SET table.sql-dialect=hive; CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hr:00:00\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL \u0026#39;5\u0026#39; SECOND -- 在 TIMESTAMP 列声明 watermark。 ) WITH (...); -- streaming sql, insert into hive table INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(log_ts, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM hive_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and hr=\u0026#39;12\u0026#39;; 如果在 TIMESTAMP_LTZ 列定义了 watermark 并且使用 partition-time 提交，需要对 sink.partition-commit.watermark-time-zone 设置会话时区，否则分区提交会发生在几个小时后。
SET table.sql-dialect=hive; CREATE TABLE hive_table ( user_id STRING, order_amount DOUBLE ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES ( \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hr:00:00\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.watermark-time-zone\u0026#39;=\u0026#39;Asia/Shanghai\u0026#39;, -- 假设用户配置的时区是 \u0026#39;Asia/Shanghai\u0026#39;。 \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;metastore,success-file\u0026#39; ); SET table.sql-dialect=default; CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, ts BIGINT, -- time in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND -- 在 TIMESTAMP_LTZ 列声明 watermark。 ) WITH (...); -- streaming sql, insert into hive table INSERT INTO TABLE hive_table SELECT user_id, order_amount, DATE_FORMAT(ts_ltz, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(ts_ltz, \u0026#39;HH\u0026#39;) FROM kafka_table; -- batch sql, select with partition pruning SELECT * FROM hive_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and hr=\u0026#39;12\u0026#39;; 默认情况下，对于流，Flink 仅支持重命名 committers，对于 S3 文件系统不支持流写的 exactly-once 语义。 通过将以下参数设置为 false，可以实现 exactly-once 写入 S3。 这会调用 Flink 原生的 writer ，但是仅针对 parquet 和 orc 文件类型有效。 这个配置项可以在 TableConfig 中配置，该配置项对作业的所有 sink 都生效。
键 默认值 类型 描述 table.exec.hive.fallback-mapred-writer true Boolean 如果是 false，使用 Flink 原生的 writer 去写 parquet 和 orc 文件；如果是 true，使用 hadoop mapred record writer 去写 parquet 和 orc 文件。 动态分区的写入 # 不同于静态分区的写入总是需要用户指定分区列的值，动态分区允许用户在写入数据的时候不指定分区列的值。 比如，有这样一个分区表：
CREATE TABLE fact_tz(x int) PARTITIONED BY (day STRING, hour STRING); 用户可以使用如下的 SQL 语句向该分区表写入数据：
INSERT INTO TABLE fact_tz PARTITION (day, hour) select 1, \u0026#39;2022-8-8\u0026#39;, \u0026#39;14\u0026#39;; 在该 SQL 语句中，用户没有指定分区列的值，这就是一个典型的动态分区写入的例子。
默认情况下, 如果是动态分区的写入, 在实际写入目标表之前，Flink 将额外对数据按照动态分区列进行排序。 这就意味着 sink 节点收到的数据都是按分区排序的，即首先收到一个分区的数据，然后收到另一个分区的数据，不同分区的数据不会混在一起。 这样 Hive sink 节点就可以一次只维护一个分区的 writer，否则，Hive sink 需要维护收到的数据对应的所有分区的 writer，如果分区的 writer 过多的话，则可能会导致内存溢出（OutOfMemory）异常。
为了避免额外的排序，你可以将作业的配置项 table.exec.hive.sink.sort-by-dynamic-partition.enable（默认是 true）设置为 false。 但是这种配置下，如之前所述，如果单个 sink 节点收到的动态分区数过多的话，则有可能会出现内存溢出的异常。
如果数据倾斜不严重的话，你可以在 SQL 语句中添加 DISTRIBUTED BY \u0026lt;partition_field\u0026gt; 将相同分区的数据分布到到相同的 sink 节点上来缓解单个 sink 节点的分区 writer 过多的问题。
此外，你也可以在 SQL 语句中添加 DISTRIBUTED BY \u0026lt;partition_field\u0026gt; 来达到将 table.exec.hive.sink.sort-by-dynamic-partition.enable 设置为 false 的效果。
注意：
该配置项 table.exec.hive.sink.sort-by-dynamic-partition.enable 只在批模式下生效。 目前，只有在 Flink 批模式下使用了 Hive 方言，才可以使用 DISTRIBUTED BY 和 SORTED BY。 自动收集统计信息 # 在使用 Flink 写入 Hive 表的时候，Flink 将默认自动收集写入数据的统计信息然后将其提交至 Hive metastore 中。 但在某些情况下，你可能不想自动收集统计信息，因为收集这些统计信息可能会花费一定的时间。 为了避免 Flink 自动收集统计信息，你可以设置作业参数 table.exec.hive.sink.statistic-auto-gather.enable (默认是 true) 为 false。
如果写入的 Hive 表是以 Parquet 或者 ORC 格式存储的时候，numFiles/totalSize/numRows/rawDataSize 这些统计信息可以被 Flink 收集到。 否则, 只有 numFiles/totalSize 可以被收集到。
对于 Parquet 或者 ORC 格式的表，为了快速收集到统计信息 numRows/rawDataSize， Flink 只会读取文件的 footer。但是在文件数量很多的情况下，这可能也会比较耗时，你可以通过 设置作业参数 table.exec.hive.sink.statistic-auto-gather.thread-num（默认是 3）为一个更大的值来加快统计信息的收集。
注意：
只有批模式才支持自动收集统计信息，流模式目前还不支持自动收集统计信息。 格式 # Flink 对 Hive 的集成已经在如下的文件格式进行了测试：
Text CSV SequenceFile ORC Parquet `}),e.add({id:111,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/json/",title:"JSON",section:"Formats",content:` Json format # To use the JSON format you need to add the Flink JSON dependency to your project:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-json\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading/writing JSON records via the JsonSerializationSchema/JsonDeserializationSchema. These utilize the Jackson library, and support any type that is supported by Jackson, including, but not limited to, POJOs and ObjectNode.
The JsonDeserializationSchema can be used with any connector that supports the DeserializationSchema.
For example, this is how you use it with a KafkaSource to deserialize a POJO:
JsonDeserializationSchema\u0026lt;SomePojo\u0026gt; jsonFormat = new JsonDeserializationSchema\u0026lt;\u0026gt;(SomePojo.class); KafkaSource\u0026lt;SomePojo\u0026gt; source = KafkaSource.\u0026lt;SomePojo\u0026gt;builder() .setValueOnlyDeserializer(jsonFormat) ... The JsonSerializationSchema can be used with any connector that supports the SerializationSchema.
For example, this is how you use it with a KafkaSink to serialize a POJO:
JsonSerializationSchema\u0026lt;SomePojo\u0026gt; jsonFormat = new JsonSerializationSchema\u0026lt;\u0026gt;(); KafkaSink\u0026lt;SomePojo\u0026gt; source = KafkaSink.\u0026lt;SomePojo\u0026gt;builder() .setRecordSerializer( new KafkaRecordSerializationSchemaBuilder\u0026lt;\u0026gt;() .setValueSerializationSchema(jsonFormat) ... Custom Mapper # Both schemas have constructors that accept a SerializableSupplier\u0026lt;ObjectMapper\u0026gt;, acting a factory for object mappers. With this factory you gain full control over the created mapper, and can enable/disable various Jackson features or register modules to extend the set of supported types or add additional functionality.
JsonSerializationSchema\u0026lt;SomeClass\u0026gt; jsonFormat = new JsonSerializationSchema\u0026lt;\u0026gt;( () -\u0026gt; new ObjectMapper() .enable(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS)) .registerModule(new ParameterNamesModule()); Python # In PyFlink, JsonRowSerializationSchema and JsonRowDeserializationSchema are built-in support for Row type. Here are examples on how to use it in KafkaSource and KafkaSink:
row_type_info = Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;], [Types.STRING(), Types.INT()]) json_format = JsonRowDeserializationSchema.builder().type_info(row_type_info).build() source = KafkaSource.builder() \\ .set_value_only_deserializer(json_format) \\ .build() row_type_info = Types.ROW_NAMED([\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;], [Types.STRING(), Types.INT()]) json_format = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build() sink = KafkaSink.builder() \\ .set_record_serializer( KafkaRecordSerializationSchema.builder() .set_topic(\u0026#39;test\u0026#39;) .set_value_serialization_schema(json_format) .build() ) \\ .build() `}),e.add({id:112,href:"/flink/flink-docs-master/zh/docs/libs/gelly/library_methods/",title:"Library Methods",section:"Graphs",content:` Library Methods # Gelly has a growing collection of graph algorithms for easily analyzing large-scale Graphs.
Gelly\u0026rsquo;s library methods can be used by simply calling the run() method on the input graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph\u0026lt;Long, Long, NullValue\u0026gt; graph = ...; // run Label Propagation for 30 iterations to detect communities on the input graph DataSet\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithCommunity = graph.run(new LabelPropagation\u0026lt;Long\u0026gt;(30)); // print the result verticesWithCommunity.print(); Scala val env = ExecutionEnvironment.getExecutionEnvironment val graph: Graph[java.lang.Long, java.lang.Long, NullValue] = ... // run Label Propagation for 30 iterations to detect communities on the input graph val verticesWithCommunity = graph.run(new LabelPropagation[java.lang.Long, java.lang.Long, NullValue](30)) // print the result verticesWithCommunity.print() Community Detection # Overview # In graph theory, communities refer to groups of nodes that are well connected internally, but sparsely connected to other groups. This library method is an implementation of the community detection algorithm described in the paper Towards real-time community detection in large networks.
Details # The algorithm is implemented using scatter-gather iterations. Initially, each vertex is assigned a Tuple2 containing its initial value along with a score equal to 1.0. In each iteration, vertices send their labels and scores to their neighbors. Upon receiving messages from its neighbors, a vertex chooses the label with the highest score and subsequently re-scores it using the edge values, a user-defined hop attenuation parameter, delta, and the superstep number. The algorithm converges when vertices no longer update their value or when the maximum number of iterations is reached.
Usage # The algorithm takes as input a Graph with any vertex type, Long vertex values, and Double edge values. It returns a Graph of the same type as the input, where the vertex values correspond to the community labels, i.e. two vertices belong to the same community if they have the same vertex value. The constructor takes two parameters:
maxIterations: the maximum number of iterations to run. delta: the hop attenuation parameter, with default value 0.5. Label Propagation # Overview # This is an implementation of the well-known Label Propagation algorithm described in this paper. The algorithm discovers communities in a graph, by iteratively propagating labels between neighbors. Unlike the Community Detection library method, this implementation does not use scores associated with the labels.
Details # The algorithm is implemented using scatter-gather iterations. Labels are expected to be of type Comparable and are initialized using the vertex values of the input Graph. The algorithm iteratively refines discovered communities by propagating labels. In each iteration, a vertex adopts the label that is most frequent among its neighbors\u0026rsquo; labels. In case of a tie (i.e. two or more labels appear with the same frequency), the algorithm picks the greater label. The algorithm converges when no vertex changes its value or the maximum number of iterations has been reached. Note that different initializations might lead to different results.
Usage # The algorithm takes as input a Graph with a Comparable vertex type, a Comparable vertex value type and an arbitrary edge value type. It returns a DataSet of vertices, where the vertex value corresponds to the community in which this vertex belongs after convergence. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. Connected Components # Overview # This is an implementation of the Weakly Connected Components algorithm. Upon convergence, two vertices belong to the same component, if there is a path from one to the other, without taking edge direction into account.
Details # The algorithm is implemented using scatter-gather iterations. This implementation uses a comparable vertex value as initial component identifier (ID). Vertices propagate their current value in each iteration. Upon receiving component IDs from its neighbors, a vertex adopts a new component ID if its value is lower than its current component ID. The algorithm converges when vertices no longer update their component ID value or when the maximum number of iterations has been reached.
Usage # The result is a DataSet of vertices, where the vertex value corresponds to the assigned component. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. GSA Connected Components # Overview # This is an implementation of the Weakly Connected Components algorithm. Upon convergence, two vertices belong to the same component, if there is a path from one to the other, without taking edge direction into account.
Details # The algorithm is implemented using gather-sum-apply iterations. This implementation uses a comparable vertex value as initial component identifier (ID). In the gather phase, each vertex collects the vertex value of their adjacent vertices. In the sum phase, the minimum among those values is selected. In the apply phase, the algorithm sets the minimum value as the new vertex value if it is smaller than the current value. The algorithm converges when vertices no longer update their component ID value or when the maximum number of iterations has been reached.
Usage # The result is a DataSet of vertices, where the vertex value corresponds to the assigned component. The constructor takes one parameter:
maxIterations: the maximum number of iterations to run. Single Source Shortest Paths # Overview # An implementation of the Single-Source-Shortest-Paths algorithm for weighted graphs. Given a source vertex, the algorithm computes the shortest paths from this source to all other nodes in the graph.
Details # The algorithm is implemented using scatter-gather iterations. In each iteration, a vertex sends to its neighbors a message containing the sum its current distance and the edge weight connecting this vertex with the neighbor. Upon receiving candidate distance messages, a vertex calculates the minimum distance and, if a shorter path has been discovered, it updates its value. If a vertex does not change its value during a superstep, then it does not produce messages for its neighbors for the next superstep. The computation terminates after the specified maximum number of supersteps or when there are no value updates.
Usage # The algorithm takes as input a Graph with any vertex type and Double edge values. The vertex values can be any type and are not used by this algorithm. The vertex type must implement equals(). The output is a DataSet of vertices where the vertex values correspond to the minimum distances from the given source vertex. The constructor takes two parameters:
srcVertexId The vertex ID of the source vertex. maxIterations: the maximum number of iterations to run. GSA Single Source Shortest Paths # The algorithm is implemented using gather-sum-apply iterations.
See the Single Source Shortest Paths library method for implementation details and usage information.
Triangle Enumerator # Overview # This library method enumerates unique triangles present in the input graph. A triangle consists of three edges that connect three vertices with each other. This implementation ignores edge directions.
Details # The basic triangle enumeration algorithm groups all edges that share a common vertex and builds triads, i.e., triples of vertices that are connected by two edges. Then, all triads are filtered for which no third edge exists that closes the triangle. For a group of n edges that share a common vertex, the number of built triads is quadratic ((n*(n-1))/2). Therefore, an optimization of the algorithm is to group edges on the vertex with the smaller output degree to reduce the number of triads. This implementation extends the basic algorithm by computing output degrees of edge vertices and grouping on edges on the vertex with the smaller degree.
Usage # The algorithm takes a directed graph as input and outputs a DataSet of Tuple3. The Vertex ID type has to be Comparable. Each Tuple3 corresponds to a triangle, with the fields containing the IDs of the vertices forming the triangle.
Summarization # Overview # The summarization algorithm computes a condensed version of the input graph by grouping vertices and edges based on their values. In doing so, the algorithm helps to uncover insights about patterns and distributions in the graph. One possible use case is the visualization of communities where the whole graph is too large and needs to be summarized based on the community identifier stored at a vertex.
Details # In the resulting graph, each vertex represents a group of vertices that share the same value. An edge, that connects a vertex with itself, represents all edges with the same edge value that connect vertices from the same vertex group. An edge between different vertices in the output graph represents all edges with the same edge value between members of different vertex groups in the input graph.
The algorithm is implemented using Flink data operators. First, vertices are grouped by their value and a representative is chosen from each group. For any edge, the source and target vertex identifiers are replaced with the corresponding representative and grouped by source, target and edge value. Output vertices and edges are created from their corresponding groupings.
Usage # The algorithm takes a directed, vertex (and possibly edge) attributed graph as input and outputs a new graph where each vertex represents a group of vertices and each edge represents a group of edges from the input graph. Furthermore, each vertex and edge in the output graph stores the common group value and the number of represented elements.
Clustering # Average Clustering Coefficient # Overview # The average clustering coefficient measures the mean connectedness of a graph. Scores range from 0.0 (no edges between neighbors) to 1.0 (complete graph).
Details # See the Local Clustering Coefficient library method for a detailed explanation of clustering coefficient. The Average Clustering Coefficient is the average of the Local Clustering Coefficient scores over all vertices with at least two neighbors. Each vertex, independent of degree, has equal weight for this score.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult containing the total number of vertices and average clustering coefficient of the graph. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Global Clustering Coefficient # Overview # The global clustering coefficient measures the connectedness of a graph. Scores range from 0.0 (no edges between neighbors) to 1.0 (complete graph).
Details # See the Local Clustering Coefficient library method for a detailed explanation of clustering coefficient. The Global Clustering Coefficient is the ratio of connected neighbors over the entire graph. Vertices with higher degrees have greater weight for this score because the count of neighbor pairs is quadratic in degree.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult containing the total number of triplets and triangles in the graph. The result class provides a method to compute the global clustering coefficient score. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Local Clustering Coefficient # Overview # The local clustering coefficient measures the connectedness of each vertex\u0026rsquo;s neighborhood. Scores range from 0.0 (no edges between neighbors) to 1.0 (neighborhood is a clique).
Details # An edge between neighbors of a vertex is a triangle. Counting edges between neighbors is equivalent to counting the number of triangles which include the vertex. The clustering coefficient score is the number of edges between neighbors divided by the number of potential edges between neighbors.
See the Triangle Listing library method for a detailed explanation of triangle enumeration.
Usage # Directed and undirected variants are provided. The algorithms take a simple graph as input and output a DataSet of UnaryResult containing the vertex ID, vertex degree, and number of triangles containing the vertex. The result class provides a method to compute the local clustering coefficient score. The graph ID type must be Comparable and Copyable.
setIncludeZeroDegreeVertices: include results for vertices with a degree of zero setParallelism: override the parallelism of operators processing small amounts of data Triadic Census # Overview # A triad is formed by any three vertices in a graph. Each triad contains three pairs of vertices which may be connected or unconnected. The Triadic Census counts the occurrences of each type of triad with the graph.
Details # This analytic counts the four undirected triad types (formed with 0, 1, 2, or 3 connecting edges) or 16 directed triad types by counting the triangles from Triangle Listing and running Vertex Metrics to obtain the number of triplets and edges. Triangle counts are then deducted from triplet counts, and triangle and triplet counts are removed from edge counts.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for querying the count of each triad type. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data Triangle Listing # Overview # Enumerates all triangles in the graph. A triangle is composed of three edges connecting three vertices into cliques of size 3.
Details # Triangles are listed by joining open triplets (two edges with a common neighbor) against edges on the triplet endpoints. This implementation uses optimizations from Schank\u0026rsquo;s algorithm to improve performance with high-degree vertices. Triplets are generated from the lowest degree vertex since each triangle need only be listed once. This greatly reduces the number of generated triplets which is quadratic in vertex degree.
Usage # Directed and undirected variants are provided. The algorithms take a simple graph as input and output a DataSet of TertiaryResult containing the three triangle vertices and, for the directed algorithm, a bitmask marking each of the six potential edges connecting the three vertices. The graph ID type must be Comparable and Copyable.
setParallelism: override the parallelism of operators processing small amounts of data setSortTriangleVertices: normalize the triangle listing such that for each result (K0, K1, K2) the vertex IDs are sorted K0 \u0026lt; K1 \u0026lt; K2 Link Analysis # Hyperlink-Induced Topic Search # Overview # Hyperlink-Induced Topic Search (HITS, or \u0026ldquo;Hubs and Authorities\u0026rdquo;) computes two interdependent scores for every vertex in a directed graph. Good hubs are those which point to many good authorities and good authorities are those pointed to by many good hubs.
Details # Every vertex is assigned the same initial hub and authority scores. The algorithm then iteratively updates the scores until termination. During each iteration new hub scores are computed from the authority scores, then new authority scores are computed from the new hub scores. The scores are then normalized and optionally tested for convergence. HITS is similar to PageRank but vertex scores are emitted in full to each neighbor whereas in PageRank the vertex score is first divided by the number of neighbors.
Usage # The algorithm takes a simple directed graph as input and outputs a DataSet of UnaryResult containing the vertex ID, hub score, and authority score. Termination is configured by the number of iterations and/or a convergence threshold on the iteration sum of the change in scores over all vertices.
setIncludeZeroDegreeVertices: whether to include zero-degree vertices in the iterative computation setParallelism: override the operator parallelism PageRank # Overview # PageRank is an algorithm that was first used to rank web search engine results. Today, the algorithm and many variations are used in various graph application domains. The idea of PageRank is that important or relevant vertices tend to link to other important vertices.
Details # The algorithm operates in iterations, where pages distribute their scores to their neighbors (pages they have links to) and subsequently update their scores based on the sum of values they receive. In order to consider the importance of a link from one page to another, scores are divided by the total number of out-links of the source page. Thus, a page with 10 links will distribute 1/10 of its score to each neighbor, while a page with 100 links will distribute 1/100 of its score to each neighboring page.
Usage # The algorithm takes a directed graph as input and outputs a DataSet where each Result contains the vertex ID and PageRank score. Termination is configured with a maximum number of iterations and/or a convergence threshold on the sum of the change in score for each vertex between iterations.
setParallelism: override the operator parallelism Metric # Vertex Metrics # Overview # This graph analytic computes the following statistics for both directed and undirected graphs:
number of vertices number of edges average degree number of triplets maximum degree maximum number of triplets The following statistics are additionally computed for directed graphs:
number of unidirectional edges number of bidirectional edges maximum out degree maximum in degree Details # The statistics are computed over vertex degrees generated from degree.annotate.directed.VertexDegrees or degree.annotate.undirected.VertexDegree.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for the computed statistics. The graph ID type must be Comparable.
setIncludeZeroDegreeVertices: include results for vertices with a degree of zero setParallelism: override the operator parallelism setReduceOnTargetId (undirected only): the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID Edge Metrics # Overview # This graph analytic computes the following statistics:
number of triangle triplets number of rectangle triplets maximum number of triangle triplets maximum number of rectangle triplets Details # The statistics are computed over edge degrees generated from degree.annotate.directed.EdgeDegreesPair or degree.annotate.undirected.EdgeDegreePair and grouped by vertex.
Usage # Directed and undirected variants are provided. The analytics take a simple graph as input and output an AnalyticResult with accessor methods for the computed statistics. The graph ID type must be Comparable.
setParallelism: override the operator parallelism setReduceOnTargetId (undirected only): the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID Similarity # Adamic-Adar # Overview # Adamic-Adar measures the similarity between pairs of vertices as the sum of the inverse logarithm of degree over shared neighbors. Scores are non-negative and unbounded. A vertex with higher degree has greater overall influence but is less influential to each pair of neighbors.
Details # The algorithm first annotates each vertex with the inverse of the logarithm of the vertex degree then joins this score onto edges by source vertex. Grouping on the source vertex, each pair of neighbors is emitted with the vertex score. Grouping on vertex pairs, the Adamic-Adar score is summed.
See the Jaccard Index library method for a similar algorithm.
Usage # The algorithm takes a simple undirected graph as input and outputs a DataSet of BinaryResult containing two vertex IDs and the Adamic-Adar similarity score. The graph ID type must be Copyable.
setMinimumRatio: filter out Adamic-Adar scores less than the given ratio times the average score setMinimumScore: filter out Adamic-Adar scores less than the given minimum setParallelism: override the parallelism of operators processing small amounts of data Jaccard Index # Overview # The Jaccard Index measures the similarity between vertex neighborhoods and is computed as the number of shared neighbors divided by the number of distinct neighbors. Scores range from 0.0 (no shared neighbors) to 1.0 (all neighbors are shared).
Details # Counting shared neighbors for pairs of vertices is equivalent to counting connecting paths of length two. The number of distinct neighbors is computed by storing the sum of degrees of the vertex pair and subtracting the count of shared neighbors, which are double-counted in the sum of degrees.
The algorithm first annotates each edge with the target vertex\u0026rsquo;s degree. Grouping on the source vertex, each pair of neighbors is emitted with the degree sum. Grouping on vertex pairs, the shared neighbors are counted.
Usage # The algorithm takes a simple undirected graph as input and outputs a DataSet of tuples containing two vertex IDs, the number of shared neighbors, and the number of distinct neighbors. The result class provides a method to compute the Jaccard Index score. The graph ID type must be Copyable.
setMaximumScore: filter out Jaccard Index scores greater than or equal to the given maximum fraction setMinimumScore: filter out Jaccard Index scores less than the given minimum fraction setParallelism: override the parallelism of operators processing small amounts of data Back to top
`}),e.add({id:113,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/parquet/",title:"Parquet",section:"Formats",content:` Parquet format # Flink 支持读取 Parquet 文件并生成 Flink RowData 和 Avro 记录。 要使用 Parquet format，你需要将 flink-parquet 依赖添加到项目中：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-parquet\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 要使用 Avro 格式，你需要将 parquet-avro 依赖添加到项目中：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.parquet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;parquet-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.2\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;it.unimi.dsi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fastutil\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 为了在 PyFlink 作业中使用 Parquet format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 此格式与新的 Source 兼容，可以同时在批和流模式下使用。 因此，你可使用此格式处理以下两类数据：
有界数据: 列出所有文件并全部读取。 无界数据：监控目录中出现的新文件 当你开启一个 File Source，会被默认为有界读取。 如果你想在连续读取模式下使用 File Source，你必须额外调用 AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration)。 Vectorized reader
Java // Parquet rows are decoded in batches FileSource.forBulkFileFormat(BulkFormat,Path...) // Monitor the Paths to read data as unbounded data FileSource.forBulkFileFormat(BulkFormat,Path...) .monitorContinuously(Duration.ofMillis(5L)) .build(); Python # Parquet rows are decoded in batches FileSource.for_bulk_file_format(BulkFormat, Path...) # Monitor the Paths to read data as unbounded data FileSource.for_bulk_file_format(BulkFormat, Path...) \\ .monitor_continuously(Duration.of_millis(5)) \\ .build() Avro Parquet reader
Java // Parquet rows are decoded in batches FileSource.forRecordStreamFormat(StreamFormat,Path...) // Monitor the Paths to read data as unbounded data FileSource.forRecordStreamFormat(StreamFormat,Path...) .monitorContinuously(Duration.ofMillis(5L)) .build(); Python `}),e.add({id:114,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/process_function/",title:"Process Function",section:"算子",content:` Process Function # The ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).
For fault-tolerant state, the ProcessFunction gives access to Flink\u0026rsquo;s keyed state, accessible via the RuntimeContext, similar to the way other stateful functions can access keyed state.
The timers allow applications to react to changes in processing time and in event time. Every call to the function processElement(...) gets a Context object which gives access to the element\u0026rsquo;s event time timestamp, and to the TimerService. The TimerService can be used to register callbacks for future event-/processing-time instants. With event-time timers, the onTimer(...) method is called when the current watermark is advanced up to or beyond the timestamp of the timer, while with processing-time timers, onTimer(...) is called when wall clock time reaches the specified time. During that call, all states are again scoped to the key with which the timer was created, allowing timers to manipulate keyed state.
If you want to access keyed state and timers you have to apply the ProcessFunction on a keyed stream: stream.keyBy(...).process(new MyProcessFunction()); Low-level Joins # To realize low-level operations on two inputs, applications can use CoProcessFunction or KeyedCoProcessFunction. This function is bound to two different inputs and gets individual calls to processElement1(...) and processElement2(...) for records from the two different inputs.
Implementing a low level join typically follows this pattern:
Create a state object for one input (or both) Update the state upon receiving elements from its input Upon receiving elements from the other input, probe the state and produce the joined result For example, you might be joining customer data to financial trades, while keeping state for the customer data. If you care about having complete and deterministic joins in the face of out-of-order events, you can use a timer to evaluate and emit the join for a trade when the watermark for the customer data stream has passed the time of that trade.
Example # In the following example a KeyedProcessFunction maintains counts per key, and emits a key/count pair whenever a minute passes (in event time) without an update for that key:
The count, key, and last-modification-timestamp are stored in a ValueState, which is implicitly scoped by key. For each record, the KeyedProcessFunction increments the counter and sets the last-modification timestamp The function also schedules a callback one minute into the future (in event time) Upon each callback, it checks the callback\u0026rsquo;s event time timestamp against the last-modification time of the stored count and emits the key/count if they match (i.e., no further update occurred during that minute) This simple example could have been implemented with session windows. We use KeyedProcessFunction here to illustrate the basic pattern it provides. Java import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.streaming.api.functions.KeyedProcessFunction.Context; import org.apache.flink.streaming.api.functions.KeyedProcessFunction.OnTimerContext; import org.apache.flink.util.Collector; // the source data stream DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; stream = ...; // apply the process function onto a keyed stream DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; result = stream .keyBy(value -\u0026gt; value.f0) .process(new CountWithTimeoutFunction()); /** * The data type stored in the state */ public class CountWithTimestamp { public String key; public long count; public long lastModified; } /** * The implementation of the ProcessFunction that maintains the count and timeouts */ public class CountWithTimeoutFunction extends KeyedProcessFunction\u0026lt;Tuple, Tuple2\u0026lt;String, String\u0026gt;, Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; { /** The state that is maintained by this process function */ private ValueState\u0026lt;CountWithTimestamp\u0026gt; state; @Override public void open(Configuration parameters) throws Exception { state = getRuntimeContext().getState(new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;myState\u0026#34;, CountWithTimestamp.class)); } @Override public void processElement( Tuple2\u0026lt;String, String\u0026gt; value, Context ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { // retrieve the current count CountWithTimestamp current = state.value(); if (current == null) { current = new CountWithTimestamp(); current.key = value.f0; } // update the state\u0026#39;s count current.count++; // set the state\u0026#39;s timestamp to the record\u0026#39;s assigned event time timestamp current.lastModified = ctx.timestamp(); // write the state back state.update(current); // schedule the next timer 60 seconds from the current event time ctx.timerService().registerEventTimeTimer(current.lastModified + 60000); } @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { // get the state for the key that scheduled the timer CountWithTimestamp result = state.value(); // check if this is an outdated timer or the latest timer if (timestamp == result.lastModified + 60000) { // emit the state on timeout out.collect(new Tuple2\u0026lt;String, Long\u0026gt;(result.key, result.count)); } } } Scala import org.apache.flink.api.common.state.ValueState import org.apache.flink.api.common.state.ValueStateDescriptor import org.apache.flink.api.java.tuple.Tuple import org.apache.flink.streaming.api.functions.KeyedProcessFunction import org.apache.flink.util.Collector // the source data stream val stream: DataStream[Tuple2[String, String]] = ... // apply the process function onto a keyed stream val result: DataStream[Tuple2[String, Long]] = stream .keyBy(_._1) .process(new CountWithTimeoutFunction()) /** * The data type stored in the state */ case class CountWithTimestamp(key: String, count: Long, lastModified: Long) /** * The implementation of the ProcessFunction that maintains the count and timeouts */ class CountWithTimeoutFunction extends KeyedProcessFunction[Tuple, (String, String), (String, Long)] { /** The state that is maintained by this process function */ lazy val state: ValueState[CountWithTimestamp] = getRuntimeContext .getState(new ValueStateDescriptor[CountWithTimestamp](\u0026#34;myState\u0026#34;, classOf[CountWithTimestamp])) override def processElement( value: (String, String), ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#Context, out: Collector[(String, Long)]): Unit = { // initialize or retrieve/update the state val current: CountWithTimestamp = state.value match { case null =\u0026gt; CountWithTimestamp(value._1, 1, ctx.timestamp) case CountWithTimestamp(key, count, lastModified) =\u0026gt; CountWithTimestamp(key, count + 1, ctx.timestamp) } // write the state back state.update(current) // schedule the next timer 60 seconds from the current event time ctx.timerService.registerEventTimeTimer(current.lastModified + 60000) } override def onTimer( timestamp: Long, ctx: KeyedProcessFunction[Tuple, (String, String), (String, Long)]#OnTimerContext, out: Collector[(String, Long)]): Unit = { state.value match { case CountWithTimestamp(key, count, lastModified) if (timestamp == lastModified + 60000) =\u0026gt; out.collect((key, count)) case _ =\u0026gt; } } } Python import datetime from pyflink.common import Row, WatermarkStrategy from pyflink.common.typeinfo import Types from pyflink.common.watermark_strategy import TimestampAssigner from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext from pyflink.datastream.state import ValueStateDescriptor from pyflink.table import StreamTableEnvironment class CountWithTimeoutFunction(KeyedProcessFunction): def __init__(self): self.state = None def open(self, runtime_context: RuntimeContext): self.state = runtime_context.get_state(ValueStateDescriptor( \u0026#34;my_state\u0026#34;, Types.PICKLED_BYTE_ARRAY())) def process_element(self, value, ctx: \u0026#39;KeyedProcessFunction.Context\u0026#39;): # retrieve the current count current = self.state.value() if current is None: current = Row(value.f1, 0, 0) # update the state\u0026#39;s count current[1] += 1 # set the state\u0026#39;s timestamp to the record\u0026#39;s assigned event time timestamp current[2] = ctx.timestamp() # write the state back self.state.update(current) # schedule the next timer 60 seconds from the current event time ctx.timer_service().register_event_time_timer(current[2] + 60000) def on_timer(self, timestamp: int, ctx: \u0026#39;KeyedProcessFunction.OnTimerContext\u0026#39;): # get the state for the key that scheduled the timer result = self.state.value() # check if this is an outdated timer or the latest timer if timestamp == result[2] + 60000: # emit the state on timeout yield result[0], result[1] class MyTimestampAssigner(TimestampAssigner): def __init__(self): self.epoch = datetime.datetime.utcfromtimestamp(0) def extract_timestamp(self, value, record_timestamp) -\u0026gt; int: return int((value[0] - self.epoch).total_seconds() * 1000) if __name__ == \u0026#39;__main__\u0026#39;: env = StreamExecutionEnvironment.get_execution_environment() t_env = StreamTableEnvironment.create(stream_execution_environment=env) t_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE my_source ( a TIMESTAMP(3), b VARCHAR, c VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;rows-per-second\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) stream = t_env.to_append_stream( t_env.from_path(\u0026#39;my_source\u0026#39;), Types.ROW([Types.SQL_TIMESTAMP(), Types.STRING(), Types.STRING()])) watermarked_stream = stream.assign_timestamps_and_watermarks( WatermarkStrategy.for_monotonous_timestamps() .with_timestamp_assigner(MyTimestampAssigner())) # apply the process function onto a keyed stream result = watermarked_stream.key_by(lambda value: value[1]) \\ .process(CountWithTimeoutFunction()) \\ .print() env.execute() Before Flink 1.4.0, when called from a processing-time timer, the ProcessFunction.onTimer() method sets the current processing time as event-time timestamp. This behavior is very subtle and might not be noticed by users. Well, it\u0026rsquo;s harmful because processing-time timestamps are indeterministic and not aligned with watermarks. Besides, user-implemented logic depends on this wrong timestamp highly likely is unintendedly faulty. So we\u0026rsquo;ve decided to fix it. Upon upgrading to 1.4.0, Flink jobs that are using this incorrect event-time timestamp will fail, and users should adapt their jobs to the correct logic. The KeyedProcessFunction # KeyedProcessFunction, as an extension of ProcessFunction, gives access to the key of timers in its onTimer(...) method.
Java @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;OUT\u0026gt; out) throws Exception { K key = ctx.getCurrentKey(); // ... } Scala override def onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT]): Unit = { var key = ctx.getCurrentKey // ... } Python def on_timer(self, timestamp: int, ctx: \u0026#39;KeyedProcessFunction.OnTimerContext\u0026#39;): key = ctx.get_current_key() # ... Timers # Both types of timers (processing-time and event-time) are internally maintained by the TimerService and enqueued for execution.
The TimerService deduplicates timers per key and timestamp, i.e., there is at most one timer per key and timestamp. If multiple timers are registered for the same timestamp, the onTimer() method will be called just once.
Flink synchronizes invocations of onTimer() and processElement(). Hence, users do not have to worry about concurrent modification of state.
Fault Tolerance # Timers are fault tolerant and checkpointed along with the state of the application. In case of a failure recovery or when starting an application from a savepoint, the timers are restored.
Checkpointed processing-time timers that were supposed to fire before their restoration, will fire immediately. This might happen when an application recovers from a failure or when it is started from a savepoint. Timers are always asynchronously checkpointed, except for the combination of RocksDB backend / with incremental snapshots / with heap-based timers (will be resolved with FLINK-10026). Notice that large numbers of timers can increase the checkpointing time because timers are part of the checkpointed state. See the \u0026ldquo;Timer Coalescing\u0026rdquo; section for advice on how to reduce the number of timers. Timer Coalescing # Since Flink maintains only one timer per key and timestamp, you can reduce the number of timers by reducing the timer resolution to coalesce them.
For a timer resolution of 1 second (event or processing time), you can round down the target time to full seconds. Timers will fire at most 1 second earlier but not later than requested with millisecond accuracy. As a result, there are at most one timer per key and second.
Java long coalescedTime = ((ctx.timestamp() + timeout) / 1000) * 1000; ctx.timerService().registerProcessingTimeTimer(coalescedTime); Scala val coalescedTime = ((ctx.timestamp + timeout) / 1000) * 1000 ctx.timerService.registerProcessingTimeTimer(coalescedTime) Python coalesced_time = ((ctx.timestamp() + timeout) // 1000) * 1000 ctx.timer_service().register_processing_time_timer(coalesced_time) Since event-time timers only fire with watermarks coming in, you may also schedule and coalesce these timers with the next watermark by using the current one:
Java long coalescedTime = ctx.timerService().currentWatermark() + 1; ctx.timerService().registerEventTimeTimer(coalescedTime); Scala val coalescedTime = ctx.timerService.currentWatermark + 1 ctx.timerService.registerEventTimeTimer(coalescedTime) Python coalesced_time = ctx.timer_service().current_watermark() + 1 ctx.timer_service().register_event_time_timer(coalesced_time) Timers can also be stopped and removed as follows:
Stopping a processing-time timer:
Java long timestampOfTimerToStop = ...; ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop); Scala val timestampOfTimerToStop = ... ctx.timerService.deleteProcessingTimeTimer(timestampOfTimerToStop) Python timestamp_of_timer_to_stop = ... ctx.timer_service().delete_processing_time_timer(timestamp_of_timer_to_stop) Stopping an event-time timer:
Java long timestampOfTimerToStop = ...; ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop); Scala val timestampOfTimerToStop = ... ctx.timerService.deleteEventTimeTimer(timestampOfTimerToStop) Python timestamp_of_timer_to_stop = ... ctx.timer_service().delete_event_time_timer(timestamp_of_timer_to_stop) Stopping a timer has no effect if no such timer with the given timestamp is registered. Back to top
`}),e.add({id:115,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/process_function/",title:"Process Function",section:"Operators",content:` Process Function # ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).
Please refer to Process Function for more details about the concept and usage of ProcessFunction.
Execution behavior of timer # Python user-defined functions are executed in a separate Python process from Flink\u0026rsquo;s operators which run in a JVM, the timer registration requests made in ProcessFunction will be sent to the Java operator asynchronously. Once received timer registration requests, the Java operator will register it into the underlying timer service.
If the registered timer has already passed the current time (the current system time for processing time timer, or the current watermark for event time), it will be triggered immediately.
Note that, due to the asynchronous processing characteristics, it may happen that the timer was triggered a little later than the actual time. For example, a registered processing time timer of 10:00:00 may be actually processed at 10:00:05.
`}),e.add({id:116,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/protobuf/",title:"Protobuf",section:"Formats",content:` Protobuf Format # Format: Serialization Schema Format: Deserialization Schema
The Protocol Buffers Protobuf format allows you to read and write Protobuf data, based on Protobuf generated classes.
Dependencies # In order to use the Protobuf format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-protobuf\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a table with Protobuf format # Here is an example to create a table using the Kafka connector and Protobuf format.
Below is the proto definition file.
syntax = \u0026#34;proto2\u0026#34;; package com.example; option java_package = \u0026#34;com.example\u0026#34;; option java_multiple_files = true; message SimpleTest { optional int64 uid = 1; optional string name = 2; optional int32 category_type = 3; optional bytes content = 4; optional double price = 5; map\u0026lt;int64, InnerMessageTest\u0026gt; value_map = 6; repeated InnerMessageTest value_arr = 7; optional Corpus corpus_int = 8; optional Corpus corpus_str = 9; message InnerMessageTest{ optional int64 v1 =1; optional int32 v2 =2; } enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 7; } } Use protoc command to compile the .proto file to java classes Then compile and package the classes (there is no need to package proto-java into the jar) Finally you should provide the jar in your classpath, e.g. pass it using -j in sql-client CREATE TABLE simple_test ( uid BIGINT, name STRING, category_type INT, content BINARY, price DOUBLE, value_map map\u0026lt;BIGINT, row\u0026lt;v1 BIGINT, v2 INT\u0026gt;\u0026gt;, value_arr array\u0026lt;row\u0026lt;v1 BIGINT, v2 INT\u0026gt;\u0026gt;, corpus_int INT, corpus_str STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;protobuf\u0026#39;, \u0026#39;protobuf.message-class-name\u0026#39; = \u0026#39;com.example.SimpleTest\u0026#39;, \u0026#39;protobuf.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39; ) Format Options # Option Required Forwarded Default Type Description format required no (none) String Specify what format to use, here should be 'protobuf'. protobuf.message-class-name required no (none) String The full name of a Protobuf generated class. The name must match the message name in the proto definition file. \$ is supported for inner class names, like 'com.exmample.OuterClass\$MessageClass' protobuf.ignore-parse-errors optional no false Boolean Optional flag to skip rows with parse errors instead of failing. protobuf.read-default-values optional yes false Boolean This option only works if the generated class's version is proto2. If this value is set to true, the format will read empty values as the default values defined in the proto file. If the value is set to false, the format will generate null values if the data element does not exist in the binary protobuf message. If the proto syntax is proto3, this value will forcibly be set to true, because proto3's standard is to use default values. protobuf.write-null-string-literal optional no "" String When serializing to protobuf data, this is the optional config to specify the string literal in Protobuf's array/map in case of null values. Data Type Mapping # The following table lists the type mapping from Flink type to Protobuf type.
Flink SQL type Protobuf type Description CHAR / VARCHAR / STRING string BOOLEAN bool BINARY / VARBINARY bytes INT int32 BIGINT int64 FLOAT float DOUBLE double ARRAY repeated Elements cannot be null, the string default value can be specified by write-null-string-literal MAP map Keys or values cannot be null, the string default value can be specified by write-null-string-literal ROW message VARCHAR / CHAR / TINYINT / SMALLINT / INTEGER / BIGINT enum The enum value of protobuf can be mapped to string or number of flink row accordingly. Null Values # As protobuf does not permit null values in maps and array, we need to auto-generate default values when converting from Flink Rows to Protobuf.
Protobuf Data Type Default Value int32 / int64 / float / double 0 string "" bool false enum first element of enum binary ByteString.EMPTY message MESSAGE.getDefaultInstance() OneOf field # In the serialization process, there\u0026rsquo;s no guarantee that the Flink fields of the same one-of group only contain at most one valid value. When serializing, each field is set in the order of Flink schema, so the field in the higher position will override the field in lower position in the same one-of group.
You can refer to Language Guide (proto2) or Language Guide (proto3) for more information about Protobuf types.
`}),e.add({id:117,href:"/flink/flink-docs-master/zh/docs/dev/python/",title:"Python API",section:"应用开发",content:" "}),e.add({id:118,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select/",title:"SELECT 与 WHERE",section:"Queries 查询",content:` SELECT 与 WHERE 子句 # Batch Streaming
SELECT 语句的常见语法格式如下所示：
SELECT select_list FROM table_expression [ WHERE boolean_expression ] 这里的 table_expression 可以是任意的数据源。它可以是一张已经存在的表、视图或者 VALUES 子句，也可以是多个现有表的关联结果、或一个子查询。这里我们假设 Orders 表在 Catalog 中处于可用状态，那么下面的语句会从 Orders 表中读出所有的行。
SELECT * FROM Orders 在 select_list 处的 * 表示查询操作将会解析所有列。但是，我们不鼓励在生产中使用 *，因为它会使查询操作在应对 Catalog 变化的时候鲁棒性降低。相反，可以在 select_list 处指定可用列的子集，或者使用声明的列进行计算。例如，假设 Orders 表中有名为 order_id、price 和 tax 的列，那么你可以编写如下查询：
SELECT order_id, price + tax FROM Orders 查询操作还可以在 VALUES 子句中使用内联数据。每一个元组对应一行，另外可以通过设置别名来为每一列指定名称。
SELECT order_id, price FROM (VALUES (1, 2.0), (2, 3.1)) AS t (order_id, price) 可以根据 WHERE 子句对行数据进行过滤。
SELECT price + tax FROM Orders WHERE id = 10 此外，在任意一行的列上你可以调用内置函数和用户自定义标量函数（user-defined scalar functions）。当然，在使用前用户自定义函数（ user-defined functions）必须已经注册到 Catalog 中。
SELECT PRETTY_PRINT(order_id) FROM Orders Back to top
`}),e.add({id:119,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/formats/text_files/",title:"Text files",section:"Formats",content:` Text files format # Flink 支持使用 TextLineInputFormat 从文件中读取文本行。此 format 使用 Java 的内置 InputStreamReader 以支持的字符集编码来解码字节流。 要使用该 format，你需要将 Flink Connector Files 依赖项添加到项目中：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-files\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; PyFlink 用户可直接使用相关接口，无需添加依赖。
此 format 与新 Source 兼容，可以在批处理和流模式下使用。 因此，你可以通过两种方式使用此 format：
批处理模式的有界读取 流模式的连续读取：监视目录中出现的新文件 有界读取示例:
在此示例中，我们创建了一个 DataStream，其中包含作为字符串的文本文件的行。 此处不需要水印策略，因为记录不包含事件时间戳。
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), /* Flink Path */) .build(); final DataStream\u0026lt;String\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python source = FileSource.for_record_stream_format(StreamFormat.text_line_format(), *path).build() stream = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) 连续读取示例: 在此示例中，我们创建了一个 DataStream，随着新文件被添加到目录中，其中包含的文本文件行的字符串流将无限增长。我们每秒会进行新文件监控。 此处不需要水印策略，因为记录不包含事件时间戳。
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(new TextLineInputFormat(), /* Flink Path */) .monitorContinuously(Duration.ofSeconds(1L)) .build(); final DataStream\u0026lt;String\u0026gt; stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;file-source\u0026#34;); Python source = FileSource \\ .for_record_stream_format(StreamFormat.text_line_format(), *path) \\ .monitor_continously(Duration.of_seconds(1)) \\ .build() stream = env.from_source(source, WatermarkStrategy.no_watermarks(), \u0026#34;file-source\u0026#34;) `}),e.add({id:120,href:"/flink/flink-docs-master/zh/docs/connectors/table/upsert-kafka/",title:"Upsert Kafka",section:"Table API Connectors",content:` Upsert Kafka SQL 连接器 # Scan Source: Unbounded Sink: Streaming Upsert Mode
Upsert Kafka 连接器支持以 upsert 方式从 Kafka topic 中读取数据并将数据写入 Kafka topic。
作为 source，upsert-kafka 连接器生产 changelog 流，其中每条数据记录代表一个更新或删除事件。更准确地说，数据记录中的 value 被解释为同一 key 的最后一个 value 的 UPDATE，如果有这个 key（如果不存在相应的 key，则该更新被视为 INSERT）。用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。
作为 sink，upsert-kafka 连接器可以消费 changelog 流。它会将 INSERT/UPDATE_AFTER 数据作为正常的 Kafka 消息写入，并将 DELETE 数据以 value 为空的 Kafka 消息写入（表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中。
依赖 # In order to use the Upsert Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Upsert Kafka version Maven dependency SQL Client JAR universal \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kafka\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Upsert Kafka 连接器不是二进制发行版的一部分，请查阅这里了解如何在集群运行中引用 Upsert Kafka 连接器。
完整示例 # 下面的示例展示了如何创建和使用 Upsert Kafka 表：
CREATE TABLE pageviews_per_region ( user_region STRING, pv BIGINT, uv BIGINT, PRIMARY KEY (user_region) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews_per_region\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;key.format\u0026#39; = \u0026#39;avro\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro\u0026#39; ); CREATE TABLE pageviews ( user_id BIGINT, page_id BIGINT, viewtime TIMESTAMP, user_region STRING, WATERMARK FOR viewtime AS viewtime - INTERVAL \u0026#39;2\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ); -- 计算 pv、uv 并插入到 upsert-kafka sink INSERT INTO pageviews_per_region SELECT user_region, COUNT(*), COUNT(DISTINCT user_id) FROM pageviews GROUP BY user_region; 注意 确保在 DDL 中定义主键。
Available Metadata # See the regular Kafka connector for a list of all available metadata fields.
连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必选 (none) String 指定要使用的连接器，Upsert Kafka 连接器使用：'upsert-kafka'。 topic 必选 (none) String 用于读取和写入的 Kafka topic 名称。 properties.bootstrap.servers 必选 (none) String 以逗号分隔的 Kafka brokers 列表。 properties.* 可选 (none) String 该选项可以传递任意的 Kafka 参数。选项的后缀名必须匹配定义在 Kafka 参数文档中的参数名。 Flink 会自动移除 选项名中的 "properties." 前缀，并将转换后的键名以及值传入 KafkaClient。 例如，你可以通过 'properties.allow.auto.create.topics' = 'false' 来禁止自动创建 topic。 但是，某些选项，例如'key.deserializer' 和 'value.deserializer' 是不允许通过该方式传递参数，因为 Flink 会重写这些参数的值。 key.format 必选 (none) String 用于对 Kafka 消息中 key 部分序列化和反序列化的格式。key 字段由 PRIMARY KEY 语法指定。支持的格式包括 'csv'、'json'、'avro'。请参考格式页面以获取更多详细信息和格式参数。 key.fields-prefix optional (none) String Defines a custom prefix for all fields of the key format to avoid name clashes with fields of the value format. By default, the prefix is empty. If a custom prefix is defined, both the table schema and 'key.fields' will work with prefixed names. When constructing the data type of the key format, the prefix will be removed and the non-prefixed names will be used within the key format. Please note that this option requires that 'value.fields-include' must be set to 'EXCEPT_KEY'. value.format 必选 (none) String 用于对 Kafka 消息中 value 部分序列化和反序列化的格式。支持的格式包括 'csv'、'json'、'avro'。请参考格式页面以获取更多详细信息和格式参数。 value.fields-include 必选 'ALL' String 控制哪些字段应该出现在 value 中。可取值： ALL：消息的 value 部分将包含 schema 中所有的字段，包括定义为主键的字段。 EXCEPT_KEY：记录的 value 部分包含 schema 的所有字段，定义为主键的字段除外。 sink.parallelism 可选 (none) Integer 定义 upsert-kafka sink 算子的并行度。默认情况下，由框架确定并行度，与上游链接算子的并行度保持一致。 sink.buffer-flush.max-rows 可选 0 Integer 缓存刷新前，最多能缓存多少条记录。当 sink 收到很多同 key 上的更新时，缓存将保留同 key 的最后一条记录，因此 sink 缓存能帮助减少发往 Kafka topic 的数据量，以及避免发送潜在的 tombstone 消息。 可以通过设置为 '0' 来禁用它。默认，该选项是未开启的。注意，如果要开启 sink 缓存，需要同时设置 'sink.buffer-flush.max-rows' 和 'sink.buffer-flush.interval' 两个选项为大于零的值。 sink.buffer-flush.interval 可选 0 Duration 缓存刷新的间隔时间，超过该时间后异步线程将刷新缓存数据。当 sink 收到很多同 key 上的更新时，缓存将保留同 key 的最后一条记录，因此 sink 缓存能帮助减少发往 Kafka topic 的数据量，以及避免发送潜在的 tombstone 消息。 可以通过设置为 '0' 来禁用它。默认，该选项是未开启的。注意，如果要开启 sink 缓存，需要同时设置 'sink.buffer-flush.max-rows' 和 'sink.buffer-flush.interval' 两个选项为大于零的值。 特性 # Key and Value Formats # See the regular Kafka connector for more explanation around key and value formats. However, note that this connector requires both a key and value format where the key fields are derived from the PRIMARY KEY constraint.
The following example shows how to specify and configure key and value formats. The format options are prefixed with either the 'key' or 'value' plus format identifier.
CREATE TABLE KafkaTable ( \`ts\` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39;, \`user_id\` BIGINT, \`item_id\` BIGINT, \`behavior\` STRING, PRIMARY KEY (\`user_id\`) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, ... \u0026#39;key.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;key.json.ignore-parse-errors\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;value.json.fail-on-missing-field\u0026#39; = \u0026#39;false\u0026#39;, \u0026#39;value.fields-include\u0026#39; = \u0026#39;EXCEPT_KEY\u0026#39; ) 主键约束 # Upsert Kafka 始终以 upsert 方式工作，并且需要在 DDL 中定义主键。在具有相同主键值的消息按序存储在同一个分区的前提下，在 changelog source 定义主键意味着 在物化后的 changelog 上主键具有唯一性。定义的主键将决定哪些字段出现在 Kafka 消息的 key 中。
一致性保证 # 默认情况下，如果启用 checkpoint，Upsert Kafka sink 会保证至少一次将数据插入 Kafka topic。
这意味着，Flink 可以将具有相同 key 的重复记录写入 Kafka topic。但由于该连接器以 upsert 的模式工作，该连接器作为 source 读入时，可以确保具有相同主键值下仅最后一条消息会生效。因此，upsert-kafka 连接器可以像 HBase sink 一样实现幂等写入。
为每个分区生成相应的 watermark # Flink 支持根据 Upsert Kafka 的 每个分区的数据特性发送相应的 watermark。当使用这个特性的时候，watermark 是在 Kafka consumer 内部生成的。 合并每个分区 生成的 watermark 的方式和 stream shuffle 的方式是一致的。 数据源产生的 watermark 是取决于该 consumer 负责的所有分区中当前最小的 watermark。如果该 consumer 负责的部分分区是 idle 的，那么整体的 watermark 并不会前进。在这种情况下，可以通过设置合适的 table.exec.source.idle-timeout 来缓解这个问题。
如想获得更多细节，请查阅 Kafka watermark strategies.
数据类型映射 # Upsert Kafka 用字节存储消息的 key 和 value，因此没有 schema 或数据类型。消息按格式进行序列化和反序列化，例如：csv、json、avro。因此数据类型映射表由指定的格式确定。请参考格式页面以获取更多详细信息。
Back to top
`}),e.add({id:121,href:"/flink/flink-docs-master/zh/docs/flinkdev/ide_setup/",title:"导入 Flink 到 IDE 中",section:"Flink 开发",content:` 导入 Flink 到 IDE 中 # 以下章节描述了如何将 Flink 项目导入到 IDE 中以进行 Flink 本身的源码开发。有关 Flink 程序编写的信息，请参阅 Java API 和 Scala API 快速入门指南。
每当你的 IDE 无法正常工作时，请优先尝试使用 Maven 命令行（mvn clean package -DskipTests），因为它可能是由于你的 IDE 中存在错误或未正确设置。 准备 # 首先，请从我们的仓库中拉取 Flink 源，例如：
git clone https://github.com/apache/flink.git 忽略重构提交 # 我们在 .git-blame-ignore-revs 中保留了一个大的重构提交列表。使用 git blame 查看更改注释时，忽略这些注释会很有帮助。你可以使用以下方法来配置 git 和你的 IDE：
git config blame.ignoreRevsFile .git-blame-ignore-revs IntelliJ IDEA # 该指南介绍了关于如何设置 IntelliJ IDEA IDE 来进行 Flink 核心开发。众所周知由于 Eclipse 混合 Scala 和 Java 项目时存在问题，因此越来越多的贡献者正在迁移到 IntelliJ IDEA。
以下文档描述了 IntelliJ IDEA 2020.3 (https://www.jetbrains.com/idea/download/) 的设置步骤以及 Flink 的导入步骤。
安装 Scala 插件 # IntelliJ 提供了插件设置来安装 Scala 插件。如果尚未安装，请在导入 Flink 之前按照以下说明来进行操作以启用对 Scala 项目和文件的支持：
转到 IntelliJ Settings → Plugins 并选择 \u0026ldquo;Marketplace\u0026rdquo;。 选择并安装 \u0026ldquo;Scala\u0026rdquo; 插件。 如果出现提示，请重启 IntelliJ。 导入 Flink # 启动 IntelliJ IDEA 并选择 New → Project from Existing Sources。 选择已克隆的 Flink 存储库的根文件夹。 选择 \u0026ldquo;Import project from external model\u0026rdquo;，然后选择 \u0026ldquo;Maven\u0026rdquo;。 保留默认选项，然后依次单击 \u0026ldquo;Next\u0026rdquo;，直到到达 SDK 部分。 如果未列出 SDK，请使用左上角的 \u0026ldquo;+\u0026rdquo; 号创建一个。选择 \u0026ldquo;JDK\u0026rdquo;，选择 JDK 主目录，然后单击 \u0026ldquo;OK\u0026rdquo;。选择最合适的 JDK 版本。注意：一个好的经验法则是选择与活动 Maven 配置文件匹配的 JDK 版本。 单击 \u0026ldquo;Next\u0026rdquo; 继续，直到完成导入。 右键单击已导入的 Flink 项目 → Maven → Generate Sources and Update Folders。请注意：这会将 Flink 库安装在本地 Maven 存储库中，默认情况下位于 \u0026ldquo;/home/\$USER/.m2/repository/org/apache/flink/\u0026quot;。另外 mvn clean package -DskipTests 也可以创建 IDE 运行所需的文件，但无需安装库。 编译项目（Build → Make Project）。 代码格式化 # 我们使用 Spotless plugin 和 google-java-format 一起格式化我们的 Java 代码。
你可以通过以下步骤来将 IDE 配置为在保存时自动应用格式设置：
下载 google-java-format plugin v1.7.0.6 打开 Settings → Plugins，点击齿轮图标并选择 \u0026ldquo;Install Plugin from Disk\u0026rdquo;。导航到下载的 zip 文件并选择它。 在插件设置中，启用插件并将代码样式更改为 \u0026ldquo;AOSP\u0026rdquo;（4 个空格的缩进）。 请记住不要将此插件更新为更高版本！ 安装 Save Actions plugin。 启用插件，以及 \u0026ldquo;Optimize imports\u0026rdquo; 和 \u0026ldquo;Reformat file\u0026rdquo;。 在 \u0026ldquo;Save Actions\u0026rdquo; 设置页面中，为 .*\\.java 设置 \u0026ldquo;File Path Inclusion\u0026rdquo;。否则你将在编辑其他文件中意外的重新格式化。 Java 规范检查 # IntelliJ 使用 Checkstyle-IDEA 插件在 IDE 中支持 checkstyle。
从 IntelliJ 插件存储库中安装 \u0026ldquo;Checkstyle-IDEA\u0026rdquo; 插件。 通过 Settings → Tools → Checkstyle 配置插件。 将 \u0026ldquo;Scan Scope\u0026rdquo; 设置为仅 Java 源（包括测试）。 在 \u0026ldquo;Checkstyle Version\u0026rdquo; 下拉菜单中选择 8.14 版本，然后单击 \u0026ldquo;apply\u0026rdquo;。此步骤很重要，请勿跳过！ 在 \u0026ldquo;Configuration File\u0026rdquo; 窗格中，点击 \u0026ldquo;+\u0026rdquo; 图标添加新配置： 将 \u0026ldquo;Description\u0026rdquo; 设置为 Flink。 选择 \u0026ldquo;Use a local Checkstyle file\u0026rdquo; ，然后将其指向你存储库中 \u0026quot;tools/maven/checkstyle.xml\u0026quot; 文件。 选中 \u0026ldquo;Store relative to project location\u0026rdquo; 框，然后单击 \u0026ldquo;Next\u0026rdquo;。 将 \u0026ldquo;checkstyle.suppressions.file\u0026rdquo; 属性值配置为 \u0026quot;suppressions.xml\u0026quot;，然后单击 \u0026ldquo;Next\u0026rdquo;，然后单击 \u0026ldquo;Finish\u0026rdquo;。 选择 \u0026ldquo;Flink\u0026rdquo; 作为唯一的激活配置文件，单击 \u0026ldquo;Apply\u0026rdquo;，然后点击 \u0026ldquo;OK\u0026rdquo;。 Checkstyle 现在将在编辑器中针对任何违反 Checkstyle 的行为发出警告。 插件安装完成后你可以通过 Settings → Editor → Code Style → Java → Scheme 下拉框旁边的齿轮图标， 直接导入 \u0026quot;tools/maven/checkstyle.xml\u0026quot; 文件。 例如，这将自动调整导入布局。
你可以通过打开 Checkstyle 工具窗口并单击 \u0026ldquo;Check Module\u0026rdquo; 按钮来扫描整个模块。扫描不应报告任何错误。
存在一些模块没有完全被 CheckStyle 格式化，其中包括 flink-core，flink-optimizer 和 flink-runtime。不过，请确保你在这些模块中添加/修改的代码仍然符合 checkstyle 规则。 Scala 规范检查 # 在 IntelliJ 中启用 scalastyle，通过选择 Settings → Editor → Inspections，然后搜索 \u0026ldquo;Scala style inspections\u0026rdquo;，还要放置 \u0026quot;tools/maven/scalastyle-config.xml\u0026quot; 在 \u0026quot;\u0026lt;root\u0026gt;/.idea\u0026quot; 或 \u0026quot;\u0026lt;root\u0026gt;/project\u0026quot; 目录中。
版权信息 # 每个文件都需要包含 Apache 许可证作为标头。这可以在 IntelliJ 中通过添加 Copyright Profile 来自动配置：
打开 Settings → Editor → Copyright → Copyright Profiles。
添加一个新的版权文件命名为 Apache。
添加以下文本作为许可证文本：
Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. 转到 Editor → Copyright 并选择 Apache 版权文件作为该项目的默认版权文件。
点击 Apply，保存变更配置。
常问问题 # 本节列出了开发人员过去使用 IntelliJ 时遇到的问题：
编译失败 invalid flag: --add-exports=java.base/sun.net.util=ALL-UNNAMED 这意味着 IntelliJ 仍激活了 Java 11 配置文件，尽管使用了较旧的 JDK。解决方法：打开 Maven 工具窗口（View → tool Windows → Maven），取消选中 Java 11 配置文件并重新导入项目。
编译失败 cannot find symbol: symbol: method defineClass(...) location: class sun.misc.Unsafe 这意味着 IntelliJ 正在为该项目使用 JDK 11，但是你正在使用不支持 Java 11 的 Flink 版本。这通常在将 IntelliJ 设置为使用 JDK 11 并检出 Flink 的旧版本（\u0026lt;= 1.9）时发生。解决方法：打开项目设置窗口（File → Project Structure → Project Settings: Project），然后选择 JDK 8 作为项目 SDK。如果要使用 JDK 11，则可能必须在切换回新的 Flink 版本后恢复此状态。
编译失败 程序包sun.misc不存在 这意味着 IntelliJ 正在为该项目使用 JDK 11, 但使用了 --release 编译选项来编译 JDK8 字节码，这个编译选项和Flink当前的build方式不兼容。解决办法： 进入 \u0026ldquo;Preferences\u0026rdquo; → \u0026ldquo;Build, Execution, Deployment\u0026rdquo; → \u0026ldquo;Compiler\u0026rdquo; → \u0026ldquo;Java Compiler\u0026rdquo;, 取消选中 \u0026ldquo;Use \u0026lsquo;\u0026ndash;release\u0026rsquo; option for cross-compilation (Java 9 and later)\u0026rdquo;.
运行 Flink Examples 且 Flink 出现关于 NoClassDefFoundError 错误信息 这可能是由于将 Flink 依赖项设置为 provided，导致它们没有自动放置在类路径中。你可以在运行配置中选中 \u0026ldquo;Include dependencies with \u0026lsquo;Provided\u0026rsquo; scope\u0026rdquo; 框，也可以创建一个调用 main() 方法的测试示例（provided 依赖关系在测试类路径中可用）。
Eclipse # 注意: 根据我们的经验，以下配置会导致 Flink 无法正常工作，如：由于与 Scala IDE 3.0.3 捆绑在一起的旧 Eclipse 版本的缺陷或者是由于 Scala IDE 4.4.1 中绑定的 Scala 版本不兼容。 我们建议改为使用 IntelliJ（请参见上文）
PyCharm # 关于如何为开发 flink-python 模块而设置 PyCharm 的简要指南。
以下文档介绍了使用 Flink Python 源设置 PyCharm 2019.1.3 (https://www.jetbrains.com/pycharm/download/) 的步骤。
导入 flink-python # 如果你位于 PyCharm 启动界面中：
启动 PyCharm，然后选择 \u0026ldquo;Open\u0026rdquo;。 在克隆的 Flink 仓库中选择 flink-python 文件夹。 如果你使用 PyCharm 打开了一个项目：
选择 File → Open。 在克隆的 Flink 仓库中选择 flink-python 文件夹。 Python 规范检查 # Apache Flink 的 Python 代码检查样式应在项目中引入 flake8 的外部工具。
将 flake8 安装在使用的 Python 解释器中（请参阅(https://pypi.org/project/flake8/)）。 选择 \u0026ldquo;PyCharm → Preferences\u0026hellip; → Tools → External Tools → +（在右侧页面的左下角）\u0026quot;，然后配置外部工具。 将 \u0026ldquo;Name\u0026rdquo; 设置为 \u0026ldquo;flake8\u0026rdquo;。 将 \u0026ldquo;Description\u0026rdquo; 设置为 \u0026ldquo;code style check\u0026rdquo;。 将 \u0026ldquo;Program\u0026rdquo; 设置为 Python 解释器路径（例如 /usr/bin/python）。 将 \u0026ldquo;Arguments\u0026rdquo; 设置为 \u0026ldquo;-m flake8 --config=tox.ini\u0026rdquo;。 将 \u0026ldquo;Working directory\u0026rdquo; 设置为 \u0026ldquo;\$ProjectFileDir\$\u0026quot;。 现在，你可以通过以下操作来检查你的 Python 代码样式： \u0026ldquo;右键单击 flink-python 项目中的任何文件或文件夹 → External Tools → flake8\u0026rdquo;。
Back to top
`}),e.add({id:122,href:"/flink/flink-docs-master/zh/docs/try-flink/table_api/",title:"基于 Table API 实现实时报表",section:"Try Flink",content:` 基于 Table API 实现实时报表 # Apache Flink 提供了 Table API 作为批流统一的关系型 API。也就是说，在无界的实时流数据或者有界的批数据集上进行查询具有相同的语义，得到的结果一致。 Flink 的 Table API 可以简化数据分析、构建数据流水线以及 ETL 应用的定义。
你接下来要搭建的是什么系统？ # 在本教程中，你将学习构建一个通过账户来追踪金融交易的实时看板。 数据流水线为：先从 Kafka 中读取数据，再将结果写入到 MySQL 中，最后通过 Grafana 展示。
准备条件 # 我们默认你对 Java 或者 Scala 有一定了解，当然如果你使用的是其他编程语言，也可以继续学习。 同时也默认你了解基本的关系型概念，例如 SELECT 、GROUP BY 等语句。
困难求助 # 如果遇到问题，可以参考 社区支持资源。 Flink 的 用户邮件列表 是 Apahe 项目中最活跃的一个，这也是快速寻求帮助的重要途径。
在 Windows 环境下，如果用来生成数据的 docker 容器启动失败，请检查使用的脚本是否正确。 例如 docker-entrypoint.sh 是容器 table-walkthrough_data-generator_1 所需的 bash 脚本。 如果不可用，会报 standard_init_linux.go:211: exec user process caused \u0026ldquo;no such file or directory\u0026rdquo; 的错误。 一种解决办法是在 docker-entrypoint.sh 的第一行将脚本执行器切换到 sh 如何跟着教程练习 # 本教程依赖如下运行环境：
Java 11 Maven Docker 注意： 本文中使用的 Apache Flink Docker 镜像仅适用于 Apache Flink 发行版。
由于你目前正在浏览快照版的文档，因此下文中引用的分支可能已经不存在了，请先通过左侧菜单下方的版本选择器切换到发行版文档再查看。
我们使用的配置文件位于 flink-playgrounds 代码仓库中， 当下载完成后，在你的 IDE 中打开 flink-playground/table-walkthrough 项目，并找到文件 SpendReport。
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); tEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); Table transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); 代码分析 # 执行环境 # 前两行代码创建了 TableEnvironment（表环境）。 创建表环境时，你可以设置作业属性，定义应用的批流模式，以及创建数据源。 我们先创建一个标准的表环境，并选择流式执行器。
EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(settings); 注册表 # 接下来，在当前 catalog 中创建表，这样就可以读写外部系统的数据，批流数据都可以。 表类型的 source 可以读取外部系统中的数据，这些外部系统可以是数据库、键值类型存储、消息队列或者文件系统。 而表类型的 sink 则可以将表中的数据写到外部存储系统。 不同的 source 或 sink 类型，有不同的表格式（formats），例如 CSV, JSON, Avro, 或者 Parquet。
tEnv.executeSql(\u0026#34;CREATE TABLE transactions (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; amount BIGINT,\\n\u0026#34; + \u0026#34; transaction_time TIMESTAMP(3),\\n\u0026#34; + \u0026#34; WATERMARK FOR transaction_time AS transaction_time - INTERVAL \u0026#39;5\u0026#39; SECOND\\n\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;topic\u0026#39; = \u0026#39;transactions\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); 这里注册了两张表：一张存交易数据的输入表，一张存消费报告的输出表。 交易表（transactions）存的是信用卡交易记录，记录包含账户 ID（account_id），交易时间（transaction_time），以及美元单位的金额（amount）。 交易表实际是一个 Kafka topic 上的逻辑视图，视图对应的 topic 是 transactions，表格式是 CSV。
tEnv.executeSql(\u0026#34;CREATE TABLE spend_report (\\n\u0026#34; + \u0026#34; account_id BIGINT,\\n\u0026#34; + \u0026#34; log_ts TIMESTAMP(3),\\n\u0026#34; + \u0026#34; amount BIGINT\\n,\u0026#34; + \u0026#34; PRIMARY KEY (account_id, log_ts) NOT ENFORCED\u0026#34; + \u0026#34;) WITH (\\n\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysql:3306/sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;spend_report\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;driver\u0026#39; = \u0026#39;com.mysql.jdbc.Driver\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;sql-demo\u0026#39;,\\n\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;demo-sql\u0026#39;\\n\u0026#34; + \u0026#34;)\u0026#34;); 第二张 spend_report 表存储聚合后的最终结果，底层存储是 MySQL 数据库中的一张表。
查询数据 # 配置好环境并注册好表后，你就可以开始开发你的第一个应用了。 通过 TableEnvironment 实例 ，你可以使用函数 from 从输入表读取数据，然后将结果调用 executeInsert 写入到输出表。 函数 report 用于实现具体的业务逻辑，这里暂时未实现。
Table transactions = tEnv.from(\u0026#34;transactions\u0026#34;); report(transactions).executeInsert(\u0026#34;spend_report\u0026#34;); 测试 # 项目还包含一个测试类 SpendReportTest，辅助验证报表逻辑。 该测试类的表环境使用的是批处理模式。
EnvironmentSettings settings = EnvironmentSettings.inBatchMode(); TableEnvironment tEnv = TableEnvironment.create(settings); 提供批流统一的语义是 Flink 的重要特性，这意味着应用的开发和测试可以在批模式下使用静态数据集完成，而实际部署到生产时再切换为流式。
尝试下 # 在作业拉起来的大体处理框架下，你可以再添加一些业务逻辑。 现在的目标是创建一个报表，报表按照账户显示一天中每个小时的总支出。因此，毫秒粒度的时间戳字段需要向下舍入到小时。
Flink 支持使用纯 SQL 或者 Table API 开发关系型数据应用。
其中，Table API 是受 SQL 启发设计出的一套链式 DSL，可以用 Python、Java 或者 Scala 开发，在 IDE 中也集成的很好。 同时也如 SQL 查询一样，Table 应用可以按列查询，或者按列分组。 通过类似 floor 以及 sum 这样的 系统函数，你已经可以开发这个报表了。
public static Table report(Table transactions) { return transactions.select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;transaction_time\u0026#34;).floor(TimeIntervalUnit.HOUR).as(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 用户自定义函数 # Flink 内置的函数是有限的，有时是需要通过 用户自定义函数来拓展这些函数。 假如 floor 函数不是系统预设函数，也可以自己实现。
import java.time.LocalDateTime; import java.time.temporal.ChronoUnit; import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.functions.ScalarFunction; public class MyFloor extends ScalarFunction { public @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime eval( @DataTypeHint(\u0026#34;TIMESTAMP(3)\u0026#34;) LocalDateTime timestamp) { return timestamp.truncatedTo(ChronoUnit.HOURS); } } 然后就可以在你的应用中使用了。
public static Table report(Table transactions) { return transactions.select( \$(\u0026#34;account_id\u0026#34;), call(MyFloor.class, \$(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 这条查询会从表 transactions 消费所有的记录，然后计算报表所需内容，最后将结果以高效、可拓展的方式输出。 按此逻辑实现，可以通过测试。
添加窗口函数 # 在数据处理中，按照时间做分组是常见操作，在处理无限流时更是如此。 按时间分组的函数叫 window，Flink 提供了灵活的窗口函数语法。 最常见的窗口是 Tumble ，窗口区间长度固定，并且区间不重叠。
public static Table report(Table transactions) { return transactions .window(Tumble.over(lit(1).hour()).on(\$(\u0026#34;transaction_time\u0026#34;)).as(\u0026#34;log_ts\u0026#34;)) .groupBy(\$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;)) .select( \$(\u0026#34;account_id\u0026#34;), \$(\u0026#34;log_ts\u0026#34;).start().as(\u0026#34;log_ts\u0026#34;), \$(\u0026#34;amount\u0026#34;).sum().as(\u0026#34;amount\u0026#34;)); } 上面的代码含义为：使用滚动窗口，窗口按照指定的时间戳字段划分，区间为一小时。 比如，时间戳为 2019-06-01 01:23:47 的行会进入窗口 2019-06-01 01:00:00中。
不同于其他属性，时间在一个持续不断的流式应用中总是向前移动，因此基于时间的聚合总是不重复的。
不同于 floor 以及 UDF，窗口函数是 [内部的]intrinsics，可以运行时优化。 批环境中，如果需要按照时间属性分组数据，窗口函数也有便利的 API。
按此逻辑实现，测试也可以通过。
再用流式处理一次！ # 这次编写的应用是一个功能齐全、有状态的分布式流式应用！ 查询语句持续消费 Kafka 中交易数据流，然后计算每小时的消费，最后当窗口结束时立刻提交结果。 由于输入是无边界的，停止作业需要手工操作。 同时，由于作业使用了窗口函数，Flink 会执行一些特定的优化，例如当框架检测出窗口结束时，清理状态数据。
本次教程中的流式应用，已经完全容器化，并可以在本地运行。 环境中具体包含了 Kafka 的 topic、持续数据生成器、MySQL 以及 Grafana。
在 table-walkthrough 目录下启动 docker-compose 脚本。
\$ docker-compose build \$ docker-compose up -d 运行中的作业信息可以通过 Flink console 查看。
结果数据在 MySQL 中查看。
\$ docker-compose exec mysql mysql -Dsql-demo -usql-demo -pdemo-sql mysql\u0026gt; use sql-demo; Database changed mysql\u0026gt; select count(*) from spend_report; +----------+ | count(*) | +----------+ | 110 | +----------+ 完整的可视化结果可以访问 Grafana！
`}),e.add({id:123,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/",title:"流式概念",section:"Table API \u0026 SQL",content:""}),e.add({id:124,href:"/flink/flink-docs-master/zh/docs/deployment/memory/",title:"内存配置",section:"Deployment",content:""}),e.add({id:125,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_setup_jobmanager/",title:"配置 JobManager 内存",section:"内存配置",content:` 配置 JobManager 内存 # JobManager 是 Flink 集群的控制单元。 它由三种不同的组件组成：ResourceManager、Dispatcher 和每个正在运行作业的 JobMaster。 本篇文档将介绍 JobManager 内存在整体上以及细粒度上的配置方法。
本文接下来介绍的内存配置方法适用于 1.11 及以上版本。 Flink 在 1.11 版本中对内存配置部分进行了较大幅度的改动，从早期版本升级的用户请参考升级指南。
提示 本篇内存配置文档仅针对 JobManager！ 与 TaskManager 相比，JobManager 具有相似但更加简单的内存模型。
配置总内存 # 配置 JobManager 内存最简单的方法就是进程的配置总内存。 本地执行模式下不需要为 JobManager 进行内存配置，配置参数将不会生效。
详细配置 # 如上图所示，下表中列出了 Flink JobManager 内存模型的所有组成部分，以及影响其大小的相关配置参数。
组成部分 配置参数 描述 JVM 堆内存 jobmanager.memory.heap.size JobManager 的 JVM 堆内存。 堆外内存 jobmanager.memory.off-heap.size JobManager 的堆外内存（直接内存或本地内存）。 JVM Metaspace jobmanager.memory.jvm-metaspace.size Flink JVM 进程的 Metaspace。 JVM 开销 jobmanager.memory.jvm-overhead.min jobmanager.memory.jvm-overhead.max jobmanager.memory.jvm-overhead.fraction 用于其他 JVM 开销的本地内存，例如栈空间、垃圾回收空间等。该内存部分为基于进程总内存的受限的等比内存部分。 配置 JVM 堆内存 # 如配置总内存中所述，另一种配置 JobManager 内存的方式是明确指定 JVM 堆内存的大小（jobmanager.memory.heap.size）。 通过这种方式，用户可以更好地掌控用于以下用途的 JVM 堆内存大小。
Flink 框架 在作业提交时（例如一些特殊的批处理 Source）及 Checkpoint 完成的回调函数中执行的用户代码 Flink 需要多少 JVM 堆内存，很大程度上取决于运行的作业数量、作业的结构及上述用户代码的需求。
提示 如果已经明确设置了 JVM 堆内存，建议不要再设置进程总内存或 Flink 总内存，否则可能会造成内存配置冲突。
在启动 JobManager 进程时，Flink 启动脚本及客户端通过设置 JVM 参数 -Xms 和 -Xmx 来管理 JVM 堆空间的大小。 请参考 JVM 参数。
配置堆外内存 # 堆外内存包括 JVM 直接内存 和 本地内存。 可以通过配置参数 jobmanager.memory.enable-jvm-direct-memory-limit 设置是否启用 JVM 直接内存限制。 如果该配置项设置为 true，Flink 会根据配置的堆外内存大小设置 JVM 参数 -XX:MaxDirectMemorySize。 请参考 JVM 参数。
可以通过配置参数 jobmanager.memory.off-heap.size 设置堆外内存的大小。 如果遇到 JobManager 进程抛出 “OutOfMemoryError: Direct buffer memory” 的异常，可以尝试调大这项配置。 请参考常见问题。
以下情况可能用到堆外内存：
Flink 框架依赖（例如 Akka 的网络通信） 在作业提交时（例如一些特殊的批处理 Source）及 Checkpoint 完成的回调函数中执行的用户代码 提示 如果同时配置了 Flink 总内存和 JVM 堆内存，且没有配置堆外内存，那么堆外内存的大小将会是 Flink 总内存减去JVM 堆内存。 这种情况下，堆外内存的默认大小将不会生效。
本地执行 # 如果你是在本地运行 Flink（例如在 IDE 中）而非创建一个集群，那么 JobManager 的内存配置将不会生效。
`}),e.add({id:126,href:"/flink/flink-docs-master/zh/docs/deployment/advanced/logging/",title:"日志",section:"Advanced",content:` 如何使用日志记录 # 所有 Flink 进程都会创建一个文本格式的日志文件，其中包含该进程中发生的各种事件的信息。 这些日志提供了深入了解 Flink 内部工作的途径，同时可以用来输出检测出的问题（以 WARN/ERROR 消息的形式），还可以辅助调试问题。
日志文件可以通过 Job-/TaskManager 对应的 WebUI 页面访问。所使用的 Resource Provider（如 YARN）可能会提供额外的访问方式来访问日志。
Flink 中的日志记录是使用 SLF4J 日志接口实现的。这允许你不需要修改 Flink 的源代码就可以使用任何支持 SLF4J 的日志框架。
默认情况下，使用 Log4j 2 作为底层日志框架。
配置 Log4j 2 # Log4j 2 是通过 property 配置文件进行配置的。
Flink 发行版在 conf 目录中附带了以下 log4j 配置文件，如果启用了 Log4j 2，则会自动使用如下文件：
log4j-cli.properties：Flink 命令行使用（例如 flink run）； log4j-session.properties：Flink 命令行在启动基于 Kubernetes/Yarn 的 Session 集群时使用（例如 kubernetes-session.sh/yarn-session.sh）； log4j-console.properties：Job-/TaskManagers 在前台模式运行时使用（例如 Kubernetes）； log4j.properties： Job-/TaskManagers 默认使用的日志配置。 Log4j 会定期扫描这些文件的变更，并在必要时调整日志记录行为。默认情况下30秒检查一次，监测间隔可以通过 Log4j 配置文件的 monitorInterval 配置项进行设置。
与 Log4j 1 的兼容 # Flink 附带了 Log4j API bridge 相关的依赖，使当前基于 Log4j1 开发的应用程序可以继续正常运行。
如果有基于 Log4j 1 的自定义配置文件或代码，请查看官方 Log4j 兼容和迁移指南。
配置 Log4j1 # 要将 Flink 与 Log4j 1 一起使用，必须确保：
Classpath 中不存在 org.apache.logging.log4j:log4j-core、org.apache.logging.log4j:log4j-slf4j-impl 和 org.apache.logging.log4j:log4j-1.2-api； 且 Classpath 中存在 log4j:log4j、org.slf4j:slf4j-log4j12、org.apache.logging.log4j:log4j-to-slf4j 和 org.apache.logging.log4j:log4j-api。 如果在 IDE 中使用 Log4j 1，则必须在 pom 文件中使用上述 Classpath 中应该存在的 jars 依赖项来替换 Classpath 中不应该存在的 jars 依赖项，并尽可能的排除那些传递依赖于 Classpath 中不存在 jars 的依赖项。
对于 Flink 发行版，这意味着你必须
从 lib 目录中移除 log4j-core，log4j-slf4j-impl 和 log4j-1.2-api jars； 往 lib 目录中添加 log4j，slf4j-log4j12 和 log4j-to-slf4j jars； 用适配的 Log4j1 版本替换 conf 目录中的所有 log4j 配置文件。 配置 logback # 要将 Flink 与 logback 一起使用，必须确保：
Classpath 中不存在 org.apache.logging.log4j:log4j-slf4j-impl； Classpath 中存在 ch.qos.logback:logback-core 和 ch.qos.logback:logback-classic。 如果在 IDE 中使用 logback，则必须在 pom 文件中使用上述 Classpath 中应该存在的 jars 依赖项来替换 Classpath 中不应该存在的 jars 依赖项，并尽可能的排除那些传递依赖于 Classpath 中不存在 jars 的依赖项。
对于 Flink 发行版，这意味着你必须
从 lib 目录中移除 log4j-slf4j-impl jars； 向 lib 目录中添加 logback-core 和 logback-classic jars。 Flink 发行版在 conf 目录中附带了以下 logback 配置文件，如果启用了 logback，则会自动使用这些文件：
logback-session.properties: Flink 命令行在启动基于 Kubernetes/Yarn 的 Session 集群时使用（例如 kubernetes-session.sh/yarn-session.sh）； logback-console.properties：Job-/TaskManagers 在前台模式运行时使用（例如 Kubernetes）； logback.xml: 命令行和 Job-/TaskManager 默认使用的日志配置。 Logback 1.3+ 需要 SLF4J 2，目前不支持。 开发人员的最佳实践 # 通过将相应 Class 的类型对象作为参数调用 org.slf4j.LoggerFactory#LoggerFactory.getLogger 方法可以创建一个 SLF4J 的 logger。
强烈建议将 logger 字段设置为 private static final 修饰的类型。
import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class Foobar { private static final Logger LOG = LoggerFactory.getLogger(Foobar.class); public static void main(String[] args) { LOG.info(\u0026#34;Hello world!\u0026#34;); } } 为了最大限度地利用 SLF4J，建议使用其占位符机制。使用占位符可以在日志级别设置得太高而不会记录消息的情况下避免不必要的字符串构造。
占位符的语法如下：
LOG.info(\u0026#34;This message contains {} placeholders. {}\u0026#34;, 2, \u0026#34;Yippie\u0026#34;); 占位符也可以和要记录的异常一起使用。
catch(Exception exception){ LOG.error(\u0026#34;An {} occurred.\u0026#34;, \u0026#34;error\u0026#34;, exception); } Back to top
`}),e.add({id:127,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/versioned_tables/",title:"时态表（Temporal Tables）",section:"流式概念",content:` 时态表（Temporal Tables） # 时态表（Temporal Table）是一张随时间变化的表 \u0026ndash; 在 Flink 中称为动态表，时态表中的每条记录都关联了一个或多个时间段，所有的 Flink 表都是时态的（动态的）。
时态表包含表的一个或多个有版本的表快照，时态表可以是一张跟踪所有变更记录的表（例如数据库表的 changelog，包含多个表快照），也可以是物化所有变更之后的表（例如数据库表，只有最新表快照）。
版本: 时态表可以划分成一系列带版本的表快照集合，表快照中的版本代表了快照中所有记录的有效区间，有效区间的开始时间和结束时间可以通过用户指定，根据时态表是否可以追踪自身的历史版本与否，时态表可以分为 版本表 和 普通表。
版本表: 如果时态表中的记录可以追踪和并访问它的历史版本，这种表我们称之为版本表，来自数据库的 changelog 可以定义成版本表。
普通表: 如果时态表中的记录仅仅可以追踪并和它的最新版本，这种表我们称之为普通表，来自数据库 或 HBase 的表可以定义成普通表。
设计初衷 # 关联一张版本表 # 以订单流关联产品表这个场景举例，orders 表包含了来自 Kafka 的实时订单流，product_changelog 表来自数据库表 products 的 changelog , 产品的价格在数据库表 products 中是随时间实时变化的。
SELECT * FROM product_changelog; (changelog kind) update_time product_id product_name price ================= =========== ========== ============ ===== +(INSERT) 00:01:00 p_001 scooter 11.11 +(INSERT) 00:02:00 p_002 basketball 23.11 -(UPDATE_BEFORE) 12:00:00 p_001 scooter 11.11 +(UPDATE_AFTER) 12:00:00 p_001 scooter 12.99 -(UPDATE_BEFORE) 12:00:00 p_002 basketball 23.11 +(UPDATE_AFTER) 12:00:00 p_002 basketball 19.99 -(DELETE) 18:00:00 p_001 scooter 12.99 表 product_changelog 表示数据库表 products不断增长的 changelog, 比如，产品 scooter 在时间点 00:01:00的初始价格是 11.11, 在 12:00:00 的时候涨价到了 12.99, 在 18:00:00 的时候这条产品价格记录被删除。
如果我们想输出 product_changelog 表在 10:00:00 对应的版本，表的内容如下所示：
update_time product_id product_name price =========== ========== ============ ===== 00:01:00 p_001 scooter 11.11 00:02:00 p_002 basketball 23.11 如果我们想输出 product_changelog 表在 13:00:00 对应的版本，表的内容如下所示：
update_time product_id product_name price =========== ========== ============ ===== 12:00:00 p_001 scooter 12.99 12:00:00 p_002 basketball 19.99 上述例子中，products 表的版本是通过 update_time 和 product_id 进行追踪的，product_id 对应 product_changelog 表的主键，update_time 对应事件时间。
在 Flink 中, 这由版本表表示。
关联一张普通表 # 另一方面，某些用户案列需要连接变化的维表，该表是外部数据库表。
假设 LatestRates 是一个物化的最新汇率表 (比如：一张 HBase 表)，LatestRates 总是表示 HBase 表 Rates 的最新内容。
我们在 10:15:00 时查询到的内容如下所示：
10:15:00 \u0026gt; SELECT * FROM LatestRates; currency rate ========= ==== US Dollar 102 Euro 114 Yen 1 我们在 11:00:00 时查询到的内容如下所示：
11:00:00 \u0026gt; SELECT * FROM LatestRates; currency rate ========= ==== US Dollar 102 Euro 116 Yen 1 在 Flink 中, 这由普通表表示。
时态表 # Flink 使用主键约束和事件时间来定义一张版本表和版本视图。
声明版本表 # 在 Flink 中，定义了主键约束和事件时间属性的表就是版本表。
-- 定义一张版本表 CREATE TABLE product_changelog ( product_id STRING, product_name STRING, product_price DECIMAL(10, 4), update_time TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, PRIMARY KEY(product_id) NOT ENFORCED, -- (1) 定义主键约束 WATERMARK FOR update_time AS update_time -- (2) 通过 watermark 定义事件时间 ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39; ); 行 (1) 为表 product_changelog 定义了主键, 行 (2) 把 update_time 定义为表 product_changelog 的事件时间，因此 product_changelog 是一张版本表。
注意: METADATA FROM 'value.source.timestamp' VIRTUAL 语法的意思是从每条 changelog 中抽取 changelog 对应的数据库表中操作的执行时间，强烈推荐使用数据库表中操作的 执行时间作为事件时间 ，否则通过时间抽取的版本可能和数据库中的版本不匹配。
声明版本视图 # Flink 也支持定义版本视图只要一个视图包含主键和事件时间便是一个版本视图。
假设我们有表 RatesHistory 如下所示：
-- 定义一张 append-only 表 CREATE TABLE RatesHistory ( currency_time TIMESTAMP(3), currency STRING, rate DECIMAL(38, 10), WATERMARK FOR currency_time AS currency_time -- 定义事件时间 ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;rates\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; -- 普通的 append-only 流 ) 表 RatesHistory 代表一个兑换日元货币汇率表（日元汇率为1），该表是不断增长的 append-only 表。 例如，欧元 兑换 日元 从 09:00:00 到 10:45:00 的汇率为 114。从 10:45:00 到 11:15:00 的汇率为 116。
SELECT * FROM RatesHistory; currency_time currency rate ============= ========= ==== 09:00:00 US Dollar 102 09:00:00 Euro 114 09:00:00 Yen 1 10:45:00 Euro 116 11:15:00 Euro 119 11:49:00 Pounds 108 为了在 RatesHistory 上定义版本表，Flink 支持通过去重查询定义版本视图， 去重查询可以产出一个有序的 changelog 流，去重查询能够推断主键并保留原始数据流的事件时间属性。
CREATE VIEW versioned_rates AS SELECT currency, rate, currency_time -- (1) \`currency_time\` 保留了事件时间 FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY currency -- (2) \`currency\` 是去重 query 的 unique key，可以作为主键 ORDER BY currency_time DESC) AS rowNum FROM RatesHistory ) WHERE rowNum = 1; -- 视图 \`versioned_rates\` 将会产出如下的 changelog: (changelog kind) currency_time currency rate ================ ============= ========= ==== +(INSERT) 09:00:00 US Dollar 102 +(INSERT) 09:00:00 Euro 114 +(INSERT) 09:00:00 Yen 1 +(UPDATE_AFTER) 10:45:00 Euro 116 +(UPDATE_AFTER) 11:15:00 Euro 119 +(INSERT) 11:49:00 Pounds 108 行 (1) 保留了事件时间作为视图 versioned_rates 的事件时间，行 (2) 使得视图 versioned_rates 有了主键, 因此视图 versioned_rates 是一个版本视图。
视图中的去重 query 会被 Flink 优化并高效地产出 changelog stream, 产出的 changelog 保留了主键约束和事件时间。
如果我们想输出 versioned_rates 表在 11:00:00 对应的版本，表的内容如下所示：
currency_time currency rate ============= ========== ==== 09:00:00 US Dollar 102 09:00:00 Yen 1 10:45:00 Euro 116 如果我们想输出 versioned_rates 表在 12:00:00 对应的版本，表的内容如下所示：
currency_time currency rate ============= ========== ==== 09:00:00 US Dollar 102 09:00:00 Yen 1 10:45:00 Euro 119 11:49:00 Pounds 108 声明普通表 # 普通表的声明和 Flink 建表 DDL 一致，参考 create table 页面获取更多如何建表的信息。
-- 用 DDL 定义一张 HBase 表，然后我们可以在 SQL 中将其当作一张时态表使用 -- \u0026#39;currency\u0026#39; 列是 HBase 表中的 rowKey CREATE TABLE LatestRates ( currency STRING, fam1 ROW\u0026lt;rate DOUBLE\u0026gt; ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;hbase-1.4\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;rates\u0026#39;, \u0026#39;zookeeper.quorum\u0026#39; = \u0026#39;localhost:2181\u0026#39; ); 注意 理论上讲任意都能用作时态表并在基于处理时间的时态表 Join 中使用，但当前支持作为时态表的普通表必须实现接口 LookupableTableSource。接口 LookupableTableSource 的实例只能作为时态表用于基于处理时间的时态 Join 。
通过 LookupableTableSource 定义的表意味着该表具备了在运行时通过一个或多个 key 去查询外部存储系统的能力，当前支持在 基于处理时间的时态表 join 中使用的表包括 JDBC, HBase 和 Hive。
另请参阅 LookupableTableSource页面了解更多信息。
在基于处理时间的时态表 Join 中支持任意表作为时态表会在不远的将来支持。
Back to top
`}),e.add({id:128,href:"/flink/flink-docs-master/zh/docs/dev/datastream/event-time/",title:"事件时间",section:"DataStream API",content:""}),e.add({id:129,href:"/flink/flink-docs-master/zh/docs/learn-flink/etl/",title:"数据管道 \u0026 ETL",section:"实践练习",content:` 数据管道 \u0026amp; ETL # Apache Flink 的一种常见应用场景是 ETL（抽取、转换、加载）管道任务。从一个或多个数据源获取数据，进行一些转换操作和信息补充，将结果存储起来。在这个教程中，我们将介绍如何使用 Flink 的 DataStream API 实现这类应用。
这里注意，Flink 的 Table 和 SQL API 完全可以满足很多 ETL 使用场景。但无论你最终是否直接使用 DataStream API，对这里介绍的基本知识有扎实的理解都是有价值的。
无状态的转换 # 本节涵盖了 map() 和 flatmap()，这两种算子可以用来实现无状态转换的基本操作。本节中的示例建立在你已经熟悉 flink-training-repo 中的出租车行程数据的基础上。
map() # 在第一个练习中，你将过滤出租车行程数据中的事件。在同一代码仓库中，有一个 GeoUtils 类，提供了一个静态方法 GeoUtils.mapToGridCell(float lon, float lat)，它可以将位置坐标（经度，维度）映射到 100x100 米的对应不同区域的网格单元。
现在让我们为每个出租车行程时间的数据对象增加 startCell 和 endCell 字段。你可以创建一个继承 TaxiRide 的 EnrichedRide 类，添加这些字段：
public static class EnrichedRide extends TaxiRide { public int startCell; public int endCell; public EnrichedRide() {} public EnrichedRide(TaxiRide ride) { this.rideId = ride.rideId; this.isStart = ride.isStart; ... this.startCell = GeoUtils.mapToGridCell(ride.startLon, ride.startLat); this.endCell = GeoUtils.mapToGridCell(ride.endLon, ride.endLat); } public String toString() { return super.toString() + \u0026#34;,\u0026#34; + Integer.toString(this.startCell) + \u0026#34;,\u0026#34; + Integer.toString(this.endCell); } } 然后你可以创建一个应用来转换这个流
DataStream\u0026lt;TaxiRide\u0026gt; rides = env.addSource(new TaxiRideSource(...)); DataStream\u0026lt;EnrichedRide\u0026gt; enrichedNYCRides = rides .filter(new RideCleansingSolution.NYCFilter()) .map(new Enrichment()); enrichedNYCRides.print(); 使用这个 MapFunction:
public static class Enrichment implements MapFunction\u0026lt;TaxiRide, EnrichedRide\u0026gt; { @Override public EnrichedRide map(TaxiRide taxiRide) throws Exception { return new EnrichedRide(taxiRide); } } flatmap() # MapFunction 只适用于一对一的转换：对每个进入算子的流元素，map() 将仅输出一个转换后的元素。对于除此以外的场景，你将要使用 flatmap()。
DataStream\u0026lt;TaxiRide\u0026gt; rides = env.addSource(new TaxiRideSource(...)); DataStream\u0026lt;EnrichedRide\u0026gt; enrichedNYCRides = rides .flatMap(new NYCEnrichment()); enrichedNYCRides.print(); 其中用到的 FlatMapFunction :
public static class NYCEnrichment implements FlatMapFunction\u0026lt;TaxiRide, EnrichedRide\u0026gt; { @Override public void flatMap(TaxiRide taxiRide, Collector\u0026lt;EnrichedRide\u0026gt; out) throws Exception { FilterFunction\u0026lt;TaxiRide\u0026gt; valid = new RideCleansing.NYCFilter(); if (valid.filter(taxiRide)) { out.collect(new EnrichedRide(taxiRide)); } } } 使用接口中提供的 Collector ，flatmap() 可以输出你想要的任意数量的元素，也可以一个都不发。
Back to top
Keyed Streams # keyBy() # 将一个流根据其中的一些属性来进行分区是十分有用的，这样我们可以使所有具有相同属性的事件分到相同的组里。例如，如果你想找到从每个网格单元出发的最远的出租车行程。按 SQL 查询的方式来考虑，这意味着要对 startCell 进行 GROUP BY 再排序，在 Flink 中这部分可以用 keyBy(KeySelector) 实现。
rides .flatMap(new NYCEnrichment()) .keyBy(enrichedRide -\u0026gt; enrichedRide.startCell) 每个 keyBy 会通过 shuffle 来为数据流进行重新分区。总体来说这个开销是很大的，它涉及网络通信、序列化和反序列化。
通过计算得到键 # KeySelector 不仅限于从事件中抽取键。你也可以按想要的方式计算得到键值，只要最终结果是确定的，并且实现了 hashCode() 和 equals()。这些限制条件不包括产生随机数或者返回 Arrays 或 Enums 的 KeySelector，但你可以用元组和 POJO 来组成键，只要他们的元素遵循上述条件。
键必须按确定的方式产生，因为它们会在需要的时候被重新计算，而不是一直被带在流记录中。
例如，比起创建一个新的带有 startCell 字段的 EnrichedRide 类，用这个字段作为 key：
keyBy(enrichedRide -\u0026gt; enrichedRide.startCell) 我们更倾向于这样做：
keyBy(ride -\u0026gt; GeoUtils.mapToGridCell(ride.startLon, ride.startLat)) Keyed Stream 的聚合 # 以下代码为每个行程结束事件创建了一个新的包含 startCell 和时长（分钟）的元组流：
import org.joda.time.Interval; DataStream\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; minutesByStartCell = enrichedNYCRides .flatMap(new FlatMapFunction\u0026lt;EnrichedRide, Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt;() { @Override public void flatMap(EnrichedRide ride, Collector\u0026lt;Tuple2\u0026lt;Integer, Minutes\u0026gt;\u0026gt; out) throws Exception { if (!ride.isStart) { Interval rideInterval = new Interval(ride.startTime, ride.endTime); Minutes duration = rideInterval.toDuration().toStandardMinutes(); out.collect(new Tuple2\u0026lt;\u0026gt;(ride.startCell, duration)); } } }); 现在就可以产生一个流，对每个 startCell 仅包含那些最长行程的数据。
有很多种方法表示使用哪个字段作为键。前面使用 EnrichedRide POJO 的例子，用字段名来指定键。而这个使用 Tuple2 对象的例子中，用字段在元组中的序号（从0开始）来指定键。
minutesByStartCell .keyBy(value -\u0026gt; value.f0) // .keyBy(value -\u0026gt; value.startCell) .maxBy(1) // duration .print(); 现在每次行程时长达到新的最大值，都会输出一条新记录，例如下面这个对应 50797 网格单元的数据：
... 4\u0026gt; (64549,5M) 4\u0026gt; (46298,18M) 1\u0026gt; (51549,14M) 1\u0026gt; (53043,13M) 1\u0026gt; (56031,22M) 1\u0026gt; (50797,6M) ... 1\u0026gt; (50797,8M) ... 1\u0026gt; (50797,11M) ... 1\u0026gt; (50797,12M) （隐式的）状态 # 这是培训中第一个涉及到有状态流的例子。尽管状态的处理是透明的，Flink 必须跟踪每个不同的键的最大时长。
只要应用中有状态，你就应该考虑状态的大小。如果键值的数量是无限的，那 Flink 的状态需要的空间也同样是无限的。
在流处理场景中，考虑有限窗口的聚合往往比整个流聚合更有意义。
reduce() 和其他聚合算子 # 上面用到的 maxBy() 只是 Flink 中 KeyedStream 上众多聚合函数中的一个。还有一个更通用的 reduce() 函数可以用来实现你的自定义聚合。
Back to top
有状态的转换 # Flink 为什么要参与状态管理？ # 在 Flink 不参与管理状态的情况下，你的应用也可以使用状态，但 Flink 为其管理状态提供了一些引人注目的特性：
本地性: Flink 状态是存储在使用它的机器本地的，并且可以以内存访问速度来获取 持久性: Flink 状态是容错的，例如，它可以自动按一定的时间间隔产生 checkpoint，并且在任务失败后进行恢复 纵向可扩展性: Flink 状态可以存储在集成的 RocksDB 实例中，这种方式下可以通过增加本地磁盘来扩展空间 横向可扩展性: Flink 状态可以随着集群的扩缩容重新分布 可查询性: Flink 状态可以通过使用 状态查询 API 从外部进行查询。 在本节中你将学习如何使用 Flink 的 API 来管理 keyed state。
Rich Functions # 至此，你已经看到了 Flink 的几种函数接口，包括 FilterFunction， MapFunction，和 FlatMapFunction。这些都是单一抽象方法模式。
对其中的每一个接口，Flink 同样提供了一个所谓 \u0026ldquo;rich\u0026rdquo; 的变体，如 RichFlatMapFunction，其中增加了以下方法，包括：
open(Configuration c) close() getRuntimeContext() open() 仅在算子初始化时调用一次。可以用来加载一些静态数据，或者建立外部服务的链接等。
getRuntimeContext() 为整套潜在有趣的东西提供了一个访问途径，最明显的，它是你创建和访问 Flink 状态的途径。
一个使用 Keyed State 的例子 # 在这个例子里，想象你有一个要去重的事件数据流，对每个键只保留第一个事件。下面是完成这个功能的应用，使用一个名为 Deduplicator 的 RichFlatMapFunction ：
private static class Event { public final String key; public final long timestamp; ... } public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new EventSource()) .keyBy(e -\u0026gt; e.key) .flatMap(new Deduplicator()) .print(); env.execute(); } 为了实现这个功能，Deduplicator 需要记录每个键是否已经有了相应的记录。它将通过使用 Flink 的 keyed state 接口来做这件事。
当你使用像这样的 keyed stream 的时候，Flink 会为每个状态中管理的条目维护一个键值存储。
Flink 支持几种不同方式的 keyed state，这个例子使用的是最简单的一个，叫做 ValueState。意思是对于 每个键 ，Flink 将存储一个单一的对象 —— 在这个例子中，存储的是一个 Boolean 类型的对象。
我们的 Deduplicator 类有两个方法：open() 和 flatMap()。open() 方法通过定义 ValueStateDescriptor\u0026lt;Boolean\u0026gt; 建立了管理状态的使用。构造器的参数定义了这个状态的名字（“keyHasBeenSeen”），并且为如何序列化这些对象提供了信息（在这个例子中的 Types.BOOLEAN）。
public static class Deduplicator extends RichFlatMapFunction\u0026lt;Event, Event\u0026gt; { ValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; @Override public void open(Configuration conf) { ValueStateDescriptor\u0026lt;Boolean\u0026gt; desc = new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;keyHasBeenSeen\u0026#34;, Types.BOOLEAN); keyHasBeenSeen = getRuntimeContext().getState(desc); } @Override public void flatMap(Event event, Collector\u0026lt;Event\u0026gt; out) throws Exception { if (keyHasBeenSeen.value() == null) { out.collect(event); keyHasBeenSeen.update(true); } } } 当 flatMap 方法调用 keyHasBeenSeen.value() 时，Flink 会在 当前键的上下文 中检索状态值，只有当状态为 null 时，才会输出当前事件。这种情况下，它同时也将更新 keyHasBeenSeen 为 true。
这种访问和更新按键分区的状态的机制也许看上去很神奇，因为在 Deduplicator 的实现中，键不是明确可见的。当 Flink 运行时调用 RichFlatMapFunction 的 open 方法时， 是没有事件的，所以这个时候上下文中不含有任何键。但当它调用 flatMap 方法，被处理的事件的键在运行时中就是可用的了，并且被用来确定操作哪个 Flink 状态后端的入口。
部署在分布式集群时，将会有很多 Deduplicator 的实例，每一个实例将负责整个键空间的互斥子集中的一个。所以，当你看到一个单独的 ValueState，比如
ValueState\u0026lt;Boolean\u0026gt; keyHasBeenSeen; 要理解这个代表的不仅仅是一个单独的布尔类型变量，而是一个分布式的共享键值存储。
清理状态 # 上面例子有一个潜在的问题：当键空间是无界的时候将发生什么？Flink 会对每个使用过的键都存储一个 Boolean 类型的实例。如果是键是有限的集合还好，但在键无限增长的应用中，清除再也不会使用的状态是很必要的。这通过在状态对象上调用 clear() 来实现，如下：
keyHasBeenSeen.clear() 对一个给定的键值，你也许想在它一段时间不使用后来做这件事。当学习 ProcessFunction 的相关章节时，你将看到在事件驱动的应用中怎么用定时器来做这个。
也可以选择使用 状态的过期时间（TTL），为状态描述符配置你想要旧状态自动被清除的时间。
Non-keyed State # 在没有键的上下文中我们也可以使用 Flink 管理的状态。这也被称作 算子的状态。它包含的接口是很不一样的，由于对用户定义的函数来说使用 non-keyed state 是不太常见的，所以这里就不多介绍了。这个特性最常用于 source 和 sink 的实现。
Back to top
Connected Streams # 相比于下面这种预先定义的转换：
有时你想要更灵活地调整转换的某些功能，比如数据流的阈值、规则或者其他参数。Flink 支持这种需求的模式称为 connected streams ，一个单独的算子有两个输入流。
connected stream 也可以被用来实现流的关联。
示例 # 在这个例子中，一个控制流是用来指定哪些词需要从 streamOfWords 里过滤掉的。 一个称为 ControlFunction 的 RichCoFlatMapFunction 作用于连接的流来实现这个功能。
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; control = env .fromElements(\u0026#34;DROP\u0026#34;, \u0026#34;IGNORE\u0026#34;) .keyBy(x -\u0026gt; x); DataStream\u0026lt;String\u0026gt; streamOfWords = env .fromElements(\u0026#34;Apache\u0026#34;, \u0026#34;DROP\u0026#34;, \u0026#34;Flink\u0026#34;, \u0026#34;IGNORE\u0026#34;) .keyBy(x -\u0026gt; x); control .connect(streamOfWords) .flatMap(new ControlFunction()) .print(); env.execute(); } 这里注意两个流只有键一致的时候才能连接。 keyBy 的作用是将流数据分区，当 keyed stream 被连接时，他们必须按相同的方式分区。这样保证了两个流中所有键相同的事件发到同一个实例上。这样也使按键关联两个流成为可能。
在这个例子中，两个流都是 DataStream\u0026lt;String\u0026gt; 类型的，并且都将字符串作为键。正如你将在下面看到的，RichCoFlatMapFunction 在状态中存了一个布尔类型的变量，这个变量被两个流共享。
public static class ControlFunction extends RichCoFlatMapFunction\u0026lt;String, String, String\u0026gt; { private ValueState\u0026lt;Boolean\u0026gt; blocked; @Override public void open(Configuration config) { blocked = getRuntimeContext() .getState(new ValueStateDescriptor\u0026lt;\u0026gt;(\u0026#34;blocked\u0026#34;, Boolean.class)); } @Override public void flatMap1(String control_value, Collector\u0026lt;String\u0026gt; out) throws Exception { blocked.update(Boolean.TRUE); } @Override public void flatMap2(String data_value, Collector\u0026lt;String\u0026gt; out) throws Exception { if (blocked.value() == null) { out.collect(data_value); } } } RichCoFlatMapFunction 是一种可以被用于一对连接流的 FlatMapFunction，并且它可以调用 rich function 的接口。这意味着它可以是有状态的。
布尔变量 blocked 被用于记录在数据流 control 中出现过的键（在这个例子中是单词），并且这些单词从 streamOfWords 过滤掉。这是 keyed state，并且它是被两个流共享的，这也是为什么两个流必须有相同的键值空间。
在 Flink 运行时中，flatMap1 和 flatMap2 在连接流有新元素到来时被调用 —— 在我们的例子中，control 流中的元素会进入 flatMap1，streamOfWords 中的元素会进入 flatMap2。这是由两个流连接的顺序决定的，本例中为 control.connect(streamOfWords)。
认识到你没法控制 flatMap1 和 flatMap2 的调用顺序是很重要的。这两个输入流是相互竞争的关系，Flink 运行时将根据从一个流或另一个流中消费的事件做它要做的。对于需要保证时间和/或顺序的场景，你会发现在 Flink 的管理状态中缓存事件一直到它们能够被处理是必须的。（注意：如果你真的迫切需要，可以使用自定义的算子实现 InputSelectable 接口，在两输入算子消费它的输入流时增加一些顺序上的限制。）
Back to top
动手练习 # 本节的动手练习是 行程和票价练习 。
Back to top
延展阅读 # 数据流转换 有状态流的处理 Back to top
`}),e.add({id:130,href:"/flink/flink-docs-master/zh/docs/ops/debugging/application_profiling/",title:"应用程序分析与调试",section:"Debugging",content:` 应用程序分析与调试 # Apache Flink 自定义日志概述 # 每个独立的 JobManager，TaskManager，HistoryServer，ZooKeeper 守护进程都将 stdout 和 stderr 重定向到名称后缀为 .out 的文件，并将其内部的日志记录写入到 .log 后缀的文件。用户可以在 env.java.opts，env.java.opts.jobmanager，env.java.opts.taskmanager，env.java.opts.historyserver 和 env.java.opts.client 配置项中配置 Java 选项（包括 log 相关的选项），同样也可以使用脚本变量 FLINK_LOG_PREFIX 定义日志文件，并将选项括在双引号中以供后期使用。日志文件将使用 FLINK_LOG_PREFIX 与默认的 .out 和 .log 后缀一起滚动。
使用 Java Flight Recorder 分析 # Java Flight Recorder 是 Oracle JDK 内置的分析和事件收集框架。Java Mission Control 是一套先进的工具，可以对 Java Flight Recorder 收集的大量数据进行高效和详细的分析。配置示例：
env.java.opts: \u0026#34;-XX:+UnlockCommercialFeatures -XX:+UnlockDiagnosticVMOptions -XX:+FlightRecorder -XX:+DebugNonSafepoints -XX:FlightRecorderOptions=defaultrecording=true,dumponexit=true,dumponexitpath=\${FLINK_LOG_PREFIX}.jfr\u0026#34; 使用 JITWatch 分析 # JITWatch Java HotSpot JIT 编译器的日志分析器和可视化工具，用于检查内联决策、热点方法、字节码和汇编。配置示例：
env.java.opts: \u0026#34;-XX:+UnlockDiagnosticVMOptions -XX:+TraceClassLoading -XX:+LogCompilation -XX:LogFile=\${FLINK_LOG_PREFIX}.jit -XX:+PrintAssembly\u0026#34; 分析内存溢出（OOM）问题 # 如果你的 Flink 应用程序遇到 OutOfMemoryExceptions，那么启用在内存溢出错误时堆转储（Heap Dump）是一个好主意。
env.java.opts: \u0026#34;-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=\${FLINK_LOG_PREFIX}.hprof\u0026#34; 堆转储（Heap Dump）将使你能够方便地分析用户代码中潜在的内存泄漏问题。如果内存泄漏是由 Flink 引起的，那么请联系开发人员邮件列表。
分析内存和垃圾回收行为 # 内存使用和垃圾回收会对你的应用程序产生巨大的影响。如果 GC 停顿时间过长，其影响小到性能下降，大到集群完全瘫痪。如果你想更好地理解应用程序的内存和 GC 行为，可以在 TaskManagers 上启用内存日志记录。
taskmanager.debug.memory.log: true taskmanager.debug.memory.log-interval: 10000 // 10s interval 如果你想了解更详细的 GC 统计数据，可以通过以下方式激活 JVM 的 GC 日志记录：
env.java.opts: \u0026#34;-Xloggc:\${FLINK_LOG_PREFIX}.gc.log -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+PrintPromotionFailure -XX:+PrintGCCause\u0026#34; Back to top
`}),e.add({id:131,href:"/flink/flink-docs-master/zh/docs/dev/",title:"应用开发",section:"Docs",content:" "}),e.add({id:132,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/debezium/",title:"Debezium",section:"Formats",content:` Debezium Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Debezium 是一个 CDC（Changelog Data Capture，变更数据捕获）的工具，可以把来自 MySQL、PostgreSQL、Oracle、Microsoft SQL Server 和许多其他数据库的更改实时流式传输到 Kafka 中。 Debezium 为变更日志提供了统一的格式结构，并支持使用 JSON 和 Apache Avro 序列化消息。
Flink 支持将 Debezium JSON 和 Avro 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Debezium 格式的 JSON 或 Avro 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Debezium 消息。
依赖 # Debezium Avro # In order to use the Debezium format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-confluent-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Debezium Json # In order to use the Debezium format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in 注意: 请参考 Debezium 文档，了解如何设置 Debezium Kafka Connect 用来将变更日志同步到 Kafka 主题。
如何使用 Debezium Format # Debezium 为变更日志提供了统一的格式，这是一个 JSON 格式的从 MySQL product 表捕获的更新操作的简单示例:
{ \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;source\u0026#34;: {...}, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1589362330904, \u0026#34;transaction\u0026#34;: null } 注意: 请参考 Debezium 文档，了解每个字段的含义。
MySQL 产品表有4列（id、name、description、weight）。上面的 JSON 消息是 products 表上的一条更新事件，其中 id = 111 的行的 weight 值从 5.18 更改为 5.15。假设此消息已同步到 Kafka 主题 products_binlog，则可以使用以下 DDL 来使用此主题并解析更改事件。
SQL CREATE TABLE topic_products ( -- schema 与 MySQL 的 products 表完全相同 id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, -- 使用 \u0026#39;debezium-json\u0026#39; format 来解析 Debezium 的 JSON 消息 -- 如果 Debezium 用 Avro 编码消息，请使用 \u0026#39;debezium-avro-confluent\u0026#39; \u0026#39;format\u0026#39; = \u0026#39;debezium-json\u0026#39; -- 如果 Debezium 用 Avro 编码消息，请使用 \u0026#39;debezium-avro-confluent\u0026#39; ) 在某些情况下，用户在设置 Debezium Kafka Connect 时，可能会开启 Kafka 的配置 'value.converter.schemas.enable'，用来在消息体中包含 schema 信息。然后，Debezium JSON 消息可能如下所示:
{ \u0026#34;schema\u0026#34;: {...}, \u0026#34;payload\u0026#34;: { \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;source\u0026#34;: {...}, \u0026#34;op\u0026#34;: \u0026#34;u\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1589362330904, \u0026#34;transaction\u0026#34;: null } } 为了解析这一类信息，你需要在上述 DDL WITH 子句中添加选项 'debezium-json.schema-include' = 'true'（默认为 false）。通常情况下，建议不要包含 schema 的描述，因为这样会使消息变得非常冗长，并降低解析性能。
在将主题注册为 Flink 表之后，可以将 Debezium 消息用作变更日志源。
SQL -- MySQL \u0026#34;products\u0026#34; 的实时物化视图 -- 计算相同产品的最新平均重量 SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- 将 MySQL \u0026#34;products\u0026#34; 表的所有数据和增量更改同步到 -- Elasticsearch \u0026#34;products\u0026#34; 索引，供将来查找 INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Attention Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format.
Key Data Type Description schema STRING NULL JSON string describing the schema of the payload. Null if the schema is not included in the Debezium record. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts_ms field in the Debezium record. source.timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the source system created the event. Corresponds to the source.ts_ms field in the Debezium record. source.database STRING NULL The originating database. Corresponds to the source.db field in the Debezium record if available. source.schema STRING NULL The originating database schema. Corresponds to the source.schema field in the Debezium record if available. source.table STRING NULL The originating database table. Corresponds to the source.table or source.collection field in the Debezium record if available. source.properties MAP\u0026lt;STRING, STRING\u0026gt; NULL Map of various source properties. Corresponds to the source field in the Debezium record. The following example shows how to access Debezium metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, event_time TIMESTAMP(3) METADATA FROM \u0026#39;value.source.timestamp\u0026#39; VIRTUAL, origin_database STRING METADATA FROM \u0026#39;value.source.database\u0026#39; VIRTUAL, origin_schema STRING METADATA FROM \u0026#39;value.source.schema\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.source.table\u0026#39; VIRTUAL, origin_properties MAP\u0026lt;STRING, STRING\u0026gt; METADATA FROM \u0026#39;value.source.properties\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39; ); Format 参数 # Flink 提供了 debezium-avro-confluent 和 debezium-json 两种 format 来解析 Debezium 生成的 JSON 格式和 Avro 格式的消息。 请使用 debezium-avro-confluent 来解析 Debezium 的 Avro 消息，使用 debezium-json 来解析 Debezium 的 JSON 消息。
Debezium Avro # 参数 是否必选 默认值 类型 描述 format required (none) String Specify what format to use, here should be 'debezium-avro-confluent'. debezium-avro-confluent.basic-auth.credentials-source optional (none) String Basic auth credentials source for Schema Registry debezium-avro-confluent.basic-auth.user-info optional (none) String Basic auth user info for schema registry debezium-avro-confluent.bearer-auth.credentials-source optional (none) String Bearer auth credentials source for Schema Registry debezium-avro-confluent.bearer-auth.token optional (none) String Bearer auth token for Schema Registry debezium-avro-confluent.properties optional (none) Map Properties map that is forwarded to the underlying Schema Registry. This is useful for options that are not officially exposed via Flink config options. However, note that Flink options have higher precedence. debezium-avro-confluent.ssl.keystore.location optional (none) String Location / File of SSL keystore debezium-avro-confluent.ssl.keystore.password optional (none) String Password for SSL keystore debezium-avro-confluent.ssl.truststore.location optional (none) String Location / File of SSL truststore debezium-avro-confluent.ssl.truststore.password optional (none) String Password for SSL truststore debezium-avro-confluent.subject optional (none) String The Confluent Schema Registry subject under which to register the schema used by this format during serialization. By default, 'kafka' and 'upsert-kafka' connectors use '\u0026lt;topic_name\u0026gt;-value' or '\u0026lt;topic_name\u0026gt;-key' as the default subject name if this format is used as the value or key format. But for other connectors (e.g. 'filesystem'), the subject option is required when used as sink. debezium-avro-confluent.url required (none) String The URL of the Confluent Schema Registry to fetch/register schemas. Debezium Json # 参数 是否必选 默认值 类型 描述 format 必选 (none) String 指定要使用的格式，此处应为 'debezium-json'。 debezium-json.schema-include 可选 false Boolean 设置 Debezium Kafka Connect 时，用户可以启用 Kafka 配置 'value.converter.schemas.enable' 以在消息中包含 schema。此选项表明 Debezium JSON 消息是否包含 schema。 debezium-json.ignore-parse-errors 可选 false Boolean 当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null。 debezium-json.timestamp-format.standard 可选 'SQL' String 声明输入和输出的时间戳格式。当前支持的格式为'SQL' 以及 'ISO-8601'： 可选参数 'SQL' 将会以 "yyyy-MM-dd HH:mm:ss.s{precision}" 的格式解析时间戳, 例如 '2020-12-30 12:13:14.123'，且会以相同的格式输出。 可选参数 'ISO-8601' 将会以 "yyyy-MM-ddTHH:mm:ss.s{precision}" 的格式解析输入时间戳, 例如 '2020-12-30T12:13:14.123' ，且会以相同的格式输出。 debezium-json.map-null-key.mode 选填 'FAIL' String 指定处理 Map 中 key 值为空的方法. 当前支持的值有 'FAIL', 'DROP' 和 'LITERAL': Option 'FAIL' 将抛出异常，如果遇到 Map 中 key 值为空的数据。 Option 'DROP' 将丢弃 Map 中 key 值为空的数据项。 Option 'LITERAL' 将使用字符串常量来替换 Map 中的空 key 值。字符串常量的值由 'debezium-json.map-null-key.literal' 定义。 debezium-json.map-null-key.literal 选填 'null' String 当 'debezium-json.map-null-key.mode' 是 LITERAL 的时候，指定字符串常量替换 Map 中的空 key 值。 debezium-json.encode.decimal-as-plain-number 选填 false Boolean 将所有 DECIMAL 类型的数据保持原状，不使用科学计数法表示。例：0.000000027 默认会表示为 2.7E-8。当此选项设为 true 时，则会表示为 0.000000027。 注意事项 # 重复的变更事件 # 在正常的操作环境下，Debezium 应用能以 exactly-once 的语义投递每条变更事件。在这种情况下，Flink 消费 Debezium 产生的变更事件能够工作得很好。 然而，当有故障发生时，Debezium 应用只能保证 at-least-once 的投递语义。可以查看 Debezium 官方文档 了解更多关于 Debezium 的消息投递语义。 这也意味着，在非正常情况下，Debezium 可能会投递重复的变更事件到 Kafka 中，当 Flink 从 Kafka 中消费的时候就会得到重复的事件。 这可能会导致 Flink query 的运行得到错误的结果或者非预期的异常。因此，建议在这种情况下，将作业参数 table.exec.source.cdc-events-duplicate 设置成 true，并在该 source 上定义 PRIMARY KEY。 框架会生成一个额外的有状态算子，使用该 primary key 来对变更事件去重并生成一个规范化的 changelog 流。
消费 Debezium Postgres Connector 产生的数据 # 如果你正在使用 Debezium PostgreSQL Connector 捕获变更到 Kafka，请确保被监控表的 REPLICA IDENTITY 已经被配置成 FULL 了，默认值是 DEFAULT。 否则，Flink SQL 将无法正确解析 Debezium 数据。
当配置为 FULL 时，更新和删除事件将完整包含所有列的之前的值。当为其他配置时，更新和删除事件的 \u0026ldquo;before\u0026rdquo; 字段将只包含 primary key 字段的值，或者为 null（没有 primary key）。 你可以通过运行 ALTER TABLE \u0026lt;your-table-name\u0026gt; REPLICA IDENTITY FULL 来更改 REPLICA IDENTITY 的配置。 请阅读 Debezium 关于 PostgreSQL REPLICA IDENTITY 的文档 了解更多。
数据类型映射 # 目前，Debezium Format 使用 JSON Format 进行序列化和反序列化。有关数据类型映射的更多详细信息，请参考 JSON Format 文档 和 Confluent Avro Format 文档。
`}),e.add({id:133,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/drop/",title:"DROP 语句",section:"SQL",content:` DROP 语句 # DROP 语句可用于删除指定的 catalog，也可用于从当前或指定的 Catalog 中删除一个已经注册的表、视图或函数。
Flink SQL 目前支持以下 DROP 语句：
DROP CATALOG DROP TABLE DROP DATABASE DROP VIEW DROP FUNCTION 执行 DROP 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句。
Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 DROP 语句。 若 DROP 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 DROP 语句。 若 DROP 操作执行成功，execute_sql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 DROP 语句
SQL CLI 可以在 SQL CLI 中执行 DROP 语句。
以下的例子展示了如何在 SQL CLI 中执行一个 DROP 语句。
Java TableEnvironment tableEnv = TableEnvironment.create(...); // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // 字符串数组： [\u0026#34;Orders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // 从 catalog 删除 “Orders” 表 tableEnv.executeSql(\u0026#34;DROP TABLE Orders\u0026#34;); // 空字符串数组 String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // 字符串数组： [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // 从 catalog 删除 “Orders” 表 tableEnv.executeSql(\u0026#34;DROP TABLE Orders\u0026#34;) // 空字符串数组 val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() Python table_env = TableEnvironment.create(...) # 字符串数组： [\u0026#34;Orders\u0026#34;] tables = table_env.listTables() # or table_env.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() # 从 catalog 删除 “Orders” 表 table_env.execute_sql(\u0026#34;DROP TABLE Orders\u0026#34;) # 空字符串数组 tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; Orders Flink SQL\u0026gt; DROP TABLE Orders; [INFO] Table has been removed. Flink SQL\u0026gt; SHOW TABLES; [INFO] Result was empty. DROP CATALOG # DROP CATALOG [IF EXISTS] catalog_name 删除给定名字的 catalog。
IF EXISTS
如果目标 catalog 不存在，则不会执行任何操作。
DROP TABLE # DROP TABLE [IF EXISTS] [catalog_name.][db_name.]table_name 根据给定的表名删除某个表。若需要删除的表不存在，则抛出异常。
IF EXISTS
表不存在时不会进行任何操作。
DROP DATABASE # DROP DATABASE [IF EXISTS] [catalog_name.]db_name [ (RESTRICT | CASCADE) ] 根据给定的表名删除数据库。若需要删除的数据库不存在会抛出异常 。
IF EXISTS
若数据库不存在，不执行任何操作。
RESTRICT
当删除一个非空数据库时，会触发异常。（默认为开）
CASCADE
删除一个非空数据库时，把相关联的表与函数一并删除。
DROP VIEW # DROP [TEMPORARY] VIEW [IF EXISTS] [catalog_name.][db_name.]view_name 删除一个有 catalog 和数据库命名空间的视图。若需要删除的视图不存在，则会产生异常。
TEMPORARY
删除一个有 catalog 和数据库命名空间的临时视图。
IF EXISTS
若视图不存在，则不会进行任何操作。
依赖管理 Flink 没有使用 CASCADE / RESTRICT 关键字来维护视图的依赖关系，当前的方案是在用户使用视图时再提示错误信息，比如在视图的底层表已经被删除等场景。
DROP FUNCTION # DROP [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF EXISTS] [catalog_name.][db_name.]function_name; 删除一个有 catalog 和数据库命名空间的 catalog function。若需要删除的函数不存在，则会产生异常。
TEMPORARY
删除一个有 catalog 和数据库命名空间的临时 catalog function。
TEMPORARY SYSTEM
删除一个没有数据库命名空间的临时系统函数。
IF EXISTS
若函数不存在，则不会进行任何操作。
`}),e.add({id:134,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/elasticsearch/",title:"Elasticsearch",section:"DataStream Connectors",content:` Elasticsearch 连接器 # 此连接器提供可以向 Elasticsearch 索引请求文档操作的 sinks。 要使用此连接器，请根据 Elasticsearch 的安装版本将以下依赖之一添加到你的项目中：
Elasticsearch 版本 Maven 依赖 6.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch6\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 7.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch7\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 Elasticsearch connector ，需要添加下列依赖： Elasticsearch version PyFlink JAR 6.x Only available for stable releases. 7.x and later versions Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 请注意，流连接器目前不是二进制发行版的一部分。 有关如何将程序和用于集群执行的库一起打包，参考此文档。
安装 Elasticsearch # Elasticsearch 集群的设置可以参考此文档。
Elasticsearch Sink # 下面的示例展示了如何配置并创建一个 sink：
Java Elasticsearch 6:
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.connector.elasticsearch.sink.Elasticsearch6SinkBuilder; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.HashMap; import java.util.Map; DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch6SinkBuilder\u0026lt;String\u0026gt;() // 下面的设置使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 .setBulkFlushMaxActions(1) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) .build()); private static IndexRequest createIndexRequest(String element) { Map\u0026lt;String, Object\u0026gt; json = new HashMap\u0026lt;\u0026gt;(); json.put(\u0026#34;data\u0026#34;, element); return Requests.indexRequest() .index(\u0026#34;my-index\u0026#34;) .type(\u0026#34;my-type\u0026#34;) .id(element) .source(json); } Elasticsearch 7:
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkBuilder; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.HashMap; import java.util.Map; DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch7SinkBuilder\u0026lt;String\u0026gt;() // 下面的设置使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 .setBulkFlushMaxActions(1) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) .build()); private static IndexRequest createIndexRequest(String element) { Map\u0026lt;String, Object\u0026gt; json = new HashMap\u0026lt;\u0026gt;(); json.put(\u0026#34;data\u0026#34;, element); return Requests.indexRequest() .index(\u0026#34;my-index\u0026#34;) .id(element) .source(json); } Scala Elasticsearch 6:
import org.apache.flink.api.connector.sink.SinkWriter import org.apache.flink.connector.elasticsearch.sink.{Elasticsearch6SinkBuilder, RequestIndexer} import org.apache.flink.streaming.api.datastream.DataStream import org.apache.http.HttpHost import org.elasticsearch.action.index.IndexRequest import org.elasticsearch.client.Requests val input: DataStream[String] = ... input.sinkTo( new Elasticsearch6SinkBuilder[String] // 下面的设置使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 .setBulkFlushMaxActions(1) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) .build()) def createIndexRequest(element: (String)): IndexRequest = { val json = Map( \u0026#34;data\u0026#34; -\u0026gt; element.asInstanceOf[AnyRef] ) Requests.indexRequest.index(\u0026#34;my-index\u0026#34;).source(mapAsJavaMap(json)) } Elasticsearch 7:
import org.apache.flink.api.connector.sink.SinkWriter import org.apache.flink.connector.elasticsearch.sink.{Elasticsearch7SinkBuilder, RequestIndexer} import org.apache.flink.streaming.api.datastream.DataStream import org.apache.http.HttpHost import org.elasticsearch.action.index.IndexRequest import org.elasticsearch.client.Requests val input: DataStream[String] = ... input.sinkTo( new Elasticsearch7SinkBuilder[String] // 下面的设置使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 .setBulkFlushMaxActions(1) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) .build()) def createIndexRequest(element: (String)): IndexRequest = { val json = Map( \u0026#34;data\u0026#34; -\u0026gt; element.asInstanceOf[AnyRef] ) Requests.indexRequest.index(\u0026#34;my-index\u0026#34;).\`type\`(\u0026#34;my-type\u0026#34;).source(mapAsJavaMap(json)) } Python Elasticsearch 6 静态索引:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch6SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... # 下面的 set_bulk_flush_max_actions 使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 es6_sink = Elasticsearch6SinkBuilder() \\ .set_bulk_flush_max_actions(1) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es6_sink).name(\u0026#39;es6 sink\u0026#39;) Elasticsearch 6 动态索引:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch6SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... es_sink = Elasticsearch6SinkBuilder() \\ .set_emitter(ElasticsearchEmitter.dynamic_index(\u0026#39;name\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es6_sink).name(\u0026#39;es6 dynamic index sink\u0026#39;) Elasticsearch 7 静态索引:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... # 下面的 set_bulk_flush_max_actions 使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来 es7_sink = Elasticsearch7SinkBuilder() \\ .set_bulk_flush_max_actions(1) \\ .set_emitter(ElasticsearchEmitter.static(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 sink\u0026#39;) Elasticsearch 7 动态索引:
from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter env = StreamExecutionEnvironment.get_execution_environment() env.add_jars(ELASTICSEARCH_SQL_CONNECTOR_PATH) input = ... es7_sink = Elasticsearch7SinkBuilder() \\ .set_emitter(ElasticsearchEmitter.dynamic_index(\u0026#39;name\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 dynamic index sink\u0026#39;) 需要注意的是，该示例仅演示了对每个传入的元素执行单个索引请求。 通常，ElasticsearchSinkFunction 可用于执行多个不同类型的请求（例如 DeleteRequest、 UpdateRequest 等）。
在内部，Flink Elasticsearch Sink 的每个并行实例使用一个 BulkProcessor 向集群发送操作请求。 这会在元素批量发送到集群之前进行缓存。 BulkProcessor 一次执行一个批量请求，即不会存在两个并行刷新缓存的操作。
Elasticsearch Sinks 和容错 # 通过启用 Flink checkpoint，Flink Elasticsearch Sink 保证至少一次将操作请求发送到 Elasticsearch 集群。 这是通过在进行 checkpoint 时等待 BulkProcessor 中所有挂起的操作请求来实现。 这有效地保证了在触发 checkpoint 之前所有的请求被 Elasticsearch 成功确认，然后继续处理发送到 sink 的记录。
关于 checkpoint 和容错的更多详细信息，请参见容错文档。
要使用具有容错特性的 Elasticsearch Sinks，需要在执行环境中启用作业拓扑的 checkpoint：
Java Elasticsearch 6:
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); // 每 5000 毫秒执行一次 checkpoint Elasticsearch6SinkBuilder sinkBuilder = new Elasticsearch6SinkBuilder\u0026lt;String\u0026gt;() .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))); Elasticsearch 7:
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); // 每 5000 毫秒执行一次 checkpoint Elasticsearch7SinkBuilder sinkBuilder = new Elasticsearch7SinkBuilder\u0026lt;String\u0026gt;() .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))); Scala Elasticsearch 6:
val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableCheckpointing(5000) // 每 5000 毫秒执行一次 checkpoint val sinkBuilder = new Elasticsearch6SinkBuilder[String] .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) Elasticsearch 7:
val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableCheckpointing(5000) // 每 5000 毫秒执行一次 checkpoint val sinkBuilder = new Elasticsearch7SinkBuilder[String] .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) Python Elasticsearch 6:
env = StreamExecutionEnvironment.get_execution_environment() # 每 5000 毫秒执行一次 checkpoint env.enable_checkpointing(5000) sink_builder = Elasticsearch6SinkBuilder() \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) Elasticsearch 7:
env = StreamExecutionEnvironment.get_execution_environment() # 每 5000 毫秒执行一次 checkpoint env.enable_checkpointing(5000) sink_builder = Elasticsearch7SinkBuilder() \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) Using UpdateRequests with deterministic ids and the upsert method it is possible to achieve exactly-once semantics in Elasticsearch when AT_LEAST_ONCE delivery is configured for the connector. 处理失败的 Elasticsearch 请求 # Elasticsearch 操作请求可能由于多种原因而失败，包括节点队列容量暂时已满或者要被索引的文档格式错误。 Flink Elasticsearch Sink 允许用户通过通过指定一个退避策略来重试请求。
下面是一个例子：
Java Elasticsearch 6:
DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch6SinkBuilder\u0026lt;String\u0026gt;() .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) // 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()); Elasticsearch 7:
DataStream\u0026lt;String\u0026gt; input = ...; input.sinkTo( new Elasticsearch7SinkBuilder\u0026lt;String\u0026gt;() .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter( (element, context, indexer) -\u0026gt; indexer.add(createIndexRequest(element))) // 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()); Scala Elasticsearch 6:
val input: DataStream[String] = ... input.sinkTo( new Elasticsearch6SinkBuilder[String] .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) // 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()) Elasticsearch 7:
val input: DataStream[String] = ... input.sinkTo( new Elasticsearch7SinkBuilder[String] .setHosts(new HttpHost(\u0026#34;127.0.0.1\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setEmitter((element: String, context: SinkWriter.Context, indexer: RequestIndexer) =\u0026gt; indexer.add(createIndexRequest(element))) // 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 .setBulkFlushBackoffStrategy(FlushBackoffType.EXPONENTIAL, 5, 1000) .build()) Python Elasticsearch 6:
input = ... # 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 es_sink = Elasticsearch6SinkBuilder() \\ .set_bulk_flush_backoff_strategy(FlushBackoffType.CONSTANT, 5, 1000) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;bar\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es_sink).name(\u0026#39;es6 sink\u0026#39;) Elasticsearch 7:
input = ... # 这里启用了一个指数退避重试策略，初始延迟为 1000 毫秒且最大重试次数为 5 es7_sink = Elasticsearch7SinkBuilder() \\ .set_bulk_flush_backoff_strategy(FlushBackoffType.EXPONENTIAL, 5, 1000) \\ .set_emitter(ElasticsearchEmitter.static_index(\u0026#39;foo\u0026#39;, \u0026#39;id\u0026#39;)) \\ .set_hosts([\u0026#39;localhost:9200\u0026#39;]) \\ .build() input.sink_to(es7_sink).name(\u0026#39;es7 sink\u0026#39;) 上面的示例 sink 重新添加由于资源受限（例如：队列容量已满）而失败的请求。对于其它类型的故障，例如文档格式错误，sink 将会失败。 如若未设置 BulkFlushBackoffStrategy (或者 FlushBackoffType.NONE)，那么任何类型的错误都会导致 sink 失败。
重要提示：在失败时将请求重新添加回内部 BulkProcessor 会导致更长的 checkpoint，因为在进行 checkpoint 时，sink 还需要等待重新添加的请求被刷新。 例如，当使用 FlushBackoffType.EXPONENTIAL 时， checkpoint 会进行等待，直到 Elasticsearch 节点队列有足够的容量来处理所有挂起的请求，或者达到最大重试次数。 配置内部批量处理器 # 通过使用以下在 Elasticsearch6SinkBuilder 中提供的方法，可以进一步配置内部的 BulkProcessor 关于其如何刷新缓存操作请求的行为：
setBulkFlushMaxActions(int numMaxActions)：刷新前最大缓存的操作数。 setBulkFlushMaxSizeMb(int maxSizeMb)：刷新前最大缓存的数据量（以兆字节为单位）。 setBulkFlushInterval(long intervalMillis)：刷新的时间间隔（不论缓存操作的数量或大小如何）。 还支持配置如何对暂时性请求错误进行重试：
setBulkFlushBackoffStrategy(FlushBackoffType flushBackoffType, int maxRetries, long delayMillis)：退避延迟的类型，CONSTANT 或者 EXPONENTIAL，退避重试次数，退避重试的时间间隔。 对于常量延迟来说，此值是每次重试间的间隔。对于指数延迟来说，此值是延迟的初始值。 可以在此文档找到 Elasticsearch 的更多信息。
将 Elasticsearch 连接器打包到 Uber-Jar 中 # 建议构建一个包含所有依赖的 uber-jar (可执行的 jar)，以便更好地执行你的 Flink 程序。 (更多信息参见此文档)。
或者，你可以将连接器的 jar 文件放入 Flink 的 lib/ 目录下，使其在全局范围内可用，即可用于所有的作业。
Back to top
`}),e.add({id:135,href:"/flink/flink-docs-master/zh/docs/deployment/finegrained_resource/",title:"Fine-Grained Resource Management",section:"Deployment",content:` Fine-Grained Resource Management # Apache Flink works hard to auto-derive sensible default resource requirements for all applications out of the box. For users who wish to fine-tune their resource consumption, based on knowledge of their specific scenarios, Flink offers fine-grained resource management.
This page describes the fine-grained resource management’s usage, applicable scenarios, and how it works.
Note: This feature is currently an MVP (“minimum viable product”) feature and only available to DataStream API. Applicable Scenarios # Typical scenarios that potentially benefit from fine-grained resource management are where:
Tasks have significantly different parallelisms.
The resource needed for an entire pipeline is too much to fit into a single slot/task manager.
Batch jobs where resources needed for tasks of different stages are significantly different
An in-depth discussion on why fine-grained resource management can improve resource efficiency for the above scenarios is presented in How it improves resource efficiency.
How it works # As described in Flink Architecture, task execution resources in a TaskManager are split into many slots. The slot is the basic unit of both resource scheduling and resource requirement in Flink\u0026rsquo;s runtime.
With fine-grained resource management, the slots requests contain specific resource profiles, which users can specify. Flink will respect those user-specified resource requirements and dynamically cut an exactly-matched slot out of the TaskManager’s available resources. As shown above, there is a requirement for a slot with 0.25 Core and 1GB memory, and Flink allocates Slot 1 for it.
Previously in Flink, the resource requirement only contained the required slots, without fine-grained resource profiles, namely coarse-grained resource management. The TaskManager had a fixed number of identical slots to fulfill those requirements. For the resource requirement without a specified resource profile, Flink will automatically decide a resource profile. Currently, the resource profile of it is calculated from TaskManager’s total resource and taskmanager.numberOfTaskSlots, just like in coarse-grained resource management. As shown above, the total resource of TaskManager is 1 Core and 4 GB memory and the number of task slots is set to 2, Slot 2 is created with 0.5 Core and 2 GB memory for the requirement without a specified resource profile.
After the allocation of Slot 1 and Slot 2, there is 0.25 Core and 1 GB memory remaining as the free resources in the TaskManager. These free resources can be further partitioned to fulfill the following resource requirements.
Please refer to Resource Allocation Strategy for more details.
Usage # To use fine-grained resource management, you need to:
Configure to enable fine-grained resource management.
Specify the resource requirement.
Enable Fine-Grained Resource Management # To enable fine-grained resource management, you need to configure the cluster.fine-grained-resource-management.enabled to true.
Without this configuration, the Flink runtime cannot schedule the slots with your specified resource requirement and the job will fail with an exception. Specify Resource Requirement for Slot Sharing Group # Fine-grained resource requirements are defined on slot sharing groups. A slot sharing group is a hint that tells the JobManager operators/tasks in it CAN be put into the same slot.
For specifying the resource requirement, you need to:
Define the slot sharing group and the operators it contains.
Specify the resource of the slot sharing group.
There are two approaches to define the slot sharing group and the operators it contains:
You can define a slot sharing group only by its name and attach it to an operator through the slotSharingGroup(String name).
You can construct a SlotSharingGroup instance, which contains the name and an optional resource profile of the slot sharing group. The SlotSharingGroup can be attached to an operator through slotSharingGroup(SlotSharingGroup ssg).
You can specify the resource profile for your slot sharing groups:
If you set the slot sharing group through slotSharingGroup(SlotSharingGroup ssg), you can specify the resource profile in constructing the SlotSharingGroup instance.
If you only set the name of slot sharing group with slotSharingGroup(String name). You can construct a SlotSharingGroup instance with the same name along with the resource profile and register the resource of them with StreamExecutionEnvironment#registerSlotSharingGroup(SlotSharingGroup ssg).
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SlotSharingGroup ssgA = SlotSharingGroup.newBuilder(\u0026#34;a\u0026#34;) .setCpuCores(1.0) .setTaskHeapMemoryMB(100) .build(); SlotSharingGroup ssgB = SlotSharingGroup.newBuilder(\u0026#34;b\u0026#34;) .setCpuCores(0.5) .setTaskHeapMemoryMB(100) .build(); someStream.filter(...).slotSharingGroup(\u0026#34;a\u0026#34;) // Set the slot sharing group with name “a” .map(...).slotSharingGroup(ssgB); // Directly set the slot sharing group with name and resource. env.registerSlotSharingGroup(ssgA); // Then register the resource of group “a” Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val ssgA = SlotSharingGroup.newBuilder(\u0026#34;a\u0026#34;) .setCpuCores(1.0) .setTaskHeapMemoryMB(100) .build() val ssgB = SlotSharingGroup.newBuilder(\u0026#34;b\u0026#34;) .setCpuCores(0.5) .setTaskHeapMemoryMB(100) .build() someStream.filter(...).slotSharingGroup(\u0026#34;a\u0026#34;) // Set the slot sharing group with name “a” .map(...).slotSharingGroup(ssgB) // Directly set the slot sharing group with name and resource. env.registerSlotSharingGroup(ssgA) // Then register the resource of group “a” Python env = StreamExecutionEnvironment.get_execution_environment() ssg_a = SlotSharingGroup.builder(\u0026#39;a\u0026#39;) \\ .set_cpu_cores(1.0) \\ .set_task_heap_memory_mb(100) \\ .build() ssg_b = SlotSharingGroup.builder(\u0026#39;b\u0026#39;) \\ .set_cpu_cores(0.5) \\ .set_task_heap_memory_mb(100) \\ .build() some_stream.filter(...).slot_sharing_group(\u0026#39;a\u0026#39;) # Set the slot sharing group with name \u0026#34;a\u0026#34; .map(...).slot_sharing_group(ssg_b) # Directly set the slot sharing group with name and resource. env.register_slot_sharing_group(ssg_a) # Then register the resource of group \u0026#34;a\u0026#34; Note: Each slot sharing group can only attach to one specified resource, any conflict will fail the compiling of your job. In constructing the SlotSharingGroup, you can set the following resource components for the slot sharing group:
CPU Cores. Defines how many CPU cores are needed. Required to be explicitly configured with positive value. Task Heap Memory. Defines how much task heap memory is needed. Required to be explicitly configured with positive value. Task Off-Heap Memory. Defines how much task off-heap memory is needed, can be 0. Managed Memory. Defines how much task managed memory is needed, can be 0. External Resources. Defines the external resources needed, can be empty. Java // Directly build a slot sharing group with specific resource SlotSharingGroup ssgWithResource = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;) .setCpuCores(1.0) // required .setTaskHeapMemoryMB(100) // required .setTaskOffHeapMemoryMB(50) .setManagedMemory(MemorySize.ofMebiBytes(200)) .setExternalResource(\u0026#34;gpu\u0026#34;, 1.0) .build(); // Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment SlotSharingGroup ssgWithName = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;).build(); env.registerSlotSharingGroup(ssgWithResource); Scala // Directly build a slot sharing group with specific resource val ssgWithResource = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;) .setCpuCores(1.0) // required .setTaskHeapMemoryMB(100) // required .setTaskOffHeapMemoryMB(50) .setManagedMemory(MemorySize.ofMebiBytes(200)) .setExternalResource(\u0026#34;gpu\u0026#34;, 1.0) .build() // Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment val ssgWithName = SlotSharingGroup.newBuilder(\u0026#34;ssg\u0026#34;).build() env.registerSlotSharingGroup(ssgWithResource) Python # Directly build a slot sharing group with specific resource ssg_with_resource = SlotSharingGroup.builder(\u0026#39;ssg\u0026#39;) \\ .set_cpu_cores(1.0) \\ .set_task_heap_memory_mb(100) \\ .set_task_off_heap_memory_mb(50) \\ .set_managed_memory(MemorySize.of_mebi_bytes(200)) \\ .set_external_resource(\u0026#39;gpu\u0026#39;, 1.0) \\ .build() # Build a slot sharing group without specific resource and then register the resource of it in StreamExecutionEnvironment ssg_with_name = SlotSharingGroup.builder(\u0026#39;ssg\u0026#39;).build() env.register_slot_sharing_group(ssg_with_resource) Note: You can construct a SlotSharingGroup with or without specifying its resource profile. With specifying the resource profile, you need to explicitly set the CPU cores and Task Heap Memory with a positive value, other components are optional. Limitations # Since fine-grained resource management is a new, experimental feature, not all features supported by the default scheduler are also available with it. The Flink community is working on addressing these limitations.
No support for the Elastic Scaling. The elastic scaling only supports slot requests without specified-resource at the moment.
No support for task manager redundancy. The slotmanager.redundant-taskmanager-num is used to start redundant TaskManagers to speed up job recovery. This config option will not take effect in fine-grained resource management at the moment.
No support for evenly spread out slot strategy. This strategy tries to spread out the slots evenly across all available TaskManagers. The strategy is not supported in the first version of fine-grained resource management and cluster.evenly-spread-out-slots will not take effect in it at the moment.
Limited integration with Flink’s Web UI. Slots in fine-grained resource management can have different resource specs. The web UI only shows the slot number without its details at the moment.
Limited integration with batch jobs. At the moment, fine-grained resource management requires batch workloads to be executed with types of all edges being BLOCKING. To do that, you need to configure fine-grained.shuffle-mode.all-blocking to true. Notice that this may affect the performance. See FLINK-20865 for more details.
Hybrid resource requirements are not recommended. It is not recommended to specify the resource requirements only for some parts of the job and leave the requirements for the rest unspecified. Currently, the unspecified requirement can be fulfilled with slots of any resource. The actual resource acquired by it can be inconsistent across different job executions or failover.
Slot allocation result might not be optimal. As the slot requirements contain multiple dimensions of resources, the slot allocation is indeed a multi-dimensional packing problem, which is NP-hard. The default resource allocation strategy might not achieve optimal slot allocation and can lead to resource fragments or resource allocation failure in some scenarios.
Notice # Setting the slot sharing group may change the performance. Setting chain-able operators to different slot sharing groups may break operator chains, and thus change the performance.
Slot sharing group will not restrict the scheduling of operators. The slot sharing group only hints the scheduler that the grouped operators CAN be deployed into a shared slot. There\u0026rsquo;s no guarantee that the scheduler always deploys the grouped operator together. In cases grouped operators are deployed into separate slots, the slot resources will be derived from the specified group requirement.
Deep Dive # How it improves resource efficiency # In this section, we deep dive into how fine-grained resource management improves resource efficiency, which can help you to understand whether it can benefit your jobs.
Previously, Flink adopted a coarse-grained resource management approach, where tasks are deployed into predefined, usually identical slots without the notion of how many resources each slot contains. For many jobs, using coarse-grained resource management and simply putting all tasks into one slot sharing group works well enough in terms of resource utilization.
For many streaming jobs that all tasks have the same parallelism, each slot will contain an entire pipeline. Ideally, all pipelines should use roughly the same resources, which can be satisfied easily by tuning the resources of the identical slots.
Resource consumption of tasks varies over time. When consumption of a task decreases, the extra resources can be used by another task whose consumption is increasing. This, known as the peak shaving and valley filling effect, reduces the overall resource needed.
However, there are cases where coarse-grained resource management does not work well.
Tasks may have different parallelisms. Sometimes, such different parallelisms cannot be avoided. E.g., the parallelism of source/sink/lookup tasks might be constrained by the partitions and IO load of the external upstream/downstream system. In such cases, slots with fewer tasks would need fewer resources than those with the entire pipeline of tasks.
Sometimes the resource needed for the entire pipeline might be too much to be put into a single slot/TaskManager. In such cases, the pipeline needs to be split into multiple SSGs, which may not always have the same resource requirement.
For batch jobs, not all the tasks can be executed at the same time. Thus, the instantaneous resource requirement of the pipeline changes over time.
Trying to execute all tasks with identical slots can result in non-optimal resource utilization. The resource of the identical slots has to be able to fulfill the highest resource requirement, which will be wasteful for other requirements. When expensive external resources like GPU are involved, such waste can become even harder to afford. The fine-grained resource management leverages slots of different resources to improve resource utilization in such scenarios.
Resource Allocation Strategy # In this section, we talk about the slot partitioning mechanism in Flink runtime and the resource allocation strategy, including how the Flink runtime selects a TaskManager to cut slots and allocates TaskManagers on Native Kubernetes and YARN. Note that the resource allocation strategy is pluggable in Flink runtime and here we introduce its default implementation in the first step of fine-grained resource management. In the future, there might be various strategies that users can select for different scenarios.
As described in How it works section, Flink will cut an exactly matched slot out of the TaskManager for slot requests with specified resources. The internal process is shown above. The TaskManager will be launched with total resources but no predefined slots. When a slot request with 0.25 Core and 1GB memory arrives, Flink will select a TaskManager with enough free resources and create a new slot with the requested resources. If a slot is freed, it will return its resources to the available resources of the TaskManager.
In the current resource allocation strategy, Flink will traverse all the registered TaskManagers and select the first one who has enough free resources to fulfill the slot request. When there is no TaskManager that has enough free resources, Flink will try to allocate a new TaskManager when deploying on Native Kubernetes or YARN. In the current strategy, Flink will allocate identical TaskManagers according to user’s configuration. As the resource spec of TaskManagers is pre-defined:
There might be resource fragments in the cluster. E.g. if there are two slot requests with 3 GB heap memory while the total heap memory of TaskManager is 4 GB, Flink will start two TaskManagers and there will be 1 GB heap memory wasted in each TaskManager. In the future, there might be a resource allocation strategy that can allocate heterogeneous TaskManagers according to the slot requests of the job and thus mitigate the resource fragment.
You need to make sure the resource components configured for the slot sharing group are no larger than the total resources of the TaskManager. Otherwise, your job will fail with an exception.
Back to top
`}),e.add({id:136,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/firehose/",title:"Firehose",section:"DataStream Connectors",content:` Amazon Kinesis Data Firehose Sink # The Firehose sink writes to Amazon Kinesis Data Firehose.
Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to setup a Kinesis Data Firehose delivery stream.
To use the connector, add the following Maven dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-aws-kinesis-firehose\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 AWS Kinesis Firehose connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 The KinesisFirehoseSink uses AWS v2 SDK for Java to write data from a Flink stream into a Firehose delivery stream.
Java Properties sinkProperties = new Properties(); // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;eu-west-1\u0026#34;); // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); KinesisFirehoseSink\u0026lt;String\u0026gt; kdfSink = KinesisFirehoseSink.\u0026lt;String\u0026gt;builder() .setFirehoseClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setDeliveryStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(4 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1000 * 1024) // Optional .build(); flinkStream.sinkTo(kdfSink); Scala Properties sinkProperties = new Properties() // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;eu-west-1\u0026#34;) // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) val kdfSink = KinesisFirehoseSink.\u0026lt;String\u0026gt;builder() .setFirehoseClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setDeliveryStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(4 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1000 * 1024) // Optional .build() flinkStream.sinkTo(kdfSink) Python sink_properties = { # Required \u0026#39;aws.region\u0026#39;: \u0026#39;eu-west-1\u0026#39;, # Optional, provide via alternative routes e.g. environment variables \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39; } kdf_sink = KinesisFirehoseSink.builder() \\ .set_firehose_client_properties(sink_properties) \\ # Required .set_serialization_schema(SimpleStringSchema()) \\ # Required .set_delivery_stream_name(\u0026#39;your-stream-name\u0026#39;) \\ # Required .set_fail_on_error(False) \\ # Optional .set_max_batch_size(500) \\ # Optional .set_max_in_flight_requests(50) \\ # Optional .set_max_buffered_requests(10000) \\ # Optional .set_max_batch_size_in_bytes(5 * 1024 * 1024) \\ # Optional .set_max_time_in_buffer_ms(5000) \\ # Optional .set_max_record_size_in_bytes(1 * 1024 * 1024) \\ # Optional .build() Configurations # Flink\u0026rsquo;s Firehose sink is created by using the static builder KinesisFirehoseSink.\u0026lt;InputType\u0026gt;builder().
setFirehoseClientProperties(Properties sinkProperties) Required. Supplies credentials, region and other parameters to the Firehose client. setSerializationSchema(SerializationSchema serializationSchema) Required. Supplies a serialization schema to the Sink. This schema is used to serialize elements before sending to Firehose. setDeliveryStreamName(String deliveryStreamName) Required. Name of the delivery stream to sink to. setFailOnError(boolean failOnError) Optional. Default: false. Whether failed requests to write records to Firehose are treated as fatal exceptions in the sink. setMaxBatchSize(int maxBatchSize) Optional. Default: 500. Maximum size of a batch to write to Firehose. setMaxInFlightRequests(int maxInFlightRequests) Optional. Default: 50. The maximum number of in flight requests allowed before the sink applies backpressure. setMaxBufferedRequests(int maxBufferedRequests) Optional. Default: 10_000. The maximum number of records that may be buffered in the sink before backpressure is applied. setMaxBatchSizeInBytes(int maxBatchSizeInBytes) Optional. Default: 4 * 1024 * 1024. The maximum size (in bytes) a batch may become. All batches sent will be smaller than or equal to this size. setMaxTimeInBufferMS(int maxTimeInBufferMS) Optional. Default: 5000. The maximum time a record may stay in the sink before being flushed. setMaxRecordSizeInBytes(int maxRecordSizeInBytes) Optional. Default: 1000 * 1024. The maximum record size that the sink will accept, records larger than this will be automatically rejected. build() Constructs and returns the Firehose sink. Using Custom Firehose Endpoints # It is sometimes desirable to have Flink operate as a consumer or producer against a Firehose VPC endpoint or a non-AWS Firehose endpoint such as Localstack; this is especially useful when performing functional testing of a Flink application. The AWS endpoint that would normally be inferred by the AWS region set in the Flink configuration must be overridden via a configuration property.
To override the AWS endpoint, set the AWSConfigConstants.AWS_ENDPOINT and AWSConfigConstants.AWS_REGION properties. The region will be used to sign the endpoint URL.
Java Properties producerConfig = new Properties(); producerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); producerConfig.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4566\u0026#34;); Scala val producerConfig = new Properties() producerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) producerConfig.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4566\u0026#34;) Python producer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4566\u0026#39; } Back to top
`}),e.add({id:137,href:"/flink/flink-docs-master/zh/docs/connectors/table/firehose/",title:"Firehose",section:"Table API Connectors",content:` Amazon Kinesis Data Firehose SQL Connector # Sink: Streaming Append Mode The Kinesis Data Firehose connector allows for writing data into Amazon Kinesis Data Firehose (KDF).
Dependencies # In order to use the AWS Kinesis Firehose connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis-firehose\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. How to create a Kinesis Data Firehose table # Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to set up a Kinesis Data Firehose delivery stream. The following example shows how to create a table backed by a Kinesis Data Firehose delivery stream with minimum required options:
CREATE TABLE FirehoseTable ( \`user_id\` BIGINT, \`item_id\` BIGINT, \`category_id\` BIGINT, \`behavior\` STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;firehose\u0026#39;, \u0026#39;delivery-stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Connector Options # Option Required Default Type Description Common Options connector required (none) String Specify what connector to use. For Kinesis Data Firehose use 'firehose'. delivery-stream required (none) String Name of the Kinesis Data Firehose delivery stream backing this table. format required (none) String The format used to deserialize and serialize Kinesis Data Firehose records. See Data Type Mapping for details. aws.region required (none) String The AWS region where the delivery stream is defined. This option is required for KinesisFirehoseSink creation. aws.endpoint optional (none) String The AWS endpoint for Amazon Kinesis Data Firehose. aws.trust.all.certificates optional false Boolean If true accepts all SSL certificates. Authentication Options aws.credentials.provider optional AUTO String A credentials provider to use when authenticating against the Kinesis endpoint. See Authentication for details. aws.credentials.basic.accesskeyid optional (none) String The AWS access key ID to use when setting credentials provider type to BASIC. aws.credentials.basic.secretkey optional (none) String The AWS secret key to use when setting credentials provider type to BASIC. aws.credentials.profile.path optional (none) String Optional configuration for profile path if credential provider type is set to be PROFILE. aws.credentials.profile.name optional (none) String Optional configuration for profile name if credential provider type is set to be PROFILE. aws.credentials.role.arn optional (none) String The role ARN to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.sessionName optional (none) String The role session name to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.externalId optional (none) String The external ID to use when credential provider type is set to ASSUME_ROLE. aws.credentials.role.provider optional (none) String The credentials provider that provides credentials for assuming the role when credential provider type is set to ASSUME_ROLE. Roles can be nested, so this value can again be set to ASSUME_ROLE aws.credentials.webIdentityToken.file optional (none) String The absolute path to the web identity token file that should be used if provider type is set to WEB_IDENTITY_TOKEN. Sink Options sink.http-client.max-concurrency optional 10000 Integer Maximum number of allowed concurrent requests by FirehoseAsyncClient to be delivered to delivery stream. sink.http-client.read-timeout optional 360000 Integer Maximum amount of time in ms for requests to be sent by FirehoseAsyncClient to delivery stream before failure. sink.http-client.protocol.version optional HTTP2 String Http version used by FirehoseAsyncClient. sink.batch.max-size optional 500 Integer Maximum batch size of elements to be passed to FirehoseAsyncClient to be written downstream to delivery stream. sink.requests.max-inflight optional 16 Integer Request threshold for uncompleted requests by FirehoseAsyncClientbefore blocking new write requests. sink.requests.max-buffered optional 10000 String request buffer threshold by FirehoseAsyncClient before blocking new write requests. sink.flush-buffer.size optional 5242880 Long Threshold value in bytes for writer buffer in FirehoseAsyncClient before flushing. sink.flush-buffer.timeout optional 5000 Long Threshold time in ms for an element to be in a buffer of FirehoseAsyncClient before flushing. sink.fail-on-error optional false Boolean Flag used for retrying failed requests. If set any request failure will not be retried and will fail the job. Authorization # Make sure to create an appropriate IAM policy to allow reading writing to the Kinesis Data Firehose delivery stream.
Authentication # Depending on your deployment you would choose a different Credentials Provider to allow access to Kinesis Data Firehose. By default, the AUTO Credentials Provider is used. If the access key ID and secret key are set in the deployment configuration, this results in using the BASIC provider.
A specific AWSCredentialsProvider can be optionally set using the aws.credentials.provider setting. Supported values are:
AUTO - Use the default AWS Credentials Provider chain that searches for credentials in the following order: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE, and EC2/ECS credentials provider. BASIC - Use access key ID and secret key supplied as configuration. ENV_VAR - Use AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY environment variables. SYS_PROP - Use Java system properties aws.accessKeyId and aws.secretKey. PROFILE - Use an AWS credentials profile to create the AWS credentials. ASSUME_ROLE - Create AWS credentials by assuming a role. The credentials for assuming the role must be supplied. WEB_IDENTITY_TOKEN - Create AWS credentials by assuming a role using Web Identity Token. Data Type Mapping # Kinesis Data Firehose stores records as Base64-encoded binary data objects, so it doesn\u0026rsquo;t have a notion of internal record structure. Instead, Kinesis Data Firehose records are deserialized and serialized by formats, e.g. \u0026lsquo;avro\u0026rsquo;, \u0026lsquo;csv\u0026rsquo;, or \u0026lsquo;json\u0026rsquo;. To determine the data type of the messages in your Kinesis Data Firehose backed tables, pick a suitable Flink format with the format keyword. Please refer to the Formats pages for more details.
Notice # The current implementation for the Kinesis Data Firehose SQL connector only supports Kinesis Data Firehose backed sinks and doesn\u0026rsquo;t provide an implementation for source queries. Queries similar to:
SELECT * FROM FirehoseTable; should result in an error similar to
Connector firehose can only be used as a sink. It cannot be used as a source. Back to top
`}),e.add({id:138,href:"/flink/flink-docs-master/zh/docs/libs/gelly/graph_algorithms/",title:"Graph Algorithms",section:"Graphs",content:` Graph Algorithms # The logic blocks with which the Graph API and top-level algorithms are assembled are accessible in Gelly as graph algorithms in the org.apache.flink.graph.asm package. These algorithms provide optimization and tuning through configuration parameters and may provide implicit runtime reuse when processing the same input with a similar configuration.
VertexInDegree # Annoate vertices of a directed graph with the in-degree.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; inDegree = graph .run(new VertexInDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with an in-degree of zero.
setParallelism: override the operator parallelism
VertexOutDegree # Annotate vertices of a directed graph with the out-degree.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; outDegree = graph .run(new VertexOutDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with an out-degree of zero.
setParallelism: override the operator parallelism
VertexDegrees # Annotate vertices of a directed graph with the degree, out-degree, and in-degree.
DataSet\u0026lt;Vertex\u0026lt;K, Tuple2\u0026lt;LongValue, LongValue\u0026gt;\u0026gt;\u0026gt; degrees = graph .run(new VertexDegrees().setIncludeZeroDegreeVertices(true)); Optional configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with out- and in-degree of zero.
setParallelism: override the operator parallelism.
EdgeSourceDegrees # Annotate edges of a directed graph with the degree, out-degree, and in-degree of the source ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, Degrees\u0026gt;\u0026gt;\u0026gt; sourceDegrees = graph .run(new EdgeSourceDegrees()); Optional configuration:
setParallelism: override the operator parallelism. EdgeTargetDegrees # Annotate edges of a directed graph with the degree, out-degree, and in-degree of the target ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, Degrees\u0026gt;\u0026gt;\u0026gt; targetDegrees = graph .run(new EdgeTargetDegrees()); Optional configuration:
setParallelism: override the operator parallelism. EdgeDegreesPair # Annotate edges of a directed graph with the degree, out-degree, and in-degree of both the source and target vertices.
DataSet\u0026lt;Vertex\u0026lt;K, LongValue\u0026gt;\u0026gt; degree = graph .run(new VertexDegree() .setIncludeZeroDegreeVertices(true) .setReduceOnTargetId(true)); Optional Configuration:
setIncludeZeroDegreeVertices: by default only the edge set is processed for the computation of degree; when this flag is set an additional join is performed against the vertex set in order to output vertices with a degree of zero
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
EdgeSourceDegree # Annotate edges of an undirected graph with degree of the source ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, LongValue\u0026gt;\u0026gt;\u0026gt; sourceDegree = graph .run(new EdgeSourceDegree() .setReduceOnTargetId(true)); Optional Configuration:
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
EdgeTargetDegree # Annotate edges of an undirected graph with degree of the target ID.
DataSet\u0026lt;Edge\u0026lt;K, Tuple2\u0026lt;EV, LongValue\u0026gt;\u0026gt;\u0026gt; targetDegree = graph .run(new EdgeTargetDegree() .setReduceOnSourceId(true)); Optional configuration:
setParallelism: override the operator parallelism
setReduceOnSourceId: the degree can be counted from either the edge source or target IDs. By default the target IDs are counted. Reducing on source IDs may optimize the algorithm if the input edge list is sorted by source ID.
EdgeDegreePair # Annotate edges of an undirected graph with the degree of both the source and target vertices.
DataSet\u0026lt;Edge\u0026lt;K, Tuple3\u0026lt;EV, LongValue, LongValue\u0026gt;\u0026gt;\u0026gt; pairDegree = graph .run(new EdgeDegreePair().setReduceOnTargetId(true)); Optional configuration:
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
MaximumDegree # Filter an undirected graph by maximum degree.
Graph\u0026lt;K, VV, EV\u0026gt; filteredGraph = graph .run(new MaximumDegree(5000) .setBroadcastHighDegreeVertices(true) .setReduceOnTargetId(true)); Optional configuration:
setBroadcastHighDegreeVertices: join high-degree vertices using a broadcast-hash to reduce data shuffling when removing a relatively small number of high-degree vertices.
setParallelism: override the operator parallelism
setReduceOnTargetId: the degree can be counted from either the edge source or target IDs. By default the source IDs are counted. Reducing on target IDs may optimize the algorithm if the input edge list is sorted by target ID.
Simplify # Remove self-loops and duplicate edges from a directed graph.
graph.run(new Simplify()); TranslateGraphIds # Translate vertex and edge IDs using the given TranslateFunction.
graph.run(new TranslateGraphIds(new LongValueToStringValue())); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism TranslateVertexValues # Translate vertex values using the given TranslateFunction.
graph.run(new TranslateVertexValues(new LongValueAddOffset(vertexCount))); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism TranslateEdgeValues # Translate edge values using the given TranslateFunction.
graph.run(new TranslateEdgeValues(new Nullify())); Required configuration:
translator: implements type or value conversion Optional configuration:
setParallelism: override the operator parallelism Back to top
`}),e.add({id:139,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/hive_functions/",title:"Hive Functions",section:"Hive",content:` Hive Functions # Use Hive Built-in Functions via HiveModule # The HiveModule provides Hive built-in functions as Flink system (built-in) functions to Flink SQL and Table API users.
For detailed information, please refer to HiveModule.
Java String name = \u0026#34;myhive\u0026#34;; String version = \u0026#34;2.3.4\u0026#34;; tableEnv.loadModue(name, new HiveModule(version)); Scala val name = \u0026#34;myhive\u0026#34; val version = \u0026#34;2.3.4\u0026#34; tableEnv.loadModue(name, new HiveModule(version)); Python from pyflink.table.module import HiveModule name = \u0026#34;myhive\u0026#34; version = \u0026#34;2.3.4\u0026#34; t_env.load_module(name, HiveModule(version)) YAML modules: - name: core type: core - name: myhive type: hive Some Hive built-in functions in older versions have thread safety issues. We recommend users patch their own Hive to fix them. Hive User Defined Functions # Users can use their existing Hive User Defined Functions in Flink.
Supported UDF types include:
UDF GenericUDF GenericUDTF UDAF GenericUDAFResolver2 Upon query planning and execution, Hive\u0026rsquo;s UDF and GenericUDF are automatically translated into Flink\u0026rsquo;s ScalarFunction, Hive\u0026rsquo;s GenericUDTF is automatically translated into Flink\u0026rsquo;s TableFunction, and Hive\u0026rsquo;s UDAF and GenericUDAFResolver2 are translated into Flink\u0026rsquo;s AggregateFunction.
To use a Hive User Defined Function, user have to
set a HiveCatalog backed by Hive Metastore that contains that function as current catalog of the session include a jar that contains that function in Flink\u0026rsquo;s classpath Using Hive User Defined Functions # Assuming we have the following Hive functions registered in Hive Metastore:
/** * Test simple udf. Registered under name \u0026#39;myudf\u0026#39; */ public class TestHiveSimpleUDF extends UDF { public IntWritable evaluate(IntWritable i) { return new IntWritable(i.get()); } public Text evaluate(Text text) { return new Text(text.toString()); } } /** * Test generic udf. Registered under name \u0026#39;mygenericudf\u0026#39; */ public class TestHiveGenericUDF extends GenericUDF { @Override public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { checkArgument(arguments.length == 2); checkArgument(arguments[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) arguments[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); if (arguments[0] instanceof IntObjectInspector || arguments[0] instanceof StringObjectInspector) { return arguments[0]; } else { throw new RuntimeException(\u0026#34;Not support argument: \u0026#34; + arguments[0]); } } @Override public Object evaluate(DeferredObject[] arguments) throws HiveException { return arguments[0].get(); } @Override public String getDisplayString(String[] children) { return \u0026#34;TestHiveGenericUDF\u0026#34;; } } /** * Test split udtf. Registered under name \u0026#39;mygenericudtf\u0026#39; */ public class TestHiveUDTF extends GenericUDTF { @Override public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException { checkArgument(argOIs.length == 2); // TEST for constant arguments checkArgument(argOIs[1] instanceof ConstantObjectInspector); Object constant = ((ConstantObjectInspector) argOIs[1]).getWritableConstantValue(); checkArgument(constant instanceof IntWritable); checkArgument(((IntWritable) constant).get() == 1); return ObjectInspectorFactory.getStandardStructObjectInspector( Collections.singletonList(\u0026#34;col1\u0026#34;), Collections.singletonList(PrimitiveObjectInspectorFactory.javaStringObjectInspector)); } @Override public void process(Object[] args) throws HiveException { String str = (String) args[0]; for (String s : str.split(\u0026#34;,\u0026#34;)) { forward(s); forward(s); } } @Override public void close() { } } From Hive CLI, we can see they are registered:
hive\u0026gt; show functions; OK ...... mygenericudf myudf myudtf Then, users can use them in SQL as:
Flink SQL\u0026gt; select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b, s from mysourcetable, lateral table(myudtf(name, 1)) as T(s); `}),e.add({id:140,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/kinesis/",title:"Kinesis",section:"DataStream Connectors",content:` 亚马逊 Kinesis 数据流 SQL 连接器 # Kinesis 连接器提供访问 Amazon Kinesis Data Streams 。
使用此连接器, 取决于您是否读取数据和/或写入数据，增加下面依赖项的一个或多个到您的项目中:
KDS Connectivity Maven Dependency Source \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Sink \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-aws-kinesis-streams\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 由于许可证问题，以前的版本中 flink-connector-kinesis 工件没有部署到Maven中心库。有关更多信息，请参阅特定版本的文档。
为了在 PyFlink 作业中使用 Kinesis connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 使用亚马逊 Kinesis 流服务 # 遵循 Amazon Kinesis Streams Developer Guide 的指令建立 Kinesis 流。
配置用 IAM 访问 Kinesis # 确保创建合适的 IAM 策略允许从 Kinesis 中读取和写入数据。查阅例子 这里 。
取决于您的部署，您可以选择一个不同的证书提供商来允许访问 Kinesis 。 缺省使用 AUTO 的证书供应商。 如果访问密钥ID和秘密密钥在部署配置中设置，结果使用 BASIC 提供商。
可以通过使用 AWSConfigConstants.AWS_CREDENTIALS_PROVIDER 选择性地设置一个特定的证书提供商。
支持的证书提供者是:
AUTO - 使用缺省的 AWS Credentials Provider chain 按如下顺序搜索证书: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE 和 EC2/ECS 证书提供者。 BASIC - 使用访问密钥ID和秘密密钥作为配置。 ENV_VAR - 使用 AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY 环境变量。 SYS_PROP - 使用 Java 系统属性 aws.accessKeyId and aws.secretKey。 PROFILE - 使用一个 AWS 证书档案创建 AWS 证书。 ASSUME_ROLE - 通过承担角色创建 AWS 证书。 需要提供承担角色的证书。 WEB_IDENTITY_TOKEN - 通过使用网络身份令牌承担的角色创建 AWS 证书。 Kinesis 消费者 # FlinkKinesisConsumer 是在相同的 AWS 服务区域订阅了多个 AWS Kinesis 流的一个精确一次消费的并行流数据源，可以在作业运行时透明地处理流的重新分片。 消费者的每个子任务负责从多个Kinesis分片中取数据记录。每个子任务取的分片数量在 Kinesis 关闭和创建分片时发生改变。
从 Kinesis 流中消费数据前，确保所有的流在亚马逊 Kinesis 数据流控制台使用状态 \u0026ldquo;ACTIVE\u0026rdquo; 创建。
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); consumerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) consumerConfig.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) val env = StreamExecutionEnvironment.getExecutionEnvironment val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer(\u0026#34;stream-1\u0026#34;, SimpleStringSchema(), consumer_config)) 上面是一个使用消费者的简单例子。 消费者使用 java.util.Properties 实例配置，配置的健可以在 AWSConfigConstants (AWS 特定的参数) 和 ConsumerConfigConstants (Kinesis 消费者参数)。这个例子演示了在 AWS区域 \u0026ldquo;us-east-1\u0026rdquo; 消费一个 Kinesis流。AWS 证书使用基本的方法提供：AWS访问密钥 ID和秘密访问密钥直接在配置中提供。数据从 Kinesis流中最新的位置消费（另一个选项是把 ConsumerConfigConstants.STREAM_INITIAL_POSITION 设置为 TRIM_HORIZON ，让消费者从 Kinesis 流中可能的最早记录开始读取）。
别的消费者的可选配置项可以在 ConsumerConfigConstants 找到。
请注意，Flink Kinesis 消费源的配置并行度可以完全独立于 Kinesis 流的总的分片数。 当分片数大于消费者的并行度时，每个消费者子任务可以订阅多个分片；否则 如果分片的数量小于消费者的并行度，那么一些消费者子任务将处于空闲状态，并等待它被分配 新的分片（即当流被重新分片以增加用于更高配置的 Kinesis 服务吞吐量的分片数）。
也请注意，默认基于分片和流名称的哈希分配分片给子任务， 这将或多或少地平衡子任务之间的分片。 然而，假设在流上使用默认的 Kinesis 分片管理（UpdateShardCount 用 UNIFORM_SCALING）， 将 UniformShardAssigner 设置为消费者的分片分配器将更均匀地将分片分配到子任务。 假设传入的 Kinesis 记录被分配了随机的 Kinesis PartitionKey 或 ExplicitHashKey 值，结果是一致的子任务加载。 如果默认分配器和 UniformShardAssigner 都不够，则可以设置自定义实现的 KinesisShardAssigner 。
DeserializationSchema # Flink Kinesis消费者还需要一个模式来了解如何将 Kinesis 数据流中的二进制数据转换为 Java 对象。 KinesisDeserializationSchema 允许用户指定这样的模式。T deserialize(byte[] recordValue, String partitionKey, String seqNum, long approxArrivalTimestamp, String stream, String shardId) 方法让每个 Kinesis 记录被调用。
为了方便, Flink 提供下面开箱即用的模式:
TypeInformationSerializationSchema 基于 Flink 的 TypeInformation 创建一个模式。 如果数据被Flink读和写，是有用的。 这是替代其它通用序列化方法的性能良好的 Flink特定的模式。
GlueSchemaRegistryJsonDeserializationSchema 提供查找写模式的能力（模式用于写记录）。 在 AWS Glue Schema Registry. 使用此模式, 反序列化模式记录将被 从 AWS Glue Schema Registry 中检索到的模式读取并将其转换为用手动提供的模式描述通用化的记录 com.amazonaws.services.schemaregistry.serializers.json.JsonDataWithSchema 或者 mbknor-jackson-jsonSchema 生成的一个 JAVA POJO。
要使用此反序列化模式，必须添加以下附加依赖项:
GlueSchemaRegistryJsonDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-jsonschema-glue-schema-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! AvroDeserializationSchema 它使用静态提供的模式读取以 Avro 格式序列化的数据。它可以 推断从 Avro 生成的类的模式（ AvroDeserializationSchema.forSpecific(...) ) 或者它可以与 GenericRecords 一起使用 一个手动提供的模式（使用 AvroDeserializationSchema.forGeneric(...)）。这个反序列化的模式期望 序列化模式记录没有包含嵌入的模式。
您可以使用 AWS Glue Schema Registry 检索写模式。类似地，反序列化记录将用来自 AWS Glue Schema Registry的模式读取并转换 （通过 GlueSchemaRegistryAvroDeserializationSchema.forGeneric(...) 或 GlueSchemaRegistryAvroDeserializationSchema.forSpecific(...) ）。 更多 AWS Glue Schema Registry 和 Apache Flink 集成的信息参阅 Use Case: Amazon Kinesis Data Analytics for Apache Flink. 要使用此反序列化模式，必须添加以下附加依赖项:
AvroDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! GlueSchemaRegistryAvroDeserializationSchema \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro-glue-schema-registry\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 配置开始位置 # Flink Kinesis Consumer 当前提供如下的选项来配置读取 Kinesis 流的开始位置，只需在提供的配置属性中设置 ConsumerConfigConstants.STREAM_INITIAL_POSITION 为以下值之一（选项的命名完全遵循 the namings used by the AWS Kinesis Streams service ）:
您可以配置源表通过 scan.stream.initpos 选项从一个特定的位置开始读取一个 Kinesis 数据流支持的表。
LATEST: 读取从最新的记录开始的分片。 TRIM_HORIZON: 读取从最早可能的记录开始的分片（取决于保留的设置，数据可能被 Kinesis 修剪）。 AT_TIMESTAMP: 读取从一个特定的时间戳开始的分片。时间戳也必须在配置中指定属性，为 ConsumerConfigConstants.STREAM_INITIAL_TIMESTAMP 提供值，采用以下日期模式之一： 一个非负双精度值表示从 Unix 纪元开始的秒的数量 （例如，1459799926.480）。 一个用户定义的模式，ConsumerConfigConstants.STREAM_TIMESTAMP_DATE_FORMAT 提供的 SimpleDateFormat 有效模式， 如果 ConsumerConfigConstants.STREAM_TIMESTAMP_DATE_FORMAT 未被定义，那么默认的模式是 yyyy-MM-dd'T'HH:mm:ss.SSSXXX。 例如，时间戳的值是 2016-04-04 而用户定义的模式是 yyyy-MM-dd 或者时间戳的值是 2016-04-04T19:58:46.480-00:00 而用户未定义模式。 精确一次的用户定义状态更新语义的容错性 # 启用 Flink 的检查点后，Flink Kinesis Consuemer 将消费 Kinesis 流中分片的记录，并且 定期检查每个分片的进度。如果作业失败， Flink 会将流程序恢复到 最新完成的检查点的状态，并从存储在检查点中的程序开始重新消费 Kinesis 分片中的记录。
产生检查点的间隔定义了程序在出现故障时最多需要返回的量。
要使用容错的 Kinesis Consumers，需要在执行环境中启用拓扑检查点：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(5000); // checkpoint every 5000 msecs Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableCheckpointing(5000) // checkpoint every 5000 msecs Python env = StreamExecutionEnvironment.get_execution_environment() env.enable_checkpointing(5000) # checkpoint every 5000 msecs 还请注意，只有当有足够的处理槽可用于重新启动拓扑时， Flink 才能重新启动拓扑。 因此，如果拓扑由于 TaskManager 的丢失而失败，那么之后必须仍然有足够的可用插槽。 Flink on YARN 支持自动重启丢失的 YARN 容器。
使用增强型扇出 # Enhanced Fan-Out (EFO) 增加了每个 Kinesis 数据流的并行消费者的最大数量。 没有 EFO , 所有的并行 Kinesis 消费者共享一个单一的按分片分配的读取配额。 有了 EFO , 每个消费者获取一个不同的专用的按分片分配的读取配额，允许读吞吐量按消费者的数量放大。 使用 EFO 将 incur additional cost 。
为了启动 EFO ，两个附加的配置参数是需要的:
RECORD_PUBLISHER_TYPE: 确定是使用 EFO 还是 POLLING. 默认 RecordPublisher 是 POLLING 。 EFO_CONSUMER_NAME: 用于识别消费者的名字。 对于给定的 Kinesis 数据流，每个消费者必须具有唯一的名称。 然而，消费者名称在数据流中不必是唯一的。 重用消费名称将导致现有订阅终止。 下面的代码片段显示了配置 EFO 消费者的简单示例。
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) EFO Stream Consumer 注册/注销 # 为了使用EFO，必须针对您希望使用的每个流注册流消费者。 默认情况下，FlinkKinesisConsumer 将在 Flink 作业启动时自动注册流消费者。 流消费者将使用 EFO_CONSUMER_NAME 配置提供的名称注册。 FlinkKinesisConsumer 提供了三种注册策略：
注册 LAZY (默认): Flink作业开始运行时，将注册流消费者。 如果流消费者已经存在，则将重用它。 这是大多数应用程序的首选策略。 然而，并行度大于1的作业将导致任务竞争注册和获取流消费者 ARN。 对于并行度非常大的作业，这可能会导致启动时间增加。 描述操作限制为20 transactions per second, 这意味着应用程序启动时间将增加大约 parallelism/20 seconds。 EAGER: 流消费者在 FlinkKinesisConsumer 构造函数中注册。 如果流消费者已经存在，则将重用它。 这将导致在构建作业时发生注册， 在 Flink JobManager 或客户端环境中提交作业。 使用此策略会导致单线程注册和检索流消费者 ARN， 通过 LAZY（具有大并行度）减少启动时间。 然而，请考虑客户端环境将需要访问 AWS 服务。 NONE: 流消费者注册不是由 FlinkKinesisConsumer 执行的。 必须使用 [AWS CLI或SDK] 在外部执行注册(https://aws.amazon.com/tools/) 。 调用 RegisterStreamConsumer 。 应通过消费者配置向作业提供流消费者 ARN。 注销 LAZY|EAGER (默认):当作业正常关闭时，将注销流消费者。 如果作业在执行关闭钩子程序时终止，流消费者将保持活动状态。 在这种情况下，当应用程序重新启动时，流消费者将被优雅地重用。 NONE: 流消费者注销不是由 FlinkKinesisConsumer 执行的。 下面是一个使用 EAGER 注册策略的配置例子：
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.EAGER.name()); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.EAGER.name()); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39;, \u0026#39;flink.stream.efo.registration\u0026#39;: \u0026#39;EAGER\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) 下面是一个使用 NONE 注册策略的配置例子：
Java Properties consumerConfig = new Properties(); consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;); consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.NONE.name()); consumerConfig.put(ConsumerConfigConstants.efoConsumerArn(\u0026#34;stream-name\u0026#34;), \u0026#34;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#34;); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; kinesis = env.addSource(new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), consumerConfig)); Scala val consumerConfig = new Properties() consumerConfig.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) consumerConfig.put(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \u0026#34;LATEST\u0026#34;) consumerConfig.put(ConsumerConfigConstants.RECORD_PUBLISHER_TYPE, ConsumerConfigConstants.RecordPublisherType.EFO.name()); consumerConfig.put(ConsumerConfigConstants.EFO_CONSUMER_NAME, \u0026#34;my-flink-efo-consumer\u0026#34;); consumerConfig.put(ConsumerConfigConstants.EFO_REGISTRATION_TYPE, ConsumerConfigConstants.EFORegistrationType.NONE.name()); consumerConfig.put(ConsumerConfigConstants.efoConsumerArn(\u0026#34;stream-name\u0026#34;), \u0026#34;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#34;); val env = StreamExecutionEnvironment.getExecutionEnvironment() val kinesis = env.addSource(new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema, consumerConfig)) Python consumer_config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;flink.stream.initpos\u0026#39;: \u0026#39;LATEST\u0026#39;, \u0026#39;flink.stream.recordpublisher\u0026#39;: \u0026#39;EFO\u0026#39;, \u0026#39;flink.stream.efo.consumername\u0026#39;: \u0026#39;my-flink-efo-consumer\u0026#39;, \u0026#39;flink.stream.efo.consumerarn.stream-name\u0026#39;: \u0026#39;arn:aws:kinesis:\u0026lt;region\u0026gt;:\u0026lt;account\u0026gt;\u0026gt;:stream/\u0026lt;stream-name\u0026gt;/consumer/\u0026lt;consumer-name\u0026gt;:\u0026lt;create-timestamp\u0026gt;\u0026#39; } env = StreamExecutionEnvironment.get_execution_environment() kinesis = env.add_source(FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config)) 消费记录的事件时间 # 如果流拓扑选择使用 event time notion 进行记录 时间戳，默认情况下将使用 approximate arrival timestamp。一旦记录被流成功接收和存储，这个时间戳就会被 Kinesis 附加到记录上。 请注意，此时间戳通常称为 Kinesis 服务器端时间戳，并且不能保证准确性或顺序正确性（即时间戳可能并不总是升序）。
用户可以选择使用自定义时间戳覆盖此默认值如描述 here, 或者使用 predefined ones 中的一个文件。之后，它可以通过以下方式传递给消费者：
Java FlinkKinesisConsumer\u0026lt;String\u0026gt; consumer = new FlinkKinesisConsumer\u0026lt;\u0026gt;( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), kinesisConsumerConfig); consumer.setPeriodicWatermarkAssigner(new CustomAssignerWithPeriodicWatermarks()); DataStream\u0026lt;String\u0026gt; stream = env .addSource(consumer) .print(); Scala val consumer = new FlinkKinesisConsumer[String]( \u0026#34;kinesis_stream_name\u0026#34;, new SimpleStringSchema(), kinesisConsumerConfig); consumer.setPeriodicWatermarkAssigner(new CustomAssignerWithPeriodicWatermarks()); val stream = env .addSource(consumer) .print(); Python consumer = FlinkKinesisConsumer( \u0026#34;kinesis_stream_name\u0026#34;, SimpleStringSchema(), consumer_config) stream = env.add_source(consumer).print() 在内部，每个分片/消费者线程执行一个分配器器实例（请参阅下面的线程模型）。 当指定分配器时，对于从 Kinesis 读取的每个记录， extractTimestamp(T element, long previousElementTimestamp) 被调用来为记录分配时间戳，并调用 getCurrentWatermark 来确定分片的新水印。 然后，消费者子任务的水印被确定为其所有分片的最小水印，并定期发出。 每个分片的水印对于处理分片之间的不同消费速度至关重要，否则可能导致 解决依赖水印的下游逻辑问题，例如错误的延迟数据丢弃。
默认情况下，如果分片不提供新记录，水印将暂停。 ConsumerConfigConstants.SHARD_IDLE_INTERVAL_MILLIS 属性通过超时允许水印在存在空闲分片的情况下继续。
分片消费者的事件时间对齐 # Flink Kinesis Consumer 可选地支持并行消费者子任务（及其线程）之间的同步 避免事件时间偏离，相关的问题描述在 Event time synchronization across sources 。
要启用同步，请在消费者上设置水印跟踪器：
Java JobManagerWatermarkTracker watermarkTracker = new JobManagerWatermarkTracker(\u0026#34;myKinesisSource\u0026#34;); consumer.setWatermarkTracker(watermarkTracker); Python watermark_tracker = WatermarkTracker.job_manager_watermark_tracker(\u0026#34;myKinesisSource\u0026#34;) consumer.set_watermark_tracker(watermark_tracker) JobManagerWatermarkTracker 使用全局聚合来同步每个子任务的水印。每个子任务 使用每个分片队列来控制记录的传输速率，该速率基于水印队列中的下一条记录在全局队列的领先程度。
\u0026ldquo;提前发送\u0026rdquo; 限制通过 ConsumerConfigConstants.WATERMARK_LOOKAHEAD_MILLIS 配置。较小的值减少 倾斜和吞吐量。较大的值允许子任务在等待全局水印推进前继续运行。
吞吐量方程的另一个可变因素是跟踪器传播水印的频率。可以通过 ConsumerConfigConstants.WATERMARK_SYNC_MILLIS 配置间隔。较小的值会减少发送等待，并以增加与作业管理器的通信为代价。
由于发生倾斜时记录会在队列中累积，因此需要增加内存消耗。内存消耗增加的多少取决于平均记录大小。对于较大的尺寸， 可能需要通过 ConsumerConfigConstants.WATERMARK_SYNC_QUEUE_CAPACITY 调整发射队列容量。
线程模型 # Flink Kinesis Consumer 使用多个线程进行分片发现和数据消费。
分片发现 # 对于分片发现，每个并行消费者子任务都将有一个线程，该线程不断查询 Kinesis 获取分片信息 即使在消费者启动时，子任务最初没有可读取的分片。换句话说，如果消费者以10的并行度运行， 不管订阅流的分片总的数量多少，总共会有10个线程不断查询 Kinesis。
轮询（默认）记录发布者 # 对于 POLLING 数据消费，将创建一个线程来消费每个发现的分片。线程将在它负责消费的分片 由于流重分片而关闭。换句话说，每个开放的分片总会有一个线程消费。
增强的扇出记录发布者 # 对于 EFO 数据消费，线程模型与 POLLING 相同，需要额外的线程池处理和 Kinesis 的异步通信。 AWS SDK v2.x KinesisAsyncClient 为 Netty 使用附加的线程池处理 IO 和异步响应。每个并行消费 子任务都有自己的 KinesisAsyncClient 实例。换句话说，如果消费者以 10 的并行度运行，那么总共 将有 10 个KinesisAsyncClient 实例。 在注册和注销流消费者时，一个单独的客户端将会被创建和随后销毁。
内部使用的 Kinesis APIs # Flink Kinesis Consumer 内部使用 AWS Java SDK 调用 Kinesis APIs 用于分片和数据消费。由于亚马逊在 API 的 service limits for Kinesis Streams 消费者将与用户正在运行的其他非 Flink 消费应用程序竞争。 下面是消费者调用的 API 列表，其中描述了消费者如何使用 API 以及信息关于如何处理 Flink Kinesis Consumer 因这些服务限制而可能出现的任何错误或警告。
Shard Discovery # ListShards: 通常为每个并 行消费子任务中的单个线程调用来发现由于流重分片而产生的任何新分片。默认情况下，消费者每隔10秒执行分片发现，并 将无限期重试，直到得到来自 Kinesis 的结果。如果这会干扰其他非 Flink 消费应用程序，则用户可以在提供的配置属性 设置 ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS 的值调用此 API 减慢消费。这把发现的间隔设置为 了不同的值。请注意，此设置直接影响了发现一个新的分片开启消费的最大延迟，因为在间隔期间不会发现分片。 轮询（默认）记录发布者 # GetShardIterator: 这 仅在每个分片消费线程启动时调用一次，如果 Kinesis 抱怨 API 已超过，默认情况下最多尝试3次。注意，由于此API的速率 是按每个分片（而不是每个流）限制，消费者本身不应超过限制。通常，如果发生这种情况，用户可以尝试减慢任何其它调用 此 API 的非 Flink 消费应用程序的速度，或通过在提供的配置属性设置以 ConsumerConfigConstants.SHARD_GETITERATOR_* 为前缀的键以修改消费者对此API调用的重试行为。
GetRecords: 通常为每个分片线 程调用从 Kinesis 中获取记录。当一个分片有多个并发消费者时（当如果任何其他非Flink消费应用程序正在运行），则可能会 超过每个分片的速率限制。默认情况下，每次 API 调用时，如果 Kinesis 投诉 API 的数据大小/事务限制已超过限制，消费者 将重试，默认最多重试3次。用户可以尝试减慢其它非 Flink 消费应用程序的速度，也可以通过在提供的配置属性设置 ConsumerConfigConstants.SHARD_GETRECORDS_MAX 和 ConsumerConfigConstants.SHARD_GETRECORDS_INTERVAL_MILLIS 的 键来调整消费者的吞吐量。设置前者调整每个消费线程在每次调用时尝试从分片中获取的最大记录数（默认值为10000），同时 设置后者修改每次获取之间的睡眠间隔的睡眠间隔时间（默认值为200）。调用此 API 的消费者的重试行为也可以通过使用 ConsumerConfigConstants.SHARD_GETRECORDS_* 为前缀的其它键进行修改。
增强的扇出记录发布者 # SubscribeToShard: 通常为 每个分片消费线程调用来获得分片订阅。分片订阅通常在5分钟内处于活动状态，但如果抛出任何可恢复的错误，则需要重新订阅。一 旦获得订阅，消费将收到流 SubscribeToShardEventss 。 重试和补偿参数可以使用 ConsumerConfigConstants.SUBSCRIBE_TO_SHARD_* 键配置。
DescribeStreamSummary: 通常为每个流调用在流消费者注册时调用一次。默认情况下，LAZY 注册策略按作业并行度放大调用数。EAGER 注册策略按每个流 调用一次。NONE 注册策略不调用。重试和补偿参数可以使用 ConsumerConfigConstants.STREAM_DESCRIBE_* 键进行配置。
DescribeStreamConsumer: 流消费者注册和注销时调用。对于每个流，将定期调用此服务直到流消费者在注册/注销收到报告 ACTIVE/not found。默认情况下， LAZY 注册策略按作业并行度放大调用数。EAGER 注册策略按每个流只调用一次此服务用于注册。NONE 注册策略不调用次服务。 重试和补偿参数可以使用 ConsumerConfigConstants.DESCRIBE_STREAM_CONSUMER_* 键进行配置。
RegisterStreamConsumer: 每个流消费者注册时调用一次，除非配置了 NONE 注册策略。重试和补偿参数可以使用 ConsumerConfigConstants.REGISTER_STREAM_* 键进行配置。
DeregisterStreamConsumer: 每个流消费者注销时调用一次，除非配置了 NONE 或者 EAGER 注册策略。重试和补偿参数可以使用 ConsumerConfigConstants.DEREGISTER_STREAM_* 键进行配置。
Kinesis 流接收器 # Kinesis Streams 接收器 (此后简称 \u0026ldquo;Kinesis 接收器\u0026rdquo;) 使用 AWS v2 SDK for Java 从 Flink 流数据写入数据到 Kinesis 流。
写数据到 Kinesis 流，确保流在 Amazon Kinesis Data Stream 控制台标记为 \u0026ldquo;ACTIVE\u0026rdquo;。
为了让监控工作，访问流的用户需要访问 CloudWatch 服务。
Java Properties sinkProperties = new Properties(); // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); KinesisStreamsSink\u0026lt;String\u0026gt; kdsSink = KinesisStreamsSink.\u0026lt;String\u0026gt;builder() .setKinesisClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setPartitionKeyGenerator(element -\u0026gt; String.valueOf(element.hashCode())) // Required .setStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10_000) // Optional .setMaxBatchSizeInBytes(5 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1 * 1024 * 1024) // Optional .build(); DataStream\u0026lt;String\u0026gt; simpleStringStream = ...; simpleStringStream.sinkTo(kdsSink); Scala val sinkProperties = new Properties() // Required sinkProperties.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) // Optional, provide via alternative routes e.g. environment variables sinkProperties.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) sinkProperties.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) val kdsSink = KinesisStreamsSink.\u0026lt;String\u0026gt;builder() .setKinesisClientProperties(sinkProperties) // Required .setSerializationSchema(new SimpleStringSchema()) // Required .setPartitionKeyGenerator(element -\u0026gt; String.valueOf(element.hashCode())) // Required .setStreamName(\u0026#34;your-stream-name\u0026#34;) // Required .setFailOnError(false) // Optional .setMaxBatchSize(500) // Optional .setMaxInFlightRequests(50) // Optional .setMaxBufferedRequests(10000) // Optional .setMaxBatchSizeInBytes(5 * 1024 * 1024) // Optional .setMaxTimeInBufferMS(5000) // Optional .setMaxRecordSizeInBytes(1 * 1024 * 1024) // Optional .build() val simpleStringStream = ... simpleStringStream.sinkTo(kdsSink) Python # Required sink_properties = { # Required \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, # Optional, provide via alternative routes e.g. environment variables \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4567\u0026#39; } kds_sink = KinesisStreamsSink.builder() \\ .set_kinesis_client_properties(sink_properties) \\ # Required .set_serialization_schema(SimpleStringSchema()) \\ # Required .set_partition_key_generator(PartitionKeyGenerator.fixed()) \\ # Required .set_stream_name(\u0026#34;your-stream-name\u0026#34;) \\ # Required .set_fail_on_error(False) \\ # Optional .set_max_batch_size(500) \\ # Optional .set_max_in_flight_requests(50) \\ # Optional .set_max_buffered_requests(10000) \\ # Optional .set_max_batch_size_in_bytes(5 * 1024 * 1024) \\ # Optional .set_max_time_in_buffer_ms(5000) \\ # Optional .set_max_record_size_in_bytes(1 * 1024 * 1024) \\ # Optional .build() simple_string_stream = ... simple_string_stream.sink_to(kds_sink) 上面是使用Kinesis 接收器的一个简单例子。从创建一个配置了 AWS_REGION, AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY\`\`java.util.Properties 的 java.util.Properties 实例开始。 然后可以使用构建器构造接收器。可选配置的默认值如上所示。其中一些值是根据 configuration on KDS 设置的。
您将始终需要指定您的序列化模式和逻辑用于从一个记录中生成 partition key 。
由于多种原因，Kinesis 数据流可能无法持久化请求中的部分或全部记录。如果 failOnError 发生，则会引发运行时异常。否则，这些记录将在缓冲区中重新入队列以供重试。
Kinesis Sink 通过 Flink 的 metrics system 提供了一些指标分析连接器的行为。所有公开指标的列表可在 here 找到。
根据 Kinesis Data Streams 的最大值，接收器默认最大记录大小为1MB，最大批量大小为5MB 。可以找到详细说明这些最大值的 AWS 文档 here 。
Kinesis 接收器和容错 # 接收器设计用于参与 Flink 的检查点，以提供至少一次处理的保证。它通过在执行检查点的同时完成任何等待的请求来实现这一点。这有效地确保了在继续处理更多记录之前，在检查点之前触发的所有请求都已成功传送到 Kinesis Data Streams 。
如果Flink需要从检查点（或保存点）还原，则自该检查点以来写入的数据将再次写入 Kinesis，从而导致流中出现重复。此外，接收器在内部使用 PutRecords API 调用，这并不保证维持事件顺序。
背压 # 当接收器缓冲区填满并写入接收器时，接收器中的背压会增加开始表现出阻塞行为。有关 Kinesis Data Streams 的速率限制的更多信息，可以在 Quotas and Limits 找到。
通常通过增加内部队列的大小来减少背压：
Java KinesisStreamsSink kdsSink = KinesisStreamsSink.builder() \u0026hellip; .setMaxBufferedRequests(10_000) \u0026hellip;
Python kds_sink = KinesisStreamsSink.builder() \\ .set_max_buffered_requests(10000) \\ .build() Kinesis 生产者 # 旧的 Kinesis 接收器 org.apache.flink.streaming.connectors.kinesis.FlinkKinesisProducer 已弃用，可能会随 Flink 的未来版本一起删除, 请用 Kinesis Sink 代替。 新的接收器使用 AWS v2 SDK for Java 然而老的接收器使用 Kinesis Producer Library. 因此，新的接收器不支持 aggregation 。
使用自定义 Kinesis 端点 # 有时，需要让 Flink 作为源或接收器，针对 Kinesis VPC 端点或非 AWS 进行操作Kinesis 端点，例如 Kinesalite; 这在执行 Flink 应用程序时特别有用。通常由 Flink 配置中设置的 AWS 区域集推断的 AWS 端点必须通过配置属性覆盖。
要覆盖 AWS 端点，设置 AWSConfigConstants.AWS_ENDPOINT 和 AWSConfigConstants.AWS_REGION 属性。该区域将用于对端点 URL 进行签名。
Java Properties config = new Properties(); config.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;); config.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;); config.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;); config.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4567\u0026#34;); Scala val config = new Properties() config.put(AWSConfigConstants.AWS_REGION, \u0026#34;us-east-1\u0026#34;) config.put(AWSConfigConstants.AWS_ACCESS_KEY_ID, \u0026#34;aws_access_key_id\u0026#34;) config.put(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, \u0026#34;aws_secret_access_key\u0026#34;) config.put(AWSConfigConstants.AWS_ENDPOINT, \u0026#34;http://localhost:4567\u0026#34;) Python config = { \u0026#39;aws.region\u0026#39;: \u0026#39;us-east-1\u0026#39;, \u0026#39;aws.credentials.provider.basic.accesskeyid\u0026#39;: \u0026#39;aws_access_key_id\u0026#39;, \u0026#39;aws.credentials.provider.basic.secretkey\u0026#39;: \u0026#39;aws_secret_access_key\u0026#39;, \u0026#39;aws.endpoint\u0026#39;: \u0026#39;http://localhost:4567\u0026#39; } Back to top
`}),e.add({id:141,href:"/flink/flink-docs-master/zh/docs/connectors/table/kinesis/",title:"Kinesis",section:"Table API Connectors",content:" Amazon Kinesis Data Streams SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode\nThe Kinesis connector allows for reading data from and writing data into Amazon Kinesis Data Streams (KDS).\nDependencies # In order to use the Kinesis connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.\nMaven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-kinesis\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Kinesis 连接器目前并不包含在 Flink 的二进制发行版中，请查阅这里了解如何在集群运行中引用 Kinesis 连接器。\nHow to create a Kinesis data stream table # Follow the instructions from the Amazon KDS Developer Guide to set up a Kinesis stream. The following example shows how to create a table backed by a Kinesis data stream:\nCREATE TABLE KinesisTable ( `user_id` BIGINT, `item_id` BIGINT, `category_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) ) PARTITIONED BY (user_id, item_id) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kinesis\u0026#39;, \u0026#39;stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;scan.stream.initpos\u0026#39; = \u0026#39;LATEST\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Available Metadata # The following metadata can be exposed as read-only (VIRTUAL) columns in a table definition.\nKey Data Type Description timestamp TIMESTAMP_LTZ(3) NOT NULL The approximate time when the record was inserted into the stream. shard-id VARCHAR(128) NOT NULL The unique identifier of the shard within the stream from which the record was read. sequence-number VARCHAR(128) NOT NULL The unique identifier of the record within its shard. The extended CREATE TABLE example demonstrates the syntax for exposing these metadata fields:\nCREATE TABLE KinesisTable ( `user_id` BIGINT, `item_id` BIGINT, `category_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3), `arrival_time` TIMESTAMP(3) METADATA FROM \u0026#39;timestamp\u0026#39; VIRTUAL, `shard_id` VARCHAR(128) NOT NULL METADATA FROM \u0026#39;shard-id\u0026#39; VIRTUAL, `sequence_number` VARCHAR(128) NOT NULL METADATA FROM \u0026#39;sequence-number\u0026#39; VIRTUAL ) PARTITIONED BY (user_id, item_id) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kinesis\u0026#39;, \u0026#39;stream\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;aws.region\u0026#39; = \u0026#39;us-east-2\u0026#39;, \u0026#39;scan.stream.initpos\u0026#39; = \u0026#39;LATEST\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Connector Options # Option Required Default Type Description Common Options connector required (none) String Specify what connector to use. For Kinesis use 'kinesis'. stream required (none) String Name of the Kinesis data stream backing this table. format required (none) String The format used to deserialize and serialize Kinesis data stream records. See Data Type Mapping for details. aws.region optional (none) String The AWS region where the stream is defined. Either this or aws.endpoint are required. aws.endpoint optional (none) String The AWS endpoint for Kinesis (derived from the AWS region setting if not set). Either this or aws.region are required. aws.trust.all.certificates optional false Boolean If true accepts all SSL certificates. Authentication Options aws.credentials.provider optional AUTO String A credentials provider to use when authenticating against the Kinesis endpoint. See Authentication for details. aws.credentials.basic.accesskeyid optional (none) String The AWS access key ID to use when setting credentials provider type to BASIC. aws.credentials.basic.secretkey optional (none) String The AWS secret key to use when setting credentials provider type to BASIC. aws.credentials.profile.path optional (none) String Optional configuration for profile path if credential provider type is set to be PROFILE. aws.credentials.profile.name optional (none) String Optional configuration for profile name if credential provider type is set to be PROFILE. aws.credentials.role.arn optional (none) String The role ARN to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.sessionName optional (none) String The role session name to use when credential provider type is set to ASSUME_ROLE or WEB_IDENTITY_TOKEN. aws.credentials.role.externalId optional (none) String The external ID to use when credential provider type is set to ASSUME_ROLE. aws.credentials.role.provider optional (none) String The credentials provider that provides credentials for assuming the role when credential provider type is set to ASSUME_ROLE. Roles can be nested, so this value can again be set to ASSUME_ROLE aws.credentials.webIdentityToken.file optional (none) String The absolute path to the web identity token file that should be used if provider type is set to WEB_IDENTITY_TOKEN. Source Options scan.stream.initpos optional LATEST String Initial position to be used when reading from the table. See Start Reading Position for details. scan.stream.initpos-timestamp optional (none) String The initial timestamp to start reading Kinesis stream from (when scan.stream.initpos is AT_TIMESTAMP). See Start Reading Position for details. scan.stream.initpos-timestamp-format optional yyyy-MM-dd'T'HH:mm:ss.SSSXXX String The date format of initial timestamp to start reading Kinesis stream from (when scan.stream.initpos is AT_TIMESTAMP). See Start Reading Position for details. scan.stream.recordpublisher optional POLLING String The RecordPublisher type to use for sources. See Enhanced Fan-Out for details. scan.stream.efo.consumername optional (none) String The name of the EFO consumer to register with KDS. See Enhanced Fan-Out for details. scan.stream.efo.registration optional LAZY String Determine how and when consumer de-/registration is performed (LAZY|EAGER|NONE). See Enhanced Fan-Out for details. scan.stream.efo.consumerarn optional (none) String The prefix of consumer ARN for a given stream. See Enhanced Fan-Out for details. scan.stream.efo.http-client.max-concurrency optional 10000 Integer Maximum number of allowed concurrent requests for the EFO client. See Enhanced Fan-Out for details. scan.stream.describe.maxretries optional 50 Integer The maximum number of describeStream attempts if we get a recoverable exception. scan.stream.describe.backoff.base optional 2000 Long The base backoff time (in milliseconds) between each describeStream attempt (for consuming from DynamoDB streams). scan.stream.describe.backoff.max optional 5000 Long The maximum backoff time (in milliseconds) between each describeStream attempt (for consuming from DynamoDB streams). scan.stream.describe.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each describeStream attempt (for consuming from DynamoDB streams). scan.list.shards.maxretries optional 10 Integer The maximum number of listShards attempts if we get a recoverable exception. scan.list.shards.backoff.base optional 1000 Long The base backoff time (in milliseconds) between each listShards attempt. scan.list.shards.backoff.max optional 5000 Long The maximum backoff time (in milliseconds) between each listShards attempt. scan.list.shards.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each listShards attempt. scan.stream.describestreamconsumer.maxretries optional 50 Integer The maximum number of describeStreamConsumer attempts if we get a recoverable exception. scan.stream.describestreamconsumer.backoff.base optional 2000 Long The base backoff time (in milliseconds) between each describeStreamConsumer attempt. scan.stream.describestreamconsumer.backoff.max optional 5000 Long The maximum backoff time (in milliseconds) between each describeStreamConsumer attempt. scan.stream.describestreamconsumer.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each describeStreamConsumer attempt. scan.stream.registerstreamconsumer.maxretries optional 10 Integer The maximum number of registerStream attempts if we get a recoverable exception. scan.stream.registerstreamconsumer.timeout optional 60 Integer The maximum time in seconds to wait for a stream consumer to become active before giving up. scan.stream.registerstreamconsumer.backoff.base optional 500 Long The base backoff time (in milliseconds) between each registerStream attempt. scan.stream.registerstreamconsumer.backoff.max optional 2000 Long The maximum backoff time (in milliseconds) between each registerStream attempt. scan.stream.registerstreamconsumer.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each registerStream attempt. scan.stream.deregisterstreamconsumer.maxretries optional 10 Integer The maximum number of deregisterStream attempts if we get a recoverable exception. scan.stream.deregisterstreamconsumer.timeout optional 60 Integer The maximum time in seconds to wait for a stream consumer to deregister before giving up. scan.stream.deregisterstreamconsumer.backoff.base optional 500 Long The base backoff time (in milliseconds) between each deregisterStream attempt. scan.stream.deregisterstreamconsumer.backoff.max optional 2000 Long The maximum backoff time (in milliseconds) between each deregisterStream attempt. scan.stream.deregisterstreamconsumer.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each deregisterStream attempt. scan.shard.subscribetoshard.maxretries optional 10 Integer The maximum number of subscribeToShard attempts if we get a recoverable exception. scan.shard.subscribetoshard.backoff.base optional 1000 Long The base backoff time (in milliseconds) between each subscribeToShard attempt. scan.shard.subscribetoshard.backoff.max optional 2000 Long The maximum backoff time (in milliseconds) between each subscribeToShard attempt. scan.shard.subscribetoshard.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each subscribeToShard attempt. scan.shard.getrecords.maxrecordcount optional 10000 Integer The maximum number of records to try to get each time we fetch records from a AWS Kinesis shard. scan.shard.getrecords.maxretries optional 3 Integer The maximum number of getRecords attempts if we get a recoverable exception. scan.shard.getrecords.backoff.base optional 300 Long The base backoff time (in milliseconds) between getRecords attempts if we get a ProvisionedThroughputExceededException. scan.shard.getrecords.backoff.max optional 1000 Long The maximum backoff time (in milliseconds) between getRecords attempts if we get a ProvisionedThroughputExceededException. scan.shard.getrecords.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each getRecords attempt. scan.shard.getrecords.intervalmillis optional 200 Long The interval (in milliseconds) between each getRecords request to a AWS Kinesis shard in milliseconds. scan.shard.getiterator.maxretries optional 3 Integer The maximum number of getShardIterator attempts if we get ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.base optional 300 Long The base backoff time (in milliseconds) between getShardIterator attempts if we get a ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.max optional 1000 Long The maximum backoff time (in milliseconds) between getShardIterator attempts if we get a ProvisionedThroughputExceededException. scan.shard.getiterator.backoff.expconst optional 1.5 Double The power constant for exponential backoff between each getShardIterator attempt. scan.shard.discovery.intervalmillis optional 10000 Integer The interval between each attempt to discover new shards. scan.shard.adaptivereads optional false Boolean The config to turn on adaptive reads from a shard. See the AdaptivePollingRecordPublisher documentation for details. scan.shard.idle.interval optional -1 Long The interval (in milliseconds) after which to consider a shard idle for purposes of watermark generation. A positive value will allow the watermark to progress even when some shards don't receive new records. scan.watermark.sync.interval optional 30000 Long The interval (in milliseconds) for periodically synchronizing the shared watermark state. scan.watermark.lookahead.millis optional 0 Long The maximum delta (in milliseconds) allowed for the reader to advance ahead of the shared global watermark. scan.watermark.sync.queue.capacity optional 100 Integer The maximum number of records that will be buffered before suspending consumption of a shard. Sink Options sink.partitioner optional random or row-based String Optional output partitioning from Flink's partitions into Kinesis shards. See Sink Partitioning for details. sink.partitioner-field-delimiter optional | String Optional field delimiter for a fields-based partitioner derived from a PARTITION BY clause. See Sink Partitioning for details. sink.producer.* optional (none) Deprecated options previously used by the legacy connector. Options with equivalant alternatives in KinesisStreamsSink are matched to their respective properties. Unsupported options are logged out to user as warnings. sink.http-client.max-concurrency optional 10000 Integer Maximum number of allowed concurrent requests by KinesisAsyncClient. sink.http-client.read-timeout optional 360000 Integer Maximum amount of time in ms for requests to be sent by KinesisAsyncClient. sink.http-client.protocol.version optional HTTP2 String Http version used by Kinesis Client. sink.batch.max-size optional 500 Integer Maximum batch size of elements to be passed to KinesisAsyncClient to be written downstream. sink.requests.max-inflight optional 16 Integer Request threshold for uncompleted requests by KinesisAsyncClientbefore blocking new write requests and applying backpressure. sink.requests.max-buffered optional 10000 String Request buffer threshold for buffered requests by KinesisAsyncClient before blocking new write requests and applying backpressure. sink.flush-buffer.size optional 5242880 Long Threshold value in bytes for writer buffer in KinesisAsyncClient before flushing. sink.flush-buffer.timeout optional 5000 Long Threshold time in milliseconds for an element to be in a buffer ofKinesisAsyncClient before flushing. Features # Authorization # Make sure to create an appropriate IAM policy to allow reading from / writing to the Kinesis data streams.\nAuthentication # Depending on your deployment you would choose a different Credentials Provider to allow access to Kinesis. By default, the AUTO Credentials Provider is used. If the access key ID and secret key are set in the deployment configuration, this results in using the BASIC provider.\nA specific AWSCredentialsProvider can be optionally set using the aws.credentials.provider setting. Supported values are:\nAUTO - Use the default AWS Credentials Provider chain that searches for credentials in the following order: ENV_VARS, SYS_PROPS, WEB_IDENTITY_TOKEN, PROFILE, and EC2/ECS credentials provider. BASIC - Use access key ID and secret key supplied as configuration. ENV_VAR - Use AWS_ACCESS_KEY_ID \u0026amp; AWS_SECRET_ACCESS_KEY environment variables. SYS_PROP - Use Java system properties aws.accessKeyId and aws.secretKey. PROFILE - Use an AWS credentials profile to create the AWS credentials. ASSUME_ROLE - Create AWS credentials by assuming a role. The credentials for assuming the role must be supplied. WEB_IDENTITY_TOKEN - Create AWS credentials by assuming a role using Web Identity Token. Start Reading Position # You can configure table sources to start reading a table-backing Kinesis data stream from a specific position through the scan.stream.initpos option. Available values are:\nLATEST: read shards starting from the latest record. TRIM_HORIZON: read shards starting from the earliest record possible (data may be trimmed by Kinesis depending on the current retention settings of the backing stream). AT_TIMESTAMP: read shards starting from a specified timestamp. The timestamp value should be specified through the scan.stream.initpos-timestamp in one of the following formats: A non-negative double value representing the number of seconds that has elapsed since the Unix epoch (for example, 1459799926.480). A value conforming to a user-defined SimpleDateFormat specified at scan.stream.initpos-timestamp-format. If a user does not define a format, the default pattern will be yyyy-MM-dd'T'HH:mm:ss.SSSXXX. For example, timestamp value is 2016-04-04 and user-defined format is yyyy-MM-dd, or timestamp value is 2016-04-04T19:58:46.480-00:00 and a user-defined format is not provided. Sink Partitioning # Kinesis data streams consist of one or more shards, and the sink.partitioner option allows you to control how records written into a multi-shard Kinesis-backed table will be partitioned between its shards. Valid values are:\nfixed: Kinesis PartitionKey values derived from the Flink subtask index, so each Flink partition ends up in at most one Kinesis partition (assuming that no re-sharding takes place at runtime). random: Kinesis PartitionKey values are assigned randomly. This is the default value for tables not defined with a PARTITION BY clause. Custom FixedKinesisPartitioner subclass: e.g. 'org.mycompany.MyPartitioner'. Records written into tables defining a PARTITION BY clause will always be partitioned based on a concatenated projection of the PARTITION BY fields. In this case, the sink.partitioner field cannot be used to modify this behavior (attempting to do this results in a configuration error). You can, however, use the sink.partitioner-field-delimiter option to set the delimiter of field values in the concatenated PartitionKey string (an empty string is also a valid delimiter). Enhanced Fan-Out # Enhanced Fan-Out (EFO) increases the maximum number of concurrent consumers per Kinesis data stream. Without EFO, all concurrent Kinesis consumers share a single read quota per shard. Using EFO, each consumer gets a distinct dedicated read quota per shard, allowing read throughput to scale with the number of consumers.\nNote Using EFO will incur additional cost.\nYou can enable and configure EFO with the following properties:\nscan.stream.recordpublisher: Determines whether to use EFO or POLLING. scan.stream.efo.consumername: A name to identify the consumer when the above value is EFO. scan.stream.efo.registration: Strategy for (de-)registration of EFO consumers with the name given by the scan.stream.efo.consumername value. Valid strategies are: LAZY (default): Stream consumers are registered when the Flink job starts running. If the stream consumer already exists, it will be reused. This is the preferred strategy for the majority of applications. However, jobs with parallelism greater than 1 will result in tasks competing to register and acquire the stream consumer ARN. For jobs with very large parallelism this can result in an increased start-up time. The describe operation has a limit of 20 transactions per second, this means application startup time will increase by roughly parallelism/20 seconds. EAGER: Stream consumers are registered in the FlinkKinesisConsumer constructor. If the stream consumer already exists, it will be reused. This will result in registration occurring when the job is constructed, either on the Flink Job Manager or client environment submitting the job. Using this strategy results in a single thread registering and retrieving the stream consumer ARN, reducing startup time over LAZY (with large parallelism). However, consider that the client environment will require access to the AWS services. NONE: Stream consumer registration is not performed by FlinkKinesisConsumer. Registration must be performed externally using the AWS CLI or SDK to invoke RegisterStreamConsumer. Stream consumer ARNs should be provided to the job via the consumer configuration. scan.stream.efo.consumerarn.\u0026lt;stream-name\u0026gt;: ARNs identifying externally registered ARN-consumers (substitute \u0026lt;stream-name\u0026gt; with the name of your stream in the parameter name). Use this if you choose to use NONE as a scan.stream.efo.registration strategy. Note For a given Kinesis data stream, each EFO consumer must have a unique name. However, consumer names do not have to be unique across data streams. Reusing a consumer name will result in existing subscriptions being terminated.\nNote With the LAZY strategy, stream consumers are de-registered when the job is shutdown gracefully. In the event that a job terminates within executing the shutdown hooks, stream consumers will remain active. In this situation the stream consumers will be gracefully reused when the application restarts. With the NONE and EAGER strategies, stream consumer de-registration is not performed by FlinkKinesisConsumer.\nData Type Mapping # Kinesis stores records as Base64-encoded binary data objects, so it doesn\u0026rsquo;t have a notion of internal record structure. Instead, Kinesis records are deserialized and serialized by formats, e.g. \u0026lsquo;avro\u0026rsquo;, \u0026lsquo;csv\u0026rsquo;, or \u0026lsquo;json\u0026rsquo;. To determine the data type of the messages in your Kinesis-backed tables, pick a suitable Flink format with the format keyword. Please refer to the Formats pages for more details.\nUpdates in 1.15 # Kinesis table API connector sink data stream depends on FlinkKinesisProducer till 1.14, with the introduction of KinesisStreamsSink in 1.15 kinesis table API sink connector has been migrated to the new KinesisStreamsSink. Authentication options have been migrated identically while sink configuration options are now compatible with KinesisStreamsSink.\nOptions configuring FlinkKinesisProducer are now deprecated with fallback support for common configuration options with KinesisStreamsSink.\nKinesisStreamsSink uses KinesisAsyncClient to send records to kinesis, which doesn\u0026rsquo;t support aggregation. In consequence, table options configuring aggregation in the deprecated FlinkKinesisProducer are now deprecated and will be ignored, this includes sink.producer.aggregation-enabled and sink.producer.aggregation-count.\nNote Migrating applications with deprecated options will result in the incompatible deprecated options being ignored and warned to users.\nKinesis table API source connector still depends on FlinkKinesisConsumer with no change in configuration options.\nBack to top\n"}),e.add({id:142,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/kubernetes/",title:"Kubernetes 设置",section:"Standalone",content:` Kubernetes 安装 # 入门 # 本 入门 指南描述了如何在 Kubernetes 上部署 Flink Session 集群。
介绍 # 本文描述了如何使用 Flink standalone 部署模式在 Kubernetes 上部署 standalone 模式的 Flink 集群。通常我们建议新用户使用 native Kubernetes 部署模式在 Kubernetes上部署 Flink。
准备 # 本指南假设存在一个 Kubernets 的运行环境。你可以通过运行 kubectl get nodes 命令来确保 Kubernetes 环境运行正常，该命令展示所有连接到 Kubernets 集群的 node 节点信息。
如果你想在本地运行 Kubernetes，建议使用 MiniKube。
如果使用 MiniKube，请确保在部署 Flink 集群之前先执行 minikube ssh 'sudo ip link set docker0 promisc on'，否则 Flink 组件不能自动地将自己映射到 Kubernetes Service 中。 Kubernetes 上的 Flink session 集群 # Flink session 集群 是以一种长期运行的 Kubernetes Deployment 形式执行的。你可以在一个 session 集群 上运行多个 Flink 作业。当然，只有 session 集群部署好以后才可以在上面提交 Flink 作业。
在 Kubernetes 上部署一个基本的 Flink session 集群 时，一般包括下面三个组件：
运行 JobManager 的 Deployment； 运行 TaskManagers 的 Deployment； 暴露 JobManager 上 REST 和 UI 端口的 Service； 使用通用集群资源定义中提供的文件内容来创建以下文件，并使用 kubectl 命令来创建相应的组件：
# Configuration 和 service 的定义 \$ kubectl create -f flink-configuration-configmap.yaml \$ kubectl create -f jobmanager-service.yaml # 为集群创建 deployment \$ kubectl create -f jobmanager-session-deployment.yaml \$ kubectl create -f taskmanager-session-deployment.yaml 接下来，我们设置端口转发以访问 Flink UI 页面并提交作业：
运行 kubectl port-forward \${flink-jobmanager-pod} 8081:8081 将 jobmanager 的 web ui 端口映射到本地 8081。 在浏览器中导航到 http://localhost:8081 页面。 此外，也可以使用如下命令向集群提交作业： \$ ./bin/flink run -m localhost:8081 ./examples/streaming/TopSpeedWindowing.jar 可以使用以下命令停止运行 flink 集群：
\$ kubectl delete -f jobmanager-service.yaml \$ kubectl delete -f flink-configuration-configmap.yaml \$ kubectl delete -f taskmanager-session-deployment.yaml \` kubectl delete -f jobmanager-session-deployment.yaml Back to top
部署模式 # Application 集群模式 # Flink Application 集群 是运行单个 Application 的专用集群，部署集群时要保证该 Application 可用。
在 Kubernetes 上部署一个基本的 Flink Application 集群 时，一般包括下面三个组件：
一个运行 JobManager 的 Application； 运行若干个 TaskManager 的 Deployment； 暴露 JobManager 上 REST 和 UI 端口的 Service； 检查 Application 集群资源定义 并做出相应的调整：
jobmanager-job.yaml 中的 args 属性必须指定用户作业的主类。也可以参考如何设置 JobManager 参数来了解如何将额外的 args 传递给 jobmanager-job.yaml 配置中指定的 Flink 镜像。
job artifacts 参数必须可以从 资源定义示例 中的 job-artifacts-volume 处获取。假如是在 minikube 集群中创建这些组件，那么定义示例中的 job-artifacts-volume 可以挂载为主机的本地目录。如果不使用 minikube 集群，那么可以使用 Kubernetes 集群中任何其它可用类型的 volume 来提供 job artifacts。此外，还可以构建一个已经包含 job artifacts 参数的自定义镜像。
在创建通用集群组件后，指定 Application 集群资源定义文件，执行 kubectl 命令来启动 Flink Application 集群：
\$ kubectl create -f jobmanager-job.yaml \$ kubectl create -f taskmanager-job-deployment.yaml 要停止单个 application 集群，可以使用 kubectl 命令来删除相应组件以及 通用集群资源对应的组件 ：
\$ kubectl delete -f taskmanager-job-deployment.yaml \$ kubectl delete -f jobmanager-job.yaml Per-Job 集群模式 # 在 Kubernetes 上部署 Standalone 集群时不支持 Per-Job 集群模式。
Session 集群模式 # 本文档开始部分的入门指南中描述了 Session 集群模式的部署。
Back to top
Kubernetes 上运行 Standalone 集群指南 # Configuration # 所有配置项都展示在配置页面上。在 config map 配置文件 flink-configuration-configmap.yaml 中，可以将配置添加在 flink-conf.yaml 部分。
在 Kubernets 上访问 Flink # 接下来可以访问 Flink UI 页面并通过不同的方式提交作业：
kubectl proxy:
在终端运行 kubectl proxy 命令。 在浏览器中导航到 http://localhost:8001/api/v1/namespaces/default/services/flink-jobmanager:webui/proxy。 kubectl port-forward:
运行 kubectl port-forward \${flink-jobmanager-pod} 8081:8081 将 jobmanager 的 web ui 端口映射到本地的 8081。 在浏览器中导航到 http://localhost:8081。 此外，也可以使用如下命令向集群提交作业： \$ ./bin/flink run -m localhost:8081 ./examples/streaming/TopSpeedWindowing.jar 基于 jobmanager 的 rest 服务上创建 NodePort service：
运行 kubectl create -f jobmanager-rest-service.yaml 来基于 jobmanager 创建 NodePort service。jobmanager-rest-service.yaml 的示例文件可以在 附录 中找到。 运行 kubectl get svc flink-jobmanager-rest 来查询 server 的 node-port，然后再浏览器导航到 http://\u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt;。 如果使用 minikube 集群，可以执行 minikube ip 命令来查看 public ip。 与 port-forward 方案类似，也可以使用如下命令向集群提交作业。 \$ ./bin/flink run -m \u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt; ./examples/streaming/TopSpeedWindowing.jar 调试和访问日志 # 通过查看 Flink 的日志文件，可以很轻松地发现许多常见错误。如果你有权访问 Flink 的 Web 用户界面，那么可以在页面上访问 JobManager 和 TaskManager 日志。
如果启动 Flink 出现问题，也可以使用 Kubernetes 工具集访问日志。使用 kubectl get pods 命令查看所有运行的 pods 资源。针对上面的快速入门示例，你可以看到三个 pod：
\$ kubectl get pods NAME READY STATUS RESTARTS AGE flink-jobmanager-589967dcfc-m49xv 1/1 Running 3 3m32s flink-taskmanager-64847444ff-7rdl4 1/1 Running 3 3m28s flink-taskmanager-64847444ff-nnd6m 1/1 Running 3 3m28s 现在你可以通过运行 kubectl logs flink-jobmanager-589967dcfc-m49xv 来访问日志。
高可用的 Standalone Kubernetes # 对于在 Kubernetes 上实现HA，可以参考当前的 Kubernets 高可用服务。
Kubernetes 高可用 Services # Session 模式和 Application 模式集群都支持使用 Kubernetes 高可用服务。需要在 flink-configuration-configmap.yaml 中添加如下 Flink 配置项。
Note 配置了 HA 存储目录相对应的文件系统必须在运行时可用。请参阅自定义Flink 镜像和启用文件系统插件获取更多相关信息。
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ ... kubernetes.cluster-id: \u0026lt;cluster-id\u0026gt; high-availability: kubernetes high-availability.storageDir: hdfs:///flink/recovery restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 10 ... 此外，你必须使用具有创建、编辑、删除 ConfigMap 权限的 service 账号启动 JobManager 和 TaskManager pod。请查看如何为 pod 配置 service 账号获取更多信息。
当启用了高可用，Flink 会使用自己的 HA 服务进行服务发现。因此，JobManager Pod 会使用 IP 地址而不是 Kubernetes 的 service 名称来作为 jobmanager.rpc.address 的配置项启动。完整配置请参考附录。
Standby JobManagers # 通常，只启动一个 JobManager pod 就足够了，因为一旦 pod 崩溃，Kubernetes 就会重新启动它。如果要实现更快的恢复，需要将 jobmanager-session-deployment-ha.yaml 中的 replicas 配置 或 jobmanager-application-ha.yaml 中的 parallelism 配置设定为大于 1 的整型值来启动 Standby JobManagers。
启用 Queryable State # 如果你为 TaskManager 创建了 NodePort service，那么你就可以访问 TaskManager 的 Queryable State 服务：
运行 kubectl create -f taskmanager-query-state-service.yaml 来为 taskmanager pod 创建 NodePort service。taskmanager-query-state-service.yaml 的示例文件可以从附录中找到。 运行 kubectl get svc flink-taskmanager-query-state 来查询 service 对应 node-port 的端口号。然后你就可以创建 QueryableStateClient(\u0026lt;public-node-ip\u0026gt;, \u0026lt;node-port\u0026gt; 来提交状态查询。 在 Reactive 模式下使用 Standalone Kubernetes # Reactive Mode 允许在 Application 集群 始终根据可用资源调整作业并行度的模式下运行 Flink。与 Kubernetes 结合使用，TaskManager 部署的副本数决定了可用资源。增加副本数将扩大作业规模，而减少副本数将会触发缩减作业规模。通过使用 Horizontal Pod Autoscaler 也可以自动实现该功能。
要在 Kubernetes 上使用 Reactive Mode，请按照使用 Application 集群部署作业 完成相同的步骤。但是要使用 flink-reactive-mode-configuration-configmap.yaml 配置文件来代替 flink-configuration-configmap.yaml。该文件包含了针对 Flink 的 scheduler-mode: reactive 配置。
一旦你部署了 Application 集群，就可以通过修改 flink-taskmanager 的部署副本数量来扩大或缩小作业的并行度。
Back to top
附录 # 通用集群资源定义 # flink-configuration-configmap.yaml
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 queryable-state.proxy.ports: 6125 jobmanager.memory.process.size: 1600m taskmanager.memory.process.size: 1728m parallelism.default: 2 log4j-console.properties: |+ # 如下配置会同时影响用户代码和 Flink 的日志行为 rootLogger.level = INFO rootLogger.appenderRef.console.ref = ConsoleAppender rootLogger.appenderRef.rolling.ref = RollingFileAppender # 如果你只想改变 Flink 的日志行为则可以取消如下的注释部分 #logger.flink.name = org.apache.flink #logger.flink.level = INFO # 下面几行将公共 libraries 或 connectors 的日志级别保持在 INFO 级别。 # root logger 的配置不会覆盖此处配置。 # 你必须手动修改这里的日志级别。 logger.akka.name = akka logger.akka.level = INFO logger.kafka.name= org.apache.kafka logger.kafka.level = INFO logger.hadoop.name = org.apache.hadoop logger.hadoop.level = INFO logger.zookeeper.name = org.apache.zookeeper logger.zookeeper.level = INFO # 将所有 info 级别的日志输出到 console appender.console.name = ConsoleAppender appender.console.type = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # 将所有 info 级别的日志输出到指定的 rolling file appender.rolling.name = RollingFileAppender appender.rolling.type = RollingFile appender.rolling.append = false appender.rolling.fileName = \${sys:log.file} appender.rolling.filePattern = \${sys:log.file}.%i appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size=100MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 10 # 关闭 Netty channel handler 中不相关的（错误）警告 logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline logger.netty.level = OFF flink-reactive-mode-configuration-configmap.yaml
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 2 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 queryable-state.proxy.ports: 6125 jobmanager.memory.process.size: 1600m taskmanager.memory.process.size: 1728m parallelism.default: 2 scheduler-mode: reactive execution.checkpointing.interval: 10s log4j-console.properties: |+ # 如下配置会同时影响用户代码和 Flink 的日志行为 rootLogger.level = INFO rootLogger.appenderRef.console.ref = ConsoleAppender rootLogger.appenderRef.rolling.ref = RollingFileAppender # 如果你只想改变 Flink 的日志行为则可以取消如下的注释部分 #logger.flink.name = org.apache.flink #logger.flink.level = INFO # 下面几行将公共 libraries 或 connectors 的日志级别保持在 INFO 级别。 # root logger 的配置不会覆盖此处配置。 # 你必须手动修改这里的日志级别。 logger.akka.name = akka logger.akka.level = INFO logger.kafka.name= org.apache.kafka logger.kafka.level = INFO logger.hadoop.name = org.apache.hadoop logger.hadoop.level = INFO logger.zookeeper.name = org.apache.zookeeper logger.zookeeper.level = INFO # 将所有 info 级别的日志输出到 console appender.console.name = ConsoleAppender appender.console.type = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # 将所有 info 级别的日志输出到指定的 rolling file appender.rolling.name = RollingFileAppender appender.rolling.type = RollingFile appender.rolling.append = false appender.rolling.fileName = \${sys:log.file} appender.rolling.filePattern = \${sys:log.file}.%i appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n appender.rolling.policies.type = Policies appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size=100MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 10 # 关闭 Netty channel handler 中不相关的（错误）警告 logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline logger.netty.level = OFF jobmanager-service.yaml 。可选的 service，仅在非 HA 模式下需要。
apiVersion: v1 kind: Service metadata: name: flink-jobmanager spec: type: ClusterIP ports: - name: rpc port: 6123 - name: blob-server port: 6124 - name: webui port: 8081 selector: app: flink component: jobmanager jobmanager-rest-service.yaml。可选的 service，该 service 将 jobmanager 的 rest 端口暴露为公共 Kubernetes node 的节点端口。
apiVersion: v1 kind: Service metadata: name: flink-jobmanager-rest spec: type: NodePort ports: - name: rest port: 8081 targetPort: 8081 nodePort: 30081 selector: app: flink component: jobmanager taskmanager-query-state-service.yaml。可选的 service，该 service 将 TaskManager 的端口暴露为公共 Kubernetes node 的节点端口，通过该端口来访问 queryable state 服务。
apiVersion: v1 kind: Service metadata: name: flink-taskmanager-query-state spec: type: NodePort ports: - name: query-state port: 6125 targetPort: 6125 nodePort: 30025 selector: app: flink component: taskmanager Session 集群资源定义 # jobmanager-session-deployment-non-ha.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: apache/flink:latest args: [\u0026#34;jobmanager\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties jobmanager-session-deployment-ha.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 # 通过设置大于 1 的整型值来开启 Standby JobManager selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: apache/flink:latest env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP # 下面的 args 参数会使用 POD_IP 对应的值覆盖 config map 中 jobmanager.rpc.address 的属性值。 args: [\u0026#34;jobmanager\u0026#34;, \u0026#34;\$(POD_IP)\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 serviceAccountName: flink-service-account # 拥有创建、编辑、删除 ConfigMap 权限的 Service 账号 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties taskmanager-session-deployment.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: apache/flink:latest args: [\u0026#34;taskmanager\u0026#34;] ports: - containerPort: 6122 name: rpc - containerPort: 6125 name: query-state livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties Application 集群资源定义 # jobmanager-application-non-ha.yaml
apiVersion: batch/v1 kind: Job metadata: name: flink-jobmanager spec: template: metadata: labels: app: flink component: jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: apache/flink:latest env: args: [\u0026#34;standalone-job\u0026#34;, \u0026#34;--job-classname\u0026#34;, \u0026#34;com.job.ClassName\u0026#34;, \u0026lt;optional arguments\u0026gt;, \u0026lt;job arguments\u0026gt;] # 可选的参数项: [\u0026#34;--job-id\u0026#34;, \u0026#34;\u0026lt;job id\u0026gt;\u0026#34;, \u0026#34;--fromSavepoint\u0026#34;, \u0026#34;/path/to/savepoint\u0026#34;, \u0026#34;--allowNonRestoredState\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts jobmanager-application-ha.yaml
apiVersion: batch/v1 kind: Job metadata: name: flink-jobmanager spec: parallelism: 1 # 通过设置大于 1 的整型值来开启 Standby JobManager template: metadata: labels: app: flink component: jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: apache/flink:latest env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP # 下面的 args 参数会使用 POD_IP 对应的值覆盖 config map 中 jobmanager.rpc.address 的属性值。 args: [\u0026#34;standalone-job\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;\$(POD_IP)\u0026#34;, \u0026#34;--job-classname\u0026#34;, \u0026#34;com.job.ClassName\u0026#34;, \u0026lt;optional arguments\u0026gt;, \u0026lt;job arguments\u0026gt;] # 可选参数项: [\u0026#34;--job-id\u0026#34;, \u0026#34;\u0026lt;job id\u0026gt;\u0026#34;, \u0026#34;--fromSavepoint\u0026#34;, \u0026#34;/path/to/savepoint\u0026#34;, \u0026#34;--allowNonRestoredState\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob-server - containerPort: 8081 name: webui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 serviceAccountName: flink-service-account # 拥有创建、编辑、删除 ConfigMap 权限的 Service 账号 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts taskmanager-job-deployment.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: apache/flink:latest env: args: [\u0026#34;taskmanager\u0026#34;] ports: - containerPort: 6122 name: rpc - containerPort: 6125 name: query-state livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ - name: job-artifacts-volume mountPath: /opt/flink/usrlib securityContext: runAsUser: 9999 # 参考官方 flink 镜像中的 _flink_ 用户，如有必要可以修改 volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j-console.properties path: log4j-console.properties - name: job-artifacts-volume hostPath: path: /host/path/to/job/artifacts Back to top
`}),e.add({id:143,href:"/flink/flink-docs-master/zh/docs/libs/",title:"Libraries",section:"Docs",content:" "}),e.add({id:144,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/plugins/",title:"Plugins",section:"File Systems",content:` Plugins # Plugins facilitate a strict separation of code through restricted classloaders. Plugins cannot access classes from other plugins or from Flink that have not been specifically whitelisted. This strict isolation allows plugins to contain conflicting versions of the same library without the need to relocate classes or to converge to common versions. Currently, file systems and metric reporters are pluggable but in the future, connectors, formats, and even user code should also be pluggable.
Isolation and plugin structure # Plugins reside in their own folders and can consist of several jars. The names of the plugin folders are arbitrary.
flink-dist ├── conf ├── lib ... └── plugins ├── s3 │ ├── aws-credential-provider.jar │ └── flink-s3-fs-hadoop.jar └── azure └── flink-azure-fs-hadoop.jar Each plugin is loaded through its own classloader and completely isolated from any other plugin. Hence, the flink-s3-fs-hadoop and flink-azure-fs-hadoop can depend on different conflicting library versions. There is no need to relocate any class during the creation of fat jars (shading).
Plugins may access certain whitelisted packages from Flink\u0026rsquo;s lib/ folder. In particular, all necessary service provider interfaces (SPI) are loaded through the system classloader, so that no two versions of org.apache.flink.core.fs.FileSystem exist at any given time, even if users accidentally bundle it in their fat jar. This singleton class requirement is strictly necessary so that the Flink runtime has an entry point into the plugin. Service classes are discovered through the java.util.ServiceLoader, so make sure to retain the service definitions in META-INF/services during shading.
Note Currently, more Flink core classes are still accessible from plugins as we flesh out the SPI system.
Furthermore, the most common logger frameworks are whitelisted, such that logging is uniformly possible across Flink core, plugins, and user code.
File Systems # All file systems are pluggable. That means they can and should be used as plugins. To use a pluggable file system, copy the corresponding JAR file from the opt directory to a directory under plugins directory of your Flink distribution before starting Flink, e.g.
mkdir ./plugins/s3-fs-hadoop cp ./opt/flink-s3-fs-hadoop-1.16-SNAPSHOT.jar ./plugins/s3-fs-hadoop/ The s3 file systems (flink-s3-fs-presto and flink-s3-fs-hadoop) can only be used as plugins as we already removed the relocations. Placing them in libs/ will result in system failures. Because of the strict isolation, file systems do not have access to credential providers in lib/ anymore. Please add any needed providers to the respective plugin folder. Metric Reporters # All metric reporters that Flink provides can be used as plugins. See the metrics documentation for more details.
Back to top
`}),e.add({id:145,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/queryable_state/",title:"Queryable State",section:"状态与容错",content:` Queryable State # 目前 querable state 的客户端 API 还在不断演进，不保证现有接口的稳定性。在后续的 Flink 版本中有可能发生 API 变化。 简而言之, 这个特性将 Flink 的 managed keyed (partitioned) state (参考 Working with State) 暴露给外部，从而用户可以在 Flink 外部查询作业 state。 在某些场景中，Queryable State 消除了对外部系统的分布式操作以及事务的需求，比如 KV 存储系统，而这些外部系统往往会成为瓶颈。除此之外，这个特性对于调试作业非常有用。
注意: 进行查询时，state 会在并发线程中被访问，但 state 不会进行同步和拷贝。这种设计是为了避免同步和拷贝带来的作业延时。对于使用 Java 堆内存的 state backend， 比如 MemoryStateBackend 或者 FsStateBackend，它们获取状态时不会进行拷贝，而是直接引用状态对象，所以对状态的 read-modify-write 是不安全的，并且可能会因为并发修改导致查询失败。但 RocksDBStateBackend 是安全的，不会遇到上述问题。 架构 # 在展示如何使用 Queryable State 之前，先简单描述一下该特性的组成部分，主要包括以下三部分:
QueryableStateClient，默认运行在 Flink 集群外部，负责提交用户的查询请求； QueryableStateClientProxy，运行在每个 TaskManager 上(即 Flink 集群内部)，负责接收客户端的查询请求，从所负责的 Task Manager 获取请求的 state，并返回给客户端； QueryableStateServer, 运行在 TaskManager 上，负责服务本地存储的 state。 客户端连接到一个代理，并发送请求获取特定 k 对应的 state。 如 Working with State 所述，keyed state 按照 Key Groups 进行划分，每个 TaskManager 会分配其中的一些 key groups。代理会询问 JobManager 以找到 k 所属 key group 的 TaskManager。根据返回的结果, 代理将会向运行在 TaskManager 上的 QueryableStateServer 查询 k 对应的 state， 并将结果返回给客户端。
激活 Queryable State # 为了在 Flink 集群上使用 queryable state，需要进行以下操作：
将 flink-queryable-state-runtime-1.16-SNAPSHOT.jar 从 Flink distribution 的 opt/ 目录拷贝到 lib/ 目录； 将参数 queryable-state.enable 设置为 true。详细信息以及其它配置可参考文档 Configuration。 为了验证集群的 queryable state 已经被激活，可以检查任意 task manager 的日志中是否包含 \u0026ldquo;Started the Queryable State Proxy Server @ \u0026hellip;\u0026quot;。
将 state 设置为可查询的 # 激活集群的 queryable state 功能后，还要将 state 设置为可查询的才能对外可见，可以通过以下两种方式进行设置：
创建 QueryableStateStream，它会作为一个 sink，并将输入数据转化为 queryable state； 通过 stateDescriptor.setQueryable(String queryableStateName) 将 state 描述符所表示的 keyed state 设置成可查询的。 接下来的部分将详细解释这两种方式。
Queryable State Stream # 在 KeyedStream 上调用 .asQueryableState(stateName, stateDescriptor) 将会返回一个 QueryableStateStream， 它会将流数据转化为 queryable state。 对应不同的 state 类型，asQueryableState() 有以下一些方法变体：
// ValueState QueryableStateStream asQueryableState( String queryableStateName, ValueStateDescriptor stateDescriptor) // Shortcut for explicit ValueStateDescriptor variant QueryableStateStream asQueryableState(String queryableStateName) // ReducingState QueryableStateStream asQueryableState( String queryableStateName, ReducingStateDescriptor stateDescriptor) 注意: 没有可查询的 ListState sink，因为这种情况下 list 会不断增长，并且可能不会被清理，最终会消耗大量的内存。 返回的 QueryableStateStream 可以被视作一个sink，而且不能再被进一步转换。在内部实现上，一个 QueryableStateStream 被转换成一个 operator，使用输入的数据来更新 queryable state。state 如何更新是由 asQueryableState 提供的 StateDescriptor 来决定的。在下面的代码中, keyed stream 的所有数据将会通过 ValueState.update(value) 来更新状态：
stream.keyBy(value -\u0026gt; value.f0).asQueryableState(\u0026#34;query-name\u0026#34;); 这个行为类似于 Scala API 中的 flatMapWithState。
Managed Keyed State # operator 中的 Managed keyed state (参考 Using Managed Keyed State) 可以通过 StateDescriptor.setQueryable(String queryableStateName) 将 state descriptor 设置成可查询的，从而使 state 可查询，如下所示：
ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information descriptor.setQueryable(\u0026#34;query-name\u0026#34;); // queryable state name 注意: 参数 queryableStateName 可以任意选取，并且只被用来进行查询，它可以和 state 的名称不同。 这种方式不会限制 state 类型，即任意的 ValueState、ReduceState、ListState、MapState、AggregatingState 以及已弃用的 FoldingState 均可作为 queryable state。
查询 state # 目前为止，你已经激活了集群的 queryable state 功能，并且将一些 state 设置成了可查询的，接下来将会展示如何进行查询。
为了进行查询，可以使用辅助类 QueryableStateClient，这个类位于 flink-queryable-state-client 的 jar 中，在项目的 pom.xml 需要显示添加对 flink-queryable-state-client 和 flink-core 的依赖, 如下所示：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-queryable-state-client-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 关于依赖的更多信息, 可以参考如何配置 Flink 项目。
QueryableStateClient 将提交你的请求到内部代理，代理会处理请求并返回结果。客户端的初始化只需要提供一个有效的 TaskManager 主机名 (每个 task manager 上都运行着一个 queryable state 代理)，以及代理监听的端口号。关于如何配置代理以及端口号可以参考 Configuration Section.
QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); 客户端就绪后，为了查询类型为 K 的 key，以及类型为 V 的state，可以使用如下方法：
CompletableFuture\u0026lt;S\u0026gt; getKvState( JobID jobId, String queryableStateName, K key, TypeInformation\u0026lt;K\u0026gt; keyTypeInfo, StateDescriptor\u0026lt;S, V\u0026gt; stateDescriptor) 该方法会返回一个最终将包含 state 的 queryable state 实例，该实例可通过 JobID 和 queryableStateName 识别。在方法参数中，key 用来指定所要查询的状态所属的 key。 keyTypeInfo 告诉 Flink 如何对 key 进行序列化和反序列化。stateDescriptor 包含了所请求 state 的必要信息，即 state 类型(Value，Reduce 等等)， 以及如何对其进行序列化和反序列。
细心的读者会注意到返回的 future 包含类型为 S 的值，即一个存储实际值的 State 对象。它可以是Flink支持的任何类型的 state：ValueState、ReduceState、 ListState、MapState、AggregatingState 以及弃用的 FoldingState。
注意： 这些 state 对象不允许对其中的 state 进行修改。你可以通过 valueState.get() 获取实际的 state， 或者通过 mapState.entries() 遍历所有 \u0026lt;K, V\u0026gt;，但是不能修改它们。举例来说，对返回的 list state 调用 add() 方法将会导致 UnsupportedOperationException。 注意: 客户端是异步的，并且可能被多个线程共享。客户端不再使用后需要通过 QueryableStateClient.shutdown() 来终止，从而释放资源。 示例 # 下面的例子扩展自 CountWindowAverage (参考 Using Managed Keyed State)， 将其中的 state 设置成可查询的，并展示了如何进行查询：
public class CountWindowAverage extends RichFlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { private transient ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; sum; // a tuple containing the count and the sum @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; input, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) throws Exception { Tuple2\u0026lt;Long, Long\u0026gt; currentSum = sum.value(); currentSum.f0 += 1; currentSum.f1 += input.f1; sum.update(currentSum); if (currentSum.f0 \u0026gt;= 2) { out.collect(new Tuple2\u0026lt;\u0026gt;(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, // the state name TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); // type information descriptor.setQueryable(\u0026#34;query-name\u0026#34;); sum = getRuntimeContext().getState(descriptor); } } 上面的代码作为作业运行后，可以获取作业的 ID，然后可以通过下面的方式查询任何 key 下的 state。
QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); // the state descriptor of the state to be fetched. ValueStateDescriptor\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; descriptor = new ValueStateDescriptor\u0026lt;\u0026gt;( \u0026#34;average\u0026#34;, TypeInformation.of(new TypeHint\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;() {})); CompletableFuture\u0026lt;ValueState\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;\u0026gt; resultFuture = client.getKvState(jobId, \u0026#34;query-name\u0026#34;, key, BasicTypeInfo.LONG_TYPE_INFO, descriptor); // now handle the returned value resultFuture.thenAccept(response -\u0026gt; { try { Tuple2\u0026lt;Long, Long\u0026gt; res = response.get(); } catch (Exception e) { e.printStackTrace(); } }); Configuration # 下面的配置会影响 queryable state 服务器端和客户端的行为，它们定义在 QueryableStateOptions。
State Server # queryable-state.server.ports: 服务器端口范围，如果同一台机器上运行了多个 task manager，可以避免端口冲突。指定的可以是一个具体的端口号，如 \u0026ldquo;9123\u0026rdquo;， 可以是一个端口范围，如 \u0026ldquo;50100-50200\u0026rdquo;，或者可以是端口范围以及端口号的组合，如 \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;。默认端口号是 9067。 queryable-state.server.network-threads: 服务器端 network (event loop) thread 的数量，用来接收查询请求 (如果设置为0，则线程数为 slot 数)。 queryable-state.server.query-threads: 服务器端处理查询请求的线程数 (如果设置为0，则线程数为 slot 数)。 Proxy # queryable-state.proxy.ports: 代理的服务端口范围。如果同一台机器上运行了多个 task manager，可以避免端口冲突。指定的可以是一个具体的端口号，如 \u0026ldquo;9123\u0026rdquo;， 可以是一个端口范围，如\u0026quot;50100-50200\u0026rdquo;，或者可以是端口范围以及端口号的组合，如 \u0026ldquo;50100-50200,50300-50400,51234\u0026rdquo;。默认端口号是 9069。 queryable-state.proxy.network-threads: 代理上 network (event loop) thread 的数量，用来接收查询请求 (如果设置为0，则线程数为 slot 数)。 queryable-state.proxy.query-threads: 代理上处理查询请求的线程数 (如果设置为0，则线程数为 slot 数)。 限制 # queryable state 的生命周期受限于作业的生命周期，比如 tasks 在启动时注册可查询状态，并在退出时注销。在后续版本中，希望能够将其解耦 从而允许 task 结束后依然能够查询 state，并且通过 state 备份来加速恢复。 目前是通过 tell 来通知可用的 KvState。将来会使用 asks 和 acknowledgements 来提升稳定性。 服务器端和客户端会记录请求的统计信息。因为统计信息目前不会暴露给外部，所以这个功能默认没有开启。如果将来支持通过 Metrics 系统发布这些数据，将开启统计功能。 Back to top
`}),e.add({id:146,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select-distinct/",title:"SELECT DISTINCT",section:"Queries 查询",content:` SELECT DISTINCT # Batch Streaming
如果使用”SELECT DISTINCT“查询,所有的复制行都会从结果集(每个分组只会保留一行)中被删除.
SELECT DISTINCT id FROM Orders 对于流式查询, 计算查询结果所需要的状态可能会源源不断地增长,而状态大小又依赖不同行的数量.此时,可以通过配置文件为状态设置合适的存活时间(TTL),以防止过大的状态可能对查询结果的正确性的影响.具体配置可参考:查询相关的配置.
Back to top
`}),e.add({id:147,href:"/flink/flink-docs-master/zh/docs/deployment/speculative_execution/",title:"Speculative Execution",section:"Deployment",content:` 预测执行 # 这个文档描述了预测执行的背景，使用方法，以及如何验证其有效性。
背景 # 预测执行是一种用于缓解异常机器节点导致作业执行缓慢的机制。机器节点异常包括硬件异常，偶发的输入输出繁忙，高 CPU 负载等问题。 这些问题会导致运行在其上的任务比起在其他节点上运行的任务慢很多，从而影响到整个作业的执行时长。
在这种情况下，预测执行会为这些慢任务创建一些新的执行实例并部署在正常的机器节点上。这些新的执行实例和其对应的老执行实例(慢任务) 会消费相同的数据，并产出相同的结果。而那些老执行实例也会被保留继续执行。这些执行实例(包括新实例和老实例)中首先成功结束的执行 实例会被认可，其产出的结果会对下游任务可见，其他实例则会被取消掉。
为了实现这个机制，Flink 会通过一个慢任务检测器来检测慢任务。检测到的慢任务位于的机器节点会被识别为异常机器节点，并被加入机器 节点黑名单中。调度器则会为这些慢节点创建新的执行实例，并将其部署到未被加黑的机器节点上。
使用方法 # 本章节描述了如何使用预测执行，包含如何启用，调优，以及开发/改进自定义 source 来支持预测执行。
注意: Flink 尚不支持 sink 的预测执行。这个能力会在后续版本中得到完善。 注意：Flink 不支持 DataSet 作业的预测执行，因为 DataSet API 在不久的将来会被废弃。现在推荐使用 DataStream API 来开发 Flink 批处理作业。 启用预测执行 # 要启用预测执行，你需要设置以下配置项：
jobmanager.scheduler: AdaptiveBatch 因为当前只有 Adaptive Batch Scheduler 支持预测执行. jobmanager.adaptive-batch-scheduler.speculative.enabled: true 配置调优 # 考虑到不同作业的差异，为了让预测执行获得更好的效果，你可以调优下列调度器配置项：
jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration 你还可以调优下列慢任务检测器的配置项：
slow-task-detector.check-interval slow-task-detector.execution-time.baseline-lower-bound slow-task-detector.execution-time.baseline-multiplier slow-task-detector.execution-time.baseline-ratio 让 Source 支持预测执行 # 如果你的作业有用到自定义 Source , 并且这个 Source 用到了自定义的 SourceEvent , 你需要修改该 Source 的 SplitEnumerator 实现接口 SupportsHandleExecutionAttemptSourceEvent 。
public interface SupportsHandleExecutionAttemptSourceEvent { void handleSourceEvent(int subtaskId, int attemptNumber, SourceEvent sourceEvent); } 这意味着 SplitEnumerator 需要知道是哪个执行实例发出了这个事件。否则，JobManager 会在收到 SourceEvent 的时候报错从而导致作业失败。
除此之外的 Source 不需要额外的改动就可以进行预测执行，包括 SourceFunction Source , InputFormat Source , 和 新版 Source . Apache Flink 官方提供的 Source 都支持预测执行。
检查预测执行的效果 # 在启用预测执行后，当出现慢任务触发预测执行时，Web UI 会在作业页面的节点信息的 SubTasks 分页展示预测执行实例。Web UI 还会在 Overview 和 Task Managers 页面展示当前被加黑的 TaskManager。
你还可以通过检查这些 指标 来判断预测执行的有效性。
Back to top
`}),e.add({id:148,href:"/flink/flink-docs-master/zh/docs/deployment/resource-providers/yarn/",title:"YARN",section:"Resource Providers",content:` Apache Hadoop YARN # Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on YARN.
Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN\u0026rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.
Preparation # This Getting Started section assumes a functional YARN environment, starting from version 2.8.5. YARN environments are provided most conveniently through services such as Amazon EMR, Google Cloud DataProc or products like Cloudera. Manually setting up a YARN environment locally or on a cluster is not recommended for following through this Getting Started tutorial.
Make sure your YARN cluster is ready for accepting Flink applications by running yarn top. It should show no error messages. Download a recent Flink distribution from the download page and unpack it. Important Make sure that the HADOOP_CLASSPATH environment variable is set up (it can be checked by running echo \$HADOOP_CLASSPATH). If not, set it up using export HADOOP_CLASSPATH=\`hadoop classpath\` Starting a Flink Session on YARN # Once you\u0026rsquo;ve made sure that the HADOOP_CLASSPATH environment variable is set, you can launch a Flink on YARN session, and submit an example job:
# we assume to be in the root directory of # the unzipped Flink distribution # (0) export HADOOP_CLASSPATH export HADOOP_CLASSPATH=\`hadoop classpath\` # (1) Start YARN Session ./bin/yarn-session.sh --detached # (2) You can now access the Flink Web Interface through the # URL printed in the last lines of the command output, or through # the YARN ResourceManager web UI. # (3) Submit example job ./bin/flink run ./examples/streaming/TopSpeedWindowing.jar # (4) Stop YARN session (replace the application id based # on the output of the yarn-session.sh command) echo \u0026#34;stop\u0026#34; | ./bin/yarn-session.sh -id application_XXXXX_XXX Congratulations! You have successfully run a Flink application by deploying Flink on YARN.
Back to top
Deployment Modes Supported by Flink on YARN # For production use, we recommend deploying Flink Applications in the Per-job or Application Mode, as these modes provide a better isolation for the Applications.
Application Mode # Application Mode will launch a Flink cluster on YARN, where the main() method of the application jar gets executed on the JobManager in YARN. The cluster will shut down as soon as the application has finished. You can manually stop the cluster using yarn application -kill \u0026lt;ApplicationId\u0026gt; or by cancelling the Flink job.
./bin/flink run-application -t yarn-application ./examples/streaming/TopSpeedWindowing.jar Once an Application Mode cluster is deployed, you can interact with it for operations like cancelling or taking a savepoint.
# List running job on the cluster ./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY # Cancel running job ./bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY \u0026lt;jobId\u0026gt; Note that cancelling your job on an Application Cluster will stop the cluster.
To unlock the full potential of the application mode, consider using it with the yarn.provided.lib.dirs configuration option and pre-upload your application jar to a location accessible by all nodes in your cluster. In this case, the command could look like:
./bin/flink run-application -t yarn-application \\ -Dyarn.provided.lib.dirs=\u0026#34;hdfs://myhdfs/my-remote-flink-dist-dir\u0026#34; \\ hdfs://myhdfs/jars/my-application.jar The above will allow the job submission to be extra lightweight as the needed Flink jars and the application jar are going to be picked up by the specified remote locations rather than be shipped to the cluster by the client.
Per-Job Cluster Mode # The Per-job Cluster mode will launch a Flink cluster on YARN, then run the provided application jar locally and finally submit the JobGraph to the JobManager on YARN. If you pass the --detached argument, the client will stop once the submission is accepted.
The YARN cluster will stop once the job has stopped.
./bin/flink run -t yarn-per-job --detached ./examples/streaming/TopSpeedWindowing.jar Once a Per-Job Cluster is deployed, you can interact with it for operations like cancelling or taking a savepoint.
# List running job on the cluster ./bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY # Cancel running job ./bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY \u0026lt;jobId\u0026gt; Note that cancelling your job on an Per-Job Cluster will stop the cluster.
Session Mode # We describe deployment with the Session Mode in the Getting Started guide at the top of the page.
The Session Mode has two operation modes:
attached mode (default): The yarn-session.sh client submits the Flink cluster to YARN, but the client keeps running, tracking the state of the cluster. If the cluster fails, the client will show the error. If the client gets terminated, it will signal the cluster to shut down as well. detached mode (-d or --detached): The yarn-session.sh client submits the Flink cluster to YARN, then the client returns. Another invocation of the client, or YARN tools is needed to stop the Flink cluster. The session mode will create a hidden YARN properties file in /tmp/.yarn-properties-\u0026lt;username\u0026gt;, which will be picked up for cluster discovery by the command line interface when submitting a job.
You can also manually specify the target YARN cluster in the command line interface when submitting a Flink job. Here\u0026rsquo;s an example:
./bin/flink run -t yarn-session \\ -Dyarn.application.id=application_XXXX_YY \\ ./examples/streaming/TopSpeedWindowing.jar You can re-attach to a YARN session using the following command:
./bin/yarn-session.sh -id application_XXXX_YY Besides passing configuration via the conf/flink-conf.yaml file, you can also pass any configuration at submission time to the ./bin/yarn-session.sh client using -Dkey=value arguments.
The YARN session client also has a few \u0026ldquo;shortcut arguments\u0026rdquo; for commonly used settings. They can be listed with ./bin/yarn-session.sh -h.
Back to top
Flink on YARN Reference # Configuring Flink on YARN # The YARN-specific configurations are listed on the configuration page.
The following configuration parameters are managed by Flink on YARN, as they might get overwritten by the framework at runtime:
jobmanager.rpc.address (dynamically set to the address of the JobManager container by Flink on YARN) io.tmp.dirs (If not set, Flink sets the temporary directories defined by YARN) high-availability.cluster-id (automatically generated ID to distinguish multiple clusters in the HA service) If you need to pass additional Hadoop configuration files to Flink, you can do so via the HADOOP_CONF_DIR environment variable, which accepts a directory name containing Hadoop configuration files. By default, all required Hadoop configuration files are loaded from the classpath via the HADOOP_CLASSPATH environment variable.
Resource Allocation Behavior # A JobManager running on YARN will request additional TaskManagers, if it can not run all submitted jobs with the existing resources. In particular when running in Session Mode, the JobManager will, if needed, allocate additional TaskManagers as additional jobs are submitted. Unused TaskManagers are freed up again after a timeout.
The memory configurations for JobManager and TaskManager processes will be respected by the YARN implementation. The number of reported VCores is by default equal to the number of configured slots per TaskManager. The yarn.containers.vcores allows overwriting the number of vcores with a custom value. In order for this parameter to work you should enable CPU scheduling in your YARN cluster.
Failed containers (including the JobManager) are replaced by YARN. The maximum number of JobManager container restarts is configured via yarn.application-attempts (default 1). The YARN Application will fail once all attempts are exhausted.
High-Availability on YARN # High-Availability on YARN is achieved through a combination of YARN and a high availability service.
Once a HA service is configured, it will persist JobManager metadata and perform leader elections.
YARN is taking care of restarting failed JobManagers. The maximum number of JobManager restarts is defined through two configuration parameters. First Flink\u0026rsquo;s yarn.application-attempts configuration will default 2. This value is limited by YARN\u0026rsquo;s yarn.resourcemanager.am.max-attempts, which also defaults to 2.
Note that Flink is managing the high-availability.cluster-id configuration parameter when deploying on YARN. Flink sets it per default to the YARN application id. You should not overwrite this parameter when deploying an HA cluster on YARN. The cluster ID is used to distinguish multiple HA clusters in the HA backend (for example Zookeeper). Overwriting this configuration parameter can lead to multiple YARN clusters affecting each other.
Container Shutdown Behaviour # YARN 2.3.0 \u0026lt; version \u0026lt; 2.4.0. All containers are restarted if the application master fails. YARN 2.4.0 \u0026lt; version \u0026lt; 2.6.0. TaskManager containers are kept alive across application master failures. This has the advantage that the startup time is faster and that the user does not have to wait for obtaining the container resources again. YARN 2.6.0 \u0026lt;= version: Sets the attempt failure validity interval to the Flinks\u0026rsquo; Akka timeout value. The attempt failure validity interval says that an application is only killed after the system has seen the maximum number of application attempts during one interval. This avoids that a long lasting job will deplete it\u0026rsquo;s application attempts. Hadoop YARN 2.4.0 has a major bug (fixed in 2.5.0) preventing container restarts from a restarted Application Master/Job Manager container. See FLINK-4142 for details. We recommend using at least Hadoop 2.5.0 for high availability setups on YARN. Supported Hadoop versions. # Flink on YARN is compiled against Hadoop 2.8.5, and all Hadoop versions \u0026gt;= 2.8.5 are supported, including Hadoop 3.x.
For providing Flink with the required Hadoop dependencies, we recommend setting the HADOOP_CLASSPATH environment variable already introduced in the Getting Started / Preparation section.
If that is not possible, the dependencies can also be put into the lib/ folder of Flink.
Flink also offers pre-bundled Hadoop fat jars for placing them in the lib/ folder, on the Downloads / Additional Components section of the website. These pre-bundled fat jars are shaded to avoid dependency conflicts with common libraries. The Flink community is not testing the YARN integration against these pre-bundled jars.
Running Flink on YARN behind Firewalls # Some YARN clusters use firewalls for controlling the network traffic between the cluster and the rest of the network. In those setups, Flink jobs can only be submitted to a YARN session from within the cluster\u0026rsquo;s network (behind the firewall). If this is not feasible for production use, Flink allows to configure a port range for its REST endpoint, used for the client-cluster communication. With this range configured, users can also submit jobs to Flink crossing the firewall.
The configuration parameter for specifying the REST endpoint port is rest.bind-port. This configuration option accepts single ports (for example: \u0026ldquo;50010\u0026rdquo;), ranges (\u0026ldquo;50000-50025\u0026rdquo;), or a combination of both.
User jars \u0026amp; Classpath # Session Mode
When deploying Flink with Session Mode on Yarn, only the JAR file specified in startup command will be recognized as user-jars and included into user classpath.
PerJob Mode \u0026amp; Application Mode
When deploying Flink with PerJob/Application Mode on Yarn, the JAR file specified in startup command and all JAR files in Flink\u0026rsquo;s usrlib folder will be recognized as user-jars. By default Flink will include the user-jars into the system classpath. This behavior can be controlled with the yarn.classpath.include-user-jar parameter.
When setting this to DISABLED Flink will include the jar in the user classpath instead.
The user-jars position in the classpath can be controlled by setting the parameter to one of the following:
ORDER: (default) Adds the jar to the system classpath based on the lexicographic order. FIRST: Adds the jar to the beginning of the system classpath. LAST: Adds the jar to the end of the system classpath. Please refer to the Debugging Classloading Docs for details.
Back to top
`}),e.add({id:149,href:"/flink/flink-docs-master/zh/docs/deployment/elastic_scaling/",title:"弹性扩缩容",section:"Deployment",content:` 弹性扩缩容 # 在 Apache Flink 中，可以通过手动停止 Job，然后从停止时创建的 Savepoint 恢复，最后重新指定并行度的方式来重新扩缩容 Job。
这个文档描述了那些可以使 Flink 自动调整并行度的选项。
Reactive 模式 # Reactive 模式是一个 MVP （minimum viable product，最小可行产品）特性。目前 Flink 社区正在积极地从邮件列表中获取用户的使用反馈。请注意文中列举的一些局限性。 在 Reactive 模式下，Job 会使用集群中所有的资源。当增加 TaskManager 时，Job 会自动扩容。当删除时，就会自动缩容。Flink 会管理 Job 的并行度，始终会尽可能地使用最大值。
当发生扩缩容时，Job 会被重启，并且会从最新的 Checkpoint 中恢复。这就意味着不需要花费额外的开销去创建 Savepoint。当然，所需要重新处理的数据量取决于 Checkpoint 的间隔时长，而恢复的时间取决于状态的大小。
借助 Reactive 模式，Flink 用户可以通过一些外部的监控服务产生的指标，例如：消费延迟、CPU 利用率汇总、吞吐量、延迟等，实现一个强大的自动扩缩容机制。当上述的这些指标超出或者低于一定的阈值时，增加或者减少 TaskManager 的数量。在 Kubernetes 中，可以通过改变 Deployment 的副本数（Replica Factor） 实现。而在 AWS 中，可以通过改变 Auto Scaling 组 来实现。这类外部服务只需要负责资源的分配以及回收，而 Flink 则负责在这些资源上运行 Job。
入门 # 你可以参考下面的步骤试用 Reactive 模式。以下步骤假设你使用的是单台机器部署 Flink。
# 以下步骤假设你当前目录处于 Flink 发行版的根目录。 # 将 Job 拷贝到 lib/ 目录下 cp ./examples/streaming/TopSpeedWindowing.jar lib/ # 使用 Reactive 模式提交 Job ./bin/standalone-job.sh start -Dscheduler-mode=reactive -Dexecution.checkpointing.interval=\u0026#34;10s\u0026#34; -j org.apache.flink.streaming.examples.windowing.TopSpeedWindowing # 启动第一个 TaskManager ./bin/taskmanager.sh start 让我们快速解释下上面每一条执行的命令：
./bin/standalone-job.sh start 使用 Application 模式 部署 Flink。 -Dscheduler-mode=reactive 启动 Reactive 模式。 -Dexecution.checkpointing.interval=\u0026quot;10s\u0026quot; 配置 Checkpoint 和重启策略。 最后一个参数是 Job 的主函数名。 你现在已经启动了一个 Reactive 模式下的 Flink Job。在Web 界面上，你可以看到 Job 运行在一个 TaskManager 上。如果你想要扩容，可以再添加一个 TaskManager，
# 额外启动一个 TaskManager ./bin/taskmanager.sh start 如果想要缩容，可以关掉一个 TaskManager。
# 关闭 TaskManager ./bin/taskmanager.sh stop 用法 # 配置 # 通过将 scheduler-mode 配置成 reactive，你可以开启 Reactive 模式。
每个独立算子的并行度都将由调度器来决定，而不是由配置决定。当并行度在算子上或者整个 Job 上被显式设置时，这些值被会忽略。
而唯一能影响并行度的方式只有通过设置算子的最大并行度（调度器不会忽略这个值）。 最大并行度 maxParallelism 参数的值最大不能超过 2^15（32768）。如果你们没有给算子或者整个 Job 设置最大并行度，会采用默认的最大并行度规则。 这个值很有可能会低于它的最大上限。当使用默认的调度模式时，请参考并行度的最佳实践。
需要注意的是，过大的并行度会影响 Job 的性能，因为 Flink 为此需要维护更多的内部结构。
当开启 Reactive 模式时，jobmanager.adaptive-scheduler.resource-wait-timeout 配置的默认值是 -1。这意味着，JobManager 会一直等待，直到拥有足够的资源。 如果你想要 JobManager 在没有拿到足够的 TaskManager 的一段时间后关闭，可以配置这个参数。
当开启 Reactive 模式时，jobmanager.adaptive-scheduler.resource-stabilization-timeout 配置的默认值是 0：Flink 只要有足够的资源，就会启动 Job。 在 TaskManager 一个一个而不是同时启动的情况下，会造成 Job 在每一个 TaskManager 启动时重启一次。当你希望等待资源稳定后再启动 Job，那么可以增加这个配置的值。 另外，你还可以配置 jobmanager.adaptive-scheduler.min-parallelism-increase：这个配置能够指定在扩容前需要满足的最小额外增加的并行总数。例如，你的 Job 由并行度为 2 的 Source 和并行度为 2 的 Sink组成，并行总数为 4。这个配置的默认值是 1，所以任意并行总数的增加都会导致重启。
建议 # 为有状态的 Job 配置周期性的 Checkpoint：Reactive 模式在扩缩容时通过最新完成的 Checkpoint 恢复。如果没有配置周期性的 Checkpoint，你的程序会丢失状态。Checkpoint 同时还配置了重启策略，Reactive会使用配置的重启策略：如果没有设置，Reactive 模式会让 Job 失败而不是运行扩缩容。
在 Ractive 模式下缩容可能会导致长时间的停顿，因为 Flink 需要等待 JobManager 和已经停止的 TaskManager 间心跳超时。当你降低 Job 并行度时，你会发现 Job 会停顿大约 50 秒左右。
这是由于默认的心跳超时时间是 50 秒。在你的基础设施允许的情况下，可以降低 heartbeat.timeout 的值。但是降低超时时间，会导致比如在网络拥堵或者 GC Pause 的时候，TaskManager 无法响应心跳。需要注意的是，heartbeat.interval 配置需要低于超时时间。
局限性 # 由于 Reactive 模式是一个新的实验特性，并不是所有在默认调度器下的功能都能支持（也包括 Adaptive 调度器）。Flink 社区正在解决这些局限性。
仅支持 Standalone 部署模式。其他主动的部署模式实现（例如：原生的 Kubernetes 以及 YARN）都明确不支持。Session 模式也同样不支持。仅支持单 Job 的部署。
仅支持如下的部署方式：Application 模式下的 Standalone 部署（可以参考上文）、Application 模式下的 Docker 部署 以及 Standalone 的 Kubernetes Application 集群模式。
Adaptive 调度器的局限性 同样也适用于 Reactive 模式.
Adaptive 调度器 # 只推荐高级用户直接使用 Adaptive 调度器（而不是通过 Reactive 模式使用），因为在一个 Session 集群中对于多个 Job 的 Slot 的分配行为是不确定的。 Adaptive 调度器可以基于现有的 Slot 调整 Job 的并行度。它会在 Slot 数目不足时，自动减少并行度。这种情况包括在提交时资源不够，或者在 Job 运行时 TaskManager 不可用。当有新的 Slot 加入时，Job 将会自动扩容至配置的并行度。 在 Reactive 模式下（详见上文），并行度配置会被忽略，即无限大，使得 Job 尽可能地使用资源。 你也可以不使用 Reactive 模式而仅使用 Adaptive 调度器，但这种情况会有如下的局限性：
如果你在 Session 集群上使用 Adaptive 调度器，在这个集群中运行的多个 Job，他们间 Slot 的分布是无法保证的。 相比默认的调度器，Adaptive 调度器其中一个优势在于，它能够优雅地处理 TaskManager 丢失所造成的问题，因为对它来说就仅仅是缩容。
用法 # 需要设置如下的配置参数：
jobmanager.scheduler: adaptive：将默认的调度器换成 Adaptive。 cluster.declarative-resource-management.enabled：声明式资源管理必须开启（默认开启）。 Adaptive 调度器可以通过所有在名字包含 adaptive-scheduler 的配置修改其行为。
局限性 # 只支持流式 Job：Adaptive 调度器的第一个版本仅支持流式 Job。当提交的是一个批处理 Job 时，我们会自动换回默认调度器。 不支持本地恢复：本地恢复是将 Task 调度到状态尽可能的被重用的机器上的功能。不支持这个功能意味着 Adaptive 调度器需要每次从 Checkpoint 的存储中下载整个 State。 不支持部分故障恢复: 部分故障恢复意味着调度器可以只重启失败 Job 其中某一部分（在 Flink 的内部结构中被称之为 Region）而不是重启整个 Job。这个限制只会影响那些独立并行（Embarrassingly Parallel）Job的恢复时长，默认的调度器可以重启失败的部分，然而 Adaptive 将需要重启整个 Job。 与 Flink Web UI 的集成受限: Adaptive 调度器会在 Job 的生命周期中改变它的并行度。Web UI 上只显示 Job 当前的并行度。 空闲 Slot: 如果 Slot 共享组的最大并行度不相等，提供给 Adaptive 调度器所使用的的 Slot 可能不会被使用。 扩缩容事件会触发 Job 和 Task 重启，Task 重试的次数也会增加。 Adaptive Batch Scheduler # Adaptive Batch Scheduler 是一种可以自动推导每个算子并行度的批作业调度器。如果算子未设置并行度，调度器将根据其消费的数据量的大小来推导其并行度。这可以带来诸多好处：
批作业用户可以从并行度调优中解脱出来 根据数据量自动推导并行度可以更好地适应每天变化的数据量 SQL作业中的算子也可以分配不同的并行度 用法 # 使用 Adaptive Batch Scheduler 自动推导算子的并行度，需要：
启用 Adaptive Batch Scheduler 配置算子的并行度为 -1 启用 Adaptive Batch Scheduler # 为了启用 Adaptive Batch Scheduler, 你需要：
配置 jobmanager.scheduler: AdaptiveBatch 由于 \u0026ldquo;只支持所有数据交换都为 BLOCKING 模式的作业\u0026rdquo;, 需要将 execution.batch-shuffle-mode 配置为 ALL-EXCHANGES-BLOCKING(默认值) 。 除此之外，使用 Adaptive Batch Scheduler 时，以下相关配置也可以调整:
jobmanager.adaptive-batch-scheduler.min-parallelism: 允许自动设置的并行度最小值。需要配置为 2 的幂，否则也会被自动调整为最接近且大于其的 2 的幂。 jobmanager.adaptive-batch-scheduler.max-parallelism: 允许自动设置的并行度最大值。需要配置为 2 的幂，否则也会被自动调整为最接近且小于其的 2 的幂。 jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task: 期望每个任务平均处理的数据量大小。由于顶点的并行度会被调整为 2^N，因此实际每个任务平均处理的数据量大小将是该值的 0.75~1.5 倍。 另外需要注意的是，当出现数据倾斜，或者确定的并行度达到最大并行度（由于数据过多）时，一些任务实际处理的数据可能会远远超过这个值。 jobmanager.adaptive-batch-scheduler.default-source-parallelism: source 算子的默认并行度 配置算子的并行度为 -1 # Adaptive Batch Scheduler 只会为用户未指定并行度的算子（并行度为 -1）推导并行度。 所以如果你想自动推导算子的并行度，需要进行以下配置：
配置 parallelism.default: -1 对于 SQL 作业，需要配置 table.exec.resource.default-parallelism: -1 对于 DataStream/DataSet 作业，不要在作业中通过算子的 setParallelism() 方法来指定并行度 对于 DataStream/DataSet 作业，不要在作业中通过 StreamExecutionEnvironment/ExecutionEnvironment 的 setParallelism() 方法来指定并行度 性能调优 # 建议使用 Sort Shuffle 并且设置 taskmanager.network.memory.buffers-per-channel 为 0。 这会解耦并行度与需要的网络内存，对于大规模作业，这样可以降低遇到 \u0026ldquo;Insufficient number of network buffers\u0026rdquo; 错误的可能性。 建议将 jobmanager.adaptive-batch-scheduler.max-parallelism 设置为最坏情况下预期需要的并行度。不建议配置太大的值，否则可能会影响性能。这个配置项会影响上游任务产出的 subpartition 的数量，过多的 subpartition 可能会影响 hash shuffle 的性能，或者由于小包影响网络传输的性能。 局限性 # 只支持批作业: Adaptive Batch Scheduler 只支持批作业。当提交的是一个流作业时，会抛出异常。 只支持所有数据交换都为 BLOCKING 模式的作业: 目前 Adaptive Batch Scheduler 只支持 shuffle mode 为 ALL-EXCHANGES-BLOCKING 的作业。 推导出的并行度是 2 的幂: 为了使子分区可以均匀分配给下游任务，jobmanager.adaptive-batch-scheduler.max-parallelism 应该被配置为 2^N, 推导出的并行度会是 2^M, 且满足 M \u0026lt;= N。 不支持 FileInputFormat 类型的 source: 不支持 FileInputFormat 类型的 source, 包括 StreamExecutionEnvironment#readFile(...) StreamExecutionEnvironment#readTextFile(...) 和 StreamExecutionEnvironment#createInput(FileInputFormat, ...)。 当使用 Adaptive Batch Scheduler 时，用户应该使用新版的 Source API (FileSystem DataStream Connector 或 FileSystem SQL Connector) 来读取文件. Web UI 上展示的上游输出的数据量和下游收到的数据量可能不一致: 在使用 Adaptive Batch Scheduler 时，对于 broadcast 边，上游算子发送的数据量和下游算子接收的数据量可能会不相等，这在 Web UI 的显示上可能会困扰用户。细节详见 FLIP-187。 Back to top
`}),e.add({id:150,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_tuning/",title:"调优指南",section:"内存配置",content:` 调优指南 # 本文在的基本的配置指南的基础上，介绍如何根据具体的使用场景调整内存配置，以及在不同使用场景下分别需要重点关注哪些配置参数。
独立部署模式（Standalone Deployment）下的内存配置 # 独立部署模式下，我们通常更关注 Flink 应用本身使用的内存大小。 建议配置 Flink 总内存（taskmanager.memory.flink.size 或者 jobmanager.memory.flink.size）或其组成部分。 此外，如果出现 Metaspace 不足的问题，可以调整 JVM Metaspace 的大小。
这种情况下通常无需配置进程总内存，因为不管是 Flink 还是部署环境都不会对 JVM 开销 进行限制，它只与机器的物理资源相关。
容器（Container）的内存配置 # 在容器化部署模式（Containerized Deployment）下（Kubernetes 或 Yarn），建议配置进程总内存（taskmanager.memory.process.size 或者 jobmanager.memory.process.size）。 该配置参数用于指定分配给 Flink JVM 进程的总内存，也就是需要申请的容器大小。
提示 如果配置了 Flink 总内存，Flink 会自动加上 JVM 相关的内存部分，根据推算出的进程总内存大小申请容器。
注意： 如果 Flink 或者用户代码分配超过容器大小的非托管的堆外（本地）内存，部署环境可能会杀掉超用内存的容器，造成作业执行失败。 请参考容器内存超用中的相关描述。
State Backend 的内存配置 # 本章节内容仅与 TaskManager 相关。
在部署 Flink 流处理应用时，可以根据 State Backend 的类型对集群的配置进行优化。
Heap State Backend # 执行无状态作业或者使用 Heap State Backend（MemoryStateBackend 或 FsStateBackend）时，建议将托管内存设置为 0。 这样能够最大化分配给 JVM 上用户代码的内存。
RocksDB State Backend # RocksDBStateBackend 使用本地内存。 默认情况下，RocksDB 会限制其内存用量不超过用户配置的托管内存。 因此，使用这种方式存储状态时，配置足够多的托管内存是十分重要的。 如果你关闭了 RocksDB 的内存控制，那么在容器化部署模式下如果 RocksDB 分配的内存超出了申请容器的大小（进程总内存），可能会造成 TaskExecutor 被部署环境杀掉。 请同时参考如何调整 RocksDB 内存以及 state.backend.rocksdb.memory.managed。
批处理作业的内存配置 # Flink 批处理算子使用托管内存来提高处理效率。 算子运行时，部分操作可以直接在原始数据上进行，而无需将数据反序列化成 Java 对象。 这意味着托管内存对应用的性能具有实质上的影响。 因此 Flink 会在不超过其配置限额的前提下，尽可能分配更多的托管内存。 Flink 明确知道可以使用的内存大小，因此可以有效避免 OutOfMemoryError 的发生。 当托管内存不足时，Flink 会优雅地将数据落盘。
`}),e.add({id:151,href:"/flink/flink-docs-master/zh/docs/dev/configuration/connector/",title:"连接器和格式",section:"项目配置",content:` 连接器和格式 # Flink 应用程序可以通过连接器读取和写入各种外部系统。它支持多种格式，以便对数据进行编码和解码以匹配 Flink 的数据结构。
DataStream 和 Table API/SQL 都提供了连接器和格式的概述。
可用的组件 # 为了使用连接器和格式，您需要确保 Flink 可以访问实现了这些功能的组件。对于 Flink 社区支持的每个连接器，我们在 Maven Central 发布了两类组件：
flink-connector-\u0026lt;NAME\u0026gt; 这是一个精简 JAR，仅包括连接器代码，但不包括最终的第三方依赖项； flink-sql-connector-\u0026lt;NAME\u0026gt; 这是一个包含连接器第三方依赖项的 uber JAR； 这同样适用于格式。请注意，某些连接器可能没有相应的 flink-sql-connector-\u0026lt;NAME\u0026gt; 组件，因为它们不需要第三方依赖项。
uber/fat JAR 主要与SQL 客户端一起使用，但您也可以在任何 DataStream/Table 应用程序中使用它们。 使用组件 # 为了使用连接器/格式模块，您可以：
把精简 JAR 及其传递依赖项打包进您的作业 JAR； 把 uber JAR 打包进您的作业 JAR； 把 uber JAR 直接复制到 Flink 发行版的 /lib 文件夹内； 关于打包依赖项，请查看 Maven 和 Gradle 指南。有关 Flink 发行版的参考，请查看Flink 依赖剖析。
决定是打成 uber JAR、精简 JAR 还是仅在发行版包含依赖项取决于您和您的使用场景。如果您使用 uber JAR，您将对作业里的依赖项版本有更多的控制权；如果您使用精简 JAR，由于您可以在不更改连接器版本的情况下更改版本（允许二进制兼容），您将对传递依赖项有更多的控制权；如果您直接在 Flink 发行版的 /lib 目录里内嵌连接器 uber JAR，您将能够在一处控制所有作业的连接器版本。 `}),e.add({id:152,href:"/flink/flink-docs-master/zh/docs/learn-flink/streaming_analytics/",title:"流式分析",section:"实践练习",content:` 流式分析 # Event Time and Watermarks # 概要 # Flink 明确支持以下三种时间语义:
事件时间(event time)： 事件产生的时间，记录的是设备生产(或者存储)事件的时间
摄取时间(ingestion time)： Flink 读取事件时记录的时间
处理时间(processing time)： Flink pipeline 中具体算子处理事件的时间
为了获得可重现的结果，例如在计算过去的特定一天里第一个小时股票的最高价格时，我们应该使用事件时间。这样的话，无论什么时间去计算都不会影响输出结果。然而如果使用处理时间的话，实时应用程序的结果是由程序运行的时间所决定。多次运行基于处理时间的实时程序，可能得到的结果都不相同，也可能会导致再次分析历史数据或者测试新代码变得异常困难。
使用 Event Time # 如果想要使用事件时间，需要额外给 Flink 提供一个时间戳提取器和 Watermark 生成器，Flink 将使用它们来跟踪事件时间的进度。这将在选节使用 Watermarks 中介绍，但是首先我们需要解释一下 watermarks 是什么。
Watermarks # 让我们通过一个简单的示例来演示为什么需要 watermarks 及其工作方式。
在此示例中，我们将看到带有混乱时间戳的事件流，如下所示。显示的数字表达的是这些事件实际发生时间的时间戳。到达的第一个事件发生在时间 4，随后发生的事件发生在更早的时间 2，依此类推：
··· 23 19 22 24 21 14 17 13 12 15 9 11 7 2 4 → 假设我们要对数据流排序，我们想要达到的目的是：应用程序应该在数据流里的事件到达时就有一个算子（我们暂且称之为排序）开始处理事件，这个算子所输出的流是按照时间戳排序好的。
让我们重新审视这些数据:
(1) 我们的排序器看到的第一个事件的时间戳是 4，但是我们不能立即将其作为已排序的流释放。因为我们并不能确定它是有序的，并且较早的事件有可能并未到达。事实上，如果站在上帝视角，我们知道，必须要等到时间戳为 2 的元素到来时，排序器才可以有事件输出。
需要一些缓冲，需要一些时间，但这都是值得的
(2) 接下来的这一步，如果我们选择的是固执的等待，我们永远不会有结果。首先，我们看到了时间戳为 4 的事件，然后看到了时间戳为 2 的事件。可是，时间戳小于 2 的事件接下来会不会到来呢？可能会，也可能不会。再次站在上帝视角，我们知道，我们永远不会看到时间戳 1。
最终，我们必须勇于承担责任，并发出指令，把带有时间戳 2 的事件作为已排序的事件流的开始
(3)然后，我们需要一种策略，该策略定义：对于任何给定时间戳的事件，Flink 何时停止等待较早事件的到来。
这正是 watermarks 的作用 — 它们定义何时停止等待较早的事件。
Flink 中事件时间的处理取决于 watermark 生成器，后者将带有时间戳的特殊元素插入流中形成 watermarks。事件时间 t 的 watermark 代表 t 之前（很可能）都已经到达。
当 watermark 以 2 或更大的时间戳到达时，事件流的排序器应停止等待，并输出 2 作为已经排序好的流。
(4) 我们可能会思考，如何决定 watermarks 的不同生成策略
每个事件都会延迟一段时间后到达，然而这些延迟有所不同，有些事件可能比其他事件延迟得更多。一种简单的方法是假定这些延迟受某个最大延迟的限制。Flink 将此策略称为 最大无序边界 (bounded-out-of-orderness) watermark。当然，我们可以想像出更好的生成 watermark 的方法，但是对于大多数应用而言，固定延迟策略已经足够了。
延迟 VS 正确性 # watermarks 给了开发者流处理的一种选择，它们使开发人员在开发应用程序时可以控制延迟和完整性之间的权衡。与批处理不同，批处理中的奢侈之处在于可以在产生任何结果之前完全了解输入，而使用流式传输，我们不被允许等待所有的时间都产生了，才输出排序好的数据，这与流相违背。
我们可以把 watermarks 的边界时间配置的相对较短，从而冒着在输入了解不完全的情况下产生结果的风险-即可能会很快产生错误结果。或者，你可以等待更长的时间，并利用对输入流的更全面的了解来产生结果。
当然也可以实施混合解决方案，先快速产生初步结果，然后在处理其他（最新）数据时向这些结果提供更新。对于有一些对延迟的容忍程度很低，但是又对结果有很严格的要求的场景下，或许是一个福音。
延迟 # 延迟是相对于 watermarks 定义的。Watermark(t) 表示事件流的时间已经到达了 t; watermark 之后的时间戳 ≤ t 的任何事件都被称之为延迟事件。
使用 Watermarks # 如果想要使用基于带有事件时间戳的事件流，Flink 需要知道与每个事件相关的时间戳，而且流必须包含 watermark。
动手练习中使用的出租车数据源已经为我们处理了这些详细信息。但是，在您自己的应用程序中，您将必须自己进行处理，这通常是通过实现一个类来实现的，该类从事件中提取时间戳，并根据需要生成 watermarks。最简单的方法是使用 WatermarkStrategy：
DataStream\u0026lt;Event\u0026gt; stream = ...; WatermarkStrategy\u0026lt;Event\u0026gt; strategy = WatermarkStrategy .\u0026lt;Event\u0026gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -\u0026gt; event.timestamp); DataStream\u0026lt;Event\u0026gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(strategy); Back to top
Windows # Flink 在窗口的场景处理上非常有表现力。
在本节中，我们将学习：
如何使用窗口来计算无界流上的聚合, Flink 支持哪种类型的窗口，以及 如何使用窗口聚合来实现 DataStream 程序 概要 # 我们在操作无界数据流时，经常需要应对以下问题，我们经常把无界数据流分解成有界数据流聚合分析:
每分钟的浏览量 每位用户每周的会话数 每个传感器每分钟的最高温度 用 Flink 计算窗口分析取决于两个主要的抽象操作：Window Assigners，将事件分配给窗口（根据需要创建新的窗口对象），以及 Window Functions，处理窗口内的数据。
Flink 的窗口 API 还具有 Triggers 和 Evictors 的概念，Triggers 确定何时调用窗口函数，而 Evictors 则可以删除在窗口中收集的元素。
举一个简单的例子，我们一般这样使用键控事件流（基于 key 分组的输入事件流）：
stream .keyBy(\u0026lt;key selector\u0026gt;) .window(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;); 您不是必须使用键控事件流（keyed stream），但是值得注意的是，如果不使用键控事件流，我们的程序就不能 并行 处理。
stream .windowAll(\u0026lt;window assigner\u0026gt;) .reduce|aggregate|process(\u0026lt;window function\u0026gt;); 窗口分配器 # Flink 有一些内置的窗口分配器，如下所示：
通过一些示例来展示关于这些窗口如何使用，或者如何区分它们：
滚动时间窗口 每分钟页面浏览量 TumblingEventTimeWindows.of(Time.minutes(1)) 滑动时间窗口 每10秒钟计算前1分钟的页面浏览量 SlidingEventTimeWindows.of(Time.minutes(1), Time.seconds(10)) 会话窗口 每个会话的网页浏览量，其中会话之间的间隔至少为30分钟 EventTimeSessionWindows.withGap(Time.minutes(30)) 以下都是一些可以使用的间隔时间 Time.milliseconds(n), Time.seconds(n), Time.minutes(n), Time.hours(n), 和 Time.days(n)。
基于时间的窗口分配器（包括会话时间）既可以处理 事件时间，也可以处理 处理时间。这两种基于时间的处理没有哪一个更好，我们必须折衷。使用 处理时间，我们必须接受以下限制：
无法正确处理历史数据, 无法正确处理超过最大无序边界的数据, 结果将是不确定的, 但是有自己的优势，较低的延迟。
使用基于计数的窗口时，请记住，只有窗口内的事件数量到达窗口要求的数值时，这些窗口才会触发计算。尽管可以使用自定义触发器自己实现该行为，但无法应对超时和处理部分窗口。
我们可能在有些场景下，想使用全局 window assigner 将每个事件（相同的 key）都分配给某一个指定的全局窗口。 很多情况下，一个比较好的建议是使用 ProcessFunction，具体介绍在这里。
窗口应用函数 # 我们有三种最基本的操作窗口内的事件的选项:
像批量处理，ProcessWindowFunction 会缓存 Iterable 和窗口内容，供接下来全量计算； 或者像流处理，每一次有事件被分配到窗口时，都会调用 ReduceFunction 或者 AggregateFunction 来增量计算； 或者结合两者，通过 ReduceFunction 或者 AggregateFunction 预聚合的增量计算结果在触发窗口时， 提供给 ProcessWindowFunction 做全量计算。 接下来展示一段 1 和 3 的示例，每一个实现都是计算传感器的最大值。在每一个一分钟大小的事件时间窗口内, 生成一个包含 (key,end-of-window-timestamp, max_value) 的一组结果。
ProcessWindowFunction 示例 # DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .process(new MyWastefulMax()); public static class MyWastefulMax extends ProcessWindowFunction\u0026lt; SensorReading, // 输入类型 Tuple3\u0026lt;String, Long, Integer\u0026gt;, // 输出类型 String, // 键类型 TimeWindow\u0026gt; { // 窗口类型 @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; events, Collector\u0026lt;Tuple3\u0026lt;String, Long, Integer\u0026gt;\u0026gt; out) { int max = 0; for (SensorReading event : events) { max = Math.max(event.value, max); } out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } 在当前实现中有一些值得关注的地方：
Flink 会缓存所有分配给窗口的事件流，直到触发窗口为止。这个操作可能是相当昂贵的。 Flink 会传递给 ProcessWindowFunction 一个 Context 对象，这个对象内包含了一些窗口信息。Context 接口 展示大致如下: public abstract class Context implements java.io.Serializable { public abstract W window(); public abstract long currentProcessingTime(); public abstract long currentWatermark(); public abstract KeyedStateStore windowState(); public abstract KeyedStateStore globalState(); } windowState 和 globalState 可以用来存储当前的窗口的 key、窗口或者当前 key 的每一个窗口信息。这在一些场景下会很有用，试想，我们在处理当前窗口的时候，可能会用到上一个窗口的信息。
增量聚合示例 # DataStream\u0026lt;SensorReading\u0026gt; input = ...; input .keyBy(x -\u0026gt; x.key) .window(TumblingEventTimeWindows.of(Time.minutes(1))) .reduce(new MyReducingMax(), new MyWindowFunction()); private static class MyReducingMax implements ReduceFunction\u0026lt;SensorReading\u0026gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() \u0026gt; r2.value() ? r1 : r2; } } private static class MyWindowFunction extends ProcessWindowFunction\u0026lt; SensorReading, Tuple3\u0026lt;String, Long, SensorReading\u0026gt;, String, TimeWindow\u0026gt; { @Override public void process( String key, Context context, Iterable\u0026lt;SensorReading\u0026gt; maxReading, Collector\u0026lt;Tuple3\u0026lt;String, Long, SensorReading\u0026gt;\u0026gt; out) { SensorReading max = maxReading.iterator().next(); out.collect(Tuple3.of(key, context.window().getEnd(), max)); } } 请注意 Iterable\u0026lt;SensorReading\u0026gt; 将只包含一个读数 \u0026ndash; MyReducingMax 计算出的预先汇总的最大值。
晚到的事件 # 默认场景下，超过最大无序边界的事件会被删除，但是 Flink 给了我们两个选择去控制这些事件。
您可以使用一种称为旁路输出 的机制来安排将要删除的事件收集到侧输出流中，这里是一个示例:
OutputTag\u0026lt;Event\u0026gt; lateTag = new OutputTag\u0026lt;Event\u0026gt;(\u0026#34;late\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Event\u0026gt; result = stream .keyBy(...) .window(...) .sideOutputLateData(lateTag) .process(...); DataStream\u0026lt;Event\u0026gt; lateStream = result.getSideOutput(lateTag); 我们还可以指定 允许的延迟(allowed lateness) 的间隔，在这个间隔时间内，延迟的事件将会继续分配给窗口（同时状态会被保留），默认状态下，每个延迟事件都会导致窗口函数被再次调用（有时也称之为 late firing ）。
默认情况下，允许的延迟为 0。换句话说，watermark 之后的元素将被丢弃（或发送到侧输出流）。
举例说明:
stream .keyBy(...) .window(...) .allowedLateness(Time.seconds(10)) .process(...); 当允许的延迟大于零时，只有那些超过最大无序边界以至于会被丢弃的事件才会被发送到侧输出流（如果已配置）。
深入了解窗口操作 # Flink 的窗口 API 某些方面有一些奇怪的行为，可能和我们预期的行为不一致。 根据 Flink 用户邮件列表 和其他地方一些频繁被问起的问题, 以下是一些有关 Windows 的底层事实，这些信息可能会让您感到惊讶。
滑动窗口是通过复制来实现的 # 滑动窗口分配器可以创建许多窗口对象，并将每个事件复制到每个相关的窗口中。例如，如果您每隔 15 分钟就有 24 小时的滑动窗口，则每个事件将被复制到 4 * 24 = 96 个窗口中。
时间窗口会和时间对齐 # 仅仅因为我们使用的是一个小时的处理时间窗口并在 12:05 开始运行您的应用程序，并不意味着第一个窗口将在 1:05 关闭。第一个窗口将长 55 分钟，并在 1:00 关闭。
请注意，滑动窗口和滚动窗口分配器所采用的 offset 参数可用于改变窗口的对齐方式。有关详细的信息，请参见 滚动窗口 和 滑动窗口 。
window 后面可以接 window # 比如说:
stream .keyBy(t -\u0026gt; t.key) .window(\u0026lt;window assigner\u0026gt;) .reduce(\u0026lt;reduce function\u0026gt;) .windowAll(\u0026lt;same window assigner\u0026gt;) .reduce(\u0026lt;same reduce function\u0026gt;); 可能我们会猜测以 Flink 的能力，想要做到这样看起来是可行的（前提是你使用的是 ReduceFunction 或 AggregateFunction ），但不是。
之所以可行，是因为时间窗口产生的事件是根据窗口结束时的时间分配时间戳的。例如，一个小时小时的窗口所产生的所有事件都将带有标记一个小时结束的时间戳。后面的窗口内的数据消费和前面的流产生的数据是一致的。
空的时间窗口不会输出结果 # 事件会触发窗口的创建。换句话说，如果在特定的窗口内没有事件，就不会有窗口，就不会有输出结果。
Late Events Can Cause Late Merges # 会话窗口的实现是基于窗口的一个抽象能力，窗口可以 聚合。会话窗口中的每个数据在初始被消费时，都会被分配一个新的窗口，但是如果窗口之间的间隔足够小，多个窗口就会被聚合。延迟事件可以弥合两个先前分开的会话间隔，从而产生一个虽然有延迟但是更加准确地结果。
Back to top
实践练习 # 本节附带的动手练习是 Hourly Tips Exercise .
Back to top
延伸阅读 # Timely Stream Processing Windows Back to top
`}),e.add({id:153,href:"/flink/flink-docs-master/zh/docs/deployment/cli/",title:"命令行界面",section:"Deployment",content:` 命令行界面 # Flink provides a Command-Line Interface (CLI) bin/flink to run programs that are packaged as JAR files and to control their execution. The CLI is part of any Flink setup, available in local single node setups and in distributed setups. It connects to the running JobManager specified in conf/flink-conf.yaml.
Job Lifecycle Management # A prerequisite for the commands listed in this section to work is to have a running Flink deployment like Kubernetes, YARN or any other option available. Feel free to start a Flink cluster locally to try the commands on your own machine.
Submitting a Job # Submitting a job means uploading the job\u0026rsquo;s JAR and related dependencies to the Flink cluster and initiating the job execution. For the sake of this example, we select a long-running job like examples/streaming/StateMachineExample.jar. Feel free to select any other JAR archive from the examples/ folder or deploy your own job.
\$ ./bin/flink run \\ --detached \\ ./examples/streaming/StateMachineExample.jar Submitting the job using --detached will make the command return after the submission is done. The output contains (besides other things) the ID of the newly submitted job.
Usage with built-in data generator: StateMachineExample [--error-rate \u0026lt;probability-of-invalid-transition\u0026gt;] [--sleep \u0026lt;sleep-per-record-in-ms\u0026gt;] Usage with Kafka: StateMachineExample --kafka-topic \u0026lt;topic\u0026gt; [--brokers \u0026lt;brokers\u0026gt;] Options for both the above setups: [--backend \u0026lt;file|rocks\u0026gt;] [--checkpoint-dir \u0026lt;filepath\u0026gt;] [--async-checkpoints \u0026lt;true|false\u0026gt;] [--incremental-checkpoints \u0026lt;true|false\u0026gt;] [--output \u0026lt;filepath\u0026gt; OR null for stdout] Using standalone source with error rate 0.000000 and sleep delay 1 millis Job has been submitted with JobID cca7bc1061d61cf15238e92312c2fc20 The usage information printed lists job-related parameters that can be added to the end of the job submission command if necessary. For the purpose of readability, we assume that the returned JobID is stored in a variable JOB_ID for the commands below:
\$ export JOB_ID=\u0026#34;cca7bc1061d61cf15238e92312c2fc20\u0026#34; There is another action called run-application available to run the job in Application Mode. This documentation does not address this action individually as it works similarly to the run action in terms of the CLI frontend.
The run and run-application commands support passing additional configuration parameters via the -D argument. For example setting the maximum parallelism for a job can be done by setting -Dpipeline.max-parallelism=120. This argument is very useful for configuring per-job or application mode clusters, because you can pass any configuration parameter to the cluster, without changing the configuration file.
When submitting a job to an existing session cluster, only execution configuration parameters are supported.
Job Monitoring # You can monitor any running jobs using the list action:
\$ ./bin/flink list Waiting for response... ------------------ Running/Restarting Jobs ------------------- 30.11.2020 16:02:29 : cca7bc1061d61cf15238e92312c2fc20 : State machine job (RUNNING) -------------------------------------------------------------- No scheduled jobs. Jobs that were submitted but not started, yet, would be listed under \u0026ldquo;Scheduled Jobs\u0026rdquo;.
Creating a Savepoint # Savepoints can be created to save the current state a job is in. All that\u0026rsquo;s needed is the JobID:
\$ ./bin/flink savepoint \\ \$JOB_ID \\ /tmp/flink-savepoints Triggering savepoint for job cca7bc1061d61cf15238e92312c2fc20. Waiting for response... Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab You can resume your program from this savepoint with the run command. The savepoint folder is optional and needs to be specified if state.savepoints.dir isn\u0026rsquo;t set.
Lastly, you can optionally provide what should be the binary format of the savepoint.
The path to the savepoint can be used later on to restart the Flink job.
Disposing a Savepoint # The savepoint action can be also used to remove savepoints. --dispose with the corresponding savepoint path needs to be added:
\$ ./bin/flink savepoint \\ --dispose \\ /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ \$JOB_ID Disposing savepoint \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;. Waiting for response... Savepoint \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39; disposed. If you use custom state instances (for example custom reducing state or RocksDB state), you have to specify the path to the program JAR with which the savepoint was triggered. Otherwise, you will run into a ClassNotFoundException:
\$ ./bin/flink savepoint \\ --dispose \u0026lt;savepointPath\u0026gt; \\ --jarfile \u0026lt;jarFile\u0026gt; Triggering the savepoint disposal through the savepoint action does not only remove the data from the storage but makes Flink clean up the savepoint-related metadata as well.
Terminating a Job # Stopping a Job Gracefully Creating a Final Savepoint # Another action for stopping a job is stop. It is a more graceful way of stopping a running streaming job as the stop flows from source to sink. When the user requests to stop a job, all sources will be requested to send the last checkpoint barrier that will trigger a savepoint, and after the successful completion of that savepoint, they will finish by calling their cancel() method.
\$ ./bin/flink stop \\ --savepointPath /tmp/flink-savepoints \\ \$JOB_ID Suspending job \u0026#34;cca7bc1061d61cf15238e92312c2fc20\u0026#34; with a savepoint. Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab We have to use --savepointPath to specify the savepoint folder if state.savepoints.dir isn\u0026rsquo;t set.
If the --drain flag is specified, then a MAX_WATERMARK will be emitted before the last checkpoint barrier. This will make all registered event-time timers fire, thus flushing out any state that is waiting for a specific watermark, e.g. windows. The job will keep running until all sources properly shut down. This allows the job to finish processing all in-flight data, which can produce some records to process after the savepoint taken while stopping.
Use the --drain flag if you want to terminate the job permanently. If you want to resume the job at a later point in time, then do not drain the pipeline because it could lead to incorrect results when the job is resumed. Lastly, you can optionally provide what should be the binary format of the savepoint.
Cancelling a Job Ungracefully # Cancelling a job can be achieved through the cancel action:
\$ ./bin/flink cancel \$JOB_ID Cancelling job cca7bc1061d61cf15238e92312c2fc20. Cancelled job cca7bc1061d61cf15238e92312c2fc20. The corresponding job\u0026rsquo;s state will be transitioned from Running to Cancelled. Any computations will be stopped.
The --withSavepoint flag allows creating a savepoint as part of the job cancellation. This feature is deprecated. Use the stop action instead. Starting a Job from a Savepoint # Starting a job from a savepoint can be achieved using the run (and run-application) action.
\$ ./bin/flink run \\ --detached \\ --fromSavepoint /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab \\ ./examples/streaming/StateMachineExample.jar Usage with built-in data generator: StateMachineExample [--error-rate \u0026lt;probability-of-invalid-transition\u0026gt;] [--sleep \u0026lt;sleep-per-record-in-ms\u0026gt;] Usage with Kafka: StateMachineExample --kafka-topic \u0026lt;topic\u0026gt; [--brokers \u0026lt;brokers\u0026gt;] Options for both the above setups: [--backend \u0026lt;file|rocks\u0026gt;] [--checkpoint-dir \u0026lt;filepath\u0026gt;] [--async-checkpoints \u0026lt;true|false\u0026gt;] [--incremental-checkpoints \u0026lt;true|false\u0026gt;] [--output \u0026lt;filepath\u0026gt; OR null for stdout] Using standalone source with error rate 0.000000 and sleep delay 1 millis Job has been submitted with JobID 97b20a0a8ffd5c1d656328b0cd6436a6 See how the command is equal to the initial run command except for the --fromSavepoint parameter which is used to refer to the state of the previously stopped job. A new JobID is generated that can be used to maintain the job.
By default, we try to match the whole savepoint state to the job being submitted. If you want to allow to skip savepoint state that cannot be restored with the new job you can set the --allowNonRestoredState flag. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered and you still want to use the savepoint.
\$ ./bin/flink run \\ --fromSavepoint \u0026lt;savepointPath\u0026gt; \\ --allowNonRestoredState ... This is useful if your program dropped an operator that was part of the savepoint.
You can also select the restore mode which should be used for the savepoint. The mode controls who takes ownership of the files of the specified savepoint.
Back to top
CLI Actions # Here\u0026rsquo;s an overview of actions supported by Flink\u0026rsquo;s CLI tool:
Action Purpose run This action executes jobs. It requires at least the jar containing the job. Flink- or job-related arguments can be passed if necessary. run-application This action executes jobs in Application Mode. Other than that, it requires the same parameters as the run action. info This action can be used to print an optimized execution graph of the passed job. Again, the jar containing the job needs to be passed. list This action lists all running or scheduled jobs. savepoint This action can be used to create or disposing savepoints for a given job. It might be necessary to specify a savepoint directory besides the JobID, if the state.savepoints.dir parameter was not specified in conf/flink-conf.yaml. cancel This action can be used to cancel running jobs based on their JobID. stop This action combines the cancel and savepoint actions to stop a running job but also create a savepoint to start from again. A more fine-grained description of all actions and their parameters can be accessed through bin/flink --help or the usage information of each individual action bin/flink \u0026lt;action\u0026gt; --help.
Back to top
Advanced CLI # REST API # The Flink cluster can be also managed using the REST API. The commands described in previous sections are a subset of what is offered by Flink\u0026rsquo;s REST endpoints. Therefore, tools like curl can be used to get even more out of Flink.
Selecting Deployment Targets # Flink is compatible with multiple cluster management frameworks like Kubernetes or YARN which are described in more detail in the Resource Provider section. Jobs can be submitted in different Deployment Modes. The parameterization of a job submission differs based on the underlying framework and Deployment Mode.
bin/flink offers a parameter --target to handle the different options. In addition to that, jobs have to be submitted using either run (for Session and Per-Job Mode) or run-application (for Application Mode). See the following summary of parameter combinations:
YARN ./bin/flink run --target yarn-session: Submission to an already running Flink on YARN cluster ./bin/flink run --target yarn-per-job: Submission spinning up a Flink on YARN cluster in Per-Job Mode ./bin/flink run-application --target yarn-application: Submission spinning up Flink on YARN cluster in Application Mode Kubernetes ./bin/flink run --target kubernetes-session: Submission to an already running Flink on Kubernetes cluster ./bin/flink run-application --target kubernetes-application: Submission spinning up a Flink on Kubernetes cluster in Application Mode Standalone: ./bin/flink run --target local: Local submission using a MiniCluster in Session Mode ./bin/flink run --target remote: Submission to an already running Flink cluster The --target will overwrite the execution.target specified in the conf/flink-conf.yaml.
For more details on the commands and the available options, please refer to the Resource Provider-specific pages of the documentation.
Submitting PyFlink Jobs # Currently, users are able to submit a PyFlink job via the CLI. It does not require to specify the JAR file path or the entry main class, which is different from the Java job submission.
When submitting Python job via flink run, Flink will run the command \u0026ldquo;python\u0026rdquo;. Please run the following command to confirm that the python executable in current environment points to a supported Python version of 3.6+. \$ python --version # the version printed here must be 3.6+ The following commands show different PyFlink job submission use-cases:
Run a PyFlink job: \$ ./bin/flink run --python examples/python/table/word_count.py Run a PyFlink job with additional source and resource files. Files specified in --pyFiles will be added to the PYTHONPATH and, therefore, available in the Python code. \$ ./bin/flink run \\ --python examples/python/table/word_count.py \\ --pyFiles file:///user.txt,hdfs:///\$namenode_address/username.txt Run a PyFlink job which will reference Java UDF or external connectors. JAR file specified in --jarfile will be uploaded to the cluster. \$ ./bin/flink run \\ --python examples/python/table/word_count.py \\ --jarfile \u0026lt;jarFile\u0026gt; Run a PyFlink job with pyFiles and the main entry module specified in --pyModule: \$ ./bin/flink run \\ --pyModule table.word_count \\ --pyFiles examples/python/table Submit a PyFlink job on a specific JobManager running on host \u0026lt;jobmanagerHost\u0026gt; (adapt the command accordingly): \$ ./bin/flink run \\ --jobmanager \u0026lt;jobmanagerHost\u0026gt;:8081 \\ --python examples/python/table/word_count.py Run a PyFlink job using a YARN cluster in Per-Job Mode: \$ ./bin/flink run \\ --target yarn-per-job --python examples/python/table/word_count.py Run a PyFlink job using a YARN cluster in Application Mode: \$ ./bin/flink run-application -t yarn-application \\ -Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=1024m \\ -Dyarn.application.name=\u0026lt;ApplicationName\u0026gt; \\ -Dyarn.ship-files=/path/to/shipfiles \\ -pyarch shipfiles/venv.zip \\ -pyclientexec venv.zip/venv/bin/python3 \\ -pyexec venv.zip/venv/bin/python3 \\ -py shipfiles/word_count.py Note It assumes that the Python dependencies needed to execute the job are already placed in the directory /path/to/shipfiles. For example, it should contain venv.zip and word_count.py for the above example.
Note As it executes the job on the JobManager in YARN application mode, the paths specified in -pyarch and -py are paths relative to shipfiles which is the directory name of the shipped files.
Note The archive files specified via -pyarch will be distributed to the TaskManagers through blob server where the file size limit is 2 GB. If the size of an archive file is more than 2 GB, you could upload it to a distributed file system and then use the path in the command line option -pyarch.
Run a PyFlink application on a native Kubernetes cluster having the cluster ID \u0026lt;ClusterId\u0026gt;, it requires a docker image with PyFlink installed, please refer to Enabling PyFlink in docker: \$ ./bin/flink run-application \\ --target kubernetes-application \\ --parallelism 8 \\ -Dkubernetes.cluster-id=\u0026lt;ClusterId\u0026gt; \\ -Dtaskmanager.memory.process.size=4096m \\ -Dkubernetes.taskmanager.cpu=2 \\ -Dtaskmanager.numberOfTaskSlots=4 \\ -Dkubernetes.container.image=\u0026lt;PyFlinkImageName\u0026gt; \\ --pyModule word_count \\ --pyFiles /opt/flink/examples/python/table/word_count.py To learn more available options, please refer to Kubernetes or YARN which are described in more detail in the Resource Provider section.
Besides --pyFiles, --pyModule and --python mentioned above, there are also some other Python related options. Here\u0026rsquo;s an overview of all the Python related options for the actions run and run-application supported by Flink\u0026rsquo;s CLI tool:
Option Description -py,--python Python script with the program entry point. The dependent resources can be configured with the --pyFiles option. -pym,--pyModule Python module with the program entry point. This option must be used in conjunction with --pyFiles. -pyfs,--pyFiles Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (',') could be used as the separator to specify multiple files (e.g., --pyFiles file:///tmp/myresource.zip,hdfs:///\$namenode_address/myresource2.zip). -pyarch,--pyArchives Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g., --pyArchives file:///tmp/py37.zip,file:///tmp/data.zip#data --pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). -pyclientexec,--pyClientExecutable The path of the Python interpreter used to launch the Python process when submitting the Python jobs via \\"flink run\\" or compiling the Java/Scala jobs containing Python UDFs. (e.g., --pyArchives file:///tmp/py37.zip --pyClientExecutable py37.zip/py37/python) -pyexec,--pyExecutable Specify the path of the python interpreter used to execute the python UDF worker (e.g.: --pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u003e= 20.3) and SetupTools (version \u003e= 37.0.0). Please ensure that the specified environment meets the above requirements. -pyreq,--pyRequirements Specify the requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists (e.g., --pyRequirements file:///tmp/requirements.txt#file:///tmp/cached_dir). In addition to the command line options during submitting the job, it also supports to specify the dependencies via configuration or Python API inside the code. Please refer to the dependency management for more details.
Back to top
`}),e.add({id:154,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/asyncio/",title:"异步 I/O",section:"算子",content:` 用于外部数据访问的异步 I/O # 本文讲解 Flink 用于访问外部数据存储的异步 I/O API。 对于不熟悉异步或者事件驱动编程的用户，建议先储备一些关于 Future 和事件驱动编程的知识。
提示：这篇文档 FLIP-12: 异步 I/O 的设计和实现介绍了关于设计和实现异步 I/O 功能的细节。 对于新增的重试支持的实现细节可以参考FLIP-232: 为 DataStream API 异步 I/O 操作增加重试支持。
对于异步 I/O 操作的需求 # 在与外部系统交互（用数据库中的数据扩充流数据）的时候，需要考虑与外部系统的通信延迟对整个流处理应用的影响。
简单地访问外部数据库的数据，比如使用 MapFunction，通常意味着同步交互： MapFunction 向数据库发送一个请求然后一直等待，直到收到响应。在许多情况下，等待占据了函数运行的大部分时间。
与数据库异步交互是指一个并行函数实例可以并发地处理多个请求和接收多个响应。这样，函数在等待的时间可以发送其他请求和接收其他响应。至少等待的时间可以被多个请求摊分。大多数情况下，异步交互可以大幅度提高流处理的吞吐量。
注意： 仅仅提高 MapFunction 的并行度（parallelism）在有些情况下也可以提升吞吐量，但是这样做通常会导致非常高的资源消耗：更多的并行 MapFunction 实例意味着更多的 Task、更多的线程、更多的 Flink 内部网络连接、 更多的与数据库的网络连接、更多的缓冲和更多程序内部协调的开销。
先决条件 # 如上节所述，正确地实现数据库（或键/值存储）的异步 I/O 交互需要支持异步请求的数据库客户端。许多主流数据库都提供了这样的客户端。
如果没有这样的客户端，可以通过创建多个客户端并使用线程池处理同步调用的方法，将同步客户端转换为有限并发的客户端。然而，这种方法通常比正规的异步客户端效率低。
异步 I/O API # Flink 的异步 I/O API 允许用户在流处理中使用异步请求客户端。API 处理与数据流的集成，同时还能处理好顺序、事件时间和容错等。
在具备异步数据库客户端的基础上，实现数据流转换操作与数据库的异步 I/O 交互需要以下三部分：
实现分发请求的 AsyncFunction 获取数据库交互的结果并发送给 ResultFuture 的 回调 函数 将异步 I/O 操作应用于 DataStream 作为 DataStream 的一次转换操作, 启用或者不启用重试。 下面是基本的代码模板：
Java // 这个例子使用 Java 8 的 Future 接口（与 Flink 的 Future 相同）实现了异步请求和回调。 /** * 实现 \u0026#39;AsyncFunction\u0026#39; 用于发送请求和设置回调。 */ class AsyncDatabaseRequest extends RichAsyncFunction\u0026lt;String, Tuple2\u0026lt;String, String\u0026gt;\u0026gt; { /** 能够利用回调函数并发发送请求的数据库客户端 */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception { client = new DatabaseClient(host, post, credentials); } @Override public void close() throws Exception { client.close(); } @Override public void asyncInvoke(String key, final ResultFuture\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultFuture) throws Exception { // 发送异步请求，接收 future 结果 final Future\u0026lt;String\u0026gt; result = client.query(key); // 设置客户端完成请求后要执行的回调函数 // 回调函数只是简单地把结果发给 future CompletableFuture.supplyAsync(new Supplier\u0026lt;String\u0026gt;() { @Override public String get() { try { return result.get(); } catch (InterruptedException | ExecutionException e) { // 显示地处理异常。 return null; } } }).thenAccept( (String dbResult) -\u0026gt; { resultFuture.complete(Collections.singleton(new Tuple2\u0026lt;\u0026gt;(key, dbResult))); }); } } // 创建初始 DataStream DataStream\u0026lt;String\u0026gt; stream = ...; // 应用异步 I/O 转换操作，不启用重试 DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100); // 或 应用异步 I/O 转换操作并启用重试 // 通过工具类创建一个异步重试策略, 或用户实现自定义的策略 AsyncRetryStrategy asyncRetryStrategy = new AsyncRetryStrategies.FixedDelayRetryStrategyBuilder(3, 100L) // maxAttempts=3, fixedDelay=100ms .retryIfResult(RetryPredicates.EMPTY_RESULT_PREDICATE) .retryIfException(RetryPredicates.HAS_EXCEPTION_PREDICATE) .build(); // 应用异步 I/O 转换操作并启用重试 DataStream\u0026lt;Tuple2\u0026lt;String, String\u0026gt;\u0026gt; resultStream = AsyncDataStream.unorderedWaitWithRetry(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100, asyncRetryStrategy); Scala /** * 实现 \u0026#39;AsyncFunction\u0026#39; 用于发送请求和设置回调。 */ class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] { /** 能够利用回调函数并发发送请求的数据库客户端 */ lazy val client: DatabaseClient = new DatabaseClient(host, post, credentials) /** 用于 future 回调的上下文环境 */ implicit lazy val executor: ExecutionContext = ExecutionContext.fromExecutor(Executors.directExecutor()) override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit = { // 发送异步请求，接收 future 结果 val resultFutureRequested: Future[String] = client.query(str) // 设置客户端完成请求后要执行的回调函数 // 回调函数只是简单地把结果发给 future resultFutureRequested.onSuccess { case result: String =\u0026gt; resultFuture.complete(Iterable((str, result))) } } } // 创建初始 DataStream val stream: DataStream[String] = ... // 应用异步 I/O 转换操作，不启用重试 val resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100) // 或 应用异步 I/O 转换操作并启用重试 // 创建一个异步重试策略 val asyncRetryStrategy: AsyncRetryStrategy[OUT] = ... // 应用异步 I/O 转换操作并启用重试 val resultStream: DataStream[(String, String)] = AsyncDataStream.unorderedWaitWithRetry(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100, asyncRetryStrategy) 重要提示： 第一次调用 ResultFuture.complete 后 ResultFuture 就完成了。 后续的 complete 调用都将被忽略。
下面两个参数控制异步操作：
Timeout： 超时参数定义了异步操作执行多久未完成、最终认定为失败的时长，如果启用重试，则可能包括多个重试请求。 它可以防止一直等待得不到响应的请求。
Capacity： 容量参数定义了可以同时进行的异步请求数。 即使异步 I/O 通常带来更高的吞吐量，执行异步 I/O 操作的算子仍然可能成为流处理的瓶颈。 限制并发请求的数量可以确保算子不会持续累积待处理的请求进而造成积压，而是在容量耗尽时触发反压。
AsyncRetryStrategy: 重试策略参数定义了什么条件会触发延迟重试以及延迟的策略，例如，固定延迟、指数后退延迟、自定义实现等。
超时处理 # 当异步 I/O 请求超时的时候，默认会抛出异常并重启作业。 如果你想处理超时，可以重写 AsyncFunction#timeout 方法。 重写 AsyncFunction#timeout 时别忘了调用 ResultFuture.complete() 或者 ResultFuture.completeExceptionally() 以便告诉Flink这条记录的处理已经完成。如果超时发生时你不想发出任何记录，你可以调用 ResultFuture.complete(Collections.emptyList()) 。
结果的顺序 # AsyncFunction 发出的并发请求经常以不确定的顺序完成，这取决于请求得到响应的顺序。 Flink 提供两种模式控制结果记录以何种顺序发出。
无序模式： 异步请求一结束就立刻发出结果记录。 流中记录的顺序在经过异步 I/O 算子之后发生了改变。 当使用 处理时间 作为基本时间特征时，这个模式具有最低的延迟和最少的开销。 此模式使用 AsyncDataStream.unorderedWait(...) 方法。
有序模式: 这种模式保持了流的顺序。发出结果记录的顺序与触发异步请求的顺序（记录输入算子的顺序）相同。为了实现这一点，算子将缓冲一个结果记录直到这条记录前面的所有记录都发出（或超时）。由于记录或者结果要在 checkpoint 的状态中保存更长的时间，所以与无序模式相比，有序模式通常会带来一些额外的延迟和 checkpoint 开销。此模式使用 AsyncDataStream.orderedWait(...) 方法。
事件时间 # 当流处理应用使用事件时间时，异步 I/O 算子会正确处理 watermark。对于两种顺序模式，这意味着以下内容：
无序模式： Watermark 既不超前于记录也不落后于记录，即 watermark 建立了顺序的边界。 只有连续两个 watermark 之间的记录是无序发出的。 在一个 watermark 后面生成的记录只会在这个 watermark 发出以后才发出。 在一个 watermark 之前的所有输入的结果记录全部发出以后，才会发出这个 watermark。
这意味着存在 watermark 的情况下，无序模式 会引入一些与有序模式 相同的延迟和管理开销。开销大小取决于 watermark 的频率。
有序模式： 连续两个 watermark 之间的记录顺序也被保留了。开销与使用处理时间 相比，没有显著的差别。
请记住，摄入时间 是一种特殊的事件时间，它基于数据源的处理时间自动生成 watermark。
容错保证 # 异步 I/O 算子提供了完全的精确一次容错保证。它将在途的异步请求的记录保存在 checkpoint 中，在故障恢复时重新触发请求。
重试支持 # 重试支持为异步 I/O 操作引入了一个内置重试机制，它对用户的异步函数实现逻辑是透明的。
AsyncRetryStrategy: 异步重试策略包含了触发重试条件 AsyncRetryPredicate 定义，以及根据当前已尝试次数判断是否继续重试、下次重试间隔时长的接口方法。 需要注意，在满足触发重试条件后，有可能因为当前重试次数超过预设的上限放弃重试，或是在任务结束时被强制终止重试（这种情况下，系统以最后一次执行的结果或异常作为最终状态）。
AsyncRetryPredicate: 触发重试条件可以选择基于返回结果、 执行异常来定义条件，两种条件是或的关系，满足其一即会触发。
实现提示 # 在实现使用 Executor（或者 Scala 中的 ExecutionContext）和回调的 Futures 时，建议使用 DirectExecutor，因为通常回调的工作量很小，DirectExecutor 避免了额外的线程切换开销。回调通常只是把结果发送给 ResultFuture，也就是把它添加进输出缓冲。从这里开始，包括发送记录和与 chenkpoint 交互在内的繁重逻辑都将在专有的线程池中进行处理。
DirectExecutor 可以通过 org.apache.flink.util.concurrent.Executors.directExecutor() 或 com.google.common.util.concurrent.MoreExecutors.directExecutor() 获得。
警告 # Flink 不以多线程方式调用 AsyncFunction
我们想在这里明确指出一个经常混淆的地方：AsyncFunction 不是以多线程方式调用的。 只有一个 AsyncFunction 实例，它被流中相应分区内的每个记录顺序地调用。除非 asyncInvoke(...) 方法快速返回并且依赖于（客户端的）回调, 否则无法实现正确的异步 I/O。
例如，以下情况导致阻塞的 asyncInvoke(...) 函数，从而使异步行为无效：
使用同步数据库客户端，它的查询方法调用在返回结果前一直被阻塞。 在 asyncInvoke(...) 方法内阻塞等待异步客户端返回的 future 类型对象 默认情况下，AsyncFunction 的算子（异步等待算子）可以在作业图的任意处使用，但它不能与SourceFunction/SourceStreamTask组成算子链
启用重试后可能需要更大的缓冲队列容量
新的重试功能可能会导致更大的队列容量要求，最大数量可以近似地评估如下。
inputRate * retryRate * avgRetryDuration 例如，对于一个输入率=100条记录/秒的任务，其中1%的元素将平均触发1次重试，平均重试时间为60秒，额外的队列容量要求为:
100条记录/秒 * 1% * 60s = 60 也就是说，在无序输出模式下，给工作队列增加 60 个容量可能不会影响吞吐量； 而在有序模式下，头部元素是关键点，它未完成的时间越长，算子提供的处理延迟就越长, 在相同的超时约束下，如果头元素事实上获得了更多的重试, 那重试功能可能会增加头部元素的处理时间即未完成时间，也就是说在有序模式下，增大队列容量并不是总能提升吞吐。
当队列容量增长时（ 这是缓解背压的常用方法），OOM 的风险会随之增加。对于 ListState 存储来说，理论的上限是 Integer.MAX_VALUE， 所以, 虽然事实上队列容量的限制是一样的，但我们在生产中不能把队列容量增加到太大，这种情况下增加任务的并行性也许更可行。
Back to top
`}),e.add({id:155,href:"/flink/flink-docs-master/zh/docs/dev/datastream/user_defined_functions/",title:"用户自定义 Functions",section:"DataStream API",content:` \u0026lsquo;用户自定义 Functions\u0026rsquo; # 大多数操作都需要用户自定义 function。本节列出了实现用户自定义 function 的不同方式。还会介绍 Accumulators（累加器），可用于深入了解你的 Flink 应用程序。
Java 实现接口 # 最基本的方法是实现提供的接口：
class MyMapFunction implements MapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } data.map(new MyMapFunction()); 匿名类 # 你可以将 function 当做匿名类传递：
data.map(new MapFunction\u0026lt;String, Integer\u0026gt; () { public Integer map(String value) { return Integer.parseInt(value); } }); Java 8 Lambdas # Flink 在 Java API 中还支持 Java 8 Lambdas 表达式。
data.filter(s -\u0026gt; s.startsWith(\u0026#34;http://\u0026#34;)); data.reduce((i1,i2) -\u0026gt; i1 + i2); Rich functions # 所有需要用户自定义 function 的转化操作都可以将 rich function 作为参数。例如，你可以将下面代码
class MyMapFunction implements MapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } 替换成
class MyMapFunction extends RichMapFunction\u0026lt;String, Integer\u0026gt; { public Integer map(String value) { return Integer.parseInt(value); } } 并将 function 照常传递给 map transformation:
data.map(new MyMapFunction()); Rich functions 也可以定义成匿名类：
data.map (new RichMapFunction\u0026lt;String, Integer\u0026gt;() { public Integer map(String value) { return Integer.parseInt(value); } }); Scala Lambda Functions # 正如你在上面的例子中看到的，所有的操作同可以通过 lambda 表达式来描述：
val data: DataSet[String] = // [...] data.filter { _.startsWith(\u0026#34;http://\u0026#34;) } val data: DataSet[Int] = // [...] data.reduce { (i1,i2) =\u0026gt; i1 + i2 } // or data.reduce { _ + _ } Rich functions # 所有将 lambda 表达式作为参数的转化操作都可以用 rich function 来代替。例如，你可以将下面代码
data.map { x =\u0026gt; x.toInt } 替换成
class MyMapFunction extends RichMapFunction[String, Int] { def map(in: String): Int = in.toInt } 并将 function 传递给 map transformation:
data.map(new MyMapFunction()) Rich functions 也可以定义成匿名类:
data.map (new RichMapFunction[String, Int] { def map(in: String): Int = in.toInt }) 除了用户自定义的 function（map，reduce 等），Rich functions 还提供了四个方法：open、close、getRuntimeContext 和 setRuntimeContext。这些方法对于参数化 function （参阅 给 function 传递参数）， 创建和最终确定本地状态，访问广播变量（参阅 广播变量），以及访问运行时信息，例如累加器和计数器（参阅 累加器和计数器），以及迭代器的相关信息（参阅 迭代器） 有很大作用。
Back to top
累加器和计数器 # 累加器是具有加法运算和最终累加结果的一种简单结构，可在作业结束后使用。
最简单的累加器就是计数器: 你可以使用 Accumulator.add(V value) 方法将其递增。在作业结束时，Flink 会汇总（合并）所有部分的结果并将其发送给客户端。 在调试过程中或在你想快速了解有关数据更多信息时,累加器作用很大。
Flink 目前有如下内置累加器。每个都实现了 累加器 接口。
IntCounter , LongCounter 和 DoubleCounter : 有关使用计数器的示例，请参见下文。 直方图 : 离散数量的柱状直方图实现。在内部，它只是整形到整形的映射。你可以使用它来计算值的分布，例如，单词计数程序的每行单词的分布情况。 如何使用累加器：
首先，在需要使用累加器的用户自定义的转换 function 中创建一个累加器对象（此处是计数器）。
private IntCounter numLines = new IntCounter(); 其次，你必须在 rich function 的 open() 方法中注册累加器对象。也可以在此处定义名称。
getRuntimeContext().addAccumulator(\u0026#34;num-lines\u0026#34;, this.numLines); 现在你可以在操作 function 中的任何位置（包括 open() 和 close() 方法中）使用累加器。
this.numLines.add(1); 最终整体结果会存储在由执行环境的 execute() 方法返回的 JobExecutionResult 对象中（当前只有等待作业完成后执行才起作用）。
myJobExecutionResult.getAccumulatorResult(\u0026#34;num-lines\u0026#34;); 单个作业的所有累加器共享一个命名空间。因此你可以在不同的操作 function 里面使用同一个累加器。Flink 会在内部将所有具有相同名称的累加器合并起来。
关于累加器和迭代的注意事项：当前累加器的结果只有在整个作业结束后才可用。我们还计划在下一次迭代中提供上一次的迭代结果。你可以使用 聚合器 来计算每次迭代的统计信息，并基于此类统计信息来终止迭代。
定制累加器：
要实现自己的累加器，你只需要实现累加器接口即可。如果你认为自定义累加器应随 Flink 一起提供，请尽管创建 pull request。
你可以选择实现 Accumulator 或 SimpleAccumulator 。
Accumulator\u0026lt;V,R\u0026gt; 的实现十分灵活: 它定义了将要添加的值类型 V，并定义了最终的结果类型 R。例如，对于直方图，V 是一个数字且 R 是一个直方图。 SimpleAccumulator 适用于两种类型都相同的情况，例如计数器。
Back to top
`}),e.add({id:156,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/",title:"状态与容错",section:"DataStream API",content:" "}),e.add({id:157,href:"/flink/flink-docs-master/zh/docs/internals/job_scheduling/",title:"作业调度",section:"内幕",content:` 作业调度 # 这篇文档简要描述了 Flink 怎样调度作业, 怎样在 JobManager 里描述和追踪作业状态
调度 # Flink 通过 Task Slots 来定义执行资源。每个 TaskManager 有一到多个 task slot，每个 task slot 可以运行一条由多个并行 task 组成的流水线。 这样一条流水线由多个连续的 task 组成，比如并行度为 n 的 MapFunction 和 并行度为 n 的 ReduceFunction。需要注意的是 Flink 经常并发执行连续的 task，不仅在流式作业中到处都是，在批量作业中也很常见。
下图很好的阐释了这一点，一个由数据源、MapFunction 和 ReduceFunction 组成的 Flink 作业，其中数据源和 MapFunction 的并行度为 4 ，ReduceFunction 的并行度为 3 。流水线由一系列的 Source - Map - Reduce 组成，运行在 2 个 TaskManager 组成的集群上，每个 TaskManager 包含 3 个 slot，整个作业的运行如下图所示。
Flink 内部通过 SlotSharingGroup 和 CoLocationGroup 来定义哪些 task 可以共享一个 slot， 哪些 task 必须严格放到同一个 slot。
JobManager 数据结构 # 在作业执行期间，JobManager 会持续跟踪各个 task，决定何时调度下一个或一组 task，处理已完成的 task 或执行失败的情况。
JobManager 会接收到一个 JobGraph ，用来描述由多个算子顶点 ( JobVertex ) 组成的数据流图，以及中间结果数据 ( IntermediateDataSet )。每个算子都有自己的可配置属性，比如并行度和运行的代码。除此之外，JobGraph 还包含算子代码执行所必须的依赖库。
JobManager 会将 JobGraph 转换成 ExecutionGraph 。可以将 ExecutionGraph 理解为并行版本的 JobGraph，对于每一个顶点 JobVertex，它的每个并行子 task 都有一个 ExecutionVertex 。一个并行度为 100 的算子会有 1 个 JobVertex 和 100 个 ExecutionVertex。ExecutionVertex 会跟踪子 task 的执行状态。 同一个 JobVertex 的所有 ExecutionVertex 都通过 ExecutionJobVertex 来持有，并跟踪整个算子的运行状态。ExecutionGraph 除了这些顶点，还包含中间数据结果和分片情况 IntermediateResult 和 IntermediateResultPartition 。前者跟踪中间结果的状态，后者跟踪每个分片的状态。
每个 ExecutionGraph 都有一个与之相关的作业状态信息，用来描述当前的作业执行状态。
Flink 作业刚开始会处于 created 状态，然后切换到 running 状态，当所有任务都执行完之后会切换到 finished 状态。如果遇到失败的话，作业首先切换到 failing 状态以便取消所有正在运行的 task。如果所有 job 节点都到达最终状态并且 job 无法重启， 那么 job 进入 failed 状态。如果作业可以重启，那么就会进入到 restarting 状态，当作业彻底重启之后会进入到 created 状态。
如果用户取消了 job 话，它会进入到 cancelling 状态，并取消所有正在运行的 task。当所有正在运行的 task 进入到最终状态的时候，job 进入 cancelled 状态。
Finished、canceled 和 failed 会导致全局的终结状态，并且触发作业的清理。跟这些状态不同，suspended 状态只是一个局部的终结。局部的终结意味着作业的执行已经被对应的 JobManager 终结，但是集群中另外的 JobManager 依然可以从高可用存储里获取作业信息并重启。因此一个处于 suspended 状态的作业不会被彻底清理掉。
在整个 ExecutionGraph 执行期间，每个并行 task 都会经历多个阶段，从 created 状态到 finished 或 failed。下图展示了各种状态以及他们之间的转换关系。由于一个 task 可能会被执行多次(比如在异常恢复时)，ExecutionVertex 的执行是由 Execution 来跟踪的，每个 ExecutionVertex 会记录当前的执行，以及之前的执行。
Back to top
`}),e.add({id:158,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/alter/",title:"ALTER 语句",section:"SQL",content:` ALTER 语句 # ALTER 语句用于修改一个已经在 Catalog 中注册的表、视图或函数定义。
Flink SQL 目前支持以下 ALTER 语句：
ALTER TABLE ALTER VIEW ALTER DATABASE ALTER FUNCTION 执行 ALTER 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 ALTER 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 ALTER 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 ALTER 语句。 若 ALTER 操作执行成功，execute_sql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 ALTER 语句。
SQL CLI 可以在 SQL CLI 中执行 ALTER 语句。
以下的例子展示了如何在 SQL CLI 中执行一个 ALTER 语句。
Java TableEnvironment tableEnv = TableEnvironment.create(...); // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;); // 字符串数组： [\u0026#34;Orders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // 把 “Orders” 的表名改为 “NewOrders” tableEnv.executeSql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;); // 字符串数组：[\u0026#34;NewOrders\u0026#34;] String[] tables = tableEnv.listTables(); // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // 注册名为 “Orders” 的表 tableEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) // 字符串数组： [\u0026#34;Orders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // 把 “Orders” 的表名改为 “NewOrders” tableEnv.executeSql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;) // 字符串数组：[\u0026#34;NewOrders\u0026#34;] val tables = tableEnv.listTables() // or tableEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() Python table_env = TableEnvironment.create(...) # 字符串数组： [\u0026#34;Orders\u0026#34;] tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() # 把 “Orders” 的表名改为 “NewOrders” table_env.execute_sql(\u0026#34;ALTER TABLE Orders RENAME TO NewOrders;\u0026#34;) # 字符串数组：[\u0026#34;NewOrders\u0026#34;] tables = table_env.list_tables() # or table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; Orders Flink SQL\u0026gt; ALTER TABLE Orders RENAME TO NewOrders; [INFO] Table has been removed. Flink SQL\u0026gt; SHOW TABLES; NewOrders ALTER TABLE # 重命名表 ALTER TABLE [catalog_name.][db_name.]table_name RENAME TO new_table_name 把原有的表名更改为新的表名。
设置或修改表属性 ALTER TABLE [catalog_name.][db_name.]table_name SET (key1=val1, key2=val2, ...) 为指定的表设置一个或多个属性。若个别属性已经存在于表中，则使用新的值覆盖旧的值。
ALTER VIEW # ALTER VIEW [catalog_name.][db_name.]view_name RENAME TO new_view_name Renames a given view to a new name within the same catalog and database.
ALTER VIEW [catalog_name.][db_name.]view_name AS new_query_expression Changes the underlying query defining the given view to a new query.
ALTER DATABASE # ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...) 在数据库中设置一个或多个属性。若个别属性已经在数据库中设定，将会使用新值覆盖旧值。
ALTER FUNCTION # ALTER [TEMPORARY|TEMPORARY SYSTEM] FUNCTION [IF EXISTS] [catalog_name.][db_name.]function_name AS identifier [LANGUAGE JAVA|SCALA|PYTHON] 修改一个有 catalog 和数据库命名空间的 catalog function ，需要指定一个新的 identifier ，可指定 language tag 。若函数不存在，删除会抛出异常。
如果 language tag 是 JAVA 或者 SCALA ，则 identifier 是 UDF 实现类的全限定名。关于 JAVA/SCALA UDF 的实现，请参考 自定义函数。
如果 language tag 是 PYTHON ， 则 identifier 是 UDF 对象的全限定名，例如 pyflink.table.tests.test_udf.add。关于 PYTHON UDF 的实现，请参考 Python UDFs。
TEMPORARY
修改一个有 catalog 和数据库命名空间的临时 catalog function ，并覆盖原有的 catalog function 。
TEMPORARY SYSTEM
修改一个没有数据库命名空间的临时系统 catalog function ，并覆盖系统内置的函数。
IF EXISTS
若函数不存在，则不进行任何操作。
LANGUAGE JAVA|SCALA|PYTHON
Language tag 用于指定 Flink runtime 如何执行这个函数。目前，只支持 JAVA，SCALA 和 PYTHON，且函数的默认语言为 JAVA。
`}),e.add({id:159,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/canal/",title:"Canal",section:"Formats",content:` Canal Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Canal 是一个 CDC（ChangeLog Data Capture，变更日志数据捕获）工具，可以实时地将 MySQL 变更传输到其他系统。Canal 为变更日志提供了统一的数据格式，并支持使用 JSON 或 protobuf 序列化消息（Canal 默认使用 protobuf）。
Flink 支持将 Canal 的 JSON 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常的有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等。 Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Canal 格式的 JSON 消息，输出到 Kafka 等存储中。 但需要注意的是，目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息。因此，Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Canal 消息。
注意：未来会支持 Canal protobuf 类型消息的解析以及输出 Canal 格式的消息。
依赖 # In order to use the Canal format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in 注意：有关如何部署 Canal 以将变更日志同步到消息队列，请参阅 Canal 文档。
如何使用 Canal Format # Canal 为变更日志提供了统一的格式，下面是一个从 MySQL 库 products 表中捕获更新操作的简单示例：
{ \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;111\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;5.18\u0026#34; } ], \u0026#34;database\u0026#34;: \u0026#34;inventory\u0026#34;, \u0026#34;es\u0026#34;: 1589373560000, \u0026#34;id\u0026#34;: 9, \u0026#34;isDdl\u0026#34;: false, \u0026#34;mysqlType\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;INTEGER\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VARCHAR(255)\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;VARCHAR(512)\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;FLOAT\u0026#34; }, \u0026#34;old\u0026#34;: [ { \u0026#34;weight\u0026#34;: \u0026#34;5.15\u0026#34; } ], \u0026#34;pkNames\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;sql\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sqlType\u0026#34;: { \u0026#34;id\u0026#34;: 4, \u0026#34;name\u0026#34;: 12, \u0026#34;description\u0026#34;: 12, \u0026#34;weight\u0026#34;: 7 }, \u0026#34;table\u0026#34;: \u0026#34;products\u0026#34;, \u0026#34;ts\u0026#34;: 1589373560798, \u0026#34;type\u0026#34;: \u0026#34;UPDATE\u0026#34; } 注意：有关各个字段的含义，请参阅 Canal 文档
MySQL products 表有4列（id，name，description 和 weight）。上面的 JSON 消息是 products 表上的一个更新事件，表示 id = 111 的行数据上 weight 字段值从5.15变更成为 5.18。假设消息已经同步到了一个 Kafka 主题：products_binlog，那么就可以使用以下DDL来从这个主题消费消息并解析变更事件。
CREATE TABLE topic_products ( -- 元数据与 MySQL \u0026#34;products\u0026#34; 表完全相同 id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;canal-json\u0026#39; -- 使用 canal-json 格式 ) 将 Kafka 主题注册成 Flink 表之后，就可以将 Canal 消息用作变更日志源。
-- 关于MySQL \u0026#34;products\u0026#34; 表的实时物化视图 -- 计算相同产品的最新平均重量 SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- 将 MySQL \u0026#34;products\u0026#34; 表的所有数据和增量更改同步到 -- Elasticsearch \u0026#34;products\u0026#34; 索引以供将来搜索 INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Attention Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format.
Key Data Type Description database STRING NULL The originating database. Corresponds to the database field in the Canal record if available. table STRING NULL The originating database table. Corresponds to the table field in the Canal record if available. sql-type MAP\u0026lt;STRING, INT\u0026gt; NULL Map of various sql types. Corresponds to the sqlType field in the Canal record if available. pk-names ARRAY\u0026lt;STRING\u0026gt; NULL Array of primary key names. Corresponds to the pkNames field in the Canal record if available. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts field in the Canal record. The following example shows how to access Canal metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_database STRING METADATA FROM \u0026#39;value.database\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, origin_sql_type MAP\u0026lt;STRING, INT\u0026gt; METADATA FROM \u0026#39;value.sql-type\u0026#39; VIRTUAL, origin_pk_names ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.pk-names\u0026#39; VIRTUAL, origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;canal-json\u0026#39; ); Format 参数 # 选项 要求 默认 类型 描述 format 必填 (none) String 指定要使用的格式，此处应为 'canal-json'. canal-json.ignore-parse-errors 选填 false Boolean 当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null。 canal-json.timestamp-format.standard 选填 'SQL' String 指定输入和输出时间戳格式。当前支持的值是 'SQL' 和 'ISO-8601': 选项 'SQL' 将解析 "yyyy-MM-dd HH:mm:ss.s{precision}" 格式的输入时间戳，例如 '2020-12-30 12:13:14.123'，并以相同格式输出时间戳。 选项 'ISO-8601' 将解析 "yyyy-MM-ddTHH:mm:ss.s{precision}" 格式的输入时间戳，例如 '2020-12-30T12:13:14.123'，并以相同的格式输出时间戳。 canal-json.map-null-key.mode 选填 'FAIL' String 指定处理 Map 中 key 值为空的方法. 当前支持的值有 'FAIL', 'DROP' 和 'LITERAL': Option 'FAIL' 将抛出异常，如果遇到 Map 中 key 值为空的数据。 Option 'DROP' 将丢弃 Map 中 key 值为空的数据项。 Option 'LITERAL' 将使用字符串常量来替换 Map 中的空 key 值。字符串常量的值由 'canal-json.map-null-key.literal' 定义。 canal-json.map-null-key.literal 选填 'null' String 当 'canal-json.map-null-key.mode' 是 LITERAL 的时候，指定字符串常量替换 Map 中的空 key 值。 canal-json.encode.decimal-as-plain-number 选填 false Boolean 将所有 DECIMAL 类型的数据保持原状，不使用科学计数法表示。例：0.000000027 默认会表示为 2.7E-8。当此选项设为 true 时，则会表示为 0.000000027。 canal-json.database.include optional (none) String 一个可选的正则表达式，通过正则匹配 Canal 记录中的 "database" 元字段，仅读取指定数据库的 changelog 记录。正则字符串与 Java 的 Pattern 兼容。 canal-json.table.include optional (none) String 一个可选的正则表达式，通过正则匹配 Canal 记录中的 "table" 元字段，仅读取指定表的 changelog 记录。正则字符串与 Java 的 Pattern 兼容。 注意事项 # 重复的变更事件 # 在正常的操作环境下，Canal 应用能以 exactly-once 的语义投递每条变更事件。在这种情况下，Flink 消费 Canal 产生的变更事件能够工作得很好。 然而，当有故障发生时，Canal 应用只能保证 at-least-once 的投递语义。 这也意味着，在非正常情况下，Canal 可能会投递重复的变更事件到消息队列中，当 Flink 从消息队列中消费的时候就会得到重复的事件。 这可能会导致 Flink query 的运行得到错误的结果或者非预期的异常。因此，建议在这种情况下，建议在这种情况下，将作业参数 table.exec.source.cdc-events-duplicate 设置成 true，并在该 source 上定义 PRIMARY KEY。 框架会生成一个额外的有状态算子，使用该 primary key 来对变更事件去重并生成一个规范化的 changelog 流。
数据类型映射 # 目前，Canal Format 使用 JSON Format 进行序列化和反序列化。 有关数据类型映射的更多详细信息，请参阅 JSON Format 文档。
`}),e.add({id:160,href:"/flink/flink-docs-master/zh/docs/connectors/",title:"Connectors",section:"Docs",content:""}),e.add({id:161,href:"/flink/flink-docs-master/zh/docs/deployment/filesystems/",title:"File Systems",section:"Deployment",content:""}),e.add({id:162,href:"/flink/flink-docs-master/zh/docs/try-flink/flink-operations-playground/",title:"Flink 操作场景",section:"Try Flink",content:` Flink 操作场景 # Apache Flink 可以以多种方式在不同的环境中部署，抛开这种多样性而言，Flink 集群的基本构建方式和操作原则仍然是相同的。
在这篇文章里，你将会学习如何管理和运行 Flink 任务，了解如何部署和监控应用程序、Flink 如何从失败作业中进行恢复，同时你还会学习如何执行一些日常操作任务，如升级和扩容。
注意：本文中使用的 Apache Flink Docker 镜像仅适用于 Apache Flink 发行版。 由于你目前正在浏览快照版的文档，因此下文中引用的分支可能已经不存在了，请先通过左侧菜单下方的版本选择器切换到发行版文档再查看。 场景说明 # 这篇文章中的所有操作都是基于如下两个集群进行的： Flink Session Cluster 以及一个 Kafka 集群， 我们会在下文带领大家一起搭建这两个集群。
一个 Flink 集群总是包含一个 JobManager 以及一个或多个 Flink TaskManager。JobManager 负责处理 Job 提交、 Job 监控以及资源管理。Flink TaskManager 运行 worker 进程， 负责实际任务 Tasks 的执行，而这些任务共同组成了一个 Flink Job。 在这篇文章中， 我们会先运行一个 TaskManager，接下来会扩容到多个 TaskManager。 另外，这里我们会专门使用一个 client 容器来提交 Flink Job， 后续还会使用该容器执行一些操作任务。需要注意的是，Flink 集群的运行并不需要依赖 client 容器， 我们这里引入只是为了使用方便。
这里的 Kafka 集群由一个 Zookeeper 服务端和一个 Kafka Broker 组成。
一开始，我们会往 JobManager 提交一个名为 Flink 事件计数 的 Job，此外，我们还创建了两个 Kafka Topic：input 和 output。
该 Job 负责从 input topic 消费点击事件 ClickEvent，每个点击事件都包含一个 timestamp 和一个 page 属性。 这些事件将按照 page 属性进行分组，然后按照每 15s 窗口 windows 进行统计， 最终结果输出到 output topic 中。
总共有 6 种不同的 page 属性，针对特定 page，我们会按照每 15s 产生 1000 个点击事件的速率生成数据。 因此，针对特定 page，该 Flink job 应该能在每个窗口中输出 1000 个该 page 的点击数据。
Back to top
环境搭建 # 环境搭建只需要几步就可以完成，我们将会带你过一遍必要的操作命令， 并说明如何验证我们正在操作的一切都是运行正常的。
你需要在自己的主机上提前安装好 docker (1.12+) 和 docker-compose (2.1+)。
我们所使用的配置文件位于 flink-playgrounds 仓库中， 首先检出该仓库并构建 docker 镜像：
git clone https://github.com/apache/flink-playgrounds.git cd flink-playgrounds/operations-playground docker-compose build 接下来在开始运行之前先在 Docker 主机上创建检查点和保存点目录（这些卷由 jobmanager 和 taskmanager 挂载，如 docker-compose.yaml 中所指定的）：
mkdir -p /tmp/flink-checkpoints-directory mkdir -p /tmp/flink-savepoints-directory 然后启动环境：
docker-compose up -d 接下来你可以执行如下命令来查看正在运行中的 Docker 容器：
docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------------------------------------------- operations-playground_clickevent-generator_1 /docker-entrypoint.sh java ... Up 6123/tcp, 8081/tcp operations-playground_client_1 /docker-entrypoint.sh flin ... Exit 0 operations-playground_jobmanager_1 /docker-entrypoint.sh jobm ... Up 6123/tcp, 0.0.0.0:8081-\u0026gt;8081/tcp operations-playground_kafka_1 start-kafka.sh Up 0.0.0.0:9094-\u0026gt;9094/tcp operations-playground_taskmanager_1 /docker-entrypoint.sh task ... Up 6123/tcp, 8081/tcp operations-playground_zookeeper_1 /bin/sh -c /usr/sbin/sshd ... Up 2181/tcp, 22/tcp, 2888/tcp, 3888/tcp 从上面的信息可以看出 client 容器已成功提交了 Flink Job (Exit 0)， 同时包含数据生成器在内的所有集群组件都处于运行中状态 (Up)。
你可以执行如下命令停止 docker 环境：
docker-compose down -v 环境讲解 # 在这个搭建好的环境中你可以尝试和验证很多事情，在下面的两个部分中我们将向你展示如何与 Flink 集群进行交互以及演示并讲解 Flink 的一些核心特性。
Flink WebUI 界面 # 观察Flink集群首先想到的就是 Flink WebUI 界面：打开浏览器并访问 http://localhost:8081，如果一切正常，你将会在界面上看到一个 TaskManager 和一个处于 \u0026ldquo;RUNNING\u0026rdquo; 状态的名为 Click Event Count 的 Job。
Flink WebUI 界面包含许多关于 Flink 集群以及运行在其上的 Jobs 的有用信息，比如：JobGraph、Metrics、Checkpointing Statistics、TaskManager Status 等等。
日志 # JobManager
JobManager 日志可以通过 docker-compose 命令进行查看。
docker-compose logs -f jobmanager JobManager 刚启动完成之时，你会看到很多关于 checkpoint completion (检查点完成)的日志。
TaskManager
TaskManager 日志也可以通过同样的方式进行查看。
docker-compose logs -f taskmanager TaskManager 刚启动完成之时，你同样会看到很多关于 checkpoint completion (检查点完成)的日志。
Flink CLI # Flink CLI 相关命令可以在 client 容器内进行使用。 比如，想查看 Flink CLI 的 help 命令，可以通过如下方式进行查看：
docker-compose run --no-deps client flink --help Flink REST API # Flink REST API 可以通过本机的 localhost:8081 进行访问，也可以在 client 容器中通过 jobmanager:8081 进行访问。 比如，通过如下命令可以获取所有正在运行中的 Job：
curl localhost:8081/jobs 注意: 如果你的主机上没有 curl 命令，那么你可以通过 client 容器进行访问（类似于 Flink CLI 命令）：
docker-compose run --no-deps client curl jobmanager:8081/jobs Kafka Topics # 可以运行如下命令查看 Kafka Topics 中的记录：
//input topic (1000 records/s) docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic input //output topic (24 records/min) docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Back to top
核心特性探索 # 到目前为止，你已经学习了如何与 Flink 以及 Docker 容器进行交互，现在让我们看一些常用的操作命令。 本节中的各部分命令不需要按任何特定的顺序执行，这些命令大部分都可以通过 CLI 或 RESTAPI 执行。
获取所有运行中的 Job # CLI 命令
docker-compose run --no-deps client flink list 预期输出
Waiting for response... ------------------ Running/Restarting Jobs ------------------- 16.07.2019 16:37:55 : \u0026lt;job-id\u0026gt; : Click Event Count (RUNNING) -------------------------------------------------------------- No scheduled jobs. REST API 请求
curl localhost:8081/jobs 预期响应 (结果已格式化)
{ \u0026#34;jobs\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34; } ] } 一旦 Job 提交，Flink 会默认为其生成一个 JobID，后续对该 Job 的 所有操作（无论是通过 CLI 还是 REST API）都需要带上 JobID。
Job 失败与恢复 # 在 Job (部分)失败的情况下，Flink 对事件处理依然能够提供精确一次的保障， 在本节中你将会观察到并能够在某种程度上验证这种行为。
Step 1: 观察输出 # 如前文所述，事件以特定速率生成，刚好使得每个统计窗口都包含确切的 1000 条记录。 因此，你可以实时查看 output topic 的输出，确定失败恢复后所有的窗口依然输出正确的统计数字， 以此来验证 Flink 在 TaskManager 失败时能够成功恢复，而且不丢失数据、不产生数据重复。
为此，通过控制台命令消费 output topic，保持消费直到 Job 从失败中恢复 (Step 3)。
docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Step 2: 模拟失败 # 为了模拟部分失败故障，你可以 kill 掉一个 TaskManager，这种失败行为在生产环境中就相当于 TaskManager 进程挂掉、TaskManager 机器宕机或者从框架或用户代码中抛出的一个临时异常（例如，由于外部资源暂时不可用）而导致的失败。
docker-compose kill taskmanager 几秒钟后，JobManager 就会感知到 TaskManager 已失联，接下来它会 取消 Job 运行并且立即重新提交该 Job 以进行恢复。 当 Job 重启后，所有的任务都会处于 SCHEDULED 状态，如以下截图中紫色方格所示：
注意：虽然 Job 的所有任务都处于 SCHEDULED 状态，但整个 Job 的状态却显示为 RUNNING。 此时，由于 TaskManager 提供的 TaskSlots 资源不够用，Job 的所有任务都不能成功转为 RUNNING 状态，直到有新的 TaskManager 可用。在此之前，该 Job 将经历一个取消和重新提交 不断循环的过程。
与此同时，数据生成器 (data generator) 一直不断地往 input topic 中生成 ClickEvent 事件，在生产环境中也经常出现这种 Job 挂掉但源头还在不断产生数据的情况。
Step 3: 失败恢复 # 一旦 TaskManager 重启成功，它将会重新连接到 JobManager。
docker-compose up -d taskmanager 当 TaskManager 注册成功后，JobManager 就会将处于 SCHEDULED 状态的所有任务调度到该 TaskManager 的可用 TaskSlots 中运行，此时所有的任务将会从失败前最近一次成功的 checkpoint 进行恢复， 一旦恢复成功，它们的状态将转变为 RUNNING。
接下来该 Job 将快速处理 Kafka input 事件的全部积压（在 Job 中断期间累积的数据）， 并以更快的速度(\u0026gt;24 条记录/分钟)产生输出，直到它追上 kafka 的 lag 延迟为止。 此时观察 output topic 输出， 你会看到在每一个时间窗口中都有按 page 进行分组的记录，而且计数刚好是 1000。 由于我们使用的是 FlinkKafkaProducer \u0026ldquo;至少一次\u0026quot;模式，因此你可能会看到一些记录重复输出多次。
注意：在大部分生产环境中都需要一个资源管理器 (Kubernetes、Yarn)对 失败的 Job 进行自动重启。 Job 升级与扩容 # 升级 Flink 作业一般都需要两步：第一，使用 Savepoint 优雅地停止 Flink Job。 Savepoint 是整个应用程序状态的一次快照（类似于 checkpoint ），该快照是在一个明确定义的、全局一致的时间点生成的。第二，从 Savepoint 恢复启动待升级的 Flink Job。 在此，“升级”包含如下几种含义：
配置升级（比如 Job 并行度修改） Job 拓扑升级（比如添加或者删除算子） Job 的用户自定义函数升级 在开始升级之前，你可能需要实时查看 Output topic 输出， 以便观察在升级过程中没有数据丢失或损坏。
docker-compose exec kafka kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 --topic output Step 1: 停止 Job # 要优雅停止 Job，需要使用 JobID 通过 CLI 或 REST API 调用 “stop” 命令。 JobID 可以通过获取所有运行中的 Job 接口或 Flink WebUI 界面获取，拿到 JobID 后就可以继续停止作业了：
CLI 命令
docker-compose run --no-deps client flink stop \u0026lt;job-id\u0026gt; 预期输出
Suspending job \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; with a savepoint. Suspended job \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; with a savepoint. Savepoint 已保存在 state.savepoints.dir 指定的路径中，该配置在 flink-conf.yaml 中定义，flink-conf.yaml 挂载在本机的 /tmp/flink-savepoints-directory/ 目录下。 在下一步操作中我们会用到这个 Savepoint 路径，如果我们是通过 REST API 操作的， 那么 Savepoint 路径会随着响应结果一起返回，我们可以直接查看文件系统来确认 Savepoint 保存情况。
命令
ls -lia /tmp/flink-savepoints-directory 预期输出
total 0 17 drwxr-xr-x 3 root root 60 17 jul 17:05 . 2 drwxrwxrwt 135 root root 3420 17 jul 17:09 .. 1002 drwxr-xr-x 2 root root 140 17 jul 17:05 savepoint-\u0026lt;short-job-id\u0026gt;-\u0026lt;uuid\u0026gt; REST API 请求
# 停止 Job curl -X POST localhost:8081/jobs/\u0026lt;job-id\u0026gt;/stop -d \u0026#39;{\u0026#34;drain\u0026#34;: false}\u0026#39; 预期响应 (结果已格式化)
{ \u0026#34;request-id\u0026#34;: \u0026#34;\u0026lt;trigger-id\u0026gt;\u0026#34; } 请求
# 检查停止结果并获取 savepoint 路径 curl localhost:8081/jobs/\u0026lt;job-id\u0026gt;/savepoints/\u0026lt;trigger-id\u0026gt; 预期响应 (结果已格式化)
{ \u0026#34;status\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;COMPLETED\u0026#34; }, \u0026#34;operation\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34; } } Step 2a: 重启 Job (不作任何变更) # 现在你可以从这个 Savepoint 重新启动待升级的 Job，为了简单起见，不对该 Job 作任何变更就直接重启。
CLI 命令
docker-compose run --no-deps client flink run -s \u0026lt;savepoint-path\u0026gt; \\ -d /opt/ClickCountJob.jar \\ --bootstrap.servers kafka:9092 --checkpointing --event-time 预期输出
Starting execution of program Job has been submitted with JobID \u0026lt;job-id\u0026gt; REST API 请求
# 从客户端容器上传 JAR docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\ -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload 预期响应 (结果已格式化)
{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } 请求
# 提交 Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\ -d \u0026#39;{\u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; 预期响应 (结果已格式化)
{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } 一旦该 Job 再次处于 RUNNING 状态，你将从 output Topic 中看到数据在快速输出， 因为刚启动的 Job 正在处理停止期间积压的大量数据。另外，你还会看到在升级期间 没有产生任何数据丢失：所有窗口都在输出 1000。
Step 2b: 重启 Job (修改并行度) # 在从 Savepoint 重启 Job 之前，你还可以通过修改并行度来达到扩容 Job 的目的。
CLI 命令
docker-compose run --no-deps client flink run -p 3 -s \u0026lt;savepoint-path\u0026gt; \\ -d /opt/ClickCountJob.jar \\ --bootstrap.servers kafka:9092 --checkpointing --event-time 预期输出
Starting execution of program Job has been submitted with JobID \u0026lt;job-id\u0026gt; REST API 请求
# Uploading the JAR from the Client container docker-compose run --no-deps client curl -X POST -H \u0026#34;Expect:\u0026#34; \\ -F \u0026#34;jarfile=@/opt/ClickCountJob.jar\u0026#34; http://jobmanager:8081/jars/upload 预期响应 (结果已格式化)
{ \u0026#34;filename\u0026#34;: \u0026#34;/tmp/flink-web-\u0026lt;uuid\u0026gt;/flink-web-upload/\u0026lt;jar-id\u0026gt;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } 请求
# 提交 Job curl -X POST http://localhost:8081/jars/\u0026lt;jar-id\u0026gt;/run \\ -d \u0026#39;{\u0026#34;parallelism\u0026#34;: 3, \u0026#34;programArgs\u0026#34;: \u0026#34;--bootstrap.servers kafka:9092 --checkpointing --event-time\u0026#34;, \u0026#34;savepointPath\u0026#34;: \u0026#34;\u0026lt;savepoint-path\u0026gt;\u0026#34;}\u0026#39; 预期响应 (结果已格式化)
{ \u0026#34;jobid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34; } 现在 Job 已重新提交，但由于我们提高了并行度所以导致 TaskSlots 不够用（1 个 TaskSlot 可用，总共需要 3 个），最终 Job 会重启失败。通过如下命令：
docker-compose scale taskmanager=2 你可以向 Flink 集群添加第二个 TaskManager（为 Flink 集群提供 2 个 TaskSlots 资源）， 它会自动向 JobManager 注册，TaskManager 注册完成后，Job 会再次处于 \u0026ldquo;RUNNING\u0026rdquo; 状态。
一旦 Job 再次运行起来，从 output Topic 的输出中你会看到在扩容期间数据依然没有丢失： 所有窗口的计数都正好是 1000。
查询 Job 指标 # 可以通过 JobManager 提供的 REST API 来获取系统和用户指标
具体请求方式取决于我们想查询哪类指标，Job 相关的指标分类可通过 jobs/\u0026lt;job-id\u0026gt;/metrics 获得，而要想查询某类指标的具体值则可以在请求地址后跟上 get 参数。
请求
curl \u0026#34;localhost:8081/jobs/\u0026lt;jod-id\u0026gt;/metrics?get=lastCheckpointSize\u0026#34; 预期响应 (结果已格式化且去除了占位符)
[ { \u0026#34;id\u0026#34;: \u0026#34;lastCheckpointSize\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;9378\u0026#34; } ] REST API 不仅可以用于查询指标，还可以用于获取正在运行中的 Job 详细信息。
请求
# 可以从结果中获取感兴趣的 vertex-id curl localhost:8081/jobs/\u0026lt;jod-id\u0026gt; 预期响应 (结果已格式化)
{ \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;isStoppable\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066026, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374793, \u0026#34;now\u0026#34;: 1564467440819, \u0026#34;timestamps\u0026#34;: { \u0026#34;CREATED\u0026#34;: 1564467066026, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;SUSPENDED\u0026#34;: 0, \u0026#34;FAILING\u0026#34;: 0, \u0026#34;CANCELLING\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 1564467066126, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;RESTARTING\u0026#34;: 0 }, \u0026#34;vertices\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066423, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374396, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 0, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 5033461, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 0, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 166351, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066469, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374350, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 5085332, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 316, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 166305, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 6, \u0026#34;write-records-complete\u0026#34;: true } }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;start-time\u0026#34;: 1564467066476, \u0026#34;end-time\u0026#34;: -1, \u0026#34;duration\u0026#34;: 374343, \u0026#34;tasks\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 2, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;metrics\u0026#34;: { \u0026#34;read-bytes\u0026#34;: 20668, \u0026#34;read-bytes-complete\u0026#34;: true, \u0026#34;write-bytes\u0026#34;: 0, \u0026#34;write-bytes-complete\u0026#34;: true, \u0026#34;read-records\u0026#34;: 6, \u0026#34;read-records-complete\u0026#34;: true, \u0026#34;write-records\u0026#34;: 0, \u0026#34;write-records-complete\u0026#34;: true } } ], \u0026#34;status-counts\u0026#34;: { \u0026#34;CREATED\u0026#34;: 0, \u0026#34;FINISHED\u0026#34;: 0, \u0026#34;DEPLOYING\u0026#34;: 0, \u0026#34;RUNNING\u0026#34;: 4, \u0026#34;CANCELING\u0026#34;: 0, \u0026#34;FAILED\u0026#34;: 0, \u0026#34;CANCELED\u0026#34;: 0, \u0026#34;RECONCILING\u0026#34;: 0, \u0026#34;SCHEDULED\u0026#34;: 0 }, \u0026#34;plan\u0026#34;: { \u0026#34;jid\u0026#34;: \u0026#34;\u0026lt;job-id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Click Event Count\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STREAMING\u0026#34;, \u0026#34;nodes\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEventStatistics Sink\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;FORWARD\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Counter\u0026#34;, \u0026#34;inputs\u0026#34;: [ { \u0026#34;num\u0026#34;: 0, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;ship_strategy\u0026#34;: \u0026#34;HASH\u0026#34;, \u0026#34;exchange\u0026#34;: \u0026#34;pipelined_bounded\u0026#34; } ], \u0026#34;optimizer_properties\u0026#34;: {} }, { \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;vertex-id\u0026gt;\u0026#34;, \u0026#34;parallelism\u0026#34;: 2, \u0026#34;operator\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;operator_strategy\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ClickEvent Source\u0026#34;, \u0026#34;optimizer_properties\u0026#34;: {} } ] } } 请查阅 REST API 参考，该参考上有完整的指标查询接口信息，包括如何查询不同种类的指标（例如 TaskManager 指标）。
Back to top
延伸拓展 # 你可能已经注意到了，Click Event Count 这个 Job 在启动时总是会带上 --checkpointing 和 --event-time 两个参数， 如果我们去除这两个参数，那么 Job 的行为也会随之改变。
--checkpointing 参数开启了 checkpoint 配置，checkpoint 是 Flink 容错机制的重要保证。 如果你没有开启 checkpoint，那么在 Job 失败与恢复这一节中，你将会看到数据丢失现象发生。
--event-time 参数开启了 Job 的 事件时间 机制，该机制会使用 ClickEvent 自带的时间戳进行统计。 如果不指定该参数，Flink 将结合当前机器时间使用事件处理时间进行统计。如此一来，每个窗口计数将不再是准确的 1000 了。
Click Event Count 这个 Job 还有另外一个选项，该选项默认是关闭的，你可以在 client 容器的 docker-compose.yaml 文件中添加该选项从而观察该 Job 在反压下的表现，该选项描述如下：
--backpressure 将一个额外算子添加到 Job 中，该算子会在偶数分钟内产生严重的反压（比如：10:12 期间，而 10:13 期间不会）。这种现象可以通过多种网络指标观察到，比如：outputQueueLength 和 outPoolUsage 指标，通过 WebUI 上的反压监控也可以观察到。 `}),e.add({id:163,href:"/flink/flink-docs-master/zh/docs/libs/gelly/graph_generators/",title:"Graph Generators",section:"Graphs",content:` Graph Generators # Gelly provides a collection of scalable graph generators. Each generator is
parallelizable, in order to create large datasets scale-free, generating the same graph regardless of parallelism thrifty, using as few operators as possible Graph generators are configured using the builder pattern. The parallelism of generator operators can be set explicitly by calling setParallelism(parallelism). Lowering the parallelism will reduce the allocation of memory and network buffers.
Graph-specific configuration must be called first, then configuration common to all generators, and lastly the call to generate(). The following example configures a grid graph with two dimensions, configures the parallelism, and generates the graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); boolean wrapEndpoints = false; int parallelism = 4; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new GridGraph(env) .addDimension(2, wrapEndpoints) .addDimension(4, wrapEndpoints) .setParallelism(parallelism) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.GridGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment wrapEndpoints = false val parallelism = 4 val graph = new GridGraph(env.getJavaEnv).addDimension(2, wrapEndpoints).addDimension(4, wrapEndpoints).setParallelism(parallelism).generate() Circulant Graph # A circulant graph is an oriented graph configured with one or more contiguous ranges of offsets. Edges connect integer vertex IDs whose difference equals a configured offset. The circulant graph with no offsets is the empty graph and the graph with the maximum range is the complete graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CirculantGraph(env, vertexCount) .addRange(1, 2) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CirculantGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CirculantGraph(env.getJavaEnv, vertexCount).addRange(1, 2).generate() Complete Graph # An undirected graph connecting every distinct pair of vertices.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CompleteGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CompleteGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CompleteGraph(env.getJavaEnv, vertexCount).generate() Cycle Graph # An undirected graph where the set of edges form a single cycle by connecting each vertex to two adjacent vertices in a chained loop.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new CycleGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.CycleGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new CycleGraph(env.getJavaEnv, vertexCount).generate() Echo Graph # An echo graph is a circulant graph with n vertices defined by the width of a single range of offsets centered at n/2. A vertex is connected to \u0026lsquo;far\u0026rsquo; vertices, which connect to \u0026rsquo;near\u0026rsquo; vertices, which connect to \u0026lsquo;far\u0026rsquo; vertices, \u0026hellip;.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; long vertexDegree = 2; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new EchoGraph(env, vertexCount, vertexDegree) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.EchoGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val vertexDegree = 2 val graph = new EchoGraph(env.getJavaEnv, vertexCount, vertexDegree).generate() Empty Graph # A graph containing no edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new EmptyGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.EmptyGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new EmptyGraph(env.getJavaEnv, vertexCount).generate() Grid Graph # An undirected graph connecting vertices in a regular tiling in one or more dimensions. Each dimension is configured separately. When the dimension size is at least three the endpoints are optionally connected by setting wrapEndpoints. Changing the following example to addDimension(4, true) would connect 0 to 3 and 4 to 7.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); boolean wrapEndpoints = false; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new GridGraph(env) .addDimension(2, wrapEndpoints) .addDimension(4, wrapEndpoints) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.GridGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val wrapEndpoints = false val graph = new GridGraph(env.getJavaEnv).addDimension(2, wrapEndpoints).addDimension(4, wrapEndpoints).generate() Hypercube Graph # An undirected graph where edges form an n-dimensional hypercube. Each vertex in a hypercube connects to one other vertex in each dimension.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long dimensions = 3; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new HypercubeGraph(env, dimensions) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.HypercubeGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val dimensions = 3 val graph = new HypercubeGraph(env.getJavaEnv, dimensions).generate() Path Graph # An undirected graph where the set of edges form a single path by connecting two endpoint vertices with degree 1 and all midpoint vertices with degree 2. A path graph can be formed by removing a single edge from a cycle graph.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 5; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new PathGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.PathGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 5 val graph = new PathGraph(env.getJavaEnv, vertexCount).generate() RMat Graph # A directed power-law multigraph generated using the Recursive Matrix (R-Mat) model.
RMat is a stochastic generator configured with a source of randomness implementing the RandomGenerableFactory interface. Provided implementations are JDKRandomGeneratorFactory and MersenneTwisterFactory. These generate an initial sequence of random values which are then used as seeds for generating the edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); RandomGenerableFactory\u0026lt;JDKRandomGenerator\u0026gt; rnd = new JDKRandomGeneratorFactory(); int vertexCount = 1 \u0026lt;\u0026lt; scale; int edgeCount = edgeFactor * vertexCount; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new RMatGraph\u0026lt;\u0026gt;(env, rnd, vertexCount, edgeCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.RMatGraph val env = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 1 \u0026lt;\u0026lt; scale val edgeCount = edgeFactor * vertexCount val graph = new RMatGraph(env.getJavaEnv, rnd, vertexCount, edgeCount).generate() The default RMat constants can be overridden as shown in the following example. The constants define the interdependence of bits from each generated edge\u0026rsquo;s source and target labels. The RMat noise can be enabled and progressively perturbs the constants while generating each edge.
The RMat generator can be configured to produce a simple graph by removing self-loops and duplicate edges. Symmetrization is performed either by a \u0026ldquo;clip-and-flip\u0026rdquo; throwing away the half matrix above the diagonal or a full \u0026ldquo;flip\u0026rdquo; preserving and mirroring all edges.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); RandomGenerableFactory\u0026lt;JDKRandomGenerator\u0026gt; rnd = new JDKRandomGeneratorFactory(); int vertexCount = 1 \u0026lt;\u0026lt; scale; int edgeCount = edgeFactor * vertexCount; boolean clipAndFlip = false; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new RMatGraph\u0026lt;\u0026gt;(env, rnd, vertexCount, edgeCount) .setConstants(0.57f, 0.19f, 0.19f) .setNoise(true, 0.10f) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.RMatGraph val env = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 1 \u0026lt;\u0026lt; scale val edgeCount = edgeFactor * vertexCount clipAndFlip = false val graph = new RMatGraph(env.getJavaEnv, rnd, vertexCount, edgeCount).setConstants(0.57f, 0.19f, 0.19f).setNoise(true, 0.10f).generate() Singleton Edge Graph # An undirected graph containing isolated two-paths where every vertex has degree 1.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexPairCount = 4; // note: configured with the number of vertex pairs Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new SingletonEdgeGraph(env, vertexPairCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.SingletonEdgeGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexPairCount = 4 // note: configured with the number of vertex pairs val graph = new SingletonEdgeGraph(env.getJavaEnv, vertexPairCount).generate() Star Graph # An undirected graph containing a single central vertex connected to all other leaf vertices.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); long vertexCount = 6; Graph\u0026lt;LongValue, NullValue, NullValue\u0026gt; graph = new StarGraph(env, vertexCount) .generate(); Scala import org.apache.flink.api.scala._ import org.apache.flink.graph.generator.StarGraph val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment val vertexCount = 6 val graph = new StarGraph(env.getJavaEnv, vertexCount).generate() Back to top
`}),e.add({id:164,href:"/flink/flink-docs-master/zh/docs/connectors/table/jdbc/",title:"JDBC",section:"Table API Connectors",content:` JDBC SQL 连接器 # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append \u0026amp; Upsert Mode
JDBC 连接器允许使用 JDBC 驱动向任意类型的关系型数据库读取或者写入数据。本文档描述了针对关系型数据库如何通过建立 JDBC 连接器来执行 SQL 查询。
如果在 DDL 中定义了主键，JDBC sink 将以 upsert 模式与外部系统交换 UPDATE/DELETE 消息；否则，它将以 append 模式与外部系统交换消息且不支持消费 UPDATE/DELETE 消息。
依赖 # In order to use the JDBC connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-jdbc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. JDBC 连接器不是二进制发行版的一部分，请查阅这里了解如何在集群运行中引用 JDBC 连接器。
在连接到具体数据库时，也需要对应的驱动依赖，目前支持的驱动如下：
Driver Group Id Artifact Id JAR MySQL mysql mysql-connector-java 下载 Oracle com.oracle.database.jdbc ojdbc8 下载 PostgreSQL org.postgresql postgresql 下载 Derby org.apache.derby derby 下载 当前，JDBC 连接器和驱动不在 Flink 二进制发布包中，请参阅这里了解在集群上执行时何连接它们。
如何创建 JDBC 表 # JDBC table 可以按如下定义：
-- 在 Flink SQL 中注册一张 MySQL 表 \u0026#39;users\u0026#39; CREATE TABLE MyUserTable ( id BIGINT, name STRING, age INT, status BOOLEAN, PRIMARY KEY (id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;users\u0026#39; ); -- 从另一张表 \u0026#34;T\u0026#34; 将数据写入到 JDBC 表中 INSERT INTO MyUserTable SELECT id, name, age, status FROM T; -- 查看 JDBC 表中的数据 SELECT id, name, age, status FROM MyUserTable; -- JDBC 表在时态表关联中作为维表 SELECT * FROM myTopic LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = MyUserTable.id; 连接器参数 # 参数 是否必填 默认值 类型 描述 connector 必填 (none) String 指定使用什么类型的连接器，这里应该是'jdbc'。 url 必填 (none) String JDBC 数据库 url。 table-name 必填 (none) String 连接到 JDBC 表的名称。 driver 可选 (none) String 用于连接到此 URL 的 JDBC 驱动类名，如果不设置，将自动从 URL 中推导。 username 可选 (none) String JDBC 用户名。如果指定了 'username' 和 'password' 中的任一参数，则两者必须都被指定。 password 可选 (none) String JDBC 密码。 connection.max-retry-timeout 可选 60s Duration 最大重试超时时间，以秒为单位且不应该小于 1 秒。 scan.partition.column 可选 (none) String 用于将输入进行分区的列名。请参阅下面的分区扫描部分了解更多详情。 scan.partition.num 可选 (none) Integer 分区数。 scan.partition.lower-bound 可选 (none) Integer 第一个分区的最小值。 scan.partition.upper-bound 可选 (none) Integer 最后一个分区的最大值。 scan.fetch-size 可选 0 Integer 每次循环读取时应该从数据库中获取的行数。如果指定的值为 '0'，则该配置项会被忽略。 scan.auto-commit 可选 true Boolean 在 JDBC 驱动程序上设置 auto-commit 标志， 它决定了每个语句是否在事务中自动提交。有些 JDBC 驱动程序，特别是 Postgres，可能需要将此设置为 false 以便流化结果。 lookup.cache.max-rows 可选 (none) Integer lookup cache 的最大行数，若超过该值，则最老的行记录将会过期。 默认情况下，lookup cache 是未开启的。请参阅下面的 Lookup Cache 部分了解更多详情。 lookup.cache.ttl 可选 (none) Duration lookup cache 中每一行记录的最大存活时间，若超过该时间，则最老的行记录将会过期。 默认情况下，lookup cache 是未开启的。请参阅下面的 Lookup Cache 部分了解更多详情。 lookup.cache.caching-missing-key 可选 true Boolean 标记缓存丢失的键，默认为true lookup.max-retries 可选 3 Integer 查询数据库失败的最大重试时间。 sink.buffer-flush.max-rows 可选 100 Integer flush 前缓存记录的最大值，可以设置为 '0' 来禁用它。 sink.buffer-flush.interval 可选 1s Duration flush 间隔时间，超过该时间后异步线程将 flush 数据。可以设置为 '0' 来禁用它。注意, 为了完全异步地处理缓存的 flush 事件，可以将 'sink.buffer-flush.max-rows' 设置为 '0' 并配置适当的 flush 时间间隔。 sink.max-retries 可选 3 Integer 写入记录到数据库失败后的最大重试次数。 sink.parallelism 可选 (none) Integer 用于定义 JDBC sink 算子的并行度。默认情况下，并行度是由框架决定：使用与上游链式算子相同的并行度。 特性 # 键处理 # 当写入数据到外部数据库时，Flink 会使用 DDL 中定义的主键。如果定义了主键，则连接器将以 upsert 模式工作，否则连接器将以 append 模式工作。
在 upsert 模式下，Flink 将根据主键判断插入新行或者更新已存在的行，这种方式可以确保幂等性。为了确保输出结果是符合预期的，推荐为表定义主键并且确保主键是底层数据库中表的唯一键或主键。在 append 模式下，Flink 会把所有记录解释为 INSERT 消息，如果违反了底层数据库中主键或者唯一约束，INSERT 插入可能会失败。
有关 PRIMARY KEY 语法的更多详细信息，请参见 CREATE TABLE DDL。
分区扫描 # 为了在并行 Source task 实例中加速读取数据，Flink 为 JDBC table 提供了分区扫描的特性。
如果下述分区扫描参数中的任一项被指定，则下述所有的分区扫描参数必须都被指定。这些参数描述了在多个 task 并行读取数据时如何对表进行分区。 scan.partition.column 必须是相关表中的数字、日期或时间戳列。注意，scan.partition.lower-bound 和 scan.partition.upper-bound 用于决定分区的起始位置和过滤表中的数据。如果是批处理作业，也可以在提交 flink 作业之前获取最大值和最小值。
scan.partition.column：输入用于进行分区的列名。 scan.partition.num：分区数。 scan.partition.lower-bound：第一个分区的最小值。 scan.partition.upper-bound：最后一个分区的最大值。 Lookup Cache # JDBC 连接器可以用在时态表关联中作为一个可 lookup 的 source (又称为维表)，当前只支持同步的查找模式。
默认情况下，lookup cache 是未启用的，你可以设置 lookup.cache.max-rows and lookup.cache.ttl 参数来启用。
lookup cache 的主要目的是用于提高时态表关联 JDBC 连接器的性能。默认情况下，lookup cache 不开启，所以所有请求都会发送到外部数据库。 当 lookup cache 被启用时，每个进程（即 TaskManager）将维护一个缓存。Flink 将优先查找缓存，只有当缓存未查找到时才向外部数据库发送请求，并使用返回的数据更新缓存。 当缓存命中最大缓存行 lookup.cache.max-rows 或当行超过最大存活时间 lookup.cache.ttl 时，缓存中最老的行将被设置为已过期。 缓存中的记录可能不是最新的，用户可以将 lookup.cache.ttl 设置为一个更小的值以获得更好的刷新数据，但这可能会增加发送到数据库的请求数。所以要做好吞吐量和正确性之间的平衡。
默认情况下，flink 会缓存主键的空查询结果，你可以通过将 lookup.cache.caching-missing-key 设置为 false 来切换行为。
幂等写入 # 如果在 DDL 中定义了主键，JDBC sink 将使用 upsert 语义而不是普通的 INSERT 语句。upsert 语义指的是如果底层数据库中存在违反唯一性约束，则原子地添加新行或更新现有行，这种方式确保了幂等性。
如果出现故障，Flink 作业会从上次成功的 checkpoint 恢复并重新处理，这可能导致在恢复过程中重复处理消息。强烈推荐使用 upsert 模式，因为如果需要重复处理记录，它有助于避免违反数据库主键约束和产生重复数据。
除了故障恢复场景外，数据源（kafka topic）也可能随着时间的推移自然地包含多个具有相同主键的记录，这使得 upsert 模式是用户期待的。
由于 upsert 没有标准的语法，因此下表描述了不同数据库的 DML 语法：
Database Upsert Grammar MySQL INSERT .. ON DUPLICATE KEY UPDATE .. Oracle MERGE INTO .. USING (..) ON (..) WHEN MATCHED THEN UPDATE SET (..) WHEN NOT MATCHED THEN INSERT (..) VALUES (..) PostgreSQL INSERT .. ON CONFLICT .. DO UPDATE SET .. JDBC Catalog # JdbcCatalog 允许用户通过 JDBC 协议将 Flink 连接到关系数据库。
目前，JDBC Catalog 有两个实现，即 Postgres Catalog 和 MySQL Catalog。目前支持如下 catalog 方法。其他方法目前尚不支持。
// Postgres Catalog \u0026amp; MySQL Catalog 支持的方法 databaseExists(String databaseName); listDatabases(); getDatabase(String databaseName); listTables(String databaseName); getTable(ObjectPath tablePath); tableExists(ObjectPath tablePath); 其他的 Catalog 方法现在尚不支持。
JDBC Catalog 的使用 # 本小节主要描述如果创建并使用 Postgres Catalog 或 MySQL Catalog。 请参阅 Dependencies 部分了解如何配置 JDBC 连接器和相应的驱动。
JDBC catalog 支持以下参数:
name：必填，catalog 的名称。 default-database：必填，默认要连接的数据库。 username：必填，Postgres/MySQL 账户的用户名。 password：必填，账户的密码。 base-url：必填，（不应该包含数据库名） 对于 Postgres Catalog base-url 应为 \u0026quot;jdbc:postgresql://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;\u0026quot; 的格式。 对于 MySQL Catalog base-url 应为 \u0026quot;jdbc:mysql://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;\u0026quot; 的格式。 SQL CREATE CATALOG my_catalog WITH( \u0026#39;type\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;default-database\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;username\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;password\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;base-url\u0026#39; = \u0026#39;...\u0026#39; ); USE CATALOG my_catalog; Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name = \u0026#34;my_catalog\u0026#34;; String defaultDatabase = \u0026#34;mydb\u0026#34;; String username = \u0026#34;...\u0026#34;; String password = \u0026#34;...\u0026#34;; String baseUrl = \u0026#34;...\u0026#34; JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog(\u0026#34;my_catalog\u0026#34;, catalog); // 设置 JdbcCatalog 为会话的当前 catalog tableEnv.useCatalog(\u0026#34;my_catalog\u0026#34;); Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(settings) val name = \u0026#34;my_catalog\u0026#34; val defaultDatabase = \u0026#34;mydb\u0026#34; val username = \u0026#34;...\u0026#34; val password = \u0026#34;...\u0026#34; val baseUrl = \u0026#34;...\u0026#34; val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl) tableEnv.registerCatalog(\u0026#34;my_catalog\u0026#34;, catalog) // 设置 JdbcCatalog 为会话的当前 catalog tableEnv.useCatalog(\u0026#34;my_catalog\u0026#34;) Python from pyflink.table.catalog import JdbcCatalog environment_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(environment_settings) name = \u0026#34;my_catalog\u0026#34; default_database = \u0026#34;mydb\u0026#34; username = \u0026#34;...\u0026#34; password = \u0026#34;...\u0026#34; base_url = \u0026#34;...\u0026#34; catalog = JdbcCatalog(name, default_database, username, password, base_url) t_env.register_catalog(\u0026#34;my_catalog\u0026#34;, catalog) # 设置 JdbcCatalog 为会话的当前 catalog t_env.use_catalog(\u0026#34;my_catalog\u0026#34;) YAML execution: ... current-catalog: my_catalog # 设置目标 JdbcCatalog 为会话的当前 catalog current-database: mydb catalogs: - name: my_catalog type: jdbc default-database: mydb username: ... password: ... base-url: ... JDBC Catalog for PostgreSQL # PostgreSQL 元空间映射 # 除了数据库之外，postgreSQL 还有一个额外的命名空间 schema。一个 Postgres 实例可以拥有多个数据库，每个数据库可以拥有多个 schema，其中一个 schema 默认名为 “public”，每个 schema 可以包含多张表。 在 Flink 中，当查询由 Postgres catalog 注册的表时，用户可以使用 schema_name.table_name 或只有 table_name，其中 schema_name 是可选的，默认值为 “public”。
因此，Flink Catalog 和 Postgres 之间的元空间映射如下：
Flink Catalog Metaspace Structure Postgres Metaspace Structure catalog name (defined in Flink only) N/A database name database name table name [schema_name.]table_name Flink 中的 Postgres 表的完整路径应该是 \u0026quot;\u0026lt;catalog\u0026gt;.\u0026lt;db\u0026gt;.\`\u0026lt;schema.table\u0026gt;\`\u0026quot;。如果指定了 schema，请注意需要转义 \u0026lt;schema.table\u0026gt;。
这里提供了一些访问 Postgres 表的例子：
-- 扫描 \u0026#39;public\u0026#39; schema（即默认 schema）中的 \u0026#39;test_table\u0026#39; 表，schema 名称可以省略 SELECT * FROM mypg.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- 扫描 \u0026#39;custom_schema\u0026#39; schema 中的 \u0026#39;test_table2\u0026#39; 表， -- 自定义 schema 不能省略，并且必须与表一起转义。 SELECT * FROM mypg.mydb.\`custom_schema.test_table2\` SELECT * FROM mydb.\`custom_schema.test_table2\`; SELECT * FROM \`custom_schema.test_table2\`; JDBC Catalog for MySQL # MySQL 元空间映射 # MySQL 实例中的数据库与 MySQL Catalog 注册的 catalog 下的数据库处于同一个映射层级。一个 MySQL 实例可以拥有多个数据库，每个数据库可以包含多张表。 在 Flink 中，当查询由 MySQL catalog 注册的表时，用户可以使用 database.table_name 或只使用 table_name，其中 database 是可选的，默认值为创建 MySQL Catalog 时指定的默认数据库。
因此，Flink Catalog 和 MySQL catalog 之间的元空间映射如下：
Flink Catalog Metaspace Structure MySQL Metaspace Structure catalog name (defined in Flink only) N/A database name database name table name table_name Flink 中的 MySQL 表的完整路径应该是 \u0026quot;\`\u0026lt;catalog\u0026gt;\`.\`\u0026lt;db\u0026gt;\`.\`\u0026lt;table\u0026gt;\`\u0026quot;。
这里提供了一些访问 MySQL 表的例子：
-- 扫描 默认数据库中的 \u0026#39;test_table\u0026#39; 表 SELECT * FROM mysql_catalog.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- 扫描 \u0026#39;given_database\u0026#39; 数据库中的 \u0026#39;test_table2\u0026#39; 表， SELECT * FROM mysql_catalog.given_database.test_table2; SELECT * FROM given_database.test_table2; 数据类型映射 # Flink 支持连接到多个使用方言（dialect）的数据库，如 MySQL、Oracle、PostgreSQL、Derby 等。其中，Derby 通常是用于测试目的。下表列出了从关系数据库数据类型到 Flink SQL 数据类型的类型映射，映射表可以使得在 Flink 中定义 JDBC 表更加简单。
MySQL type Oracle type PostgreSQL type Flink SQL type TINYINT TINYINT SMALLINT
TINYINT UNSIGNED SMALLINT
INT2
SMALLSERIAL
SERIAL2 SMALLINT INT
MEDIUMINT
SMALLINT UNSIGNED INTEGER
SERIAL INT BIGINT
INT UNSIGNED BIGINT
BIGSERIAL BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT BIGINT FLOAT BINARY_FLOAT REAL
FLOAT4 FLOAT DOUBLE
DOUBLE PRECISION BINARY_DOUBLE FLOAT8
DOUBLE PRECISION DOUBLE NUMERIC(p, s)
DECIMAL(p, s) SMALLINT
FLOAT(s)
DOUBLE PRECISION
REAL
NUMBER(p, s) NUMERIC(p, s)
DECIMAL(p, s) DECIMAL(p, s) BOOLEAN
TINYINT(1) BOOLEAN BOOLEAN DATE DATE DATE DATE TIME [(p)] DATE TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] DATETIME [(p)] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n)
VARCHAR(n)
TEXT CHAR(n)
VARCHAR(n)
CLOB CHAR(n)
CHARACTER(n)
VARCHAR(n)
CHARACTER VARYING(n)
TEXT STRING BINARY
VARBINARY
BLOB RAW(s)
BLOB BYTEA BYTES ARRAY ARRAY Back to top
`}),e.add({id:165,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state_backends/",title:"State Backends",section:"状态与容错",content:` State Backends # Flink 提供了多种 state backends，它用于指定状态的存储方式和位置。
状态可以位于 Java 的堆或堆外内存。取决于你的 state backend，Flink 也可以自己管理应用程序的状态。 为了让应用程序可以维护非常大的状态，Flink 可以自己管理内存（如果有必要可以溢写到磁盘）。 默认情况下，所有 Flink Job 会使用配置文件 flink-conf.yaml 中指定的 state backend。
但是，配置文件中指定的默认 state backend 会被 Job 中指定的 state backend 覆盖，如下所示。
关于可用的 state backend 更多详细信息，包括其优点、限制和配置参数等，请参阅部署和运维的相应部分。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(...); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(...) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(...) Back to top
`}),e.add({id:166,href:"/flink/flink-docs-master/zh/docs/internals/task_lifecycle/",title:"Task 生命周期",section:"内幕",content:` Task 生命周期 # Task 是 Flink 的基本执行单元。算子的每个并行实例都在 task 里执行。例如，一个并行度为 5 的算子，它的每个实例都由一个单独的 task 来执行。
StreamTask 是 Flink 流式计算引擎中所有不同 task 子类的基础。本文会深入讲解 StreamTask 生命周期的不同阶段，并阐述每个阶段的主要方法。
算子生命周期简介 # 因为 task 是算子并行实例的执行实体，所以它的生命周期跟算子的生命周期紧密联系在一起。因此，在深入介绍 StreamTask 生命周期之前，先简要介绍一下代表算子生命周期的基本方法。这些方法按调用的先后顺序如下所示。考虑到算子可能是用户自定义函数（UDF），因此我们在每个算子下也展示（以缩进的方式）了 UDF 生命周期中调用的各个方法。AbstractUdfStreamOperator 是所有执行 UDF 的算子的基类，如果算子继承了 AbstractUdfStreamOperator，那么这些方法都是可用的。
// 初始化阶段 OPERATOR::setup UDF::setRuntimeContext OPERATOR::initializeState OPERATOR::open UDF::open // 处理阶段（对每个 element 或 watermark 调用） OPERATOR::processElement UDF::run OPERATOR::processWatermark // checkpointing 阶段（对每个 checkpoint 异步调用） OPERATOR::snapshotState // 通知 operator 处理记录的过程结束 OPERATOR::finish // 结束阶段 OPERATOR::close UDF::close 简而言之，在算子初始化时调用 setup() 来初始化算子的特定设置，比如 RuntimeContext 和指标收集的数据结构。在这之后，算子通过 initializeState() 初始化状态，算子的所有初始化工作在 open() 方法中执行，比如在继承 AbstractUdfStreamOperator 的情况下，初始化用户自定义函数。
initializeState() 既包含在初始化过程中算子状态的初始化逻辑（比如注册 keyed 状态），又包含异常后从 checkpoint 中恢复原有状态的逻辑。在接下来的篇幅会进行更详细的介绍。 当所有初始化都完成之后，算子开始处理流入的数据。流入的数据可以分为三种类型：用户数据、watermark 和 checkpoint barriers。每种类型的数据都有单独的方法来处理。用户数据通过 processElement() 方法来处理，watermark 通过 processWatermark() 来处理，checkpoint barriers 会调用（异步）snapshotState() 方法触发 checkpoint。对于每个流入的数据，根据其类型调用上述相应的方法。注意，processElement() 方法也是用户自定义函数逻辑执行的地方，比如用户自定义 MapFunction 里的 map() 方法。
最后，在算子正常无故障的情况下（比如，如果流式数据是有限的，并且最后一个数据已经到达），会调用 finish() 方法结束算子并进行必要的清理工作（比如刷新所有缓冲数据，或发送处理结束的标记数据）。在这之后会调用 close() 方法来释放算子持有的资源（比如算子数据持有的本地内存）。
在作业失败或手动取消的情况下，会略过从算子异常位置到 close() 中间的所有步骤，直接跳到 close() 方法结束算子。
Checkpoints: 算子的 snapshotState() 方法是在收到 checkpoint barrier 后异步调用的。Checkpoint 在处理阶段执行，即算子打开之后，结束之前的这个阶段。这个方法的职责是存储算子的当前状态到一个特定的状态后端，当作业失败后恢复执行时会从这个后端恢复状态数据。下面我们简要描述了 Flink 的 checkpoint 机制，如果想了解更多 Flink checkpoint 相关的原理，可以读一读 数据流容错。
Task 生命周期 # 在上文对算子主要阶段的简介之后，本节将详细介绍 task 在集群执行期间是如何调用相关方法的。这里所说的阶段主要包含在 StreamTask 类的 invoke() 方法里。本文档后续内容将分成两个子章节，一节描述了 task 在正常无故障情况下的执行阶段（请参考常规执行），另一节（稍微简短的部分）描述了 task 取消之后的执行阶段（请参考中断执行），不管是手动取消还是其他原因（比如执行期间遇到异常）导致的取消。
常规执行 # Task 在没有中断的情况下执行到结束的阶段如下所示：
TASK::setInitialState TASK::invoke create basic utils (config, etc) and load the chain of operators setup-operators task-specific-init initialize-operator-states open-operators run finish-operators wait for the final checkponit completed (if enabled) close-operators task-specific-cleanup common-cleanup 如上所示，在恢复 task 配置和初始化一些重要的运行时参数之后，task 的下一步是读取 task 级别的初始状态。这一步在 setInitialState() 方法里完成，在下面两种情况尤其重要：
当 Task 从失败中恢复并从最近一次成功的 checkpoint 重启的时候 当 Task 从 savepoint 恢复的时候。 如果 task 是第一次执行的话，它的初始状态为空。
在恢复初始状态之后，task 进入到 invoke() 方法。在这里，首先调用 setup() 方法来初始化本地计算涉及到的每个算子，然后调用本地的 init() 方法来做特定 task 的初始化。这里所说的特定 task，取决于 task 的类型 (SourceTask、OneInputStreamTask 或 TwoInputStreamTask 等)。这一步可能会有所不同，但无论如何这是获取 task 范围内所需资源的地方。例如，OneInputStreamTask，代表期望一个单一输入流的 task，初始化与本地任务相关输入流的不同分区位置的连接。
在申请到必要的资源之后，不同算子和用户定义函数开始从上面读到的 task 范围状态数据里获取它们各自的状态值。这一部分是算子调用 initializeState() 完成的。每个有状态的算子都应该重写该方法，包含状态的初始化逻辑，既适用于作业第一次执行的场景，又适用于 task 从 checkpoint 或 savepoint 中恢复的场景。
现在 task 里的所有算子都已经被初始化了，每个算子里的 open()方法也通过 StreamTask 的 openAllOperators() 方法调用了。这个方法执行所有操作的初始化，比如在定时器服务里注册获取到的定时器。一个 task 可能会执行多个算子，即一个算子消费它之前算子的输出数据流。在这种情况下，open() 方法从最后一个算子调用到第一个算子，即最后一个算子的输出刚好也是整个 task 的输出。这样做是为了当第一个算子开始处理 task 的输入数据流时，所有下游算子已经准备接收它的输出数据了。
task 里多个连续算子的开启是从后往前依次执行。 现在 task 可以恢复执行，算子可以开始处理新输入的数据。在这里，特定 task 的 run() 方法会被调用。这个方法会一直运行直到没有更多输入数据进来（有限的数据流）或者 task 被取消了（人为的或其他的原因）。这里也是算子定义的 processElement() 方法和 processWatermark() 方法执行的地方。
在运行到完成的情况下，即没有更多的输入数据要处理，从run()方法退出后，task 进入关闭阶段。首先定时器服务停止注册任何新的定时器（比如从正在执行的定时器里注册），清理掉所有还未启动的定时器，并等待当前执行中的定时器运行结束。然后通过调用 finishAllOperators() 方法调用每个算子的 finish() 方法来通知所有参与计算的算子。然后所有缓存的输出数据会刷出去以便下游 task 处理。 如果开启了部分任务结束后继续 checkpoint 的功能，任务将 等待下一个 checkpoint 结束 来保证使用两阶段提交的算子可能最终提交所有的记录。 最终 task 通过调用每个算子的 close() 方法来尝试清理掉算子持有的所有资源。与我们之前提到的开启算子不同是，开启时从后往前依次调用 open()；而关闭时刚好相反，从前往后依次调用 close()。
task 里的多个连续算子的关闭是从前往后依次执行。 最后，当所有算子都已经关闭，所有资源都已被释放时，task 关掉它的定时器服务，进行特定 task 的清理操作，例如清理掉所有内部缓存，然后进行常规的 task 清理操作，包括关闭所有的输出管道，清理所有输出缓存等。
Checkpoints: 之前我们看到在执行 initializeState() 方法期间，在从异常失败中恢复的情况下，task 和它内部的所有算子函数都从最后一次成功的 checkpoint 数据里获取对应的状态信息。Flink 里的 checkpoint 是根据用户自定义的时间间隔周期执行的，并且在一个与主 task 线程不同的单独线程里执行。这也是我们没有把 checkpoint 过程涵盖在 task 生命周期的主要阶段里的原因。简而言之，Flink 作业的输入数据 source task 会定时插入一种叫 checkpoint barrier 的特殊数据，并跟正常数据一起从 source 流入到 sink。source task 在处于运行模式后发送这些 barrier，同时会假设 CheckpointCoordinator 也在运行。当 task 接收到这样的 barrier 之后，会通过 task 算子里的 snapshotState() 方法调度 checkpoint 线程执行具体任务。在 checkpoint 处理期间，task 依然可以接收输入数据，但是数据会被缓存起来，当 checkpoint 执行成功之后才会被处理和发送到下游算子。
中断执行 # 在前面的章节，我们描述的是运行直到完成的 task 生命周期。在任意时间点取消 task 的话，正常的执行过程会被中断，从这个时候开始只会进行以下操作，关闭定时器服务、执行特定 task 的清理、执行所有算子的关闭，执行常规 task 的清理。
Back to top
`}),e.add({id:167,href:"/flink/flink-docs-master/zh/docs/dev/configuration/testing/",title:"测试的依赖项",section:"项目配置",content:` 用于测试的依赖项 # Flink 提供了用于测试作业的实用程序，您可以将其添加为依赖项。
DataStream API 测试 # 如果要为使用 DataStream API 构建的作业开发测试用例，则需要添加以下依赖项：
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. testCompile "org.apache.flink:flink-test-utils:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. 在各种测试实用程序中，该模块提供了 MiniCluster （一个可配置的轻量级 Flink 集群，能在 JUnit 测试中运行），可以直接执行作业。
有关如何使用这些实用程序的更多细节，请查看 DataStream API 测试。
Table API 测试 # 如果您想在您的 IDE 中本地测试 Table API 和 SQL 程序，除了前述提到的 flink-test-utils 之外，您还要添加以下依赖项：
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. testCompile "org.apache.flink:flink-table-test-utils:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. 这将自动引入查询计划器和运行时，分别用于计划和执行查询。
flink-table-test-utils 模块已在 Flink 1.15 中引入，目前被认为是实验性的。 `}),e.add({id:168,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_trouble/",title:"常见问题",section:"内存配置",content:` 常见问题 # IllegalConfigurationException # 如果遇到从 TaskExecutorProcessUtils 或 JobManagerProcessUtils 抛出的 IllegalConfigurationException 异常，这通常说明您的配置参数中存在无效值（例如内存大小为负数、占比大于 1 等）或者配置冲突。 请根据异常信息，确认出错的内存部分的相关文档及配置信息。
OutOfMemoryError: Java heap space # 该异常说明 JVM 的堆空间过小。 可以通过增大总内存、TaskManager 的任务堆内存、JobManager 的 JVM 堆内存等方法来增大 JVM 堆空间。
提示 也可以增大 TaskManager 的框架堆内存。 这是一个进阶配置，只有在确认是 Flink 框架自身需要更多内存时才应该去调整。
OutOfMemoryError: Direct buffer memory # 该异常通常说明 JVM 的直接内存限制过小，或者存在直接内存泄漏（Direct Memory Leak）。 请确认用户代码及外部依赖中是否使用了 JVM 直接内存，以及如果使用了直接内存，是否配置了足够的内存空间。 可以通过调整堆外内存来增大直接内存限制。 有关堆外内存的配置方法，请参考 TaskManager、JobManager 以及 JVM 参数的相关文档。
OutOfMemoryError: Metaspace # 该异常说明 JVM Metaspace 限制过小。 可以尝试调整 TaskManager、JobManager 的 JVM Metaspace。
IOException: Insufficient number of network buffers # 该异常仅与 TaskManager 相关。
该异常通常说明网络内存过小。 可以通过调整以下配置参数增大网络内存：
taskmanager.memory.network.min taskmanager.memory.network.max taskmanager.memory.network.fraction 容器（Container）内存超用 # 如果 Flink 容器尝试分配超过其申请大小的内存（Yarn 或 Kubernetes），这通常说明 Flink 没有预留出足够的本地内存。 可以通过外部监控系统或者容器被部署环境杀掉时的错误信息判断是否存在容器内存超用。
对于 JobManager 进程，你还可以尝试启用 JVM 直接内存限制（jobmanager.memory.enable-jvm-direct-memory-limit），以排除 JVM 直接内存泄漏的可能性。
If RocksDBStateBackend is used：
and memory controlling is disabled: You can try to increase the TaskManager\u0026rsquo;s managed memory. and memory controlling is enabled and non-heap memory increases during savepoint or full checkpoints: This may happen due to the glibc memory allocator (see glibc bug). You can try to add the environment variable MALLOC_ARENA_MAX=1 for TaskManagers. 此外，还可以尝试增大 JVM 开销。
请参考如何配置容器内存。
`}),e.add({id:169,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-tvf/",title:"窗口函数",section:"Queries 查询",content:` Windowing table-valued functions (Windowing TVFs) # Batch Streaming
Windows are at the heart of processing infinite streams. Windows split the stream into “buckets” of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink SQL and how the programmer can benefit to the maximum from its offered functionality.
Apache Flink provides several window table-valued functions (TVF) to divide the elements of your table into windows, including:
Tumble Windows Hop Windows Cumulate Windows Session Windows (will be supported soon) Note that each element can logically belong to more than one window, depending on the windowing table-valued function you use. For example, HOP windowing creates overlapping windows wherein a single element can be assigned to multiple windows.
Windowing TVFs are Flink defined Polymorphic Table Functions (abbreviated PTF). PTF is part of the SQL 2016 standard, a special table-function, but can have a table as a parameter. PTF is a powerful feature to change the shape of a table. Because PTFs are used semantically like tables, their invocation occurs in a FROM clause of a SELECT statement.
Windowing TVFs is a replacement of legacy Grouped Window Functions. Windowing TVFs is more SQL standard compliant and more powerful to support complex window-based computations, e.g. Window TopN, Window Join. However, Grouped Window Functions can only support Window Aggregation.
See more how to apply further computations based on windowing TVF:
Window Aggregation Window TopN Window Join Window Deduplication Window Functions # Apache Flink provides 3 built-in windowing TVFs: TUMBLE, HOP and CUMULATE. The return value of windowing TVF is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. In streaming mode, the \u0026ldquo;window_time\u0026rdquo; field is a time attributes of the window. In batch mode, the \u0026ldquo;window_time\u0026rdquo; field is an attribute of type TIMESTAMP or TIMESTAMP_LTZ based on input time field type. The \u0026ldquo;window_time\u0026rdquo; field can be used in subsequent time-based operations, e.g. another windowing TVF, or interval joins, over aggregations. The value of window_time always equal to window_end - 1ms.
TUMBLE # The TUMBLE function assigns each element to a window of specified window size. Tumbling windows have a fixed size and do not overlap. For example, suppose you specify a tumbling window with a size of 5 minutes. In that case, Flink will evaluate the current window, and a new window started every five minutes, as illustrated by the following figure.
The TUMBLE function assigns a window for each row of a relation based on a time attribute field. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of TUMBLE is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after window TVF.
TUMBLE function takes three required parameters, one optional parameter:
TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ]) data: is a table parameter that can be any relation with a time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to tumbling windows. size: is a duration specifying the width of the tumbling windows. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+ | bidtime | price | item | +------------------+-------+------+ | 2020-04-15 08:05 | 4.00 | C | | 2020-04-15 08:07 | 2.00 | A | | 2020-04-15 08:09 | 5.00 | D | | 2020-04-15 08:11 | 3.00 | B | | 2020-04-15 08:13 | 1.00 | E | | 2020-04-15 08:17 | 6.00 | F | +------------------+-------+------+ Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the tumbling windowed table Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
HOP # The HOP function assigns elements to windows of fixed length. Like a TUMBLE windowing function, the size of the windows is configured by the window size parameter. An additional window slide parameter controls how frequently a hopping window is started. Hence, hopping windows can be overlapping if the slide is smaller than the window size. In this case, elements are assigned to multiple windows. Hopping windows are also known as \u0026ldquo;sliding windows\u0026rdquo;.
For example, you could have windows of size 10 minutes that slides by 5 minutes. With this, you get every 5 minutes a window that contains the events that arrived during the last 10 minutes, as depicted by the following figure.
The HOP function assigns windows that cover rows within the interval of size and shifting every slide based on a time attribute field. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of HOP is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after windowing TVF.
HOP takes four required parameters, one optional parameter:
HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ]) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to hopping windows. slide: is a duration specifying the duration between the start of sequential hopping windows size: is a duration specifying the width of the hopping windows. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
\u0026gt; SELECT * FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first \u0026gt; SELECT * FROM TABLE( HOP( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SLIDE =\u0026gt; INTERVAL \u0026#39;5\u0026#39; MINUTES, SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:05 | 2020-04-15 08:15 | 2020-04-15 08:14:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:15 | 2020-04-15 08:25 | 2020-04-15 08:24:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the hopping windowed table \u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | | 2020-04-15 08:15 | 2020-04-15 08:25 | 6.00 | +------------------+------------------+-------+ CUMULATE # Cumulating windows are very useful in some scenarios, such as tumbling windows with early firing in a fixed window interval. For example, a daily dashboard draws cumulative UVs from 00:00 to every minute, the UV at 10:00 represents the total number of UV from 00:00 to 10:00. This can be easily and efficiently implemented by CUMULATE windowing.
The CUMULATE function assigns elements to windows that cover rows within an initial interval of step size and expand to one more step size (keep window start fixed) every step until the max window size. You can think CUMULATE function as applying TUMBLE windowing with max window size first, and split each tumbling windows into several windows with same window start and window ends of step-size difference. So cumulating windows do overlap and don\u0026rsquo;t have a fixed size.
For example, you could have a cumulating window for 1 hour step and 1 day max size, and you will get windows: [00:00, 01:00), [00:00, 02:00), [00:00, 03:00), \u0026hellip;, [00:00, 24:00) for every day.
The CUMULATE functions assigns windows based on a time attribute column. In streaming mode, the time attribute field must be either event or processing time attributes. In batch mode, the time attribute field of window table function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ. The return value of CUMULATE is a new relation that includes all columns of original relation as well as additional 3 columns named \u0026ldquo;window_start\u0026rdquo;, \u0026ldquo;window_end\u0026rdquo;, \u0026ldquo;window_time\u0026rdquo; to indicate the assigned window. The original time attribute \u0026ldquo;timecol\u0026rdquo; will be a regular timestamp column after window TVF.
CUMULATE takes four required parameters, one optional parameter:
CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size) data: is a table parameter that can be any relation with an time attribute column. timecol: is a column descriptor indicating which time attributes column of data should be mapped to cumulating windows. step: is a duration specifying the increased window size between the end of sequential cumulating windows. size: is a duration specifying the max width of the cumulating windows. size must be an integral multiple of step. offset: is an optional parameter to specify the offset which window start would be shifted by. Here is an example invocation on the Bid table:
\u0026gt; SELECT * FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first \u0026gt; SELECT * FROM TABLE( CUMULATE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), STEP =\u0026gt; INTERVAL \u0026#39;2\u0026#39; MINUTES, SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:06 | 2020-04-15 08:05:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 | | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:08 | 2020-04-15 08:07:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:12 | 2020-04-15 08:11:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:16 | 2020-04-15 08:15:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:18 | 2020-04-15 08:17:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the cumulating windowed table \u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:06 | 4.00 | | 2020-04-15 08:00 | 2020-04-15 08:08 | 6.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:12 | 3.00 | | 2020-04-15 08:10 | 2020-04-15 08:14 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:16 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Window Offset # Offset is an optional parameter which could be used to change the window assignment. It could be positive duration and negative duration. Default values for window offset is 0. The same record maybe assigned to the different window if set different offset value. For example, which window would be assigned to for a record with timestamp 2021-06-30 00:00:04 for a Tumble window with 10 MINUTE as size?
If offset value is -16 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset value is -6 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset is -4 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). If offset is 0, the record assigns to window [2021-06-30 00:00:00, 2021-06-30 00:10:00). If offset is 4 MINUTE, the record assigns to window [2021-06-29 23:54:00, 2021-06-30 00:04:00). If offset is 6 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). If offset is 16 MINUTE, the record assigns to window [2021-06-29 23:56:00, 2021-06-30 00:06:00). We could find that, some windows offset parameters may have same effect on the assignment of windows. In the above case, -16 MINUTE, -6 MINUTE and 4 MINUTE have same effect for a Tumble window with 10 MINUTE as size. Note: The effect of window offset is just for updating window assignment, it has no effect on Watermark.
We show an example to describe how to use offset in Tumble window in the following SQL.
-- NOTE: Currently Flink doesn\u0026#39;t support evaluating individual window table-valued function, -- window table-valued function should be used with aggregate operation, -- this example is just used for explaining the syntax and the data produced by table-valued function. Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES, INTERVAL \u0026#39;1\u0026#39; MINUTES)); -- or with the named params -- note: the DATA param must be the first Flink SQL\u0026gt; SELECT * FROM TABLE( TUMBLE( DATA =\u0026gt; TABLE Bid, TIMECOL =\u0026gt; DESCRIPTOR(bidtime), SIZE =\u0026gt; INTERVAL \u0026#39;10\u0026#39; MINUTES, OFFSET =\u0026gt; INTERVAL \u0026#39;1\u0026#39; MINUTES)); +------------------+-------+------+------------------+------------------+-------------------------+ | bidtime | price | item | window_start | window_end | window_time | +------------------+-------+------+------------------+------------------+-------------------------+ | 2020-04-15 08:05 | 4.00 | C | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:07 | 2.00 | A | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:09 | 5.00 | D | 2020-04-15 08:01 | 2020-04-15 08:11 | 2020-04-15 08:10:59.999 | | 2020-04-15 08:11 | 3.00 | B | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | | 2020-04-15 08:13 | 1.00 | E | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | | 2020-04-15 08:17 | 6.00 | F | 2020-04-15 08:11 | 2020-04-15 08:21 | 2020-04-15 08:20:59.999 | +------------------+-------+------+------------------+------------------+-------------------------+ -- apply aggregation on the tumbling windowed table Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES, INTERVAL \u0026#39;1\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:01 | 2020-04-15 08:11 | 11.00 | | 2020-04-15 08:11 | 2020-04-15 08:21 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Back to top
`}),e.add({id:170,href:"/flink/flink-docs-master/zh/docs/deployment/ha/",title:"高可用",section:"Deployment",content:" "}),e.add({id:171,href:"/flink/flink-docs-master/zh/docs/learn-flink/event_driven/",title:"事件驱动应用",section:"实践练习",content:` 事件驱动应用 # 处理函数（Process Functions） # 简介 # ProcessFunction 将事件处理与 Timer，State 结合在一起，使其成为流处理应用的强大构建模块。 这是使用 Flink 创建事件驱动应用程序的基础。它和 RichFlatMapFunction 十分相似， 但是增加了 Timer。
示例 # 如果你已经体验了 流式分析训练 的动手实践， 你应该记得，它是采用 TumblingEventTimeWindow 来计算每个小时内每个司机的小费总和， 像下面的示例这样：
// 计算每个司机每小时的小费总和 DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .window(TumblingEventTimeWindows.of(Time.hours(1))) .process(new AddTips()); 使用 KeyedProcessFunction 去实现相同的操作更加直接且更有学习意义。 让我们开始用以下代码替换上面的代码：
// 计算每个司机每小时的小费总和 DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); 在这个代码片段中，一个名为 PseudoWindow 的 KeyedProcessFunction 被应用于 KeyedStream， 其结果是一个 DataStream\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; （与使用 Flink 内置时间窗口的实现生成的流相同）。
PseudoWindow 的总体轮廓示意如下：
// 在时长跨度为一小时的窗口中计算每个司机的小费总和。 // 司机ID作为 key。 public static class PseudoWindow extends KeyedProcessFunction\u0026lt;Long, TaxiFare, Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; { private final long durationMsec; public PseudoWindow(Time duration) { this.durationMsec = duration.toMilliseconds(); } @Override // 在初始化期间调用一次。 public void open(Configuration conf) { . . . } @Override // 每个票价事件（TaxiFare-Event）输入（到达）时调用，以处理输入的票价事件。 public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } @Override // 当当前水印（watermark）表明窗口现在需要完成的时候调用。 public void onTimer(long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { . . . } } 注意事项：
有几种类型的 ProcessFunctions \u0026ndash; 不仅包括 KeyedProcessFunction，还包括 CoProcessFunctions、BroadcastProcessFunctions 等.
KeyedProcessFunction 是一种 RichFunction。作为 RichFunction，它可以访问使用 Managed Keyed State 所需的 open 和 getRuntimeContext 方法。
有两个回调方法须要实现： processElement 和 onTimer。每个输入事件都会调用 processElement 方法； 当计时器触发时调用 onTimer。它们可以是基于事件时间（event time）的 timer，也可以是基于处理时间（processing time）的 timer。 除此之外，processElement 和 onTimer 都提供了一个上下文对象，该对象可用于与 TimerService 交互。 这两个回调还传递了一个可用于发出结果的 Collector。
open() 方法 # // 每个窗口都持有托管的 Keyed state 的入口，并且根据窗口的结束时间执行 keyed 策略。 // 每个司机都有一个单独的MapState对象。 private transient MapState\u0026lt;Long, Float\u0026gt; sumOfTips; @Override public void open(Configuration conf) { MapStateDescriptor\u0026lt;Long, Float\u0026gt; sumDesc = new MapStateDescriptor\u0026lt;\u0026gt;(\u0026#34;sumOfTips\u0026#34;, Long.class, Float.class); sumOfTips = getRuntimeContext().getMapState(sumDesc); } 由于票价事件（fare-event）可能会乱序到达，有时需要在计算输出前一个小时结果前，处理下一个小时的事件。 这样能够保证“乱序造成的延迟数据”得到正确处理（放到前一个小时中）。 实际上，如果 Watermark 延迟比窗口长度长得多，则可能有多个窗口同时打开，而不仅仅是两个。 此实现通过使用 MapState 来支持处理这一点，该 MapState 将每个窗口的结束时间戳映射到该窗口的小费总和。
processElement() 方法 # public void processElement( TaxiFare fare, Context ctx, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long eventTime = fare.getEventTime(); TimerService timerService = ctx.timerService(); if (eventTime \u0026lt;= timerService.currentWatermark()) { // 事件延迟；其对应的窗口已经触发。 } else { // 将 eventTime 向上取值并将结果赋值到包含当前事件的窗口的末尾时间点。 long endOfWindow = (eventTime - (eventTime % durationMsec) + durationMsec - 1); // 在窗口完成时将启用回调 timerService.registerEventTimeTimer(endOfWindow); // 将此票价的小费添加到该窗口的总计中。 Float sum = sumOfTips.get(endOfWindow); if (sum == null) { sum = 0.0F; } sum += fare.tip; sumOfTips.put(endOfWindow, sum); } } 需要考虑的事项：
延迟的事件怎么处理？watermark 后面的事件（即延迟的）正在被删除。 如果你想做一些比这更高级的操作，可以考虑使用旁路输出（Side outputs），这将在下一节中解释。
本例使用一个 MapState，其中 keys 是时间戳（timestamp），并为同一时间戳设置一个 Timer。 这是一种常见的模式；它使得在 Timer 触发时查找相关信息变得简单高效。
onTimer() 方法 # public void onTimer( long timestamp, OnTimerContext context, Collector\u0026lt;Tuple3\u0026lt;Long, Long, Float\u0026gt;\u0026gt; out) throws Exception { long driverId = context.getCurrentKey(); // 查找刚结束的一小时结果。 Float sumOfTips = this.sumOfTips.get(timestamp); Tuple3\u0026lt;Long, Long, Float\u0026gt; result = Tuple3.of(driverId, timestamp, sumOfTips); out.collect(result); this.sumOfTips.remove(timestamp); } 注意：
传递给 onTimer 的 OnTimerContext context 可用于确定当前 key。
我们的 pseudo-windows 在当前 Watermark 到达每小时结束时触发，此时调用 onTimer。 这个 onTimer 方法从 sumOfTips 中删除相关的条目，这样做的效果是不可能容纳延迟的事件。 这相当于在使用 Flink 的时间窗口时将 allowedLateness 设置为零。
性能考虑 # Flink 提供了为 RocksDB 优化的 MapState 和 ListState 类型。 相对于 ValueState，更建议使用 MapState 和 ListState，因为使用 RocksDBStateBackend 的情况下， MapState 和 ListState 比 ValueState 性能更好。 RocksDBStateBackend 可以附加到 ListState，而无需进行（反）序列化， 对于 MapState，每个 key/value 都是一个单独的 RocksDB 对象，因此可以有效地访问和更新 MapState。
Back to top
旁路输出（Side Outputs） # 简介 # 有几个很好的理由希望从 Flink 算子获得多个输出流，如下报告条目：
异常情况（exceptions） 格式错误的事件（malformed events） 延迟的事件（late events） operator 告警（operational alerts），如与外部服务的连接超时 旁路输出（Side outputs）是一种方便的方法。除了错误报告之外，旁路输出也是实现流的 n 路分割的好方法。
示例 # 现在你可以对上一节中忽略的延迟事件执行某些操作。
Side output channel 与 OutputTag\u0026lt;T\u0026gt; 相关联。这些标记拥有自己的名称，并与对应 DataStream 类型一致。
private static final OutputTag\u0026lt;TaxiFare\u0026gt; lateFares = new OutputTag\u0026lt;TaxiFare\u0026gt;(\u0026#34;lateFares\u0026#34;) {}; 上面显示的是一个静态 OutputTag\u0026lt;TaxiFare\u0026gt; ，当在 PseudoWindow 的 processElement 方法中发出延迟事件时，可以引用它：
if (eventTime \u0026lt;= timerService.currentWatermark()) { // 事件延迟，其对应的窗口已经触发。 ctx.output(lateFares, fare); } else { . . . } 以及当在作业的 main 中从该旁路输出访问流时：
// 计算每个司机每小时的小费总和 SingleOutputStreamOperator hourlyTips = fares .keyBy((TaxiFare fare) -\u0026gt; fare.driverId) .process(new PseudoWindow(Time.hours(1))); hourlyTips.getSideOutput(lateFares).print(); 或者，可以使用两个同名的 OutputTag 来引用同一个旁路输出，但如果这样做，它们必须具有相同的类型。
Back to top
结语 # 在本例中，你已经了解了如何使用 ProcessFunction 重新实现一个简单的时间窗口。 当然，如果 Flink 内置的窗口 API 能够满足你的开发需求，那么一定要优先使用它。 但如果你发现自己在考虑用 Flink 的窗口做些错综复杂的事情，不要害怕自己动手。
此外，ProcessFunctions 对于计算分析之外的许多其他用例也很有用。 下面的实践练习提供了一个完全不同的例子。
ProcessFunctions 的另一个常见用例是清理过时 State。如果你回想一下 Rides and Fares Exercise ， 其中使用 RichCoFlatMapFunction 来计算简单 Join，那么示例方案假设 TaxiRides 和 TaxiFares 两个事件是严格匹配为一个有效 数据对（必须同时出现）并且每一组这样的有效数据对都和一个唯一的 rideId 严格对应。如果数据对中的某个 TaxiRides 事件（TaxiFares 事件） 丢失，则同一 rideId 对应的另一个出现的 TaxiFares 事件（TaxiRides 事件）对应的 State 则永远不会被清理掉。 所以这里可以使用 KeyedCoProcessFunction 的实现代替它（RichCoFlatMapFunction），并且可以使用计时器来检测和清除任何过时 的 State。
Back to top
实践练习 # 本节的实践练习是 Long Ride Alerts Exercise .
Back to top
延伸阅读 # 处理函数（ProcessFunction） 旁路输出（Side Outputs） Back to top
`}),e.add({id:172,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/filesystem/",title:"文件系统",section:"DataStream Connectors",content:` 文件系统 # 连接器提供了 BATCH 模式和 STREAMING 模式统一的 Source 和 Sink。Flink FileSystem abstraction 支持连接器对文件系统进行（分区）文件读写。文件系统连接器为 BATCH 和 STREAMING 模式提供了相同的保证，而且对 STREAMING 模式执行提供了精确一次（exactly-once）语义保证。
连接器支持对任意（分布式的）文件系统（例如，POSIX、 S3、 HDFS）以某种数据格式 format (例如，Avro、 CSV、 Parquet) 对文件进行写入，或者读取后生成数据流或一组记录。
File Source # File Source 是基于 Source API 同时支持批模式和流模式文件读取的统一 Source。 File Source 分为以下两个部分：SplitEnumerator 和 SourceReader。
SplitEnumerator 负责发现和识别需要读取的文件，并将这些文件分配给 SourceReader 进行读取。 SourceReader 请求需要处理的文件，并从文件系统中读取该文件。 可能需要指定某种 format 与 File Source 联合进行解析 CSV、解码AVRO、或者读取 Parquet 列式文件。
有界流和无界流 # 有界的 File Source（通过 SplitEnumerator）列出所有文件（一个过滤出隐藏文件的递归目录列表）并读取。
无界的 File Source 由配置定期扫描文件的 enumerator 创建。 在无界的情况下，SplitEnumerator 将像有界的 File Source 一样列出所有文件，但是不同的是，经过一个时间间隔之后，重复上述操作。 对于每一次列举操作，SplitEnumerator 会过滤掉之前已经检测过的文件，将新扫描到的文件发送给 SourceReader。
使用方法 # 可以通过调用以下 API 建立一个 File Source：
Java // 从文件流中读取文件内容 FileSource.forRecordStreamFormat(StreamFormat,Path...); // 从文件中一次读取一批记录 FileSource.forBulkFileFormat(BulkFormat,Path...); Python # 从文件流中读取文件内容 FileSource.for_record_stream_format(stream_format, *path) # 从文件中一次读取一批记录 FileSource.for_bulk_file_format(bulk_format, *path) 可以通过创建 FileSource.FileSourceBuilder 设置 File Source 的所有参数。
对于有界/批的使用场景，File Source 需要处理给定路径下的所有文件。 对于无界/流的使用场景，File Source 会定期检查路径下的新文件并读取。
当创建一个 File Source 时（通过上述任意方法创建的 FileSource.FileSourceBuilder）， 默认情况下，Source 为有界/批的模式。可以调用 AbstractFileSource.AbstractFileSourceBuilder.monitorContinuously(Duration) 设置 Source 为持续的流模式。
Java final FileSource\u0026lt;String\u0026gt; source = FileSource.forRecordStreamFormat(...) .monitorContinuously(Duration.ofMillis(5)) .build(); Python source = FileSource.for_record_stream_format(...) \\ .monitor_continously(Duration.of_millis(5)) \\ .build() Format Types # 通过 file formats 定义的文件 readers 读取每个文件。 其中定义了解析和读取文件内容的逻辑。Source 支持多个解析类。 这些接口是实现简单性和灵活性/效率之间的折衷。
StreamFormat 从文件流中读取文件内容。它是最简单的格式实现， 并且提供了许多拆箱即用的特性（如 Checkpoint 逻辑），但是限制了可应用的优化（例如对象重用，批处理等等）。
BulkFormat 从文件中一次读取一批记录。 它虽然是最 \u0026ldquo;底层\u0026rdquo; 的格式实现，但是提供了优化实现的最大灵活性。
TextLine Format # 使用 StreamFormat 格式化文件中的文本行。 Java 中内置的 InputStreamReader 对使用了支持各种字符集的字节流进行解码。 此格式不支持从 Checkpoint 进行恢复优化。在恢复时，将重新读取并放弃在最后一个 Checkpoint 之前处理的行数。 这是由于无法通过字符集解码器追踪文件中的行偏移量，及其内部缓冲输入流和字符集解码器的状态。
SimpleStreamFormat 抽象类 # 这是 StreamFormat 的简单版本，适用于不可拆分的格式。 可以通过实现 SimpleStreamFormat 接口自定义读取数组或文件：
Java private static final class ArrayReaderFormat extends SimpleStreamFormat\u0026lt;byte[]\u0026gt; { private static final long serialVersionUID = 1L; @Override public Reader\u0026lt;byte[]\u0026gt; createReader(Configuration config, FSDataInputStream stream) throws IOException { return new ArrayReader(stream); } @Override public TypeInformation\u0026lt;byte[]\u0026gt; getProducedType() { return PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO; } } final FileSource\u0026lt;byte[]\u0026gt; source = FileSource.forRecordStreamFormat(new ArrayReaderFormat(), path).build(); CsvReaderFormat 是一个实现 SimpleStreamFormat 接口的例子。类似这样进行初始化：
CsvReaderFormat\u0026lt;SomePojo\u0026gt; csvFormat = CsvReaderFormat.forPojo(SomePojo.class); FileSource\u0026lt;SomePojo\u0026gt; source = FileSource.forRecordStreamFormat(csvFormat, Path.fromLocalFile(...)).build(); 对于 CSV Format 的解析，在这个例子中，是根据使用 Jackson 库的 SomePojo 的字段自动生成的。（注意：可能需要添加 @JsonPropertyOrder({field1, field2, ...}) 这个注释到自定义的类上，并且字段顺序与 CSV 文件列的顺序完全匹配)。
如果需要对 CSV 模式或解析选项进行更细粒度的控制，可以使用 CsvReaderFormat 的更底层的 forSchema 静态工厂方法：
CsvReaderFormat\u0026lt;T\u0026gt; forSchema(Supplier\u0026lt;CsvMapper\u0026gt; mapperFactory, Function\u0026lt;CsvMapper, CsvSchema\u0026gt; schemaGenerator, TypeInformation\u0026lt;T\u0026gt; typeInformation) Bulk Format # BulkFormat 一次读取并解析一批记录。BulkFormat 的实现包括 ORC Format或 Parquet Format等。 外部的 BulkFormat 类主要充当 reader 的配置持有者和工厂角色。BulkFormat.Reader 是在 BulkFormat#createReader(Configuration, FileSourceSplit) 方法中创建的，然后完成读取操作。如果在流的 checkpoint 执行期间基于 checkpoint 创建 Bulk reader，那么 reader 是在 BulkFormat#restoreReader(Configuration, FileSourceSplit) 方法中重新创建的。
可以通过将 SimpleStreamFormat 包装在 StreamFormatAdapter 中转换为 BulkFormat：
BulkFormat\u0026lt;SomePojo, FileSourceSplit\u0026gt; bulkFormat = new StreamFormatAdapter\u0026lt;\u0026gt;(CsvReaderFormat.forPojo(SomePojo.class)); 自定义文件枚举类 # Java /** * 针对 Hive 数据源的 FileEnumerator 实现类，基于 HiveTablePartition 生成拆分文件 */ public class HiveSourceFileEnumerator implements FileEnumerator { // 构造方法 public HiveSourceFileEnumerator(...) { ... } /*** * 拆分给定路径下的所有相关文件。{@code * minDesiredSplits} 是一个可选项，代表需要多少次拆分才能正确利用并行度 */ @Override public Collection\u0026lt;FileSourceSplit\u0026gt; enumerateSplits(Path[] paths, int minDesiredSplits) throws IOException { // createInputSplits:splitting files into fragmented collections return new ArrayList\u0026lt;\u0026gt;(createInputSplits(...)); } ... /*** * 创建 HiveSourceFileEnumerator 的工厂 */ public static class Provider implements FileEnumerator.Provider { ... @Override public FileEnumerator create() { return new HiveSourceFileEnumerator(...); } } } // 使用自定义文件枚举类 new HiveSource\u0026lt;\u0026gt;( ..., new HiveSourceFileEnumerator.Provider( partitions != null ? partitions : Collections.emptyList(), new JobConfWrapper(jobConf)), ...); 当前限制 # 对于大量积压的文件，Watermark 效果不佳。这是因为 Watermark 急于在一个文件中推进，而下一个文件可能包含比 Watermark 更晚的数据。
对于无界 File Sources，枚举器会会将当前所有已处理文件的路径记录到 state 中，在某些情况下，这可能会导致状态变得相当大。 未来计划将引入一种压缩的方式来跟踪已经处理的文件（例如，将修改时间戳保持在边界以下）。
后记 # 如果对新设计的 Source API 中的 File Sources 是如何工作的感兴趣，可以阅读本部分作为参考。关于新的 Source API 的更多细节，请参考 documentation on data sources 和在 FLIP-27 中获取更加具体的讨论详情。 File Sink # File Sink 将传入的数据写入存储桶中。考虑到输入流可以是无界的，每个桶中的数据被组织成有限大小的 Part 文件。 完全可以配置为基于时间的方式往桶中写入数据，比如可以设置每个小时的数据写入一个新桶中。这意味着桶中将包含一个小时间隔内接收到的记录。
桶目录中的数据被拆分成多个 Part 文件。对于相应的接收数据的桶的 Sink 的每个 Subtask，每个桶将至少包含一个 Part 文件。将根据配置的滚动策略来创建其他 Part 文件。 对于 Row-encoded Formats（参考 Format Types）默认的策略是根据 Part 文件大小进行滚动，需要指定文件打开状态最长时间的超时以及文件关闭后的非活动状态的超时时间。 对于 Bulk-encoded Formats 在每次创建 Checkpoint 时进行滚动，并且用户也可以添加基于大小或者时间等的其他条件。
重要: 在 STREAMING 模式下使用 FileSink 需要开启 Checkpoint 功能。 文件只在 Checkpoint 成功时生成。如果没有开启 Checkpoint 功能，文件将永远停留在 in-progress 或者 pending 的状态，并且下游系统将不能安全读取该文件数据。 Format Types # FileSink 不仅支持 Row-encoded 也支持 Bulk-encoded，例如 Apache Parquet。 这两种格式可以通过如下的静态方法进行构造：
Row-encoded sink: FileSink.forRowFormat(basePath, rowEncoder) Bulk-encoded sink: FileSink.forBulkFormat(basePath, bulkWriterFactory) 不论创建 Row-encoded Format 或者 Bulk-encoded Format 的 Sink 时，都必须指定桶的路径以及对数据进行编码的逻辑。
请参考 JavaDoc 文档 FileSink 来获取所有的配置选项以及更多的不同数据格式实现的详细信息。
Row-encoded Formats # Row-encoded Format 需要指定一个 Encoder，在输出数据到文件过程中被用来将单个行数据序列化为 OutputStream。
除了 bucket assigner，RowFormatBuilder 还允许用户指定以下属性：
Custom RollingPolicy ：自定义滚动策略覆盖 DefaultRollingPolicy bucketCheckInterval (默认值 = 1 min) ：基于滚动策略设置的检查时间间隔 写入字符串的基本用法如下：
Java import org.apache.flink.api.common.serialization.SimpleStringEncoder; import org.apache.flink.core.fs.Path; import org.apache.flink.configuration.MemorySize; import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy; import java.time.Duration; DataStream\u0026lt;String\u0026gt; input = ...; final FileSink\u0026lt;String\u0026gt; sink = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder\u0026lt;String\u0026gt;(\u0026#34;UTF-8\u0026#34;)) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(Duration.ofMinutes(15)) .withInactivityInterval(Duration.ofMinutes(5)) .withMaxPartSize(MemorySize.ofMebiBytes(1024)) .build()) .build(); input.sinkTo(sink); Scala import org.apache.flink.api.common.serialization.SimpleStringEncoder import org.apache.flink.core.fs.Path import org.apache.flink.configuration.MemorySize import org.apache.flink.connector.file.sink.FileSink import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy import java.time.Duration val input: DataStream[String] = ... val sink: FileSink[String] = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder[String](\u0026#34;UTF-8\u0026#34;)) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(Duration.ofMinutes(15)) .withInactivityInterval(Duration.ofMinutes(5)) .withMaxPartSize(MemorySize.ofMebiBytes(1024)) .build()) .build() input.sinkTo(sink) Python data_stream = ... sink = FileSink \\ .for_row_format(OUTPUT_PATH, Encoder.simple_string_encoder(\u0026#34;UTF-8\u0026#34;)) \\ .with_rolling_policy(RollingPolicy.default_rolling_policy( part_size=1024 ** 3, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)) \\ .build() data_stream.sink_to(sink) 这个例子中创建了一个简单的 Sink，默认的将记录分配给小时桶。 例子中还指定了滚动策略，当满足以下三个条件的任何一个时都会将 In-progress 状态文件进行滚动：
包含了至少15分钟的数据量 从没接收延时5分钟之外的新纪录 文件大小已经达到 1GB（写入最后一条记录之后） Bulk-encoded Formats # Bulk-encoded 的 Sink 的创建和 Row-encoded 的相似，但不需要指定 Encoder，而是需要指定 BulkWriter.Factory，请参考文档 BulkWriter.Factory 。 BulkWriter 定义了如何添加和刷新新数据以及如何最终确定一批记录使用哪种编码字符集的逻辑。
Flink 内置了5种 BulkWriter 工厂类：
ParquetWriterFactory AvroWriterFactory SequenceFileWriterFactory CompressWriterFactory OrcBulkWriterFactory 重要 Bulk-encoded Format 仅支持一种继承了 CheckpointRollingPolicy 类的滚动策略。 在每个 Checkpoint 都会滚动。另外也可以根据大小或处理时间进行滚动。 Parquet Format # Flink 内置了为 Avro Format 数据创建 Parquet 写入工厂的快捷方法。在 AvroParquetWriters 类中可以发现那些方法以及相关的使用说明。
为了让 Parquet Format 数据写入更加通用，用户需要创建 ParquetWriterFactory 并且自定义实现 ParquetBuilder 接口。
如果在程序中使用 Parquet 的 Bulk-encoded Format，需要添加如下依赖到项目中：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-parquet_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 Parquet format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 类似这样使用 FileSink 写入 Parquet Format 的 Avro 数据：
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.avro.AvroParquetWriters; import org.apache.avro.Schema; Schema schema = ...; DataStream\u0026lt;GenericRecord\u0026gt; input = ...; final FileSink\u0026lt;GenericRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, AvroParquetWriters.forGenericRecord(schema)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.avro.AvroParquetWriters import org.apache.avro.Schema val schema: Schema = ... val input: DataStream[GenericRecord] = ... val sink: FileSink[GenericRecord] = FileSink .forBulkFormat(outputBasePath, AvroParquetWriters.forGenericRecord(schema)) .build() input.sinkTo(sink) Python schema = AvroSchema.parse_string(JSON_SCHEMA) # data_stream 的数据类型可以为符合 schema 的原生 Python 数据结构，其类型为默认的 Types.PICKLED_BYTE_ARRAY() data_stream = ... avro_type_info = GenericRecordAvroTypeInfo(schema) sink = FileSink \\ .for_bulk_format(OUTPUT_BASE_PATH, AvroParquetWriters.for_generic_record(schema)) \\ .build() # 必须通过 map 操作来指定其 Avro 类型信息，用于数据的序列化 data_stream.map(lambda e: e, output_type=avro_type_info).sink_to(sink) 类似这样使用 FileSink 写入 Parquet Format 的 Protobuf 数据：
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters; // ProtoRecord 是一个生成 protobuf 的类 DataStream\u0026lt;ProtoRecord\u0026gt; input = ...; final FileSink\u0026lt;ProtoRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, ParquetProtoWriters.forType(ProtoRecord.class)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.parquet.protobuf.ParquetProtoWriters // ProtoRecord 是一个生成 protobuf 的类 val input: DataStream[ProtoRecord] = ... val sink: FileSink[ProtoRecord] = FileSink .forBulkFormat(outputBasePath, ParquetProtoWriters.forType(classOf[ProtoRecord])) .build() input.sinkTo(sink) PyFlink 用户可以使用 ParquetBulkWriters 来创建一个将 Row 数据写入 Parquet 文件的 BulkWriterFactory 。
row_type = DataTypes.ROW([ DataTypes.FIELD(\u0026#39;string\u0026#39;, DataTypes.STRING()), DataTypes.FIELD(\u0026#39;int_array\u0026#39;, DataTypes.ARRAY(DataTypes.INT())) ]) sink = FileSink.for_bulk_format( OUTPUT_DIR, ParquetBulkWriters.for_row_type( row_type, hadoop_config=Configuration(), utc_timestamp=True, ) ).build() ds.sink_to(sink) Avro Format # Flink 也支持写入数据到 Avro Format 文件。在 AvroWriters 类中可以发现一系列创建 Avro writer 工厂的便利方法及其相关说明。
如果在程序中使用 AvroWriters，需要添加如下依赖到项目中：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-avro\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 Avro format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 类似这样使用 FileSink 写入数据到 Avro Format 文件中：
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.avro.AvroWriters; import org.apache.avro.Schema; Schema schema = ...; DataStream\u0026lt;GenericRecord\u0026gt; input = ...; final FileSink\u0026lt;GenericRecord\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, AvroWriters.forGenericRecord(schema)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.formats.avro.AvroWriters import org.apache.avro.Schema val schema: Schema = ... val input: DataStream[GenericRecord] = ... val sink: FileSink[GenericRecord] = FileSink .forBulkFormat(outputBasePath, AvroWriters.forGenericRecord(schema)) .build() input.sinkTo(sink) Python schema = AvroSchema.parse_string(JSON_SCHEMA) # data_stream 的数据类型可以为符合 schema 的原生 Python 数据结构，其类型为默认的 Types.PICKLED_BYTE_ARRAY() data_stream = ... avro_type_info = GenericRecordAvroTypeInfo(schema) sink = FileSink \\ .for_bulk_format(OUTPUT_BASE_PATH, AvroBulkWriters.for_generic_record(schema)) \\ .build() # 必须通过 map 操作来指定其 Avro 类型信息，用于数据的序列化 data_stream.map(lambda e: e, output_type=avro_type_info).sink_to(sink) 对于自定义创建的 Avro writers，例如，支持压缩功能，用户需要创建 AvroWriterFactory 并且自定义实现 AvroBuilder 接口:
Java AvroWriterFactory\u0026lt;?\u0026gt; factory = new AvroWriterFactory\u0026lt;\u0026gt;((AvroBuilder\u0026lt;Address\u0026gt;) out -\u0026gt; { Schema schema = ReflectData.get().getSchema(Address.class); DatumWriter\u0026lt;Address\u0026gt; datumWriter = new ReflectDatumWriter\u0026lt;\u0026gt;(schema); DataFileWriter\u0026lt;Address\u0026gt; dataFileWriter = new DataFileWriter\u0026lt;\u0026gt;(datumWriter); dataFileWriter.setCodec(CodecFactory.snappyCodec()); dataFileWriter.create(schema, out); return dataFileWriter; }); DataStream\u0026lt;Address\u0026gt; stream = ... stream.sinkTo(FileSink.forBulkFormat( outputBasePath, factory).build()); Scala val factory = new AvroWriterFactory[Address](new AvroBuilder[Address]() { override def createWriter(out: OutputStream): DataFileWriter[Address] = { val schema = ReflectData.get.getSchema(classOf[Address]) val datumWriter = new ReflectDatumWriter[Address](schema) val dataFileWriter = new DataFileWriter[Address](datumWriter) dataFileWriter.setCodec(CodecFactory.snappyCodec) dataFileWriter.create(schema, out) dataFileWriter } }) val stream: DataStream[Address] = ... stream.sinkTo(FileSink.forBulkFormat( outputBasePath, factory).build()); ORC Format # ORC Format 的数据采用 Bulk-encoded Format，Flink 提供了 Vectorizer 接口的具体实现类 OrcBulkWriterFactory。
像其他列格式一样也是采用 Bulk-encoded Format，Flink 中 OrcBulkWriter 是使用 ORC 的 VectorizedRowBatch 实现批的方式输出数据的。
由于输入数据已经被转换成了 VectorizedRowBatch，所以用户必须继承抽象类 Vectorizer 并且覆写类中 vectorize(T element, VectorizedRowBatch batch) 这个方法。正如看到的那样，此方法中提供了用户直接使用的 VectorizedRowBatch 类的实例，因此，用户不得不编写从输入 element 到 ColumnVectors 的转换逻辑，然后设置在 VectorizedRowBatch 实例中。
例如，如果是 Person 类型的输入元素，如下所示：
Java class Person { private final String name; private final int age; ... } 然后，转换 Person 类型元素的实现并在 VectorizedRowBatch 中设置，如下所示：
Java import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector; import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector; import java.io.IOException; import java.io.Serializable; import java.nio.charset.StandardCharsets; public class PersonVectorizer extends Vectorizer\u0026lt;Person\u0026gt; implements Serializable {	public PersonVectorizer(String schema) { super(schema); } @Override public void vectorize(Person element, VectorizedRowBatch batch) throws IOException { BytesColumnVector nameColVector = (BytesColumnVector) batch.cols[0]; LongColumnVector ageColVector = (LongColumnVector) batch.cols[1]; int row = batch.size++; nameColVector.setVal(row, element.getName().getBytes(StandardCharsets.UTF_8)); ageColVector.vector[row] = element.getAge(); } } Scala import java.nio.charset.StandardCharsets import org.apache.hadoop.hive.ql.exec.vector.{BytesColumnVector, LongColumnVector} class PersonVectorizer(schema: String) extends Vectorizer[Person](schema) { override def vectorize(element: Person, batch: VectorizedRowBatch): Unit = { val nameColVector = batch.cols(0).asInstanceOf[BytesColumnVector] val ageColVector = batch.cols(1).asInstanceOf[LongColumnVector] nameColVector.setVal(batch.size + 1, element.getName.getBytes(StandardCharsets.UTF_8)) ageColVector.vector(batch.size + 1) = element.getAge } } 如果在程序中使用 ORC 的 Bulk-encoded Format，需要添加如下依赖到项目中：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-orc_2.12\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 然后，类似这样使用 FileSink 以 ORC Format 输出数据：
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.orc.writer.OrcBulkWriterFactory; String schema = \u0026#34;struct\u0026lt;_col0:string,_col1:int\u0026gt;\u0026#34;; DataStream\u0026lt;Person\u0026gt; input = ...; final OrcBulkWriterFactory\u0026lt;Person\u0026gt; writerFactory = new OrcBulkWriterFactory\u0026lt;\u0026gt;(new PersonVectorizer(schema)); final FileSink\u0026lt;Person\u0026gt; sink = FileSink .forBulkFormat(outputBasePath, writerFactory) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.orc.writer.OrcBulkWriterFactory val schema: String = \u0026#34;struct\u0026lt;_col0:string,_col1:int\u0026gt;\u0026#34; val input: DataStream[Person] = ... val writerFactory = new OrcBulkWriterFactory(new PersonVectorizer(schema)); val sink: FileSink[Person] = FileSink .forBulkFormat(outputBasePath, writerFactory) .build() input.sinkTo(sink) OrcBulkWriterFactory 还可以采用 Hadoop 的 Configuration 和 Properties，这样就可以提供自定义的 Hadoop 配置 和 ORC 输出属性。
Java String schema = ...; Configuration conf = ...; Properties writerProperties = new Properties(); writerProperties.setProperty(\u0026#34;orc.compress\u0026#34;, \u0026#34;LZ4\u0026#34;); // 其他 ORC 属性也可以使用类似方式进行设置 final OrcBulkWriterFactory\u0026lt;Person\u0026gt; writerFactory = new OrcBulkWriterFactory\u0026lt;\u0026gt;( new PersonVectorizer(schema), writerProperties, conf); Scala val schema: String = ... val conf: Configuration = ... val writerProperties: Properties = new Properties() writerProperties.setProperty(\u0026#34;orc.compress\u0026#34;, \u0026#34;LZ4\u0026#34;) // 其他 ORC 属性也可以使用类似方式进行设置 val writerFactory = new OrcBulkWriterFactory( new PersonVectorizer(schema), writerProperties, conf) 完整的 ORC 输出属性列表可以参考 此文档 。
用户在重写 vectorize(...) 方法时可以调用 addUserMetadata(...) 方法来添加自己的元数据到 ORC 文件中。
Java public class PersonVectorizer extends Vectorizer\u0026lt;Person\u0026gt; implements Serializable {	@Override public void vectorize(Person element, VectorizedRowBatch batch) throws IOException { ... String metadataKey = ...; ByteBuffer metadataValue = ...; this.addUserMetadata(metadataKey, metadataValue); } } Scala class PersonVectorizer(schema: String) extends Vectorizer[Person](schema) { override def vectorize(element: Person, batch: VectorizedRowBatch): Unit = { ... val metadataKey: String = ... val metadataValue: ByteBuffer = ... addUserMetadata(metadataKey, metadataValue) } } PyFlink 用户可以使用 OrcBulkWriters 来创建将数据写入 Orc 文件的 BulkWriterFactory 。
为了在 PyFlink 作业中使用 ORC format ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 row_type = DataTypes.ROW([ DataTypes.FIELD(\u0026#39;name\u0026#39;, DataTypes.STRING()), DataTypes.FIELD(\u0026#39;age\u0026#39;, DataTypes.INT()), ]) sink = FileSink.for_bulk_format( OUTPUT_DIR, OrcBulkWriters.for_row_type( row_type=row_type, writer_properties=Configuration(), hadoop_config=Configuration(), ) ).build() ds.sink_to(sink) Hadoop SequenceFile Format # 如果在程序中使用 SequenceFile 的 Bulk-encoded Format，需要添加如下依赖到项目中：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-sequence-file\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 类似这样创建一个简单的 SequenceFile：
Java import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.configuration.GlobalConfiguration; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.SequenceFile; import org.apache.hadoop.io.Text; DataStream\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = ...; Configuration hadoopConf = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration()); final FileSink\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; sink = FileSink .forBulkFormat( outputBasePath, new SequenceFileWriterFactory\u0026lt;\u0026gt;(hadoopConf, LongWritable.class, Text.class)) .build(); input.sinkTo(sink); Scala import org.apache.flink.connector.file.sink.FileSink; import org.apache.flink.configuration.GlobalConfiguration import org.apache.hadoop.conf.Configuration import org.apache.hadoop.io.LongWritable import org.apache.hadoop.io.SequenceFile import org.apache.hadoop.io.Text; val input: DataStream[(LongWritable, Text)] = ... val hadoopConf: Configuration = HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration()) val sink: FileSink[(LongWritable, Text)] = FileSink .forBulkFormat( outputBasePath, new SequenceFileWriterFactory(hadoopConf, LongWritable.class, Text.class)) .build() input.sinkTo(sink) SequenceFileWriterFactory 提供额外的构造参数设置是否开启压缩功能。
桶分配 # 桶的逻辑定义了如何将数据分配到基本输出目录内的子目录中。
Row-encoded Format 和 Bulk-encoded Format (参考 Format Types) 使用了 DateTimeBucketAssigner 作为默认的分配器。 默认的分配器 DateTimeBucketAssigner 会基于使用了格式为 yyyy-MM-dd--HH 的系统默认时区来创建小时桶。日期格式（ 即 桶大小）和时区都可以手动配置。
还可以在格式化构造器中通过调用 .withBucketAssigner(assigner) 方法指定自定义的 BucketAssigner。
Flink 内置了两种 BucketAssigners：
DateTimeBucketAssigner ：默认的基于时间的分配器 BasePathBucketAssigner ：分配所有文件存储在基础路径上（单个全局桶） PyFlink 只支持 DateTimeBucketAssigner 和 BasePathBucketAssigner 。 滚动策略 # RollingPolicy 定义了何时关闭给定的 In-progress Part 文件，并将其转换为 Pending 状态，然后在转换为 Finished 状态。 Finished 状态的文件，可供查看并且可以保证数据的有效性，在出现故障时不会恢复。 在 STREAMING 模式下，滚动策略结合 Checkpoint 间隔（到下一个 Checkpoint 成功时，文件的 Pending 状态才转换为 Finished 状态）共同控制 Part 文件对下游 readers 是否可见以及这些文件的大小和数量。在 BATCH 模式下，Part 文件在 Job 最后对下游才变得可见，滚动策略只控制最大的 Part 文件大小。
Flink 内置了两种 RollingPolicies：
DefaultRollingPolicy OnCheckpointRollingPolicy PyFlink 只支持 DefaultRollingPolicy 和 OnCheckpointRollingPolicy 。 Part 文件生命周期 # 为了在下游使用 FileSink 作为输出，需要了解生成的输出文件的命名和生命周期。
Part 文件可以处于以下三种状态中的任意一种：
In-progress ：当前正在写入的 Part 文件处于 in-progress 状态 Pending ：由于指定的滚动策略）关闭 in-progress 状态文件，并且等待提交 Finished ：流模式(STREAMING)下的成功的 Checkpoint 或者批模式(BATCH)下输入结束，文件的 Pending 状态转换为 Finished 状态 只有 Finished 状态下的文件才能被下游安全读取，并且保证不会被修改。
对于每个活动的桶，在任何给定时间每个写入 Subtask 中都有一个 In-progress 状态的 Part 文件，但可能有多个 Pending 状态和 Finished 状态的文件。
Part 文件示例
为了更好的了解这些文件的生命周期，让我们看一个只有 2 个 Sink Subtask 的简单例子：
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 └── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575 当这个 Part 文件 part-81fc4980-a6af-41c8-9937-9939408a734b-0 滚动时（比如说此文件变的很大时），此文件将进入 Pending 状态并且不能重命名。Sink 就会打开一个新的 Part 文件： part-81fc4980-a6af-41c8-9937-9939408a734b-1：
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0.inprogress.ea65a428-a1d0-4a0b-bbc5-7a436a75e575 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 part-81fc4980-a6af-41c8-9937-9939408a734b-0 现在是 Pending 状态，并且在下一个 Checkpoint 成功过后，立即成为 Finished 状态：
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 根据桶策略创建新桶时，不会影响当前 In-progress 状态的文件：
└── 2019-08-25--12 ├── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── part-81fc4980-a6af-41c8-9937-9939408a734b-0 └── part-81fc4980-a6af-41c8-9937-9939408a734b-1.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 └── 2019-08-25--13 └── part-4005733d-a830-4323-8291-8866de98b582-0.inprogress.2b475fec-1482-4dea-9946-eb4353b475f1 旧桶仍然可以接收新记录，因为桶策略是在每条记录上进行评估的。
Part 文件配置 # Finished 状态与 In-progress 状态的文件只能通过命名来区分。
默认的，文件命名策略如下:
In-progress / Pending：part-\u0026lt;uid\u0026gt;-\u0026lt;partFileIndex\u0026gt;.inprogress.uid Finished：part-\u0026lt;uid\u0026gt;-\u0026lt;partFileIndex\u0026gt; 当 Sink Subtask 实例化时，这的 uid 是一个分配给 Subtask 的随机 ID 值。这个 uid 不具有容错机制，所以当 Subtask 从故障恢复时，uid 会重新生成。 Flink 允许用户给 Part 文件名添加一个前缀和/或后缀。 使用 OutputFileConfig 来完成上述功能。 例如，Sink 将在创建文件的文件名上添加前缀 \u0026ldquo;prefix\u0026rdquo; 和后缀 \u0026ldquo;.ext\u0026rdquo;，如下所示：
└── 2019-08-25--12 ├── prefix-4005733d-a830-4323-8291-8866de98b582-0.ext ├── prefix-4005733d-a830-4323-8291-8866de98b582-1.ext.inprogress.bd053eb0-5ecf-4c85-8433-9eff486ac334 ├── prefix-81fc4980-a6af-41c8-9937-9939408a734b-0.ext └── prefix-81fc4980-a6af-41c8-9937-9939408a734b-1.ext.inprogress.bc279efe-b16f-47d8-b828-00ef6e2fbd11 用户也可以使用 OutputFileConfig 采用如下方式添加前缀和后缀：
Java OutputFileConfig config = OutputFileConfig .builder() .withPartPrefix(\u0026#34;prefix\u0026#34;) .withPartSuffix(\u0026#34;.ext\u0026#34;) .build(); FileSink\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; sink = FileSink .forRowFormat((new Path(outputPath), new SimpleStringEncoder\u0026lt;\u0026gt;(\u0026#34;UTF-8\u0026#34;)) .withBucketAssigner(new KeyBucketAssigner()) .withRollingPolicy(OnCheckpointRollingPolicy.build()) .withOutputFileConfig(config) .build(); Scala val config = OutputFileConfig .builder() .withPartPrefix(\u0026#34;prefix\u0026#34;) .withPartSuffix(\u0026#34;.ext\u0026#34;) .build() val sink = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder[String](\u0026#34;UTF-8\u0026#34;)) .withBucketAssigner(new KeyBucketAssigner()) .withRollingPolicy(OnCheckpointRollingPolicy.build()) .withOutputFileConfig(config) .build() Python config = OutputFileConfig \\ .builder() \\ .with_part_prefix(\u0026#34;prefix\u0026#34;) \\ .with_part_suffix(\u0026#34;.ext\u0026#34;) \\ .build() sink = FileSink \\ .for_row_format(OUTPUT_PATH, Encoder.simple_string_encoder(\u0026#34;UTF-8\u0026#34;)) \\ .with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()) \\ .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \\ .with_output_file_config(config) \\ .build() 文件合并 # 从 1.15 版本开始 FileSink 开始支持已经提交 pending 文件的合并，从而允许应用设置一个较小的时间周期并且避免生成大量的小文件。 尤其是当用户使用 bulk 格式 的时候： 这种格式要求用户必须在 checkpoint 的时候切换文件。
文件合并功能可以通过以下代码打开：
Java FileSink\u0026lt;Integer\u0026gt; fileSink= FileSink.forRowFormat(new Path(path),new SimpleStringEncoder\u0026lt;Integer\u0026gt;()) .enableCompact( FileCompactStrategy.Builder.newBuilder() .setNumCompactThreads(1024) .enableCompactionOnCheckpoint(5) .build(), new RecordWiseFileCompactor\u0026lt;\u0026gt;( new DecoderBasedReader.Factory\u0026lt;\u0026gt;(SimpleStringDecoder::new))) .build(); Scala val fileSink: FileSink[Integer] = FileSink.forRowFormat(new Path(path), new SimpleStringEncoder[Integer]()) .enableCompact( FileCompactStrategy.Builder.newBuilder() .setNumCompactThreads(1024) .enableCompactionOnCheckpoint(5) .build(), new RecordWiseFileCompactor( new DecoderBasedReader.Factory(() =\u0026gt; new SimpleStringDecoder))) .build() Python file_sink = FileSink \\ .for_row_format(PATH, Encoder.simple_string_encoder()) \\ .enable_compact( FileCompactStrategy.builder() .set_size_threshold(1024) .enable_compaction_on_checkpoint(5) .build(), FileCompactor.concat_file_compactor()) \\ .build() 这一功能开启后，在文件转为 pending 状态与文件最终提交之间会进行文件合并。这些 pending 状态的文件将首先被提交为一个以 . 开头的 临时文件。这些文件随后将会按照用户指定的策略和合并方式进行合并并生成合并后的 pending 状态的文件。 然后这些文件将被发送给 Committer 并提交为正式文件，在这之后，原始的临时文件也会被删除掉。
当开启文件合并功能时，用户需要指定 FileCompactStrategy 与 FileCompactor 。
FileCompactStrategy 指定何时以及哪些文件将被合并。 目前有两个并行的条件：目标文件大小与间隔的 Checkpoint 数量。当目前缓存的文件的总大小达到指定的阈值，或自上次合并后经过的 Checkpoint 次数已经达到指定次数时， FileSink 将创建一个异步任务来合并当前缓存的文件。
FileCompactor 指定如何将给定的路径列表对应的文件进行合并将结果写入到文件中。 根据如何写文件，它可以分为两类：
OutputStreamBasedFileCompactor : 用户将合并后的结果写入一个输出流中。通常在用户不希望或者无法从输入文件中读取记录时使用。这种类型的 CompactingFileWriter 的一个例子是 ConcatFileCompactor ，它直接将给定的文件进行合并并将结果写到输出流中。 RecordWiseFileCompactor ： 这种类型的 CompactingFileWriter 会逐条读出输入文件的记录用户，然后和FileWriter一样写入输出文件中。CompactingFileWriter 的一个例子是 RecordWiseFileCompactor ，它从给定的文件中读出记录并写出到 CompactingFileWriter 中。用户需要指定如何从原始文件中读出记录。 注意事项1 一旦启用了文件合并功能，此后若需要再关闭，必须在构建FileSink时显式调用disableCompact方法。
注意事项2 如果启用了文件合并功能，文件可见的时间会被延长。
PyFlink 只支持 ConcatFileCompactor 和 IdenticalFileCompactor 。 重要注意事项 # 通用注意事项 # 注意事项 1：当使用的 Hadoop 版本 \u0026lt; 2.7 时， 当每次 Checkpoint 时请使用 OnCheckpointRollingPolicy 滚动 Part 文件。原因是：如果 Part 文件 \u0026ldquo;穿越\u0026rdquo; 了 Checkpoint 的时间间隔， 然后，从失败中恢复过来时，FileSink 可能会使用文件系统的 truncate() 方法丢弃处于 In-progress 状态文件中的未提交数据。 这个方法在 Hadoop 2.7 版本之前是不支持的，Flink 将抛出异常。
注意事项 2：鉴于 Flink 的 Sink 和 UDF 通常不会区分正常作业终止（例如 有限输入流）和 由于故障而终止， 在 Job 正常终止时，最后一个 In-progress 状态文件不会转换为 \u0026ldquo;Finished\u0026rdquo; 状态。
注意事项 3：Flink 和 FileSink 永远不会覆盖已提交数据。 鉴于此，假定一个 In-progress 状态文件被后续成功的 Checkpoint 提交了，当尝试从这个旧的 Checkpoint / Savepoint 进行恢复时，FileSink 将拒绝继续执行并将抛出异常，因为程序无法找到 In-progress 状态的文件。
注意事项 4：目前，FileSink 仅支持以下3种文件系统：HDFS、 S3 和 Local。如果在运行时使用了不支持的文件系统，Flink 将抛出异常。
BATCH 注意事项 # 注意事项 1：虽然 Writer 是以用户指定的 parallelism 执行的，然而 Committer 是以 parallelism = 1 执行的。
注意事项 2：Pending 状态文件被提交并且所有输入数据被处理完后，才转换为 Finished 状态。
注意事项 3：当系统处于高可用状态下，并且正当 Committers 进行提交时如果 JobManager 发生了故障，那么可能会有副本。这种情况将会在 Flink 的未来版本中进行修复。（可以参考 FLIP-147 ） 。
S3 注意事项 # 注意事项 1：对于 S3，FileSink 仅支持基于 Hadoop-based 文件系统的实现，而不支持基于 Presto 的实现。 如果 Job 中使用 FileSink 写入 S3，但是希望使用基于 Presto 的 Sink 做 Checkpoint，建议明确使用 \u0026ldquo;s3a://\u0026rdquo; （对于 Hadoop）作为 Sink 目标路径格式并且使用 \u0026ldquo;s3p://\u0026rdquo; 作为 Checkpoint 的目标路径格式（对于 Presto）。 对于 Sink 和 Checkpoint 同时使用 \u0026ldquo;s3://\u0026rdquo; 可能导致不可控的行为，由于两者的实现 \u0026ldquo;监听\u0026rdquo; 同一格式路径。
注意事项 2：在保证高效的同时还要保证 exactly-once 语义，FileSink 使用了 S3 的 Multi-part Upload 功能（MPU 功能开箱即用）。 此功能允许以独立的块上传文件（因此称为 \u0026ldquo;multi-part\u0026rdquo;），当 MPU 的所有块都上传成功时，这些块就可以合并生成原始文件。 对于非活动的 MPU，S3 支持桶生命周期规则，用户可以使用该规则终止在启动后指定天数内未完成的多块上传操作。 这意味着，如果设置了这个规则，并在某些文件未完全上传的情况下执行 Savepoint，则其关联的 MPU 可能会在 Job 重启前超时。 这将导致 Job 无法从该 Savepoint 恢复，因为 Pending 状态的 Part 文件已不存在，那么 Flink Job 将失败并抛出异常，因为程序试图获取那些不存在的文件导致了失败。
Back to top
`}),e.add({id:173,href:"/flink/flink-docs-master/zh/docs/ops/metrics/",title:"指标",section:"Operations",content:` 指标 # Flink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from any user function that extends RichFunction by calling getRuntimeContext().getMetricGroup(). This method returns a MetricGroup object on which you can create and register new metrics.
Metric types # Flink supports Counters, Gauges, Histograms and Meters.
Counter # A Counter is used to count something. The current value can be in- or decremented using inc()/inc(long n) or dec()/dec(long n). You can create and register a Counter by calling counter(String name) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient Counter counter; @Override public void open(Configuration config) { this.counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCounter\u0026#34;); } @Override public String map(String value) throws Exception { this.counter.inc(); return value; } } Scala class MyMapper extends RichMapFunction[String,String] { @transient private var counter: Counter = _ override def open(parameters: Configuration): Unit = { counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCounter\u0026#34;) } override def map(value: String): String = { counter.inc() value } } Python class MyMapper(MapFunction): def __init__(self): self.counter = None def open(self, runtime_context: RuntimeContext): self.counter = runtime_context \\ .get_metrics_group() \\ .counter(\u0026#34;my_counter\u0026#34;) def map(self, value: str): self.counter.inc() return value Alternatively you can also use your own Counter implementation:
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient Counter counter; @Override public void open(Configuration config) { this.counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCustomCounter\u0026#34;, new CustomCounter()); } @Override public String map(String value) throws Exception { this.counter.inc(); return value; } } Scala class MyMapper extends RichMapFunction[String,String] { @transient private var counter: Counter = _ override def open(parameters: Configuration): Unit = { counter = getRuntimeContext() .getMetricGroup() .counter(\u0026#34;myCustomCounter\u0026#34;, new CustomCounter()) } override def map(value: String): String = { counter.inc() value } } Python Still not supported in Python API. Gauge # A Gauge provides a value of any type on demand. In order to use a Gauge you must first create a class that implements the org.apache.flink.metrics.Gauge interface. There is no restriction for the type of the returned value. You can register a gauge by calling gauge(String name, Gauge gauge) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;String, String\u0026gt; { private transient int valueToExpose = 0; @Override public void open(Configuration config) { getRuntimeContext() .getMetricGroup() .gauge(\u0026#34;MyGauge\u0026#34;, new Gauge\u0026lt;Integer\u0026gt;() { @Override public Integer getValue() { return valueToExpose; } }); } @Override public String map(String value) throws Exception { valueToExpose++; return value; } } Scala new class MyMapper extends RichMapFunction[String,String] { @transient private var valueToExpose = 0 override def open(parameters: Configuration): Unit = { getRuntimeContext() .getMetricGroup() .gauge[Int, ScalaGauge[Int]](\u0026#34;MyGauge\u0026#34;, ScalaGauge[Int]( () =\u0026gt; valueToExpose ) ) } override def map(value: String): String = { valueToExpose += 1 value } } Python class MyMapper(MapFunction): def __init__(self): self.value_to_expose = 0 def open(self, runtime_context: RuntimeContext): runtime_context \\ .get_metrics_group() \\ .gauge(\u0026#34;my_gauge\u0026#34;, lambda: self.value_to_expose) def map(self, value: str): self.value_to_expose += 1 return value Note that reporters will turn the exposed object into a String, which means that a meaningful toString() implementation is required.
Histogram # A Histogram measures the distribution of long values. You can register one by calling histogram(String name, Histogram histogram) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Histogram histogram; @Override public void open(Configuration config) { this.histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new MyHistogram()); } @Override public Long map(Long value) throws Exception { this.histogram.update(value); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var histogram: Histogram = _ override def open(parameters: Configuration): Unit = { histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new MyHistogram()) } override def map(value: Long): Long = { histogram.update(value) value } } Python Still not supported in Python API. Flink does not provide a default implementation for Histogram, but offers a Wrapper that allows usage of Codahale/DropWizard histograms. To use this wrapper add the following dependency in your pom.xml:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-metrics-dropwizard\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; You can then register a Codahale/DropWizard histogram like this:
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Histogram histogram; @Override public void open(Configuration config) { com.codahale.metrics.Histogram dropwizardHistogram = new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500)); this.histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new DropwizardHistogramWrapper(dropwizardHistogram)); } @Override public Long map(Long value) throws Exception { this.histogram.update(value); return value; } } Scala class MyMapper extends RichMapFunction[Long, Long] { @transient private var histogram: Histogram = _ override def open(config: Configuration): Unit = { com.codahale.metrics.Histogram dropwizardHistogram = new com.codahale.metrics.Histogram(new SlidingWindowReservoir(500)) histogram = getRuntimeContext() .getMetricGroup() .histogram(\u0026#34;myHistogram\u0026#34;, new DropwizardHistogramWrapper(dropwizardHistogram)) } override def map(value: Long): Long = { histogram.update(value) value } } Python Still not supported in Python API. Meter # A Meter measures an average throughput. An occurrence of an event can be registered with the markEvent() method. Occurrence of multiple events at the same time can be registered with markEvent(long n) method. You can register a meter by calling meter(String name, Meter meter) on a MetricGroup.
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Meter meter; @Override public void open(Configuration config) { this.meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new MyMeter()); } @Override public Long map(Long value) throws Exception { this.meter.markEvent(); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var meter: Meter = _ override def open(config: Configuration): Unit = { meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new MyMeter()) } override def map(value: Long): Long = { meter.markEvent() value } } Python class MyMapperMeter(MapFunction): def __init__(self): self.meter = None def open(self, runtime_context: RuntimeContext): # an average rate of events per second over 120s, default is 60s. self.meter = runtime_context \\ .get_metrics_group() \\ .meter(\u0026#34;my_meter\u0026#34;, time_span_in_seconds=120) def map(self, value: str): self.meter.mark_event() return value Flink offers a Wrapper that allows usage of Codahale/DropWizard meters. To use this wrapper add the following dependency in your pom.xml:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-metrics-dropwizard\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; You can then register a Codahale/DropWizard meter like this:
Java public class MyMapper extends RichMapFunction\u0026lt;Long, Long\u0026gt; { private transient Meter meter; @Override public void open(Configuration config) { com.codahale.metrics.Meter dropwizardMeter = new com.codahale.metrics.Meter(); this.meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new DropwizardMeterWrapper(dropwizardMeter)); } @Override public Long map(Long value) throws Exception { this.meter.markEvent(); return value; } } Scala class MyMapper extends RichMapFunction[Long,Long] { @transient private var meter: Meter = _ override def open(config: Configuration): Unit = { val dropwizardMeter: com.codahale.metrics.Meter = new com.codahale.metrics.Meter() meter = getRuntimeContext() .getMetricGroup() .meter(\u0026#34;myMeter\u0026#34;, new DropwizardMeterWrapper(dropwizardMeter)) } override def map(value: Long): Long = { meter.markEvent() value } } Python Still not supported in Python API. Scope # Every metric is assigned an identifier and a set of key-value pairs under which the metric will be reported.
The identifier is based on 3 components: a user-defined name when registering the metric, an optional user-defined scope and a system-provided scope. For example, if A.B is the system scope, C.D the user scope and E the name, then the identifier for the metric will be A.B.C.D.E.
You can configure which delimiter to use for the identifier (default: .) by setting the metrics.scope.delimiter key in conf/flink-conf.yaml.
User Scope # You can define a user scope by calling MetricGroup#addGroup(String name), MetricGroup#addGroup(int name) or MetricGroup#addGroup(String key, String value). These methods affect what MetricGroup#getMetricIdentifier and MetricGroup#getScopeComponents return.
Java counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetrics\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); Scala counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetrics\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) Python counter = runtime_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) counter = runtime_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) System Scope # The system scope contains context information about the metric, for example in which task it was registered or what job that task belongs to.
Which context information should be included can be configured by setting the following keys in conf/flink-conf.yaml. Each of these keys expect a format string that may contain constants (e.g. \u0026ldquo;taskmanager\u0026rdquo;) and variables (e.g. \u0026ldquo;\u0026lt;task_id\u0026gt;\u0026rdquo;) which will be replaced at runtime.
metrics.scope.jm Default: \u0026lt;host\u0026gt;.jobmanager Applied to all metrics that were scoped to a job manager. metrics.scope.jm.job Default: \u0026lt;host\u0026gt;.jobmanager.\u0026lt;job_name\u0026gt; Applied to all metrics that were scoped to a job manager and job. metrics.scope.tm Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt; Applied to all metrics that were scoped to a task manager. metrics.scope.tm.job Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt; Applied to all metrics that were scoped to a task manager and job. metrics.scope.task Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;subtask_index\u0026gt; Applied to all metrics that were scoped to a task. metrics.scope.operator Default: \u0026lt;host\u0026gt;.taskmanager.\u0026lt;tm_id\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt; Applied to all metrics that were scoped to an operator. There are no restrictions on the number or order of variables. Variables are case sensitive.
The default scope for operator metrics will result in an identifier akin to localhost.taskmanager.1234.MyJob.MyOperator.0.MyMetric
If you also want to include the task name but omit the task manager information you can specify the following format:
metrics.scope.operator: \u0026lt;host\u0026gt;.\u0026lt;job_name\u0026gt;.\u0026lt;task_name\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;subtask_index\u0026gt;
This could create the identifier localhost.MyJob.MySource_-\u0026gt;_MyOperator.MyOperator.0.MyMetric.
Note that for this format string an identifier clash can occur should the same job be run multiple times concurrently, which can lead to inconsistent metric data. As such it is advised to either use format strings that provide a certain degree of uniqueness by including IDs (e.g \u0026lt;job_id\u0026gt;) or by assigning unique names to jobs and operators.
List of all Variables # JobManager: \u0026lt;host\u0026gt; TaskManager: \u0026lt;host\u0026gt;, \u0026lt;tm_id\u0026gt; Job: \u0026lt;job_id\u0026gt;, \u0026lt;job_name\u0026gt; Task: \u0026lt;task_id\u0026gt;, \u0026lt;task_name\u0026gt;, \u0026lt;task_attempt_id\u0026gt;, \u0026lt;task_attempt_num\u0026gt;, \u0026lt;subtask_index\u0026gt; Operator: \u0026lt;operator_id\u0026gt;,\u0026lt;operator_name\u0026gt;, \u0026lt;subtask_index\u0026gt; Important: For the Batch API, \u0026lt;operator_id\u0026gt; is always equal to \u0026lt;task_id\u0026gt;.
User Variables # You can define a user variable by calling MetricGroup#addGroup(String key, String value). This method affects what MetricGroup#getMetricIdentifier, MetricGroup#getScopeComponents and MetricGroup#getAllVariables() returns.
Important: User variables cannot be used in scope formats.
Java counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;); Scala counter = getRuntimeContext() .getMetricGroup() .addGroup(\u0026#34;MyMetricsKey\u0026#34;, \u0026#34;MyMetricsValue\u0026#34;) .counter(\u0026#34;myCounter\u0026#34;) Python counter = runtime_context .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) Reporter # For information on how to set up Flink\u0026rsquo;s metric reporters please take a look at the metric reporters documentation.
System metrics # By default Flink gathers several metrics that provide deep insights on the current state. This section is a reference of all these metrics.
The tables below generally feature 5 columns:
The \u0026ldquo;Scope\u0026rdquo; column describes which scope format is used to generate the system scope. For example, if the cell contains \u0026ldquo;Operator\u0026rdquo; then the scope format for \u0026ldquo;metrics.scope.operator\u0026rdquo; is used. If the cell contains multiple values, separated by a slash, then the metrics are reported multiple times for different entities, like for both job- and taskmanagers.
The (optional)\u0026ldquo;Infix\u0026rdquo; column describes which infix is appended to the system scope.
The \u0026ldquo;Metrics\u0026rdquo; column lists the names of all metrics that are registered for the given scope and infix.
The \u0026ldquo;Description\u0026rdquo; column provides information as to what a given metric is measuring.
The \u0026ldquo;Type\u0026rdquo; column describes which metric type is used for the measurement.
Note that all dots in the infix/metric name columns are still subject to the \u0026ldquo;metrics.delimiter\u0026rdquo; setting.
Thus, in order to infer the metric identifier:
Take the scope-format based on the \u0026ldquo;Scope\u0026rdquo; column Append the value in the \u0026ldquo;Infix\u0026rdquo; column if present, and account for the \u0026ldquo;metrics.delimiter\u0026rdquo; setting Append metric name. CPU # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.CPU Load The recent CPU usage of the JVM. Gauge Time The CPU time used by the JVM. Gauge Memory # The memory-related metrics require Oracle\u0026rsquo;s memory management (also included in OpenJDK\u0026rsquo;s Hotspot implementation) to be in place. Some metrics might not be exposed when using other JVM implementations (e.g. IBM\u0026rsquo;s J9).
Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.Memory Heap.Used The amount of heap memory currently used (in bytes). Gauge Heap.Committed The amount of heap memory guaranteed to be available to the JVM (in bytes). Gauge Heap.Max The maximum amount of heap memory that can be used for memory management (in bytes). This value might not be necessarily equal to the maximum value specified through -Xmx or the equivalent Flink configuration parameter. Some GC algorithms allocate heap memory that won't be available to the user code and, therefore, not being exposed through the heap metrics. Gauge NonHeap.Used The amount of non-heap memory currently used (in bytes). Gauge NonHeap.Committed The amount of non-heap memory guaranteed to be available to the JVM (in bytes). Gauge NonHeap.Max The maximum amount of non-heap memory that can be used for memory management (in bytes). Gauge Metaspace.Used The amount of memory currently used in the Metaspace memory pool (in bytes). Gauge Metaspace.Committed The amount of memory guaranteed to be available to the JVM in the Metaspace memory pool (in bytes). Gauge Metaspace.Max The maximum amount of memory that can be used in the Metaspace memory pool (in bytes). Gauge Direct.Count The number of buffers in the direct buffer pool. Gauge Direct.MemoryUsed The amount of memory used by the JVM for the direct buffer pool (in bytes). Gauge Direct.TotalCapacity The total capacity of all buffers in the direct buffer pool (in bytes). Gauge Mapped.Count The number of buffers in the mapped buffer pool. Gauge Mapped.MemoryUsed The amount of memory used by the JVM for the mapped buffer pool (in bytes). Gauge Mapped.TotalCapacity The number of buffers in the mapped buffer pool (in bytes). Gauge Status.Flink.Memory Managed.Used The amount of managed memory currently used. Gauge Managed.Total The total amount of managed memory. Gauge Threads # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.Threads Count The total number of live threads. Gauge GarbageCollection # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.GarbageCollector \u0026lt;GarbageCollector\u0026gt;.Count The total number of collections that have occurred. Gauge \u0026lt;GarbageCollector\u0026gt;.Time The total time spent performing garbage collection. Gauge ClassLoader # Scope Infix Metrics Description Type Job-/TaskManager Status.JVM.ClassLoader ClassesLoaded The total number of classes loaded since the start of the JVM. Gauge ClassesUnloaded The total number of classes unloaded since the start of the JVM. Gauge Network # Deprecated: use Default shuffle service metrics Scope Infix Metrics Description Type TaskManager Status.Network AvailableMemorySegments The number of unused memory segments. Gauge TotalMemorySegments The number of allocated memory segments. Gauge Task buffers inputQueueLength The number of queued input buffers. (ignores LocalInputChannels which are using blocking subpartitions) Gauge outputQueueLength The number of queued output buffers. Gauge inPoolUsage An estimate of the input buffers usage. (ignores LocalInputChannels) Gauge inputFloatingBuffersUsage An estimate of the floating input buffers usage. (ignores LocalInputChannels) Gauge inputExclusiveBuffersUsage An estimate of the exclusive input buffers usage. (ignores LocalInputChannels) Gauge outPoolUsage An estimate of the output buffers usage. The pool usage can be \u003e 100% if overdraft buffers are being used. Gauge Network.\u0026lt;Input|Output\u0026gt;.\u0026lt;gate|partition\u0026gt;
(only available if taskmanager.network.detailed-metrics config option is set) totalQueueLen Total number of queued buffers in all input/output channels. Gauge minQueueLen Minimum number of queued buffers in all input/output channels. Gauge maxQueueLen Maximum number of queued buffers in all input/output channels. Gauge avgQueueLen Average number of queued buffers in all input/output channels. Gauge Default shuffle service # Metrics related to data exchange between task executors using netty network communication.
Scope Infix Metrics Description Type TaskManager Status.Shuffle.Netty AvailableMemorySegments The number of unused memory segments. Gauge UsedMemorySegments The number of used memory segments. Gauge TotalMemorySegments The number of allocated memory segments. Gauge AvailableMemory The amount of unused memory in bytes. Gauge UsedMemory The amount of used memory in bytes. Gauge TotalMemory The amount of allocated memory in bytes. Gauge RequestedMemoryUsage Experimental: The usage of the network memory. Shows (as percentage) the total amount of requested memory from all of the subtasks. It can exceed 100% as not all requested memory is required for subtask to make progress. However if usage exceeds 100% throughput can suffer greatly and please consider increasing available network memory, or decreasing configured size of network buffer pools. Gauge Task Shuffle.Netty.Input.Buffers inputQueueLength The number of queued input buffers. Gauge inputQueueSize The real size of queued input buffers in bytes. The size for local input channels is always \`0\` since the local channel takes records directly from the output queue. Gauge inPoolUsage An estimate of the input buffers usage. (ignores LocalInputChannels) Gauge inputFloatingBuffersUsage An estimate of the floating input buffers usage. (ignores LocalInputChannels) Gauge inputExclusiveBuffersUsage An estimate of the exclusive input buffers usage. (ignores LocalInputChannels) Gauge Shuffle.Netty.Output.Buffers outputQueueLength The number of queued output buffers. Gauge outputQueueSize The real size of queued output buffers in bytes. Gauge outPoolUsage An estimate of the output buffers usage. The pool usage can be \u003e 100% if overdraft buffers are being used. Gauge Shuffle.Netty.\u0026lt;Input|Output\u0026gt;.\u0026lt;gate|partition\u0026gt;
(only available if taskmanager.network.detailed-metrics config option is set) totalQueueLen Total number of queued buffers in all input/output channels. Gauge minQueueLen Minimum number of queued buffers in all input/output channels. Gauge maxQueueLen Maximum number of queued buffers in all input/output channels. Gauge avgQueueLen Average number of queued buffers in all input/output channels. Gauge Shuffle.Netty.Input numBytesInLocal The total number of bytes this task has read from a local source. Counter numBytesInLocalPerSecond The number of bytes this task reads from a local source per second. Meter numBytesInRemote The total number of bytes this task has read from a remote source. Counter numBytesInRemotePerSecond The number of bytes this task reads from a remote source per second. Meter numBuffersInLocal The total number of network buffers this task has read from a local source. Counter numBuffersInLocalPerSecond The number of network buffers this task reads from a local source per second. Meter numBuffersInRemote The total number of network buffers this task has read from a remote source. Counter numBuffersInRemotePerSecond The number of network buffers this task reads from a remote source per second. Meter Cluster # Scope Metrics Description Type JobManager numRegisteredTaskManagers The number of registered taskmanagers. Gauge numPendingTaskManagers (only applicable to Native Kubernetes / YARN) The number of outstanding taskmanagers that Flink has requested. Gauge numRunningJobs The number of running jobs. Gauge taskSlotsAvailable The number of available task slots. Gauge taskSlotsTotal The total number of task slots. Gauge Availability # The metrics in this table are available for each of the following job states: INITIALIZING, CREATED, RUNNING, RESTARTING, CANCELLING, FAILING. Whether these metrics are reported depends on the metrics.job.status.enable setting.
Evolving The semantics of these metrics may change in later releases.
Scope Metrics Description Type Job (only available on JobManager) \u0026lt;jobStatus\u0026gt;State For a given state, return 1 if the job is currently in that state, otherwise return 0. Gauge \u0026lt;jobStatus\u0026gt;Time For a given state, if the job is currently in that state, return the time (in milliseconds) since the job transitioned into that state, otherwise return 0. Gauge \u0026lt;jobStatus\u0026gt;TimeTotal For a given state, return how much time (in milliseconds) the job has spent in that state in total. Gauge Experimental
While the job is in the RUNNING state the metrics in this table provide additional details on what the job is currently doing. Whether these metrics are reported depends on the metrics.job.status.enable setting.
Scope Metrics Description Type Job (only available on JobManager) deployingState Return 1 if the job is currently deploying* tasks, otherwise return 0. Gauge deployingTime Return the time (in milliseconds) since the job has started deploying* tasks, otherwise return 0. Gauge deployingTimeTotal Return how much time (in milliseconds) the job has spent deploying* tasks in total. Gauge *A job is considered to be deploying tasks when:
for streaming jobs, any task is in the DEPLOYING state for batch jobs, if at least 1 task is in the DEPLOYING state, and there are no INITIALIZING/RUNNING tasks Scope Metrics Description Type Job (only available on JobManager) uptime The time that the job has been running without interruption. Returns -1 for completed jobs (in milliseconds).
Gauge downtime For jobs currently in a failing/recovering situation, the time elapsed during this outage. Returns 0 for running jobs and -1 for completed jobs (in milliseconds).
Gauge fullRestarts Attention: deprecated, use numRestarts. Gauge numRestarts The total number of restarts since this job was submitted, including full restarts and fine-grained restarts. Gauge {
Checkpointing # Note that for failed checkpoints, metrics are updated on a best efforts basis and may be not accurate.
Scope Metrics Description Type Job (only available on JobManager) lastCheckpointDuration The time it took to complete the last checkpoint (in milliseconds). Gauge lastCheckpointSize The checkpointed size of the last checkpoint (in bytes), this metric could be different from lastCheckpointFullSize if incremental checkpoint or changelog is enabled. Gauge lastCheckpointFullSize The full size of the last checkpoint (in bytes). Gauge lastCheckpointExternalPath The path where the last external checkpoint was stored. Gauge lastCheckpointRestoreTimestamp Timestamp when the last checkpoint was restored at the coordinator (in milliseconds). Gauge numberOfInProgressCheckpoints The number of in progress checkpoints. Gauge numberOfCompletedCheckpoints The number of successfully completed checkpoints. Gauge numberOfFailedCheckpoints The number of failed checkpoints. Gauge totalNumberOfCheckpoints The number of total checkpoints (in progress, completed, failed). Gauge Task checkpointAlignmentTime The time in nanoseconds that the last barrier alignment took to complete, or how long the current alignment has taken so far (in nanoseconds). This is the time between receiving first and the last checkpoint barrier. You can find more information in the [Monitoring State and Checkpoints section](//localhost/flink/flink-docs-master/zh/docs/ops/state/large_state_tuning/#monitoring-state-and-checkpoints) Gauge checkpointStartDelayNanos The time in nanoseconds that elapsed between the creation of the last checkpoint and the time when the checkpointing process has started by this Task. This delay shows how long it takes for the first checkpoint barrier to reach the task. A high value indicates back-pressure. If only a specific task has a long start delay, the most likely reason is data skew. Gauge State Access Latency # Scope Metrics Description Type Task/Operator stateClearLatency The latency of clear operation for state Histogram valueStateGetLatency The latency of Get operation for value state Histogram valueStateUpdateLatency The latency of update operation for value state Histogram listStateGetLatency The latency of get operation for list state Histogram listStateAddLatency The latency of add operation for list state Histogram listStateAddAllLatency The latency of addAll operation for list state Histogram listStateUpdateLatency The latency of update operation for list state Histogram listStateMergeNamespacesLatency The latency of merge namespace operation for list state Histogram mapStateGetLatency The latency of get operation for map state Histogram mapStatePutLatency The latency of put operation for map state Histogram mapStatePutAllLatency The latency of putAll operation for map state Histogram mapStateRemoveLatency The latency of remove operation for map state Histogram mapStateContainsLatency The latency of contains operation for map state Histogram mapStateEntriesInitLatency The init latency of entries operation for map state Histogram mapStateKeysInitLatency The init latency of keys operation for map state Histogram mapStateValuesInitLatency The init latency of values operation for map state Histogram mapStateIteratorInitLatency The init latency of iterator operation for map state Histogram mapStateIsEmptyLatency The latency of isEmpty operation for map state Histogram mapStateIteratorHasNextLatency The latency of iterator#hasNext operation for map state Histogram mapStateIteratorNextLatency The latency of iterator#next operation for map state Histogram mapStateIteratorRemoveLatency The latency of iterator#remove operation for map state Histogram aggregatingStateGetLatency The latency of get operation for aggregating state Histogram aggregatingStateAddLatency The latency of add operation for aggregating state Histogram aggregatingStateMergeNamespacesLatency The latency of merge namespace operation for aggregating state Histogram reducingStateGetLatency The latency of get operation for reducing state Histogram reducingStateAddLatency The latency of add operation for reducing state Histogram reducingStateMergeNamespacesLatency The latency of merge namespace operation for reducing state Histogram RocksDB # Certain RocksDB native metrics are available but disabled by default, you can find full documentation here
State Changelog # Note that the metrics are only available via reporters.
Scope Metrics Description Type Job (only available on TaskManager) numberOfUploadRequests Total number of upload requests made Counter numberOfUploadFailures Total number of failed upload requests (request may be retried after the failure) Counter attemptsPerUpload The number of attempts per upload Histogram totalAttemptsPerUpload The total count distributions of attempts for per upload Histogram uploadBatchSizes The number of upload tasks (coming from one or more writers, i.e. backends/tasks) that were grouped together and form a single upload resulting in a single file Histogram uploadLatenciesNanos The latency distributions of uploads Histogram uploadSizes The size distributions of uploads Histogram uploadQueueSize Current size of upload queue. Queue items can be packed together and form a single upload. Gauge Task/Operator startedMaterialization The number of started materializations. Counter completedMaterialization The number of successfully completed materializations. Counter failedMaterialization The number of failed materializations. Counter lastFullSizeOfMaterialization The full size of the materialization part of the last reported checkpoint (in bytes). Gauge lastIncSizeOfMaterialization The incremental size of the materialization part of the last reported checkpoint (in bytes). Gauge lastFullSizeOfNonMaterialization The full size of the non-materialization part of the last reported checkpoint (in bytes). Gauge lastIncSizeOfNonMaterialization The incremental size of the non-materialization part of the last reported checkpoint (in bytes). Gauge IO # Scope Metrics Description Type Job (only available on TaskManager) [\u0026lt;source_id\u0026gt;.[\u0026lt;source_subtask_index\u0026gt;.]]\u0026lt;operator_id\u0026gt;.\u0026lt;operator_subtask_index\u0026gt;.latency The latency distributions from a given source (subtask) to an operator subtask (in milliseconds), depending on the latency granularity. Histogram Task numBytesInLocal Attention: deprecated, use Default shuffle service metrics. Counter numBytesInLocalPerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBytesInRemote Attention: deprecated, use Default shuffle service metrics. Counter numBytesInRemotePerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBuffersInLocal Attention: deprecated, use Default shuffle service metrics. Counter numBuffersInLocalPerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBuffersInRemote Attention: deprecated, use Default shuffle service metrics. Counter numBuffersInRemotePerSecond Attention: deprecated, use Default shuffle service metrics. Meter numBytesOut The total number of bytes this task has emitted. Counter numBytesOutPerSecond The number of bytes this task emits per second. Meter numBuffersOut The total number of network buffers this task has emitted. Counter numBuffersOutPerSecond The number of network buffers this task emits per second. Meter isBackPressured Whether the task is back-pressured. Gauge idleTimeMsPerSecond The time (in milliseconds) this task is idle (has no data to process) per second. Idle time excludes back pressured time, so if the task is back pressured it is not idle. Meter busyTimeMsPerSecond The time (in milliseconds) this task is busy (neither idle nor back pressured) per second. Can be NaN, if the value could not be calculated. Gauge backPressuredTimeMsPerSecond The time (in milliseconds) this task is back pressured (soft or hard) per second. It's a sum of softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond. Gauge softBackPressuredTimeMsPerSecond The time (in milliseconds) this task is softly back pressured per second. Softly back pressured task will be still responsive and capable of for example triggering unaligned checkpoints. Gauge hardBackPressuredTimeMsPerSecond The time (in milliseconds) this task is back pressured in a hard way per second. During hard back pressured task is completely blocked and unresponsive preventing for example unaligned checkpoints from triggering. Gauge maxSoftBackPressuredTimeMs Maximum recorded duration of a single consecutive period of the task being softly back pressured in the last sampling period. Please check softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond for more information. Gauge maxHardBackPressuredTimeMs Maximum recorded duration of a single consecutive period of the task being in the hard back pressure state in the last sampling period. Please check softBackPressuredTimeMsPerSecond and hardBackPressuredTimeMsPerSecond for more information. Gauge mailboxMailsPerSecond The number of actions processed from the task's mailbox per second which includes all actions, e.g., checkpointing, timer, or cancellation actions. Meter mailboxLatencyMs The latency is the time that actions spend waiting in the task's mailbox before being processed. The metric is a statistic of the latency in milliseconds that is measured approximately once every second and includes the last 60 measurements. Histogram mailboxQueueSize The number of actions in the task's mailbox that are waiting to be processed. Gauge Task (only if buffer debloating is enabled and in non-source tasks) estimatedTimeToConsumeBuffersMs The estimated time (in milliseconds) by the buffer debloater to consume all of the buffered data in the network exchange preceding this task. This value is calculated by approximated amount of the in-flight data and calculated throughput. Gauge debloatedBufferSize The desired buffer size (in bytes) calculated by the buffer debloater. Buffer debloater is trying to reduce buffer size when the amount of in-flight data (after taking into account current throughput) exceeds the configured target value. Gauge Task/Operator numRecordsIn The total number of records this operator/task has received. Counter numRecordsInPerSecond The number of records this operator/task receives per second. Meter numRecordsOut The total number of records this operator/task has emitted. Counter numRecordsOutPerSecond The number of records this operator/task sends per second. Meter numLateRecordsDropped The number of records this operator/task has dropped due to arriving late. Counter currentInputWatermark The last watermark this operator/tasks has received (in milliseconds). Note: For operators/tasks with 2 inputs this is the minimum of the last received watermarks.
Gauge Operator currentInputNWatermark The last watermark this operator has received in its N'th input (in milliseconds), with index N starting from 1. For example currentInput1Watermark, currentInput2Watermark, ... Note: Only for operators with 2 or more inputs.
Gauge currentOutputWatermark The last watermark this operator has emitted (in milliseconds). Gauge watermarkAlignmentDrift The current drift from the minimal watermark emitted by all sources/tasks/splits that belong to the same watermark group. Note: Available only when watermark alignment is enabled and the first common watermark is announced. You can configure the update interval in the WatermarkStrategy.
Gauge numSplitsProcessed The total number of InputSplits this data source has processed (if the operator is a data source). Gauge Connectors # Kafka Connectors # Scope Metrics User Variables Description Type Operator commitsSucceeded n/a The total number of successful offset commits to Kafka, if offset committing is turned on and checkpointing is enabled. Counter Operator commitsFailed n/a The total number of offset commit failures to Kafka, if offset committing is turned on and checkpointing is enabled. Note that committing offsets back to Kafka is only a means to expose consumer progress, so a commit failure does not affect the integrity of Flink's checkpointed partition offsets. Counter Operator committedOffsets topic, partition The last successfully committed offsets to Kafka, for each partition. A particular partition's metric can be specified by topic name and partition id. Gauge Operator currentOffsets topic, partition The consumer's current read offset, for each partition. A particular partition's metric can be specified by topic name and partition id. Gauge Kinesis 源 # 范围 指标 用户变量 描述 类型 Operator millisBehindLatest stream, shardId 消费者落后于流头部的毫秒数， 对每个Kinesis分片，表示费者落后当前时间多久。 可以通过流名称和分片id指定一个特定分片的指标值。 值为0表示记录处理已完成，并且没有新记录在此时处理。 值为-1表示尚未报告指标值。 Gauge Operator sleepTimeMillis stream, shardId 消费者在从Kinesis获取记录之前睡眠的毫秒数。 可以通过流名称和分片id指定特定分片的指标值。 Gauge Operator maxNumberOfRecordsPerFetch stream, shardId 消费者在对Kinesis的单个getRecords调用中请求的最大记录数。如果ConsumerConfigConstants.SHARD_USE_ADAPTIVE_READS 设置为true，自适应计算该值，以最大化来自Kinesis的2Mbps读取限制。 Gauge Operator numberOfAggregatedRecordsPerFetch stream, shardId 消费者在对Kinesis的单个getRecords调用中获取的聚合的Kinesis记录数。 Gauge Operator numberOfDeggregatedRecordsPerFetch stream, shardId 消费者在对Kinesis的单个getRecords调用中获取的非聚合的Kinesis记录数。 Gauge Operator averageRecordSizeBytes stream, shardId 以字节为单位的Kinesis记录的平均大小，由消费者在单个getRecords调用中获取。 Gauge Operator runLoopTimeNanos stream, shardId 消费者在运行循环中花费的实际时间（纳秒）。 Gauge Operator loopFrequencyHz stream, shardId 一秒钟内调用getRecords的次数。 Gauge Operator bytesRequestedPerFetch stream, shardId 在对getRecords的单个调用中请求的字节数（2 Mbps / loopFrequencyHz）。 Gauge Kinesis 接收器 # 范围 指标 描述 类型 Operator numRecordsOutErrors (已弃用, 请使用numRecordsSendErrors) 被拒绝的记录写入数。 Counter Operator numRecordsSendErrors 被拒绝的记录写入数。 Counter Operator CurrentSendTime 最后一批请求的1次往返所用的毫秒数。 Gauge HBase Connectors # Scope Metrics User Variables Description Type Operator lookupCacheHitRate n/a 查找的缓存命中率。 Gauge System resources # System resources reporting is disabled by default. When metrics.system-resource is enabled additional metrics listed below will be available on Job- and TaskManager. System resources metrics are updated periodically and they present average values for a configured interval (metrics.system-resource-probing-interval).
System resources reporting requires an optional dependency to be present on the classpath (for example placed in Flink\u0026rsquo;s lib directory):
com.github.oshi:oshi-core:6.1.5 (licensed under MIT license) Including it\u0026rsquo;s transitive dependencies:
net.java.dev.jna:jna-platform:jar:5.10.0 net.java.dev.jna:jna:jar:5.10.0 Failures in this regard will be reported as warning messages like NoClassDefFoundError logged by SystemResourcesMetricsInitializer during the startup.
System CPU # Scope Infix Metrics Description Job-/TaskManager System.CPU Usage Overall % of CPU usage on the machine. Idle % of CPU Idle usage on the machine. Sys % of System CPU usage on the machine. User % of User CPU usage on the machine. IOWait % of IOWait CPU usage on the machine. Irq % of Irq CPU usage on the machine. SoftIrq % of SoftIrq CPU usage on the machine. Nice % of Nice Idle usage on the machine. Load1min Average CPU load over 1 minute Load5min Average CPU load over 5 minute Load15min Average CPU load over 15 minute UsageCPU* % of CPU usage per each processor System memory # Scope Infix Metrics Description Job-/TaskManager System.Memory Available Available memory in bytes Total Total memory in bytes System.Swap Used Used swap bytes Total Total swap in bytes System network # Scope Infix Metrics Description Job-/TaskManager System.Network.INTERFACE_NAME ReceiveRate Average receive rate in bytes per second SendRate Average send rate in bytes per second 预测执行 # 以下指标可以用来衡量预测执行的有效性。
Scope Metrics Description Type Job (only available on JobManager) numSlowExecutionVertices 当前的慢执行节点数量。 Gauge numEffectiveSpeculativeExecutions 有效的预测执行数量，即比初始执行实例更早结束的预测执行实例的数量。 Counter End-to-End latency tracking # Flink allows to track the latency of records travelling through the system. This feature is disabled by default. To enable the latency tracking you must set the latencyTrackingInterval to a positive number in either the Flink configuration or ExecutionConfig.
At the latencyTrackingInterval, the sources will periodically emit a special record, called a LatencyMarker. The marker contains a timestamp from the time when the record has been emitted at the sources. Latency markers can not overtake regular user records, thus if records are queuing up in front of an operator, it will add to the latency tracked by the marker.
Note that the latency markers are not accounting for the time user records spend in operators as they are bypassing them. In particular the markers are not accounting for the time records spend for example in window buffers. Only if operators are not able to accept new records, thus they are queuing up, the latency measured using the markers will reflect that.
The LatencyMarkers are used to derive a distribution of the latency between the sources of the topology and each downstream operator. These distributions are reported as histogram metrics. The granularity of these distributions can be controlled in the Flink configuration. For the highest granularity subtask Flink will derive the latency distribution between every source subtask and every downstream subtask, which results in quadratic (in the terms of the parallelism) number of histograms.
Currently, Flink assumes that the clocks of all machines in the cluster are in sync. We recommend setting up an automated clock synchronisation service (like NTP) to avoid false latency results.
Warning Enabling latency metrics can significantly impact the performance of the cluster (in particular for subtask granularity). It is highly recommended to only use them for debugging purposes.
State access latency tracking # Flink also allows to track the keyed state access latency for standard Flink state-backends or customized state backends which extending from AbstractStateBackend. This feature is disabled by default. To enable this feature you must set the state.backend.latency-track.keyed-state-enabled to true in the Flink configuration.
Once tracking keyed state access latency is enabled, Flink will sample the state access latency every N access, in which N is defined by state.backend.latency-track.sample-interval. This configuration has a default value of 100. A smaller value will get more accurate results but have a higher performance impact since it is sampled more frequently.
As the type of this latency metrics is histogram, state.backend.latency-track.history-size will control the maximum number of recorded values in history, which has the default value of 128. A larger value of this configuration will require more memory, but will provide a more accurate result.
Warning Enabling state-access-latency metrics may impact the performance. It is recommended to only use them for debugging purposes.
REST API integration # Metrics can be queried through the Monitoring REST API.
Below is a list of available endpoints, with a sample JSON response. All endpoints are of the sample form http://hostname:8081/jobmanager/metrics, below we list only the path part of the URLs.
Values in angle brackets are variables, for example http://hostname:8081/jobs/\u0026lt;jobid\u0026gt;/metrics will have to be requested for example as http://hostname:8081/jobs/7684be6004e4e955c2a558a9bc463f65/metrics.
Request metrics for a specific entity:
/jobmanager/metrics /taskmanagers/\u0026lt;taskmanagerid\u0026gt;/metrics /jobs/\u0026lt;jobid\u0026gt;/metrics /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/\u0026lt;subtaskindex\u0026gt; Request metrics aggregated across all entities of the respective type:
/taskmanagers/metrics /jobs/metrics /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/metrics Request metrics aggregated over a subset of all entities of the respective type:
/taskmanagers/metrics?taskmanagers=A,B,C /jobs/metrics?jobs=D,E,F /jobs/\u0026lt;jobid\u0026gt;/vertices/\u0026lt;vertexid\u0026gt;/subtasks/metrics?subtask=1,2,3 Warning Metric names can contain special characters that you need to be escape when querying metrics. For example, \u0026ldquo;a_+_b\u0026rdquo; would be escaped to \u0026ldquo;a_%2B_b\u0026rdquo;.
List of characters that should be escaped:
Character Escape Sequence # %23 \$ %24 \u0026 %26 + %2B / %2F ; %3B = %3D ? %3F @ %40 Request a list of available metrics:
GET /jobmanager/metrics
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34; } ] Request the values for specific (unaggregated) metrics:
GET taskmanagers/ABCDE/metrics?get=metric1,metric2
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;34\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; } ] Request aggregated values for specific metrics:
GET /taskmanagers/metrics?get=metric1,metric2
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;min\u0026#34;: 1, \u0026#34;max\u0026#34;: 34, \u0026#34;avg\u0026#34;: 15, \u0026#34;sum\u0026#34;: 45 }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;min\u0026#34;: 2, \u0026#34;max\u0026#34;: 14, \u0026#34;avg\u0026#34;: 7, \u0026#34;sum\u0026#34;: 16 } ] Request specific aggregated values for specific metrics:
GET /taskmanagers/metrics?get=metric1,metric2\u0026amp;agg=min,max
[ { \u0026#34;id\u0026#34;: \u0026#34;metric1\u0026#34;, \u0026#34;min\u0026#34;: 1, \u0026#34;max\u0026#34;: 34 }, { \u0026#34;id\u0026#34;: \u0026#34;metric2\u0026#34;, \u0026#34;min\u0026#34;: 2, \u0026#34;max\u0026#34;: 14 } ] Dashboard integration # Metrics that were gathered for each task or operator can also be visualized in the Dashboard. On the main page for a job, select the Metrics tab. After selecting one of the tasks in the top graph you can select metrics to display using the Add Metric drop-down menu.
Task metrics are listed as \u0026lt;subtask_index\u0026gt;.\u0026lt;metric_name\u0026gt;. Operator metrics are listed as \u0026lt;subtask_index\u0026gt;.\u0026lt;operator_name\u0026gt;.\u0026lt;metric_name\u0026gt;. Each metric will be visualized as a separate graph, with the x-axis representing time and the y-axis the measured value. All graphs are automatically updated every 10 seconds, and continue to do so when navigating to another page.
There is no limit as to the number of visualized metrics; however only numeric metrics can be visualized.
Back to top
`}),e.add({id:174,href:"/flink/flink-docs-master/zh/docs/libs/gelly/bipartite_graph/",title:"Bipartite Graph",section:"Graphs",content:` Bipartite Graph # Bipartite Graph currently only supported in Gelly Java API. Bipartite Graph # A bipartite graph (also called a two-mode graph) is a type of graph where vertices are separated into two disjoint sets. These sets are usually called top and bottom vertices. An edge in this graph can only connect vertices from opposite sets (i.e. bottom vertex to top vertex) and cannot connect two vertices in the same set.
These graphs have wide application in practice and can be a more natural choice for particular domains. For example to represent authorship of scientific papers top vertices can represent scientific papers while bottom nodes will represent authors. Naturally an edge between a top and a bottom nodes would represent an authorship of a particular scientific paper. Another common example for applications of bipartite graphs is relationships between actors and movies. In this case an edge represents that a particular actor played in a movie.
Bipartite graphs are used instead of regular graphs (one-mode) for the following practical reasons:
They preserve more information about a connection between vertices. For example instead of a single link between two researchers in a graph that represents that they authored a paper together a bipartite graph preserves the information about what papers they authored Bipartite graphs can encode the same information more compactly than one-mode graphs Graph Representation # A BipartiteGraph is represented by:
A DataSet of top nodes A DataSet of bottom nodes A DataSet of edges between top and bottom nodes As in the Graph class nodes are represented by the Vertex type and the same rules apply to its types and values.
The graph edges are represented by the BipartiteEdge type. A BipartiteEdge is defined by a top ID (the ID of the top Vertex), a bottom ID (the ID of the bottom Vertex) and an optional value. The main difference between the Edge and BipartiteEdge is that IDs of nodes it links can be of different types. Edges with no value have a NullValue value type.
Java BipartiteEdge\u0026lt;Long, String, Double\u0026gt; e = new BipartiteEdge\u0026lt;Long, String, Double\u0026gt;(1L, \u0026#34;id1\u0026#34;, 0.5); Double weight = e.getValue(); // weight = 0.5 Scala // Scala API is not yet supported Back to top
Graph Creation # You can create a BipartiteGraph in the following ways:
from a DataSet of top vertices, a DataSet of bottom vertices and a DataSet of edges: Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; topVertices = ...; DataSet\u0026lt;Vertex\u0026lt;String, Long\u0026gt;\u0026gt; bottomVertices = ...; DataSet\u0026lt;Edge\u0026lt;String, String, Double\u0026gt;\u0026gt; edges = ...; Graph\u0026lt;String, String, Long, Long, Double\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); Scala // Scala API is not yet supported Graph Transformations # Projection: Projection is a common operation for bipartite graphs that converts a bipartite graph into a regular graph. There are two types of projections: top and bottom projections. Top projection preserves only top nodes in the result graph and creates a link between them in a new graph only if there is an intermediate bottom node both top nodes connect to in the original graph. Bottom projection is the opposite to top projection, i.e. only preserves bottom nodes and connects a pair of nodes if they are connected in the original graph. Gelly supports two sub-types of projections: simple projections and full projections. The only difference between them is what data is associated with edges in the result graph.
In the case of a simple projection each node in the result graph contains a pair of values of bipartite edges that connect nodes in the original graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Vertices (1, \u0026#34;top1\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; topVertices = ...; // Vertices (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; bottomVertices = ...; // Edge that connect vertex 2 to vertex 1 and vertex 4 to vertex 1: // (1, 2, \u0026#34;1-2-edge\u0026#34;); (1, 4, \u0026#34;1-4-edge\u0026#34;) DataSet\u0026lt;Edge\u0026lt;Long, Long, String\u0026gt;\u0026gt; edges = ...; BipartiteGraph\u0026lt;Long, Long, String, String, String\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); // Result graph with two vertices: // (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) // // and one edge that contains ids of bottom edges and a tuple with // values of intermediate edges in the original bipartite graph: // (2, 4, (\u0026#34;1-2-edge\u0026#34;, \u0026#34;1-4-edge\u0026#34;)) Graph\u0026lt;Long, String, Tuple2\u0026lt;String, String\u0026gt;\u0026gt; graph bipartiteGraph.projectionBottomSimple(); Scala // Scala API is not yet supported Full projection preserves all the information about the connection between two vertices and stores it in Projection instances. This includes value and id of an intermediate vertex, source and target vertex values and source and target edge values:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Vertices (1, \u0026#34;top1\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; topVertices = ...; // Vertices (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) DataSet\u0026lt;Vertex\u0026lt;Long, String\u0026gt;\u0026gt; bottomVertices = ...; // Edge that connect vertex 2 to vertex 1 and vertex 4 to vertex 1: // (1, 2, \u0026#34;1-2-edge\u0026#34;); (1, 4, \u0026#34;1-4-edge\u0026#34;) DataSet\u0026lt;Edge\u0026lt;Long, Long, String\u0026gt;\u0026gt; edges = ...; BipartiteGraph\u0026lt;Long, Long, String, String, String\u0026gt; graph = BipartiteGraph.fromDataSet(topVertices, bottomVertices, edges, env); // Result graph with two vertices: // (2, \u0026#34;bottom2\u0026#34;); (4, \u0026#34;bottom4\u0026#34;) // and one edge that contains ids of bottom edges and a tuple that // contains id and value of the intermediate edge, values of connected vertices // and values of intermediate edges in the original bipartite graph: // (2, 4, (1, \u0026#34;top1\u0026#34;, \u0026#34;bottom2\u0026#34;, \u0026#34;bottom4\u0026#34;, \u0026#34;1-2-edge\u0026#34;, \u0026#34;1-4-edge\u0026#34;)) Graph\u0026lt;String, String, Projection\u0026lt;Long, String, String, String\u0026gt;\u0026gt; graph bipartiteGraph.projectionBottomFull(); Scala // Scala API is not yet supported Back to top
`}),e.add({id:175,href:"/flink/flink-docs-master/zh/docs/deployment/",title:"Deployment",section:"Docs",content:" "}),e.add({id:176,href:"/flink/flink-docs-master/zh/docs/connectors/table/elasticsearch/",title:"Elasticsearch",section:"Table API Connectors",content:` Elasticsearch SQL 连接器 # Sink: Batch Sink: Streaming Append \u0026amp; Upsert Mode
Elasticsearch 连接器允许将数据写入到 Elasticsearch 引擎的索引中。本文档描述运行 SQL 查询时如何设置 Elasticsearch 连接器。
连接器可以工作在 upsert 模式，使用 DDL 中定义的主键与外部系统交换 UPDATE/DELETE 消息。
如果 DDL 中没有定义主键，那么连接器只能工作在 append 模式，只能与外部系统交换 INSERT 消息。
依赖 # In order to use the Elasticsearch connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Elasticsearch version Maven dependency SQL Client JAR 6.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch6\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 7.x and later versions \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-elasticsearch7\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. Elasticsearch 连接器不是二进制发行版的一部分，请查阅这里了解如何在集群运行中引用 Elasticsearch 连接器。
如何创建 Elasticsearch 表 # 以下示例展示了如何创建 Elasticsearch sink 表：
CREATE TABLE myUserTable ( user_id STRING, user_name STRING, uv BIGINT, pv BIGINT, PRIMARY KEY (user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;elasticsearch-7\u0026#39;, \u0026#39;hosts\u0026#39; = \u0026#39;http://localhost:9200\u0026#39;, \u0026#39;index\u0026#39; = \u0026#39;users\u0026#39; ); 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必选 (none) String 指定要使用的连接器，有效值为： elasticsearch-6：连接到 Elasticsearch 6.x 的集群。 elasticsearch-7：连接到 Elasticsearch 7.x 及更高版本的集群。 hosts 必选 (none) String 要连接到的一台或多台 Elasticsearch 主机，例如 'http://host_name:9092;http://host_name:9093'。 index 必选 (none) String Elasticsearch 中每条记录的索引。可以是一个静态索引（例如 'myIndex'）或一个动态索引（例如 'index-{log_ts|yyyy-MM-dd}'）。 更多详细信息，请参见下面的动态索引部分。 document-type 6.x 版本中必选 (none) String Elasticsearch 文档类型。在 elasticsearch-7 中不再需要。 document-id.key-delimiter 可选 _ String 复合键的分隔符（默认为"_"），例如，指定为"\$"将导致文档 ID 为"KEY1\$KEY2\$KEY3"。 username 可选 (none) String 用于连接 Elasticsearch 实例的用户名。请注意，Elasticsearch 没有预绑定安全特性，但你可以通过如下指南启用它来保护 Elasticsearch 集群。 password 可选 (none) String 用于连接 Elasticsearch 实例的密码。如果配置了username，则此选项也必须配置为非空字符串。 failure-handler optional fail String 对 Elasticsearch 请求失败情况下的失败处理策略。有效策略为： fail：如果请求失败并因此导致作业失败，则抛出异常。 ignore：忽略失败并放弃请求。 retry-rejected：重新添加由于队列容量饱和而失败的请求。 自定义类名称：使用 ActionRequestFailureHandler 的子类进行失败处理。 sink.flush-on-checkpoint optional true Boolean 在进行 checkpoint 时是否保证刷出缓冲区中的数据。如果关闭这一选项，在进行checkpoint时 sink 将不再为所有进行 中的请求等待 Elasticsearch 的执行完成确认。因此，在这种情况下 sink 将不对至少一次的请求的一致性提供任何保证。 sink.bulk-flush.max-actions 可选 1000 Integer 每个批量请求的最大缓冲操作数。 可以设置为'0'来禁用它。 sink.bulk-flush.max-size 可选 2mb MemorySize 每个批量请求的缓冲操作在内存中的最大值。单位必须为 MB。 可以设置为'0'来禁用它。 sink.bulk-flush.interval 可选 1s Duration flush 缓冲操作的间隔。 可以设置为'0'来禁用它。注意，'sink.bulk-flush.max-size'和'sink.bulk-flush.max-actions'都设置为'0'的这种 flush 间隔设置允许对缓冲操作进行完全异步处理。 sink.bulk-flush.backoff.strategy 可选 DISABLED String 指定在由于临时请求错误导致任何 flush 操作失败时如何执行重试。有效策略为： DISABLED：不执行重试，即第一次请求错误后失败。 CONSTANT：等待重试之间的回退延迟。 EXPONENTIAL：先等待回退延迟，然后在重试之间指数递增。 sink.bulk-flush.backoff.max-retries 可选 (none) Integer 最大回退重试次数。 sink.bulk-flush.backoff.delay 可选 (none) Duration 每次退避尝试之间的延迟。对于 CONSTANT 退避策略，该值是每次重试之间的延迟。对于 EXPONENTIAL 退避策略，该值是初始的延迟。 connection.path-prefix 可选 (none) String 添加到每个 REST 通信中的前缀字符串，例如，'/v1'。 connection.request-timeout 可选 (none) Duration 从连接管理器请求连接的超时时间。超时时间必须大于或者等于 0，如果设置为 0 则是无限超时。 connection.timeout 可选 (none) Duration 建立请求的超时时间 。超时时间必须大于或者等于 0 ，如果设置为 0 则是无限超时。 socket.timeout 可选 (none) Duration 等待数据的 socket 的超时时间 (SO_TIMEOUT)。超时时间必须大于或者等于 0，如果设置为 0 则是无限超时。 format 可选 json String Elasticsearch 连接器支持指定格式。该格式必须生成一个有效的 json 文档。 默认使用内置的 'json' 格式。更多详细信息，请参阅 JSON Format 页面。 特性 # Key 处理 # Elasticsearch sink 可以根据是否定义了一个主键来确定是在 upsert 模式还是 append 模式下工作。 如果定义了主键，Elasticsearch sink 将以 upsert 模式工作，该模式可以消费包含 UPDATE/DELETE 消息的查询。 如果未定义主键，Elasticsearch sink 将以 append 模式工作，该模式只能消费包含 INSERT 消息的查询。
在 Elasticsearch 连接器中，主键用于计算 Elasticsearch 的文档 id，文档 id 为最多 512 字节且不包含空格的字符串。 Elasticsearch 连接器通过使用 document-id.key-delimiter 指定的键分隔符按照 DDL 中定义的顺序连接所有主键字段，为每一行记录生成一个文档 ID 字符串。 某些类型不允许作为主键字段，因为它们没有对应的字符串表示形式，例如，BYTES，ROW，ARRAY，MAP 等。 如果未指定主键，Elasticsearch 将自动生成文档 id。
有关 PRIMARY KEY 语法的更多详细信息，请参见 CREATE TABLE DDL。
动态索引 # Elasticsearch sink 同时支持静态索引和动态索引。
如果你想使用静态索引，则 index 选项值应为纯字符串，例如 'myusers'，所有记录都将被写入到 \u0026ldquo;myusers\u0026rdquo; 索引中。
如果你想使用动态索引，你可以使用 {field_name} 来引用记录中的字段值来动态生成目标索引。 你也可以使用 '{field_name|date_format_string}' 将 TIMESTAMP/DATE/TIME 类型的字段值转换为 date_format_string 指定的格式。 date_format_string 与 Java 的 DateTimeFormatter 兼容。 例如，如果选项值设置为 'myusers-{log_ts|yyyy-MM-dd}'，则 log_ts 字段值为 2020-03-27 12:25:55 的记录将被写入到 \u0026ldquo;myusers-2020-03-27\u0026rdquo; 索引中。
你也可以使用 '{now()|date_format_string}' 将当前的系统时间转换为 date_format_string 指定的格式。now() 对应的时间类型是 TIMESTAMP_WITH_LTZ 。 在将系统时间格式化为字符串时会使用 session 中通过 table.local-time-zone 中配置的时区。 使用 NOW(), now(), CURRENT_TIMESTAMP, current_timestamp 均可以。
注意: 使用当前系统时间生成的动态索引时， 对于 changelog 的流，无法保证同一主键对应的记录能产生相同的索引名, 因此使用基于系统时间的动态索引，只能支持 append only 的流。
数据类型映射 # Elasticsearch 将文档存储在 JSON 字符串中。因此数据类型映射介于 Flink 数据类型和 JSON 数据类型之间。 Flink 为 Elasticsearch 连接器使用内置的 'json' 格式。更多类型映射的详细信息，请参阅 JSON Format 页面。
Back to top
`}),e.add({id:177,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/insert/",title:"INSERT 语句",section:"SQL",content:` INSERT 语句 # INSERT 语句用来向表中添加行。
执行 INSERT 语句 # Java 单条 INSERT 语句，可以使用 TableEnvironment 中的 executeSql() 方法执行。executeSql() 方法执行 INSERT 语句时会立即提交一个 Flink 作业，并且返回一个 TableResult 对象，通过该对象可以获取 JobClient 方便的操作提交的作业。 多条 INSERT 语句，使用 TableEnvironment 中的 createStatementSet 创建一个 StatementSet 对象，然后使用 StatementSet 中的 addInsertSql() 方法添加多条 INSERT 语句，最后通过 StatementSet 中的 execute() 方法来执行。
以下的例子展示了如何在 TableEnvironment 中执行一条 INSERT 语句，或者通过 StatementSet 执行多条 INSERT 语句。
Scala 单条 INSERT 语句，可以使用 TableEnvironment 中的 executeSql() 方法执行。executeSql() 方法执行 INSERT 语句时会立即提交一个 Flink 作业，并且返回一个 TableResult 对象，通过该对象可以获取 JobClient 方便的操作提交的作业。 多条 INSERT 语句，使用 TableEnvironment 中的 createStatementSet 创建一个 StatementSet 对象，然后使用 StatementSet 中的 addInsertSql() 方法添加多条 INSERT 语句，最后通过 StatementSet 中的 execute() 方法来执行。
以下的例子展示了如何在 TableEnvironment 中执行一条 INSERT 语句，或者通过 StatementSet 执行多条 INSERT 语句。
Python 单条 INSERT 语句，可以使用 TableEnvironment 中的 execute_sql() 方法执行。execute_sql() 方法执行 INSERT 语句时会立即提交一个 Flink 作业，并且返回一个 TableResult 对象，通过该对象可以获取 JobClient 方便的操作提交的作业。 多条 INSERT 语句，使用 TableEnvironment 中的 create_statement_set 创建一个 StatementSet 对象，然后使用 StatementSet 中的 add_insert_sql() 方法添加多条 INSERT 语句，最后通过 StatementSet 中的 execute() 方法来执行。
以下的例子展示了如何在 TableEnvironment 中执行一条 INSERT 语句，或者通过 StatementSet 执行多条 INSERT 语句。
SQL CLI 可以在 SQL CLI 中执行 INSERT 语句
以下的例子展示了如何在 SQL CLI 中执行一条 INSERT 语句。
Java TableEnvironment tEnv = TableEnvironment.create(...); // 注册一个 \u0026#34;Orders\u0026#34; 源表，和 \u0026#34;RubberOrders\u0026#34; 结果表 tEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product VARCHAR, amount INT) WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;); // 运行一条 INSERT 语句，将源表的数据输出到结果表中 TableResult tableResult1 = tEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); // 通过 TableResult 来获取作业状态 System.out.println(tableResult1.getJobClient().get().getJobStatus()); //---------------------------------------------------------------------------- // 注册一个 \u0026#34;GlassOrders\u0026#34; 结果表用于运行多 INSERT 语句 tEnv.executeSql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;); // 运行多条 INSERT 语句，将原表数据输出到多个结果表中 StatementSet stmtSet = tEnv.createStatementSet(); // \`addInsertSql\` 方法每次只接收单条 INSERT 语句 stmtSet.addInsertSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;); stmtSet.addInsertSql( \u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;); // 执行刚刚添加的所有 INSERT 语句 TableResult tableResult2 = stmtSet.execute(); // 通过 TableResult 来获取作业状态 System.out.println(tableResult1.getJobClient().get().getJobStatus()); Scala val tEnv = TableEnvironment.create(...) // 注册一个 \u0026#34;Orders\u0026#34; 源表，和 \u0026#34;RubberOrders\u0026#34; 结果表 tEnv.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) // 运行一个 INSERT 语句，将源表的数据输出到结果表中 val tableResult1 = tEnv.executeSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) // 通过 TableResult 来获取作业状态 println(tableResult1.getJobClient().get().getJobStatus()) //---------------------------------------------------------------------------- // 注册一个 \u0026#34;GlassOrders\u0026#34; 结果表用于运行多 INSERT 语句 tEnv.executeSql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;); // 运行多个 INSERT 语句，将原表数据输出到多个结果表中 val stmtSet = tEnv.createStatementSet() // \`addInsertSql\` 方法每次只接收单条 INSERT 语句 stmtSet.addInsertSql( \u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) stmtSet.addInsertSql( \u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;) // 执行刚刚添加的所有 INSERT 语句 val tableResult2 = stmtSet.execute() // 通过 TableResult 来获取作业状态 println(tableResult1.getJobClient().get().getJobStatus()) Python table_env = TableEnvironment.create(...) # 注册一个 \u0026#34;Orders\u0026#34; 源表，和 \u0026#34;RubberOrders\u0026#34; 结果表 table_env.executeSql(\u0026#34;CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...)\u0026#34;) table_env.executeSql(\u0026#34;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)\u0026#34;) # 运行一条 INSERT 语句，将源表的数据输出到结果表中 table_result1 = table_env \\ .executeSql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) # 通过 TableResult 来获取作业状态 print(table_result1.get_job_client().get_job_status()) #---------------------------------------------------------------------------- # 注册一个 \u0026#34;GlassOrders\u0026#34; 结果表用于运行多 INSERT 语句 table_env.execute_sql(\u0026#34;CREATE TABLE GlassOrders(product VARCHAR, amount INT) WITH (...)\u0026#34;) # 运行多条 INSERT 语句，将原表数据输出到多个结果表中 stmt_set = table_env.create_statement_set() # \`add_insert_sql\` 方法每次只接收单条 INSERT 语句 stmt_set \\ .add_insert_sql(\u0026#34;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;\u0026#34;) stmt_set \\ .add_insert_sql(\u0026#34;INSERT INTO GlassOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Glass%\u0026#39;\u0026#34;) # 执行刚刚添加的所有 INSERT 语句 table_result2 = stmt_set.execute() # 通过 TableResult 来获取作业状态 print(table_result2.get_job_client().get_job_status()) SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders (\`user\` BIGINT, product STRING, amount INT) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; Orders RubberOrders Flink SQL\u0026gt; INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE \u0026#39;%Rubber%\u0026#39;; [INFO] Submitting SQL update statement to the cluster... [INFO] Table update statement has been successfully submitted to the cluster: Back to top
将 SELECT 查询数据插入表中 # 通过 INSERT 语句，可以将查询的结果插入到表中，
语法 # [EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name [PARTITION part_spec] select_statement part_spec: (part_col_name1=val1 [, part_col_name2=val2, ...]) OVERWRITE
INSERT OVERWRITE 将会覆盖表中或分区中的任何已存在的数据。否则，新数据会追加到表中或分区中。
PARTITION
PARTITION 语句应该包含需要插入的静态分区列与值。
示例 # -- 创建一个分区表 CREATE TABLE country_page_view (user STRING, cnt INT, date STRING, country STRING) PARTITIONED BY (date, country) WITH (...) -- 追加行到该静态分区中 (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- Insert语句的开头可以额外增加EXECUTE关键字,带EXECUTE关键字和不带是等价的 EXECUTE INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- 追加行到分区 (date, country) 中，其中 date 是静态分区 \u0026#39;2019-8-30\u0026#39;；country 是动态分区，其值由每一行动态决定 INSERT INTO country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;) SELECT user, cnt, country FROM page_view_source; -- 覆盖行到静态分区 (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) INSERT OVERWRITE country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;, country=\u0026#39;China\u0026#39;) SELECT user, cnt FROM page_view_source; -- 覆盖行到分区 (date, country) 中，其中 date 是静态分区 \u0026#39;2019-8-30\u0026#39;；country 是动态分区，其值由每一行动态决定 INSERT OVERWRITE country_page_view PARTITION (date=\u0026#39;2019-8-30\u0026#39;) SELECT user, cnt, country FROM page_view_source; 将值插入表中 # 通过 INSERT 语句，也可以直接将值插入到表中，
语法 # [EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name VALUES values_row [, values_row ...] values_row: : (val1 [, val2, ...]) OVERWRITE
INSERT OVERWRITE 将会覆盖表中的任何已存在的数据。否则，新数据会追加到表中。
示例 # CREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...); EXECUTE INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); 插入数据到多张表 # STATEMENT SET 可以实现通过一个语句插入数据到多个表。
语法 # EXECUTE STATEMENT SET BEGIN insert_statement; ... insert_statement; END; insert_statement: \u0026lt;insert_from_select\u0026gt;|\u0026lt;insert_from_values\u0026gt; 示例 # CREATE TABLE students (name STRING, age INT, gpa DECIMAL(3, 2)) WITH (...); EXECUTE STATEMENT SET BEGIN INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); INSERT INTO students VALUES (\u0026#39;fred flintstone\u0026#39;, 35, 1.28), (\u0026#39;barney rubble\u0026#39;, 32, 2.32); END; Back to top
`}),e.add({id:178,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/maxwell/",title:"Maxwell",section:"Formats",content:` Maxwell Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Maxwell is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into Kafka, Kinesis and other streaming connectors. Maxwell provides a unified format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Maxwell JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as
synchronizing incremental data from databases to other systems auditing logs real-time materialized views on databases temporal join changing history of a database table and so on. Flink also supports to encode the INSERT/UPDATE/DELETE messages in Flink SQL as Maxwell JSON messages, and emit to external systems like Kafka. However, currently Flink can\u0026rsquo;t combine UPDATE_BEFORE and UPDATE_AFTER into a single UPDATE message. Therefore, Flink encodes UPDATE_BEFORE and UDPATE_AFTER as DELETE and INSERT Maxwell messages.
Dependencies # In order to use the Maxwell format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in Note: please refer to Maxwell documentation about how to synchronize changelog to Kafka topics with Maxwell JSON.
How to use Maxwell format # Maxwell provides a unified format for changelog, here is a simple example for an update operation captured from a MySQL products table in JSON format:
{ \u0026#34;database\u0026#34;:\u0026#34;test\u0026#34;, \u0026#34;table\u0026#34;:\u0026#34;e\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;insert\u0026#34;, \u0026#34;ts\u0026#34;:1477053217, \u0026#34;xid\u0026#34;:23396, \u0026#34;commit\u0026#34;:true, \u0026#34;position\u0026#34;:\u0026#34;master.000006:800911\u0026#34;, \u0026#34;server_id\u0026#34;:23042, \u0026#34;thread_id\u0026#34;:108, \u0026#34;primary_key\u0026#34;: [1, \u0026#34;2016-10-21 05:33:37.523000\u0026#34;], \u0026#34;primary_key_columns\u0026#34;: [\u0026#34;id\u0026#34;, \u0026#34;c\u0026#34;], \u0026#34;data\u0026#34;:{ \u0026#34;id\u0026#34;:111, \u0026#34;name\u0026#34;:\u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;:\u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;:5.15 }, \u0026#34;old\u0026#34;:{ \u0026#34;weight\u0026#34;:5.18, } } Note: please refer to Maxwell documentation about the meaning of each fields.
The MySQL products table has 4 columns (id, name, description and weight). The above JSON message is an update change event on the products table where the weight value of the row with id = 111 is changed from 5.18 to 5.15. Assuming this messages is synchronized to Kafka topic products_binlog, then we can use the following DDL to consume this topic and interpret the change events.
CREATE TABLE topic_products ( -- schema is totally the same to the MySQL \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_binlog\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;maxwell-json\u0026#39; ) After registering the topic as a Flink table, then you can consume the Maxwell messages as a changelog source.
-- a real-time materialized view on the MySQL \u0026#34;products\u0026#34; -- which calculate the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of MySQL \u0026#34;products\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format. Key Data Type Description database STRING NULL The originating database. Corresponds to the database field in the Maxwell record if available. table STRING NULL The originating database table. Corresponds to the table field in the Maxwell record if available. primary-key-columns ARRAY\u0026lt;STRING\u0026gt; NULL Array of primary key names. Corresponds to the primary_key_columns field in the Maxwell record if available. ingestion-timestamp TIMESTAMP_LTZ(3) NULL The timestamp at which the connector processed the event. Corresponds to the ts field in the Maxwell record. The following example shows how to access Maxwell metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_database STRING METADATA FROM \u0026#39;value.database\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, origin_primary_key_columns ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.primary-key-columns\u0026#39; VIRTUAL, origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;maxwell-json\u0026#39; ); Format Options # Option Required Default Type Description format required (none) String Specify what format to use, here should be 'maxwell-json'. maxwell-json.ignore-parse-errors optional false Boolean Skip fields and rows with parse errors instead of failing. Fields are set to null in case of errors. maxwell-json.timestamp-format.standard optional 'SQL' String Specify the input and output timestamp format. Currently supported values are 'SQL' and 'ISO-8601': Option 'SQL' will parse input timestamp in "yyyy-MM-dd HH:mm:ss.s{precision}" format, e.g '2020-12-30 12:13:14.123' and output timestamp in the same format. Option 'ISO-8601'will parse input timestamp in "yyyy-MM-ddTHH:mm:ss.s{precision}" format, e.g '2020-12-30T12:13:14.123' and output timestamp in the same format. maxwell-json.map-null-key.mode optional 'FAIL' String Specify the handling mode when serializing null keys for map data. Currently supported values are 'FAIL', 'DROP' and 'LITERAL': Option 'FAIL' will throw exception when encountering map with null key. Option 'DROP' will drop null key entries for map data. Option 'LITERAL' will replace null key with string literal. The string literal is defined by maxwell-json.map-null-key.literal option. maxwell-json.map-null-key.literal optional 'null' String Specify string literal to replace null key when 'maxwell-json.map-null-key.mode' is LITERAL. maxwell-json.encode.decimal-as-plain-number optional false Boolean Encode all decimals as plain numbers instead of possible scientific notations. By default, decimals may be written using scientific notation. For example, 0.000000027 is encoded as 2.7E-8 by default, and will be written as 0.000000027 if set this option to true. Caveats # Duplicate change events # The Maxwell application allows to deliver every change event exactly-once. Flink works pretty well when consuming Maxwell produced events in this situation. If Maxwell application works in at-least-once delivery, it may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions. Thus, it is recommended to set job configuration table.exec.source.cdc-events-duplicate to true and define PRIMARY KEY on the source in this situation. Framework will generate an additional stateful operator, and use the primary key to deduplicate the change events and produce a normalized changelog stream.
Data Type Mapping # Currently, the Maxwell format uses JSON for serialization and deserialization. Please refer to JSON Format documentation for more details about the data type mapping.
`}),e.add({id:179,href:"/flink/flink-docs-master/zh/docs/deployment/metric_reporters/",title:"Metric Reporters",section:"Deployment",content:` Metric Reporters # Flink 支持用户将 Flink 的各项运行时指标发送给外部系统。 了解更多指标方面信息可查看 metric 系统相关文档。
你可以通过 conf/flink-conf.yaml 文件来配置一种或多种发送器，将运行时指标暴露给外部系统。 发送器会在 TaskManager、Flink 作业启动时进行实例化。
下面列出了所有发送器都适用的参数，可以通过配置文件中的 metrics.reporter.\u0026lt;reporter_name\u0026gt;.\u0026lt;property\u0026gt; 项进行配置。有些发送器有自己特有的配置，详见该发送器章节下的具体说明。
键 默认值 数据类型 描述 factory.class (none) String 命名为 \u0026lt;name\u0026gt; 发送器的工厂类名称。 interval 10 s Duration 命名为 \u0026lt;name\u0026gt; 发送器的发送间隔，只支持 push 类型发送器。 scope.delimiter "." String 命名为 \u0026lt;name\u0026gt; 发送器的指标标识符中的间隔符。 scope.variables.additional Map 命名为 \u0026lt;name\u0026gt; 发送器的 map 形式的变量列表，只支持 tags 类型发送器。 scope.variables.excludes "." String 命名为 \u0026lt;name\u0026gt; 发送器应该忽略的一组变量，只支持 tags 类型发送器。 filter.includes "*:*:*" List\u0026lt;String\u0026gt; 命名为 \u0026lt;name\u0026gt; 发送器应包含的运行指标，其过滤条件以列表形式表示，该列表中每一个过滤条件都应遵循如下规范：
\u0026lt;scope\u0026gt;[:\u0026lt;name\u0026gt;[,\u0026lt;name\u0026gt;][:\u0026lt;type\u0026gt;[,\u0026lt;type\u0026gt;]]]`}),e.add({id:180,href:"/flink/flink-docs-master/zh/docs/deployment/repls/python_shell/",title:"Python REPL",section:"REPLs",content:` Python REPL # Flink附带了一个集成的交互式Python Shell。 它既能够运行在本地启动的local模式，也能够运行在集群启动的cluster模式下。 本地安装Flink，请看本地安装页面。 您也可以从源码安装Flink，请看从源码构建 Flink页面。
注意 Python Shell会调用“python”命令。关于Python执行环境的要求，请参考Python Table API环境安装。
你可以通过PyPi安装PyFlink，然后使用Python Shell:
# 安装 PyFlink \$ python -m pip install apache-flink # 执行脚本 \$ pyflink-shell.sh local 关于如何在一个Cluster集群上运行Python shell，可以参考启动章节介绍。
使用 # 当前Python shell支持Table API的功能。 在启动之后，Table Environment的相关内容将会被自动加载。 可以通过变量\u0026quot;bt_env\u0026quot;来使用BatchTableEnvironment，通过变量\u0026quot;st_env\u0026quot;来使用StreamTableEnvironment。
Table API # 下面是一个通过Python Shell 运行的简单示例: stream \u0026gt;\u0026gt;\u0026gt; import tempfile \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; import shutil \u0026gt;\u0026gt;\u0026gt; sink_path = tempfile.gettempdir() + \u0026#39;/streaming.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; if os.path.exists(sink_path): ... if os.path.isfile(sink_path): ... os.remove(sink_path) ... else: ... shutil.rmtree(sink_path) \u0026gt;\u0026gt;\u0026gt; s_env.set_parallelism(1) \u0026gt;\u0026gt;\u0026gt; t = st_env.from_elements([(1, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;), (2, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; st_env.create_temporary_table(\u0026#34;stream_sink\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) ... .schema(Schema.new_builder() ... .column(\u0026#34;a\u0026#34;, DataTypes.BIGINT()) ... .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) ... .column(\u0026#34;c\u0026#34;, DataTypes.STRING()) ... .build()) ... .option(\u0026#34;path\u0026#34;, path) ... .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) ... .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) ... .build()) ... .build()) \u0026gt;\u0026gt;\u0026gt; t.select(col(\u0026#39;a\u0026#39;) + 1, col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;))\\ ... .execute_insert(\u0026#34;stream_sink\u0026#34;).wait() \u0026gt;\u0026gt;\u0026gt; # 如果作业运行在local模式, 你可以执行以下代码查看结果: \u0026gt;\u0026gt;\u0026gt; with open(os.path.join(sink_path, os.listdir(sink_path)[0]), \u0026#39;r\u0026#39;) as f: ... print(f.read()) batch \u0026gt;\u0026gt;\u0026gt; import tempfile \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; import shutil \u0026gt;\u0026gt;\u0026gt; sink_path = tempfile.gettempdir() + \u0026#39;/batch.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; if os.path.exists(sink_path): ... if os.path.isfile(sink_path): ... os.remove(sink_path) ... else: ... shutil.rmtree(sink_path) \u0026gt;\u0026gt;\u0026gt; b_env.set_parallelism(1) \u0026gt;\u0026gt;\u0026gt; t = bt_env.from_elements([(1, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;), (2, \u0026#39;hi\u0026#39;, \u0026#39;hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; st_env.create_temporary_table(\u0026#34;batch_sink\u0026#34;, TableDescriptor.for_connector(\u0026#34;filesystem\u0026#34;) ... .schema(Schema.new_builder() ... .column(\u0026#34;a\u0026#34;, DataTypes.BIGINT()) ... .column(\u0026#34;b\u0026#34;, DataTypes.STRING()) ... .column(\u0026#34;c\u0026#34;, DataTypes.STRING()) ... .build()) ... .option(\u0026#34;path\u0026#34;, path) ... .format(FormatDescriptor.for_format(\u0026#34;csv\u0026#34;) ... .option(\u0026#34;field-delimiter\u0026#34;, \u0026#34;,\u0026#34;) ... .build()) ... .build()) \u0026gt;\u0026gt;\u0026gt; t.select(col(\u0026#39;a\u0026#39;) + 1, col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;))\\ ... .execute_insert(\u0026#34;batch_sink\u0026#34;).wait() \u0026gt;\u0026gt;\u0026gt; # 如果作业运行在local模式, 你可以执行以下代码查看结果: \u0026gt;\u0026gt;\u0026gt; with open(os.path.join(sink_path, os.listdir(sink_path)[0]), \u0026#39;r\u0026#39;) as f: ... print(f.read()) 启动 # 查看Python Shell提供的可选参数，可以使用:
pyflink-shell.sh --help Local # Python Shell运行在local模式下，只需要执行:
pyflink-shell.sh local Remote # Python Shell运行在一个指定的JobManager上，通过关键字remote和对应的JobManager 的地址和端口号来进行指定:
pyflink-shell.sh remote \u0026lt;hostname\u0026gt; \u0026lt;portnumber\u0026gt; Yarn Python Shell cluster # Python Shell可以运行在YARN集群之上。Python shell在Yarn上部署一个新的Flink集群，并进行连接。除了指定container数量，你也 可以指定JobManager的内存，YARN应用的名字等参数。 例如，在一个部署了两个TaskManager的Yarn集群上运行Python Shell:
pyflink-shell.sh yarn -n 2 关于所有可选的参数，可以查看本页面底部的完整说明。
Yarn Session # 如果你已经通过Flink Yarn Session部署了一个Flink集群，能够通过以下的命令连接到这个集群:
pyflink-shell.sh yarn 完整的参考 # Flink Python Shell 使用: pyflink-shell.sh [local|remote|yarn] [options] \u0026lt;args\u0026gt;... 命令: local [选项] 启动一个部署在local的Flink Python shell 使用: -h,--help 查看所有可选的参数 命令: remote [选项] \u0026lt;host\u0026gt; \u0026lt;port\u0026gt; 启动一个部署在remote集群的Flink Python shell \u0026lt;host\u0026gt; JobManager的主机名 \u0026lt;port\u0026gt; JobManager的端口号 使用: -h,--help 查看所有可选的参数 命令: yarn [选项] 启动一个部署在Yarn集群的Flink Python Shell 使用: -h,--help 查看所有可选的参数 -jm,--jobManagerMemory \u0026lt;arg\u0026gt; 具有可选单元的JobManager 的container的内存（默认值：MB) -n,--container \u0026lt;arg\u0026gt; 需要分配的YARN container的 数量 (=TaskManager的数量) -nm,--name \u0026lt;arg\u0026gt; 自定义YARN Application的名字 -qu,--queue \u0026lt;arg\u0026gt; 指定YARN的queue -s,--slots \u0026lt;arg\u0026gt; 每个TaskManager上slots的数量 -tm,--taskManagerMemory \u0026lt;arg\u0026gt; 具有可选单元的每个TaskManager 的container的内存（默认值：MB） -h | --help 打印输出使用文档 Back to top
`}),e.add({id:181,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/rabbitmq/",title:"RabbitMQ",section:"DataStream Connectors",content:` RabbitMQ 连接器 # RabbitMQ 连接器的许可证 # Flink 的 RabbitMQ 连接器依赖了 \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo;，它基于三种协议下发行：Mozilla Public License 1.1 (\u0026ldquo;MPL\u0026rdquo;)、GNU General Public License version 2 (\u0026ldquo;GPL\u0026rdquo;) 和 Apache License version 2 (\u0026ldquo;ASL\u0026rdquo;)。
Flink 自身既没有复用 \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo; 的代码，也没有将 \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo; 打二进制包。
如果用户发布的内容是基于 Flink 的 RabbitMQ 连接器的（进而重新发布了 \u0026ldquo;RabbitMQ AMQP Java Client\u0026rdquo; ），那么一定要注意这可能会受到 Mozilla Public License 1.1 (\u0026ldquo;MPL\u0026rdquo;)、GNU General Public License version 2 (\u0026ldquo;GPL\u0026rdquo;)、Apache License version 2 (\u0026ldquo;ASL\u0026rdquo;) 协议的限制.
RabbitMQ 连接器 # 这个连接器可以访问 RabbitMQ 的数据流。使用这个连接器，需要在工程里添加下面的依赖：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-rabbitmq\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 RabbitMQ connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 注意连接器现在没有包含在二进制发行版中。集群执行的相关信息请参考 这里.
安装 RabbitMQ # 安装 RabbitMQ 请参考 RabbitMQ 下载页面。安装完成之后，服务会自动拉起，应用程序就可以尝试连接到 RabbitMQ 了。
RabbitMQ Source # RMQSource 负责从 RabbitMQ 中消费数据，可以配置三种不同级别的保证：
精确一次: 保证精确一次需要以下条件 - 开启 checkpointing: 开启 checkpointing 之后，消息在 checkpoints 完成之后才会被确认（然后从 RabbitMQ 队列中删除）. 使用关联标识（Correlation ids）: 关联标识是 RabbitMQ 的一个特性，消息写入 RabbitMQ 时在消息属性中设置。 从 checkpoint 恢复时有些消息可能会被重复处理，source 可以利用关联标识对消息进行去重。 非并发 source: 为了保证精确一次的数据投递，source 必须是非并发的（并行度设置为1）。 这主要是由于 RabbitMQ 分发数据时是从单队列向多个消费者投递消息的。 至少一次: 在 checkpointing 开启的条件下，如果没有使用关联标识或者 source 是并发的， 那么 source 就只能提供至少一次的保证。
无任何保证: 如果没有开启 checkpointing，source 就不能提供任何的数据投递保证。 使用这种设置时，source 一旦接收到并处理消息，消息就会被自动确认。
下面是一个保证 exactly-once 的 RabbitMQ source 示例。 注释部分展示了更加宽松的保证应该如何配置。
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // checkpointing is required for exactly-once or at-least-once guarantees env.enableCheckpointing(...); final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build(); final DataStream\u0026lt;String\u0026gt; stream = env .addSource(new RMQSource\u0026lt;String\u0026gt;( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to consume true, // use correlation ids; can be false if only at-least-once is required new SimpleStringSchema())) // deserialization schema to turn messages into Java objects .setParallelism(1); // non-parallel source is only required for exactly-once Scala val env = StreamExecutionEnvironment.getExecutionEnvironment // checkpointing is required for exactly-once or at-least-once guarantees env.enableCheckpointing(...) val connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build val stream = env .addSource(new RMQSource[String]( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to consume true, // use correlation ids; can be false if only at-least-once is required new SimpleStringSchema)) // deserialization schema to turn messages into Java objects .setParallelism(1) // non-parallel source is only required for exactly-once Python env = StreamExecutionEnvironment.get_execution_environment() # checkpointing is required for exactly-once or at-least-once guarantees env.enable_checkpointing(...) connection_config = RMQConnectionConfig.Builder() \\ .set_host(\u0026#34;localhost\u0026#34;) \\ .set_port(5000) \\ ... .build() stream = env \\ .add_source(RMQSource( connection_config, \u0026#34;queueName\u0026#34;, True, SimpleStringSchema(), )) \\ .set_parallelism(1) 服务质量 (QoS) / 消费者预取(Consumer Prefetch) # RabbitMQ Source 通过 RMQConnectionConfig 类提供了一种简单的方式，来设置 source channel 上的 basicQos（见下方示例）。要注意的是这里的 prefetch count 是对单个 channel 设置的，并且由于每个并发的 source 都持有一个 connection/channel，因此这个值实际上会乘以 source 的并行度，来表示同一时间可以向这个 job 总共发送多少条未确认的消息。如果需要更复杂的配置，可以通过重写 RMQSource#setupChannel(Connection) 方法来实现手动配置。
Java final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30_000) ... .build(); Scala val connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30000) ... .build Python connection_config = RMQConnectionConfig.Builder() \\ .set_prefetch_count(30000) \\ ... .build() RabbitMQ Source 默认情况下是不设置 prefetch count 的，这意味着 RabbitMQ 服务器将会无限制地向 source 发送消息。因此在生产环境中，最好要设置它。当消费海量数据的队列并且启用 checkpointing 时，消息只有在做完 checkpoint 后才会被确认，因此也许需要对 prefetch count 做一些调整来减少不必要的循环。
更多关于 QoS 以及 prefetch 相关的内容可以参考 这里. 更多关于在 AMQP 0-9-1 中可选的选项可以参考 这里.
RabbitMQ Sink # 该连接器提供了一个 RMQSink 类，用来向 RabbitMQ 队列发送数据。下面是设置 RabbitMQ sink 的代码示例：
Java final DataStream\u0026lt;String\u0026gt; stream = ... final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build(); stream.addSink(new RMQSink\u0026lt;String\u0026gt;( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to send messages to new SimpleStringSchema())); // serialization schema to turn Java objects to messages Scala val stream: DataStream[String] = ... val connectionConfig = new RMQConnectionConfig.Builder() .setHost(\u0026#34;localhost\u0026#34;) .setPort(5000) ... .build stream.addSink(new RMQSink[String]( connectionConfig, // config for the RabbitMQ connection \u0026#34;queueName\u0026#34;, // name of the RabbitMQ queue to send messages to new SimpleStringSchema)) // serialization schema to turn Java objects to messages Python stream = ... connection_config = RMQConnectionConfig.Builder() \\ .set_host(\u0026#34;localhost\u0026#34;) \\ .set_port(5000) \\ ... .build() stream.add_sink(RMQSink( connection_config, # config for the RabbitMQ connection \u0026#39;queueName\u0026#39;, # name of the RabbitMQ queue to send messages to SimpleStringSchema())) # serialization schema to turn Java objects to messages 更多关于 RabbitMQ 的信息请参考 这里.
Back to top
`}),e.add({id:182,href:"/flink/flink-docs-master/zh/docs/ops/rest_api/",title:"REST API",section:"Operations",content:` REST API # Flink 具有监控 API ，可用于查询正在运行的作业以及最近完成的作业的状态和统计信息。该监控 API 被用于 Flink 自己的仪表盘，同时也可用于自定义监控工具。
该监控 API 是 REST-ful 风格的，可以接受 HTTP 请求并返回 JSON 格式的数据。
概览 # 该监控 API 由作为 JobManager 一部分运行的 web 服务器提供支持。默认情况下，该服务器监听 8081 端口，端口号可以通过修改 flink-conf.yaml 文件的 rest.port 进行配置。请注意，该监控 API 的 web 服务器和仪表盘的 web 服务器目前是相同的，因此在同一端口一起运行。不过，它们响应不同的 HTTP URL 。
在多个 JobManager 的情况下（为了高可用），每个 JobManager 将运行自己的监控 API 实例，当 JobManager 被选举成为集群 leader 时，该实例将提供已完成和正在运行作业的相关信息。
拓展 # 该 REST API 后端位于 flink-runtime 项目中。核心类是 org.apache.flink.runtime.webmonitor.WebMonitorEndpoint ，用来配置服务器和请求路由。
我们使用 Netty 和 Netty Router 库来处理 REST 请求和转换 URL 。选择该选项是因为这种组合具有轻量级依赖关系，并且 Netty HTTP 的性能非常好。
添加新的请求，需要
添加一个新的 MessageHeaders 类，作为新请求的接口， 添加一个新的 AbstractRestHandler 类，该类接收并处理 MessageHeaders 类的请求， 将处理程序添加到 org.apache.flink.runtime.webmonitor.WebMonitorEndpoint#initializeHandlers() 中。 一个很好的例子是使用 org.apache.flink.runtime.rest.messages.JobExceptionsHeaders 的 org.apache.flink.runtime.rest.handler.job.JobExceptionsHandler 。
API # 该 REST API 已版本化，可以通过在 URL 前面加上版本前缀来查询指定版本。前缀格式始终为 v[version_number] 。 例如，要访问版本 1 的 /foo/bar 接口，将查询 /v1/foo/bar 。
如果未指定版本， Flink 将默认使用支持该请求的最旧版本。
查询 不支持/不存在 的版本将返回 404 错误。
这些 API 中存在几种异步操作，例如：trigger savepoint 、 rescale a job 。它们将返回 triggerid 来标识你刚刚执行的 POST 请求，然后你需要使用该 triggerid 查询该操作的状态。
JobManager # OpenAPI specification
The OpenAPI specification is still experimental. API reference # v1 /cluster Verb: DELETE Response code: 200 OK Shuts down the cluster Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /config Verb: GET Response code: 200 OK Returns the configuration of the WebUI. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:DashboardConfiguration", "properties" : { "features" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:DashboardConfiguration:Features", "properties" : { "web-cancel" : { "type" : "boolean" }, "web-history" : { "type" : "boolean" }, "web-submit" : { "type" : "boolean" } } }, "flink-revision" : { "type" : "string" }, "flink-version" : { "type" : "string" }, "refresh-interval" : { "type" : "integer" }, "timezone-name" : { "type" : "string" }, "timezone-offset" : { "type" : "integer" } } } /datasets Verb: GET Response code: 200 OK Returns all cluster data sets. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:dataset:ClusterDataSetListResponseBody", "properties" : { "dataSets" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:dataset:ClusterDataSetEntry", "properties" : { "id" : { "type" : "string" }, "isComplete" : { "type" : "boolean" } } } } } } /datasets/delete/:triggerid Verb: GET Response code: 200 OK Returns the status for the delete operation of a cluster data set. Path parameters triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /datasets/:datasetid Verb: DELETE Response code: 202 Accepted Triggers the deletion of a cluster data set. This async operation would return a 'triggerid' for further query identifier. Path parameters datasetid - 32-character hexadecimal string value that identifies a cluster data set. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jars Verb: GET Response code: 200 OK Returns a list of all jars previously uploaded via '/jars/upload'. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo", "properties" : { "address" : { "type" : "string" }, "files" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo:JarFileInfo", "properties" : { "entry" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarListInfo:JarEntryInfo", "properties" : { "description" : { "type" : "string" }, "name" : { "type" : "string" } } } }, "id" : { "type" : "string" }, "name" : { "type" : "string" }, "uploaded" : { "type" : "integer" } } } } } } /jars/upload Verb: POST Response code: 200 OK Uploads a jar to the cluster. The jar must be sent as multi-part data. Make sure that the "Content-Type" header is set to "application/x-java-archive", as some http libraries do not add the header by default. Using 'curl' you can upload a jar via 'curl -X POST -H "Expect:" -F "jarfile=@path/to/flink-job.jar" http://hostname:port/jars/upload'. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarUploadResponseBody", "properties" : { "filename" : { "type" : "string" }, "status" : { "type" : "string", "enum" : [ "success" ] } } } /jars/:jarid Verb: DELETE Response code: 200 OK Deletes a jar previously uploaded via '/jars/upload'. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /jars/:jarid/plan Verb: POST Response code: 200 OK Returns the dataflow plan of a job contained in a jar previously uploaded via '/jars/upload'. Program arguments can be passed both via the JSON request (recommended) or query parameters. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Query parameters program-args (optional): Deprecated, please use 'programArg' instead. String value that specifies the arguments for the program or plan programArg (optional): Comma-separated list of program arguments. entry-class (optional): String value that specifies the fully qualified name of the entry point class. Overrides the class defined in the jar file manifest. parallelism (optional): Positive integer value that specifies the desired parallelism for the job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarPlanRequestBody", "properties" : { "entryClass" : { "type" : "string" }, "jobId" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "programArgs" : { "type" : "string" }, "programArgsList" : { "type" : "array", "items" : { "type" : "string" } } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobPlanInfo", "properties" : { "plan" : { "type" : "any" } } } /jars/:jarid/run Verb: POST Response code: 200 OK Submits a job by running a jar previously uploaded via '/jars/upload'. Program arguments can be passed both via the JSON request (recommended) or query parameters. Path parameters jarid - String value that identifies a jar. When uploading the jar a path is returned, where the filename is the ID. This value is equivalent to the \`id\` field in the list of uploaded jars (/jars). Query parameters allowNonRestoredState (optional): Boolean value that specifies whether the job submission should be rejected if the savepoint contains state that cannot be mapped back to the job. savepointPath (optional): String value that specifies the path of the savepoint to restore the job from. program-args (optional): Deprecated, please use 'programArg' instead. String value that specifies the arguments for the program or plan programArg (optional): Comma-separated list of program arguments. entry-class (optional): String value that specifies the fully qualified name of the entry point class. Overrides the class defined in the jar file manifest. parallelism (optional): Positive integer value that specifies the desired parallelism for the job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarRunRequestBody", "properties" : { "allowNonRestoredState" : { "type" : "boolean" }, "entryClass" : { "type" : "string" }, "jobId" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "programArgs" : { "type" : "string" }, "programArgsList" : { "type" : "array", "items" : { "type" : "string" } }, "restoreMode" : { "type" : "string", "enum" : [ "CLAIM", "NO_CLAIM", "LEGACY" ] }, "savepointPath" : { "type" : "string" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:handlers:JarRunResponseBody", "properties" : { "jobid" : { "type" : "any" } } } /jobmanager/config Verb: GET Response code: 200 OK Returns the cluster configuration. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ConfigurationInfoEntry", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } } /jobmanager/environment Verb: GET Response code: 200 OK Returns the jobmanager environment. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo", "properties" : { "classpath" : { "type" : "array", "items" : { "type" : "string" } }, "environment" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:EnvironmentVariableItem", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } }, "jvm" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:JVMInfo", "properties" : { "arch" : { "type" : "string" }, "options" : { "type" : "array", "items" : { "type" : "string" } }, "version" : { "type" : "string" } } } } } /jobmanager/logs Verb: GET Response code: 200 OK Returns the list of log files on the JobManager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogListInfo", "properties" : { "logs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogInfo", "properties" : { "mtime" : { "type" : "integer" }, "name" : { "type" : "string" }, "size" : { "type" : "integer" } } } } } } /jobmanager/metrics Verb: GET Response code: 200 OK Provides access to job manager metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobmanager/thread-dump Verb: GET Response code: 200 OK Returns the thread dump of the JobManager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo", "properties" : { "threadInfos" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo:ThreadInfo", "properties" : { "stringifiedThreadInfo" : { "type" : "string" }, "threadName" : { "type" : "string" } } } } } } /jobs Verb: GET Response code: 200 OK Returns an overview over all jobs and their current state. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobIdsWithStatusOverview", "properties" : { "jobs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobIdsWithStatusOverview:JobIdWithStatus", "properties" : { "id" : { "type" : "any" }, "status" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] } } } } } } /jobs Verb: POST Response code: 202 Accepted Submits a job. This call is primarily intended to be used by the Flink client. This call expects a multipart/form-data request that consists of file uploads for the serialized JobGraph, jars and distributed cache artifacts and an attribute named "request" for the JSON payload. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitRequestBody", "properties" : { "jobArtifactFileNames" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitRequestBody:DistributedCacheFile", "properties" : { "entryName" : { "type" : "string" }, "fileName" : { "type" : "string" } } } }, "jobGraphFileName" : { "type" : "string" }, "jobJarFileNames" : { "type" : "array", "items" : { "type" : "string" } } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobSubmitResponseBody", "properties" : { "jobUrl" : { "type" : "string" } } } /jobs/metrics Verb: GET Response code: 200 OK Provides access to aggregated job metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". jobs (optional): Comma-separated list of 32-character hexadecimal strings to select specific jobs. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/overview Verb: GET Response code: 200 OK Returns an overview over all jobs. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:MultipleJobsDetails", "properties" : { "jobs" : { "type" : "array", "items" : { "type" : "any" } } } } /jobs/:jobid Verb: GET Response code: 200 OK Returns details of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobDetailsInfo", "properties" : { "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "isStoppable" : { "type" : "boolean" }, "jid" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "plan" : { "type" : "string" }, "start-time" : { "type" : "integer" }, "state" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] }, "status-counts" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "timestamps" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "vertices" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobDetailsInfo:JobVertexDetailsInfo", "properties" : { "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "id" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "name" : { "type" : "string" }, "parallelism" : { "type" : "integer" }, "start-time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } } } } /jobs/:jobid Verb: PATCH Response code: 202 Accepted Terminates a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters mode (optional): String value that specifies the termination mode. The only supported value is: "cancel". Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ {} /jobs/:jobid/accumulators Verb: GET Response code: 200 OK Returns the accumulators for all tasks of a job, aggregated across the respective subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters includeSerializedValue (optional): Boolean value that specifies whether serialized user task accumulators should be included in the response. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobAccumulatorsInfo", "properties" : { "job-accumulators" : { "type" : "array", "items" : { "type" : "any" } }, "serialized-user-task-accumulators" : { "type" : "object", "additionalProperties" : { "type" : "any" } }, "user-task-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobAccumulatorsInfo:UserTaskAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/checkpoints Verb: GET Response code: 200 OK Returns checkpointing statistics for a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics", "properties" : { "counts" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:Counts", "properties" : { "completed" : { "type" : "integer" }, "failed" : { "type" : "integer" }, "in_progress" : { "type" : "integer" }, "restored" : { "type" : "integer" }, "total" : { "type" : "integer" } } }, "history" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics" } }, "trigger_timestamp" : { "type" : "integer" } } } }, "latest" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:LatestCheckpoints", "properties" : { "completed" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:CompletedCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "discarded" : { "type" : "boolean" }, "end_to_end_duration" : { "type" : "integer" }, "external_path" : { "type" : "string" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] } } } }, "trigger_timestamp" : { "type" : "integer" } } }, "failed" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:FailedCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "failure_message" : { "type" : "string" }, "failure_timestamp" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics" } }, "trigger_timestamp" : { "type" : "integer" } } }, "restored" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:RestoredCheckpointStatistics", "properties" : { "external_path" : { "type" : "string" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "restore_timestamp" : { "type" : "integer" } } }, "savepoint" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:CompletedCheckpointStatistics" } } }, "summary" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:Summary", "properties" : { "alignment_buffered" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "checkpointed_size" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto", "properties" : { "avg" : { "type" : "integer" }, "max" : { "type" : "integer" }, "min" : { "type" : "integer" }, "p50" : { "type" : "number" }, "p90" : { "type" : "number" }, "p95" : { "type" : "number" }, "p99" : { "type" : "number" }, "p999" : { "type" : "number" } } }, "end_to_end_duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "persisted_data" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "processed_data" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "state_size" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } } } } /jobs/:jobid/checkpoints/config Verb: GET Response code: 200 OK Returns the checkpointing configuration. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointConfigInfo", "properties" : { "aligned_checkpoint_timeout" : { "type" : "integer" }, "changelog_periodic_materialization_interval" : { "type" : "integer" }, "changelog_storage" : { "type" : "string" }, "checkpoint_storage" : { "type" : "string" }, "checkpoints_after_tasks_finish" : { "type" : "boolean" }, "externalization" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointConfigInfo:ExternalizedCheckpointInfo", "properties" : { "delete_on_cancellation" : { "type" : "boolean" }, "enabled" : { "type" : "boolean" } } }, "interval" : { "type" : "integer" }, "max_concurrent" : { "type" : "integer" }, "min_pause" : { "type" : "integer" }, "mode" : { "type" : "any" }, "state_backend" : { "type" : "string" }, "state_changelog_enabled" : { "type" : "boolean" }, "timeout" : { "type" : "integer" }, "tolerable_failed_checkpoints" : { "type" : "integer" }, "unaligned_checkpoints" : { "type" : "boolean" } } } /jobs/:jobid/checkpoints/details/:checkpointid Verb: GET Response code: 200 OK Returns details for a checkpoint. Path parameters jobid - 32-character hexadecimal string value that identifies a job. checkpointid - Long value that identifies a checkpoint. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpoint_type" : { "type" : "string", "enum" : [ "CHECKPOINT", "SAVEPOINT", "SYNC_SAVEPOINT" ] }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "is_savepoint" : { "type" : "boolean" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "tasks" : { "type" : "object", "additionalProperties" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatistics", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] } } } }, "trigger_timestamp" : { "type" : "integer" } } } /jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid Verb: GET Response code: 200 OK Returns checkpoint statistics for a task and its subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. checkpointid - Long value that identifies a checkpoint. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails", "properties" : { "alignment_buffered" : { "type" : "integer" }, "checkpointed_size" : { "type" : "integer" }, "end_to_end_duration" : { "type" : "integer" }, "id" : { "type" : "integer" }, "latest_ack_timestamp" : { "type" : "integer" }, "num_acknowledged_subtasks" : { "type" : "integer" }, "num_subtasks" : { "type" : "integer" }, "persisted_data" : { "type" : "integer" }, "processed_data" : { "type" : "integer" }, "state_size" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "IN_PROGRESS", "COMPLETED", "FAILED" ] }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:SubtaskCheckpointStatistics", "properties" : { "index" : { "type" : "integer" }, "status" : { "type" : "string" } } } }, "summary" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:Summary", "properties" : { "alignment" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:CheckpointAlignment", "properties" : { "buffered" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "persisted" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "processed" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } }, "checkpoint_duration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:TaskCheckpointStatisticsWithSubtaskDetails:CheckpointDuration", "properties" : { "async" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "sync" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } }, "checkpointed_size" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto", "properties" : { "avg" : { "type" : "integer" }, "max" : { "type" : "integer" }, "min" : { "type" : "integer" }, "p50" : { "type" : "number" }, "p90" : { "type" : "number" }, "p95" : { "type" : "number" }, "p99" : { "type" : "number" }, "p999" : { "type" : "number" } } }, "end_to_end_duration" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "start_delay" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" }, "state_size" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto" } } } } } /jobs/:jobid/config Verb: GET Response code: 200 OK Returns the configuration of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/exceptions Verb: GET Response code: 200 OK Returns the most recent exceptions that have been handled by Flink for this job. The 'exceptionHistory.truncated' flag defines whether exceptions were filtered out through the GET parameter. The backend collects only a specific amount of most recent exceptions per job. This can be configured through web.exception-history-size in the Flink configuration. The following first-level members are deprecated: 'root-exception', 'timestamp', 'all-exceptions', and 'truncated'. Use the data provided through 'exceptionHistory', instead. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters maxExceptions (optional): Comma-separated list of integer values that specifies the upper limit of exceptions to return. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory", "properties" : { "all-exceptions" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfo:ExecutionExceptionInfo", "properties" : { "exception" : { "type" : "string" }, "location" : { "type" : "string" }, "task" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "exceptionHistory" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:JobExceptionHistory", "properties" : { "entries" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:RootExceptionInfo", "properties" : { "concurrentExceptions" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobExceptionsInfoWithHistory:ExceptionInfo", "properties" : { "exceptionName" : { "type" : "string" }, "location" : { "type" : "string" }, "stacktrace" : { "type" : "string" }, "taskName" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "exceptionName" : { "type" : "string" }, "location" : { "type" : "string" }, "stacktrace" : { "type" : "string" }, "taskName" : { "type" : "string" }, "timestamp" : { "type" : "integer" } } } }, "truncated" : { "type" : "boolean" } } }, "root-exception" : { "type" : "string" }, "timestamp" : { "type" : "integer" }, "truncated" : { "type" : "boolean" } } } /jobs/:jobid/execution-result Verb: GET Response code: 200 OK Returns the result of a job execution. Gives access to the execution time of the job and to all accumulators created by this job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:JobExecutionResultResponseBody", "properties" : { "job-execution-result" : { "type" : "any" }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "required" : true, "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/jobmanager/config Verb: GET Response code: 200 OK Returns the jobmanager's configuration of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ConfigurationInfoEntry", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } } /jobs/:jobid/jobmanager/environment Verb: GET Response code: 200 OK Returns the jobmanager's environment variables of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo", "properties" : { "classpath" : { "type" : "array", "items" : { "type" : "string" } }, "environment" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:EnvironmentVariableItem", "properties" : { "key" : { "type" : "string" }, "value" : { "type" : "string" } } } }, "jvm" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:EnvironmentInfo:JVMInfo", "properties" : { "arch" : { "type" : "string" }, "options" : { "type" : "array", "items" : { "type" : "string" } }, "version" : { "type" : "string" } } } } } /jobs/:jobid/jobmanager/log-url Verb: GET Response code: 200 OK Returns the log url of jobmanager of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogUrlResponse", "properties" : { "url" : { "type" : "string" } } } /jobs/:jobid/metrics Verb: GET Response code: 200 OK Provides access to job metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/plan Verb: GET Response code: 200 OK Returns the dataflow plan of a job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobPlanInfo", "properties" : { "plan" : { "type" : "any" } } } /jobs/:jobid/rescaling Verb: PATCH Response code: 200 OK Triggers the rescaling of a job. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Query parameters parallelism (mandatory): Positive integer value that specifies the desired parallelism. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/rescaling/:triggerid Verb: GET Response code: 200 OK Returns the status of a rescaling operation. Path parameters jobid - 32-character hexadecimal string value that identifies a job. triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/savepoints Verb: POST Response code: 202 Accepted Triggers a savepoint, and optionally cancels the job afterwards. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointTriggerRequestBody", "properties" : { "cancel-job" : { "type" : "boolean" }, "formatType" : { "type" : "string", "enum" : [ "CANONICAL", "NATIVE" ] }, "target-directory" : { "type" : "string" }, "triggerId" : { "type" : "any" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/savepoints/:triggerid Verb: GET Response code: 200 OK Returns the status of a savepoint operation. Path parameters jobid - 32-character hexadecimal string value that identifies a job. triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointInfo", "properties" : { "failure-cause" : { "type" : "any" }, "location" : { "type" : "string" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /jobs/:jobid/status Verb: GET Response code: 200 OK Returns the current status of a job execution. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:messages:webmonitor:JobStatusInfo", "properties" : { "status" : { "type" : "string", "enum" : [ "INITIALIZING", "CREATED", "RUNNING", "FAILING", "FAILED", "CANCELLING", "CANCELED", "FINISHED", "RESTARTING", "SUSPENDED", "RECONCILING" ] } } } /jobs/:jobid/stop Verb: POST Response code: 202 Accepted Stops a job with a savepoint. Optionally, it can also emit a MAX_WATERMARK before taking the savepoint to flush out any state waiting for timers to fire. This async operation would return a 'triggerid' for further query identifier. Path parameters jobid - 32-character hexadecimal string value that identifies a job. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:stop:StopWithSavepointRequestBody", "properties" : { "drain" : { "type" : "boolean" }, "formatType" : { "type" : "string", "enum" : [ "CANONICAL", "NATIVE" ] }, "targetDirectory" : { "type" : "string" }, "triggerId" : { "type" : "any" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /jobs/:jobid/taskmanagers/:taskmanagerid/log-url Verb: GET Response code: 200 OK Returns the log url of jobmanager of a specific job. Path parameters jobid - 32-character hexadecimal string value that identifies a job. taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogUrlResponse", "properties" : { "url" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid Verb: GET Response code: 200 OK Returns details for a task, with a summary for each of its subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexDetailsInfo", "properties" : { "aggregated" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:AggregatedTaskDetailsInfo", "properties" : { "metrics" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } }, "id" : { "type" : "any" }, "maxParallelism" : { "type" : "integer" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "parallelism" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/accumulators Verb: GET Response code: 200 OK Returns user-defined accumulators of a task, aggregated across all subtasks. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexAccumulatorsInfo", "properties" : { "id" : { "type" : "string" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/backpressure Verb: GET Response code: 200 OK Returns back-pressure information for a job, and may initiate back-pressure sampling if necessary. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexBackPressureInfo", "properties" : { "backpressure-level" : { "type" : "string", "enum" : [ "ok", "low", "high" ] }, "end-timestamp" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "deprecated", "ok" ] }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexBackPressureInfo:SubtaskBackPressureInfo", "properties" : { "backpressure-level" : { "type" : "string", "enum" : [ "ok", "low", "high" ] }, "busyRatio" : { "type" : "number" }, "idleRatio" : { "type" : "number" }, "ratio" : { "type" : "number" }, "subtask" : { "type" : "integer" } } } } } } /jobs/:jobid/vertices/:vertexid/flamegraph Verb: GET Response code: 200 OK Returns flame graph information for a vertex, and may initiate flame graph sampling if necessary. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters type (optional): String value that specifies the Flame Graph type. Supported options are: "[FULL, ON_CPU, OFF_CPU]". Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph", "properties" : { "data" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph:Node", "properties" : { "children" : { "type" : "array", "items" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:webmonitor:threadinfo:JobVertexFlameGraph:Node" } }, "name" : { "type" : "string" }, "value" : { "type" : "integer" } } }, "endTimestamp" : { "type" : "integer" } } } /jobs/:jobid/vertices/:vertexid/metrics Verb: GET Response code: 200 OK Provides access to task metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasks/accumulators Verb: GET Response code: 200 OK Returns all user-defined accumulators for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtasksAllAccumulatorsInfo", "properties" : { "id" : { "type" : "any" }, "parallelism" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtasksAllAccumulatorsInfo:SubtaskAccumulatorsInfo", "properties" : { "attempt" : { "type" : "integer" }, "host" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } } } } /jobs/:jobid/vertices/:vertexid/subtasks/metrics Verb: GET Response code: 200 OK Provides access to aggregated subtask metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". subtasks (optional): Comma-separated list of integer ranges (e.g. "1,3,5-9") to select specific subtasks. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex Verb: GET Response code: 200 OK Returns details of the current or latest execution attempt of a subtask. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt Verb: GET Response code: 200 OK Returns details of an execution attempt of a subtask. Multiple execution attempts happen in case of failure/recovery. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. attempt - Positive integer value that identifies an execution attempt. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptDetailsInfo", "properties" : { "attempt" : { "type" : "integer" }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "start_time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "subtask" : { "type" : "integer" }, "taskmanager-id" : { "type" : "string" } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/attempts/:attempt/accumulators Verb: GET Response code: 200 OK Returns the accumulators of an execution attempt of a subtask. Multiple execution attempts happen in case of failure/recovery. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. attempt - Positive integer value that identifies an execution attempt. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:SubtaskExecutionAttemptAccumulatorsInfo", "properties" : { "attempt" : { "type" : "integer" }, "id" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "user-accumulators" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:UserAccumulator", "properties" : { "name" : { "type" : "string" }, "type" : { "type" : "string" }, "value" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/subtasks/:subtaskindex/metrics Verb: GET Response code: 200 OK Provides access to subtask metrics. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. subtaskindex - Positive integer value that identifies a subtask. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /jobs/:jobid/vertices/:vertexid/subtasktimes Verb: GET Response code: 200 OK Returns time-related information for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:SubtasksTimesInfo", "properties" : { "id" : { "type" : "string" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "subtasks" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:SubtasksTimesInfo:SubtaskTimeInfo", "properties" : { "duration" : { "type" : "integer" }, "host" : { "type" : "string" }, "subtask" : { "type" : "integer" }, "timestamps" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } } } } /jobs/:jobid/vertices/:vertexid/taskmanagers Verb: GET Response code: 200 OK Returns task information aggregated by task manager. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexTaskManagersInfo", "properties" : { "id" : { "type" : "any" }, "name" : { "type" : "string" }, "now" : { "type" : "integer" }, "taskmanagers" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:JobVertexTaskManagersInfo:TaskManagersInfo", "properties" : { "aggregated" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:AggregatedTaskDetailsInfo", "properties" : { "metrics" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } }, "status-duration" : { "type" : "object", "additionalProperties" : { "type" : "object", "additionalProperties" : { "type" : "integer" } } } } }, "duration" : { "type" : "integer" }, "end-time" : { "type" : "integer" }, "host" : { "type" : "string" }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo", "properties" : { "accumulated-backpressured-time" : { "type" : "integer" }, "accumulated-busy-time" : { "type" : "number" }, "accumulated-idle-time" : { "type" : "integer" }, "read-bytes" : { "type" : "integer" }, "read-bytes-complete" : { "type" : "boolean" }, "read-records" : { "type" : "integer" }, "read-records-complete" : { "type" : "boolean" }, "write-bytes" : { "type" : "integer" }, "write-bytes-complete" : { "type" : "boolean" }, "write-records" : { "type" : "integer" }, "write-records-complete" : { "type" : "boolean" } } }, "start-time" : { "type" : "integer" }, "status" : { "type" : "string", "enum" : [ "CREATED", "SCHEDULED", "DEPLOYING", "RUNNING", "FINISHED", "CANCELING", "CANCELED", "FAILED", "RECONCILING", "INITIALIZING" ] }, "status-counts" : { "type" : "object", "additionalProperties" : { "type" : "integer" } }, "taskmanager-id" : { "type" : "string" } } } } } } /jobs/:jobid/vertices/:vertexid/watermarks Verb: GET Response code: 200 OK Returns the watermarks for all subtasks of a task. Path parameters jobid - 32-character hexadecimal string value that identifies a job. vertexid - 32-character hexadecimal string value that identifies a job vertex. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /overview Verb: GET Response code: 200 OK Returns an overview over the Flink cluster. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:legacy:messages:ClusterOverviewWithVersion", "properties" : { "flink-commit" : { "type" : "string" }, "flink-version" : { "type" : "string" }, "jobs-cancelled" : { "type" : "integer" }, "jobs-failed" : { "type" : "integer" }, "jobs-finished" : { "type" : "integer" }, "jobs-running" : { "type" : "integer" }, "slots-available" : { "type" : "integer" }, "slots-total" : { "type" : "integer" }, "taskmanagers" : { "type" : "integer" } } } /savepoint-disposal Verb: POST Response code: 200 OK Triggers the desposal of a savepoint. This async operation would return a 'triggerid' for further query identifier. Request \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:job:savepoints:SavepointDisposalRequest", "properties" : { "savepoint-path" : { "type" : "string" } } } Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:TriggerResponse", "properties" : { "request-id" : { "type" : "any" } } } /savepoint-disposal/:triggerid Verb: GET Response code: 200 OK Returns the status of a savepoint disposal operation. Path parameters triggerid - 32-character hexadecimal string that identifies an asynchronous operation trigger ID. The ID was returned then the operation was triggered. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationResult", "properties" : { "operation" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:handler:async:AsynchronousOperationInfo", "properties" : { "failure-cause" : { "type" : "any" } } }, "status" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:queue:QueueStatus", "properties" : { "id" : { "type" : "string", "required" : true, "enum" : [ "IN_PROGRESS", "COMPLETED" ] } } } } } /taskmanagers Verb: GET Response code: 200 OK Returns an overview over all task managers. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagersInfo", "properties" : { "taskmanagers" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerInfo", "properties" : { "dataPort" : { "type" : "integer" }, "freeResource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" }, "freeSlots" : { "type" : "integer" }, "hardware" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:instance:HardwareDescription", "properties" : { "cpuCores" : { "type" : "integer" }, "freeMemory" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "physicalMemory" : { "type" : "integer" } } }, "id" : { "type" : "any" }, "jmxPort" : { "type" : "integer" }, "memoryConfiguration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:taskexecutor:TaskExecutorMemoryConfiguration", "properties" : { "frameworkHeap" : { "type" : "integer" }, "frameworkOffHeap" : { "type" : "integer" }, "jvmMetaspace" : { "type" : "integer" }, "jvmOverhead" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeap" : { "type" : "integer" }, "taskOffHeap" : { "type" : "integer" }, "totalFlinkMemory" : { "type" : "integer" }, "totalProcessMemory" : { "type" : "integer" } } }, "path" : { "type" : "string" }, "slotsNumber" : { "type" : "integer" }, "timeSinceLastHeartbeat" : { "type" : "integer" }, "totalResource" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo", "properties" : { "cpuCores" : { "type" : "number" }, "extendedResources" : { "type" : "object", "additionalProperties" : { "type" : "number" } }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeapMemory" : { "type" : "integer" }, "taskOffHeapMemory" : { "type" : "integer" } } } } } } } } /taskmanagers/metrics Verb: GET Response code: 200 OK Provides access to aggregated task manager metrics. Query parameters get (optional): Comma-separated list of string values to select specific metrics. agg (optional): Comma-separated list of aggregation modes which should be calculated. Available aggregations are: "min, max, sum, avg". taskmanagers (optional): Comma-separated list of 32-character hexadecimal strings to select specific task managers. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /taskmanagers/:taskmanagerid Verb: GET Response code: 200 OK Returns details for a task manager. "metrics.memorySegmentsAvailable" and "metrics.memorySegmentsTotal" are deprecated. Please use "metrics.nettyShuffleMemorySegmentsAvailable" and "metrics.nettyShuffleMemorySegmentsTotal" instead. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerDetailsInfo", "properties" : { "allocatedSlots" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:SlotInfo", "properties" : { "jobId" : { "type" : "any" }, "resource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" } } } }, "dataPort" : { "type" : "integer" }, "freeResource" : { "type" : "object", "\$ref" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo" }, "freeSlots" : { "type" : "integer" }, "hardware" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:instance:HardwareDescription", "properties" : { "cpuCores" : { "type" : "integer" }, "freeMemory" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "physicalMemory" : { "type" : "integer" } } }, "id" : { "type" : "any" }, "jmxPort" : { "type" : "integer" }, "memoryConfiguration" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:taskexecutor:TaskExecutorMemoryConfiguration", "properties" : { "frameworkHeap" : { "type" : "integer" }, "frameworkOffHeap" : { "type" : "integer" }, "jvmMetaspace" : { "type" : "integer" }, "jvmOverhead" : { "type" : "integer" }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeap" : { "type" : "integer" }, "taskOffHeap" : { "type" : "integer" }, "totalFlinkMemory" : { "type" : "integer" }, "totalProcessMemory" : { "type" : "integer" } } }, "metrics" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerMetricsInfo", "properties" : { "directCount" : { "type" : "integer" }, "directMax" : { "type" : "integer" }, "directUsed" : { "type" : "integer" }, "garbageCollectors" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:taskmanager:TaskManagerMetricsInfo:GarbageCollectorInfo", "properties" : { "count" : { "type" : "integer" }, "name" : { "type" : "string" }, "time" : { "type" : "integer" } } } }, "heapCommitted" : { "type" : "integer" }, "heapMax" : { "type" : "integer" }, "heapUsed" : { "type" : "integer" }, "mappedCount" : { "type" : "integer" }, "mappedMax" : { "type" : "integer" }, "mappedUsed" : { "type" : "integer" }, "memorySegmentsAvailable" : { "type" : "integer" }, "memorySegmentsTotal" : { "type" : "integer" }, "nettyShuffleMemoryAvailable" : { "type" : "integer" }, "nettyShuffleMemorySegmentsAvailable" : { "type" : "integer" }, "nettyShuffleMemorySegmentsTotal" : { "type" : "integer" }, "nettyShuffleMemorySegmentsUsed" : { "type" : "integer" }, "nettyShuffleMemoryTotal" : { "type" : "integer" }, "nettyShuffleMemoryUsed" : { "type" : "integer" }, "nonHeapCommitted" : { "type" : "integer" }, "nonHeapMax" : { "type" : "integer" }, "nonHeapUsed" : { "type" : "integer" } } }, "path" : { "type" : "string" }, "slotsNumber" : { "type" : "integer" }, "timeSinceLastHeartbeat" : { "type" : "integer" }, "totalResource" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ResourceProfileInfo", "properties" : { "cpuCores" : { "type" : "number" }, "extendedResources" : { "type" : "object", "additionalProperties" : { "type" : "number" } }, "managedMemory" : { "type" : "integer" }, "networkMemory" : { "type" : "integer" }, "taskHeapMemory" : { "type" : "integer" }, "taskOffHeapMemory" : { "type" : "integer" } } } } } /taskmanagers/:taskmanagerid/logs Verb: GET Response code: 200 OK Returns the list of log files on a TaskManager. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogListInfo", "properties" : { "logs" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:LogInfo", "properties" : { "mtime" : { "type" : "integer" }, "name" : { "type" : "string" }, "size" : { "type" : "integer" } } } } } } /taskmanagers/:taskmanagerid/metrics Verb: GET Response code: 200 OK Provides access to task manager metrics. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Query parameters get (optional): Comma-separated list of string values to select specific metrics. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "any" } /taskmanagers/:taskmanagerid/thread-dump Verb: GET Response code: 200 OK Returns the thread dump of the requested TaskManager. Path parameters taskmanagerid - 32-character hexadecimal string that identifies a task manager. Request \u0026nbsp; ▾ {} Response \u0026nbsp; ▾ { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo", "properties" : { "threadInfos" : { "type" : "array", "items" : { "type" : "object", "id" : "urn:jsonschema:org:apache:flink:runtime:rest:messages:ThreadDumpInfo:ThreadInfo", "properties" : { "stringifiedThreadInfo" : { "type" : "string" }, "threadName" : { "type" : "string" } } } } } } `}),e.add({id:183,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-agg/",title:"窗口聚合",section:"Queries 查询",content:` Window Aggregation # Window TVF Aggregation # Batch Streaming
Window aggregations are defined in the GROUP BY clause contains \u0026ldquo;window_start\u0026rdquo; and \u0026ldquo;window_end\u0026rdquo; columns of the relation applied Windowing TVF. Just like queries with regular GROUP BY clauses, queries with a group by window aggregation will compute a single result row per group.
SELECT ... FROM \u0026lt;windowed_table\u0026gt; -- relation applied windowing TVF GROUP BY window_start, window_end, ... Unlike other aggregations on continuous tables, window aggregation do not emit intermediate results but only a final result, the total aggregation at the end of the window. Moreover, window aggregations purge all intermediate state when no longer needed.
Windowing TVFs # Flink supports TUMBLE, HOP and CUMULATE types of window aggregations. In streaming mode, the time attribute field of a window table-valued function must be on either event or processing time attributes. See Windowing TVF for more windowing functions information. In batch mode, the time attribute field of a window table-valued function must be an attribute of type TIMESTAMP or TIMESTAMP_LTZ.
Here are some examples for TUMBLE, HOP and CUMULATE window aggregations.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | | supplier_id | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+-------------+ | bidtime | price | item | supplier_id | +------------------+-------+------+-------------+ | 2020-04-15 08:05 | 4.00 | C | supplier1 | | 2020-04-15 08:07 | 2.00 | A | supplier1 | | 2020-04-15 08:09 | 5.00 | D | supplier2 | | 2020-04-15 08:11 | 3.00 | B | supplier2 | | 2020-04-15 08:13 | 1.00 | E | supplier1 | | 2020-04-15 08:17 | 6.00 | F | supplier2 | +------------------+-------+------+-------------+ -- tumbling window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ -- hopping window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:05 | 2020-04-15 08:15 | 15.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | | 2020-04-15 08:15 | 2020-04-15 08:25 | 6.00 | +------------------+------------------+-------+ -- cumulative window aggregation Flink SQL\u0026gt; SELECT window_start, window_end, SUM(price) FROM TABLE( CUMULATE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;2\u0026#39; MINUTES, INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; +------------------+------------------+-------+ | window_start | window_end | price | +------------------+------------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:06 | 4.00 | | 2020-04-15 08:00 | 2020-04-15 08:08 | 6.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | 11.00 | | 2020-04-15 08:10 | 2020-04-15 08:12 | 3.00 | | 2020-04-15 08:10 | 2020-04-15 08:14 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:16 | 4.00 | | 2020-04-15 08:10 | 2020-04-15 08:18 | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | 10.00 | +------------------+------------------+-------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
GROUPING SETS # Window aggregations also support GROUPING SETS syntax. Grouping sets allow for more complex grouping operations than those describable by a standard GROUP BY. Rows are grouped separately by each specified grouping set and aggregates are computed for each group just as for simple GROUP BY clauses.
Window aggregations with GROUPING SETS require both the window_start and window_end columns have to be in the GROUP BY clause, but not in the GROUPING SETS clause.
Flink SQL\u0026gt; SELECT window_start, window_end, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, GROUPING SETS ((supplier_id), ()); +------------------+------------------+-------------+-------+ | window_start | window_end | supplier_id | price | +------------------+------------------+-------------+-------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | (NULL) | 11.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier2 | 5.00 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier1 | 6.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | (NULL) | 10.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier2 | 9.00 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier1 | 1.00 | +------------------+------------------+-------------+-------+ Each sublist of GROUPING SETS may specify zero or more columns or expressions and is interpreted the same way as though used directly in the GROUP BY clause. An empty grouping set means that all rows are aggregated down to a single group, which is output even if no input rows were present.
References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.
ROLLUP # ROLLUP is a shorthand notation for specifying a common type of grouping set. It represents the given list of expressions and all prefixes of the list, including the empty list.
Window aggregations with ROLLUP requires both the window_start and window_end columns have to be in the GROUP BY clause, but not in the ROLLUP clause.
For example, the following query is equivalent to the one above.
SELECT window_start, window_end, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, ROLLUP (supplier_id); CUBE # CUBE is a shorthand notation for specifying a common type of grouping set. It represents the given list and all of its possible subsets - the power set.
Window aggregations with CUBE requires both the window_start and window_end columns have to be in the GROUP BY clause, but not in the CUBE clause.
For example, the following two queries are equivalent.
SELECT window_start, window_end, item, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, CUBE (supplier_id, item); SELECT window_start, window_end, item, supplier_id, SUM(price) as price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, GROUPING SETS ( (supplier_id, item), (supplier_id ), ( item), ( ) ) Selecting Group Window Start and End Timestamps # The start and end timestamps of group windows can be selected with the grouped window_start and window_end columns.
Cascading Window Aggregation # The window_start and window_end columns are regular timestamp columns, not time attributes. Thus they can\u0026rsquo;t be used as time attributes in subsequent time-based operations. In order to propagate time attributes, you need to additionally add window_time column into GROUP BY clause. The window_time is the third column produced by Windowing TVFs which is a time attribute of the assigned window. Adding window_time into GROUP BY clause makes window_time also to be group key that can be selected. Then following queries can use this column for subsequent time-based operations, such as cascading window aggregations and Window TopN.
The following shows a cascading window aggregation where the first window aggregation propagates the time attribute for the second window aggregation.
-- tumbling 5 minutes for each supplier_id CREATE VIEW window1 AS -- Note: The window start and window end fields of inner Window TVF are optional in the select clause. However, if they appear in the clause, they need to be aliased to prevent name conflicting with the window start and window end of the outer Window TVF. SELECT window_start as window_5mintumble_start, window_end as window_5mintumble_end, window_time as rowtime, SUM(price) as partial_price FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;5\u0026#39; MINUTES)) GROUP BY supplier_id, window_start, window_end, window_time; -- tumbling 10 minutes on the first window SELECT window_start, window_end, SUM(partial_price) as total_price FROM TABLE( TUMBLE(TABLE window1, DESCRIPTOR(rowtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; Group Window Aggregation # Batch Streaming
Warning: Group Window Aggregation is deprecated. It\u0026rsquo;s encouraged to use Window TVF Aggregation which is more powerful and effective.
Compared to Group Window Aggregation, Window TVF Aggregation have many advantages, including:
Have all performance optimizations mentioned in Performance Tuning. Support standard GROUPING SETS syntax. Can apply Window TopN after window aggregation result. and so on. Group Window Aggregations are defined in the GROUP BY clause of a SQL query. Just like queries with regular GROUP BY clauses, queries with a GROUP BY clause that includes a group window function compute a single result row per group. The following group windows functions are supported for SQL on batch and streaming tables.
Group Window Functions # Group Window Function Description TUMBLE(time_attr, interval) Defines a tumbling time window. A tumbling time window assigns rows to non-overlapping, continuous windows with a fixed duration (interval). For example, a tumbling window of 5 minutes groups rows in 5 minutes intervals. Tumbling windows can be defined on event-time (stream + batch) or processing-time (stream). HOP(time_attr, interval, interval) Defines a hopping time window (called sliding window in the Table API). A hopping time window has a fixed duration (second interval parameter) and hops by a specified hop interval (first interval parameter). If the hop interval is smaller than the window size, hopping windows are overlapping. Thus, rows can be assigned to multiple windows. For example, a hopping window of 15 minutes size and 5 minute hop interval assigns each row to 3 different windows of 15 minute size, which are evaluated in an interval of 5 minutes. Hopping windows can be defined on event-time (stream + batch) or processing-time (stream). SESSION(time_attr, interval) Defines a session time window. Session time windows do not have a fixed duration but their bounds are defined by a time interval of inactivity, i.e., a session window is closed if no event appears for a defined gap period. For example a session window with a 30 minute gap starts when a row is observed after 30 minutes inactivity (otherwise the row would be added to an existing window) and is closed if no row is added within 30 minutes. Session windows can work on event-time (stream + batch) or processing-time (stream). Time Attributes # In streaming mode, the time_attr argument of the group window function must refer to a valid time attribute that specifies the processing time or event time of rows. See the documentation of time attributes to learn how to define time attributes.
In batch mode, the time_attr argument of the group window function must be an attribute of type TIMESTAMP.
Selecting Group Window Start and End Timestamps # The start and end timestamps of group windows as well as time attributes can be selected with the following auxiliary functions:
Auxiliary Function Description TUMBLE_START(time_attr, interval) HOP_START(time_attr, interval, interval) SESSION_START(time_attr, interval) Returns the timestamp of the inclusive lower bound of the corresponding tumbling, hopping, or session window.
TUMBLE_END(time_attr, interval) HOP_END(time_attr, interval, interval) SESSION_END(time_attr, interval) Returns the timestamp of the exclusive upper bound of the corresponding tumbling, hopping, or session window.
Note: The exclusive upper bound timestamp cannot be used as a rowtime attribute in subsequent time-based operations, such as interval joins and group window or over window aggregations.
TUMBLE_ROWTIME(time_attr, interval) HOP_ROWTIME(time_attr, interval, interval) SESSION_ROWTIME(time_attr, interval) Returns the timestamp of the inclusive upper bound of the corresponding tumbling, hopping, or session window.
The resulting attribute is a rowtime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
TUMBLE_PROCTIME(time_attr, interval) HOP_PROCTIME(time_attr, interval, interval) SESSION_PROCTIME(time_attr, interval) Returns a proctime attribute that can be used in subsequent time-based operations such as interval joins and group window or over window aggregations.
Note: Auxiliary functions must be called with exactly same arguments as the group window function in the GROUP BY clause.
The following examples show how to specify SQL queries with group windows on streaming tables.
CREATE TABLE Orders ( user BIGINT, product STRING, amount INT, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;1\u0026#39; MINUTE ) WITH (...); SELECT user, TUMBLE_START(order_time, INTERVAL \u0026#39;1\u0026#39; DAY) AS wStart, SUM(amount) FROM Orders GROUP BY TUMBLE(order_time, INTERVAL \u0026#39;1\u0026#39; DAY), user Back to top
`}),e.add({id:184,href:"/flink/flink-docs-master/zh/docs/learn-flink/fault_tolerance/",title:"容错处理",section:"实践练习",content:` 通过状态快照实现容错处理 # State Backends # 由 Flink 管理的 keyed state 是一种分片的键/值存储，每个 keyed state 的工作副本都保存在负责该键的 taskmanager 本地中。另外，Operator state 也保存在机器节点本地。Flink 定期获取所有状态的快照，并将这些快照复制到持久化的位置，例如分布式文件系统。
如果发生故障，Flink 可以恢复应用程序的完整状态并继续处理，就如同没有出现过异常。
Flink 管理的状态存储在 state backend 中。Flink 有两种 state backend 的实现 \u0026ndash; 一种基于 RocksDB 内嵌 key/value 存储将其工作状态保存在磁盘上的，另一种基于堆的 state backend，将其工作状态保存在 Java 的堆内存中。这种基于堆的 state backend 有两种类型：FsStateBackend，将其状态快照持久化到分布式文件系统；MemoryStateBackend，它使用 JobManager 的堆保存状态快照。
名称 Working State 状态备份 快照 RocksDBStateBackend 本地磁盘（tmp dir） 分布式文件系统 全量 / 增量 支持大于内存大小的状态 经验法则：比基于堆的后端慢10倍 FsStateBackend JVM Heap 分布式文件系统 全量 快速，需要大的堆内存 受限制于 GC MemoryStateBackend JVM Heap JobManager JVM Heap 全量 适用于小状态（本地）的测试和实验 当使用基于堆的 state backend 保存状态时，访问和更新涉及在堆上读写对象。但是对于保存在 RocksDBStateBackend 中的对象，访问和更新涉及序列化和反序列化，所以会有更大的开销。但 RocksDB 的状态量仅受本地磁盘大小的限制。还要注意，只有 RocksDBStateBackend 能够进行增量快照，这对于具有大量变化缓慢状态的应用程序来说是大有裨益的。
所有这些 state backends 都能够异步执行快照，这意味着它们可以在不妨碍正在进行的流处理的情况下执行快照。
Back to top
状态快照 # 定义 # 快照 \u0026ndash; 是 Flink 作业状态全局一致镜像的通用术语。快照包括指向每个数据源的指针（例如，到文件或 Kafka 分区的偏移量）以及每个作业的有状态运算符的状态副本，该状态副本是处理了 sources 偏移位置之前所有的事件后而生成的状态。
Checkpoint \u0026ndash; 一种由 Flink 自动执行的快照，其目的是能够从故障中恢复。Checkpoints 可以是增量的，并为快速恢复进行了优化。
外部化的 Checkpoint \u0026ndash; 通常 checkpoints 不会被用户操纵。Flink 只保留作业运行时的最近的 n 个 checkpoints（n 可配置），并在作业取消时删除它们。但你可以将它们配置为保留，在这种情况下，你可以手动从中恢复。
Savepoint \u0026ndash; 用户出于某种操作目的（例如有状态的重新部署/升级/缩放操作）手动（或 API 调用）触发的快照。Savepoints 始终是完整的，并且已针对操作灵活性进行了优化。
状态快照如何工作？ # Flink 使用 Chandy-Lamport algorithm 算法的一种变体，称为异步 barrier 快照（asynchronous barrier snapshotting）。
当 checkpoint coordinator（job manager 的一部分）指示 task manager 开始 checkpoint 时，它会让所有 sources 记录它们的偏移量，并将编号的 checkpoint barriers 插入到它们的流中。这些 barriers 流经 job graph，标注每个 checkpoint 前后的流部分。
Checkpoint n 将包含每个 operator 的 state，这些 state 是对应的 operator 消费了严格在 checkpoint barrier n 之前的所有事件，并且不包含在此（checkpoint barrier n）后的任何事件后而生成的状态。
当 job graph 中的每个 operator 接收到 barriers 时，它就会记录下其状态。拥有两个输入流的 Operators（例如 CoProcessFunction）会执行 barrier 对齐（barrier alignment） 以便当前快照能够包含消费两个输入流 barrier 之前（但不超过）的所有 events 而产生的状态。
Flink 的 state backends 利用写时复制（copy-on-write）机制允许当异步生成旧版本的状态快照时，能够不受影响地继续流处理。只有当快照被持久保存后，这些旧版本的状态才会被当做垃圾回收。
确保精确一次（exactly once） # 当流处理应用程序发生错误的时候，结果可能会产生丢失或者重复。Flink 根据你为应用程序和集群的配置，可以产生以下结果：
Flink 不会从快照中进行恢复（at most once） 没有任何丢失，但是你可能会得到重复冗余的结果（at least once） 没有丢失或冗余重复（exactly once） Flink 通过回退和重新发送 source 数据流从故障中恢复，当理想情况被描述为精确一次时，这并不意味着每个事件都将被精确一次处理。相反，这意味着 每一个事件都会影响 Flink 管理的状态精确一次。
Barrier 只有在需要提供精确一次的语义保证时需要进行对齐（Barrier alignment）。如果不需要这种语义，可以通过配置 CheckpointingMode.AT_LEAST_ONCE 关闭 Barrier 对齐来提高性能。
端到端精确一次 # 为了实现端到端的精确一次，以便 sources 中的每个事件都仅精确一次对 sinks 生效，必须满足以下条件：
你的 sources 必须是可重放的，并且 你的 sinks 必须是事务性的（或幂等的） Back to top
实践练习 # Flink Operations Playground 包括有关 Observing Failure \u0026amp; Recovery 的部分。
Back to top
延伸阅读 # Stateful Stream Processing State Backends Data Sources 和 Sinks 的容错保证 开启和配置 Checkpointing Checkpoints Savepoints 大状态与 Checkpoint 调优 监控 Checkpoint Task 故障恢复 Back to top
`}),e.add({id:185,href:"/flink/flink-docs-master/zh/docs/deployment/memory/mem_migration/",title:"升级指南",section:"内存配置",content:` 升级指南 # 在 1.10 和 1.11 版本中，Flink 分别对 TaskManager 和 JobManager 的内存配置方法做出了较大的改变。 部分配置参数被移除了，或是语义上发生了变化。 本篇升级指南将介绍如何将 Flink 1.9 及以前版本的 TaskManager 内存配置升级到 Flink 1.10 及以后版本， 以及如何将 Flink 1.10 及以前版本的 JobManager 内存配置升级到 Flink 1.11 及以后版本。
toc 注意： 请仔细阅读本篇升级指南。 使用原本的和新的内存配制方法可能会使内存组成部分具有截然不同的大小。 未经调整直接沿用 Flink 1.10 以前版本的 TaskManager 配置文件或 Flink 1.11 以前版本的 JobManager 配置文件，可能导致应用的行为、性能发生变化，甚至造成应用执行失败。 提示 在 1.10/1.11 版本之前，Flink 不要求用户一定要配置 TaskManager/JobManager 内存相关的参数，因为这些参数都具有默认值。 新的内存配置要求用户至少指定下列配置参数（或参数组合）的其中之一，否则 Flink 将无法启动。
TaskManager: JobManager: taskmanager.memory.flink.size jobmanager.memory.flink.size taskmanager.memory.process.size jobmanager.memory.process.size taskmanager.memory.task.heap.size 和 taskmanager.memory.managed.size jobmanager.memory.heap.size Flink 自带的默认 flink-conf.yaml 文件指定了 taskmanager.memory.process.size（\u0026gt;= 1.10）和 jobmanager.memory.process.size (\u0026gt;= 1.11)，以便与此前的行为保持一致。
可以使用这张电子表格来估算和比较原本的和新的内存配置下的计算结果。
升级 TaskManager 内存配置 # 配置参数变化 # 本节简要列出了 Flink 1.10 引入的配置参数变化，并援引其他章节中关于如何升级到新配置参数的相关描述。
下列配置参数已被彻底移除，配置它们将不会产生任何效果。
移除的配置参数 备注 taskmanager.memory.fraction 请参考新配置参数 taskmanager.memory.managed.fraction 的相关描述。 新的配置参数与被移除的配置参数在语义上有所差别，因此其配置值通常也需要做出适当调整。 请参考如何升级托管内存。 taskmanager.memory.off-heap Flink 不再支持堆上的（On-Heap）托管内存。请参考如何升级托管内存。 taskmanager.memory.preallocate Flink 不再支持内存预分配，今后托管内存将都是惰性分配的。请参考如何升级托管内存。 下列配置参数将被弃用，出于向后兼容性考虑，配置它们将被解读成对应的新配置参数。
弃用的配置参数 对应的新配置参数 taskmanager.heap.size 独立部署模式（Standalone Deployment）下：taskmanager.memory.flink.size 容器化部署模式（Containerized Deployment）下：taskmanager.memory.process.size 请参考如何升级总内存。 taskmanager.memory.size taskmanager.memory.managed.size。请参考如何升级托管内存。 taskmanager.network.memory.min taskmanager.memory.network.min taskmanager.network.memory.max taskmanager.memory.network.max taskmanager.network.memory.fraction taskmanager.memory.network.fraction 尽管网络内存的配置参数没有发生太多变化，我们仍建议您检查其配置结果。 网络内存的大小可能会受到其他内存部分大小变化的影响，例如总内存变化时，根据占比计算出的网络内存也可能发生变化。 请参考内存模型详解。
容器切除（Cut-Off）内存相关的配置参数（containerized.heap-cutoff-ratio 和 containerized.heap-cutoff-min）将不再对 TaskManager 进程生效。 请参考如何升级容器切除内存。
总内存（原堆内存） # 在原本的内存配置方法中，用于指定用于 Flink 的总内存的配置参数是 taskmanager.heap.size 或 taskmanager.heap.mb。 尽管这两个参数以“堆（Heap）”命名，实际上它们指定的内存既包含了 JVM 堆内存，也包含了其他堆外内存部分。 这两个配置参数目前已被弃用。
如果配置了上述弃用的参数，同时又没有配置与之对应的新配置参数，那它们将按如下规则对应到新的配置参数。
独立部署模式（Standalone Deployment）下：Flink 总内存（taskmanager.memory.flink.size） 容器化部署模式（Containerized Deployment）下（Yarn）：进程总内存（taskmanager.memory.process.size） 建议您尽早使用新的配置参数取代启用的配置参数，它们在今后的版本中可能会被彻底移除。
请参考如何配置总内存.
JVM 堆内存 # 此前，JVM 堆空间由托管内存（仅在配置为堆上时）及 Flink 用到的所有其他堆内存组成。 这里的其他堆内存是由总内存减去所有其他非堆内存得到的。 请参考如何升级托管内存。
现在，如果仅配置了Flink总内存或进程总内存，JVM 的堆空间依然是根据总内存减去所有其他非堆内存得到的。 请参考如何配置总内存。
此外，你现在可以更直接地控制用于任务和算子的 JVM 的堆内存（taskmanager.memory.task.heap.size），详见任务堆内存。 如果流处理作业选择使用 Heap State Backend（MemoryStateBackend 或 FsStateBackend），那么它同样需要使用 JVM 堆内存。
Flink 现在总是会预留一部分 JVM 堆内存供框架使用（taskmanager.memory.framework.heap.size）。 请参考框架内存。
托管内存 # 请参考如何配置托管内存。
明确的大小 # 原本用于指定明确的托管内存大小的配置参数（taskmanager.memory.size）已被弃用，与它具有相同语义的新配置参数为 taskmanager.memory.managed.size。 建议使用新的配置参数，原本的配置参数在今后的版本中可能会被彻底移除。
占比 # 此前，如果不指定明确的大小，也可以将托管内存配置为占用总内存减去网络内存和容器切除内存（仅在 Yarn）之后剩余部分的固定比例（taskmanager.memory.fraction）。 该配置参数已经被彻底移除，配置它不会产生任何效果。 请使用新的配置参数 taskmanager.memory.managed.fraction。 在未通过 taskmanager.memory.managed.size 指定明确大小的情况下，新的配置参数将指定托管内存在 Flink 总内存中的所占比例。
RocksDB State Backend # 流处理作业如果选择使用 RocksDBStateBackend，它使用的本地内存现在也被归为托管内存。 默认情况下，RocksDB 将限制其内存用量不超过托管内存大小，以避免在 Yarn 上容器被杀。你也可以通过设置 state.backend.rocksdb.memory.managed 来关闭 RocksDB 的内存控制。 请参考如何升级容器切除内存。
其他变化 # 此外，Flink 1.10 对托管内存还引入了下列变化：
托管内存现在总是在堆外。配置参数 taskmanager.memory.off-heap 已被彻底移除，配置它不会产生任何效果。 托管内存现在使用本地内存而非直接内存。这意味着托管内存将不在 JVM 直接内存限制的范围内。 托管内存现在总是惰性分配的。配置参数 taskmanager.memory.preallocate 已被彻底移除，配置它不会产生任何效果。 升级 JobManager 内存配置 # 在原本的内存配置方法中，用于指定 JVM 堆内存 的配置参数是:
jobmanager.heap.size jobmanager.heap.mb 尽管这两个参数以“堆（Heap）”命名，在此之前它们实际上只有在独立部署模式才完全对应于 JVM 堆内存。 在容器化部署模式下（Kubernetes 和 Yarn），它们指定的内存还包含了其他堆外内存部分。 JVM 堆空间的实际大小，是参数指定的大小减去容器切除（Cut-Off）内存后剩余的部分。 容器切除内存在 1.11 及以上版本中已被彻底移除。
从 1.11 版本开始，Flink 将采用与独立部署模式相同的方式设置这些参数。
这两个配置参数目前已被弃用。 如果配置了上述弃用的参数，同时又没有配置与之对应的新配置参数，那它们将按如下规则对应到新的配置参数。
独立部署模式（Standalone Deployment）：JVM 堆内存（jobmanager.memory.heap.size） 容器化部署模式（Containerized Deployment）下（Kubernetes、Yarn）：进程总内存（jobmanager.memory.process.size） 建议您尽早使用新的配置参数取代启用的配置参数，它们在今后的版本中可能会被彻底移除。
如果仅配置了 Flink 总内存或进程总内存，那么 JVM 堆内存将是总内存减去其他内存部分后剩余的部分。 请参考如何配置总内存。 此外，也可以通过配置 jobmanager.memory.heap.size 的方式直接指定 JVM 堆内存。
Flink JVM 进程内存限制 # 从 1.10 版本开始，Flink 通过设置相应的 JVM 参数，对 TaskManager 进程使用的 JVM Metaspace 和 JVM 直接内存进行限制。 从 1.11 版本开始，Flink 同样对 JobManager 进程使用的 JVM Metaspace 进行限制。 此外，还可以通过设置 jobmanager.memory.enable-jvm-direct-memory-limit 对 JobManager 进程的 JVM 直接内存进行限制。 请参考 JVM 参数。
Flink 通过设置上述 JVM 内存限制降低内存泄漏问题的排查难度，以避免出现容器内存溢出等问题。 请参考常见问题中关于 JVM Metaspace 和 JVM 直接内存 OutOfMemoryError 异常的描述。
容器切除（Cut-Off）内存 # 在容器化部署模式（Containerized Deployment）下，此前你可以指定切除内存。 这部分内存将预留给所有未被 Flink 计算在内的内存开销。 其主要来源是不受 Flink 直接管理的依赖使用的内存，例如 RocksDB、JVM 内部开销等。 相应的配置参数（containerized.heap-cutoff-ratio 和 containerized.heap-cutoff-min）不再生效。 新的内存配置方法引入了新的内存组成部分来具体描述这些内存用量。
TaskManager # 流处理作业如果使用了 RocksDBStateBackend，RocksDB 使用的本地内存现在将被归为托管内存。 默认情况下，RocksDB 将限制其内存用量不超过托管内存大小。 请同时参考如何升级托管内存以及如何配置托管内存。
其他堆外（直接或本地）内存开销，现在可以通过下列配置参数进行设置：
任务堆外内存（taskmanager.memory.task.off-heap.size） 框架堆外内存（taskmanager.memory.framework.off-heap.size） JVM Metaspace（taskmanager.memory.jvm-metaspace.size） JVM 开销 JobManager # 可以通过下列配置参数设置堆外（直接或本地）内存开销：
堆外内存 (jobmanager.memory.off-heap.size) JVM Metaspace (jobmanager.memory.jvm-metaspace.size) JVM 开销 flink-conf.yaml 中的默认配置 # 本节描述 Flink 自带的默认 flink-conf.yaml 文件中的变化。
原本的 TaskManager 总内存（taskmanager.heap.size）被新的配置项 taskmanager.memory.process.size 所取代。 默认值从 1024MB 增加到了 1728MB。
原本的 JobManager 总内存（jobmanager.heap.size）被新的配置项 jobmanager.memory.process.size 所取代。 默认值从 1024MB 增加到了 1600MB。
请参考如何配置总内存。
注意： 使用新的默认 flink-conf.yaml 可能会造成各内存部分的大小发生变化，从而产生性能变化。 `}),e.add({id:186,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/",title:"数据类型以及序列化",section:"状态与容错",content:""}),e.add({id:187,href:"/flink/flink-docs-master/zh/docs/dev/datastream/operators/",title:"算子",section:"DataStream API",content:" "}),e.add({id:188,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/",title:"状态数据结构升级",section:"数据类型以及序列化",content:` 状态数据结构升级 # Apache Flink 流应用通常被设计为永远或者长时间运行。 与所有长期运行的服务一样，应用程序需要随着业务的迭代而进行调整。 应用所处理的数据 schema 也会随着进行变化。
此页面概述了如何升级状态类型的数据 schema 。 目前对不同类型的状态结构（ValueState、ListState 等）有不同的限制
请注意，此页面的信息只与 Flink 自己生成的状态序列化器相关 类型序列化框架。 也就是说，在声明状态时，状态描述符不可以配置为使用特定的 TypeSerializer 或 TypeInformation ， 在这种情况下，Flink 会推断状态类型的信息：
ListStateDescriptor\u0026lt;MyPojoType\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state-name\u0026#34;, MyPojoType.class); checkpointedState = getRuntimeContext().getListState(descriptor); 在内部，状态是否可以进行升级取决于用于读写持久化状态字节的序列化器。 简而言之，状态数据结构只有在其序列化器正确支持时才能升级。 这一过程是被 Flink 的类型序列化框架生成的序列化器透明处理的（下面 列出了当前的支持范围）。
如果你想要为你的状态类型实现自定义的 TypeSerializer 并且想要学习如何实现支持状态数据结构升级的序列化器， 可以参考 自定义状态序列化器。 本文档也包含一些用于支持状态数据结构升级的状态序列化器与 Flink 状态后端存储相互作用的必要内部细节。
升级状态数据结构 # 为了对给定的状态类型进行升级，你需要采取以下几个步骤：
对 Flink 流作业进行 savepoint 操作。 升级程序中的状态类型（例如：修改你的 Avro 结构）。 从 savepoint 恢复作业。当第一次访问状态数据时，Flink 会判断状态数据 schema 是否已经改变，并进行必要的迁移。 用来适应状态结构的改变而进行的状态迁移过程是自动发生的，并且状态之间是互相独立的。 Flink 内部是这样来进行处理的，首先会检查新的序列化器相对比之前的序列化器是否有不同的状态结构；如果有， 那么之前的序列化器用来读取状态数据字节到对象，然后使用新的序列化器将对象回写为字节。
更多的迁移过程细节不在本文档谈论的范围；可以参考文档。
数据结构升级支持的数据类型 # 目前，仅支持 POJO 和 Avro 类型的 schema 升级 因此，如果你比较关注于状态数据结构的升级，那么目前来看强烈推荐使用 Pojo 或者 Avro 状态数据类型。
我们有计划支持更多的复合类型；更多的细节可以参考 FLINK-10896。
POJO 类型 # Flink 基于下面的规则来支持 POJO 类型结构的升级:
可以删除字段。一旦删除，被删除字段的前值将会在将来的 checkpoints 以及 savepoints 中删除。 可以添加字段。新字段会使用类型对应的默认值进行初始化，比如 Java 类型。 不可以修改字段的声明类型。 不可以改变 POJO 类型的类名，包括类的命名空间。 需要注意，只有从 1.8.0 及以上版本的 Flink 生产的 savepoint 进行恢复时，POJO 类型的状态才可以进行升级。 对 1.8.0 版本之前的 Flink 是没有办法进行 POJO 类型升级的。
Avro 类型 # Flink 完全支持 Avro 状态类型的升级，只要数据结构的修改是被 Avro 的数据结构解析规则认为兼容的即可。
一个例外是如果新的 Avro 数据 schema 生成的类无法被重定位或者使用了不同的命名空间，在作业恢复时状态数据会被认为是不兼容的。
Schema Migration Limitations # Flink\u0026rsquo;s schema migration has some limitations that are required to ensure correctness. For users that need to work around these limitations, and understand them to be safe in their specific use-case, consider using a custom serializer or the state processor api.
Schema evolution of keys is not supported. # The structure of a key cannot be migrated as this may lead to non-deterministic behavior. For example, if a POJO is used as a key and one field is dropped then there may suddenly be multiple separate keys that are now identical. Flink has no way to merge the corresponding values.
Additionally, the RocksDB state backend relies on binary object identity, rather than the hashCode method. Any change to the keys\u0026rsquo; object structure can lead to non-deterministic behavior.
Kryo cannot be used for schema evolution. # When Kryo is used, there is no possibility for the framework to verify if any incompatible changes have been made.
This means that if a data-structure containing a given type is serialized via Kryo, then that contained type can not undergo schema evolution.
For example, if a POJO contains a List\u0026lt;SomeOtherPojo\u0026gt;, then the List and its contents are serialized via Kryo and schema evolution is not supported for SomeOtherPojo.
Back to top
`}),e.add({id:189,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/analyze/",title:"ANALYZE 语句",section:"SQL",content:" ANALYZE 语句 # ANALYZE 语句被用于为存在的表收集统计信息，并将统计信息写入该表的 catalog 中。当前版本中，ANALYZE 语句只支持 ANALYZE TABLE， 且只能由用户手动触发。\n注意 现在, ANALYZE TABLE 只支持批模式（Batch Mode），且只能用于已存在的表， 如果表不存在或者是视图（View）则会报错。\n执行 ANALYZE TABLE 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 ANALYZE TABLE 语句。\n以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。\nScala 可以使用 TableEnvironment 的 executeSql() 方法执行 ANALYZE TABLE 语句。\n以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。\nPython 可以使用 TableEnvironment 的 execute_sql() 方法执行 ANALYZE TABLE 语句。\n以下示例展示了如何在 TableEnvironment 中执行一条 ANALYZE TABLE 语句。\nSQL CLI ANALYZE TABLE 语句可以在 SQL CLI 中执行。\n以下示例展示了如何在 SQL CLI 中执行一条 ANALYZE TABLE 语句。\nJava TableEnvironment tableEnv = TableEnvironment.create(...); // 注册名为 “Store” 的非分区表 tableEnv.executeSql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); // 注册名为 “Orders” 的分区表 tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); // 非分区表，收集表级别的统计信息(表的统计信息主要为行数(row count))。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); // 非分区表，收集表级别的统计信息和所有列的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 非分区表，收集表级别的统计信息和指定列(列: location)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); // 假设分区表 “Orders” 有 4 个分区，分区信息如下： // Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // 分区表，收集分区 Partition1 的表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); // 分区表，为所有分区收集表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); // 分区表，收集分区 Partition1 的表级别统计信息和所有列的统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和所有列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，为所有分区收集表级别统计信息和所有列的统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，收集分区 Partition1 的表级别统计信息和分区中指定列(列: amount)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和分区中指定列(列: amount，列: product)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); // 分区表，收集所有分区的表级别统计信息和指定列(列: amount，列: product)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); Scala val tableEnv = TableEnvironment.create(...) // 注册名为 “Store” 的非分区表 tableEnv.executeSql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); // 注册名为 “Orders” 的分区表 tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); // 非分区表，收集表级别的统计信息(表的统计信息主要为行数(row count))。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); // 非分区表，收集表级别的统计信息和所有列的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 非分区表，收集表级别的统计信息和指定列(列: location)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); // 假设分区表 “Orders” 有 4 个分区，分区信息如下： // Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) // Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) // 分区表，收集分区 Partition1 的表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); // 分区表，为所有分区收集表级别统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); // 分区表，收集分区 Partition1 的表级别统计信息和所有列的统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和所有列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，为所有分区收集表级别统计信息和所有列的统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); // 分区表，收集分区 Partition1 的表级别统计信息和分区中指定列(列: amount)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); // 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和分区中指定列(列: amount，列: product)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); // 分区表，收集所有分区的表级别统计信息和指定列(列: amount，列: product)的列统计信息。 tableEnv.executeSql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); Python table_env = TableEnvironment.create(...) # 注册名为 “Store” 的非分区表 table_env.execute_sql( \u0026#34;CREATE TABLE Store (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `location` VARCHAR(32),\u0026#34; + \u0026#34; `owner` VARCHAR(32)\u0026#34; + \u0026#34;) with (...)\u0026#34;); # 注册名为 “Orders” 的分区表 table_env.execute_sql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `id` BIGINT NOT NULl,\u0026#34; + \u0026#34; `product` VARCHAR(32),\u0026#34; + \u0026#34; `amount` INT,\u0026#34; + \u0026#34; `sold_year` BIGINT\u0026#34;, + \u0026#34; `sold_month` BIGINT\u0026#34;, + \u0026#34; `sold_day` BIGINT\u0026#34; + \u0026#34;) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026#34; \u0026#34;) with (...)\u0026#34;); # 非分区表，收集表级别的统计信息(表的统计信息主要为行数(row count))。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS\u0026#34;); # 非分区表，收集表级别的统计信息和所有列的列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # 非分区表，收集表级别的统计信息和指定列(列: location)的列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location\u0026#34;); # 假设分区表 “Orders” 有 4 个分区，分区信息如下： # Partition1 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) # Partition2 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;11\u0026#39;) # Partition3 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;10\u0026#39;) # Partition4 : (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;2\u0026#39;, sold_day=\u0026#39;11\u0026#39;) # 分区表，收集分区 Partition1 的表级别统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS\u0026#34;); # 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS\u0026#34;); # 分区表，为所有分区收集表级别统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS\u0026#34;); # 分区表，收集分区 Partition1 的表级别统计信息和所有列的统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和所有列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # 分区表，为所有分区收集表级别统计信息和所有列的统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS\u0026#34;); # 分区表，收集分区 Partition1 的表级别统计信息和分区中指定列(列: amount)的列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount\u0026#34;); # 分区表，收集分区 Partition1 和 Partition2 的表级别统计信息和分区中指定列(列: amount，列: product)的列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); # 分区表，收集所有分区的表级别统计信息和指定列(列: amount，列: product)的列统计信息。 table_env.execute_sql(\u0026#34;ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product\u0026#34;); SQL CLI Flink SQL\u0026gt; CREATE TABLE Store ( \u0026gt; `id` BIGINT NOT NULl, \u0026gt; `location` VARCHAR(32), \u0026gt; `owner` VARCHAR(32) \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE Orders ( \u0026gt; `id` BIGINT NOT NULl, \u0026gt; `product` VARCHAR(32), \u0026gt; `amount` INT, \u0026gt; `sold_year` BIGINT, \u0026gt; `sold_month` BIGINT, \u0026gt; `sold_day` BIGINT \u0026gt; ) PARTITIONED BY (`sold_year`, `sold_month`, `sold_day`) \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Store COMPUTE STATISTICS FOR COLUMNS location; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR ALL COLUMNS; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day=\u0026#39;10\u0026#39;) COMPUTE STATISTICS FOR COLUMNS amount; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION (sold_year=\u0026#39;2022\u0026#39;, sold_month=\u0026#39;1\u0026#39;, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product; [INFO] Execute statement succeed. Flink SQL\u0026gt; ANALYZE TABLE Orders PARTITION(sold_year, sold_month, sold_day) COMPUTE STATISTICS FOR COLUMNS amount, product; [INFO] Execute statement succeed. 语法 # ANALYZE TABLE [catalog_name.][db_name.]table_name PARTITION(partcol1[=val1] [, partcol2[=val2], ...]) COMPUTE STATISTICS [FOR COLUMNS col1 [, col2, ...] | FOR ALL COLUMNS] 对于分区表， 语法中 PARTITION(partcol1[=val1] [, partcol2[=val2], \u0026hellip;]) 是必须指定的\n如果没有指定某分区，则会收集所有分区的统计信息 如果指定了某分区，则只会收集该分区的统计信息 如果该表为非分区表，但语句中指定了分区，则会报异常 如果指定了某个分区，但是该分区不存在，则会报异常 语法中，FOR COLUMNS col1 [, col2, \u0026hellip;] 或者 FOR ALL COLUMNS 也是可选的\n如果没有指定某一列，则只会收集表级别的统计信息 如果指定的列不存在，或者该列不是物理列，则会报异常 如果指定了某一列或者某几列，则会收集列的统计信息 列级别的统计信息包括: ndv: 该列中列值不同的数量 nullCount: 该列中空值的数量 avgLen: 列值的平均长度 maxLen: 列值的最大长度 minValue: 列值的最小值 maxValue: 列值的最大值 valueCount: 该值只应用于 boolean 类型 对于列统计信息，支持类型和对应的列统计信息值如下表所示(\u0026ldquo;Y\u0026rdquo; 代表支持，\u0026ldquo;N\u0026rdquo; 代表不支持): 类型 ndv nullCount avgLen maxLen maxValue minValue valueCount BOOLEAN N Y N N N N Y TINYINT Y Y N N Y Y N SMALLINT Y Y N N Y Y N INTEGER Y Y N N Y Y N FLOAT Y Y N N Y Y N DATE Y Y N N Y Y N TIME_WITHOUT_TIME_ZONE Y Y N N Y Y N BIGINT Y Y N N Y Y N DOUBLE Y Y N N Y Y N DECIMAL Y Y N N Y Y N TIMESTAMP_WITH_LOCAL_TIME_ZONE Y Y N N Y Y N TIMESTAMP_WITHOUT_TIME_ZONE Y Y N N Y Y N CHAR Y Y Y Y N N N VARCHAR Y Y Y Y N N N other types N Y N N N N N 注意: 对于数据值定长的类型(例如：BOOLEAN, INTEGER, DOUBLE 等)， Flink 不会去收集 avgLen 和 maxLen 值。\nBack to top\n"}),e.add({id:190,href:"/flink/flink-docs-master/zh/docs/ops/batch/",title:"Batch",section:"Operations",content:""}),e.add({id:191,href:"/flink/flink-docs-master/zh/docs/ops/state/checkpoints/",title:"Checkpoints",section:"状态与容错",content:` Checkpoints # 概述 # Checkpoint 使 Flink 的状态具有良好的容错性，通过 checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。
参考 Checkpointing 查看如何在 Flink 程序中开启和配置 checkpoint。
要了解 checkpoints 和 savepoints 之间的区别，请参阅 checkpoints 与 savepoints。
保留 Checkpoint # Checkpoint 在默认的情况下仅用于恢复失败的作业，并不保留，当程序取消时 checkpoint 就会被删除。当然，你可以通过配置来保留 checkpoint，这些被保留的 checkpoint 在作业失败或取消时不会被清除。这样，你就可以使用该 checkpoint 来恢复失败的作业。
CheckpointConfig config = env.getCheckpointConfig(); config.setExternalizedCheckpointCleanup(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); ExternalizedCheckpointCleanup 配置项定义了当作业取消时，对作业 checkpoint 的操作：
ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：当作业取消时，保留作业的 checkpoint。注意，这种情况下，需要手动清除该作业保留的 checkpoint。 ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：当作业取消时，删除作业的 checkpoint。仅当作业失败时，作业的 checkpoint 才会被保留。 目录结构 # 与 savepoints 相似，checkpoint 由元数据文件、数据文件（与 state backend 相关）组成。可通过配置文件中 \u0026ldquo;state.checkpoints.dir\u0026rdquo; 配置项来指定元数据文件和数据文件的存储路径，另外也可以在代码中针对单个作业特别指定该配置项。
当前的 checkpoint 目录结构（由 FLINK-8531 引入）如下所示:
/user-defined-checkpoint-dir /{job-id} | + --shared/ + --taskowned/ + --chk-1/ + --chk-2/ + --chk-3/ ... 其中 SHARED 目录保存了可能被多个 checkpoint 引用的文件，TASKOWNED 保存了不会被 JobManager 删除的文件，EXCLUSIVE 则保存那些仅被单个 checkpoint 引用的文件。
注意: Checkpoint 目录不是公共 API 的一部分，因此可能在未来的 Release 中进行改变。 通过配置文件全局配置 # state.checkpoints.dir: hdfs:///checkpoints/ 创建 state backend 对单个作业进行配置 # env.setStateBackend(new RocksDBStateBackend(\u0026#34;hdfs:///checkpoints-data/\u0026#34;)); 从保留的 checkpoint 中恢复状态 # 与 savepoint 一样，作业可以从 checkpoint 的元数据文件恢复运行（savepoint恢复指南）。注意，如果元数据文件中信息不充分，那么 jobmanager 就需要使用相关的数据文件来恢复作业(参考目录结构)。
\$ bin/flink run -s :checkpointMetaDataPath [:runArgs] Back to top
`}),e.add({id:192,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/",title:"Custom State Serialization",section:"数据类型以及序列化",content:` Custom Serialization for Managed State # This page is targeted as a guideline for users who require the use of custom serialization for their state, covering how to provide a custom state serializer as well as guidelines and best practices for implementing serializers that allow state schema evolution.
If you\u0026rsquo;re simply using Flink\u0026rsquo;s own serializers, this page is irrelevant and can be ignored.
Using custom state serializers # When registering a managed operator or keyed state, a StateDescriptor is required to specify the state\u0026rsquo;s name, as well as information about the type of the state. The type information is used by Flink\u0026rsquo;s type serialization framework to create appropriate serializers for the state.
It is also possible to completely bypass this and let Flink use your own custom serializer to serialize managed states, simply by directly instantiating the StateDescriptor with your own TypeSerializer implementation:
Java public class CustomTypeSerializer extends TypeSerializer\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; {...}; ListStateDescriptor\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; descriptor = new ListStateDescriptor\u0026lt;\u0026gt;( \u0026#34;state-name\u0026#34;, new CustomTypeSerializer()); checkpointedState = getRuntimeContext().getListState(descriptor); Scala class CustomTypeSerializer extends TypeSerializer[(String, Integer)] {...} val descriptor = new ListStateDescriptor[(String, Integer)]( \u0026#34;state-name\u0026#34;, new CustomTypeSerializer) ) checkpointedState = getRuntimeContext.getListState(descriptor) State serializers and schema evolution # This section explains the user-facing abstractions related to state serialization and schema evolution, and necessary internal details about how Flink interacts with these abstractions.
When restoring from savepoints, Flink allows changing the serializers used to read and write previously registered state, so that users are not locked in to any specific serialization schema. When state is restored, a new serializer will be registered for the state (i.e., the serializer that comes with the StateDescriptor used to access the state in the restored job). This new serializer may have a different schema than that of the previous serializer. Therefore, when implementing state serializers, besides the basic logic of reading / writing data, another important thing to keep in mind is how the serialization schema can be changed in the future.
When speaking of schema, in this context the term is interchangeable between referring to the data model of a state type and the serialized binary format of a state type. The schema, generally speaking, can change for a few cases:
Data schema of the state type has evolved, i.e. adding or removing a field from a POJO that is used as state. Generally speaking, after a change to the data schema, the serialization format of the serializer will need to be upgraded. Configuration of the serializer has changed. In order for the new execution to have information about the written schema of state and detect whether or not the schema has changed, upon taking a savepoint of an operator\u0026rsquo;s state, a snapshot of the state serializer needs to be written along with the state bytes. This is abstracted a TypeSerializerSnapshot, explained in the next subsection.
The TypeSerializerSnapshot abstraction # public interface TypeSerializerSnapshot\u0026lt;T\u0026gt; { int getCurrentVersion(); void writeSnapshot(DataOuputView out) throws IOException; void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException; TypeSerializerSchemaCompatibility\u0026lt;T\u0026gt; resolveSchemaCompatibility(TypeSerializer\u0026lt;T\u0026gt; newSerializer); TypeSerializer\u0026lt;T\u0026gt; restoreSerializer(); } public abstract class TypeSerializer\u0026lt;T\u0026gt; { // ... public abstract TypeSerializerSnapshot\u0026lt;T\u0026gt; snapshotConfiguration(); } A serializer\u0026rsquo;s TypeSerializerSnapshot is a point-in-time information that serves as the single source of truth about the state serializer\u0026rsquo;s write schema, as well as any additional information mandatory to restore a serializer that would be identical to the given point-in-time. The logic about what should be written and read at restore time as the serializer snapshot is defined in the writeSnapshot and readSnapshot methods.
Note that the snapshot\u0026rsquo;s own write schema may also need to change over time (e.g. when you wish to add more information about the serializer to the snapshot). To facilitate this, snapshots are versioned, with the current version number defined in the getCurrentVersion method. On restore, when the serializer snapshot is read from savepoints, the version of the schema in which the snapshot was written in will be provided to the readSnapshot method so that the read implementation can handle different versions.
At restore time, the logic that detects whether or not the new serializer\u0026rsquo;s schema has changed should be implemented in the resolveSchemaCompatibility method. When previous registered state is registered again with new serializers in the restored execution of an operator, the new serializer is provided to the previous serializer\u0026rsquo;s snapshot via this method. This method returns a TypeSerializerSchemaCompatibility representing the result of the compatibility resolution, which can be one of the following:
TypeSerializerSchemaCompatibility.compatibleAsIs(): this result signals that the new serializer is compatible, meaning that the new serializer has identical schema with the previous serializer. It is possible that the new serializer has been reconfigured in the resolveSchemaCompatibility method so that it is compatible. TypeSerializerSchemaCompatibility.compatibleAfterMigration(): this result signals that the new serializer has a different serialization schema, and it is possible to migrate from the old schema by using the previous serializer (which recognizes the old schema) to read bytes into state objects, and then rewriting the object back to bytes with the new serializer (which recognizes the new schema). TypeSerializerSchemaCompatibility.incompatible(): this result signals that the new serializer has a different serialization schema, but it is not possible to migrate from the old schema. The last bit of detail is how the previous serializer is obtained in the case that migration is required. Another important role of a serializer\u0026rsquo;s TypeSerializerSnapshot is that it serves as a factory to restore the previous serializer. More specifically, the TypeSerializerSnapshot should implement the restoreSerializer method to instantiate a serializer instance that recognizes the previous serializer\u0026rsquo;s schema and configuration, and can therefore safely read data written by the previous serializer.
How Flink interacts with the TypeSerializer and TypeSerializerSnapshot abstractions # To wrap up, this section concludes how Flink, or more specifically the state backends, interact with the abstractions. The interaction is slightly different depending on the state backend, but this is orthogonal to the implementation of state serializers and their serializer snapshots.
Off-heap state backends (e.g. RocksDBStateBackend) # Register new state with a state serializer that has schema A the registered TypeSerializer for the state is used to read / write state on every state access. State is written in schema A. Take a savepoint The serializer snapshot is extracted via the TypeSerializer#snapshotConfiguration method. The serializer snapshot is written to the savepoint, as well as the already-serialized state bytes (with schema A). Restored execution re-accesses restored state bytes with new state serializer that has schema B The previous state serializer\u0026rsquo;s snapshot is restored. State bytes are not deserialized on restore, only loaded back to the state backends (therefore, still in schema A). Upon receiving the new serializer, it is provided to the restored previous serializer\u0026rsquo;s snapshot via the TypeSerializer#resolveSchemaCompatibility to check for schema compatibility. Migrate state bytes in backend from schema A to schema B If the compatibility resolution reflects that the schema has changed and migration is possible, schema migration is performed. The previous state serializer which recognizes schema A will be obtained from the serializer snapshot, via TypeSerializerSnapshot#restoreSerializer(), and is used to deserialize state bytes to objects, which in turn are re-written again with the new serializer, which recognizes schema B to complete the migration. All entries of the accessed state is migrated all-together before processing continues. If the resolution signals incompatibility, then the state access fails with an exception. Heap state backends (e.g. MemoryStateBackend, FsStateBackend) # Register new state with a state serializer that has schema A the registered TypeSerializer is maintained by the state backend. Take a savepoint, serializing all state with schema A The serializer snapshot is extracted via the TypeSerializer#snapshotConfiguration method. The serializer snapshot is written to the savepoint. State objects are now serialized to the savepoint, written in schema A. On restore, deserialize state into objects in heap The previous state serializer\u0026rsquo;s snapshot is restored. The previous serializer, which recognizes schema A, is obtained from the serializer snapshot, via TypeSerializerSnapshot#restoreSerializer(), and is used to deserialize state bytes to objects. From now on, all of the state is already deserialized. Restored execution re-accesses previous state with new state serializer that has schema B Upon receiving the new serializer, it is provided to the restored previous serializer\u0026rsquo;s snapshot via the TypeSerializer#resolveSchemaCompatibility to check for schema compatibility. If the compatibility check signals that migration is required, nothing happens in this case since for heap backends, all state is already deserialized into objects. If the resolution signals incompatibility, then the state access fails with an exception. Take another savepoint, serializing all state with schema B Same as step 2., but now state bytes are all in schema B. Predefined convenient TypeSerializerSnapshot classes # Flink provides two abstract base TypeSerializerSnapshot classes that can be used for typical scenarios: SimpleTypeSerializerSnapshot and CompositeTypeSerializerSnapshot.
Serializers that provide these predefined snapshots as their serializer snapshot must always have their own, independent subclass implementation. This corresponds to the best practice of not sharing snapshot classes across different serializers, which is more thoroughly explained in the next section.
Implementing a SimpleTypeSerializerSnapshot # The SimpleTypeSerializerSnapshot is intended for serializers that do not have any state or configuration, essentially meaning that the serialization schema of the serializer is solely defined by the serializer\u0026rsquo;s class.
There will only be 2 possible results of the compatibility resolution when using the SimpleTypeSerializerSnapshot as your serializer\u0026rsquo;s snapshot class:
TypeSerializerSchemaCompatibility.compatibleAsIs(), if the new serializer class remains identical, or TypeSerializerSchemaCompatibility.incompatible(), if the new serializer class is different then the previous one. Below is an example of how the SimpleTypeSerializerSnapshot is used, using Flink\u0026rsquo;s IntSerializer as an example:
public class IntSerializerSnapshot extends SimpleTypeSerializerSnapshot\u0026lt;Integer\u0026gt; { public IntSerializerSnapshot() { super(() -\u0026gt; IntSerializer.INSTANCE); } } The IntSerializer has no state or configurations. Serialization format is solely defined by the serializer class itself, and can only be read by another IntSerializer. Therefore, it suits the use case of the SimpleTypeSerializerSnapshot.
The base super constructor of the SimpleTypeSerializerSnapshot expects a Supplier of instances of the corresponding serializer, regardless of whether the snapshot is currently being restored or being written during snapshots. That supplier is used to create the restore serializer, as well as type checks to verify that the new serializer is of the same expected serializer class.
Implementing a CompositeTypeSerializerSnapshot # The CompositeTypeSerializerSnapshot is intended for serializers that rely on multiple nested serializers for serialization.
Before further explanation, we call the serializer, which relies on multiple nested serializer(s), as the \u0026ldquo;outer\u0026rdquo; serializer in this context. Examples for this could be MapSerializer, ListSerializer, GenericArraySerializer, etc. Consider the MapSerializer, for example - the key and value serializers would be the nested serializers, while MapSerializer itself is the \u0026ldquo;outer\u0026rdquo; serializer.
In this case, the snapshot of the outer serializer should also contain snapshots of the nested serializers, so that the compatibility of the nested serializers can be independently checked. When resolving the compatibility of the outer serializer, the compatibility of each nested serializer needs to be considered.
CompositeTypeSerializerSnapshot is provided to assist in the implementation of snapshots for these kind of composite serializers. It deals with reading and writing the nested serializer snapshots, as well as resolving the final compatibility result taking into account the compatibility of all nested serializers.
Below is an example of how the CompositeTypeSerializerSnapshot is used, using Flink\u0026rsquo;s MapSerializer as an example:
public class MapSerializerSnapshot\u0026lt;K, V\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;Map\u0026lt;K, V\u0026gt;, MapSerializer\u0026gt; { private static final int CURRENT_VERSION = 1; public MapSerializerSnapshot() { super(MapSerializer.class); } public MapSerializerSnapshot(MapSerializer\u0026lt;K, V\u0026gt; mapSerializer) { super(mapSerializer); } @Override public int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected MapSerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;K\u0026gt; keySerializer = (TypeSerializer\u0026lt;K\u0026gt;) nestedSerializers[0]; TypeSerializer\u0026lt;V\u0026gt; valueSerializer = (TypeSerializer\u0026lt;V\u0026gt;) nestedSerializers[1]; return new MapSerializer\u0026lt;\u0026gt;(keySerializer, valueSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(MapSerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getKeySerializer(), outerSerializer.getValueSerializer() }; } } When implementing a new serializer snapshot as a subclass of CompositeTypeSerializerSnapshot, the following three methods must be implemented:
#getCurrentOuterSnapshotVersion(): This method defines the version of the current outer serializer snapshot\u0026rsquo;s serialized binary format. #getNestedSerializers(TypeSerializer): Given the outer serializer, returns its nested serializers. #createOuterSerializerWithNestedSerializers(TypeSerializer[]): Given the nested serializers, create an instance of the outer serializer. The above example is a CompositeTypeSerializerSnapshot where there are no extra information to be snapshotted apart from the nested serializers\u0026rsquo; snapshots. Therefore, its outer snapshot version can be expected to never require an uptick. Some other serializers, however, contains some additional static configuration that needs to be persisted along with the nested component serializer. An example for this would be Flink\u0026rsquo;s GenericArraySerializer, which contains as configuration the class of the array element type, besides the nested element serializer.
In these cases, an additional three methods need to be implemented on the CompositeTypeSerializerSnapshot:
#writeOuterSnapshot(DataOutputView): defines how the outer snapshot information is written. #readOuterSnapshot(int, DataInputView, ClassLoader): defines how the outer snapshot information is read. #resolveOuterSchemaCompatibility(TypeSerializer): checks the compatibility based on the outer snapshot information. By default, the CompositeTypeSerializerSnapshot assumes that there isn\u0026rsquo;t any outer snapshot information to read / write, and therefore have empty default implementations for the above methods. If the subclass has outer snapshot information, then all three methods must be implemented.
Below is an example of how the CompositeTypeSerializerSnapshot is used for composite serializer snapshots that do have outer snapshot information, using Flink\u0026rsquo;s GenericArraySerializer as an example:
public final class GenericArraySerializerSnapshot\u0026lt;C\u0026gt; extends CompositeTypeSerializerSnapshot\u0026lt;C[], GenericArraySerializer\u0026gt; { private static final int CURRENT_VERSION = 1; private Class\u0026lt;C\u0026gt; componentClass; public GenericArraySerializerSnapshot() { super(GenericArraySerializer.class); } public GenericArraySerializerSnapshot(GenericArraySerializer\u0026lt;C\u0026gt; genericArraySerializer) { super(genericArraySerializer); this.componentClass = genericArraySerializer.getComponentClass(); } @Override protected int getCurrentOuterSnapshotVersion() { return CURRENT_VERSION; } @Override protected void writeOuterSnapshot(DataOutputView out) throws IOException { out.writeUTF(componentClass.getName()); } @Override protected void readOuterSnapshot(int readOuterSnapshotVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException { this.componentClass = InstantiationUtil.resolveClassByName(in, userCodeClassLoader); } @Override protected boolean resolveOuterSchemaCompatibility(GenericArraySerializer newSerializer) { return (this.componentClass == newSerializer.getComponentClass()) ? OuterSchemaCompatibility.COMPATIBLE_AS_IS : OuterSchemaCompatibility.INCOMPATIBLE; } @Override protected GenericArraySerializer createOuterSerializerWithNestedSerializers(TypeSerializer\u0026lt;?\u0026gt;[] nestedSerializers) { TypeSerializer\u0026lt;C\u0026gt; componentSerializer = (TypeSerializer\u0026lt;C\u0026gt;) nestedSerializers[0]; return new GenericArraySerializer\u0026lt;\u0026gt;(componentClass, componentSerializer); } @Override protected TypeSerializer\u0026lt;?\u0026gt;[] getNestedSerializers(GenericArraySerializer outerSerializer) { return new TypeSerializer\u0026lt;?\u0026gt;[] { outerSerializer.getComponentSerializer() }; } } There are two important things to notice in the above code snippet. First of all, since this CompositeTypeSerializerSnapshot implementation has outer snapshot information that is written as part of the snapshot, the outer snapshot version, as defined by getCurrentOuterSnapshotVersion(), must be upticked whenever the serialization format of the outer snapshot information changes.
Second of all, notice how we avoid using Java serialization when writing the component class, by only writing the classname and dynamically loading it when reading back the snapshot. Avoiding Java serialization for writing contents of serializer snapshots is in general a good practice to follow. More details about this is covered in the next section.
Implementation notes and best practices # 1. Flink restores serializer snapshots by instantiating them with their classname # A serializer\u0026rsquo;s snapshot, being the single source of truth for how a registered state was serialized, serves as an entry point to reading state in savepoints. In order to be able to restore and access previous state, the previous state serializer\u0026rsquo;s snapshot must be able to be restored.
Flink restores serializer snapshots by first instantiating the TypeSerializerSnapshot with its classname (written along with the snapshot bytes). Therefore, to avoid being subject to unintended classname changes or instantiation failures, TypeSerializerSnapshot classes should:
avoid being implemented as anonymous classes or nested classes, have a public, nullary constructor for instantiation 2. Avoid sharing the same TypeSerializerSnapshot class across different serializers # Since schema compatibility checks goes through the serializer snapshots, having multiple serializers returning the same TypeSerializerSnapshot class as their snapshot would complicate the implementation for the TypeSerializerSnapshot#resolveSchemaCompatibility and TypeSerializerSnapshot#restoreSerializer() method.
This would also be a bad separation of concerns; a single serializer\u0026rsquo;s serialization schema, configuration, as well as how to restore it, should be consolidated in its own dedicated TypeSerializerSnapshot class.
3. Avoid using Java serialization for serializer snapshot content # Java serialization should not be used at all when writing contents of a persisted serializer snapshot. Take for example, a serializer which needs to persist a class of its target type as part of its snapshot. Information about the class should be persisted by writing the class name, instead of directly serializing the class using Java. When reading the snapshot, the class name is read, and used to dynamically load the class via the name.
This practice ensures that serializer snapshots can always be safely read. In the above example, if the type class was persisted using Java serialization, the snapshot may no longer be readable once the class implementation has changed and is no longer binary compatible according to Java serialization specifics.
Migrating from deprecated serializer snapshot APIs before Flink 1.7 # This section is a guide for API migration from serializers and serializer snapshots that existed before Flink 1.7.
Before Flink 1.7, serializer snapshots were implemented as a TypeSerializerConfigSnapshot (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new TypeSerializerSnapshot interface). Moreover, the responsibility of serializer schema compatibility checks lived within the TypeSerializer, implemented in the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method.
Another major difference between the new and old abstractions is that the deprecated TypeSerializerConfigSnapshot did not have the capability of instantiating the previous serializer. Therefore, in the case where your serializer still returns a subclass of TypeSerializerConfigSnapshot as its snapshot, the serializer instance itself will always be written to savepoints using Java serialization so that the previous serializer may be available at restore time. This is very undesirable, since whether or not restoring the job will be successful is susceptible to availability of the previous serializer\u0026rsquo;s class, or in general, whether or not the serializer instance can be read back at restore time using Java serialization. This means that you be limited to the same serializer for your state, and could be problematic once you want to upgrade serializer classes or perform schema migration.
To be future-proof and have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. The steps to do this is as follows:
Implement a new subclass of TypeSerializerSnapshot. This will be the new snapshot for your serializer. Return the new TypeSerializerSnapshot as the serializer snapshot for your serializer in the TypeSerializer#snapshotConfiguration() method. Restore the job from the savepoint that existed before Flink 1.7, and then take a savepoint again. Note that at this step, the old TypeSerializerConfigSnapshot of the serializer must still exist in the classpath, and the implementation for the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method must not be removed. The purpose of this process is to replace the TypeSerializerConfigSnapshot written in old savepoints with the newly implemented TypeSerializerSnapshot for the serializer. Once you have a savepoint taken with Flink 1.7, the savepoint will contain TypeSerializerSnapshot as the state serializer snapshot, and the serializer instance will no longer be written in the savepoint. At this point, it is now safe to remove all implementations of the old abstraction (remove the old TypeSerializerConfigSnapshot implementation as will as the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) from the serializer). Back to top
`}),e.add({id:193,href:"/flink/flink-docs-master/zh/docs/ops/debugging/",title:"Debugging",section:"Operations",content:""}),e.add({id:194,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/describe/",title:"DESCRIBE 语句",section:"SQL",content:" DESCRIBE 语句 # DESCRIBE 语句用于描述表或视图的 schema。\n执行 DESCRIBE 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，executeSql() 方法会返回给定表的 schema，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。\nScala 可以使用 TableEnvironment 的 executeSql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，executeSql() 方法会返回给定表的 schema，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。\nPython 可以使用 TableEnvironment 的 execute_sql() 方法执行 DESCRIBE 语句。如果 DESCRIBE 操作执行成功，execute_sql() 方法会返回给定表的 schema，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 DESCRIBE 语句。\nSQL CLI DESCRIBE 语句可以在 SQL CLI 中执行。\n以下示例展示了如何在 SQL CLI 中执行一条 DESCRIBE 语句。\nJava TableEnvironment tableEnv = TableEnvironment.create(...); // 注册名为 “Orders” 的表 tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `user` BIGINT NOT NULl,\u0026#34; + \u0026#34; product VARCHAR(32),\u0026#34; + \u0026#34; amount INT,\u0026#34; + \u0026#34; ts TIMESTAMP(3),\u0026#34; + \u0026#34; ptime AS PROCTIME(),\u0026#34; + \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; + \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; + \u0026#34;) with (...)\u0026#34;); // 打印 schema tableEnv.executeSql(\u0026#34;DESCRIBE Orders\u0026#34;).print(); // 打印 schema tableEnv.executeSql(\u0026#34;DESC Orders\u0026#34;).print(); Scala val tableEnv = TableEnvironment.create(...) // 注册名为 “Orders” 的表 tableEnv.executeSql( \u0026#34;CREATE TABLE Orders (\u0026#34; + \u0026#34; `user` BIGINT NOT NULl,\u0026#34; + \u0026#34; product VARCHAR(32),\u0026#34; + \u0026#34; amount INT,\u0026#34; + \u0026#34; ts TIMESTAMP(3),\u0026#34; + \u0026#34; ptime AS PROCTIME(),\u0026#34; + \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; + \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; + \u0026#34;) with (...)\u0026#34;) // 打印 schema tableEnv.executeSql(\u0026#34;DESCRIBE Orders\u0026#34;).print() // 打印 schema tableEnv.executeSql(\u0026#34;DESC Orders\u0026#34;).print() Python table_env = TableEnvironment.create(...) # 注册名为 “Orders” 的表 table_env.execute_sql( \\ \u0026#34;CREATE TABLE Orders (\u0026#34; \u0026#34; `user` BIGINT NOT NULl,\u0026#34; \u0026#34; product VARCHAR(32),\u0026#34; \u0026#34; amount INT,\u0026#34; \u0026#34; ts TIMESTAMP(3),\u0026#34; \u0026#34; ptime AS PROCTIME(),\u0026#34; \u0026#34; PRIMARY KEY(`user`) NOT ENFORCED,\u0026#34; \u0026#34; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS\u0026#34; \u0026#34;) with (...)\u0026#34;); # 打印 schema table_env.execute_sql(\u0026#34;DESCRIBE Orders\u0026#34;).print() # 打印 schema table_env.execute_sql(\u0026#34;DESC Orders\u0026#34;).print() SQL CLI Flink SQL\u0026gt; CREATE TABLE Orders ( \u0026gt; `user` BIGINT NOT NULl, \u0026gt; product VARCHAR(32), \u0026gt; amount INT, \u0026gt; ts TIMESTAMP(3), \u0026gt; ptime AS PROCTIME(), \u0026gt; PRIMARY KEY(`user`) NOT ENFORCED, \u0026gt; WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; SECONDS \u0026gt; ) with ( \u0026gt; ... \u0026gt; ); [INFO] Table has been created. Flink SQL\u0026gt; DESCRIBE Orders; Flink SQL\u0026gt; DESC Orders; 上述示例的结果是： Java +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set Scala +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set Python +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | name | type | null | key | computed column | watermark | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP(3) NOT NULL *PROCTIME* | false | | PROCTIME() | | +---------+----------------------------------+-------+-----------+-----------------+----------------------------+ 5 rows in set SQL CLI root |-- user: BIGINT NOT NULL |-- product: VARCHAR(32) |-- amount: INT |-- ts: TIMESTAMP(3) *ROWTIME* |-- ptime: TIMESTAMP(3) NOT NULL *PROCTIME* AS PROCTIME() |-- WATERMARK FOR ts AS `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND |-- CONSTRAINT PK_3599338 PRIMARY KEY (user) Back to top\n语法 # { DESCRIBE | DESC } [catalog_name.][db_name.]table_name "}),e.add({id:195,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/pubsub/",title:"Google Cloud PubSub",section:"DataStream Connectors",content:` Google Cloud PubSub # 这个连接器可向 Google Cloud PubSub 读取与写入数据。添加下面的依赖来使用此连接器:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-pubsub\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 注意：此连接器最近才加到 Flink 里，还未接受广泛测试。 注意连接器目前还不是二进制发行版的一部分，添加依赖、打包配置以及集群运行信息请参考这里
Consuming or Producing PubSubMessages # 连接器可以接收和发送 Google PubSub 的信息。和 Google PubSub 一样，这个连接器能够保证至少一次的语义。
PubSub SourceFunction # PubSubSource 类的对象由构建类来构建: PubSubSource.newBuilder(...)
有多种可选的方法来创建 PubSubSource，但最低要求是要提供 Google Project、Pubsub 订阅和反序列化 PubSubMessages 的方法。
Example:
Java StreamExecutionEnvironment streamExecEnv = StreamExecutionEnvironment.getExecutionEnvironment(); DeserializationSchema\u0026lt;SomeObject\u0026gt; deserializer = (...); SourceFunction\u0026lt;SomeObject\u0026gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializer) .withProjectName(\u0026#34;project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .build(); streamExecEnv.addSource(pubsubSource); 当前还不支持 PubSub 的 source functions pulls messages 和 push endpoints。
PubSub Sink # PubSubSink 类的对象由构建类来构建: PubSubSink.newBuilder(...)
构建类的使用方式与 PubSubSource 类似。
Example:
Java DataStream\u0026lt;SomeObject\u0026gt; dataStream = (...); SerializationSchema\u0026lt;SomeObject\u0026gt; serializationSchema = (...); SinkFunction\u0026lt;SomeObject\u0026gt; pubsubSink = PubSubSink.newBuilder() .withSerializationSchema(serializationSchema) .withProjectName(\u0026#34;project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .build() dataStream.addSink(pubsubSink); Google Credentials # 应用程序需要使用 Credentials 来通过认证和授权才能使用 Google Cloud Platform 的资源，例如 PubSub。
上述的两个构建类都允许你提供 Credentials, 但是连接器默认会通过环境变量: GOOGLE_APPLICATION_CREDENTIALS 来获取 Credentials 的路径。
如果你想手动提供 Credentials，例如你想从外部系统读取 Credentials，你可以使用 PubSubSource.newBuilder(...).withCredentials(...)。
集成测试 # 在集成测试的时候，如果你不想直接连 PubSub 而是想读取和写入一个 docker container，可以参照 PubSub testing locally。
下面的例子展示了如何使用 source 来从仿真器读取信息并发送回去：
Java String hostAndPort = \u0026#34;localhost:1234\u0026#34;; DeserializationSchema\u0026lt;SomeObject\u0026gt; deserializationSchema = (...); SourceFunction\u0026lt;SomeObject\u0026gt; pubsubSource = PubSubSource.newBuilder() .withDeserializationSchema(deserializationSchema) .withProjectName(\u0026#34;my-fake-project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .withPubSubSubscriberFactory(new PubSubSubscriberFactoryForEmulator(hostAndPort, \u0026#34;my-fake-project\u0026#34;, \u0026#34;subscription\u0026#34;, 10, Duration.ofSeconds(15), 100)) .build(); SerializationSchema\u0026lt;SomeObject\u0026gt; serializationSchema = (...); SinkFunction\u0026lt;SomeObject\u0026gt; pubsubSink = PubSubSink.newBuilder() .withSerializationSchema(serializationSchema) .withProjectName(\u0026#34;my-fake-project\u0026#34;) .withSubscriptionName(\u0026#34;subscription\u0026#34;) .withHostAndPortForEmulator(hostAndPort) .build(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(pubsubSource) .addSink(pubsubSink); 至少一次语义保证 # SourceFunction # 有很多原因导致会一个信息会被多次发出，例如 Google PubSub 的故障。
另一个可能的原因是超过了确认的截止时间，即收到与确认信息之间的时间间隔。PubSubSource 只有在信息被成功快照之后才会确认以保证至少一次的语义。这意味着，如果你的快照间隔大于信息确认的截止时间，那么你订阅的信息很有可能会被多次处理。
因此，我们建议把快照的间隔设置得比信息确认截止时间更短。
参照 PubSub 来增加信息确认截止时间。
注意: PubSubMessagesProcessedNotAcked 显示了有多少信息正在等待下一个 checkpoint 还没被确认。
SinkFunction # Sink function 会把准备发到 PubSub 的信息短暂地缓存以提高性能。每次 checkpoint 前，它会刷新缓冲区，并且只有当所有信息成功发送到 PubSub 之后，checkpoint 才会成功完成。
Back to top
`}),e.add({id:196,href:"/flink/flink-docs-master/zh/docs/dev/dataset/hadoop_compatibility/",title:"Hadoop 兼容",section:"DataSet API (Legacy)",content:` Hadoop 兼容 # Flink is compatible with Apache Hadoop MapReduce interfaces and therefore allows reusing code that was implemented for Hadoop MapReduce.
You can:
use Hadoop\u0026rsquo;s Writable data types in Flink programs. use any Hadoop InputFormat as a DataSource. use any Hadoop OutputFormat as a DataSink. use a Hadoop Mapper as FlatMapFunction. use a Hadoop Reducer as GroupReduceFunction. This document shows how to use existing Hadoop MapReduce code with Flink. Please refer to the Connecting to other systems guide for reading from Hadoop supported file systems.
Project Configuration # Support for Hadoop input/output formats is part of the flink-java and flink-scala Maven modules that are always required when writing Flink jobs. The code is located in org.apache.flink.api.java.hadoop and org.apache.flink.api.scala.hadoop in an additional sub-package for the mapred and mapreduce API.
Support for Hadoop Mappers and Reducers is contained in the flink-hadoop-compatibility Maven module. This code resides in the org.apache.flink.hadoopcompatibility package.
Add the following dependency to your pom.xml if you want to reuse Mappers and Reducers.
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Using Hadoop InputFormats # To use Hadoop InputFormats with Flink the format must first be wrapped using either readHadoopFile or createHadoopInput of the HadoopInputs utility class. The former is used for input formats derived from FileInputFormat while the latter has to be used for general purpose input formats. The resulting InputFormat can be used to create a data source by using ExecutionEnvironment#createInput.
The resulting DataSet contains 2-tuples where the first field is the key and the second field is the value retrieved from the Hadoop InputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextInputFormat.
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; input = env.createInput(HadoopInputs.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath)); // Do something with the data. [...] Scala val env = ExecutionEnvironment.getExecutionEnvironment val input: DataSet[(LongWritable, Text)] = env.createInput(HadoopInputs.readHadoopFile( new TextInputFormat, classOf[LongWritable], classOf[Text], textPath)) // Do something with the data. [...] Using Hadoop OutputFormats # Flink provides a compatibility wrapper for Hadoop OutputFormats. Any class that implements org.apache.hadoop.mapred.OutputFormat or extends org.apache.hadoop.mapreduce.OutputFormat is supported. The OutputFormat wrapper expects its input data to be a DataSet containing 2-tuples of key and value. These are to be processed by the Hadoop OutputFormat.
The following example shows how to use Hadoop\u0026rsquo;s TextOutputFormat.
Java // Obtain the result we want to emit DataSet\u0026lt;Tuple2\u0026lt;Text, IntWritable\u0026gt;\u0026gt; hadoopResult = [...]; // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt; hadoopOF = // create the Flink wrapper. new HadoopOutputFormat\u0026lt;Text, IntWritable\u0026gt;( // set the Hadoop OutputFormat and specify the job. new TextOutputFormat\u0026lt;Text, IntWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. hadoopResult.output(hadoopOF); Scala // Obtain your result to emit. val hadoopResult: DataSet[(Text, IntWritable)] = [...] val hadoopOF = new HadoopOutputFormat[Text,IntWritable]( new TextOutputFormat[Text, IntWritable], new JobConf) hadoopOF.getJobConf.set(\u0026#34;mapred.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;) FileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) hadoopResult.output(hadoopOF) Using Hadoop Mappers and Reducers # Hadoop Mappers are semantically equivalent to Flink\u0026rsquo;s FlatMapFunctions and Hadoop Reducers are equivalent to Flink\u0026rsquo;s GroupReduceFunctions. Flink provides wrappers for implementations of Hadoop MapReduce\u0026rsquo;s Mapper and Reducer interfaces, i.e., you can reuse your Hadoop Mappers and Reducers in regular Flink programs. At the moment, only the Mapper and Reduce interfaces of Hadoop\u0026rsquo;s mapred API (org.apache.hadoop.mapred) are supported.
The wrappers take a DataSet\u0026lt;Tuple2\u0026lt;KEYIN,VALUEIN\u0026gt;\u0026gt; as input and produce a DataSet\u0026lt;Tuple2\u0026lt;KEYOUT,VALUEOUT\u0026gt;\u0026gt; as output where KEYIN and KEYOUT are the keys and VALUEIN and VALUEOUT are the values of the Hadoop key-value pairs that are processed by the Hadoop functions. For Reducers, Flink offers a wrapper for a GroupReduceFunction with (HadoopReduceCombineFunction) and without a Combiner (HadoopReduceFunction). The wrappers accept an optional JobConf object to configure the Hadoop Mapper or Reducer.
Flink\u0026rsquo;s function wrappers are
org.apache.flink.hadoopcompatibility.mapred.HadoopMapFunction, org.apache.flink.hadoopcompatibility.mapred.HadoopReduceFunction, and org.apache.flink.hadoopcompatibility.mapred.HadoopReduceCombineFunction. and can be used as regular Flink FlatMapFunctions or GroupReduceFunctions.
The following example shows how to use Hadoop Mapper and Reducer functions.
// Obtain data to process somehow. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = [...]; DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); Please note: The Reducer wrapper works on groups as defined by Flink\u0026rsquo;s groupBy() operation. It does not consider any custom partitioners, sort or grouping comparators you might have set in the JobConf.
Complete Hadoop WordCount Example # The following example shows a complete WordCount implementation using Hadoop data types, Input- and OutputFormats, and Mapper and Reducer implementations.
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // Set up the Hadoop TextInputFormat. Job job = Job.getInstance(); HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt; hadoopIF = new HadoopInputFormat\u0026lt;LongWritable, Text\u0026gt;( new TextInputFormat(), LongWritable.class, Text.class, job ); TextInputFormat.addInputPath(job, new Path(inputPath)); // Read data using the Hadoop TextInputFormat. DataSet\u0026lt;Tuple2\u0026lt;LongWritable, Text\u0026gt;\u0026gt; text = env.createInput(hadoopIF); DataSet\u0026lt;Tuple2\u0026lt;Text, LongWritable\u0026gt;\u0026gt; result = text // use Hadoop Mapper (Tokenizer) as MapFunction .flatMap(new HadoopMapFunction\u0026lt;LongWritable, Text, Text, LongWritable\u0026gt;( new Tokenizer() )) .groupBy(0) // use Hadoop Reducer (Counter) as Reduce- and CombineFunction .reduceGroup(new HadoopReduceCombineFunction\u0026lt;Text, LongWritable, Text, LongWritable\u0026gt;( new Counter(), new Counter() )); // Set up the Hadoop TextOutputFormat. HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt; hadoopOF = new HadoopOutputFormat\u0026lt;Text, LongWritable\u0026gt;( new TextOutputFormat\u0026lt;Text, LongWritable\u0026gt;(), job ); hadoopOF.getConfiguration().set(\u0026#34;mapreduce.output.textoutputformat.separator\u0026#34;, \u0026#34; \u0026#34;); TextOutputFormat.setOutputPath(job, new Path(outputPath)); // Emit data using the Hadoop TextOutputFormat. result.output(hadoopOF); // Execute Program env.execute(\u0026#34;Hadoop WordCount\u0026#34;); Back to top
`}),e.add({id:197,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/hybridsource/",title:"Hybrid Source",section:"DataStream Connectors",content:` Hybrid Source # HybridSource is a source that contains a list of concrete sources. It solves the problem of sequentially reading input from heterogeneous sources to produce a single input stream.
For example, a bootstrap use case may need to read several days worth of bounded input from S3 before continuing with the latest unbounded input from Kafka. HybridSource switches from FileSource to KafkaSource when the bounded file input finishes without interrupting the application.
Prior to HybridSource, it was necessary to create a topology with multiple sources and define a switching mechanism in user land, which leads to operational complexity and inefficiency.
With HybridSource the multiple sources appear as a single source in the Flink job graph and from DataStream API perspective.
For more background see FLIP-150
To use the connector, add the flink-connector-base dependency to your project:
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-base\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! (Typically comes as transitive dependency with concrete sources.)
Start position for next source # To arrange multiple sources in a HybridSource, all sources except the last one need to be bounded. Therefore, the sources typically need to be assigned a start and end position. The last source may be bounded in which case the HybridSource is bounded and unbounded otherwise. Details depend on the specific source and the external storage systems.
Here we cover the most basic and then a more complex scenario, following the File/Kafka example.
Fixed start position at graph construction time # Example: Read till pre-determined switch time from files and then continue reading from Kafka. Each source covers an upfront known range and therefore the contained sources can be created upfront as if they were used directly:
Java long switchTimestamp = ...; // derive from file input paths FileSource\u0026lt;String\u0026gt; fileSource = FileSource.forRecordStreamFormat(new TextLineInputFormat(), Path.fromLocalFile(testDir)).build(); KafkaSource\u0026lt;String\u0026gt; kafkaSource = KafkaSource.\u0026lt;String\u0026gt;builder() .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + 1)) .build(); HybridSource\u0026lt;String\u0026gt; hybridSource = HybridSource.builder(fileSource) .addSource(kafkaSource) .build(); Python switch_timestamp = ... # derive from file input paths file_source = FileSource \\ .for_record_stream_format(StreamFormat.text_line_format(), test_dir) \\ .build() kafka_source = KafkaSource \\ .builder() \\ .set_bootstrap_servers(\u0026#39;localhost:9092\u0026#39;) \\ .set_group_id(\u0026#39;MY_GROUP\u0026#39;) \\ .set_topics(\u0026#39;quickstart-events\u0026#39;) \\ .set_value_only_deserializer(SimpleStringSchema()) \\ .set_starting_offsets(KafkaOffsetsInitializer.timestamp(switch_timestamp)) \\ .build() hybrid_source = HybridSource.builder(file_source).add_source(kafka_source).build() Dynamic start position at switch time # Example: File source reads a very large backlog, taking potentially longer than retention available for next source. Switch needs to occur at \u0026ldquo;current time - X\u0026rdquo;. This requires the start time for the next source to be set at switch time. Here we require transfer of end position from the previous file enumerator for deferred construction of KafkaSource by implementing SourceFactory.
Note that enumerators need to support getting the end timestamp. This may currently require a source customization. Adding support for dynamic end position to FileSource is tracked in FLINK-23633.
Java FileSource\u0026lt;String\u0026gt; fileSource = CustomFileSource.readTillOneDayFromLatest(); HybridSource\u0026lt;String\u0026gt; hybridSource = HybridSource.\u0026lt;String, CustomFileSplitEnumerator\u0026gt;builder(fileSource) .addSource( switchContext -\u0026gt; { CustomFileSplitEnumerator previousEnumerator = switchContext.getPreviousEnumerator(); // how to get timestamp depends on specific enumerator long switchTimestamp = previousEnumerator.getEndTimestamp(); KafkaSource\u0026lt;String\u0026gt; kafkaSource = KafkaSource.\u0026lt;String\u0026gt;builder() .setStartingOffsets(OffsetsInitializer.timestamp(switchTimestamp + 1)) .build(); return kafkaSource; }, Boundedness.CONTINUOUS_UNBOUNDED) .build(); Python Still not supported in Python API. `}),e.add({id:198,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/ogg/",title:"Ogg",section:"Formats",content:` Ogg Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Oracle GoldenGate (a.k.a ogg) 是一个实现异构 IT 环境间数据实时数据集成和复制的综合软件包。 该产品集支持高可用性解决方案、实时数据集成、事务更改数据捕获、运营和分析企业系统之间的数据复制、转换和验证。Ogg 为变更日志提供了统一的格式结构，并支持使用 JSON 序列化消息。
Flink 支持将 Ogg JSON 消息解析为 INSERT/UPDATE/DELETE 消息到 Flink SQL 系统中。在很多情况下，利用这个特性非常有用，例如
将增量数据从数据库同步到其他系统 日志审计 数据库的实时物化视图 关联维度数据库的变更历史，等等 Flink 还支持将 Flink SQL 中的 INSERT/UPDATE/DELETE 消息编码为 Ogg JSON 格式的消息, 输出到 Kafka 等存储中。 但需要注意, 目前 Flink 还不支持将 UPDATE_BEFORE 和 UPDATE_AFTER 合并为一条 UPDATE 消息. 因此, Flink 将 UPDATE_BEFORE 和 UPDATE_AFTER 分别编码为 DELETE 和 INSERT 类型的 Ogg 消息。
Dependencies # Ogg Json # In order to use the Ogg the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-json\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Built-in 注意: 请参考 Ogg Kafka Handler documentation， 了解如何设置 Ogg Kafka handler 来将变更日志同步到 Kafka 的 Topic。
How to use Ogg format # Ogg 为变更日志提供了统一的格式, 这是一个 JSON 格式的从 Oracle PRODUCTS 表捕获的更新操作的简单示例：
{ \u0026#34;before\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.18 }, \u0026#34;after\u0026#34;: { \u0026#34;id\u0026#34;: 111, \u0026#34;name\u0026#34;: \u0026#34;scooter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Big 2-wheel scooter\u0026#34;, \u0026#34;weight\u0026#34;: 5.15 }, \u0026#34;op_type\u0026#34;: \u0026#34;U\u0026#34;, \u0026#34;op_ts\u0026#34;: \u0026#34;2020-05-13 15:40:06.000000\u0026#34;, \u0026#34;current_ts\u0026#34;: \u0026#34;2020-05-13 15:40:07.000000\u0026#34;, \u0026#34;primary_keys\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;pos\u0026#34;: \u0026#34;00000000000000000000143\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;PRODUCTS\u0026#34; } 注意：请参考 Debezium documentation 了解每个字段的含义.
Oracle PRODUCTS 表 有 4 列 (id, name, description and weight). 上面的 JSON 消息是 PRODUCTS 表上的一条更新事件，其中 id = 111 的行的 weight 值从 5.18 更改为 5.15. 假设此消息已同步到 Kafka 的 Topic products_ogg, 则可以使用以下 DDL 来使用该 Topic 并解析更新事件。
CREATE TABLE topic_products ( -- schema is totally the same to the Oracle \u0026#34;products\u0026#34; table id BIGINT, name STRING, description STRING, weight DECIMAL(10, 2) ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;products_ogg\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;ogg-json\u0026#39; ) 再将 Kafka Topic 注册为 Flink 表之后， 可以将 OGG 消息变为变更日志源。
-- a real-time materialized view on the Oracle \u0026#34;PRODUCTS\u0026#34; -- which calculate the latest average of weight for the same products SELECT name, AVG(weight) FROM topic_products GROUP BY name; -- synchronize all the data and incremental changes of Oracle \u0026#34;PRODUCTS\u0026#34; table to -- Elasticsearch \u0026#34;products\u0026#34; index for future searching INSERT INTO elasticsearch_products SELECT * FROM topic_products; Available Metadata # The following format metadata can be exposed as read-only (VIRTUAL) columns in a table definition.
Attention Format metadata fields are only available if the corresponding connector forwards format metadata. Currently, only the Kafka connector is able to expose metadata fields for its value format.
Key Data Type Description table STRING NULL Contains fully qualified table name. The format of the fully qualified table name is: CATALOG NAME.SCHEMA NAME.TABLE NAME primary-keys ARRAY\u0026lt;STRING\u0026gt; NULL An array variable holding the column names of the primary keys of the source table. The primary-keys field is only include in the JSON output if the includePrimaryKeys configuration property is set to true. ingestion-timestamp TIMESTAMP_LTZ(6) NULL The timestamp at which the connector processed the event. Corresponds to the current_ts field in the Ogg record. event-timestamp TIMESTAMP_LTZ(6) NULL The timestamp at which the source system created the event. Corresponds to the op_ts field in the Ogg record. The following example shows how to access Ogg metadata fields in Kafka:
CREATE TABLE KafkaTable ( origin_ts TIMESTAMP(3) METADATA FROM \u0026#39;value.ingestion-timestamp\u0026#39; VIRTUAL, event_time TIMESTAMP(3) METADATA FROM \u0026#39;value.event-timestamp\u0026#39; VIRTUAL, origin_table STRING METADATA FROM \u0026#39;value.table\u0026#39; VIRTUAL, primary_keys ARRAY\u0026lt;STRING\u0026gt; METADATA FROM \u0026#39;value.primary_keys\u0026#39; VIRTUAL, user_id BIGINT, item_id BIGINT, behavior STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;user_behavior\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;earliest-offset\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;ogg-json\u0026#39; ); Format Options # 选项 要求 默认 类型 描述 format 必填 (none) String 指定要使用的格式，此处应为 'ogg-json'. ogg-json.ignore-parse-errors 选填 false Boolean 当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null。 debezium-json.timestamp-format.standard 可选 'SQL' String 声明输入和输出的时间戳格式。当前支持的格式为'SQL' 以及 'ISO-8601'： 可选参数 'SQL' 将会以 "yyyy-MM-dd HH:mm:ss.s{precision}" 的格式解析时间戳, 例如 '2020-12-30 12:13:14.123'，且会以相同的格式输出。 可选参数 'ISO-8601' 将会以 "yyyy-MM-ddTHH:mm:ss.s{precision}" 的格式解析输入时间戳, 例如 '2020-12-30T12:13:14.123' ，且会以相同的格式输出。 ogg-json.map-null-key.mode 选填 'FAIL' String 指定处理 Map 中 key 值为空的方法. 当前支持的值有 'FAIL', 'DROP' 和 'LITERAL': Option 'FAIL' 将抛出异常。 Option 'DROP' 将丢弃 Map 中 key 值为空的数据项。 Option 'LITERAL' 将使用字符串常量来替换 Map 中的空 key 值。字符串常量的值由 ogg-json.map-null-key.literal 定义。 ogg-json.map-null-key.literal 选填 'null' String 当 'ogg-json.map-null-key.mode' 是 LITERAL 的时候，指定字符串常量替换 Map 中的空 key 值。 Data Type Mapping # 目前, Ogg format 使用 JSON format 进行序列化和反序列化。有关数据类型映射的更多详细信息，请参考 JSON Format 文档。
`}),e.add({id:199,href:"/flink/flink-docs-master/zh/docs/ops/",title:"Operations",section:"Docs",content:""}),e.add({id:200,href:"/flink/flink-docs-master/zh/docs/deployment/security/",title:"Security",section:"Deployment",content:""}),e.add({id:201,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/group-agg/",title:"分组聚合",section:"Queries 查询",content:` Group Aggregation # Batch Streaming
Like most data systems, Apache Flink supports aggregate functions; both built-in and user-defined. User-defined functions must be registered in a catalog before use.
An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT, SUM, AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.
SELECT COUNT(*) FROM Orders For streaming queries, it is important to understand that Flink runs continuous queries that never terminate. Instead, they update their result table according to the updates on its input tables. For the above query, Flink will output an updated count each time a new row is inserted into the Orders table.
Apache Flink supports the standard GROUP BY clause for aggregating data.
SELECT COUNT(*) FROM Orders GROUP BY order_id For streaming queries, the required state for computing the query result might grow infinitely. State size depends on the number of groups and the number and type of aggregation functions. For example MIN/MAX are heavy on state size while COUNT is cheap. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
Apache Flink provides a set of performance tuning ways for Group Aggregation, see more Performance Tuning.
DISTINCT Aggregation # Distinct aggregates remove duplicate values before applying an aggregation function. The following example counts the number of distinct order_ids instead of the total number of rows in the Orders table.
SELECT COUNT(DISTINCT order_id) FROM Orders For streaming queries, the required state for computing the query result might grow infinitely. State size is mostly depends on the number of distinct rows and the time that a group is maintained, short lived group by windows are not a problem. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
GROUPING SETS # Grouping sets allow for more complex grouping operations than those describable by a standard GROUP BY. Rows are grouped separately by each specified grouping set and aggregates are computed for each group just as for simple GROUP BY clauses.
SELECT supplier_id, rating, COUNT(*) AS total FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY GROUPING SETS ((supplier_id, rating), (supplier_id), ()) Results:
+-------------+--------+-------+ | supplier_id | rating | total | +-------------+--------+-------+ | supplier1 | 4 | 1 | | supplier1 | (NULL) | 2 | | (NULL) | (NULL) | 4 | | supplier1 | 3 | 1 | | supplier2 | 3 | 1 | | supplier2 | (NULL) | 2 | | supplier2 | 4 | 1 | +-------------+--------+-------+ Each sublist of GROUPING SETS may specify zero or more columns or expressions and is interpreted the same way as though it was used directly in the GROUP BY clause. An empty grouping set means that all rows are aggregated down to a single group, which is output even if no input rows were present.
References to the grouping columns or expressions are replaced by null values in result rows for grouping sets in which those columns do not appear.
For streaming queries, the required state for computing the query result might grow infinitely. State size depends on number of group sets and type of aggregation functions. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
ROLLUP # ROLLUP is a shorthand notation for specifying a common type of grouping set. It represents the given list of expressions and all prefixes of the list, including the empty list.
For example, the following query is equivalent to the one above.
SELECT supplier_id, rating, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY ROLLUP (supplier_id, rating) CUBE # CUBE is a shorthand notation for specifying a common type of grouping set. It represents the given list and all of its possible subsets - the power set.
For example, the following two queries are equivalent.
SELECT supplier_id, rating, product_id, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY CUBE (supplier_id, rating, product_id) SELECT supplier_id, rating, product_id, COUNT(*) FROM (VALUES (\u0026#39;supplier1\u0026#39;, \u0026#39;product1\u0026#39;, 4), (\u0026#39;supplier1\u0026#39;, \u0026#39;product2\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product3\u0026#39;, 3), (\u0026#39;supplier2\u0026#39;, \u0026#39;product4\u0026#39;, 4)) AS Products(supplier_id, product_id, rating) GROUP BY GROUPING SET ( ( supplier_id, product_id, rating ), ( supplier_id, product_id ), ( supplier_id, rating ), ( supplier_id ), ( product_id, rating ), ( product_id ), ( rating ), ( ) ) HAVING # HAVING eliminates group rows that do not satisfy the condition. HAVING is different from WHERE: WHERE filters individual rows before the GROUP BY while HAVING filters group rows created by GROUP BY. Each column referenced in condition must unambiguously reference a grouping column unless it appears within an aggregate function.
SELECT SUM(amount) FROM Orders GROUP BY users HAVING SUM(amount) \u0026gt; 50 The presence of HAVING turns a query into a grouped query even if there is no GROUP BY clause. It is the same as what happens when the query contains aggregate functions but no GROUP BY clause. The query considers all selected rows to form a single group, and the SELECT list and HAVING clause can only reference table columns from within aggregate functions. Such a query will emit a single row if the HAVING condition is true, zero rows if it is not true.
Back to top
`}),e.add({id:202,href:"/flink/flink-docs-master/zh/docs/connectors/table/filesystem/",title:"文件系统",section:"Table API Connectors",content:` 文件系统 SQL 连接器 # 此连接器提供了对 Flink FileSystem abstraction 支持的文件系统中分区文件的访问。
在 Flink 中包含了该文件系统连接器，不需要添加额外的依赖。相应的 jar 包可以在 Flink 工程项目的 /lib 目录下找到。从文件系统中读取或者向文件系统中写入行时，需要指定相应的 format。
文件系统连接器允许从本地或分布式文件系统进行读写。文件系统表可以定义为：
CREATE TABLE MyUserTable ( column_name1 INT, column_name2 STRING, ... part_name1 INT, part_name2 STRING ) PARTITIONED BY (part_name1, part_name2) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, -- 必选：指定连接器类型 \u0026#39;path\u0026#39; = \u0026#39;file:///path/to/whatever\u0026#39;, -- 必选：指定路径 \u0026#39;format\u0026#39; = \u0026#39;...\u0026#39;, -- 必选：文件系统连接器指定 format -- 有关更多详情，请参考 Table Formats \u0026#39;partition.default-name\u0026#39; = \u0026#39;...\u0026#39;, -- 可选：默认的分区名，动态分区模式下分区字段值是 null 或空字符串 -- 可选：该属性开启了在 sink 阶段通过动态分区字段来 shuffle 数据，该功能可以大大减少文件系统 sink 的文件数，但是可能会导致数据倾斜，默认值是 false \u0026#39;sink.shuffle-by-partition.enable\u0026#39; = \u0026#39;...\u0026#39;, ... ) 请确保包含 Flink File System specific dependencies。 基于流的文件系统 sources 仍在开发中。未来，社区将增加对常见地流式用例的支持，例如，对分区和目录的监控等。 文件系统连接器的特性与 previous legacy filesystem connector 有很大不同： path 属性指定的是目录，而不是文件，该目录下的文件也不是肉眼可读的。 分区文件 # Flink 的文件系统连接器支持分区，使用了标准的 hive。但是，不需要预先注册分区到 table catalog，而是基于目录结构自动做了分区发现。例如，根据下面的目录结构，分区表将被推断包含 datetime 和 hour 分区。
path └── datetime=2019-08-25 └── hour=11 ├── part-0.parquet ├── part-1.parquet └── hour=12 ├── part-0.parquet └── datetime=2019-08-26 └── hour=6 ├── part-0.parquet 文件系统表支持分区新增插入和分区覆盖插入。请参考 INSERT Statement。当对分区表进行分区覆盖插入时，只有相应的分区会被覆盖，而不是整个表。
File Formats # 文件系统连接器支持多种 format：
CSV：RFC-4180。是非压缩的。 JSON：注意，文件系统连接器的 JSON format 与传统的标准的 JSON file 的不同，而是非压缩的。换行符分割的 JSON。 Avro：Apache Avro。通过配置 avro.codec 属性支持压缩。 Parquet：Apache Parquet。兼容 hive。 Orc：Apache Orc。兼容 hive。 Debezium-JSON：debezium-json。 Canal-JSON：canal-json。 Raw：raw。 Source # 文件系统连接器可用于将单个文件或整个目录的数据读取到单个表中。
当使用目录作为 source 路径时，对目录中的文件进行 无序的读取。
目录监控 # 当运行模式为流模式时，文件系统连接器会自动监控输入目录。
可以使用以下属性修改监控时间间隔。
键 默认值 类型 描述 source.monitor-interval (无) Duration 设置新文件的监控时间间隔，并且必须设置 \u003e 0 的值。 每个文件都由其路径唯一标识，一旦发现新文件，就会处理一次。 已处理的文件在 source 的整个生命周期内存储在 state 中，因此，source 的 state 在 checkpoint 和 savepoint 时进行保存。 更短的时间间隔意味着文件被更快地发现，但也意味着更频繁地遍历文件系统/对象存储。 如果未设置此配置选项，则提供的路径仅被扫描一次，因此源将是有界的。 可用的 Metadata # 以下连接器 metadata 可以在表定义时作为 metadata 列进行访问。所有 metadata 都是只读的。
键 数据类型 描述 file.path STRING NOT NULL 输入文件的完整路径。 file.name STRING NOT NULL 文件名，即距离文件根路径最远的元素。 file.size BIGINT NOT NULL 文件的字节数。 file.modification-time TIMESTAMP_LTZ(3) NOT NULL 文件的修改时间。 扩展的 CREATE TABLE 示例演示了标识某个字段为 metadata 的语法：
CREATE TABLE MyUserTableWithFilepath ( column_name1 INT, column_name2 STRING, \`file.path\` STRING NOT NULL METADATA ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;file:///path/to/whatever\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) Streaming Sink # 文件系统连接器支持流写入，是基于 Flink 的 文件系统 写入文件的。CSV 和 JSON 使用的是 Row-encoded Format。Parquet、ORC 和 Avro 使用的是 Bulk-encoded Format。
可以直接编写 SQL，将流数据插入到非分区表。 如果是分区表，可以配置分区操作相关的属性。请参考分区提交了解更多详情。
滚动策略 # 分区目录下的数据被分割到 part 文件中。每个分区对应的 sink 的收到的数据的 subtask 都至少会为该分区生成一个 part 文件。根据可配置的滚动策略，当前 in-progress part 文件将被关闭，生成新的 part 文件。该策略基于大小，和指定的文件可被打开的最大 timeout 时长，来滚动 part 文件。
键 默认值 类型 描述 sink.rolling-policy.file-size 128MB MemorySize 滚动前，part 文件最大大小。 sink.rolling-policy.rollover-interval 30 min Duration 滚动前，part 文件处于打开状态的最大时长（默认值30分钟，以避免产生大量小文件）。 检查频率是由 'sink.rolling-policy.check-interval' 属性控制的。 sink.rolling-policy.check-interval 1 min Duration 基于时间的滚动策略的检查间隔。该属性控制了基于 'sink.rolling-policy.rollover-interval' 属性检查文件是否该被滚动的检查频率。 注意： 对于 bulk formats 数据 (parquet、orc、avro)，滚动策略与 checkpoint 间隔（pending 状态的文件会在下个 checkpoint 完成）控制了 part 文件的大小和个数。
注意： 对于 row formats 数据 (csv、json)，如果想使得分区文件更快在文件系统中可见，可以设置 sink.rolling-policy.file-size 或 sink.rolling-policy.rollover-interval 属性以及在 flink-conf.yaml 中的 execution.checkpointing.interval 属性。 对于其他 formats (avro、orc)，可以只设置 flink-conf.yaml 中的 execution.checkpointing.interval 属性。
文件合并 # file sink 支持文件合并，允许应用程序使用较小的 checkpoint 间隔而不产生大量小文件。
键 默认值 类型 描述 auto-compaction false Boolean 在流式 sink 中是否开启自动合并功能。数据首先会被写入临时文件。当 checkpoint 完成后，该检查点产生的临时文件会被合并。这些临时文件在合并前不可见。 compaction.file-size (无) MemorySize 合并目标文件大小，默认值为滚动文件大小。 如果启用文件合并功能，会根据目标文件大小，将多个小文件合并成大文件。 在生产环境中使用文件合并功能时，需要注意：
只有 checkpoint 内部的文件才会被合并，至少生成的文件个数与 checkpoint 个数相同。 合并前文件是不可见的，那么文件的可见时间是：checkpoint 间隔时长 + 合并时长。 如果合并时间过长，将导致反压，延长 checkpoint 所需时间。 分区提交 # 数据写入分区之后，通常需要通知下游应用。例如，在 hive metadata 中新增分区或者在目录下生成 _SUCCESS 文件。分区提交策略是可定制的。具体分区提交行为是基于 triggers 和 policies 的组合。
Trigger：分区提交时机，可以基于从分区中提取的时间对应的 watermark，或者基于处理时间。 Policy：分区提交策略，内置策略包括生成 _SUCCESS 文件和提交 hive metastore，也可以实现自定义策略，例如触发 hive 生成统计信息，合并小文件等。 注意： 分区提交仅在动态分区插入模式下才有效。
分区提交触发器 # 通过配置分区提交触发策略，来决定何时提交分区：
键 默认值 类型 描述 sink.partition-commit.trigger process-time String 分区提交触发器类型： 'process-time'：基于机器时间，既不需要分区时间提取器也不需要 watermark 生成器。一旦 "当前系统时间" 超过了 "分区创建系统时间" 和 'sink.partition-commit.delay' 之和立即提交分区。 'partition-time'：基于提取的分区时间，需要 watermark 生成。一旦 watermark 超过了 "分区创建系统时间" 和 'sink.partition-commit.delay' 之和立即提交分区。 sink.partition-commit.delay 0 s Duration 该延迟时间之前分区不会被提交。如果是按天分区，可以设置为 '1 d'，如果是按小时分区，应设置为 '1 h'。 sink.partition-commit.watermark-time-zone UTC String 解析 Long 类型的 watermark 到 TIMESTAMP 类型时所采用的时区，解析得到的 watermark 的 TIMESTAMP 会被用来跟分区时间进行比较以判断是否该被提交。这个属性仅当 \`sink.partition-commit.trigger\` 被设置为 'partition-time' 时有效。如果这个属性设置的不正确，例如，在 TIMESTAMP_LTZ 类型的列上定义了 source rowtime，如果没有设置该属性，那么用户可能会在若干个小时后才看到分区的提交。默认值为 'UTC'，意味着 watermark 是定义在 TIMESTAMP 类型的列上或者没有定义 watermark。如果 watermark 定义在 TIMESTAMP_LTZ 类型的列上，watermark 时区必须是会话时区（session time zone）。该属性的可选值要么是完整的时区名比如 'America/Los_Angeles'，要么是自定义时区，例如 'GMT-08:00'。 Flink 提供了两种类型分区提交触发器：
第一种是根据分区的处理时间。既不需要额外的分区时间，也不需要 watermark 生成。这种分区提交触发器基于分区创建时间和当前系统时间。 这种触发器更具通用性，但不是很精确。例如，数据延迟或故障将导致过早提交分区。 第二种是根据从分区字段提取的时间以及 watermark。 这需要 job 支持 watermark 生成，分区是根据时间来切割的，例如，按小时或按天分区。 不管分区数据是否完整而只想让下游尽快感知到分区：
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;process-time\u0026rsquo; (默认值) \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;0s\u0026rsquo; (默认值) 一旦数据进入分区，将立即提交分区。注意：这个分区可能会被提交多次。 如果想让下游只有在分区数据完整时才感知到分区，并且 job 中有 watermark 生成，也能从分区字段的值中提取到时间：
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;partition-time\u0026rsquo; \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;1h\u0026rsquo; (根据分区类型指定，如果是按小时分区可配置为 \u0026lsquo;1h\u0026rsquo;) 该方式是最精准地提交分区的方式，尽力确保提交分区的数据完整。 如果想让下游系统只有在数据完整时才感知到分区，但是没有 watermark，或者无法从分区字段的值中提取时间：
\u0026lsquo;sink.partition-commit.trigger\u0026rsquo;=\u0026lsquo;process-time\u0026rsquo; (默认值) \u0026lsquo;sink.partition-commit.delay\u0026rsquo;=\u0026lsquo;1h\u0026rsquo; (根据分区类型指定，如果是按小时分区可配置为 \u0026lsquo;1h\u0026rsquo;) 该方式尽量精确地提交分区，但是数据延迟或者故障将导致过早提交分区。 延迟数据的处理：延迟的记录会被写入到已经提交的对应分区中，且会再次触发该分区的提交。
分区时间提取器 # 时间提取器从分区字段值中提取时间。
键 默认值 类型 描述 partition.time-extractor.kind default String 从分区字段中提取时间的时间提取器。支持 default 和 custom。默认情况下，可以配置 timestamp pattern/formatter。对于 custom，应指定提取器类。 partition.time-extractor.class (无) String 实现 PartitionTimeExtractor 接口的提取器类。 partition.time-extractor.timestamp-pattern (无) String 允许用户使用分区字段来获取合法的 timestamp pattern 的默认 construction 方式。默认支持第一个字段按 'yyyy-MM-dd hh:mm:ss' 这种模式提取。 如果需要从一个分区字段 'dt' 提取 timestamp，可以配置成：'\$dt'。 如果需要从多个分区字段，比如 'year'、'month'、'day' 和 'hour' 提取 timestamp，可以配置成: '\$year-\$month-\$day \$hour:00:00'。 如果需要从两个分区字段 'dt' 和 'hour' 提取 timestamp，可以配置成：'\$dt \$hour:00:00'。 partition.time-extractor.timestamp-formatter yyyy-MM-dd\u0026nbsp;HH:mm:ss String 转换分区 timestamp 字符串值为 timestamp 的 formatter，分区 timestamp 字符串值通过 'partition.time-extractor.timestamp-pattern' 属性表达。例如，分区 timestamp 提取来自多个分区字段，比如 'year'、'month' 和 'day'，可以配置 'partition.time-extractor.timestamp-pattern' 属性为 '\$year\$month\$day'，并且配置 \`partition.time-extractor.timestamp-formatter\` 属性为 'yyyyMMdd'。默认的 formatter 是 'yyyy-MM-dd HH:mm:ss'。 这的 timestamp-formatter 和 Java 的 DateTimeFormatter 是通用的。 默认情况下，提取器基于由分区字段组成的 timestamp pattern。也可以指定一个实现接口 PartitionTimeExtractor 的自定义提取器。
public class HourPartTimeExtractor implements PartitionTimeExtractor { @Override public LocalDateTime extract(List\u0026lt;String\u0026gt; keys, List\u0026lt;String\u0026gt; values) { String dt = values.get(0); String hour = values.get(1); return Timestamp.valueOf(dt + \u0026#34; \u0026#34; + hour + \u0026#34;:00:00\u0026#34;).toLocalDateTime(); } } 分区提交策略 # 分区提交策略定义了提交分区时的具体操作。
第一种是 metadata 存储（metastore），仅 hive 表支持该策略，该策略下文件系统通过目录层次结构来管理分区。 第二种是 success 文件，该策略下会在分区对应的目录下生成一个名为 _SUCCESS 的空文件。 键 默认值 类型 描述 sink.partition-commit.policy.kind (无) String 分区提交策略通知下游某个分区已经写完毕可以被读取了。 metastore：向 metadata 增加分区。仅 hive 支持 metastore 策略，文件系统通过目录结构管理分区； success-file：在目录中增加 '_success' 文件； 上述两个策略可以同时指定：'metastore,success-file'。 custom：通过指定的类来创建提交策略。 支持同时指定多个提交策略：'metastore,success-file'。 sink.partition-commit.policy.class (无) String 实现 PartitionCommitPolicy 接口的分区提交策略类。只有在 custom 提交策略下才使用该类。 sink.partition-commit.success-file.name _SUCCESS String 使用 success-file 分区提交策略时的文件名，默认值是 '_SUCCESS'。 也可以自定义提交策略，例如：
public class AnalysisCommitPolicy implements PartitionCommitPolicy { private HiveShell hiveShell; @Override public void commit(Context context) throws Exception { if (hiveShell == null) { hiveShell = createHiveShell(context.catalogName()); } hiveShell.execute(String.format( \u0026#34;ALTER TABLE %s ADD IF NOT EXISTS PARTITION (%s = \u0026#39;%s\u0026#39;) location \u0026#39;%s\u0026#39;\u0026#34;, context.tableName(), context.partitionKeys().get(0), context.partitionValues().get(0), context.partitionPath())); hiveShell.execute(String.format( \u0026#34;ANALYZE TABLE %s PARTITION (%s = \u0026#39;%s\u0026#39;) COMPUTE STATISTICS FOR COLUMNS\u0026#34;, context.tableName(), context.partitionKeys().get(0), context.partitionValues().get(0))); } } Sink Parallelism # 在流模式和批模式下，向外部文件系统（包括 hive）写文件时的 parallelism 可以通过相应的 table 配置项指定。默认情况下，该 sink parallelism 与上游 chained operator 的 parallelism 一样。当配置了跟上游的 chained operator 不一样的 parallelism 时，写文件和合并文件的算子（如果开启的话）会使用指定的 sink parallelism。
键 默认值 类型 描述 sink.parallelism (无) Integer 将文件写入外部文件系统的 parallelism。这个值应该大于0否则抛异常。 注意： 目前，当且仅当上游的 changelog 模式为 INSERT-ONLY 时，才支持配置 sink parallelism。否则，程序将会抛出异常。
完整示例 # 以下示例展示了如何使用文件系统连接器编写流式查询语句，将数据从 Kafka 写入文件系统，然后运行批式查询语句读取数据。
CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, \`hour\` STRING ) PARTITIONED BY (dt, \`hour\`) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;format\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;success-file\u0026#39; ); -- 流式 sql，插入文件系统表 INSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(log_ts, \u0026#39;HH\u0026#39;) FROM kafka_table; -- 批式 sql，使用分区修剪进行选择 SELECT * FROM fs_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and \`hour\`=\u0026#39;12\u0026#39;; 如果 watermark 被定义在 TIMESTAMP_LTZ 类型的列上并且使用 partition-time 模式进行提交，sink.partition-commit.watermark-time-zone 这个属性需要设置成会话时区，否则分区提交可能会延迟若干个小时。
CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, ts BIGINT, -- 以毫秒为单位的时间 ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;5\u0026#39; SECOND -- 在 TIMESTAMP_LTZ 列上定义 watermark ) WITH (...); CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, \`hour\` STRING ) PARTITIONED BY (dt, \`hour\`) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;format\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;partition.time-extractor.timestamp-pattern\u0026#39;=\u0026#39;\$dt \$hour:00:00\u0026#39;, \u0026#39;sink.partition-commit.delay\u0026#39;=\u0026#39;1 h\u0026#39;, \u0026#39;sink.partition-commit.trigger\u0026#39;=\u0026#39;partition-time\u0026#39;, \u0026#39;sink.partition-commit.watermark-time-zone\u0026#39;=\u0026#39;Asia/Shanghai\u0026#39;, -- 假设用户配置的时区为 \u0026#39;Asia/Shanghai\u0026#39; \u0026#39;sink.partition-commit.policy.kind\u0026#39;=\u0026#39;success-file\u0026#39; ); -- 流式 sql，插入文件系统表 INSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(ts_ltz, \u0026#39;yyyy-MM-dd\u0026#39;), DATE_FORMAT(ts_ltz, \u0026#39;HH\u0026#39;) FROM kafka_table; -- 批式 sql，使用分区修剪进行选择 SELECT * FROM fs_table WHERE dt=\u0026#39;2020-05-20\u0026#39; and \`hour\`=\u0026#39;12\u0026#39;; Back to top
`}),e.add({id:203,href:"/flink/flink-docs-master/zh/docs/ops/state/checkpointing_under_backpressure/",title:"Checkpointing under backpressure",section:"状态与容错",content:` Checkpointing under backpressure # 通常情况下，对齐 Checkpoint 的时长主要受 Checkpointing 过程中的同步和异步两个部分的影响。 然而，当 Flink 作业正运行在严重的背压下时，Checkpoint 端到端延迟的主要影响因子将会是传递 Checkpoint Barrier 到 所有的算子/子任务的时间。这在 checkpointing process) 的概述中有说明原因。并且可以通过高 alignment time and start delay metrics 观察到。 当这种情况发生并成为一个问题时，有三种方法可以解决这个问题：
消除背压源头，通过优化 Flink 作业，通过调整 Flink 或 JVM 参数，抑或是通过扩容。 减少 Flink 作业中缓冲在 In-flight 数据的数据量。 启用非对齐 Checkpoints。 这些选项并不是互斥的，可以组合在一起。本文档重点介绍后两个选项。 缓冲区 Debloating # Flink 1.14 引入了一个新的工具，用于自动控制在 Flink 算子/子任务之间缓冲的 In-flight 数据的数据量。缓冲区 Debloating 机 制可以通过将属性taskmanager.network.memory.buffer-debloat.enabled设置为true来启用。
此特性对对齐和非对齐 Checkpoint 都生效，并且在这两种情况下都能缩短 Checkpointing 的时间，不过 Debloating 的效果对于 对齐 Checkpoint 最明显。 当在非对齐 Checkpoint 情况下使用缓冲区 Debloating 时，额外的好处是 Checkpoint 大小会更小，并且恢复时间更快 (需要保存 和恢复的 In-flight 数据更少)。
有关缓冲区 Debloating 功能如何工作以及如何配置的更多信息，可以参考 network memory tuning guide。 请注意，您仍然可以继续使用在前面调优指南中介绍过的方式来手动减少缓冲在 In-flight 数据的数据量。
非对齐 Checkpoints # 从Flink 1.11开始，Checkpoint 可以是非对齐的。 Unaligned checkpoints 包含 In-flight 数据(例如，存储在缓冲区中的数据)作为 Checkpoint State的一部分，允许 Checkpoint Barrier 跨越这些缓冲区。因此， Checkpoint 时长变得与当前吞吐量无关，因为 Checkpoint Barrier 实际上已经不再嵌入到数据流当中。
如果您的 Checkpointing 由于背压导致周期非常的长，您应该使用非对齐 Checkpoint。这样，Checkpointing 时间基本上就与 端到端延迟无关。请注意，非对齐 Checkpointing 会增加状态存储的 I/O，因此当状态存储的 I/O 是 整个 Checkpointing 过程当中真 正的瓶颈时，您不应当使用非对齐 Checkpointing。
为了启用非对齐 Checkpoint，您可以：
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 启用非对齐 Checkpoint env.getCheckpointConfig().enableUnalignedCheckpoints(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() // 启用非对齐 Checkpoint env.getCheckpointConfig.enableUnalignedCheckpoints() Python env = StreamExecutionEnvironment.get_execution_environment() # 启用非对齐 Checkpoint env.get_checkpoint_config().enable_unaligned_checkpoints() 或者在 flink-conf.yml 配置文件中增加配置：
execution.checkpointing.unaligned: true 对齐 Checkpoint 的超时 # 在启用非对齐 Checkpoint 后，你依然可以通过编程的方式指定对齐 Checkpoint 的超时：
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getCheckpointConfig().setAlignedCheckpointTimeout(Duration.ofSeconds(30)); 或是在 flink-conf.yml 配置文件中配置：
execution.checkpointing.aligned-checkpoint-timeout: 30 s 在启动时，每个 Checkpoint 仍然是 aligned checkpoint，但是当全局 Checkpoint 持续时间超过 aligned-checkpoint-timeout 时， 如果 aligned checkpoint 还没完成，那么 Checkpoint 将会转换为 Unaligned Checkpoint。
限制 # 并发 Checkpoint # Flink 当前并不支持并发的非对齐 Checkpoint。然而，由于更可预测的和更短的 Checkpointing 时长，可能也根本就不需要并发的 Checkpoint。此外，Savepoint 也不能与非对齐 Checkpoint 同时发生，因此它们将会花费稍长的时间。
与 Watermark 的相互影响 # 非对齐 Checkpoint 在恢复的过程中改变了关于 Watermark 的一个隐式保证。目前，Flink 确保了 Watermark 作为恢复的第一步， 而不是将最近的 Watermark 存放在 Operator 中，以方便扩缩容。在非对齐 Checkpoint 中，这意味着当恢复时，Flink 会在恢复 In-flight 数据后再生成 Watermark。如果您的 Pipeline 中使用了对每条记录都应用最新的 Watermark 的算子将会相对于 使用对齐 Checkpoint产生不同的结果。如果您的 Operator 依赖于最新的 Watermark 始终可用，解决办法是将 Watermark 存放在 OperatorState 中。在这种情况下，Watermark 应该使用单键 group 存放在 UnionState 以方便扩缩容。
Interplay with long-running record processing # Despite that unaligned checkpoints barriers are able to overtake all other records in the queue. The handling of this barrier still can be delayed if the current record takes a lot of time to be processed. This situation can occur when firing many timers all at once, for example in windowed operations. Second problematic scenario might occur when system is being blocked waiting for more than one network buffer availability when processing a single input record. Flink can not interrupt processing of a single input record, and unaligned checkpoints have to wait for the currently processed record to be fully processed. This can cause problems in two scenarios. Either as a result of serialisation of a large record that doesn\u0026rsquo;t fit into single network buffer or in a flatMap operation, that produces many output records for one input record. In such scenarios back pressure can block unaligned checkpoints until all the network buffers required to process the single input record are available. It also can happen in any other situation when the processing of the single record takes a while. As result, the time of the checkpoint can be higher than expected or it can vary.
Certain data distribution patterns are not checkpointed # 有一部分包含属性的的连接无法与 Channel 中的数据一样保存在 Checkpoint 中。为了保留这些特性并且确保没有状态冲突或 非预期的行为，非对齐 Checkpoint 对于这些类型的连接是禁用的。所有其他的交换仍然执行非对齐 Checkpoint。
点对点连接
我们目前没有任何对于点对点连接中有关数据有序性的强保证。然而，由于数据已经被以前置的 Source 或是 KeyBy 相同的方式隐式 组织，一些用户会依靠这种特性在提供的有序性保证的同时将计算敏感型的任务划分为更小的块。
只要并行度不变，非对齐 Checkpoint(UC) 将会保留这些特性。但是如果加上UC的伸缩容，这些特性将会被改变。
针对如下任务
如果我们想将并行度从 p=2 扩容到 p=3，那么需要根据 KeyGroup 将 KeyBy 的 Channel 中的数据突然的划分到3个 Channel 中去。这 很容易做到，通过使用 Operator 的 KeyGroup 范围和确定记录属于某个 Key(group) 的方法(不管实际使用的是什么方法)。对于 Forward 的 Channel，我们根本没有 KeyContext。Forward Channel 里也没有任何记录被分配了任何 KeyGroup；也无法计算它，因为无法保证 Key仍然存在。
广播 Connections
广播 Connection 带来了另一个问题。无法保证所有 Channel 中的记录都以相同的速率被消费。这可能导致某些 Task 已经应用了与 特定广播事件对应的状态变更，而其他任务则没有，如图所示。
广播分区通常用于实现广播状态，它应该跨所有 Operator 都相同。Flink 实现广播状态，通过仅 Checkpointing 有状态算子的 SubTask 0 中状态的单份副本。在恢复时，我们将该份副本发往所有的 Operator。因此，可能会发生以下情况：某个算子将很快从它的 Checkpointed Channel 消费数据并将修改应有于记录来获得状态。
Troubleshooting # Corrupted in-flight data # 以下描述的操作是最后采取的手段，因为它们将会导致数据的丢失。 为了防止 In-flight 数据损坏，或者由于其他原因导致作业应该在没有 In-flight 数据的情况下恢复，可以使用 recover-without-channel-state.checkpoint-id 属性。该属性需要指定一个 Checkpoint Id，对它来说 In-flight 中的数据将会被忽略。除非已经持久化的 In-flight 数据内部的损坏导致无 法恢复的情况，否则不要设置该属性。只有在重新部署作业后该属性才会生效，这就意味着只有启用 externalized checkpoint
时，此操作才有意义。
`}),e.add({id:204,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/explain/",title:"EXPLAIN 语句",section:"SQL",content:" EXPLAIN 语句 # EXPLAIN 语句用于解释 query 或 INSERT 语句的执行逻辑，也用于优化 query 语句的查询计划。\n执行 EXPLAIN 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，executeSql() 方法会返回解释结果，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。\nScala 可以使用 TableEnvironment 的 executeSql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，executeSql() 方法会返回解释结果，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。\nPython 可以使用 TableEnvironment 的 execute_sql() 方法执行 EXPLAIN 语句。如果 EXPLAIN 操作执行成功，execute_sql() 方法会返回解释结果，否则会抛出异常。\n以下示例展示了如何在 TableEnvironment 中执行一条 EXPLAIN 语句。\nSQL CLI EXPLAIN 语句可以在 SQL CLI 中执行。\n以下示例展示了如何在 SQL CLI 中执行一条 EXPLAIN 语句。\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 注册名为 “Orders” 的表 tEnv.executeSql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;); tEnv.executeSql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;); // 调用 TableEnvironment.explainSql() 来解释 SELECT 语句 String explanation = tEnv.explainSql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); System.out.println(explanation); // 调用 TableEnvironment.executeSql() 来解释 SELECT 语句 TableResult tableResult = tEnv.executeSql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); tableResult.print(); TableResult tableResult2 = tEnv.executeSql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;); tableResult2.print(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // 注册名为 “Orders” 的表 tEnv.executeSql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) tEnv.executeSql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) // 调用 TableEnvironment.explainSql() 来解释 SELECT 语句 val explanation = tEnv.explainSql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) println(explanation) // 调用 TableEnvironment.executeSql() 来解释 SELECT 语句 val tableResult = tEnv.executeSql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) tableResult.print() val tableResult2 = tEnv.executeSql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; + \u0026#34;UNION ALL \u0026#34; + \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) tableResult2.print() Python settings = EnvironmentSettings.new_instance()... table_env = StreamTableEnvironment.create(env, settings) t_env.execute_sql(\u0026#34;CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) t_env.execute_sql(\u0026#34;CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;)\u0026#34;) # 调用 TableEnvironment.explain_sql() 来解释 SELECT 语句 explanation1 = t_env.explain_sql( \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) print(explanation1) # 调用 TableEnvironment.execute_sql() 来解释 SELECT 语句 table_result = t_env.execute_sql( \u0026#34;EXPLAIN PLAN FOR \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) table_result.print() table_result2 = t_env.execute_sql( \u0026#34;EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026#34; \u0026#34;UNION ALL \u0026#34; \u0026#34;SELECT `count`, word FROM MyTable2\u0026#34;) table_result2.print() SQL CLI Flink SQL\u0026gt; CREATE TABLE MyTable1 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;); [INFO] Table has been created. Flink SQL\u0026gt; CREATE TABLE MyTable2 (`count` bigint, word VARCHAR(256)) WITH (\u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;); [INFO] Table has been created. Flink SQL\u0026gt; EXPLAIN PLAN FOR SELECT `count`, word FROM MyTable1 WHERE word LIKE \u0026#39;F%\u0026#39; \u0026gt; UNION ALL \u0026gt; SELECT `count`, word FROM MyTable2; Flink SQL\u0026gt; EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, JSON_EXECUTION_PLAN SELECT `count`, word FROM MyTable1 \u0026gt; WHERE word LIKE \u0026#39;F%\u0026#39; \u0026gt; UNION ALL \u0026gt; SELECT `count`, word FROM MyTable2; EXPLAIN 的结果如下:\nEXPLAIN PLAN == Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalProject(count=[$0], word=[$1]) : +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]]) +- LogicalProject(count=[$0], word=[$1]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) EXPLAIN PLAN WITH DETAILS == Abstract Syntax Tree == LogicalUnion(all=[true]) :- LogicalProject(count=[$0], word=[$1]) : +- LogicalFilter(condition=[LIKE($1, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1]]) +- LogicalProject(count=[$0], word=[$1]) +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) == Optimized Physical Plan == Union(all=[true], union=[count, word], changelogMode=[I]): rowcount = 1.05E8, cumulative cost = {3.1E8 rows, 3.05E8 cpu, 4.0E9 io, 0.0 network, 0.0 memory} :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)], changelogMode=[I]): rowcount = 5000000.0, cumulative cost = {1.05E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word], changelogMode=[I]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word], changelogMode=[I]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 2.0E9 io, 0.0 network, 0.0 memory} == Optimized Execution Plan == Union(all=[true], union=[count, word]) :- Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)]) : +- TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word]) +- TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word]) == Physical Execution Plan == { \u0026#34;nodes\u0026#34; : [ { \u0026#34;id\u0026#34; : 37, \u0026#34;type\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Data Source\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable1]], fields=[count, word])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1 }, { \u0026#34;id\u0026#34; : 38, \u0026#34;type\u0026#34; : \u0026#34;Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Operator\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Calc(select=[count, word], where=[LIKE(word, _UTF-16LE\u0026#39;F%\u0026#39;)])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1, \u0026#34;predecessors\u0026#34; : [ { \u0026#34;id\u0026#34; : 37, \u0026#34;ship_strategy\u0026#34; : \u0026#34;FORWARD\u0026#34;, \u0026#34;side\u0026#34; : \u0026#34;second\u0026#34; } ] }, { \u0026#34;id\u0026#34; : 39, \u0026#34;type\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])\u0026#34;, \u0026#34;pact\u0026#34; : \u0026#34;Data Source\u0026#34;, \u0026#34;contents\u0026#34; : \u0026#34;Source: TableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[count, word])\u0026#34;, \u0026#34;parallelism\u0026#34; : 1 } ] Back to top\nExplainDetails # 使用指定的 explainDetail 类型来打印语句的计划。 ESTIMATED_COST：生成优化器（optimizer）估算的物理节点相关的成本信息, 例如：TableSourceScan(..., cumulative cost ={1.0E8 rows, 1.0E8 cpu, 2.4E9 io, 0.0 network, 0.0 memory}) CHANGELOG_MODE：为每个物理 RelNode 生成 changelog mode。 例如：GroupAggregate(..., changelogMode=[I,UA,D]) JSON_EXECUTION_PLAN：生成 json 格式的程序执行计划。 语法 # EXPLAIN [([ExplainDetail[, ExplainDetail]*]) | PLAN FOR] \u0026lt;query_statement_or_insert_statement_or_statement_set\u0026gt; statement_set: EXECUTE STATEMENT SET BEGIN insert_statement; ... insert_statement; END; 关于 query 的语法，请查阅 Queries 页面。 关于 INSERT 的语法，请查阅 INSERT 页面。\n"}),e.add({id:205,href:"/flink/flink-docs-master/zh/docs/flinkdev/",title:"Flink 开发",section:"Docs",content:""}),e.add({id:206,href:"/flink/flink-docs-master/zh/docs/connectors/table/hbase/",title:"HBase",section:"Table API Connectors",content:` HBase SQL 连接器 # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Upsert Mode
HBase 连接器支持读取和写入 HBase 集群。本文档介绍如何使用 HBase 连接器基于 HBase 进行 SQL 查询。
HBase 连接器在 upsert 模式下运行，可以使用 DDL 中定义的主键与外部系统交换更新操作消息。但是主键只能基于 HBase 的 rowkey 字段定义。如果没有声明主键，HBase 连接器默认取 rowkey 作为主键。
依赖 # In order to use the HBase connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
HBase version Maven dependency SQL Client JAR 1.4.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-hbase-1.4\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 2.2.x \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-hbase-2.2\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. HBase 连接器不是二进制发行版的一部分，请查阅这里了解如何在集群运行中引用 HBase 连接器。
如何使用 HBase 表 # 所有 HBase 表的列簇必须定义为 ROW 类型，字段名对应列簇名（column family），嵌套的字段名对应列限定符名（column qualifier）。用户只需在表结构中声明查询中使用的的列簇和列限定符。除了 ROW 类型的列，剩下的原子数据类型字段（比如，STRING, BIGINT）将被识别为 HBase 的 rowkey，一张表中只能声明一个 rowkey。rowkey 字段的名字可以是任意的，如果是保留关键字，需要用反引号。
-- 在 Flink SQL 中注册 HBase 表 \u0026#34;mytable\u0026#34; CREATE TABLE hTable ( rowkey INT, family1 ROW\u0026lt;q1 INT\u0026gt;, family2 ROW\u0026lt;q2 STRING, q3 BIGINT\u0026gt;, family3 ROW\u0026lt;q4 DOUBLE, q5 BOOLEAN, q6 STRING\u0026gt;, PRIMARY KEY (rowkey) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;hbase-1.4\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;mytable\u0026#39;, \u0026#39;zookeeper.quorum\u0026#39; = \u0026#39;localhost:2181\u0026#39; ); -- 用 ROW(...) 构造函数构造列簇，并往 HBase 表写数据。 -- 假设 \u0026#34;T\u0026#34; 的表结构是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6] INSERT INTO hTable SELECT rowkey, ROW(f1q1), ROW(f2q2, f2q3), ROW(f3q4, f3q5, f3q6) FROM T; -- 从 HBase 表扫描数据 SELECT rowkey, family1, family3.q4, family3.q6 FROM hTable; -- temporal join HBase 表，将 HBase 表作为维表 SELECT * FROM myTopic LEFT JOIN hTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = hTable.rowkey; 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必选 (none) String 指定使用的连接器, 支持的值如下 : hbase-1.4: 连接 HBase 1.4.x 集群 hbase-2.2: 连接 HBase 2.2.x 集群 table-name 必选 (none) String 连接的 HBase 表名。默认该表在 "default" 命名空间下，指定命名空间下的表需要使用 "namespace:table"。 zookeeper.quorum 必选 (none) String HBase Zookeeper quorum 信息。 zookeeper.znode.parent 可选 /hbase String HBase 集群的 Zookeeper 根目录。 null-string-literal 可选 null String 当字符串值为 null 时的存储形式，默认存成 "null" 字符串。HBase 的 source 和 sink 的编解码将所有数据类型（除字符串外）将 null 值以空字节来存储。 sink.buffer-flush.max-size 可选 2mb MemorySize 写入的参数选项。每次写入请求缓存行的最大大小。它能提升写入 HBase 数据库的性能，但是也可能增加延迟。设置为 "0" 关闭此选项。 sink.buffer-flush.max-rows 可选 1000 Integer 写入的参数选项。 每次写入请求缓存的最大行数。它能提升写入 HBase 数据库的性能，但是也可能增加延迟。设置为 "0" 关闭此选项。 sink.buffer-flush.interval 可选 1s Duration 写入的参数选项。刷写缓存行的间隔。它能提升写入 HBase 数据库的性能，但是也可能增加延迟。设置为 "0" 关闭此选项。注意："sink.buffer-flush.max-size" 和 "sink.buffer-flush.max-rows" 同时设置为 "0"，刷写选项整个异步处理缓存行为。 sink.parallelism 可选 (none) Integer 为 HBase sink operator 定义并行度。默认情况下，并行度由框架决定，和链在一起的上游 operator 一样。 lookup.async 可选 false Boolean 是否启用异步查找。如果为真，查找将是异步的。注意：异步方式只支持 hbase-2.2 连接器 lookup.cache.max-rows 可选 -1 Long 查找缓存的最大行数，超过这个值，最旧的行将过期。注意："lookup.cache.max-rows" 和 "lookup.cache.ttl" 必须同时被设置。默认情况下，查找缓存是禁用的。 lookup.cache.ttl 可选 0 s Duration 查找缓存中每一行的最大生存时间，在这段时间内，最老的行将过期。注意："lookup.cache.max-rows" 和 "lookup.cache.ttl" 必须同时被设置。默认情况下，查找缓存是禁用的。 lookup.max-retries 可选 3 Integer 查找数据库失败时的最大重试次数。 properties.* 可选 (无) String 可以设置任意 HBase 的配置项。后缀名必须匹配在 HBase 配置文档 中定义的配置键。Flink 将移除 "properties." 配置键前缀并将变换后的配置键和值传入底层的 HBase 客户端。 例如您可以设置 'properties.hbase.security.authentication' = 'kerberos' 等kerberos认证参数。 数据类型映射表 # HBase 以字节数组存储所有数据。在读和写过程中要序列化和反序列化数据。
Flink 的 HBase 连接器利用 HBase（Hadoop) 的工具类 org.apache.hadoop.hbase.util.Bytes 进行字节数组和 Flink 数据类型转换。
Flink 的 HBase 连接器将所有数据类型（除字符串外）null 值编码成空字节。对于字符串类型，null 值的字面值由null-string-literal选项值决定。
数据类型映射表如下：
Flink 数据类型 HBase 转换 CHAR / VARCHAR / STRING byte[] toBytes(String s) String toString(byte[] b) BOOLEAN byte[] toBytes(boolean b) boolean toBoolean(byte[] b) BINARY / VARBINARY 返回 byte[]。 DECIMAL byte[] toBytes(BigDecimal v) BigDecimal toBigDecimal(byte[] b) TINYINT new byte[] { val } bytes[0] // returns first and only byte from bytes SMALLINT byte[] toBytes(short val) short toShort(byte[] bytes) INT byte[] toBytes(int val) int toInt(byte[] bytes) BIGINT byte[] toBytes(long val) long toLong(byte[] bytes) FLOAT byte[] toBytes(float val) float toFloat(byte[] bytes) DOUBLE byte[] toBytes(double val) double toDouble(byte[] bytes) DATE 从 1970-01-01 00:00:00 UTC 开始的天数，int 值。 TIME 从 1970-01-01 00:00:00 UTC 开始天的毫秒数，int 值。 TIMESTAMP 从 1970-01-01 00:00:00 UTC 开始的毫秒数，long 值。 ARRAY 不支持 MAP / MULTISET 不支持 ROW 不支持 Back to top
`}),e.add({id:207,href:"/flink/flink-docs-master/zh/docs/ops/monitoring/",title:"Monitoring",section:"Operations",content:""}),e.add({id:208,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/over-agg/",title:"Over聚合",section:"Queries 查询",content:` Over Aggregation # Batch Streaming
OVER aggregates compute an aggregated value for every input row over a range of ordered rows. In contrast to GROUP BY aggregates, OVER aggregates do not reduce the number of result rows to a single row for every group. Instead OVER aggregates produce an aggregated value for every input row.
The following query computes for every order the sum of amounts of all orders for the same product that were received within one hour before the current order.
SELECT order_id, order_time, amount, SUM(amount) OVER ( PARTITION BY product ORDER BY order_time RANGE BETWEEN INTERVAL \u0026#39;1\u0026#39; HOUR PRECEDING AND CURRENT ROW ) AS one_hour_prod_amount_sum FROM Orders The syntax for an OVER window is summarized below.
SELECT agg_func(agg_col) OVER ( [PARTITION BY col1[, col2, ...]] ORDER BY time_col range_definition), ... FROM ... You can define multiple OVER window aggregates in a SELECT clause. However, for streaming queries, the OVER windows for all aggregates must be identical due to current limitation.
ORDER BY # OVER windows are defined on an ordered sequence of rows. Since tables do not have an inherent order, the ORDER BY clause is mandatory. For streaming queries, Flink currently only supports OVER windows that are defined with an ascending time attributes order. Additional orderings are not supported.
PARTITION BY # OVER windows can be defined on a partitioned table. In presence of a PARTITION BY clause, the aggregate is computed for each input row only over the rows of its partition.
Range Definitions # The range definition specifies how many rows are included in the aggregate. The range is defined with a BETWEEN clause that defines a lower and an upper boundary. All rows between these boundaries are included in the aggregate. Flink only supports CURRENT ROW as the upper boundary.
There are two options to define the range, ROWS intervals and RANGE intervals.
RANGE intervals # A RANGE interval is defined on the values of the ORDER BY column, which is in case of Flink always a time attribute. The following RANGE interval defines that all rows with a time attribute of at most 30 minutes less than the current row are included in the aggregate.
RANGE BETWEEN INTERVAL \u0026#39;30\u0026#39; MINUTE PRECEDING AND CURRENT ROW ROW intervals # A ROWS interval is a count-based interval. It defines exactly how many rows are included in the aggregate. The following ROWS interval defines that the 10 rows preceding the current row and the current row (so 11 rows in total) are included in the aggregate.
ROWS BETWEEN 10 PRECEDING AND CURRENT ROW WINDOW The WINDOW clause can be used to define an OVER window outside of the SELECT clause. It can make queries more readable and also allows us to reuse the window definition for multiple aggregates.
SELECT order_id, order_time, amount, SUM(amount) OVER w AS sum_amount, AVG(amount) OVER w AS avg_amount FROM Orders WINDOW w AS ( PARTITION BY product ORDER BY order_time RANGE BETWEEN INTERVAL \u0026#39;1\u0026#39; HOUR PRECEDING AND CURRENT ROW) Back to top
`}),e.add({id:209,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/parquet/",title:"Parquet",section:"Formats",content:` Parquet 格式 # Format: Serialization Schema Format: Deserialization Schema
Apache Parquet 格式允许读写 Parquet 数据.
依赖 # In order to use the Parquet format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-parquet\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 如何创建基于 Parquet 格式的表 # 以下为用 Filesystem 连接器和 Parquet 格式创建表的示例，
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3), dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/tmp/user_behavior\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;parquet\u0026#39; ) Format 参数 # 参数 是否必须 默认值 类型 描述 format 必选 (none) String 指定使用的格式，此处应为"parquet"。 parquet.utc-timezone 可选 false Boolean 使用 UTC 时区或本地时区在纪元时间和 LocalDateTime 之间进行转换。Hive 0.x/1.x/2.x 使用本地时区，但 Hive 3.x 使用 UTC 时区。 Parquet 格式也支持 ParquetOutputFormat 的配置。 例如, 可以配置 parquet.compression=GZIP 来开启 gzip 压缩。
数据类型映射 # 目前，Parquet 格式类型映射与 Apache Hive 兼容，但与 Apache Spark 有所不同：
Timestamp：不论精度，映射 timestamp 类型至 int96。 Decimal：根据精度，映射 decimal 类型至固定长度字节的数组。 下表列举了 Flink 中的数据类型与 JSON 中的数据类型的映射关系。
Flink 数据类型 Parquet 类型 Parquet 逻辑类型 CHAR / VARCHAR / STRING BINARY UTF8 BOOLEAN BOOLEAN BINARY / VARBINARY BINARY DECIMAL FIXED_LEN_BYTE_ARRAY DECIMAL TINYINT INT32 INT_8 SMALLINT INT32 INT_16 INT INT32 BIGINT INT64 FLOAT FLOAT DOUBLE DOUBLE DATE INT32 DATE TIME INT32 TIME_MILLIS TIMESTAMP INT96 ARRAY LIST MAP MAP ROW STRUCT 注意 复合数据类型暂只支持写不支持读（Array、Map 与 Row）。
`}),e.add({id:210,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/pulsar/",title:"Pulsar",section:"DataStream Connectors",content:` Apache Pulsar 连接器 # Flink 当前提供 Apache Pulsar Source 和 Sink 连接器，用户可以使用它从 Pulsar 读取数据，并保证每条数据只被处理一次。
添加依赖 # Pulsar Source 当前支持 Pulsar 2.8.1 之后的版本，但是 Pulsar Source 使用到了 Pulsar 的事务机制，建议在 Pulsar 2.9.2 及其之后的版本上使用 Pulsar Source 进行数据读取。
如果想要了解更多关于 Pulsar API 兼容性设计，可以阅读文档 PIP-72。
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-pulsar\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 为了在 PyFlink 作业中使用 Pulsar connector ，需要添加下列依赖： PyFlink JAR Only available for stable releases. 在 PyFlink 中如何添加 JAR 包依赖参见 Python 依赖管理。 Flink 的流连接器并不会放到发行文件里面一同发布，阅读此文档，了解如何将连接器添加到集群实例内。
Pulsar Source # Pulsar Source 基于 Flink 最新的批流一体 API 进行开发。 使用示例 # Pulsar Source 提供了 builder 类来构造 PulsarSource 实例。下面的代码实例使用 builder 类创建的实例会从 “persistent://public/default/my-topic” 的数据开始端进行消费。对应的 Pulsar Source 使用了 Exclusive（独占）的订阅方式消费消息，订阅名称为 my-subscription，并把消息体的二进制字节流以 UTF-8 的方式编码为字符串。
Java PulsarSource\u0026lt;String\u0026gt; source = PulsarSource.builder() .setServiceUrl(serviceUrl) .setAdminUrl(adminUrl) .setStartCursor(StartCursor.earliest()) .setTopics(\u0026#34;my-topic\u0026#34;) .setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema())) .setSubscriptionName(\u0026#34;my-subscription\u0026#34;) .setSubscriptionType(SubscriptionType.Exclusive) .build(); env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Pulsar Source\u0026#34;); Python pulsar_source = PulsarSource.builder() \\ .set_service_url(\u0026#39;pulsar://localhost:6650\u0026#39;) \\ .set_admin_url(\u0026#39;http://localhost:8080\u0026#39;) \\ .set_start_cursor(StartCursor.earliest()) \\ .set_topics(\u0026#34;my-topic\u0026#34;) \\ .set_deserialization_schema( PulsarDeserializationSchema.flink_schema(SimpleStringSchema())) \\ .set_subscription_name(\u0026#39;my-subscription\u0026#39;) \\ .set_subscription_type(SubscriptionType.Exclusive) \\ .build() env.from_source(source=pulsar_source, watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;pulsar source\u0026#34;) 如果使用构造类构造 PulsarSource，一定要提供下面几个属性：
Pulsar 数据消费的地址，使用 setServiceUrl(String) 方法提供。 Pulsar HTTP 管理地址，使用 setAdminUrl(String) 方法提供。 Pulsar 订阅名称，使用 setSubscriptionName(String) 方法提供。 需要消费的 Topic 或者是 Topic 下面的分区，详见指定消费的 Topic 或者 Topic 分区。 解码 Pulsar 消息的反序列化器，详见反序列化器。 指定消费的 Topic 或者 Topic 分区 # Pulsar Source 提供了两种订阅 Topic 或 Topic 分区的方式。
Topic 列表，从这个 Topic 的所有分区上消费消息，例如： Java PulsarSource.builder().setTopics(\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;); // 从 topic \u0026#34;topic-a\u0026#34; 的 0 和 2 分区上消费 PulsarSource.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;); Python PulsarSource.builder().set_topics([\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;]) # 从 topic \u0026#34;topic-a\u0026#34; 的 0 和 2 分区上消费 PulsarSource.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;]) Topic 正则，Pulsar Source 使用给定的正则表达式匹配出所有合规的 Topic，例如： Java PulsarSource.builder().setTopicPattern(\u0026#34;topic-*\u0026#34;); Python PulsarSource.builder().set_topic_pattern(\u0026#34;topic-*\u0026#34;) Topic 名称简写 # 从 Pulsar 2.0 之后，完整的 Topic 名称格式为 {persistent|non-persistent}://租户/命名空间/topic。但是 Pulsar Source 不需要提供 Topic 名称的完整定义，因为 Topic 类型、租户、命名空间都设置了默认值。
Topic 属性 默认值 Topic 类型 persistent 租户 public 命名空间 default 下面的表格提供了当前 Pulsar Topic 支持的简写方式：
Topic 名称简写 翻译后的 Topic 名称 my-topic persistent://public/default/my-topic my-tenant/my-namespace/my-topic persistent://my-tenant/my-namespace/my-topic 对于 Non-persistent（非持久化）Topic，Pulsar Source 不支持简写名称。所以无法将 non-persistent://public/default/my-topic 简写成 non-persistent://my-topic。 Pulsar Topic 层次结构 # 对于 Pulsar 而言，Topic 分区也是一种 Topic。Pulsar 会将一个有分区的 Topic 在内部按照分区的大小拆分成等量的无分区 Topic。
由于 Pulsar 内部的分区实际实现为一个 Topic，我们将用“分区”来指代“仅有一个分区的 Topic（Non-partitioned Topic）”和“具有多个分区的 Topic 下属的分区”。
例如，在 Pulsar 的 sample 租户下面的 flink 命名空间里面创建了一个有 3 个分区的 Topic，给它起名为 simple-string。可以在 Pulsar 上看到如下的 Topic 列表：
Topic 名称 是否分区 persistent://sample/flink/simple-string 是 persistent://sample/flink/simple-string-partition-0 否 persistent://sample/flink/simple-string-partition-1 否 persistent://sample/flink/simple-string-partition-2 否 这意味着，用户可以用上面的子 Topic 去直接消费分区里面的数据，不需要再去基于上层的父 Topic 去消费全部分区的数据。例如：使用 PulsarSource.builder().setTopics(\u0026quot;sample/flink/simple-string-partition-1\u0026quot;, \u0026quot;sample/flink/simple-string-partition-2\u0026quot;) 将会只消费 Topic sample/flink/simple-string 分区 1 和 2 里面的消息。
配置 Topic 正则表达式 # 前面提到了 Pulsar Topic 有 persistent、non-persistent 两种类型，使用正则表达式消费数据的时候，Pulsar Source 会尝试从正则表达式里面解析出消息的类型。例如：PulsarSource.builder().setTopicPattern(\u0026quot;non-persistent://my-topic*\u0026quot;) 会解析出 non-persistent 这个 Topic 类型。如果用户使用 Topic 名称简写的方式，Pulsar Source 会使用默认的消息类型 persistent。
如果想用正则去消费 persistent 和 non-persistent 类型的 Topic，需要使用 RegexSubscriptionMode 定义 Topic 类型，例如：setTopicPattern(\u0026quot;topic-*\u0026quot;, RegexSubscriptionMode.AllTopics)。
反序列化器 # 反序列化器用于解析 Pulsar 消息，Pulsar Source 使用 PulsarDeserializationSchema 来定义反序列化器。用户可以在 builder 类中使用 setDeserializationSchema(PulsarDeserializationSchema) 方法配置反序列化器。
如果用户只关心消息体的二进制字节流，并不需要其他属性来解析数据。可以直接使用预定义的 PulsarDeserializationSchema。Pulsar Source里面提供了 3 种预定义的反序列化器。
使用 Pulsar 的 Schema 解析消息。
// 基础数据类型 PulsarDeserializationSchema.pulsarSchema(Schema); // 结构类型 (JSON, Protobuf, Avro, etc.) PulsarDeserializationSchema.pulsarSchema(Schema, Class); // 键值对类型 PulsarDeserializationSchema.pulsarSchema(Schema, Class, Class); 使用 Flink 的 DeserializationSchema 解析消息。 Java PulsarDeserializationSchema.flinkSchema(DeserializationSchema); Python PulsarDeserializationSchema.flink_schema(DeserializationSchema) 使用 Flink 的 TypeInformation 解析消息。 Java PulsarDeserializationSchema.flinkTypeInfo(TypeInformation, ExecutionConfig); Python PulsarDeserializationSchema.flink_type_info(TypeInformation) Pulsar 的 Message\u0026lt;byte[]\u0026gt; 包含了很多 额外的属性。例如，消息的 key、消息发送时间、消息生产时间、用户在消息上自定义的键值对属性等。可以使用 Message\u0026lt;byte[]\u0026gt; 接口来获取这些属性。
如果用户需要基于这些额外的属性来解析一条消息，可以实现 PulsarDeserializationSchema 接口。并一定要确保 PulsarDeserializationSchema.getProducedType() 方法返回的 TypeInformation 是正确的结果。Flink 使用 TypeInformation 将解析出来的结果序列化传递到下游算子。
Pulsar 订阅 # 订阅是命名好的配置规则，指导消息如何投递给消费者。Pulsar Source 需要提供一个独立的订阅名称,支持 Pulsar 的四种订阅模式：
exclusive（独占） shared（共享） failover（灾备） key_shared（key 共享） 当前 Pulsar Source 里，独占 和 灾备 的实现没有区别，如果 Flink 的一个 reader 挂了，Pulsar Source 会把所有未消费的数据交给其他的 reader 来消费数据。
默认情况下，如果没有指定订阅类型，Pulsar Source 使用共享订阅类型（SubscriptionType.Shared）。
Java // 名为 \u0026#34;my-shared\u0026#34; 的共享订阅 PulsarSource.builder().setSubscriptionName(\u0026#34;my-shared\u0026#34;); // 名为 \u0026#34;my-exclusive\u0026#34; 的独占订阅 PulsarSource.builder().setSubscriptionName(\u0026#34;my-exclusive\u0026#34;).setSubscriptionType(SubscriptionType.Exclusive); Python # 名为 \u0026#34;my-shared\u0026#34; 的共享订阅 PulsarSource.builder().set_subscription_name(\u0026#34;my-shared\u0026#34;) # 名为 \u0026#34;my-exclusive\u0026#34; 的独占订阅 PulsarSource.builder().set_subscription_name(\u0026#34;my-exclusive\u0026#34;).set_subscription_type(SubscriptionType.Exclusive) 如果想在 Pulsar Source 里面使用 key 共享 订阅，需要提供 RangeGenerator 实例。RangeGenerator 会生成一组消息 key 的 hash 范围，Pulsar Source 会基于给定的范围来消费数据。
Pulsar Source 也提供了一个名为 UniformRangeGenerator 的默认实现，它会基于 flink 数据源的并行度将 hash 范围均分。
起始消费位置 # Pulsar Source 使用 setStartCursor(StartCursor) 方法给定开始消费的位置。内置的开始消费位置有：
从 Topic 里面最早的一条消息开始消费。 Java StartCursor.earliest(); Python StartCursor.earliest() 从 Topic 里面最新的一条消息开始消费。 Java StartCursor.latest(); Python StartCursor.latest() 从给定的消息开始消费。 Java StartCursor.fromMessageId(MessageId); Python StartCursor.from_message_id(message_id) 与前者不同的是，给定的消息可以跳过，再进行消费。 Java StartCursor.fromMessageId(MessageId, boolean); Python StartCursor.from_message_id(message_id, boolean) 从给定的消息发布时间开始消费，这个方法因为名称容易导致误解现在已经不建议使用。你可以使用方法 StartCursor.fromPublishTime(long)。 Java StartCursor.fromMessageTime(long); Python StartCursor.from_message_time(int) 从给定的消息发布时间开始消费。 Java StartCursor.fromPublishTime(long); Python StartCursor.from_publish_time(int) 每条消息都有一个固定的序列号，这个序列号在 Pulsar 上有序排列，其包含了 ledger、entry、partition 等原始信息，用于在 Pulsar 底层存储上查找到具体的消息。
Pulsar 称这个序列号为 MessageId，用户可以使用 DefaultImplementation.newMessageId(long ledgerId, long entryId, int partitionIndex) 创建它。
边界 # Pulsar Source 默认情况下使用流的方式消费数据。除非任务失败或者被取消，否则将持续消费数据。用户可以使用 setBoundedStopCursor(StopCursor) 给定停止消费的位置，这种情况下会使用批的方式进行消费。使用流的方式一样可以给定停止位置，使用 setUnboundedStopCursor(StopCursor) 方法即可。
在批模式下，使用 setBoundedStopCursor(StopCursor) 来指定一个消费停止位置。
内置的停止消费位置如下：
永不停止。 Java StopCursor.never(); Python StopCursor.never() 停止于 Pulsar 启动时 Topic 里面最新的那条数据。 Java StopCursor.latest(); Python StopCursor.latest() 停止于某条消息，结果里不包含此消息。 Java StopCursor.atMessageId(MessageId); Python StopCursor.at_message_id(message_id) 停止于某条消息之后，结果里包含此消息。 Java StopCursor.afterMessageId(MessageId); Python StopCursor.after_message_id(message_id) 停止于某个给定的消息事件时间戳，比如 Message\u0026lt;byte[]\u0026gt;.getEventTime()，消费结果里不包含此时间戳的消息。 Java StopCursor.atEventTime(long); Python StopCursor.at_event_time(int) 停止于某个给定的消息事件时间戳，比如 Message\u0026lt;byte[]\u0026gt;.getEventTime()，消费结果里包含此时间戳的消息。 Java StopCursor.afterEventTime(long); Python StopCursor.after_event_time(int) 停止于某个给定的消息发布时间戳，比如 Message\u0026lt;byte[]\u0026gt;.getPublishTime()，消费结果里不包含此时间戳的消息。 Java StopCursor.atPublishTime(long); Python StopCursor.at_publish_time(int) 停止于某个给定的消息发布时间戳，比如 Message\u0026lt;byte[]\u0026gt;.getPublishTime()，消费结果里包含此时间戳的消息。 Java StopCursor.afterPublishTime(long); Python StopCursor.after_publish_time(int) Source 配置项 # 除了前面提到的配置选项，Pulsar Source 还提供了丰富的选项供 Pulsar 专家使用，在 builder 类里通过 setConfig(ConfigOption\u0026lt;T\u0026gt;, T) 和 setConfig(Configuration) 方法给定下述的全部配置。
Pulsar Java 客户端配置项 # Pulsar Source 使用 Java 客户端来创建消费实例，相关的配置定义于 Pulsar 的 ClientConfigurationData 内。在 PulsarOptions 选项中，定义大部分的可供用户定义的配置。
Key Default Type Description pulsar.client.authParamMap (none) Map Parameters for the authentication plugin. pulsar.client.authParams (none) String Parameters for the authentication plugin.
Example:
key1:val1,key2:val2 pulsar.client.authPluginClassName (none) String Name of the authentication plugin. pulsar.client.concurrentLookupRequest 5000 Integer The number of concurrent lookup requests allowed to send on each broker connection to prevent overload on the broker. It should be configured with a higher value only in case of it requires to produce or subscribe on thousands of topic using a created PulsarClient pulsar.client.connectionTimeoutMs 10000 Integer Duration (in ms) of waiting for a connection to a broker to be established.
If the duration passes without a response from a broker, the connection attempt is dropped. pulsar.client.connectionsPerBroker 1 Integer The maximum number of connections that the client library will open to a single broker.
By default, the connection pool will use a single connection for all the producers and consumers. Increasing this parameter may improve throughput when using many producers over a high latency connection. pulsar.client.enableBusyWait false Boolean Option to enable busy-wait settings.
This option will enable spin-waiting on executors and IO threads in order to reduce latency during context switches. The spinning will consume 100% CPU even when the broker is not doing any work. It is recommended to reduce the number of IO threads and BookKeeper client threads to only have fewer CPU cores busy. pulsar.client.enableTransaction false Boolean If transaction is enabled, start the transactionCoordinatorClient with PulsarClient. pulsar.client.initialBackoffIntervalNanos 100000000 Long Default duration (in nanoseconds) for a backoff interval. pulsar.client.keepAliveIntervalSeconds 30 Integer Interval (in seconds) for keeping connection between the Pulsar client and broker alive. pulsar.client.listenerName (none) String Configure the listenerName that the broker will return the corresponding advertisedListener. pulsar.client.maxBackoffIntervalNanos 60000000000 Long The maximum duration (in nanoseconds) for a backoff interval. pulsar.client.maxLookupRedirects 20 Integer The maximum number of times a lookup-request redirections to a broker. pulsar.client.maxLookupRequest 50000 Integer The maximum number of lookup requests allowed on each broker connection to prevent overload on the broker. It should be greater than pulsar.client.concurrentLookupRequest. Requests that inside pulsar.client.concurrentLookupRequest are already sent to broker, and requests beyond pulsar.client.concurrentLookupRequest and under maxLookupRequests will wait in each client cnx. pulsar.client.maxNumberOfRejectedRequestPerConnection 50 Integer The maximum number of rejected requests of a broker in a certain period (30s) after the current connection is closed and the client creates a new connection to connect to a different broker. pulsar.client.memoryLimitBytes 67108864 Long The limit (in bytes) on the amount of direct memory that will be allocated by this client instance.
Note: at this moment this is only limiting the memory for producers. Setting this to 0 will disable the limit. pulsar.client.numIoThreads 1 Integer The number of threads used for handling connections to brokers. pulsar.client.numListenerThreads 1 Integer The number of threads used for handling message listeners. The listener thread pool is shared across all the consumers and readers that are using a listener model to get messages. For a given consumer, the listener is always invoked from the same thread to ensure ordering. pulsar.client.operationTimeoutMs 30000 Integer Operation timeout (in ms). Operations such as creating producers, subscribing or unsubscribing topics are retried during this interval. If the operation is not completed during this interval, the operation will be marked as failed. pulsar.client.proxyProtocol SNI Enum
Protocol type to determine the type of proxy routing when a client connects to the proxy using pulsar.client.proxyServiceUrl.
Possible values:"SNI" pulsar.client.proxyServiceUrl (none) String Proxy-service URL when a client connects to the broker via the proxy. The client can choose the type of proxy-routing. pulsar.client.requestTimeoutMs 60000 Integer Maximum duration (in ms) for completing a request. This config option is not supported before Pulsar 2.8.1 pulsar.client.serviceUrl (none) String Service URL provider for Pulsar service.
To connect to Pulsar using client libraries, you need to specify a Pulsar protocol URL.
You can assign Pulsar protocol URLs to specific clusters and use the Pulsar scheme.
This is an example of localhost: pulsar://localhost:6650.If you have multiple brokers, the URL is as: pulsar://localhost:6550,localhost:6651,localhost:6652A URL for a production Pulsar cluster is as: pulsar://pulsar.us-west.example.com:6650If you use TLS authentication, the URL is as pulsar+ssl://pulsar.us-west.example.com:6651 pulsar.client.sslProvider (none) String The name of the security provider used for SSL connections. The default value is the default security provider of the JVM. pulsar.client.statsIntervalSeconds 60 Long Interval between each stats info.
Stats is activated with positive statsIntervalSet statsIntervalSeconds to 1 second at least. pulsar.client.tlsAllowInsecureConnection false Boolean Whether the Pulsar client accepts untrusted TLS certificate from the broker. pulsar.client.tlsCiphers List\u0026lt;String\u0026gt; A list of cipher suites. This is a named combination of authentication, encryption, MAC and the key exchange algorithm used to negotiate the security settings for a network connection using the TLS or SSL network protocol. By default all the available cipher suites are supported. pulsar.client.tlsHostnameVerificationEnable false Boolean Whether to enable TLS hostname verification. It allows to validate hostname verification when a client connects to the broker over TLS. It validates incoming x509 certificate and matches provided hostname (CN/SAN) with the expected broker's host name. It follows RFC 2818, 3.1. Server Identity hostname verification. pulsar.client.tlsProtocols List\u0026lt;String\u0026gt; The SSL protocol used to generate the SSLContext. By default, it is set TLS, which is fine for most cases. Allowed values in recent JVMs are TLS, TLSv1.3, TLSv1.2 and TLSv1.1. pulsar.client.tlsTrustCertsFilePath (none) String Path to the trusted TLS certificate file. pulsar.client.tlsTrustStorePassword (none) String The store password for the key store file. pulsar.client.tlsTrustStorePath (none) String The location of the trust store file. pulsar.client.tlsTrustStoreType "JKS" String The file format of the trust store file. pulsar.client.useKeyStoreTls false Boolean If TLS is enabled, whether use the KeyStore type as the TLS configuration parameter. If it is set to false, it means to use the default pem type configuration. pulsar.client.useTcpNoDelay true Boolean Whether to use the TCP no-delay flag on the connection to disable Nagle algorithm.
No-delay features ensures that packets are sent out on the network as soon as possible, and it is critical to achieve low latency publishes. On the other hand, sending out a huge number of small packets might limit the overall throughput. Therefore, if latency is not a concern, it is recommended to set this option to false.
By default, it is set to true. Pulsar 管理 API 配置项 # 管理 API 用于查询 Topic 的元数据和用正则订阅的时候的 Topic 查找，它与 Java 客户端共享大部分配置。下面列举的配置只供管理 API 使用，PulsarOptions 包含了这些配置 。
Key Default Type Description pulsar.admin.adminUrl (none) String The Pulsar service HTTP URL for the admin endpoint. For example, http://my-broker.example.com:8080, or https://my-broker.example.com:8443 for TLS. pulsar.admin.autoCertRefreshTime 300000 Integer The auto cert refresh time (in ms) if Pulsar admin supports TLS authentication. pulsar.admin.connectTimeout 60000 Integer The connection time out (in ms) for the PulsarAdmin client. pulsar.admin.readTimeout 60000 Integer The server response read timeout (in ms) for the PulsarAdmin client for any request. pulsar.admin.requestTimeout 300000 Integer The server request timeout (in ms) for the PulsarAdmin client for any request. Pulsar 消费者 API 配置项 # Pulsar 提供了消费者 API 和读者 API 两套 API 来进行数据消费，它们可用于不同的业务场景。Flink 上的 Pulsar Source 使用消费者 API 进行消费，它的配置定义于 Pulsar 的 ConsumerConfigurationData 内。Pulsar Source 将其中大部分的可供用户定义的配置定义于 PulsarSourceOptions 内。
Key Default Type Description pulsar.consumer.ackReceiptEnabled false Boolean Acknowledgement will return a receipt but this does not mean that the message will not be resent after getting the receipt. pulsar.consumer.ackTimeoutMillis 0 Long The timeout (in ms) for unacknowledged messages, truncated to the nearest millisecond. The timeout needs to be greater than 1 second.
By default, the acknowledge timeout is disabled and that means that messages delivered to a consumer will not be re-delivered unless the consumer crashes.
When acknowledgement timeout being enabled, if a message is not acknowledged within the specified timeout it will be re-delivered to the consumer (possibly to a different consumer in case of a shared subscription). pulsar.consumer.acknowledgementsGroupTimeMicros 100000 Long Group a consumer acknowledgment for a specified time (in μs). By default, a consumer uses 100μs grouping time to send out acknowledgments to a broker. If the group time is set to 0, acknowledgments are sent out immediately. A longer ack group time is more efficient at the expense of a slight increase in message re-deliveries after a failure. pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull false Boolean Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once a consumer reaches this threshold, it drops the outstanding unchunked-messages by silently acknowledging if pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull is true. Otherwise, it marks them for redelivery. pulsar.consumer.autoUpdatePartitionsIntervalSeconds 60 Integer The interval (in seconds) of updating partitions. This only works if autoUpdatePartitions is enabled. pulsar.consumer.consumerName (none) String The consumer name is informative and it can be used to identify a particular consumer instance from the topic stats. pulsar.consumer.cryptoFailureAction FAIL Enum
The consumer should take action when it receives a message that can not be decrypted.FAIL: this is the default option to fail messages until crypto succeeds.DISCARD: silently acknowledge but do not deliver messages to an application.CONSUME: deliver encrypted messages to applications. It is the application's responsibility to decrypt the message.
Fail to decompress the messages.
If messages contain batch messages, a client is not be able to retrieve individual messages in batch.
The delivered encrypted message contains EncryptionContext which contains encryption and compression information in. You can use an application to decrypt the consumed message payload.
Possible values:"FAIL""DISCARD""CONSUME" pulsar.consumer.deadLetterPolicy.deadLetterTopic (none) String Name of the dead topic where the failed messages are sent. pulsar.consumer.deadLetterPolicy.maxRedeliverCount 0 Integer The maximum number of times that a message are redelivered before being sent to the dead letter queue. pulsar.consumer.deadLetterPolicy.retryLetterTopic (none) String Name of the retry topic where the failed messages are sent. pulsar.consumer.expireTimeOfIncompleteChunkedMessageMillis 60000 Long If a producer fails to publish all the chunks of a message, the consumer can expire incomplete chunks if the consumer cannot receive all chunks in expire times (default 1 hour, in ms). pulsar.consumer.maxPendingChunkedMessage 10 Integer The consumer buffers chunk messages into memory until it receives all the chunks of the original message. While consuming chunk-messages, chunks from the same message might not be contiguous in the stream and they might be mixed with other messages' chunks. So, consumer has to maintain multiple buffers to manage chunks coming from different messages. This mainly happens when multiple publishers are publishing messages on the topic concurrently or publishers failed to publish all chunks of the messages.
For example, there are M1-C1, M2-C1, M1-C2, M2-C2 messages.Messages M1-C1 and M1-C2 belong to the M1 original message while M2-C1 and M2-C2 belong to the M2 message.
Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once, a consumer reaches this threshold, it drops the outstanding unchunked messages by silently acknowledging or asking the broker to redeliver messages later by marking it unacknowledged. This behavior can be controlled by the pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull option. pulsar.consumer.maxTotalReceiverQueueSizeAcrossPartitions 50000 Integer The maximum total receiver queue size across partitions.
This setting reduces the receiver queue size for individual partitions if the total receiver queue size exceeds this value. pulsar.consumer.negativeAckRedeliveryDelayMicros 60000000 Long Delay (in μs) to wait before redelivering messages that failed to be processed.
When an application uses Consumer.negativeAcknowledge(Message), failed messages are redelivered after a fixed timeout. pulsar.consumer.poolMessages false Boolean Enable pooling of messages and the underlying data buffers. pulsar.consumer.priorityLevel 0 Integer Priority level for a consumer to which a broker gives more priorities while dispatching messages in the shared subscription type.
The broker follows descending priorities. For example, 0=max-priority, 1, 2,...
In shared subscription mode, the broker first dispatches messages to the consumers on the highest priority level if they have permits. Otherwise, the broker considers consumers on the next priority level.
Example 1
If a subscription has consumer A with priorityLevel 0 and consumer B with priorityLevel 1, then the broker only dispatches messages to consumer A until it runs out permits and then starts dispatching messages to consumer B.
Example 2
Consumer Priority, Level, Permits C1, 0, 2 C2, 0, 1 C3, 0, 1 C4, 1, 2 C5, 1, 1 The order in which a broker dispatches messages to consumers is: C1, C2, C3, C1, C4, C5, C4. pulsar.consumer.properties Map A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification. pulsar.consumer.readCompacted false Boolean If enabling readCompacted, a consumer reads messages from a compacted topic rather than reading a full message backlog of a topic.
A consumer only sees the latest value for each key in the compacted topic, up until reaching the point in the topic message when compacting backlog. Beyond that point, send messages as normal.
Only enabling readCompacted on subscriptions to persistent topics, which have a single active consumer (like failure or exclusive subscriptions).
Attempting to enable it on subscriptions to non-persistent topics or on shared subscriptions leads to a subscription call throwing a PulsarClientException. pulsar.consumer.receiverQueueSize 1000 Integer Size of a consumer's receiver queue.
For example, the number of messages accumulated by a consumer before an application calls Receive.
A value higher than the default value increases consumer throughput, though at the expense of more memory utilization. pulsar.consumer.replicateSubscriptionState false Boolean If replicateSubscriptionState is enabled, a subscription state is replicated to geo-replicated clusters. pulsar.consumer.retryEnable false Boolean If enabled, the consumer will automatically retry messages. pulsar.consumer.subscriptionMode Durable Enum
Select the subscription mode to be used when subscribing to the topic.Durable: Make the subscription to be backed by a durable cursor that will retain messages and persist the current position.NonDurable: Lightweight subscription mode that doesn't have a durable cursor associated
Possible values:"Durable""NonDurable" pulsar.consumer.subscriptionName (none) String Specify the subscription name for this consumer. This argument is required when constructing the consumer. pulsar.consumer.subscriptionType Shared Enum
Subscription type.
Four subscription types are available:ExclusiveFailoverSharedKey_Shared
Possible values:"Exclusive""Shared""Failover""Key_Shared" pulsar.consumer.tickDurationMillis 1000 Long Granularity (in ms) of the ack-timeout redelivery.
A greater (for example, 1 hour) tickDurationMillis reduces the memory overhead to track messages. Pulsar Source配置项 # 下述配置主要用于性能调优或者是控制消息确认的行为。如非必要，可以不用强制配置。
Key Default Type Description pulsar.source.autoCommitCursorInterval 5000 Long This option is used only when the user disables the checkpoint and uses Exclusive or Failover subscription. We would automatically commit the cursor using the given period (in ms). pulsar.source.enableAutoAcknowledgeMessage false Boolean Flink commits the consuming position with pulsar transactions on checkpoint. However, if you have disabled the Flink checkpoint or disabled transaction for your Pulsar cluster, ensure that you have set this option to true.
The source would use pulsar client's internal mechanism and commit cursor in two ways.For Key_Shared and Shared subscription, the cursor would be committed once the message is consumed.For Exclusive and Failover subscription, the cursor would be committed in a given interval. pulsar.source.maxFetchRecords 100 Integer The maximum number of records to fetch to wait when polling. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchTime. pulsar.source.maxFetchTime 10000 Long The maximum time (in ms) to wait when fetching records. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchRecords. pulsar.source.partitionDiscoveryIntervalMs 30000 Long The interval (in ms) for the Pulsar source to discover the new partitions. A non-positive value disables the partition discovery. pulsar.source.transactionTimeoutMillis 10800000 Long This option is used in Shared or Key_Shared subscription. You should configure this option when you do not enable the pulsar.source.enableAutoAcknowledgeMessage option.
The value (in ms) should be greater than the checkpoint interval. pulsar.source.verifyInitialOffsets WARN_ON_MISMATCH Enum
Upon (re)starting the source, check whether the expected message can be read. If failure is enabled, the application fails. Otherwise, it logs a warning. A possible solution is to adjust the retention settings in Pulsar or ignoring the check result.
Possible values:"FAIL_ON_MISMATCH": Fail the consuming from Pulsar when we don't find the related cursor."WARN_ON_MISMATCH": Print a warn message and start consuming from the valid offset. 动态分区发现 # 为了能在启动 Flink 任务之后还能发现在 Pulsar 上扩容的分区或者是新创建的 Topic，Pulsar Source 提供了动态分区发现机制。该机制不需要重启 Flink 任务。对选项 PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS 设置一个正整数即可启用。
Java // 10 秒查询一次分区信息 PulsarSource.builder() .setConfig(PulsarSourceOptions.PULSAR_PARTITION_DISCOVERY_INTERVAL_MS, 10000); Python # 10 秒查询一次分区信息 PulsarSource.builder() .set_config(\u0026#34;pulsar.source.partitionDiscoveryIntervalMs\u0026#34;, 10000) 默认情况下，Pulsar 启用动态分区发现，查询间隔为 30 秒。用户可以给定一个负数，将该功能禁用。如果使用批的方式消费数据，将无法启用该功能。 事件时间和水位线 # 默认情况下，Pulsar Source 使用 Pulsar 的 Message\u0026lt;byte[]\u0026gt; 里面的时间作为解析结果的时间戳。用户可以使用 WatermarkStrategy 来自行解析出想要的消息时间，并向下游传递对应的水位线。
Java env.fromSource(pulsarSource, new CustomWatermarkStrategy(), \u0026#34;Pulsar Source With Custom Watermark Strategy\u0026#34;); Python env.from_source(pulsar_source, CustomWatermarkStrategy(), \u0026#34;Pulsar Source With Custom Watermark Strategy\u0026#34;) 这篇文档详细讲解了如何定义 WatermarkStrategy。
消息确认 # 一旦在 Topic 上创建了订阅，消息便会存储在 Pulsar 里。即使没有消费者，消息也不会被丢弃。只有当 Pulsar Source 同 Pulsar 确认此条消息已经被消费，该消息才以某种机制会被移除。Pulsar Source 支持四种订阅方式，它们的消息确认方式也大不相同。
独占和灾备订阅下的消息确认 # 独占 和 灾备 订阅下，Pulsar Source 使用累进式确认方式。确认某条消息已经被处理时，其前面消息会自动被置为已读。Pulsar Source 会在 Flink 完成检查点时将对应时刻消费的消息置为已读，以此来保证 Pulsar 状态与 Flink 状态一致。
如果用户没有在 Flink 上启用检查点，Pulsar Source 可以使用周期性提交来将消费状态提交给 Pulsar，使用配置 PulsarSourceOptions.PULSAR_AUTO_COMMIT_CURSOR_INTERVAL 来进行定义。
需要注意的是，此种场景下，Pulsar Source 并不依赖于提交到 Pulsar 的状态来做容错。消息确认只是为了能在 Pulsar 端看到对应的消费处理情况。
共享和 key 共享订阅下的消息确认 # 共享 和 key 共享 需要依次确认每一条消息，所以 Pulsar Source 在 Pulsar 事务里面进行消息确认，然后将事务提交到 Pulsar。
首先需要在 Pulsar 的 borker.conf 文件里面启用事务：
transactionCoordinatorEnabled=true Pulsar Source 创建的事务的默认超时时间为 3 小时，请确保这个时间大于 Flink 检查点的间隔。用户可以使用 PulsarSourceOptions.PULSAR_TRANSACTION_TIMEOUT_MILLIS 来设置事务的超时时间。
如果用户无法启用 Pulsar 的事务，或者是因为项目禁用了检查点，需要将 PulsarSourceOptions.PULSAR_ENABLE_AUTO_ACKNOWLEDGE_MESSAGE 选项设置为 true，消息从 Pulsar 消费后会被立刻置为已读。Pulsar Source 无法保证此种场景下的消息一致性。
Pulsar Source 在 Pulsar 上使用日志的形式记录某个事务下的消息确认，为了更好的性能，请缩短 Flink 做检查点的间隔。
Pulsar Sink # Pulsar Sink 连接器可以将经过 Flink 处理后的数据写入一个或多个 Pulsar Topic 或者 Topic 下的某些分区。
Pulsar Sink 基于 Flink 最新的 Sink API 实现。
如果想要使用旧版的使用 SinkFuntion 接口实现的 Sink 连接器，可以使用 StreamNative 维护的 pulsar-flink。
使用示例 # Pulsar Sink 使用 builder 类来创建 PulsarSink 实例。
下面示例展示了如何通过 Pulsar Sink 以“至少一次”的语义将字符串类型的数据发送给 topic1。
Java DataStream\u0026lt;String\u0026gt; stream = ... PulsarSink\u0026lt;String\u0026gt; sink = PulsarSink.builder() .setServiceUrl(serviceUrl) .setAdminUrl(adminUrl) .setTopics(\u0026#34;topic1\u0026#34;) .setSerializationSchema(PulsarSerializationSchema.flinkSchema(new SimpleStringSchema())) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build(); stream.sinkTo(sink); Python stream = ... pulsar_sink = PulsarSink.builder() \\ .set_service_url(\u0026#39;pulsar://localhost:6650\u0026#39;) \\ .set_admin_url(\u0026#39;http://localhost:8080\u0026#39;) \\ .set_topics(\u0026#34;topic1\u0026#34;) \\ .set_serialization_schema(PulsarSerializationSchema.flink_schema(SimpleStringSchema())) \\ .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\ .build() stream.sink_to(pulsar_sink) 下列为创建一个 PulsarSink 实例必需的属性：
Pulsar 数据消费的地址，使用 setServiceUrl(String) 方法提供。 Pulsar HTTP 管理地址，使用 setAdminUrl(String) 方法提供。 需要发送到的 Topic 或者是 Topic 下面的分区，详见指定写入的topic或者topic分区。 编码 Pulsar 消息的序列化器，详见序列化器。 在创建 PulsarSink 时，建议使用 setProducerName(String) 来指定 PulsarSink 内部使用的 Pulsar 生产者名称。这样方便在数据监控页面找到对应的生产者监控指标。
指定写入的 Topic 或者 Topic 分区 # PulsarSink 指定写入 Topic 的方式和 Pulsar Source 指定消费的 Topic 或者 Topic 分区的方式类似。PulsarSink 支持以 mixin 风格指定写入的 Topic 或分区。因此，可以指定一组 Topic 或者分区或者是两者都有。
Java // Topic \u0026#34;some-topic1\u0026#34; 和 \u0026#34;some-topic2\u0026#34; PulsarSink.builder().setTopics(\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;) // Topic \u0026#34;topic-a\u0026#34; 的分区 0 和 2 PulsarSink.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;) // Topic \u0026#34;topic-a\u0026#34; 以及 Topic \u0026#34;some-topic2\u0026#34; 分区 0 和 2 PulsarSink.builder().setTopics(\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;, \u0026#34;some-topic2\u0026#34;) Python # Topic \u0026#34;some-topic1\u0026#34; 和 \u0026#34;some-topic2\u0026#34; PulsarSink.builder().set_topics([\u0026#34;some-topic1\u0026#34;, \u0026#34;some-topic2\u0026#34;]) # Topic \u0026#34;topic-a\u0026#34; 的分区 0 和 2 PulsarSink.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;]) # Topic \u0026#34;topic-a\u0026#34; 以及 Topic \u0026#34;some-topic2\u0026#34; 分区 0 和 2 PulsarSink.builder().set_topics([\u0026#34;topic-a-partition-0\u0026#34;, \u0026#34;topic-a-partition-2\u0026#34;, \u0026#34;some-topic2\u0026#34;]) 动态分区发现默认处于开启状态，这意味着 PulsarSink 将会周期性地从 Pulsar 集群中查询 Topic 的元数据来获取可能有的分区数量变更信息。使用 PulsarSinkOptions.PULSAR_TOPIC_METADATA_REFRESH_INTERVAL 配置项来指定查询的间隔时间。
可以选择实现 TopicRouter 接口来自定义消息路由策略。此外，阅读 Topic 名称简写将有助于理解 Pulsar 的分区在 Pulsar 连接器中的配置方式。
如果在 PulsarSink 中同时指定了某个 Topic 和其下属的分区，那么 PulsarSink 将会自动将两者合并，仅使用外层的 Topic。
举个例子，如果通过 PulsarSink.builder().setTopics(\u0026quot;some-topic1\u0026quot;, \u0026quot;some-topic1-partition-0\u0026quot;) 来指定写入的 Topic，那么其结果等价于 PulsarSink.builder().setTopics(\u0026quot;some-topic1\u0026quot;)。
序列化器 # 序列化器（PulsarSerializationSchema）负责将 Flink 中的每条记录序列化成 byte 数组，并通过网络发送至指定的写入 Topic。和 Pulsar Source 类似的是，序列化器同时支持使用基于 Flink 的 SerializationSchema 接口实现序列化器和使用 Pulsar 原生的 Schema 类型实现的序列化器。不过序列化器并不支持 Pulsar 的 Schema.AUTO_PRODUCE_BYTES()。
如果不需要指定 Message 接口中提供的 key 或者其他的消息属性，可以从上述 2 种预定义的 PulsarSerializationSchema 实现中选择适合需求的一种使用。
使用 Pulsar 的 Schema 来序列化 Flink 中的数据。
// 原始数据类型 PulsarSerializationSchema.pulsarSchema(Schema) // 有结构数据类型（JSON、Protobuf、Avro 等） PulsarSerializationSchema.pulsarSchema(Schema, Class) // 键值对类型 PulsarSerializationSchema.pulsarSchema(Schema, Class, Class) 使用 Flink 的 SerializationSchema 来序列化数据。
Java PulsarSerializationSchema.flinkSchema(SerializationSchema) Python PulsarSerializationSchema.flink_schema(SimpleStringSchema()) 同时使用 PulsarSerializationSchema.pulsarSchema() 以及在 builder 中指定 PulsarSinkBuilder.enableSchemaEvolution() 可以启用 Schema evolution 特性。该特性会使用 Pulsar Broker 端提供的 Schema 版本兼容性检测以及 Schema 版本演进。下列示例展示了如何启用 Schema Evolution。
Schema\u0026lt;SomePojo\u0026gt; schema = Schema.AVRO(SomePojo.class); PulsarSerializationSchema\u0026lt;SomePojo\u0026gt; pulsarSchema = PulsarSerializationSchema.pulsarSchema(schema, SomePojo.class); PulsarSink\u0026lt;String\u0026gt; sink = PulsarSink.builder() ... .setSerializationSchema(pulsarSchema) .enableSchemaEvolution() .build(); 如果想要使用 Pulsar 原生的 Schema 序列化消息而不需要 Schema Evolution 特性，那么写入的 Topic 会使用 Schema.BYTES 作为消息的 Schema，对应 Topic 的消费者需要自己负责反序列化的工作。
例如，如果使用 PulsarSerializationSchema.pulsarSchema(Schema.STRING) 而不使用 PulsarSinkBuilder.enableSchemaEvolution()。那么在写入 Topic 中所记录的消息 Schema 将会是 Schema.BYTES。
消息路由策略 # 在 Pulsar Sink 中，消息路由发生在于分区之间，而非上层 Topic。对于给定 Topic 的情况，路由算法会首先会查询出 Topic 之上所有的分区信息，并在这些分区上实现消息的路由。Pulsar Sink 默认提供 2 种路由策略的实现。
KeyHashTopicRouter：使用消息的 key 对应的哈希值来取模计算出消息对应的 Topic 分区。
使用此路由可以将具有相同 key 的消息发送至同一个 Topic 分区。消息的 key 可以在自定义 PulsarSerializationSchema 时，在 serialize() 方法内使用 PulsarMessageBuilder.key(String key) 来予以指定。
如果消息没有包含 key，此路由策略将从 Topic 分区中随机选择一个发送。
可以使用 MessageKeyHash.JAVA_HASH 或者 MessageKeyHash.MURMUR3_32_HASH 两种不同的哈希算法来计算消息 key 的哈希值。使用 PulsarSinkOptions.PULSAR_MESSAGE_KEY_HASH 配置项来指定想要的哈希算法。
RoundRobinRouter：轮换使用用户给定的 Topic 分区。
消息将会轮替地选取 Topic 分区，当往某个 Topic 分区里写入指定数量的消息后，将会轮换至下一个 Topic 分区。使用 PulsarSinkOptions.PULSAR_BATCHING_MAX_MESSAGES 指定向一个 Topic 分区中写入的消息数量。
还可以通过实现 TopicRouter 接口来自定义消息路由策略，请注意 TopicRouter 的实现需要能被序列化。
在 TopicRouter 内可以指定任意的 Topic 分区（即使这个 Topic 分区不在 setTopics() 指定的列表中）。因此，当使用自定义的 TopicRouter 时，PulsarSinkBuilder.setTopics 选项是可选的。
@PublicEvolving public interface TopicRouter\u0026lt;IN\u0026gt; extends Serializable { String route(IN in, List\u0026lt;String\u0026gt; partitions, PulsarSinkContext context); default void open(SinkConfiguration sinkConfiguration) { // 默认无操作 } } 如前文所述，Pulsar 分区的内部被实现为一个无分区的 Topic，一般情况下 Pulsar 客户端会隐藏这个实现，并且提供内置的消息路由策略。Pulsar Sink 并没有使用 Pulsar 客户端提供的路由策略和封装，而是使用了 Pulsar 客户端更底层的 API 自行实现了消息路由逻辑。这样做的主要目的是能够在属于不同 Topic 的分区之间定义更灵活的消息路由策略。
详情请参考 Pulsar 的 partitioned topics。
发送一致性 # PulsarSink 支持三种发送一致性。
NONE：Flink 应用运行时可能出现数据丢失的情况。在这种模式下，Pulsar Sink 发送消息后并不会检查消息是否发送成功。此模式具有最高的吞吐量，可用于一致性没有要求的场景。 AT_LEAST_ONCE：每条消息至少有一条对应消息发送至 Pulsar，发送至 Pulsar 的消息可能会因为 Flink 应用重启而出现重复。 EXACTLY_ONCE：每条消息有且仅有一条对应消息发送至 Pulsar。发送至 Pulsar 的消息不会有重复也不会丢失。Pulsar Sink 内部依赖 Pulsar 事务和两阶段提交协议来保证每条记录都能正确发往 Pulsar。 消息延时发送 # 消息延时发送特性可以让指定发送的每一条消息需要延时一段时间后才能被下游的消费者所消费。当延时消息发送特性启用时，Pulsar Sink 会立刻将消息发送至 Pulsar Broker。但该消息在指定的延迟时间到达前将会保持对下游消费者不可见。
消息延时发送仅在 Shared 订阅模式下有效，在 Exclusive 和 Failover 模式下该特性无效。
可以使用 MessageDelayer.fixed(Duration) 创建一个 MessageDelayer 来为所有消息指定恒定的接收时延，或者实现 MessageDelayer 接口来为不同的消息指定不同的接收时延。
消息对下游消费者的可见时间应当基于 PulsarSinkContext.processTime() 计算得到。 Sink 配置项 # 可以在 builder 类里通过 setConfig(ConfigOption\u0026lt;T\u0026gt;, T) 和 setConfig(Configuration) 方法给定下述的全部配置。
PulsarClient 和 PulsarAdmin 配置项 # Pulsar Sink 和 Pulsar Source 公用的配置选项可参考
Pulsar Java 客户端配置项 Pulsar 管理 API 配置项 Pulsar 生产者 API 配置项 # Pulsar Sink 使用生产者 API 来发送消息。Pulsar 的 ProducerConfigurationData 中大部分的配置项被映射为 PulsarSinkOptions 里的选项。
Key Default Type Description pulsar.producer.batchingEnabled true Boolean Enable batch send ability, it was enabled by default. pulsar.producer.batchingMaxBytes 131072 Integer The maximum size of messages permitted in a batch. Keep the maximum consistent as previous versions. pulsar.producer.batchingMaxMessages 1000 Integer The maximum number of messages permitted in a batch. pulsar.producer.batchingMaxPublishDelayMicros 1000 Long Batching time period of sending messages. pulsar.producer.batchingPartitionSwitchFrequencyByPublishDelay 10 Integer The maximum wait time for switching topic partitions. pulsar.producer.chunkingEnabled false Boolean pulsar.producer.compressionType NONE Enum
Message data compression type used by a producer.Available options:https://github.com/lz4/lz4https://zlib.net/https://facebook.github.io/zstd/https://google.github.io/snappy/
Possible values:"NONE""LZ4""ZLIB""ZSTD""SNAPPY" pulsar.producer.initialSequenceId (none) Long The sequence id for avoiding the duplication, it's used when Pulsar doesn't have transaction. pulsar.producer.producerName (none) String A producer name which would be displayed in the Pulsar's dashboard. If no producer name was provided, we would use a Pulsar generated name instead. pulsar.producer.properties Map A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification. pulsar.producer.sendTimeoutMs 30000 Long Message send timeout in ms.If a message is not acknowledged by a server before the sendTimeout expires, an error occurs. Pulsar Sink 配置项 # 下述配置主要用于性能调优或者是控制消息确认的行为。如非必要，可以不用考虑配置。
Key Default Type Description pulsar.sink.deliveryGuarantee none Enum
Optional delivery guarantee when committing.
Possible values:"exactly-once": Records are only delivered exactly-once also under failover scenarios. To build a complete exactly-once pipeline is required that the source and sink support exactly-once and are properly configured."at-least-once": Records are ensured to be delivered but it may happen that the same record is delivered multiple times. Usually, this guarantee is faster than the exactly-once delivery."none": Records are delivered on a best effort basis. It is often the fastest way to process records but it may happen that records are lost or duplicated. pulsar.sink.enableSchemaEvolution false Boolean If you enable this option and use PulsarSerializationSchema.pulsarSchema(), we would consume and deserialize the message by using Pulsar's Schema. pulsar.sink.maxPendingMessages 1000 Integer The maximum number of pending messages in one sink parallelism. pulsar.sink.maxRecommitTimes 5 Integer The allowed transaction recommit times if we meet some retryable exception. This is used in Pulsar Transaction. pulsar.sink.messageKeyHash murmur-3-32-hash Enum
The hash policy for routing message by calculating the hash code of message key.
Possible values:"java-hash": This hash would use String.hashCode() to calculate the message key string's hash code."murmur-3-32-hash": This hash would calculate message key's hash code by using Murmur3 algorithm. pulsar.sink.topicMetadataRefreshInterval 1800000 Long Auto update the topic metadata in a fixed interval (in ms). The default value is 30 minutes. pulsar.sink.transactionTimeoutMillis 10800000 Long This option is used when the user require the DeliveryGuarantee.EXACTLY_ONCE semantic.We would use transaction for making sure the message could be write only once. Sink 监控指标 # 下列表格列出了当前 Sink 支持的监控指标，前 6 个指标是 FLIP-33: Standardize Connector Metrics 中规定的 Sink 连接器应当支持的标准指标。
Scope Metrics User Variables Description Type Operator numBytesOut n/a Pulsar Sink 启动后总共发出的字节数 Counter numBytesOutPerSecond n/a 每秒发送的字节数 Meter numRecordsOut n/a Pulsar Sink 启动后总共发出的消息数 Counter numRecordsOutPerSecond n/a 每秒发送的消息数 Meter numRecordsOutErrors n/a 总共发送消息失败的次数 Counter currentSendTime n/a 最近一条消息从被放入客户端缓冲队列到收到消息确认的时间 Gauge PulsarSink.numAcksReceived n/a 总共收到的确认数 Counter PulsarSink.sendLatencyMax n/a 所有生产者的最大发送延迟 Gauge PulsarSink.producer."ProducerName".sendLatency50Pct ProducerName 某个生产者在过去的一个窗口内的发送延迟的中位数 Gauge PulsarSink.producer."ProducerName".sendLatency75Pct ProducerName 某个生产者在过去的一个窗口内的发送延迟的 75 百分位数 Gauge PulsarSink.producer."ProducerName".sendLatency95Pct ProducerName 某个生产者在过去的一个窗口内的发送延迟的 95 百分位数 Gauge PulsarSink.producer."ProducerName".sendLatency99Pct ProducerName 某个生产者在过去的一个窗口内的发送延迟的 99 百分位数 Gauge PulsarSink.producer."ProducerName".sendLatency999Pct ProducerName 某个生产者在过去的一个窗口内的发送延迟的 99.9 百分位数 Gauge 指标 numBytesOut、numRecordsOut 和 numRecordsOutErrors 从 Pulsar Producer 实例的监控指标中获得。
currentSendTime 记录了最近一条消息从放入生产者的缓冲队列到消息被消费确认所耗费的时间。这项指标在 NONE 发送一致性下不可用。
默认情况下，Pulsar 生产者每隔 60 秒才会刷新一次监控数据，然而 Pulsar Sink 每 500 毫秒就会从 Pulsar 生产者中获得最新的监控数据。因此 numRecordsOut、numBytesOut、numAcksReceived 以及 numRecordsOutErrors 4 个指标实际上每 60 秒才会刷新一次。
如果想要更高地刷新评率，可以通过如下方式来将 Pulsar 生产者的监控数据刷新频率调整至相应值（最低为1s）： Java builder.setConfig(PulsarOptions.PULSAR_STATS_INTERVAL_SECONDS, 1L); Python builder.set_config(\u0026#34;pulsar.client.statsIntervalSeconds\u0026#34;, \u0026#34;1\u0026#34;) numBytesOutRate 和 numRecordsOutRate 指标是 Flink 内部通过 numBytesOut 和 numRecordsOut 计数器，在一个 60 秒的窗口内计算得到的。
设计思想简述 # Pulsar Sink 遵循 FLIP-191 中定义的 Sink API 设计。
无状态的 SinkWriter # 在 EXACTLY_ONCE 一致性下，Pulsar Sink 不会将事务相关的信息存放于检查点快照中。这意味着当 Flink 应用重启时，Pulsar Sink 会创建新的事务实例。上一次运行过程中任何未提交事务中的消息会因为超时中止而无法被下游的消费者所消费。这样的设计保证了 SinkWriter 是无状态的。
Pulsar Schema Evolution # Pulsar Schema Evolution 允许用户在一个 Flink 应用程序中使用的数据模型发生特定改变后（比如向基于 ARVO 的 POJO 类中增加或删除一个字段），仍能使用同一个 Flink 应用程序的代码。
可以在 Pulsar 集群内指定哪些类型的数据模型的改变是被允许的，详情请参阅 Pulsar Schema Evolution。
升级至最新的连接器 # 常见的升级步骤，请参阅升级应用程序和 Flink 版本。Pulsar 连接器没有在 Flink 端存储消费的状态，所有的消费信息都推送到了 Pulsar。所以需要注意下面的事项：
不要同时升级 Pulsar 连接器和 Pulsar 服务端的版本。 使用最新版本的 Pulsar 客户端来消费消息。 问题诊断 # 使用 Flink 和 Pulsar 交互时如果遇到问题，由于 Flink 内部实现只是基于 Pulsar 的 Java 客户端和管理 API 而开发的。
用户遇到的问题可能与 Flink 无关，请先升级 Pulsar 的版本、Pulsar 客户端的版本，或者修改 Pulsar 的配置、Pulsar 连接器的配置来尝试解决问题。
已知问题 # 本节介绍有关 Pulsar 连接器的一些已知问题。
在 Java 11 上使用不稳定 # Pulsar connector 在 Java 11 中有一些尚未修复的问题。我们当前推荐在 Java 8 环境中运行Pulsar connector.
不自动重连，而是抛出TransactionCoordinatorNotFound异常 # Pulsar 事务机制仍在积极发展中，当前版本并不稳定。 Pulsar 2.9.2 引入了这个问题 a break change。 如果您使用 Pulsar 2.9.2或更高版本与较旧的 Pulsar 客户端一起使用，您可能会收到一个“TransactionCoordinatorNotFound”异常。
您可以使用最新的pulsar-client-all分支来解决这个问题。
Back to top
`}),e.add({id:211,href:"/flink/flink-docs-master/zh/docs/deployment/repls/",title:"REPLs",section:"Deployment",content:""}),e.add({id:212,href:"/flink/flink-docs-master/zh/docs/ops/state/savepoints/",title:"Savepoints",section:"状态与容错",content:` Savepoints # 什么是 Savepoint ？ # Savepoint 是依据 Flink checkpointing 机制所创建的流作业执行状态的一致镜像。 你可以使用 Savepoint 进行 Flink 作业的停止与重启、fork 或者更新。 Savepoint 由两部分组成：稳定存储（列入 HDFS，S3，\u0026hellip;) 上包含二进制文件的目录（通常很大），和元数据文件（相对较小）。 稳定存储上的文件表示作业执行状态的数据镜像。 Savepoint 的元数据文件以（相对路径）的形式包含（主要）指向作为 Savepoint 一部分的稳定存储上的所有文件的指针。
注意: 为了允许程序和 Flink 版本之间的升级，请务必查看以下有关分配算子 ID 的部分 。 为了正确使用 savepoints，了解 checkpoints 与 savepoints 之间的区别非常重要，checkpoints 与 savepoints 中对此进行了描述。
分配算子 ID # 强烈建议你按照本节所述调整你的程序，以便将来能够升级你的程序。主要通过 uid(String) 方法手动指定算子 ID 。这些 ID 将用于恢复每个算子的状态。
DataStream\u0026lt;String\u0026gt; stream = env. // Stateful source (e.g. Kafka) with ID .addSource(new StatefulSource()) .uid(\u0026#34;source-id\u0026#34;) // ID for the source operator .shuffle() // Stateful mapper with ID .map(new StatefulMapper()) .uid(\u0026#34;mapper-id\u0026#34;) // ID for the mapper // Stateless printing sink .print(); // Auto-generated ID 如果不手动指定 ID ，则会自动生成 ID 。只要这些 ID 不变，就可以从 Savepoint 自动恢复。生成的 ID 取决于程序的结构，并且对程序更改很敏感。因此，强烈建议手动分配这些 ID 。
Savepoint 状态 # 你可以将 Savepoint 想象为每个有状态的算子保存一个映射“算子 ID -\u0026gt;状态”:
Operator ID | State ------------+------------------------ source-id | State of StatefulSource mapper-id | State of StatefulMapper 在上面的示例中，print sink 是无状态的，因此不是 Savepoint 状态的一部分。默认情况下，我们尝试将 Savepoint 的每个条目映射回新程序。
算子 # 你可以使用命令行客户端来触发 Savepoint，触发 Savepoint 并取消作业，从 Savepoint 恢复，以及删除 Savepoint。
从 Flink 1.2.0 开始，还可以使用 webui 从 Savepoint 恢复。
触发 Savepoint # 当触发 Savepoint 时，将创建一个新的 Savepoint 目录，其中存储数据和元数据。可以通过配置默认目标目录或使用触发器命令指定自定义目标目录(参见:targetDirectory参数来控制该目录的位置。
注意: 目标目录必须是 JobManager(s) 和 TaskManager(s) 都可以访问的位置，例如分布式文件系统（或者对象存储系统）上的位置。 以 FsStateBackend 或 RocksDBStateBackend 为例：
# Savepoint 目标目录 /savepoint/ # Savepoint 目录 /savepoint/savepoint-:shortjobid-:savepointid/ # Savepoint 文件包含 Checkpoint元数据 /savepoint/savepoint-:shortjobid-:savepointid/_metadata # Savepoint 状态 /savepoint/savepoint-:shortjobid-:savepointid/... 从 1.11.0 开始，你可以通过移动（拷贝）savepoint 目录到任意地方，然后再进行恢复。
在如下两种情况中不支持 savepoint 目录的移动：1）如果启用了 entropy injection ：这种情况下，savepoint 目录不包含所有的数据文件，因为注入的路径会分散在各个路径中。 由于缺乏一个共同的根目录，因此 savepoint 将包含绝对路径，从而导致无法支持 savepoint 目录的迁移。2）作业包含了 task-owned state（比如 GenericWriteAhreadLog sink）。 和 savepoint 不同，checkpoint 不支持任意移动文件，因为 checkpoint 可能包含一些文件的绝对路径。 如果你使用 MemoryStateBackend 的话，metadata 和 savepoint 的数据都会保存在 _metadata 文件中，因此不要因为没看到目录下没有数据文件而困惑。
注意: 不建议移动或删除正在运行作业的最后一个 Savepoint ，因为这可能会干扰故障恢复。因此，Savepoint 对精确一次的接收器有副作用，为了确保精确一次的语义，如果在最后一个 Savepoint 之后没有 Checkpoint ，那么将使用 Savepoint 进行恢复。 Savepoint 格式 # 你可以在 savepoint 的两种二进制格式之间进行选择：
标准格式 - 一种在所有 state backends 间统一的格式，允许你使用一种状态后端创建 savepoint 后，使用另一种状态后端恢复这个 savepoint。这是最稳定的格式，旨在与之前的版本、模式、修改等保持最大兼容性。
原生格式 - 标准格式的缺点是它的创建和恢复速度通常很慢。原生格式以特定于使用的状态后端的格式创建快照（例如 RocksDB 的 SST 文件）。
以原生格式创建 savepoint 的能力在 Flink 1.15 中引入，在那之前 savepoint 都是以标准格式创建的。 触发 Savepoint # \$ bin/flink savepoint :jobId [:targetDirectory] 这将触发 ID 为 :jobId 的作业的 Savepoint，并返回创建的 Savepoint 路径。 你需要此路径来恢复和删除 Savepoint 。你也可以指定创建 Savepoint 的格式。如果没有指定，会采用标准格式创建 Savepoint。
\$ bin/flink savepoint --type [native/canonical] :jobId [:targetDirectory] 使用 YARN 触发 Savepoint # \$ bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId 这将触发 ID 为 :jobId 和 YARN 应用程序 ID :yarnAppId 的作业的 Savepoint，并返回创建的 Savepoint 的路径。
使用 Savepoint 停止作业 # \$ bin/flink stop --type [native/canonical] --savepointPath [:targetDirectory] :jobId 这将自动触发 ID 为 :jobid 的作业的 Savepoint，并停止该作业。此外，你可以指定一个目标文件系统目录来存储 Savepoint 。该目录需要能被 JobManager(s) 和 TaskManager(s) 访问。你也可以指定创建 Savepoint 的格式。如果没有指定，会采用标准格式创建 Savepoint。
从 Savepoint 恢复 # \$ bin/flink run -s :savepointPath [:runArgs] 这将提交作业并指定要从中恢复的 Savepoint 。 你可以给出 Savepoint 目录或 _metadata 文件的路径。
跳过无法映射的状态恢复 # 默认情况下，resume 操作将尝试将 Savepoint 的所有状态映射回你要还原的程序。 如果删除了运算符，则可以通过 --allowNonRestoredState（short：-n）选项跳过无法映射到新程序的状态：
Restore 模式 # Restore 模式 决定了在 restore 之后谁拥有Savepoint 或者 externalized checkpoint的文件的所有权。在这种语境下 Savepoint 和 externalized checkpoint 的行为相似。 这里我们将它们都称为“快照”，除非另有明确说明。
如前所述，restore 模式决定了谁来接管我们从中恢复的快照文件的所有权。快照可被用户或者 Flink 自身拥有。如果快照归用户所有，Flink 不会删除其中的文件，而且 Flink 不能依赖该快照中文件的存在，因为它可能在 Flink 的控制之外被删除。
每种 restore 模式都有特定的用途。尽管如此，我们仍然认为默认的 NO_CLAIM 模式在大多数情况下是一个很好的折中方案，因为它在提供明确的所有权归属的同时只给恢复后第一个 checkpoint 带来较小的代价。
你可以通过如下方式指定 restore 模式：
\$ bin/flink run -s :savepointPath -restoreMode :mode -n [:runArgs] NO_CLAIM （默认的）
在 NO_CLAIM 模式下，Flink 不会接管快照的所有权。它会将快照的文件置于用户的控制之中，并且永远不会删除其中的任何文件。该模式下可以从同一个快照上启动多个作业。
为保证 Flink 不会依赖于该快照的任何文件，它会强制第一个（成功的） checkpoint 为全量 checkpoint 而不是增量的。这仅对state.backend: rocksdb 有影响，因为其他 backend 总是创建全量 checkpoint。
一旦第一个全量的 checkpoint 完成后，所有后续的 checkpoint 会照常创建。所以，一旦一个 checkpoint 成功制作，就可以删除原快照。在此之前不能删除原快照，因为没有任何完成的 checkpoint，Flink 会在故障时尝试从初始的快照恢复。
CLAIM
另一个可选的模式是 CLAIM 模式。该模式下 Flink 将声称拥有快照的所有权，并且本质上将其作为 checkpoint 对待：控制其生命周期并且可能会在其永远不会被用于恢复的时候删除它。因此，手动删除快照和从同一个快照上启动两个作业都是不安全的。Flink 会保持配置数量的 checkpoint。
注意：
Retained checkpoints 被存储在 \u0026lt;checkpoint_dir\u0026gt;/\u0026lt;job_id\u0026gt;/chk-\u0026lt;x\u0026gt; 这样的目录中。Flink 不会接管 \u0026lt;checkpoint_dir\u0026gt;/\u0026lt;job_id\u0026gt; 目录的所有权，而只会接管 chk-\u0026lt;x\u0026gt; 的所有权。Flink 不会删除旧作业的目录。
Native 格式支持增量的 RocksDB savepoints。对于这些 savepoints，Flink 将所有 SST 存储在 savepoints 目录中。这意味着这些 savepoints 是自包含和目录可移动的。然而，在 CLAIM 模式下恢复时，后续的 checkpoints 可能会复用一些 SST 文件，这反过来会阻止在 savepoints 被清理时删除 savepoints 目录。 Flink 之后运行期间可能会删除复用的SST 文件，但不会删除 savepoints 目录。因此，如果在 CLAIM 模式下恢复，Flink 可能会留下一个空的 savepoints 目录。
LEGACY
Legacy 模式是 Flink 在 1.15 之前的工作方式。该模式下 Flink 永远不会删除初始恢复的 checkpoint。同时，用户也不清楚是否可以删除它。导致该的问题原因是， Flink 会在用来恢复的 checkpoint 之上创建增量的 checkpoint，因此后续的 checkpoint 都有可能会依赖于用于恢复的那个 checkpoint。总而言之，恢复的 checkpoint 的所有权没有明确的界定。
删除 Savepoint # \$ bin/flink savepoint -d :savepointPath 这将删除存储在 :savepointPath 中的 Savepoint。
请注意，还可以通过常规文件系统操作手动删除 Savepoint ，而不会影响其他 Savepoint 或 Checkpoint（请记住，每个 Savepoint 都是自包含的）。 在 Flink 1.2 之前，使用上面的 Savepoint 命令执行是一个更乏味的任务。
配置 # 你可以通过 state.savepoints.dir 配置 savepoint 的默认目录。 触发 savepoint 时，将使用此目录来存储 savepoint。 你可以通过使用触发器命令指定自定义目标目录来覆盖缺省值（请参阅:targetDirectory参数）。
# 默认 Savepoint 目标目录 state.savepoints.dir: hdfs:///flink/savepoints 如果既未配置缺省值也未指定自定义目标目录，则触发 Savepoint 将失败。
注意: 目标目录必须是 JobManager(s) 和 TaskManager(s) 可访问的位置，例如，分布式文件系统上的位置。 F.A.Q # 我应该为我作业中的所有算子分配 ID 吗? # 根据经验，是的。 严格来说，仅通过 uid 方法给有状态算子分配 ID 就足够了。Savepoint 仅包含这些有状态算子的状态，无状态算子不是 Savepoint 的一部分。
在实践中，建议给所有算子分配 ID，因为 Flink 的一些内置算子（如 Window 算子）也是有状态的，而内置算子是否有状态并不很明显。 如果你完全确定算子是无状态的，则可以跳过 uid 方法。
如果我在作业中添加一个需要状态的新算子，会发生什么？ # 当你向作业添加新算子时，它将在没有任何状态的情况下进行初始化。 Savepoint 包含每个有状态算子的状态。 无状态算子根本不是 Savepoint 的一部分。 新算子的行为类似于无状态算子。
如果从作业中删除有状态的算子会发生什么? # 默认情况下，从 Savepoint 恢复时将尝试将所有状态分配给新作业。如果有状态算子被删除，则无法从 Savepoint 恢复。
你可以通过使用 run 命令设置 --allowNonRestoredState (简称：-n )来允许删除有状态算子:
\$ bin/flink run -s :savepointPath -n [:runArgs] 如果我在作业中重新排序有状态算子，会发生什么? # 如果给这些算子分配了 ID，它们将像往常一样恢复。
如果没有分配 ID ，则有状态操作符自动生成的 ID 很可能在重新排序后发生更改。这将导致你无法从以前的 Savepoint 恢复。
如果我添加、删除或重新排序作业中没有状态的算子，会发生什么? # 如果将 ID 分配给有状态操作符，则无状态操作符不会影响 Savepoint 恢复。
如果没有分配 ID ，则有状态操作符自动生成的 ID 很可能在重新排序后发生更改。这将导致你无法从以前的Savepoint 恢复。
当我在恢复时改变程序的并行度时会发生什么? # 如果 Savepoint 是用 Flink \u0026gt;= 1.2.0 触发的，并且没有使用像 Checkpointed 这样的不推荐的状态API，那么你可以简单地从 Savepoint 恢复程序并指定新的并行度。
如果你正在从 Flink \u0026lt; 1.2.0 触发的 Savepoint 恢复，或者使用现在已经废弃的 api，那么你首先必须将作业和 Savepoint 迁移到 Flink \u0026gt;= 1.2.0，然后才能更改并行度。参见升级作业和Flink版本指南。
我可以将 savepoint 文件移动到稳定存储上吗? # 这个问题的快速答案目前是“是”，从 Flink 1.11.0 版本开始，savepoint 是自包含的，你可以按需迁移 savepoint 文件后进行恢复。
Back to top
`}),e.add({id:213,href:"/flink/flink-docs-master/zh/docs/dev/dataset/local_execution/",title:"本地执行",section:"DataSet API (Legacy)",content:` 本地执行 # Flink can run on a single machine, even in a single Java Virtual Machine. This allows users to test and debug Flink programs locally. This section gives an overview of the local execution mechanisms.
The local environments and executors allow you to run Flink programs in a local Java Virtual Machine, or with within any JVM as part of existing programs. Most examples can be launched locally by simply hitting the \u0026ldquo;Run\u0026rdquo; button of your IDE.
There are two different kinds of local execution supported in Flink. The LocalExecutionEnvironment is starting the full Flink runtime, including a JobManager and a TaskManager. These include memory management and all the internal algorithms that are executed in the cluster mode.
The CollectionEnvironment is executing the Flink program on Java collections. This mode will not start the full Flink runtime, so the execution is very low-overhead and lightweight. For example a DataSet.map()-transformation will be executed by applying the map() function to all elements in a Java list.
Debugging # If you are running Flink programs locally, you can also debug your program like any other Java program. You can either use System.out.println() to write out some internal variables or you can use the debugger. It is possible to set breakpoints within map(), reduce() and all the other methods. Please also refer to the debugging section in the Java API documentation for a guide to testing and local debugging utilities in the Java API.
Maven Dependency # If you are developing your program in a Maven project, you have to add the flink-clients module using this dependency:
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Local Environment # The LocalEnvironment is a handle to local execution for Flink programs. Use it to run a program within a local JVM - standalone or embedded in other programs.
The local environment is instantiated via the method ExecutionEnvironment.createLocalEnvironment(). By default, it will use as many local threads for execution as your machine has CPU cores (hardware contexts). You can alternatively specify the desired parallelism. The local environment can be configured to log to the console using enableLogging()/disableLogging().
In most cases, calling ExecutionEnvironment.getExecutionEnvironment() is the even better way to go. That method returns a LocalEnvironment when the program is started locally (outside the command line interface), and it returns a pre-configured environment for cluster execution, when the program is invoked by the command line interface.
public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;file:///path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;file:///path/to/result\u0026#34;); JobExecutionResult res = env.execute(); } The JobExecutionResult object, which is returned after the execution finished, contains the program runtime and the accumulator results.
The LocalEnvironment allows also to pass custom configuration values to Flink.
Configuration conf = new Configuration(); conf.setFloat(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY, 0.5f); final ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(conf); Note: The local execution environments do not start any web frontend to monitor the execution.
Collection Environment # The execution on Java Collections using the CollectionEnvironment is a low-overhead approach for executing Flink programs. Typical use-cases for this mode are automated tests, debugging and code re-use.
Users can use algorithms implemented for batch processing also for cases that are more interactive. A slightly changed variant of a Flink program could be used in a Java Application Server for processing incoming requests.
Skeleton for Collection-based execution
public static void main(String[] args) throws Exception { // initialize a new Collection-based execution environment final ExecutionEnvironment env = new CollectionEnvironment(); DataSet\u0026lt;User\u0026gt; users = env.fromCollection( /* get elements from a Java Collection */); /* Data Set transformations ... */ // retrieve the resulting Tuple2 elements into a ArrayList. Collection\u0026lt;...\u0026gt; result = new ArrayList\u0026lt;...\u0026gt;(); resultDataSet.output(new LocalCollectionOutputFormat\u0026lt;...\u0026gt;(result)); // kick off execution. env.execute(); // Do some work with the resulting ArrayList (=Collection). for(... t : result) { System.err.println(\u0026#34;Result = \u0026#34;+t); } } The flink-examples-batch module contains a full example, called CollectionExecutionExample.
Please note that the execution of the collection-based Flink programs is only possible on small data, which fits into the JVM heap. The execution on collections is not multi-threaded, only one thread is used.
Back to top
`}),e.add({id:214,href:"/flink/flink-docs-master/zh/docs/deployment/advanced/",title:"Advanced",section:"Deployment",content:""}),e.add({id:215,href:"/flink/flink-docs-master/zh/docs/ops/state/checkpoints_vs_savepoints/",title:"Checkpoints 与 Savepoints",section:"状态与容错",content:` Checkpoints 与 Savepoints # 概述 # 从概念上讲，Flink 的 savepoints 与 checkpoints 的不同之处类似于传统数据库系统中的备份与恢复日志之间的差异。
Checkpoints 的主要目的是为意外失败的作业提供恢复机制。 Checkpoint 的生命周期 由 Flink 管理， 即 Flink 创建，管理和删除 checkpoint - 无需用户交互。 由于 checkpoint 被经常触发，且被用于作业恢复，所以 Checkpoint 的实现有两个设计目标：i）轻量级创建和 ii）尽可能快地恢复。 可能会利用某些特定的属性来达到这个目标，例如， 作业的代码在执行尝试时不会改变。
在用户终止作业后，会自动删除 Checkpoint（除非明确配置为保留的 Checkpoint）。 Checkpoint 以状态后端特定的（原生的）数据格式存储（有些状态后端可能是增量的）。 尽管 savepoints 在内部使用与 checkpoints 相同的机制创建，但它们在概念上有所不同，并且生成和恢复的成本可能会更高一些。Savepoints的设计更侧重于可移植性和操作灵活性，尤其是在 job 变更方面。Savepoint 的用例是针对计划中的、手动的运维。例如，可能是更新你的 Flink 版本，更改你的作业图等等。
Savepoint 仅由用户创建、拥有和删除。这意味着 Flink 在作业终止后和恢复后都不会删除 savepoint。 Savepoint 以状态后端独立的（标准的）数据格式存储（注意：从 Flink 1.15 开始，savepoint 也可以以后端特定的原生格式存储，这种格式创建和恢复速度更快，但有一些限制）。 功能和限制 # 下表概述了各种类型的 savepoint 和 checkpoint 的功能和限制。
✓ - Flink 完全支持这种类型的快照 x - Flink 不支持这种类型的快照 ! - 虽然这些操作目前有效，但 Flink 并未正式保证对它们的支持，因此它们存在一定程度的风险 操作 标准 Savepoint 原生 Savepoint 对齐 Checkpoint 非对齐 Checkpoint 更换状态后端 ✓ x x x State Processor API (写) ✓ x x x State Processor API (读) ✓ ! ! x 自包含和可移动 ✓ ✓ x x Schema 变更 ✓ ! ! ! 任意 job 升级 ✓ ✓ ✓ x 非任意 job 升级 ✓ ✓ ✓ ✓ Flink 小版本升级 ✓ ✓ ✓ x Flink bug/patch 版本升级 ✓ ✓ ✓ ✓ 扩缩容 ✓ ✓ ✓ ✓ 更换状态后端 - 配置与创建快照时使用的不同的状态后端。 State Processor API (写) - 通过 State Processor API 创建这种类型的新快照的能力。 State Processor API (读) - 通过 State Processor API 从该类型的现有快照中读取状态的能力。 自包含和可移动 - 快照目录包含从该快照恢复所需的所有内容，并且不依赖于其他快照，这意味着如果需要的话，它可以轻松移动到另一个地方。 Schema 变更 - 如果使用支持 Schema 变更的序列化器（例如 POJO 和 Avro 类型），则可以更改状态数据类型。 任意 job 升级 - 即使现有算子的 partitioning 类型（rescale, rebalance, map, 等）或运行中数据类型已经更改，也可以从该快照恢复。 非任意 job 升级 - 如果作业图拓扑和运行中数据类型保持不变，则可以使用变更后的 operator 恢复快照。 Flink 小版本升级 - 从更旧的 Flink 小版本创建的快照恢复（1.x → 1.y）。 Flink bug/patch 版本升级 - 从更旧的 Flink 补丁版本创建的快照恢复（1.14.x → 1.14.y）。 扩缩容 - 使用与快照制作时不同的并发度从该快照恢复。 Back to top
`}),e.add({id:216,href:"/flink/flink-docs-master/zh/docs/dev/dataset/",title:"DataSet API (Legacy)",section:"应用开发",content:""}),e.add({id:217,href:"/flink/flink-docs-master/zh/docs/connectors/datastream/jdbc/",title:"JDBC",section:"DataStream Connectors",content:` JDBC Connector # 该连接器可以向 JDBC 数据库写入数据。
添加下面的依赖以便使用该连接器（同时添加 JDBC 驱动）：
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-connector-jdbc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 注意该连接器目前还 不是 二进制发行版的一部分，如何在集群中运行请参考 这里。
已创建的 JDBC Sink 能够保证至少一次的语义。 更有效的精确执行一次可以通过 upsert 语句或幂等更新实现。
用法示例： Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env .fromElements(...) .addSink(JdbcSink.sink( \u0026#34;insert into books (id, title, author, price, qty) values (?,?,?,?,?)\u0026#34;, (ps, t) -\u0026gt; { ps.setInt(1, t.id); ps.setString(2, t.title); ps.setString(3, t.author); ps.setDouble(4, t.price); ps.setInt(5, t.qty); }, new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withUrl(getDbMetadata().getUrl()) .withDriverName(getDbMetadata().getDriverClass()) .build())); env.execute(); Python env = StreamExecutionEnvironment.get_execution_environment() type_info = Types.ROW([Types.INT(), Types.STRING(), Types.STRING(), Types.INT()]) env.from_collection( [(101, \u0026#34;Stream Processing with Apache Flink\u0026#34;, \u0026#34;Fabian Hueske, Vasiliki Kalavri\u0026#34;, 2019), (102, \u0026#34;Streaming Systems\u0026#34;, \u0026#34;Tyler Akidau, Slava Chernyak, Reuven Lax\u0026#34;, 2018), (103, \u0026#34;Designing Data-Intensive Applications\u0026#34;, \u0026#34;Martin Kleppmann\u0026#34;, 2017), (104, \u0026#34;Kafka: The Definitive Guide\u0026#34;, \u0026#34;Gwen Shapira, Neha Narkhede, Todd Palino\u0026#34;, 2017) ], type_info=type_info) \\ .add_sink( JdbcSink.sink( \u0026#34;insert into books (id, title, authors, year) values (?, ?, ?, ?)\u0026#34;, type_info, JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .with_url(\u0026#39;jdbc:postgresql://dbhost:5432/postgresdb\u0026#39;) .with_driver_name(\u0026#39;org.postgresql.Driver\u0026#39;) .with_user_name(\u0026#39;someUser\u0026#39;) .with_password(\u0026#39;somePassword\u0026#39;) .build() )) env.execute() 更多细节请查看 API documentation 。
`}),e.add({id:218,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/joins/",title:"Join",section:"Queries 查询",content:` Joins # Batch Streaming
Flink SQL supports complex and flexible join operations over dynamic tables. There are several different types of joins to account for the wide variety of semantics queries may require.
By default, the order of joins is not optimized. Tables are joined in the order in which they are specified in the FROM clause. You can tweak the performance of your join queries, by listing the tables with the lowest update frequency first and the tables with the highest update frequency last. Make sure to specify tables in an order that does not yield a cross join (Cartesian product), which are not supported and would cause a query to fail.
Regular Joins # Regular joins are the most generic type of join in which any new record, or changes to either side of the join, are visible and affect the entirety of the join result. For example, if there is a new record on the left side, it will be joined with all the previous and future records on the right side when the product id equals.
SELECT * FROM Orders INNER JOIN Product ON Orders.productId = Product.id For streaming queries, the grammar of regular joins is the most flexible and allow for any kind of updating (insert, update, delete) input table. However, this operation has important operational implications: it requires to keep both sides of the join input in Flink state forever. Thus, the required state for computing the query result might grow infinitely depending on the number of distinct input rows of all input tables and intermediate join results. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. INNER Equi-JOIN # Returns a simple Cartesian product restricted by the join condition. Currently, only equi-joins are supported, i.e., joins that have at least one conjunctive condition with an equality predicate. Arbitrary cross or theta joins are not supported.
SELECT * FROM Orders INNER JOIN Product ON Orders.product_id = Product.id OUTER Equi-JOIN # Returns all rows in the qualified Cartesian product (i.e., all combined rows that pass its join condition), plus one copy of each row in an outer table for which the join condition did not match with any row of the other table. Flink supports LEFT, RIGHT, and FULL outer joins. Currently, only equi-joins are supported, i.e., joins with at least one conjunctive condition with an equality predicate. Arbitrary cross or theta joins are not supported.
SELECT * FROM Orders LEFT JOIN Product ON Orders.product_id = Product.id SELECT * FROM Orders RIGHT JOIN Product ON Orders.product_id = Product.id SELECT * FROM Orders FULL OUTER JOIN Product ON Orders.product_id = Product.id Interval Joins # Returns a simple Cartesian product restricted by the join condition and a time constraint. An interval join requires at least one equi-join predicate and a join condition that bounds the time on both sides. Two appropriate range predicates can define such a condition (\u0026lt;, \u0026lt;=, \u0026gt;=, \u0026gt;), a BETWEEN predicate, or a single equality predicate that compares time attributes of the same type (i.e., processing time or event time) of both input tables.
For example, this query will join all orders with their corresponding shipments if the order was shipped four hours after the order was received.
SELECT * FROM Orders o, Shipments s WHERE o.id = s.order_id AND o.order_time BETWEEN s.ship_time - INTERVAL \u0026#39;4\u0026#39; HOUR AND s.ship_time The following predicates are examples of valid interval join conditions:
ltime = rtime ltime \u0026gt;= rtime AND ltime \u0026lt; rtime + INTERVAL '10' MINUTE ltime BETWEEN rtime - INTERVAL '10' SECOND AND rtime + INTERVAL '5' SECOND For streaming queries, compared to the regular join, interval join only supports append-only tables with time attributes. Since time attributes are quasi-monotonic increasing, Flink can remove old values from its state without affecting the correctness of the result.
Temporal Joins # A Temporal table is a table that evolves over time - otherwise known in Flink as a dynamic table. Rows in a temporal table are associated with one or more temporal periods and all Flink tables are temporal(dynamic). The temporal table contains one or more versioned table snapshots, it can be a changing history table which tracks the changes(e.g. database changelog, contains all snapshots) or a changing dimensioned table which materializes the changes(e.g. database table which contains the latest snapshot).
Event Time Temporal Join # Event Time Temporal joins allow joining against a versioned table. This means a table can be enriched with changing metadata and retrieve its value at a certain point in time.
Temporal joins take an arbitrary table (left input/probe site) and correlate each row to the corresponding row\u0026rsquo;s relevant version in the versioned table (right input/build side). Flink uses the SQL syntax of FOR SYSTEM_TIME AS OF to perform this operation from the SQL:2011 standard. The syntax of a temporal join is as follows;
SELECT [column_list] FROM table1 [AS \u0026lt;alias1\u0026gt;] [LEFT] JOIN table2 FOR SYSTEM_TIME AS OF table1.{ proctime | rowtime } [AS \u0026lt;alias2\u0026gt;] ON table1.column-name1 = table2.column-name1 With an event-time attribute (i.e., a rowtime attribute), it is possible to retrieve the value of a key as it was at some point in the past. This allows for joining the two tables at a common point in time. The versioned table will store all versions - identified by time - since the last watermark.
For example, suppose we have a table of orders, each with prices in different currencies. To properly normalize this table to a single currency, such as USD, each order needs to be joined with the proper currency conversion rate from the point-in-time when the order was placed.
-- Create a table of orders. This is a standard -- append-only dynamic table. CREATE TABLE orders ( order_id STRING, price DECIMAL(32,2), currency STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time ) WITH (/* ... */); -- Define a versioned table of currency rates. -- This could be from a change-data-capture -- such as Debezium, a compacted Kafka topic, or any other -- way of defining a versioned table. CREATE TABLE currency_rates ( currency STRING, conversion_rate DECIMAL(32, 2), update_time TIMESTAMP(3) METADATA FROM \`values.source.timestamp\` VIRTUAL, WATERMARK FOR update_time AS update_time, PRIMARY KEY(currency) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;debezium-json\u0026#39;, /* ... */ ); SELECT order_id, price, currency, conversion_rate, order_time FROM orders LEFT JOIN currency_rates FOR SYSTEM_TIME AS OF orders.order_time ON orders.currency = currency_rates.currency; order_id price currency conversion_rate order_time ======== ===== ======== =============== ========= o_001 11.11 EUR 1.14 12:00:00 o_002 12.51 EUR 1.10 12:06:00 Note: The event-time temporal join is triggered by a watermark from the left and right sides; please ensure both sides of the join have set watermark correctly.
Note: The event-time temporal join requires the primary key contained in the equivalence condition of the temporal join condition, e.g., The primary key currency_rates.currency of table currency_rates to be constrained in the condition orders.currency = currency_rates.currency.
In contrast to regular joins, the previous temporal table results will not be affected despite the changes on the build side. Compared to interval joins, temporal table joins do not define a time window within which the records will be joined. Records from the probe side are always joined with the build side\u0026rsquo;s version at the time specified by the time attribute. Thus, rows on the build side might be arbitrarily old. As time passes, no longer needed versions of the record (for the given primary key) will be removed from the state.
Processing Time Temporal Join # A processing time temporal table join uses a processing-time attribute to correlate rows to the latest version of a key in an external versioned table.
By definition, with a processing-time attribute, the join will always return the most up-to-date value for a given key. One can think of a lookup table as a simple HashMap\u0026lt;K, V\u0026gt; that stores all the records from the build side. The power of this join is it allows Flink to work directly against external systems when it is not feasible to materialize the table as a dynamic table within Flink.
The following processing-time temporal table join example shows an append-only table orders that should be joined with the table LatestRates. LatestRates is a dimension table (e.g. HBase table) that is materialized with the latest rate. At time 10:15, 10:30, 10:52, the content of LatestRates looks as follows:
10:15\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 114 Yen 1 10:30\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 114 Yen 1 10:52\u0026gt; SELECT * FROM LatestRates; currency rate ======== ====== US Dollar 102 Euro 116 \u0026lt;==== changed from 114 to 116 Yen 1 The content of LastestRates at times 10:15 and 10:30 are equal. The Euro rate has changed from 114 to 116 at 10:52.
Orders is an append-only table representing payments for the given amount and the given currency. For example, at 10:15 there was an order for an amount of 2 Euro.
SELECT * FROM Orders; amount currency ====== ========= 2 Euro \u0026lt;== arrived at time 10:15 1 US Dollar \u0026lt;== arrived at time 10:30 2 Euro \u0026lt;== arrived at time 10:52 Given these tables, we would like to calculate all Orders converted to a common currency.
amount currency rate amount*rate ====== ========= ======= ============ 2 Euro 114 228 \u0026lt;== arrived at time 10:15 1 US Dollar 102 102 \u0026lt;== arrived at time 10:30 2 Euro 116 232 \u0026lt;== arrived at time 10:52 Currently, the FOR SYSTEM_TIME AS OF syntax used in temporal join with latest version of any view/table is not support yet, you can use temporal table function syntax as following:
SELECT o_amount, r_rate FROM Orders, LATERAL TABLE (Rates(o_proctime)) WHERE r_currency = o_currency Note The reason why the FOR SYSTEM_TIME AS OF syntax used in temporal join with latest version of any table/view is not support is only the semantic consideration, because the join processing for left stream doesn\u0026rsquo;t wait for the complete snapshot of temporal table, this may mislead users in production environment. The processing-time temporal join by temporal table function also exists same semantic problem, but it has been alive for a long time, thus we support it from the perspective of compatibility.
The result is not deterministic for processing-time. The processing-time temporal join is most often used to enrich the stream with an external table (i.e., dimension table).
In contrast to regular joins, the previous temporal table results will not be affected despite the changes on the build side. Compared to interval joins, temporal table joins do not define a time window within which the records join, i.e., old rows are not stored in state.
Temporal Table Function Join # The syntax to join a table with a temporal table function is the same as in Join with Table Function.
Note: Currently only inner join and left outer join with temporal tables are supported.
Assuming Rates is a temporal table function, the join can be expressed in SQL as follows:
SELECT o_amount, r_rate FROM Orders, LATERAL TABLE (Rates(o_proctime)) WHERE r_currency = o_currency The main difference between above Temporal Table DDL and Temporal Table Function are:
The temporal table DDL can be defined in SQL but temporal table function can not; Both temporal table DDL and temporal table function support temporal join versioned table, but only temporal table function can temporal join the latest version of any table/view. Lookup Join # A lookup join is typically used to enrich a table with data that is queried from an external system. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.
The lookup join uses the above Processing Time Temporal Join syntax with the right table to be backed by a lookup source connector.
The following example shows the syntax to specify a lookup join.
-- Customers is backed by the JDBC connector and can be used for lookup joins CREATE TEMPORARY TABLE Customers ( id INT, name STRING, country STRING, zip STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://mysqlhost:3306/customerdb\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;customers\u0026#39; ); -- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN Customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; In the example above, the Orders table is enriched with data from the Customers table which resides in a MySQL database. The FOR SYSTEM_TIME AS OF clause with the subsequent processing time attribute ensures that each row of the Orders table is joined with those Customers rows that match the join predicate at the point in time when the Orders row is processed by the join operator. It also prevents that the join result is updated when a joined Customer row is updated in the future. The lookup join also requires a mandatory equality join predicate, in the example above o.customer_id = c.id.
Array Expansion # Returns a new row for each element in the given array. Unnesting WITH ORDINALITY is not yet supported.
SELECT order_id, tag FROM Orders CROSS JOIN UNNEST(tags) AS t (tag) Table Function # Joins a table with the results of a table function. Each row of the left (outer) table is joined with all rows produced by the corresponding call of the table function. User-defined table functions must be registered before use.
INNER JOIN # The row of the left (outer) table is dropped, if its table function call returns an empty result.
SELECT order_id, res FROM Orders, LATERAL TABLE(table_func(order_id)) t(res) LEFT OUTER JOIN # If a table function call returns an empty result, the corresponding outer row is preserved, and the result padded with null values. Currently, a left outer join against a lateral table requires a TRUE literal in the ON clause.
SELECT order_id, res FROM Orders LEFT OUTER JOIN LATERAL TABLE(table_func(order_id)) t(res) ON TRUE Back to top
`}),e.add({id:219,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/orc/",title:"Orc",section:"Formats",content:` Orc Format # Format: Serialization Schema Format: Deserialization Schema
Apache Orc Format 允许读写 ORC 数据。
依赖 # In order to use the ORC format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-orc\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Only available for stable releases. 如何用 Orc 格式创建一个表格 # 下面是一个用 Filesystem connector 和 Orc format 创建表格的例子
CREATE TABLE user_behavior ( user_id BIGINT, item_id BIGINT, category_id BIGINT, behavior STRING, ts TIMESTAMP(3), dt STRING ) PARTITIONED BY (dt) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/tmp/user_behavior\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;orc\u0026#39; ) Format 参数 # 参数 是否必选 默认值 类型 描述 format 必选 (none) String 指定要使用的格式，这里应该是 'orc'。 Orc 格式也支持来源于 Table properties 的表属性。 举个例子，你可以设置 orc.compress=SNAPPY 来允许spappy压缩。
数据类型映射 # Orc 格式类型的映射和 Apache Hive 是兼容的。下面的表格列出了 Flink 类型的数据和 Orc 类型的数据的映射关系。
Flink 数据类型 Orc 物理类型 Orc 逻辑类型 CHAR bytes CHAR VARCHAR bytes VARCHAR STRING bytes STRING BOOLEAN long BOOLEAN BYTES bytes BINARY DECIMAL decimal DECIMAL TINYINT long BYTE SMALLINT long SHORT INT long INT BIGINT long LONG FLOAT double FLOAT DOUBLE double DOUBLE DATE long DATE TIMESTAMP timestamp TIMESTAMP ARRAY - LIST MAP - MAP ROW - STRUCT `}),e.add({id:220,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/use/",title:"USE 语句",section:"SQL",content:` USE 语句 # USE 语句用来设置当前的 catalog 或者 database。
运行一个 USE 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 USE 语句。 若 USE 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
Scala 可以使用 TableEnvironment 中的 executeSql() 方法执行 USE 语句。 若 USE 操作执行成功，executeSql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
Python 可以使用 TableEnvironment 中的 execute_sql() 方法执行 USE 语句。 若 USE 操作执行成功，execute_sql() 方法返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下的例子展示了如何在 TableEnvironment 中执行一个 USE 语句。
SQL CLI 可以在 SQL CLI 中执行 USE 语句。
以下的例子展示了如何在 SQL CLI 中执行一个 USE 语句。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // create a catalog tEnv.executeSql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print(); // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // | cat1 | // +-----------------+ // change default catalog tEnv.executeSql(\u0026#34;USE CATALOG cat1\u0026#34;); tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // databases are empty // +---------------+ // | database name | // +---------------+ // +---------------+ // create a database tEnv.executeSql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // +---------------+ // | database name | // +---------------+ // | db1 | // +---------------+ // change default database tEnv.executeSql(\u0026#34;USE db1\u0026#34;); // change module resolution order and enabled status tEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;); tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // create a catalog tEnv.executeSql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // | cat1 | // +-----------------+ // change default catalog tEnv.executeSql(\u0026#34;USE CATALOG cat1\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // databases are empty // +---------------+ // | database name | // +---------------+ // +---------------+ // create a database tEnv.executeSql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +---------------+ // | database name | // +---------------+ // | db1 | // +---------------+ // change default database tEnv.executeSql(\u0026#34;USE db1\u0026#34;) // change module resolution order and enabled status tEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;) tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ Python table_env = StreamTableEnvironment.create(...) # create a catalog table_env.execute_sql(\u0026#34;CREATE CATALOG cat1 WITH (...)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW CATALOGS\u0026#34;).print() # +-----------------+ # | catalog name | # +-----------------+ # | default_catalog | # | cat1 | # +-----------------+ # change default catalog table_env.execute_sql(\u0026#34;USE CATALOG cat1\u0026#34;) table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # databases are empty # +---------------+ # | database name | # +---------------+ # +---------------+ # create a database table_env.execute_sql(\u0026#34;CREATE DATABASE db1 WITH (...)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # +---------------+ # | database name | # +---------------+ # | db1 | # +---------------+ # change default database table_env.execute_sql(\u0026#34;USE db1\u0026#34;) # change module resolution order and enabled status table_env.execute_sql(\u0026#34;USE MODULES hive\u0026#34;) table_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ SQL CLI Flink SQL\u0026gt; CREATE CATALOG cat1 WITH (...); [INFO] Catalog has been created. Flink SQL\u0026gt; SHOW CATALOGS; default_catalog cat1 Flink SQL\u0026gt; USE CATALOG cat1; Flink SQL\u0026gt; SHOW DATABASES; Flink SQL\u0026gt; CREATE DATABASE db1 WITH (...); [INFO] Database has been created. Flink SQL\u0026gt; SHOW DATABASES; db1 Flink SQL\u0026gt; USE db1; Flink SQL\u0026gt; USE MODULES hive; [INFO] Use modules succeeded! Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+-------+ | module name | used | +-------------+-------+ | hive | true | | core | false | +-------------+-------+ 2 rows in set Back to top
USE CATALOG # USE CATALOG catalog_name 设置当前的 catalog。所有后续命令未显式指定 catalog 的将使用此 catalog。如果指定的的 catalog 不存在，则抛出异常。默认的当前 catalog 是 default_catalog。
USE MODULES # USE MODULES module_name1[, module_name2, ...] Set the enabled modules with declared order. All subsequent commands will resolve metadata(functions/user-defined types/rules, etc.) within enabled modules and follow resolution order. A module is used by default when it is loaded. Loaded modules will become disabled if not used by USE MODULES statement. The default loaded and enabled module is core.
USE # USE [catalog_name.]database_name 设置当前的 database。所有后续命令未显式指定 database 的将使用此 database。如果指定的的 database 不存在，则抛出异常。默认的当前 database 是 default_database。
`}),e.add({id:221,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-join/",title:"窗口关联",section:"Queries 查询",content:` Window Join # Batch Streaming
A window join adds the dimension of time into the join criteria themselves. In doing so, the window join joins the elements of two streams that share a common key and are in the same window. The semantic of window join is same to the DataStream window join
For streaming queries, unlike other joins on continuous tables, window join does not emit intermediate results but only emits final results at the end of the window. Moreover, window join purge all intermediate state when no longer needed.
Usually, Window Join is used with Windowing TVF. Besides, Window Join could follow after other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Currently, Window Join requires the join on condition contains window starts equality of input tables and window ends equality of input tables.
Window Join supports INNER/LEFT/RIGHT/FULL OUTER/ANTI/SEMI JOIN.
INNER/LEFT/RIGHT/FULL OUTER # The following shows the syntax of the INNER/LEFT/RIGHT/FULL OUTER Window Join statement.
SELECT ... FROM L [LEFT|RIGHT|FULL OUTER] JOIN R -- L and R are relations applied windowing TVF ON L.window_start = R.window_start AND L.window_end = R.window_end AND ... The syntax of INNER/LEFT/RIGHT/FULL OUTER WINDOW JOIN are very similar with each other, we only give an example for FULL OUTER JOIN here. When performing a window join, all elements with a common key and a common tumbling window are joined together. We only give an example for a Window Join which works on a Tumble Window TVF. By scoping the region of time for the join into fixed five-minute intervals, we chopped our datasets into two distinct windows of time: [12:00, 12:05) and [12:05, 12:10). The L2 and R2 rows could not join together because they fell into separate windows.
Flink SQL\u0026gt; desc LeftTable; +----------+------------------------+------+-----+--------+----------------------------------+ | name | type | null | key | extras | watermark | +----------+------------------------+------+-----+--------+----------------------------------+ | row_time | TIMESTAMP(3) *ROWTIME* | true | | | \`row_time\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | num | INT | true | | | | | id | STRING | true | | | | +----------+------------------------+------+-----+--------+----------------------------------+ Flink SQL\u0026gt; SELECT * FROM LeftTable; +------------------+-----+----+ | row_time | num | id | +------------------+-----+----+ | 2020-04-15 12:02 | 1 | L1 | | 2020-04-15 12:06 | 2 | L2 | | 2020-04-15 12:03 | 3 | L3 | +------------------+-----+----+ Flink SQL\u0026gt; desc RightTable; +----------+------------------------+------+-----+--------+----------------------------------+ | name | type | null | key | extras | watermark | +----------+------------------------+------+-----+--------+----------------------------------+ | row_time | TIMESTAMP(3) *ROWTIME* | true | | | \`row_time\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | num | INT | true | | | | | id | STRING | true | | | | +----------+------------------------+------+-----+--------+----------------------------------+ Flink SQL\u0026gt; SELECT * FROM RightTable; +------------------+-----+----+ | row_time | num | id | +------------------+-----+----+ | 2020-04-15 12:01 | 2 | R2 | | 2020-04-15 12:04 | 3 | R3 | | 2020-04-15 12:05 | 4 | R4 | +------------------+-----+----+ Flink SQL\u0026gt; SELECT L.num as L_Num, L.id as L_Id, R.num as R_Num, R.id as R_Id, COALESCE(L.window_start, R.window_start) as window_start, COALESCE(L.window_end, R.window_end) as window_end FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L FULL JOIN ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R ON L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end; +-------+------+-------+------+------------------+------------------+ | L_Num | L_Id | R_Num | R_Id | window_start | window_end | +-------+------+-------+------+------------------+------------------+ | 1 | L1 | null | null | 2020-04-15 12:00 | 2020-04-15 12:05 | | null | null | 2 | R2 | 2020-04-15 12:00 | 2020-04-15 12:05 | | 3 | L3 | 3 | R3 | 2020-04-15 12:00 | 2020-04-15 12:05 | | 2 | L2 | null | null | 2020-04-15 12:05 | 2020-04-15 12:10 | | null | null | 4 | R4 | 2020-04-15 12:05 | 2020-04-15 12:10 | +-------+------+-------+------+------------------+------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
SEMI # Semi Window Joins returns a row from one left record if there is at least one matching row on the right side within the common window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE L.num IN ( SELECT num FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:03 | 3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE EXISTS ( SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:03 | 3 | L3 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
ANTI # Anti Window Joins are the obverse of the Inner Window Join: they contain all of the unjoined rows within each common window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE L.num NOT IN ( SELECT num FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:02 | 1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | | 2020-04-15 12:06 | 2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE LeftTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) L WHERE NOT EXISTS ( SELECT * FROM ( SELECT * FROM TABLE(TUMBLE(TABLE RightTable, DESCRIPTOR(row_time), INTERVAL \u0026#39;5\u0026#39; MINUTES)) ) R WHERE L.num = R.num AND L.window_start = R.window_start AND L.window_end = R.window_end); +------------------+-----+----+------------------+------------------+-------------------------+ | row_time | num | id | window_start | window_end | window_time | +------------------+-----+----+------------------+------------------+-------------------------+ | 2020-04-15 12:02 | 1 | L1 | 2020-04-15 12:00 | 2020-04-15 12:05 | 2020-04-15 12:04:59.999 | | 2020-04-15 12:06 | 2 | L2 | 2020-04-15 12:05 | 2020-04-15 12:10 | 2020-04-15 12:09:59.999 | +------------------+-----+----+------------------+------------------+-------------------------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Limitation on Join clause # Currently, The window join requires the join on condition contains window starts equality of input tables and window ends equality of input tables. In the future, we can also simplify the join on clause to only include the window start equality if the windowing TVF is TUMBLE or HOP.
Limitation on windowing TVFs of inputs # Currently, the windowing TVFs must be the same of left and right inputs. This can be extended in the future, for example, tumbling windows join sliding windows with the same window size.
Limitation on Window Join which follows after Windowing TVFs directly # Currently, if Window Join follows after Windowing TVF, the Windowing TVF has to be with Tumble Windows, Hop Windows or Cumulate Windows instead of Session windows.
Back to top
`}),e.add({id:222,href:"/flink/flink-docs-master/zh/docs/dev/configuration/advanced/",title:"高级配置",section:"项目配置",content:` 高级配置主题 # Flink 依赖剖析 # Flink 自身由一组类和依赖项组成，这些共同构成了 Flink 运行时的核心，在 Flink 应用程序启动时必须存在，会提供诸如通信协调、网络管理、检查点、容错、API、算子（如窗口）、资源管理等领域的服务。
这些核心类和依赖项都打包在 flink-dist.jar，可以在下载的发行版 /lib 文件夹中找到，也是 Flink 容器镜像的基础部分。您可以将其近似地看作是包含 String 和 List 等公用类的 Java 核心库。
为了保持核心依赖项尽可能小并避免依赖冲突，Flink Core Dependencies 不包含任何连接器或库（如 CEP、SQL、ML），以避免在类路径中有过多的类和依赖项。
Flink 发行版的 /lib 目录里还有包括常用模块在内的各种 JAR 文件，例如 执行 Table 作业的必需模块 、一组连接器和 format。默认情况下会自动加载，若要禁止加载只需将它们从 classpath 中的 /lib 目录中删除即可。
Flink 还在 /opt 文件夹下提供了额外的可选依赖项，可以通过移动这些 JAR 文件到 /lib 目录来启用这些依赖项。
有关类加载的更多细节，请查阅 Flink 类加载。
Scala 版本 # 不同的 Scala 版本二进制不兼容，所有（传递地）依赖于 Scala 的 Flink 依赖项都以它们构建的 Scala 版本为后缀（如 flink-streaming-scala_2.12）。
如果您只使用 Flink 的 Java API，您可以使用任何 Scala 版本。如果您使用 Flink 的 Scala API，则需要选择与应用程序的 Scala 匹配的 Scala 版本。
有关如何为特定 Scala 版本构建 Flink 的细节，请查阅构建指南。
2.12.8 之后的 Scala 版本与之前的 2.12.x 版本二进制不兼容，使 Flink 项目无法将其 2.12.x 版本直接升级到 2.12.8 以上。您可以按照构建指南在本地为更高版本的 Scala 构建 Flink 。为此，您需要在构建时添加 -Djapicmp.skip 以跳过二进制兼容性检查。
有关更多细节，请查阅 Scala 2.12.8 版本说明。相关部分指出：
第二项修改是二进制不兼容的：2.12.8 编译器忽略了由更早版本的 2.12 编译器生成的某些方法。然而我们相信这些方法永远不会被使用，现有的编译代码仍可工作。有关更多详细信息，请查阅pull request 描述。
Table 依赖剖析 # Flink 发行版默认包含执行 Flink SQL 任务的必要 JAR 文件（位于 /lib 目录），主要有：
flink-table-api-java-uber-1.16-SNAPSHOT.jar → 包含所有的 Java API； flink-table-runtime-1.16-SNAPSHOT.jar → 包含 Table 运行时; flink-table-planner-loader-1.16-SNAPSHOT.jar → 包含查询计划器。 以前，这些 JAR 都打包进了 flink-table.jar，自从 Flink 1.15 开始，已将其划分成三个 JAR，以允许用户使用 flink-table-planner-loader-1.16-SNAPSHOT.jar 充当 flink-table-planner_2.12-1.16-SNAPSHOT.jar。 虽然 Table Java API 内置于发行版中，但默认情况下不包含 Table Scala API。在 Flink Scala API 中使用格式和连接器时，您需要手动下载这些 JAR 包并将其放到发行版的 /lib 文件夹中（推荐），或者将它们打包为 Flink SQL 作业的 uber/fat JAR 包中的依赖项。
有关更多细节，请查阅如何连接外部系统。
Table Planner 和 Table Planner 加载器 # 从 Flink 1.15 开始，发行版包含两个 planner:
flink-table-planner_2.12-1.16-SNAPSHOT.jar, 位于 /opt 目录, 包含查询计划器； flink-table-planner-loader-1.16-SNAPSHOT.jar, 位于 /lib 目录默认被加载, 包含隐藏在单独的 classpath 里的查询计划器 (您无法直接使用 io.apache.flink.table.planner 包)。 这两个 planner JAR 文件的代码功能相同，但打包方式不同。若使用第一个文件，您必须使用与其相同版本的 Scala；若使用第二个，由于 Scala 已经被打包进该文件里，您不需要考虑 Scala 版本问题。
默认情况下，发行版使用 flink-table-planner-loader。如果想使用内部查询计划器，您可以换掉 JAR 包（拷贝 flink-table-planner_2.12.jar 并复制到发行版的 /lib 目录）。请注意，此时会被限制用于 Flink 发行版的 Scala 版本。
这两个 planner 无法同时存在于 classpath，如果您在 /lib 目录同时加载他们，Table 任务将会失败。 在即将发布的 Flink 版本中，我们将停止在 Flink 发行版中发布 flink-table-planner_2.12 组件。我们强烈建议迁移您的作业/自定义连接器/格式以使用前述 API 模块，而不依赖此内部 planner。如果您需要 planner 中尚未被 API 模块暴露的一些功能，请与社区讨论。 Hadoop 依赖 # 一般规则： 没有必要直接添加 Hadoop 依赖到您的应用程序里，唯一的例外是您通过 Hadoop 兼容 使用已有的 Hadoop 读写 format。
如果您想将 Flink 与 Hadoop 一起使用，您需要有一个包含 Hadoop 依赖项的 Flink 系统，而不是添加 Hadoop 作为应用程序依赖项。换句话说，Hadoop 必须是 Flink 系统本身的依赖，而不是用户代码的依赖。Flink 将使用 HADOOP_CLASSPATH 环境变量指定 Hadoop 依赖项，可以这样设置：
export HADOOP_CLASSPATH=\`hadoop classpath\` 这样设计有两个主要原因：
一些 Hadoop 交互可能在用户应用程序启动之前就发生在 Flink 内核。其中包括为检查点配置 HDFS、通过 Hadoop 的 Kerberos 令牌进行身份验证或在 YARN 上部署；
Flink 的反向类加载方式在核心依赖项中隐藏了许多传递依赖项。这不仅适用于 Flink 自己的核心依赖项，也适用于已有的 Hadoop 依赖项。这样，应用程序可以使用相同依赖项的不同版本，而不会遇到依赖项冲突。当依赖树变得非常大时，这非常有用。
如果您在 IDE 内开发或测试期间需要 Hadoop 依赖项（比如用于 HDFS 访问），应该限定这些依赖项的使用范围（如 test 或 provided）。
`}),e.add({id:223,href:"/flink/flink-docs-master/zh/docs/internals/",title:"内幕",section:"Docs",content:""}),e.add({id:224,href:"/flink/flink-docs-master/zh/docs/ops/upgrading/",title:"升级应用程序和 Flink 版本",section:"Operations",content:` 升级应用程序和 Flink 版本 # Flink DataStream 程序通常设计为长时间运行，例如数周、数月甚至数年。与所有长时间运行的服务一样，Flink 流式应用程序需要维护，包括修复错误、实施改进或将应用程序迁移到更高版本的 Flink 集群。
本文档介绍了如何更新 Flink 流式应用程序以及如何将正在运行的流式应用程序迁移到不同的 Flink 集群。
重启流式应用程序 # 升级流式应用程序或将应用程序迁移到不同集群的操作线基于 Flink 的 Savepoint 功能。Savepoint 是应用程序在特定时间点的状态的一致快照。 有两种方法可以从正在运行的流应用程序中获取 savepoint。
获取 Savepoint 并继续处理。 \u0026gt; ./bin/flink savepoint \u0026lt;jobID\u0026gt; [ Savepoint 的路径] 建议定期获取 Savepoint ，以便能够从之前的时间点重新启动应用程序。
作获取 Savepoint 并停止应用程序。 \u0026gt; ./bin/flink cancel -s [ Savepoint 的路径] \u0026lt;jobID\u0026gt; 这意味着应用程序在 Savepoint 完成后立即取消，即在 Savepoint 之后不进行其他 checkpoint。
给定从应用程序获取的 Savepoint ，可以从该 Savepoint 启动相同或兼容的应用程序（请参阅下面的 应用程序状态兼容性 部分）。从 Savepoint 启动应用程序意味着其算子的状态被初始化为 Savepoint 中保存的算子状态。这是通过使用 Savepoint 启动应用程序来完成的。
\u0026gt; ./bin/flink run -d -s [ Savepoint 的路径] ~/application.jar 已启动应用程序的算子在获取 Savepoint 时使用原始应用程序（即获取 Savepoint 的应用程序）的算子状态进行初始化。启动的应用程序从此时开始继续处理。
Note: 即使 Flink 始终如一地恢复应用程序的状态，它也无法恢复对外部系统的写入。如果您从未停止应用程序的 Savepoint 恢复，这可能是一个问题。在这种情况下，应用程序可能在获取 Savepoint 后发出了数据。重新启动的应用程序可能（取决于您是否更改了应用程序逻辑）再次发出相同的数据。根据 SinkFunction 和存储系统，此行为的确切效果可能会有很大不同。在向 Cassandra 等键值存储进行幂等写入的情况下，发出两次的数据可能没问题，但在追加到 Kafka 等持久日志的情况下会出现问题。在任何情况下，您都应该仔细检查和测试重新启动的应用程序的行为。
应用程序状态兼容性 # 当升级应用程序以修复错误或改进应用程序时，通常目标是在保留其状态的同时替换正在运行的应用程序的应用程序逻辑。我们通过从原始应用程序获取的 Savepoint 启动升级的应用程序来做到这一点。但是，这只有在两个应用程序状态兼容的情况下才有效，这意味着升级应用程序的算子能够使用原始应用程序的算子的状态初始化他们的状态。
在本节中，我们将讨论如何修改应用程序以保持状态兼容。
DataStream API # 匹配算子状态 # 当应用程序从 Savepoint 重新启动时，Flink 会将 Savepoint 中存储的算子状态与已启动应用程序的有状态算子进行匹配。匹配是基于算子 ID 完成的，算子 ID 也存储在 Savepoint 中。每个算子都有一个默认 ID，该 ID 源自算子在应用程序算子拓扑中的位置。因此，未修改的应用程序始终可以从其自己的 Savepoint 之一重新启动。但是，如果应用程序被修改，运营商的默认 ID 可能会发生变化。因此，只有明确指定了算子 ID，才能从 Savepoint 启动修改后的应用程序。为算子分配 ID 非常简单，使用 uid(String) 方法完成，如下所示：
val mappedEvents: DataStream[(Int, Long)] = events .map(new MyStatefulMapFunc()).uid(\u0026#34;mapper-1\u0026#34;) Note: 由于 Savepoint 中存储的算子 ID 和要启动的应用程序中的算子 ID 必须相等，因此强烈建议为将来可能升级的应用程序的所有算子分配唯一 ID。此建议适用于所有算子，即有和没有明确声明算子状态的算子，因为某些算子具有用户不可见的内部状态。在没有分配算子 ID 的情况下升级应用程序要困难得多，并且只能通过使用 setUidHash() 方法的低级解决方法来实现。
Important: 从 1.3.x 开始，链中的算子也是一样。
默认情况下， Savepoint 中存储的所有状态都必须与启动应用程序的算子匹配。但是，当从 Savepoint 启动应用程序时，用户可以明确同意跳过（从而丢弃）无法与算子匹配的状态。在 Savepoint 中未找到状态的有状态算子将使用其默认状态进行初始化。用户可以通过调用 ExecutionConfig#disableAutoGeneratedUIDs 来强制执行最佳实践，如果任何算子不包含自定义唯一 ID，则作业提交将失败。
有状态的算子和用户函数 # 升级应用程序时，用户功能和算子可以自由修改，有一个限制。无法更改算子状态的数据类型。这很重要，因为 Savepoint 的状态（当前）在加载到算子之前不能转换为不同的数据类型。因此，在升级应用程序时更改算子状态的数据类型会破坏应用程序状态的一致性，并防止升级后的应用程序从 Savepoint 重新启动。
算子状态可以是用户定义的或内部的。
用户自定义算子状态： 在具有用户定义算子状态的函数中，状态的类型由用户明确定义。尽管无法更改算子状态的数据类型，但克服此限制的解决方法可以是定义具有不同数据类型的第二个状态，并实现将状态从原始状态迁移到新状态的逻辑。这种方法需要良好的迁移策略和对 key-partitioned state 行为的深刻理解。
内部算子状态： 诸如窗口或连接算子之类的算子持有不向用户公开的内部算子状态。对于这些算子，内部状态的数据类型取决于算子的输入或输出类型。因此，更改相应的输入或输出类型会破坏应用程序状态的一致性并阻止升级。下表列出了具有内部状态的算子，并显示了状态数据类型与其输入和输出类型的关系。对于应用于 Keyed Stream 的算子，键类型 (KEY) 也始终是状态数据类型的一部分。
算子 内部算子状态的数据类型 ReduceFunction[IOT] IOT (输入输出类型) [, KEY] WindowFunction[IT, OT, KEY, WINDOW] IT (输入类型), KEY AllWindowFunction[IT, OT, WINDOW] IT (输入类型) JoinFunction[IT1, IT2, OT] IT1, IT2 (1. 和 2. 输入的类型), KEY CoGroupFunction[IT1, IT2, OT] IT1, IT2 (1. 和 2. 输入的类型), KEY 内置聚合 (sum, min, max, minBy, maxBy) 输入类型 [, KEY] 应用拓扑 # 除了改变一个或多个现有算子的逻辑外，还可以通过改变应用程序的拓扑结构来升级应用程序，即添加或删除算子、改变算子的并行度或修改算子链接行为。
通过更改拓扑来升级应用程序时，需要考虑一些事项以保持应用程序状态的一致性。
添加或删除无状态算子： 除非是以下情况之一，否则这没有问题。 添加有状态算子： 除非它接管另一个算子的状态，否则算子的状态将使用默认状态进行初始化。 移除一个有状态的算子： 除非另一个算子接手，否则已移除算子的状态将丢失。启动升级后的应用程序时，您必须明确同意丢弃该状态。 改变算子的输入输出类型： 在具有内部状态的算子之前或之后添加新算子时，您必须确保不修改有状态算子的输入或输出类型以保留内部算子状态的数据类型（详见上文）。 更改算子链接： 算子可以链接在一起以提高性能。从 1.3.x 之后的 Savepoint 恢复时，可以在保持状态一致性的同时修改链。有可能会破坏链，从而将有状态的算子移出链。还可以将新的或现有的有状态算子附加或注入到链中，或者修改链中的算子顺序。但是，当将 Savepoint 升级到 1.3.x 时，最重要的是拓扑在链接方面没有改变。应为链中的所有算子分配一个 ID，如上面 匹配算子状态 部分所述。 Table API \u0026amp; SQL # 由于 Table API 和 SQL 程序的声明性，底层算子拓扑和状态表示主要由 表规划器确定和优化。
请注意，查询和 Flink 版本的任何更改都可能导致状态不兼容。每个新的大-小 Flink 版本（例如 1.12 到 1.13）都 可能引入新的优化器规则或更专业的运行时算子来改变执行计划。 然而，社区试图保持补丁版本的状态兼容 （例如 1.13.1 到 1.13.2）。
有关详细信息， 请参阅 table state management section。
升级 Flink 框架版本 # 本节介绍跨版本升级 Flink 以及在版本之间迁移作业的一般方法。
简而言之，此过程包括 2 个基本步骤：
在以前的旧 Flink 版本中为您要迁移的作业创建一个 Savepoint 。 在新的 Flink 版本下从之前使用的 Savepoint 恢复您的工作。 除了这两个基本步骤之外，还可能需要一些额外的步骤，具体取决于您想要更改 Flink 版本的方式。 在本指南中，我们区分了两种跨 Flink 版本升级的方法： 就地升级和卷影副本升级。
对于就地更新，在获取 Savepoint 后，您需要：
停止/取消所有正在运行的作业。 关闭运行旧 Flink 版本的集群。 将 Flink 升级到集群上的较新版本。 在新版本下重启集群。 对于卷影副本，您需要：
在从 Savepoint 恢复之前，除了旧的 Flink 安装之外，设置新的 Flink 版本的新安装。 使用新的 Flink 安装从 Savepoint 恢复。 如果一切正常，停止并关闭旧的 Flink 集群。 在下文中，我们将首先介绍成功进行作业迁移的先决条件， 然后详细介绍我们之前概述的步骤。
前提条件 # 在开始迁移之前，请检查您尝试迁移的作业是否遵循 savepoints 的最佳实践。
特别是，我们建议您检查是否为您的工作中的算子设置了明确的 uid。
这是一个软前提，如果您忘记分配 uid，恢复应该仍然有效。 如果您遇到这种情况不起作用，您可以调用 setUidHash(String hash) 手动将以前 Flink 版本中生成的遗留顶点 ID 添加到您的作业中。对于每个算子（在算子链中：仅头部算子） ，您必须分配 32 个字符的十六进制字符串， 表示您可以在 web ui 或日志中看到的算子的哈希值。
除了算子 uid，目前作业迁移还有两个硬性前提条件会导致迁移失败：
RocksDB不支持迁移 semi-asynchronous 模式 Checkpoint 的状态。如果您的旧作业使用此模式，您仍然可以将作业更改为使用 fully-asynchronous 模式，然后再将 Savepoint 用作迁移的基础。
另一个重要的先决条件是所有 Savepoint 数据必须可以从新安装的相同（绝对）路径下访问。 这还包括访问从 Savepoint 文件内部引用的任何其他文件 （状态后端快照的输出）， 包括但不限于使用 State Processor API。
第 1 步：使用 Savepoint 停止现有作业 # 版本迁移的第一个主要步骤是获取 Savepoint 并停止 在旧 Flink 版本上运行的作业。
您可以使用以下命令执行此操作：
\$ bin/flink stop [--savepointPath :savepointPath] :jobId 更多详情，请阅读 savepoint documentation.
第 2 步：将您的集群更新到新的 Flink 版本。 # 在这一步中，我们更新集群的框架版本。 基本上意味着用新版本替换 Flink 安装的内容。 此步骤可能取决于您在集群中运行 Flink 的方式（例如独立运行，\u0026hellip;）。
如果您对在集群中安装 Flink 不熟悉，请阅读 deployment and cluster setup documentation.
第 3 步：从 Savepoint 恢复新 Flink 版本下的作业。 # 作为作业迁移的最后一步，您从上面在更新的集群上采取的 Savepoint 恢复。 您可以使用以下命令执行此操作：
\$ bin/flink run -s :savepointPath [:runArgs] 更多详情，请查看savepoint documentation.
兼容性速查表 # Savepoint 在 Flink 版本之间是兼容的，如下表所示：
创建于 \\ 恢复于 1.1.x 1.2.x 1.3.x 1.4.x 1.5.x 1.6.x 1.7.x 1.8.x 1.9.x 1.10.x 1.11.x 1.12.x 1.13.x 1.14.x 1.15.x 限制 1.1.x O O O 从 Flink 1.1.x 迁移到 1.2.x+ 的作业的最大并行度目前固定为作业的并行度。 这意味着迁移后无法增加并行度。 在未来的错误修复版本中可能会删除此限制。 1.2.x O O O O O O O O O O O O O O 从 Flink 1.2.x 迁移到 Flink 1.3.x+ 时，不支持同时更改并行度。 迁移到 Flink 1.3.x+ 后， 用户必须先获取一个 Savepoint ，然后再更改并行度。 为 CEP 应用程序创建的 Savepoint 无法在 1.4.x+ 中恢复。 Flink 1.2 中包含 Scala TraversableSerializer 的 Savepoint 不再与 Flink 1.8 兼容， 因为此序列化程序中的更新。 您可以通过首先升级到 Flink 1.3 和 Flink 1.7 之间的版本，然后更新到 Flink 1.8 来绕过这个限制。 1.3.x O O O O O O O O O O O O O M如果 Savepoint 包含 Scala 案例类，则从 Flink 1.3.0 迁移到 Flink 1.4.[0,1] 将失败。用户必须直接迁移到 1.4.2+。 1.4.x O O O O O O O O O O O O 1.5.x O O O O O O O O O O O 在 1.6.x 到 1.6.2 和 1.7.0 版本中恢复使用 1.5.x 创建的广播状态存在一个已知问题：FLINK-11087. 升级到 1.6.x 或 1.7.x 系列的用户需要 直接迁移到次要版本分别高于 1.6.2 和 1.7.0。 1.6.x O O O O O O O O O O 1.7.x O O O O O O O O O 1.8.x O O O O O O O O 1.9.x O O O O O O O 1.10.x O O O O O O 1.11.x O O O O O 1.12.x O O O O 1.13.x O O O Don't upgrade from 1.12.x to 1.13.x with an unaligned checkpoint. Please use a savepoint for migrating. 1.14.x O O 1.15.x O For Table API: 1.15.0 and 1.15.1 generated non-deterministic UIDs for operators that make it difficult/impossible to restore state or upgrade to next patch version. A new table.exec.uid.generation config option (with correct default behavior) disables setting a UID for new pipelines from non-compiled plans. Existing pipelines can set table.exec.uid.generation=ALWAYS if the 1.15.0/1 behavior was acceptable due to a stable environment. See FLINK-28861 for more information. Back to top
`}),e.add({id:225,href:"/flink/flink-docs-master/zh/docs/connectors/dataset/",title:"DataSet Connectors",section:"Connectors",content:` DataSet Connectors # TOC Reading from and writing to file systems # The Apache Flink project supports multiple file systems that can be used as backing stores for input and output connectors.
Connecting to other systems using Input/OutputFormat wrappers for Hadoop # Apache Flink allows users to access many different systems as data sources or sinks. The system is designed for very easy extensibility. Similar to Apache Hadoop, Flink has the concept of so called InputFormats and OutputFormats.
One implementation of these InputFormats is the HadoopInputFormat. This is a wrapper that allows users to use all existing Hadoop input formats with Flink.
This section shows some examples for connecting Flink to other systems. Read more about Hadoop compatibility in Flink.
Avro support in Flink # Flink has extensive built-in support for Apache Avro. This allows to easily read from Avro files with Flink. Also, the serialization framework of Flink is able to handle classes generated from Avro schemas. Be sure to include the Flink Avro dependency to the pom.xml of your project.
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; In order to read data from an Avro file, you have to specify an AvroInputFormat.
Example:
AvroInputFormat\u0026lt;User\u0026gt; users = new AvroInputFormat\u0026lt;User\u0026gt;(in, User.class); DataSet\u0026lt;User\u0026gt; usersDS = env.createInput(users); Note that User is a POJO generated by Avro. Flink also allows to perform string-based key selection of these POJOs. For example:
usersDS.groupBy(\u0026#34;name\u0026#34;) Note that using the GenericData.Record type is possible with Flink, but not recommended. Since the record contains the full schema, its very data intensive and thus probably slow to use.
Flink\u0026rsquo;s POJO field selection also works with POJOs generated from Avro. However, the usage is only possible if the field types are written correctly to the generated class. If a field is of type Object you can not use the field as a join or grouping key. Specifying a field in Avro like this {\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;double\u0026quot;}, works fine, however specifying it as a UNION-type with only one field ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;double\u0026quot;]},) will generate a field of type Object. Note that specifying nullable types ({\u0026quot;name\u0026quot;: \u0026quot;type_double_test\u0026quot;, \u0026quot;type\u0026quot;: [\u0026quot;null\u0026quot;, \u0026quot;double\u0026quot;]},) is possible!
Access Microsoft Azure Table Storage # Note: This example works starting from Flink 0.6-incubating
This example is using the HadoopInputFormat wrapper to use an existing Hadoop input format implementation for accessing Azure\u0026rsquo;s Table Storage.
Download and compile the azure-tables-hadoop project. The input format developed by the project is not yet available in Maven Central, therefore, we have to build the project ourselves. Execute the following commands: git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install Setup a new Flink project using the quickstarts: curl https://flink.apache.org/q/quickstart.sh | bash Add the following dependencies (in the \u0026lt;dependencies\u0026gt; section) to your pom.xml file: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-hadoop-compatibility_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.microsoft.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;microsoft-hadoop-azure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; flink-hadoop-compatibility is a Flink package that provides the Hadoop input format wrappers. microsoft-hadoop-azure is adding the project we\u0026rsquo;ve build before to our project.
The project is now prepared for starting to code. We recommend to import the project into an IDE, such as Eclipse or IntelliJ. (Import as a Maven project!). Browse to the code of the Job.java file. Its an empty skeleton for a Flink job.
Paste the following code into it:
import java.util.Map; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.DataSet; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import com.microsoft.hadoop.azure.AzureTableConfiguration; import com.microsoft.hadoop.azure.AzureTableInputFormat; import com.microsoft.hadoop.azure.WritableEntity; import com.microsoft.windowsazure.storage.table.EntityProperty; public class AzureTableExample { public static void main(String[] args) throws Exception { // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // create a AzureTableInputFormat, using a Hadoop input format wrapper HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt; hdIf = new HadoopInputFormat\u0026lt;Text, WritableEntity\u0026gt;(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job()); // set the Account URI, something like: https://apacheflink.table.core.windows.net hdIf.getConfiguration().set(AzureTableConfiguration.Keys.ACCOUNT_URI.getKey(), \u0026#34;TODO\u0026#34;); // set the secret storage key here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), \u0026#34;TODO\u0026#34;); // set the table name here hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), \u0026#34;TODO\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt; input = env.createInput(hdIf); // a little example how to use the data in a mapper. DataSet\u0026lt;String\u0026gt; fin = input.map(new MapFunction\u0026lt;Tuple2\u0026lt;Text,WritableEntity\u0026gt;, String\u0026gt;() { @Override public String map(Tuple2\u0026lt;Text, WritableEntity\u0026gt; arg0) throws Exception { System.err.println(\u0026#34;--------------------------------\\nKey = \u0026#34;+arg0.f0); WritableEntity we = arg0.f1; for(Map.Entry\u0026lt;String, EntityProperty\u0026gt; prop : we.getProperties().entrySet()) { System.err.println(\u0026#34;key=\u0026#34;+prop.getKey() + \u0026#34; ; value (asString)=\u0026#34;+prop.getValue().getValueAsString()); } return arg0.f0.toString(); } }); // emit result (this works only locally) fin.print(); // execute program env.execute(\u0026#34;Azure Example\u0026#34;); } } The example shows how to access an Azure table and turn data into Flink\u0026rsquo;s DataSet (more specifically, the type of the set is DataSet\u0026lt;Tuple2\u0026lt;Text, WritableEntity\u0026gt;\u0026gt;). With the DataSet, you can apply all known transformations to the DataSet.
Access MongoDB # This GitHub repository documents how to use MongoDB with Apache Flink (starting from 0.7-incubating).
Back to top
`}),e.add({id:226,href:"/flink/flink-docs-master/zh/docs/connectors/table/formats/raw/",title:"Raw",section:"Formats",content:` Raw Format # Format: Serialization Schema Format: Deserialization Schema
Raw format 允许读写原始（基于字节）值作为单个列。
注意: 这种格式将 null 值编码成 byte[] 类型的 null。这样在 upsert-kafka 中使用时可能会有限制，因为 upsert-kafka 将 null 值视为 墓碑消息（在键上删除）。因此，如果该字段可能具有 null 值，我们建议避免使用 upsert-kafka 连接器和 raw format 作为 value.format。
Raw format 连接器是内置的。
示例 # 例如，你可能在 Kafka 中具有原始日志数据，并希望使用 Flink SQL 读取和分析此类数据。
47.29.201.179 - - [28/Feb/2019:13:17:10 +0000] \u0026#34;GET /?p=1 HTTP/2.0\u0026#34; 200 5316 \u0026#34;https://domain.com/?p=1\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\u0026#34; \u0026#34;2.75\u0026#34; 下面的代码创建了一张表，使用 raw format 以 UTF-8 编码的形式从中读取（也可以写入）底层的 Kafka topic 作为匿名字符串值：
CREATE TABLE nginx_log ( log STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;nginx_log\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;localhost:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;testGroup\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;raw\u0026#39; ) 然后，你可以将原始数据读取为纯字符串，之后使用用户自定义函数将其分为多个字段进行进一步分析。例如 示例中的 my_split。
SELECT t.hostname, t.datetime, t.url, t.browser, ... FROM( SELECT my_split(log) as t FROM nginx_log ); 相对应的，你也可以将一个 STRING 类型的列以 UTF-8 编码的匿名字符串值写入 Kafka topic。
Format 参数 # 参数 是否必选 默认值 类型 描述 format 必选 (none) String 指定要使用的格式, 这里应该是 'raw'。 raw.charset 可选 UTF-8 String 指定字符集来编码文本字符串。 raw.endianness 可选 big-endian String 指定字节序来编码数字值的字节。有效值为'big-endian'和'little-endian'。 更多细节可查阅 字节序。 数据类型映射 # 下表详细说明了这种格式支持的 SQL 类型，包括用于编码和解码的序列化类和反序列化类的详细信息。
Flink SQL 类型 值 CHAR / VARCHAR / STRING UTF-8（默认）编码的文本字符串。
编码字符集可以通过 'raw.charset' 进行配置。 BINARY / VARBINARY / BYTES 字节序列本身。 BOOLEAN 表示布尔值的单个字节，0表示 false, 1 表示 true。 TINYINT 有符号数字值的单个字节。 SMALLINT 采用big-endian（默认）编码的两个字节。
字节序可以通过 'raw.endianness' 配置。 INT 采用 big-endian （默认）编码的四个字节。
字节序可以通过 'raw.endianness' 配置。 BIGINT 采用 big-endian （默认）编码的八个字节。
字节序可以通过 'raw.endianness' 配置。 FLOAT 采用 IEEE 754 格式和 big-endian （默认）编码的四个字节。
字节序可以通过 'raw.endianness' 配置。 DOUBLE 采用 IEEE 754 格式和 big-endian （默认）编码的八个字节。
字节序可以通过 'raw.endianness' 配置。 RAW 通过 RAW 类型的底层 TypeSerializer 序列化的字节序列。 `}),e.add({id:227,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/show/",title:"SHOW 语句",section:"SQL",content:" SHOW 语句 # SHOW 语句用于列出其相应父对象中的对象，例如 catalog、database、table 和 view、column、function 和 module。有关详细信息和其他选项，请参见各个命令。\nSHOW CREATE 语句用于打印给定对象的创建 DDL 语句。当前的 SHOW CREATE 语句仅在打印给定表和视图的 DDL 语句时可用。\n目前 Flink SQL 支持下列 SHOW 语句：\nSHOW CATALOGS SHOW CURRENT CATALOG SHOW DATABASES SHOW CURRENT DATABASE SHOW TABLES SHOW CREATE TABLE SHOW COLUMNS SHOW VIEWS SHOW CREATE VIEW SHOW FUNCTIONS SHOW MODULES SHOW FULL MODULES SHOW JARS 执行 SHOW 语句 # Java 可以使用 TableEnvironment 中的 executeSql() 方法执行 SHOW 语句。 若 SHOW 操作执行成功，executeSql() 方法返回所有对象，否则会抛出异常。\n以下的例子展示了如何在 TableEnvironment 中执行一个 SHOW 语句。\nScala 可以使用 TableEnvironment 中的 executeSql() 方法执行 SHOW 语句。 若 SHOW 操作执行成功，executeSql() 方法返回所有对象，否则会抛出异常。\n以下的例子展示了如何在 TableEnvironment 中执行一个 SHOW 语句。\nPython 可以使用 TableEnvironment 中的 execute_sql() 方法执行 SHOW 语句。 若 SHOW 操作执行成功，execute_sql() 方法返回所有对象，否则会抛出异常。\n以下的例子展示了如何在 TableEnvironment 中执行一个 SHOW 语句。\nSQL CLI 可以在 SQL CLI 中执行 SHOW 语句。\n以下的例子展示了如何在 SQL CLI 中执行一个 SHOW 语句。\nJava StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // show catalogs tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print(); // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // +-----------------+ // show current catalog tEnv.executeSql(\u0026#34;SHOW CURRENT CATALOG\u0026#34;).print(); // +----------------------+ // | current catalog name | // +----------------------+ // | default_catalog | // +----------------------+ // show databases tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print(); // +------------------+ // | database name | // +------------------+ // | default_database | // +------------------+ // show current database tEnv.executeSql(\u0026#34;SHOW CURRENT DATABASE\u0026#34;).print(); // +-----------------------+ // | current database name | // +-----------------------+ // | default_database | // +-----------------------+ // create a table tEnv.executeSql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;); // show tables tEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print(); // +------------+ // | table name | // +------------+ // | my_table | // +------------+ // show create table tEnv.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print(); // CREATE TABLE `default_catalog`.`default_db`.`my_table` ( // ... // ) WITH ( // ... // ) // show columns tEnv.executeSql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print(); // +--------+-------+------+-----+--------+-----------+ // | name | type | null | key | extras | watermark | // +--------+-------+------+-----+--------+-----------+ // | field2 | BYTES | true | | | | // +--------+-------+------+-----+--------+-----------+ // create a view tEnv.executeSql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;); // show views tEnv.executeSql(\u0026#34;SHOW VIEWS\u0026#34;).print(); // +-----------+ // | view name | // +-----------+ // | my_view | // +-----------+ // show create view tEnv.executeSql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print(); // CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as // SELECT * // FROM `default_catalog`.`default_database`.`my_table` // show functions tEnv.executeSql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print(); // +---------------+ // | function name | // +---------------+ // | mod | // | sha256 | // | ... | // +---------------+ // create a user defined function tEnv.executeSql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;); // show user defined functions tEnv.executeSql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print(); // +---------------+ // | function name | // +---------------+ // | f1 | // | ... | // +---------------+ // show modules tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ // show full modules tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | core | true | // | hive | false | // +-------------+-------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // show catalogs tEnv.executeSql(\u0026#34;SHOW CATALOGS\u0026#34;).print() // +-----------------+ // | catalog name | // +-----------------+ // | default_catalog | // +-----------------+ // show databases tEnv.executeSql(\u0026#34;SHOW DATABASES\u0026#34;).print() // +------------------+ // | database name | // +------------------+ // | default_database | // +------------------+ // create a table tEnv.executeSql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;) // show tables tEnv.executeSql(\u0026#34;SHOW TABLES\u0026#34;).print() // +------------+ // | table name | // +------------+ // | my_table | // +------------+ // show create table tEnv.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print() // CREATE TABLE `default_catalog`.`default_db`.`my_table` ( // ... // ) WITH ( // ... // ) // show columns tEnv.executeSql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print() // +--------+-------+------+-----+--------+-----------+ // | name | type | null | key | extras | watermark | // +--------+-------+------+-----+--------+-----------+ // | field2 | BYTES | true | | | | // +--------+-------+------+-----+--------+-----------+ // create a view tEnv.executeSql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;) // show views tEnv.executeSql(\u0026#34;SHOW VIEWS\u0026#34;).print() // +-----------+ // | view name | // +-----------+ // | my_view | // +-----------+ // show create view tEnv.executeSql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print(); // CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as // SELECT * // FROM `default_catalog`.`default_database`.`my_table` // show functions tEnv.executeSql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print() // +---------------+ // | function name | // +---------------+ // | mod | // | sha256 | // | ... | // +---------------+ // create a user defined function tEnv.executeSql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;) // show user defined functions tEnv.executeSql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print() // +---------------+ // | function name | // +---------------+ // | f1 | // | ... | // +---------------+ // show modules tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ // show full modules tEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | core | true | // | hive | false | // +-------------+-------+ Python table_env = StreamTableEnvironment.create(...) # show catalogs table_env.execute_sql(\u0026#34;SHOW CATALOGS\u0026#34;).print() # +-----------------+ # | catalog name | # +-----------------+ # | default_catalog | # +-----------------+ # show databases table_env.execute_sql(\u0026#34;SHOW DATABASES\u0026#34;).print() # +------------------+ # | database name | # +------------------+ # | default_database | # +------------------+ # create a table table_env.execute_sql(\u0026#34;CREATE TABLE my_table (...) WITH (...)\u0026#34;) # show tables table_env.execute_sql(\u0026#34;SHOW TABLES\u0026#34;).print() # +------------+ # | table name | # +------------+ # | my_table | # +------------+ # show create table table_env.executeSql(\u0026#34;SHOW CREATE TABLE my_table\u0026#34;).print() # CREATE TABLE `default_catalog`.`default_db`.`my_table` ( # ... # ) WITH ( # ... # ) # show columns table_env.execute_sql(\u0026#34;SHOW COLUMNS FROM my_table LIKE \u0026#39;%f%\u0026#39;\u0026#34;).print() # +--------+-------+------+-----+--------+-----------+ # | name | type | null | key | extras | watermark | # +--------+-------+------+-----+--------+-----------+ # | field2 | BYTES | true | | | | # +--------+-------+------+-----+--------+-----------+ # create a view table_env.execute_sql(\u0026#34;CREATE VIEW my_view AS SELECT * FROM my_table\u0026#34;) # show views table_env.execute_sql(\u0026#34;SHOW VIEWS\u0026#34;).print() # +-----------+ # | view name | # +-----------+ # | my_view | # +-----------+ # show create view table_env.execute_sql(\u0026#34;SHOW CREATE VIEW my_view\u0026#34;).print() # CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as # SELECT * # FROM `default_catalog`.`default_database`.`my_table` # show functions table_env.execute_sql(\u0026#34;SHOW FUNCTIONS\u0026#34;).print() # +---------------+ # | function name | # +---------------+ # | mod | # | sha256 | # | ... | # +---------------+ # create a user defined function table_env.execute_sql(\u0026#34;CREATE FUNCTION f1 AS ...\u0026#34;) # show user defined functions table_env.execute_sql(\u0026#34;SHOW USER FUNCTIONS\u0026#34;).print() # +---------------+ # | function name | # +---------------+ # | f1 | # | ... | # +---------------+ # show modules table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ # show full modules table_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | core | true | # | hive | false | # +-------------+-------+ SQL CLI Flink SQL\u0026gt; SHOW CATALOGS; default_catalog Flink SQL\u0026gt; SHOW DATABASES; default_database Flink SQL\u0026gt; CREATE TABLE my_table (...) WITH (...); [INFO] Table has been created. Flink SQL\u0026gt; SHOW TABLES; my_table Flink SQL\u0026gt; SHOW CREATE TABLE my_table; CREATE TABLE `default_catalog`.`default_db`.`my_table` ( ... ) WITH ( ... ) Flink SQL\u0026gt; SHOW COLUMNS from MyUserTable LIKE \u0026#39;%f%\u0026#39;; +--------+-------+------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +--------+-------+------+-----+--------+-----------+ | field2 | BYTES | true | | | | +--------+-------+------+-----+--------+-----------+ 1 row in set Flink SQL\u0026gt; CREATE VIEW my_view AS SELECT * from my_table; [INFO] View has been created. Flink SQL\u0026gt; SHOW VIEWS; my_view Flink SQL\u0026gt; SHOW CREATE VIEW my_view; CREATE VIEW `default_catalog`.`default_db`.`my_view`(`field1`, `field2`, ...) as SELECT * FROM `default_catalog`.`default_database`.`my_table` Flink SQL\u0026gt; SHOW FUNCTIONS; mod sha256 ... Flink SQL\u0026gt; CREATE FUNCTION f1 AS ...; [INFO] Function has been created. Flink SQL\u0026gt; SHOW USER FUNCTIONS; f1 ... Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | +-------------+ 1 row in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | +-------------+------+ 1 row in set Flink SQL\u0026gt; SHOW JARS; /path/to/addedJar.jar Back to top\nSHOW CATALOGS # SHOW CATALOGS 展示所有的 catalog。\nSHOW CURRENT CATALOG # SHOW CURRENT CATALOG 显示当前正在使用的 catalog。\nSHOW DATABASES # SHOW DATABASES 展示当前 catalog 中所有的 database。\nSHOW CURRENT DATABASE # SHOW CURRENT DATABASE 显示当前正在使用的 database。\nSHOW TABLES # SHOW TABLES [ ( FROM | IN ) [catalog_name.]database_name ] [ [NOT] LIKE \u0026lt;sql_like_pattern\u0026gt; ] 展示指定库的所有表，如果没有指定库则展示当前库的所有表。另外返回的结果能被一个可选的匹配字符串过滤。\nLIKE 根据可选的 LIKE 语句展示给定库中与 \u0026lt;sql_like_pattern\u0026gt; 是否模糊相似的所有表。\nLIKE 子句中 sql 正则式的语法与 MySQL 方言中的语法相同。\n% 匹配任意数量的字符, 也包括0数量字符, \\% 匹配一个 % 字符. _ 只匹配一个字符, \\_ 匹配一个 _ 字符. SHOW TABLES 示例 # 假定在 catalog1 的 db1 库有如下表：\nperson dim 在会话的当前库下有如下表：\nfights orders 显示指定库的所有表。 show tables from db1; -- show tables from catalog1.db1; -- show tables in db1; -- show tables in catalog1.db1; +------------+ | table name | +------------+ | dim | | person | +------------+ 2 rows in set 显示指定库中相似于指定 SQL 正则式的所有表。 show tables from db1 like \u0026#39;%n\u0026#39;; -- show tables from catalog1.db1 like \u0026#39;%n\u0026#39;; -- show tables in db1 like \u0026#39;%n\u0026#39;; -- show tables in catalog1.db1 like \u0026#39;%n\u0026#39;; +------------+ | table name | +------------+ | person | +------------+ 1 row in set 显示指定库中不相似于指定 SQL 正则式的所有表。 show tables from db1 not like \u0026#39;%n\u0026#39;; -- show tables from catalog1.db1 not like \u0026#39;%n\u0026#39;; -- show tables in db1 not like \u0026#39;%n\u0026#39;; -- show tables in catalog1.db1 not like \u0026#39;%n\u0026#39;; +------------+ | table name | +------------+ | dim | +------------+ 1 row in set 显示当前库中的所有表。 show tables; +------------+ | table name | +------------+ | items | | orders | +------------+ 2 rows in set SHOW CREATE TABLE # SHOW CREATE TABLE [catalog_name.][db_name.]table_name 展示创建指定表的 create 语句。\nAttention 目前 SHOW CREATE TABLE 只支持通过 Flink SQL DDL 创建的表。\nSHOW COLUMNS # SHOW COLUMNS ( FROM | IN ) [[catalog_name.]database.]\u0026lt;table_name\u0026gt; [ [NOT] LIKE \u0026lt;sql_like_pattern\u0026gt;] 展示给定表的所有列。\nLIKE 根据可选的 LIKE 语句展示给定表中与 \u0026lt;sql_like_pattern\u0026gt; 是否模糊相似的所有列。\nLIKE 子句中 sql 正则式的语法与 MySQL 方言中的语法相同。\nSHOW COLUMNS 示例 # 假定在 catalog1 catalog 中的 database1 数据库中有名为 orders 的表，其结构如下所示：\n+---------+-----------------------------+-------+-----------+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ 显示指定表中的所有列。 show columns from orders; -- show columns from database1.orders; -- show columns from catalog1.database1.orders; -- show columns in orders; -- show columns in database1.orders; -- show columns in catalog1.database1.orders; +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ | user | BIGINT | false | PRI(user) | | | | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----------+---------------+----------------------------+ 5 rows in set 显示指定表中相似于指定 SQL 正则式的所有列。 show columns from orders like \u0026#39;%r\u0026#39;; -- show columns from database1.orders like \u0026#39;%r\u0026#39;; -- show columns from catalog1.database1.orders like \u0026#39;%r\u0026#39;; -- show columns in orders like \u0026#39;%r\u0026#39;; -- show columns in database1.orders like \u0026#39;%r\u0026#39;; -- show columns in catalog1.database1.orders like \u0026#39;%r\u0026#39;; +------+--------+-------+-----------+--------+-----------+ | name | type | null | key | extras | watermark | +------+--------+-------+-----------+--------+-----------+ | user | BIGINT | false | PRI(user) | | | +------+--------+-------+-----------+--------+-----------+ 1 row in set 显示指定表中不相似于指定 SQL 正则式的所有列。 show columns from orders not like \u0026#39;%_r\u0026#39;; -- show columns from database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns from catalog1.database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns in orders not like \u0026#39;%_r\u0026#39;; -- show columns in database1.orders not like \u0026#39;%_r\u0026#39;; -- show columns in catalog1.database1.orders not like \u0026#39;%_r\u0026#39;; +---------+-----------------------------+-------+-----+---------------+----------------------------+ | name | type | null | key | extras | watermark | +---------+-----------------------------+-------+-----+---------------+----------------------------+ | product | VARCHAR(32) | true | | | | | amount | INT | true | | | | | ts | TIMESTAMP(3) *ROWTIME* | true | | | `ts` - INTERVAL \u0026#39;1\u0026#39; SECOND | | ptime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | AS PROCTIME() | | +---------+-----------------------------+-------+-----+---------------+----------------------------+ 4 rows in set SHOW VIEWS # SHOW VIEWS 展示当前 catalog 和当前 database 中所有的视图。\nSHOW CREATE VIEW # SHOW CREATE VIEW [catalog_name.][db_name.]view_name 展示创建指定视图的 create 语句。\nSHOW FUNCTIONS # SHOW [USER] FUNCTIONS 展示当前 catalog 和当前 database 中所有的 function，包括：系统 function 和用户定义的 function。\nUSER 仅仅展示当前 catalog 和当前 database 中用户定义的 function。\nSHOW MODULES # SHOW [FULL] MODULES 展示当前环境激活的所有 module。\nFULL 展示当前环境加载的所有 module 及激活状态。\nSHOW JARS # SHOW JARS 展示所有通过 ADD JAR 语句加入到 session classloader 中的 jar。\nAttention 当前 SHOW JARS 命令只能在 SQL CLI 中使用。\nBack to top\n"}),e.add({id:228,href:"/flink/flink-docs-master/zh/docs/concepts/glossary/",title:"词汇表",section:"概念透析",content:` 词汇表 # Flink Application Cluster # Flink Application 集群是专用的 Flink Cluster，仅从 Flink Application 执行 Flink Jobs。 Flink Cluster 的寿命与 Flink Application 的寿命有关。
Flink Job Cluster # Flink Job 集群是专用的 Flink Cluster，仅执行一个 Flink Job。 Flink Cluster 的寿命与 Flink Job 的寿命有关。
Flink Cluster # 一般情况下，Flink 集群是由一个 Flink JobManager 和一个或多个 Flink TaskManager 进程组成的分布式系统。
Event # Event 是对应用程序建模的域的状态更改的声明。它可以同时为流或批处理应用程序的 input 和 output，也可以单独是 input 或者 output 中的一种。Event 是特殊类型的 Record。
ExecutionGraph # 见 Physical Graph。
Function # Function 是由用户实现的，并封装了 Flink 程序的应用程序逻辑。大多数 Function 都由相应的 Operator 封装。
Instance # Instance 常用于描述运行时的特定类型(通常是 Operator 或者 Function)的一个具体实例。由于 Apache Flink 主要是用 Java 编写的，所以，这与 Java 中的 Instance 或 Object 的定义相对应。在 Apache Flink 的上下文中，parallel instance 也常用于强调同一 Operator 或者 Function 的多个 instance 以并行的方式运行。
Flink Application # 一个 Flink 应用程序是一个 Java 应用程序，它从 main() 方法（或通过一些其他方式）提交一个或多个 Flink Jobs。 提交 jobs 通常是通过调用 ExecutionEnvironment 的 execute() 方法来完成的。
一个应用程序的作业可以提交给一个长期运行的 Flink Session Cluster，或者提交到一个专用的 Flink Application Cluster，或提交到 Flink Job Cluster。
Flink Job # Flink Job 表示为 runtime 的 logical graph（通常也称为数据流图），通过在 Flink Application 中调用 execute() 方法来创建和提交 。
JobGraph # 见 Logical Graph。
Flink JobManager # Flink JobManager 是 Flink Cluster 的主节点。它包含三个不同的组件：Flink Resource Manager、Flink Dispatcher、运行每个 Flink Job 的 Flink JobMaster。
Flink JobMaster # JobMaster 是在 Flink JobManager 运行中的组件之一。JobManager 负责监督单个作业 Task 的执行。以前，整个 Flink JobManager 都叫做 JobManager。
JobResultStore # JobResultStore 是一个 Flink 组件，它将全局终止（已完成的、已取消的或失败的）作业的结果保存到文件系统中，从而使结果比已完成的作业更长久。 这些结果然后被 Flink 用来确定作业是否应该在高可用集群中被恢复。
Logical Graph # 逻辑图是一种有向图，其中顶点是 算子，边定义算子的输入/输出关系，并对应于数据流或数据集。通过从 Flink Application 提交作业来创建逻辑图。
逻辑图通常也称为数据流图。
Managed State # Managed State 描述了已在框架中注册的应用程序的托管状态。对于托管状态，Apache Flink 会负责持久化和重伸缩等事宜。
Operator # Logical Graph 的节点。算子执行某种操作，该操作通常由 Function 执行。Source 和 Sink 是数据输入和数据输出的特殊算子。
Operator Chain # 算子链由两个或多个连续的 Operator 组成，两者之间没有任何的重新分区。同一算子链内的算子可以彼此直接传递 record，而无需通过序列化或 Flink 的网络栈。
Partition # 分区是整个数据流或数据集的独立子集。通过将每个 Record 分配给一个或多个分区，来把数据流或数据集划分为多个分区。在运行期间，Task 会消费数据流或数据集的分区。改变数据流或数据集分区方式的转换通常称为重分区。
Physical Graph # Physical graph 是一个在分布式运行时，把 Logical Graph 转换为可执行的结果。节点是 Task，边表示数据流或数据集的输入/输出关系或 partition。
Record # Record 是数据集或数据流的组成元素。Operator 和 Function接收 record 作为输入，并将 record 作为输出发出。
Flink Session Cluster # 长时间运行的 Flink Cluster，它可以接受多个 Flink Job 的执行。此 Flink Cluster 的生命周期不受任何 Flink Job 生命周期的约束限制。以前，Flink Session Cluster 也称为 session mode 的 Flink Cluster，和 Flink Application Cluster 相对应。
State Backend # 对于流处理程序，Flink Job 的 State Backend 决定了其 state 是如何存储在每个 TaskManager 上的（ TaskManager 的 Java 堆栈或嵌入式 RocksDB），以及它在 checkpoint 时的写入位置（ Flink JobManager 的 Java 堆或者 Filesystem）。
Sub-Task # Sub-Task 是负责处理数据流 Partition 的 Task。\u0026ldquo;Sub-Task\u0026quot;强调的是同一个 Operator 或者 Operator Chain 具有多个并行的 Task 。
Task # Task 是 Physical Graph 的节点。它是基本的工作单元，由 Flink 的 runtime 来执行。Task 正好封装了一个 Operator 或者 Operator Chain 的 parallel instance。
Flink TaskManager # TaskManager 是 Flink Cluster 的工作进程。Task 被调度到 TaskManager 上执行。TaskManager 相互通信，只为在后续的 Task 之间交换数据。
Transformation # Transformation 应用于一个或多个数据流或数据集，并产生一个或多个输出数据流或数据集。Transformation 可能会在每个记录的基础上更改数据流或数据集，但也可以只更改其分区或执行聚合。虽然 Operator 和 Function 是 Flink API 的“物理”部分，但 Transformation 只是一个 API 概念。具体来说，大多数（但不是全部）Transformation 是由某些 Operator 实现的。
`}),e.add({id:229,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/set-ops/",title:"集合操作",section:"Queries 查询",content:` Set Operations # Batch Streaming
UNION # UNION and UNION ALL return the rows that are found in either table. UNION takes only distinct rows while UNION ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; create view t1(s) as values (\u0026#39;c\u0026#39;), (\u0026#39;a\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;c\u0026#39;); Flink SQL\u0026gt; create view t2(s) as values (\u0026#39;d\u0026#39;), (\u0026#39;e\u0026#39;), (\u0026#39;a\u0026#39;), (\u0026#39;b\u0026#39;), (\u0026#39;b\u0026#39;); Flink SQL\u0026gt; (SELECT s FROM t1) UNION (SELECT s FROM t2); +---+ | s| +---+ | c| | a| | b| | d| | e| +---+ Flink SQL\u0026gt; (SELECT s FROM t1) UNION ALL (SELECT s FROM t2); +---+ | c| +---+ | c| | a| | b| | b| | c| | d| | e| | a| | b| | b| +---+ INTERSECT # INTERSECT and INTERSECT ALL return the rows that are found in both tables. INTERSECT takes only distinct rows while INTERSECT ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; (SELECT s FROM t1) INTERSECT (SELECT s FROM t2); +---+ | s| +---+ | a| | b| +---+ Flink SQL\u0026gt; (SELECT s FROM t1) INTERSECT ALL (SELECT s FROM t2); +---+ | s| +---+ | a| | b| | b| +---+ EXCEPT # EXCEPT and EXCEPT ALL return the rows that are found in one table but not the other. EXCEPT takes only distinct rows while EXCEPT ALL does not remove duplicates from the result rows.
Flink SQL\u0026gt; (SELECT s FROM t1) EXCEPT (SELECT s FROM t2); +---+ | s | +---+ | c | +---+ Flink SQL\u0026gt; (SELECT s FROM t1) EXCEPT ALL (SELECT s FROM t2); +---+ | s | +---+ | c | | c | +---+ IN # Returns true if an expression exists in a given table sub-query. The sub-query table must consist of one column. This column must have the same data type as the expression.
SELECT user, amount FROM Orders WHERE product IN ( SELECT product FROM NewProducts ) The optimizer rewrites the IN condition into a join and group operation. For streaming queries, the required state for computing the query result might grow infinitely depending on the number of distinct input rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
EXISTS # SELECT user, amount FROM Orders WHERE product EXISTS ( SELECT product FROM NewProducts ) Returns true if the sub-query returns at least one row. Only supported if the operation can be rewritten in a join and group operation.
The optimizer rewrites the EXISTS operation into a join and group operation. For streaming queries, the required state for computing the query result might grow infinitely depending on the number of distinct input rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size. Note that this might affect the correctness of the query result. See query configuration for details.
Back to top
`}),e.add({id:230,href:"/flink/flink-docs-master/zh/docs/dev/table/tuning/",title:"流式聚合",section:"Table API \u0026 SQL",content:` Performance Tuning # SQL 是数据分析中使用最广泛的语言。Flink Table API 和 SQL 使用户能够以更少的时间和精力定义高效的流分析应用程序。此外，Flink Table API 和 SQL 是高效优化过的，它集成了许多查询优化和算子优化。但并不是所有的优化都是默认开启的，因此对于某些工作负载，可以通过打开某些选项来提高性能。
在这一页，我们将介绍一些实用的优化选项以及流式聚合的内部原理，它们在某些情况下能带来很大的提升。
The streaming aggregation optimizations mentioned in this page are all supported for Group Aggregations and Window TVF Aggregations now. MiniBatch 聚合 # 默认情况下，无界聚合算子是逐条处理输入的记录，即：（1）从状态中读取累加器，（2）累加/撤回记录至累加器，（3）将累加器写回状态，（4）下一条记录将再次从（1）开始处理。这种处理模式可能会增加 StateBackend 开销（尤其是对于 RocksDB StateBackend ）。此外，生产中非常常见的数据倾斜会使这个问题恶化，并且容易导致 job 发生反压。
MiniBatch 聚合的核心思想是将一组输入的数据缓存在聚合算子内部的缓冲区中。当输入的数据被触发处理时，每个 key 只需一个操作即可访问状态。这样可以大大减少状态开销并获得更好的吞吐量。但是，这可能会增加一些延迟，因为它会缓冲一些记录而不是立即处理它们。这是吞吐量和延迟之间的权衡。
下图说明了 mini-batch 聚合如何减少状态操作。
默认情况下，对于无界聚合算子来说，mini-batch 优化是被禁用的。开启这项优化，需要设置选项 table.exec.mini-batch.enabled、table.exec.mini-batch.allow-latency 和 table.exec.mini-batch.size。更多详细信息请参见配置页面。
MiniBatch optimization is always enabled for Window TVF Aggregation, regardless of the above configuration. Window TVF aggregation buffer records in managed memory instead of JVM Heap, so there is no risk of overloading GC or OOM issues. 下面的例子显示如何启用这些选项。
Java // instantiate table environment TableEnvironment tEnv = ...; // access flink configuration TableConfig configuration = tEnv.getConfig(); // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); // enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); // use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); // the maximum number of records can be buffered by each aggregate operator task Scala // instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig() // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) // use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) // the maximum number of records can be buffered by each aggregate operator task Python # instantiate table environment t_env = ... # access flink configuration configuration = t_env.get_config() # set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) # enable mini-batch optimization configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) # use 5 seconds to buffer input records configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) # the maximum number of records can be buffered by each aggregate operator task Local-Global 聚合 # Local-Global 聚合是为解决数据倾斜问题提出的，通过将一组聚合分为两个阶段，首先在上游进行本地聚合，然后在下游进行全局聚合，类似于 MapReduce 中的 Combine + Reduce 模式。例如，就以下 SQL 而言：
SELECT color, sum(id) FROM T GROUP BY color 数据流中的记录可能会倾斜，因此某些聚合算子的实例必须比其他实例处理更多的记录，这会产生热点问题。本地聚合可以将一定数量具有相同 key 的输入数据累加到单个累加器中。全局聚合将仅接收 reduce 后的累加器，而不是大量的原始输入数据。这可以大大减少网络 shuffle 和状态访问的成本。每次本地聚合累积的输入数据量基于 mini-batch 间隔。这意味着 local-global 聚合依赖于启用了 mini-batch 优化。
下图显示了 local-global 聚合如何提高性能。
下面的例子显示如何启用 local-global 聚合。
Java // instantiate table environment TableEnvironment tEnv = ...; // access flink configuration Configuration configuration = tEnv.getConfig().getConfiguration(); // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); // local-global aggregation depends on mini-batch is enabled configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); configuration.setString(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;); // enable two-phase, i.e. local-global aggregation Scala // instantiate table environment val tEnv: TableEnvironment = ... // access flink configuration val configuration = tEnv.getConfig() // set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) // local-global aggregation depends on mini-batch is enabled configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) configuration.set(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;) // enable two-phase, i.e. local-global aggregation Python # instantiate table environment t_env = ... # access flink configuration configuration = t_env.get_config() # set low-level key-value options configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) # local-global aggregation depends on mini-batch is enabled configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) configuration.set(\u0026#34;table.optimizer.agg-phase-strategy\u0026#34;, \u0026#34;TWO_PHASE\u0026#34;) # enable two-phase, i.e. local-global aggregation 拆分 distinct 聚合 # Local-Global 优化可有效消除常规聚合的数据倾斜，例如 SUM、COUNT、MAX、MIN、AVG。但是在处理 distinct 聚合时，其性能并不令人满意。
例如，如果我们要分析今天有多少唯一用户登录。我们可能有以下查询：
SELECT day, COUNT(DISTINCT user_id) FROM T GROUP BY day 如果 distinct key （即 user_id）的值分布稀疏，则 COUNT DISTINCT 不适合减少数据。即使启用了 local-global 优化也没有太大帮助。因为累加器仍然包含几乎所有原始记录，并且全局聚合将成为瓶颈（大多数繁重的累加器由一个任务处理，即同一天）。
这个优化的想法是将不同的聚合（例如 COUNT(DISTINCT col)）分为两个级别。第一次聚合由 group key 和额外的 bucket key 进行 shuffle。bucket key 是使用 HASH_CODE(distinct_key) % BUCKET_NUM 计算的。BUCKET_NUM 默认为1024，可以通过 table.optimizer.distinct-agg.split.bucket-num 选项进行配置。第二次聚合是由原始 group key 进行 shuffle，并使用 SUM 聚合来自不同 buckets 的 COUNT DISTINCT 值。由于相同的 distinct key 将仅在同一 bucket 中计算，因此转换是等效的。bucket key 充当附加 group key 的角色，以分担 group key 中热点的负担。bucket key 使 job 具有可伸缩性来解决不同聚合中的数据倾斜/热点。
拆分 distinct 聚合后，以上查询将被自动改写为以下查询：
SELECT day, SUM(cnt) FROM ( SELECT day, COUNT(DISTINCT user_id) as cnt FROM T GROUP BY day, MOD(HASH_CODE(user_id), 1024) ) GROUP BY day 下图显示了拆分 distinct 聚合如何提高性能（假设颜色表示 days，字母表示 user_id）。
注意：上面是可以从这个优化中受益的最简单的示例。除此之外，Flink 还支持拆分更复杂的聚合查询，例如，多个具有不同 distinct key （例如 COUNT(DISTINCT a), SUM(DISTINCT b) ）的 distinct 聚合，可以与其他非 distinct 聚合（例如 SUM、MAX、MIN、COUNT ）一起使用。
注意 当前，拆分优化不支持包含用户定义的 AggregateFunction 聚合。
下面的例子显示了如何启用拆分 distinct 聚合优化。
Java // instantiate table environment TableEnvironment tEnv = ...; tEnv.getConfig() .set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;); // enable distinct agg split Scala // instantiate table environment val tEnv: TableEnvironment = ... tEnv.getConfig .set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;) // enable distinct agg split Python # instantiate table environment t_env = ... t_env.get_config().set(\u0026#34;table.optimizer.distinct-agg.split.enabled\u0026#34;, \u0026#34;true\u0026#34;) # enable distinct agg split 在 distinct 聚合上使用 FILTER 修饰符 # 在某些情况下，用户可能需要从不同维度计算 UV（独立访客）的数量，例如来自 Android 的 UV、iPhone 的 UV、Web 的 UV 和总 UV。很多人会选择 CASE WHEN，例如：
SELECT day, COUNT(DISTINCT user_id) AS total_uv, COUNT(DISTINCT CASE WHEN flag IN (\u0026#39;android\u0026#39;, \u0026#39;iphone\u0026#39;) THEN user_id ELSE NULL END) AS app_uv, COUNT(DISTINCT CASE WHEN flag IN (\u0026#39;wap\u0026#39;, \u0026#39;other\u0026#39;) THEN user_id ELSE NULL END) AS web_uv FROM T GROUP BY day 但是，在这种情况下，建议使用 FILTER 语法而不是 CASE WHEN。因为 FILTER 更符合 SQL 标准，并且能获得更多的性能提升。FILTER 是用于聚合函数的修饰符，用于限制聚合中使用的值。将上面的示例替换为 FILTER 修饰符，如下所示：
SELECT day, COUNT(DISTINCT user_id) AS total_uv, COUNT(DISTINCT user_id) FILTER (WHERE flag IN (\u0026#39;android\u0026#39;, \u0026#39;iphone\u0026#39;)) AS app_uv, COUNT(DISTINCT user_id) FILTER (WHERE flag IN (\u0026#39;wap\u0026#39;, \u0026#39;other\u0026#39;)) AS web_uv FROM T GROUP BY day Flink SQL 优化器可以识别相同的 distinct key 上的不同过滤器参数。例如，在上面的示例中，三个 COUNT DISTINCT 都在 user_id 一列上。Flink 可以只使用一个共享状态实例，而不是三个状态实例，以减少状态访问和状态大小。在某些工作负载下，可以获得显著的性能提升。
Back to top
`}),e.add({id:231,href:"/flink/flink-docs-master/zh/docs/ops/production_ready/",title:"生产就绪情况核对清单",section:"Operations",content:` 生产就绪情况核对清单 # The production readiness checklist provides an overview of configuration options that should be carefully considered before bringing an Apache Flink job into production. While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs.
Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale. There is currently no way to change the maximum parallelism of an operator after a job has started without discarding that operators state. The reason maximum parallelism exists, versus allowing stateful operators to be infinitely scalable, is that it has some impact on your application\u0026rsquo;s performance and state size. Flink has to maintain specific metadata for its ability to rescale state which grows linearly with max parallelism. In general, you should choose max parallelism that is high enough to fit your future needs in scalability, while keeping it low enough to maintain reasonable performance.
Maximum parallelism must fulfill the following conditions: 0 \u0026lt; parallelism \u0026lt;= max parallelism \u0026lt;= 2^15 You can explicitly set maximum parallelism by using setMaxParallelism(int maxparallelism). If no max parallelism is set Flink will decide using a function of the operators parallelism when the job is first started:
128 : for all parallelism \u0026lt;= 128. MIN(nextPowerOfTwo(parallelism + (parallelism / 2)), 2^15) : for all parallelism \u0026gt; 128. Set UUIDs For All Operators # As mentioned in the documentation for savepoints, users should set uids for each operator in their DataStream. Uids are necessary for Flink\u0026rsquo;s mapping of operator states to operators which, in turn, is essential for savepoints. By default, operator uids are generated by traversing the JobGraph and hashing specific operator properties. While this is comfortable from a user perspective, it is also very fragile, as changes to the JobGraph (e.g., exchanging an operator) results in new UUIDs. To establish a stable mapping, we need stable operator uids provided by the user through setUid(String uid).
Choose The Right State Backend # See the description of state backends for choosing the right one for your use case.
Choose The Right Checkpoint Interval # Checkpointing is Flink\u0026rsquo;s primary fault-tolerance mechanism, wherein a snapshot of your job\u0026rsquo;s state persisted periodically to some durable location. In the case of failure, Flink will restart from the most recent checkpoint and resume processing. A jobs checkpoint interval configures how often Flink will take these snapshots. While there is no single correct answer on the perfect checkpoint interval, the community can guide what factors to consider when configuring this parameter.
What is the SLA of your service: Checkpoint interval is best understood as an expression of the jobs service level agreement (SLA). In the worst-case scenario, where a job fails one second before the next checkpoint, how much data can you tolerate reprocessing? A checkpoint interval of 5 minutes implies that Flink will never reprocess more than 5 minutes worth of data after a failure.
How often must your service deliver results: Exactly once sinks, such as Kafka or the FileSink, only make results visible on checkpoint completion. Shorter checkpoint intervals make results available more quickly but may also put additional pressure on these systems. It is important to work with stakeholders to find a delivery time that meet product requirements without putting undue load on your sinks.
How much load can your Task Managers sustain: All of Flinks\u0026rsquo; built-in state backends support asynchronous checkpointing, meaning the snapshot process will not pause data processing. However, it still does require CPU cycles and network bandwidth from your machines. Incremental checkpointing can be a powerful tool to reduce the cost of any given checkpoint.
And most importantly, test and measure your job. Every Flink application is unique, and the best way to find the appropriate checkpoint interval is to see how yours behaves in practice.
Configure JobManager High Availability # The JobManager serves as a central coordinator for each Flink deployment, being responsible for both scheduling and resource management of the cluster. It is a single point of failure within the cluster, and if it crashes, no new jobs can be submitted, and running applications will fail.
Configuring High Availability, in conjunction with Apache Zookeeper or Flinks Kubernetes based service, allows for a swift recovery and is highly recommended for production setups.
Back to top
`}),e.add({id:232,href:"/flink/flink-docs-master/zh/docs/dev/datastream/sources/",title:"数据源",section:"DataStream API",content:` 数据源 # 当前页面所描述的是 Flink 的 Data Source API 及其背后的概念和架构。 如果您对 Flink 中的 Data Source 如何工作感兴趣，或者您想实现一个新的数据 source，请阅读本文。
如果您正在寻找预定义的 source 连接器，请查看连接器文档.
Data Source 原理 # 核心组件
一个数据 source 包括三个核心组件：分片（Splits）、分片枚举器（SplitEnumerator） 以及 源阅读器（SourceReader）。
分片（Split） 是对一部分 source 数据的包装，如一个文件或者日志分区。分片是 source 进行任务分配和数据并行读取的基本粒度。
源阅读器（SourceReader） 会请求分片并进行处理，例如读取分片所表示的文件或日志分区。SourceReader 在 TaskManagers 上的 SourceOperators 并行运行，并产生并行的事件流/记录流。
分片枚举器（SplitEnumerator） 会生成分片并将它们分配给 SourceReader。该组件在 JobManager 上以单并行度运行，负责对未分配的分片进行维护，并以均衡的方式将其分配给 reader。
Source 类作为API入口，将上述三个组件结合在了一起。
流处理和批处理的统一
Data Source API 以统一的方式对无界流数据和有界批数据进行处理。
事实上，这两种情况之间的区别是非常小的：在有界/批处理情况中，枚举器生成固定数量的分片，而且每个分片都必须是有限的。但在无界流的情况下，则无需遵从限制，也就是分片大小可以不是有限的，或者枚举器将不断生成新的分片。
示例 # 以下是一些简化的概念示例，以说明在流和批处理情况下 data source 组件如何交互。
请注意，以下内容并没有准确地描述出 Kafka 和 File source 的工作方式，因为出于说明的目的，部分内容被简化处理。
有界 File Source
Source 将包含待读取目录的 URI/路径（Path），以及一个定义了如何对文件进行解析的 格式（Format）。在该情况下：
分片是一个文件，或者是文件的一个区域（如果该文件格式支持对文件进行拆分）。 SplitEnumerator 将会列举给定目录路径下的所有文件，并在收到来自 reader 的请求时对分片进行分配。一旦所有的分片都被分配完毕，则会使用 NoMoreSplits 来响应请求。 SourceReader 则会请求分片，读取所分配的分片（文件或者文件区域），并使用给定的格式进行解析。如果当前请求没有获得下一个分片，而是 NoMoreSplits，则会终止任务。 无界 Streaming File Source
这个 source 的工作方式与上面描述的基本相同，除了 SplitEnumerator 从不会使用 NoMoreSplits 来响应 SourceReader 的请求，并且还会定期列出给定 URI/路径下的文件来检查是否有新文件。一旦发现新文件，则生成对应的新分片，并将它们分配给空闲的 SourceReader。
无界 Streaming Kafka Source
Source 将具有 Kafka Topic（亦或者一系列 Topics 或者通过正则表达式匹配的 Topic）以及一个 解析器（Deserializer） 来解析记录（record）。
分片是一个 Kafka Topic Partition。 SplitEnumerator 会连接到 broker 从而列举出已订阅的 Topics 中的所有 Topic Partitions。枚举器可以重复此操作以检查是否有新的 Topics/Partitions。 SourceReader 使用 KafkaConsumer 读取所分配的分片（Topic Partition），并使用提供的 解析器 反序列化记录。由于流处理中分片（Topic Partition）大小是无限的，因此 reader 永远无法读取到数据的尾部。 有界 Kafka Source
这种情况下，除了每个分片（Topic Partition）都会有一个预定义的结束偏移量，其他与上述相同。一旦 SourceReader 读取到分片的结束偏移量，整个分片的读取就会结束。而一旦所有所分配的分片读取结束，SourceReader 也就终止任务了。
Data Source API # 本节所描述的是 FLIP—27 中引入的新 Source API 的主要接口，并为开发人员提供有关 Source 开发的相关技巧。
Source # Source API 是一个工厂模式的接口，用于创建以下组件。
Split Enumerator Source Reader Split Serializer Enumerator Checkpoint Serializer 除此之外，Source 还提供了 Boundedness 的特性，从而使得 Flink 可以选择合适的模式来运行 Flink 任务。
Source 实现应该是可序列化的，因为 Source 实例会在运行时被序列化并上传到 Flink 集群。
SplitEnumerator # SplitEnumerator 被认为是整个 Source 的“大脑”。SplitEnumerator 的典型实现如下：
SourceReader 的注册处理 SourceReader 的失败处理 SourceReader 失败时会调用 addSplitsBack() 方法。SplitEnumerator应当收回已经被分配，但尚未被该 SourceReader 确认（acknowledged）的分片。 SourceEvent 的处理 SourceEvents 是 SplitEnumerator 和 SourceReader 之间来回传递的自定义事件。可以利用此机制来执行复杂的协调任务。 分片的发现以及分配 SplitEnumerator 可以将分片分配到 SourceReader 从而响应各种事件，包括发现新的分片，新 SourceReader 的注册，SourceReader 的失败处理等 SplitEnumerator 可以在 SplitEnumeratorContext 的帮助下完成所有上述工作，其会在 SplitEnumerator 的创建或者恢复的时候提供给 Source。 SplitEnumeratorContext 允许 SplitEnumerator 检索到 reader 的必要信息并执行协调操作。 而在 Source 的实现中会将 SplitEnumeratorContext 传递给 SplitEnumerator 实例。
SplitEnumerator 的实现可以仅采用被动工作方式，即仅在其方法被调用时采取协调操作，但是一些 SplitEnumerator 的实现会采取主动性的工作方式。例如，SplitEnumerator 定期寻找分片并分配给 SourceReader。 这类问题使用 SplitEnumeratorContext 类中的 callAsync() 方法比较方便。下面的代码片段展示了如何在 SplitEnumerator 不需要自己维护线程的条件下实现这一点。
Java class MySplitEnumerator implements SplitEnumerator\u0026lt;MySplit, MyCheckpoint\u0026gt; { private final long DISCOVER_INTERVAL = 60_000L; /** * 一种发现分片的方法 */ private List\u0026lt;MySplit\u0026gt; discoverSplits() {...} @Override public void start() { ... enumContext.callAsync(this::discoverSplits, splits -\u0026gt; { Map\u0026lt;Integer, List\u0026lt;MySplit\u0026gt;\u0026gt; assignments = new HashMap\u0026lt;\u0026gt;(); int parallelism = enumContext.currentParallelism(); for (MySplit split : splits) { int owner = split.splitId().hashCode() % parallelism; assignments.computeIfAbsent(owner, new ArrayList\u0026lt;\u0026gt;()).add(split); } enumContext.assignSplits(new SplitsAssignment\u0026lt;\u0026gt;(assignments)); }, 0L, DISCOVER_INTERVAL); ... } ... } Python Python API 中尚不支持该特性。 SourceReader # SourceReader 是一个运行在Task Manager上的组件，用于处理来自分片的记录。
SourceReader 提供了一个拉动式（pull-based）处理接口。Flink 任务会在循环中不断调用 pollNext(ReaderOutput) 轮询来自 SourceReader 的记录。pollNext(ReaderOutput) 方法的返回值指示 SourceReader 的状态。
MORE_AVAILABLE - SourceReader 有可用的记录。 NOTHING_AVAILABLE - SourceReader 现在没有可用的记录，但是将来可能会有记录可用。 END_OF_INPUT - SourceReader 已经处理完所有记录，到达数据的尾部。这意味着 SourceReader 可以终止任务了。 pollNext(ReaderOutput) 会使用 ReaderOutput 作为参数，为了提高性能且在必要情况下，SourceReader 可以在一次 pollNext() 调用中返回多条记录。例如，有时外部系统的工作粒度为块。而一个块可以包含多个记录，但是 source 只能在块的边界处设置 Checkpoint。在这种情况下，SourceReader 可以一次将一个块中的所有记录通过 ReaderOutput 发送至下游。
然而，除非有必要，SourceReader 的实现应该避免在一次 pollNext(ReaderOutput) 的调用中发送多个记录。 这是因为对 SourceReader 轮询的任务线程工作在一个事件循环（event-loop）中，且不能阻塞。
在创建 SourceReader 时，相应的 SourceReaderContext 会提供给 Source，而 Source 则会将相应的上下文传递给 SourceReader 实例。SourceReader 可以通过 SourceReaderContext 将 SourceEvent 传递给相应的 SplitEnumerator 。Source 的一个典型设计模式是让 SourceReader 发送它们的本地信息给 SplitEnumerator，后者则会全局性地做出决定。
SourceReader API 是一个底层（low-level) API，允许用户自行处理分片，并使用自己的线程模型来获取和移交记录。为了帮助实现 SourceReader，Flink 提供了 SourceReaderBase 类，可以显著减少编写 SourceReader 所需要的工作量。
强烈建议连接器开发人员充分利用 SourceReaderBase 而不是从头开始编写 SourceReader。更多详细信息，请阅读 SplitReader API 部分。
Source 使用方法 # 为了通过 Source 创建 DataStream，需要将 Source 传递给 StreamExecutionEnvironment。例如，
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Source mySource = new MySource(...); DataStream\u0026lt;Integer\u0026gt; stream = env.fromSource( mySource, WatermarkStrategy.noWatermarks(), \u0026#34;MySourceName\u0026#34;); ... Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val mySource = new MySource(...) val stream = env.fromSource( mySource, WatermarkStrategy.noWatermarks(), \u0026#34;MySourceName\u0026#34;) ... Python env = StreamExecutionEnvironment.get_execution_environment() my_source = ... env.from_source( my_source, WatermarkStrategy.no_watermarks(), \u0026#34;my_source_name\u0026#34;) SplitReader API # 核心的 SourceReader API 是完全异步的， 但实际上，大多数 Sources 都会使用阻塞的操作，例如客户端（如 KafkaConsumer）的 poll() 阻塞调用，或者分布式文件系统（HDFS, S3等）的阻塞I/O操作。为了使其与异步 Source API 兼容，这些阻塞（同步）操作需要在单独的线程中进行，并在之后将数据提交给 reader 的异步线程。
SplitReader 是基于同步读取/轮询的 Source 的高级（high-level）API，例如 file source 和 Kafka source 的实现等。
核心是上面提到的 SourceReaderBase 类，其使用 SplitReader 并创建提取器（fetcher）线程来运行 SplitReader，该实现支持不同的线程处理模型。
SplitReader # SplitReader API 只有以下三个方法：
阻塞式的提取 fetch() 方法，返回值为 RecordsWithSplitIds 。 非阻塞式处理分片变动 handleSplitsChanges() 方法。 非阻塞式的唤醒 wakeUp() 方法，用于唤醒阻塞中的提取操作。 SplitReader 仅需要关注从外部系统读取记录，因此比 SourceReader 简单得多。 请查看这个类的 Java 文档以获得更多细节。
SourceReaderBase # 常见的 SourceReader 实现方式如下：
有一个线程池以阻塞的方式从外部系统提取分片。 解决内部提取线程与其他方法调用（如 pollNext(ReaderOutput)）之间的同步。 维护每个分片的水印（watermark）以保证水印对齐。 维护每个分片的状态以进行 Checkpoint。 为了减少开发新的 SourceReader 所需的工作，Flink 提供了 SourceReaderBase 类作为 SourceReader 的基本实现。 SourceReaderBase 已经实现了上述需求。要重新编写新的 SourceReader，只需要让 SourceReader 继承 SourceReaderBase，而后完善一些方法并实现 SplitReader 。
SplitFetcherManager # SourceReaderBase 支持几个开箱即用（out-of-the-box）的线程模型，取决于 SplitFetcherManager 的行为模式。 SplitFetcherManager 创建和维护一个分片提取器（SplitFetchers）池，同时每个分片提取器使用一个 SplitReader 进行提取。它还决定如何分配分片给分片提取器。
例如，如下所示，一个 SplitFetcherManager 可能有固定数量的线程，每个线程对分配给 SourceReader 的一些分片进行抓取。
以下代码片段实现了此线程模型。
Java /** * 一个SplitFetcherManager，它具有固定数量的分片提取器， * 并根据分片ID的哈希值将分片分配给分片提取器。 */ public class FixedSizeSplitFetcherManager\u0026lt;E, SplitT extends SourceSplit\u0026gt; extends SplitFetcherManager\u0026lt;E, SplitT\u0026gt; { private final int numFetchers; public FixedSizeSplitFetcherManager( int numFetchers, FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitReaderSupplier) { super(elementsQueue, splitReaderSupplier); this.numFetchers = numFetchers; // 创建 numFetchers 个分片提取器. for (int i = 0; i \u0026lt; numFetchers; i++) { startFetcher(createSplitFetcher()); } } @Override public void addSplits(List\u0026lt;SplitT\u0026gt; splitsToAdd) { // 根据它们所属的提取器将分片聚集在一起。 Map\u0026lt;Integer, List\u0026lt;SplitT\u0026gt;\u0026gt; splitsByFetcherIndex = new HashMap\u0026lt;\u0026gt;(); splitsToAdd.forEach(split -\u0026gt; { int ownerFetcherIndex = split.hashCode() % numFetchers; splitsByFetcherIndex .computeIfAbsent(ownerFetcherIndex, s -\u0026gt; new ArrayList\u0026lt;\u0026gt;()) .add(split); }); // 将分片分配给它们所属的提取器。 splitsByFetcherIndex.forEach((fetcherIndex, splitsForFetcher) -\u0026gt; { fetchers.get(fetcherIndex).addSplits(splitsForFetcher); }); } } Python Python API 中尚不支持该特性。 使用这种线程模型的SourceReader可以像下面这样创建：
Java public class FixedFetcherSizeSourceReader\u0026lt;E, T, SplitT extends SourceSplit, SplitStateT\u0026gt; extends SourceReaderBase\u0026lt;E, T, SplitT, SplitStateT\u0026gt; { public FixedFetcherSizeSourceReader( FutureCompletingBlockingQueue\u0026lt;RecordsWithSplitIds\u0026lt;E\u0026gt;\u0026gt; elementsQueue, Supplier\u0026lt;SplitReader\u0026lt;E, SplitT\u0026gt;\u0026gt; splitFetcherSupplier, RecordEmitter\u0026lt;E, T, SplitStateT\u0026gt; recordEmitter, Configuration config, SourceReaderContext context) { super( elementsQueue, new FixedSizeSplitFetcherManager\u0026lt;\u0026gt;( config.getInteger(SourceConfig.NUM_FETCHERS), elementsQueue, splitFetcherSupplier), recordEmitter, config, context); } @Override protected void onSplitFinished(Map\u0026lt;String, SplitStateT\u0026gt; finishedSplitIds) { // 在回调过程中对完成的分片进行处理。 } @Override protected SplitStateT initializedState(SplitT split) { ... } @Override protected SplitT toSplitType(String splitId, SplitStateT splitState) { ... } } Python Python API 中尚不支持该特性。 SourceReader 的实现还可以在 SplitFetcherManager 和 SourceReaderBase 的基础上编写自己的线程模型。
事件时间和水印 # Source 的实现需要完成一部分事件时间分配和水印生成的工作。离开 SourceReader 的事件流需要具有事件时间戳，并且（在流执行期间）包含水印。有关事件时间和水印的介绍，请参见及时流处理。
旧版 SourceFunction 的应用通常在之后的单独的一步中通过 stream.assignTimestampsAndWatermarks(WatermarkStrategy) 生成时间戳和水印。这个函数不应该与新的 Sources 一起使用，因为此时时间戳应该已经被分配了，而且该函数会覆盖掉之前的分片（split-aware）水印。 API # 在 DataStream API 创建期间， WatermarkStrategy 会被传递给 Source，并同时创建 TimestampAssigner 和 WatermarkGenerator 。
Java environment.fromSource( Source\u0026lt;OUT, ?, ?\u0026gt; source, WatermarkStrategy\u0026lt;OUT\u0026gt; timestampsAndWatermarks, String sourceName); Python environment.from_source( source: Source, watermark_strategy: WatermarkStrategy, source_name: str, type_info: TypeInformation = None) TimestampAssigner 和 WatermarkGenerator 作为 ReaderOutput（或 SourceOutput）的一部分透明地运行，因此 Source 实现者不必实现任何时间戳提取和水印生成的代码。
事件时间戳 # 事件时间戳的分配分为以下两步：
SourceReader 通过调用 SourceOutput.collect(event, timestamp) 将 Source 记录的时间戳添加到事件中。 该实现只能用于含有记录并且拥有时间戳特性的数据源，例如 Kafka、Kinesis、Pulsar 或 Pravega。 因此，记录中不带有时间戳特性的数据源（如文件）也就无法实现这一步了。 此步骤是 Source 连接器实现的一部分，不由使用 Source 的应用程序进行参数化设定。
由应用程序配置的 TimestampAssigner 分配最终的时间戳。 TimestampAssigner 会查看原始的 Source 记录的时间戳和事件。分配器可以直接使用 Source 记录的时间戳或者访问事件的某个字段获得最终的事件时间戳。
这种分两步的方法使用户既可以引用 Source 系统中的时间戳，也可以引用事件数据中的时间戳作为事件时间戳。
注意： 当使用没有 Source 记录的时间戳的数据源（如文件）并选择 Source 记录的时间戳作为最终的事件时间戳时，默认的事件时间戳等于 LONG_MIN (=-9,223,372,036,854,775,808)。
水印生成 # 水印生成器仅在流执行期间会被激活。批处理执行则会停用水印生成器，则下文所述的所有相关操作实际上都变为无操作。
数据 Source API 支持每个分片单独运行水印生成器。这使得 Flink 可以分别观察每个分片的事件时间进度，这对于正确处理事件时间偏差和防止空闲分区阻碍整个应用程序的事件时间进度来说是很重要的。
使用 SplitReader API 实现源连接器时，将自动进行处理。所有基于 SplitReader API 的实现都具有开箱即用（out-of-the-box）的分片水印。
为了保证更底层的 SourceReader API 可以使用每个分片的水印生成，必须将不同分片的事件输送到不同的输出（outputs）中：局部分片（Split-local） SourceOutputs。通过 createOutputForSplit(splitId) 和 releaseOutputForSplit(splitId) 方法，可以在总 ReaderOutput 上创建并发布局部分片输出。有关详细信息，请参阅该类和方法的 Java 文档。
`}),e.add({id:233,href:"/flink/flink-docs-master/zh/docs/internals/filesystems/",title:"文件系统",section:"内幕",content:` 文件系统 # Flink has its own file system abstraction via the org.apache.flink.core.fs.FileSystem class. This abstraction provides a common set of operations and minimal guarantees across various types of file system implementations.
The FileSystem\u0026rsquo;s set of available operations is quite limited, in order to support a wide range of file systems. For example, appending to or mutating existing files is not supported.
File systems are identified by a file system scheme, such as file://, hdfs://, etc.
Implementations # Flink implements the file systems directly, with the following file system schemes:
file, which represents the machine\u0026rsquo;s local file system. Other file system types are accessed by an implementation that bridges to the suite of file systems supported by Apache Hadoop. The following is an incomplete list of examples:
hdfs: Hadoop Distributed File System s3, s3n, and s3a: Amazon S3 file system gcs: Google Cloud Storage \u0026hellip; Flink loads Hadoop\u0026rsquo;s file systems transparently if it finds the Hadoop File System classes in the class path and finds a valid Hadoop configuration. By default, it looks for the Hadoop configuration in the class path. Alternatively, one can specify a custom location via the configuration entry fs.hdfs.hadoopconf.
Persistence Guarantees # These FileSystem and its FsDataOutputStream instances are used to persistently store data, both for results of applications and for fault tolerance and recovery. It is therefore crucial that the persistence semantics of these streams are well defined.
Definition of Persistence Guarantees # Data written to an output stream is considered persistent, if two requirements are met:
Visibility Requirement: It must be guaranteed that all other processes, machines, virtual machines, containers, etc. that are able to access the file see the data consistently when given the absolute file path. This requirement is similar to the close-to-open semantics defined by POSIX, but restricted to the file itself (by its absolute path).
Durability Requirement: The file system\u0026rsquo;s specific durability/persistence requirements must be met. These are specific to the particular file system. For example the {@link LocalFileSystem} does not provide any durability guarantees for crashes of both hardware and operating system, while replicated distributed file systems (like HDFS) guarantee typically durability in the presence of up n concurrent node failures, where n is the replication factor.
Updates to the file\u0026rsquo;s parent directory (such that the file shows up when listing the directory contents) are not required to be complete for the data in the file stream to be considered persistent. This relaxation is important for file systems where updates to directory contents are only eventually consistent.
The FSDataOutputStream has to guarantee data persistence for the written bytes once the call to FSDataOutputStream.close() returns.
Examples # For fault-tolerant distributed file systems, data is considered persistent once it has been received and acknowledged by the file system, typically by having been replicated to a quorum of machines (durability requirement). In addition the absolute file path must be visible to all other machines that will potentially access the file (visibility requirement).
Whether data has hit non-volatile storage on the storage nodes depends on the specific guarantees of the particular file system.
The metadata updates to the file\u0026rsquo;s parent directory are not required to have reached a consistent state. It is permissible that some machines see the file when listing the parent directory\u0026rsquo;s contents while others do not, as long as access to the file by its absolute path is possible on all nodes.
A local file system must support the POSIX close-to-open semantics. Because the local file system does not have any fault tolerance guarantees, no further requirements exist.
The above implies specifically that data may still be in the OS cache when considered persistent from the local file system\u0026rsquo;s perspective. Crashes that cause the OS cache to lose data are considered fatal to the local machine and are not covered by the local file system\u0026rsquo;s guarantees as defined by Flink.
That means that computed results, checkpoints, and savepoints that are written only to the local filesystem are not guaranteed to be recoverable from the local machine\u0026rsquo;s failure, making local file systems unsuitable for production setups.
Updating File Contents # Many file systems either do not support overwriting contents of existing files at all, or do not support consistent visibility of the updated contents in that case. For that reason, Flink\u0026rsquo;s FileSystem does not support appending to existing files, or seeking within output streams such that previously written data could be changed within the same file.
Overwriting Files # Overwriting files is in general possible. A file is overwritten by deleting it and creating a new file. However, certain filesystems cannot make that change synchronously visible to all parties that have access to the file. For example Amazon S3 guarantees only eventual consistency in the visibility of the file replacement: Some machines may see the old file, some machines may see the new file.
To avoid these consistency issues, the implementations of failure/recovery mechanisms in Flink strictly avoid writing to the same file path more than once.
Thread Safety # Implementations of FileSystem must be thread-safe: The same instance of FileSystem is frequently shared across multiple threads in Flink and must be able to concurrently create input/output streams and list file metadata.
The FSDataOutputStream and FSDataOutputStream implementations are strictly not thread-safe. Instances of the streams should also not be passed between threads in between read or write operations, because there are no guarantees about the visibility of operations across threads (many operations do not create memory fences).
Back to top
`}),e.add({id:234,href:"/flink/flink-docs-master/zh/docs/dev/datastream/execution/execution_configuration/",title:"执行配置",section:"管理执行",content:` 执行配置 # StreamExecutionEnvironment 包含了 ExecutionConfig，它允许在运行时设置作业特定的配置值。要更改影响所有作业的默认值，请参阅配置。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ExecutionConfig executionConfig = env.getConfig(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment var executionConfig = env.getConfig Python env = StreamExecutionEnvironment.get_execution_environment() execution_config = env.get_config() 以下是可用的配置选项：（默认为粗体）
setClosureCleanerLevel()。closure cleaner 的级别默认设置为 ClosureCleanerLevel.RECURSIVE。closure cleaner 删除 Flink 程序中对匿名 function 的调用类的不必要引用。禁用 closure cleaner 后，用户的匿名 function 可能正引用一些不可序列化的调用类。这将导致序列化器出现异常。可设置的值是： NONE：完全禁用 closure cleaner ，TOP_LEVEL：只清理顶级类而不递归到字段中，RECURSIVE：递归清理所有字段。
getParallelism() / setParallelism(int parallelism)。为作业设置默认的并行度。
getMaxParallelism() / setMaxParallelism(int parallelism)。为作业设置默认的最大并行度。此设置决定最大并行度并指定动态缩放的上限。
getNumberOfExecutionRetries() / setNumberOfExecutionRetries(int numberOfExecutionRetries)。设置失败任务重新执行的次数。值为零会有效地禁用容错。-1 表示使用系统默认值（在配置中定义）。该配置已弃用，请改用重启策略 。
getExecutionRetryDelay() / setExecutionRetryDelay(long executionRetryDelay)。设置系统在作业失败后重新执行之前等待的延迟（以毫秒为单位）。在 TaskManagers 上成功停止所有任务后，开始计算延迟，一旦延迟过去，任务会被重新启动。此参数对于延迟重新执行的场景很有用，当尝试重新执行作业时，由于相同的问题，作业会立刻再次失败，该参数便于作业再次失败之前让某些超时相关的故障完全浮出水面（例如尚未完全超时的断开连接）。此参数仅在执行重试次数为一次或多次时有效。该配置已被弃用，请改用重启策略 。
getExecutionMode() / setExecutionMode()。默认的执行模式是 PIPELINED。设置执行模式以执行程序。执行模式定义了数据交换是以批处理方式还是以流方式执行。
enableForceKryo() / disableForceKryo。默认情况下不强制使用 Kryo。强制 GenericTypeInformation 对 POJO 使用 Kryo 序列化器，即使我们可以将它们作为 POJO 进行分析。在某些情况下，应该优先启用该配置。例如，当 Flink 的内部序列化器无法正确处理 POJO 时。
enableForceAvro() / disableForceAvro()。默认情况下不强制使用 Avro。强制 Flink AvroTypeInfo 使用 Avro 序列化器而不是 Kryo 来序列化 Avro 的 POJO。
enableObjectReuse() / disableObjectReuse()。默认情况下，Flink 中不重用对象。启用对象重用模式会指示运行时重用用户对象以获得更好的性能。请当心，当一个算子的用户代码 function 没有意识到这种行为时可能会导致bug。
getGlobalJobParameters() / setGlobalJobParameters()。此方法允许用户将自定义对象设置为作业的全局配置。由于 ExecutionConfig 可在所有用户定义的 function 中访问，因此这是一种使配置在作业中全局可用的简单方法。
addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Serializer\u0026lt;?\u0026gt; serializer)。为指定的类型注册 Kryo 序列化器实例。
addDefaultKryoSerializer(Class\u0026lt;?\u0026gt; type, Class\u0026lt;? extends Serializer\u0026lt;?\u0026gt;\u0026gt; serializerClass)。为指定的类型注册 Kryo 序列化器的类。
registerTypeWithKryoSerializer(Class\u0026lt;?\u0026gt; type, Serializer\u0026lt;?\u0026gt; serializer)。使用 Kryo 注册指定类型并为其指定序列化器。通过使用 Kryo 注册类型，该类型的序列化将更加高效。
registerKryoType(Class\u0026lt;?\u0026gt; type)。如果类型最终被 Kryo 序列化，那么它将在 Kryo 中注册，以确保只有标记（整数 ID）被写入。如果一个类型没有在 Kryo 注册，它的全限定类名将在每个实例中被序列化，从而导致更高的 I/O 成本。
registerPojoType(Class\u0026lt;?\u0026gt; type)。将指定的类型注册到序列化栈中。如果该类型最终被序列化为 POJO，那么该类型将注册到 POJO 序列化器中。如果该类型最终被 Kryo 序列化，那么它将在 Kryo 中注册，以确保只有标记被写入。如果一个类型没有在 Kryo 注册，它的全限定类名将在每个实例中被序列化，从而导致更高的I/O成本。
注意：用 registerKryoType() 注册的类型对 Flink 的 Kryo 序列化器实例来说是不可用的。
disableAutoTypeRegistration()。自动类型注册在默认情况下是启用的。自动类型注册是将用户代码使用的所有类型（包括子类型）注册到 Kryo 和 POJO 序列化器。
setTaskCancellationInterval(long interval)。设置尝试连续取消正在运行任务的等待时间间隔（以毫秒为单位）。当一个任务被取消时，会创建一个新的线程，如果任务线程在一定时间内没有终止，新线程就会定期调用任务线程上的 interrupt() 方法。这个参数是指连续调用 interrupt() 的时间间隔，默认设置为 30000 毫秒，或 30秒 。
通过 getRuntimeContext() 方法在 Rich* function 中访问到的 RuntimeContext 也允许在所有用户定义的 function 中访问 ExecutionConfig。
Back to top
`}),e.add({id:235,href:"/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/serialization/custom_serializers/",title:"自定义序列化器",section:"数据类型以及序列化",content:` 为你的 Flink 程序注册自定义序列化器 # 如果在 Flink 程序中使用了 Flink 类型序列化器无法进行序列化的用户自定义类型，Flink 会回退到通用的 Kryo 序列化器。 可以使用 Kryo 注册自己的序列化器或序列化系统，比如 Google Protobuf 或 Apache Thrift。 使用方法是在 Flink 程序中的 ExecutionConfig 注册类类型以及序列化器。
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 为类型注册序列化器类 env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, MyCustomSerializer.class); // 为类型注册序列化器实例 MySerializer mySerializer = new MySerializer(); env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, mySerializer); 需要确保你的自定义序列化器继承了 Kryo 的序列化器类。 对于 Google Protobuf 或 Apache Thrift，这一点已经为你做好了：
final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 使用 Kryo 注册 Google Protobuf 序列化器 env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class); // 注册 Apache Thrift 序列化器为标准序列化器 // TBaseSerializer 需要初始化为默认的 kryo 序列化器 env.getConfig().addDefaultKryoSerializer(MyCustomType.class, TBaseSerializer.class); 为了使上面的例子正常工作，需要在 Maven 项目文件中（pom.xml）包含必要的依赖。 为 Apache Thrift 添加以下依赖：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-thrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- libthrift is required by chill-thrift --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.thrift\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;libthrift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.11.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 对于 Google Protobuf 需要添加以下 Maven 依赖：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.twitter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;chill-protobuf\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.6\u0026lt;/version\u0026gt; \u0026lt;!-- exclusions for dependency conversion --\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- We need protobuf for chill-protobuf --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 请根据需要调整两个依赖库的版本。
使用 Kryo JavaSerializer 的问题 # 如果你为自定义类型注册 Kryo 的 JavaSerializer，即使你提交的 jar 中包含了自定义类型的类，也可能会遇到 ClassNotFoundException 异常。 这是由于 Kryo JavaSerializer 的一个已知问题，它可能使用了错误的类加载器。
在这种情况下，你应该使用 org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer 来解决这个问题。 这个类是在 Flink 中对 JavaSerializer 的重新实现，可以确保使用用户代码的类加载器。
更多细节可以参考 FLINK-6025。
Back to top
`}),e.add({id:236,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/load/",title:"LOAD 语句",section:"SQL",content:` LOAD 语句 # LOAD 语句用于加载内置的或用户自定义的模块。
执行 LOAD 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，executeSql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，executeSql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 LOAD 语句。如果 LOAD 操作执行成功，execute_sql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 LOAD 语句。
SQL CLI LOAD 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 LOAD 语句。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 加载 hive 模块 tEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;); tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // 加载 hive 模块 tEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;) tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ Python table_env = StreamTableEnvironment.create(...) # 加载 hive 模块 table_env.execute_sql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;)\u0026#34;) table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ SQL CLI Flink SQL\u0026gt; LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;3.1.2\u0026#39;); [INFO] Load module succeeded! Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | | hive | +-------------+ Back to top
LOAD MODULE # 以下语法概述了可用的语法规则：
LOAD MODULE module_name [WITH (\u0026#39;key1\u0026#39; = \u0026#39;val1\u0026#39;, \u0026#39;key2\u0026#39; = \u0026#39;val2\u0026#39;, ...)] module_name 是一个简单的标识符。它是区分大小写的，由于它被用于执行模块发现，因此也要与模块工厂（module factory）中定义的模块类型相同。属性 ('key1' = 'val1', 'key2' = 'val2', ...) 是一个 map 结构，它包含一组键值对（不包括 \u0026rsquo;type\u0026rsquo; 的键），这些属性会被传递给模块发现服务以实例化相应的模块。 `}),e.add({id:237,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/orderby/",title:"ORDER BY 语句",section:"Queries 查询",content:` ORDER BY 语句 # Batch Streaming
ORDER BY 子句使结果行根据指定的表达式进行排序。 如果两行根据最左边的表达式相等，则根据下一个表达式进行比较，依此类推。 如果根据所有指定的表达式它们相等，则它们以与实现相关的顺序返回。
在流模式下运行时，表的主要排序顺序必须按时间属性升序。 所有后续的 orders 都可以自由选择。 但是批处理模式没有这个限制。
SELECT * FROM Orders ORDER BY order_time, order_id Back to top
`}),e.add({id:238,href:"/flink/flink-docs-master/zh/docs/ops/state/state_backends/",title:"State Backends",section:"状态与容错",content:` State Backends # 用 Data Stream API 编写的程序通常以各种形式保存状态：
在 Window 触发之前要么收集元素、要么聚合 转换函数可以使用 key/value 格式的状态接口来存储状态 转换函数可以实现 CheckpointedFunction 接口，使其本地变量具有容错能力 另请参阅 Streaming API 指南中的 状态部分 。
在启动 CheckPoint 机制时，状态会随着 CheckPoint 而持久化，以防止数据丢失、保障恢复时的一致性。 状态内部的存储格式、状态在 CheckPoint 时如何持久化以及持久化在哪里均取决于选择的 State Backend。
可用的 State Backends # Flink 内置了以下这些开箱即用的 state backends ：
HashMapStateBackend EmbeddedRocksDBStateBackend 如果不设置，默认使用 HashMapStateBackend。
HashMapStateBackend # 在 HashMapStateBackend 内部，数据以 Java 对象的形式存储在堆中。 Key/value 形式的状态和窗口算子会持有一个 hash table，其中存储着状态值、触发器。
HashMapStateBackend 的适用场景：
有较大 state，较长 window 和较大 key/value 状态的 Job。 所有的高可用场景。 建议同时将 managed memory 设为0，以保证将最大限度的内存分配给 JVM 上的用户代码。
与 EmbeddedRocksDBStateBackend 不同的是，由于 HashMapStateBackend 将数据以对象形式存储在堆中，因此重用这些对象数据是不安全的。
EmbeddedRocksDBStateBackend # EmbeddedRocksDBStateBackend 将正在运行中的状态数据保存在 RocksDB 数据库中，RocksDB 数据库默认将数据存储在 TaskManager 的数据目录。 不同于 HashMapStateBackend 中的 java 对象，数据被以序列化字节数组的方式存储，这种方式由序列化器决定，因此 key 之间的比较是以字节序的形式进行而不是使用 Java 的 hashCode 或 equals() 方法。
EmbeddedRocksDBStateBackend 会使用异步的方式生成 snapshots。
EmbeddedRocksDBStateBackend 的局限：
由于 RocksDB 的 JNI API 构建在 byte[] 数据结构之上, 所以每个 key 和 value 最大支持 2^31 字节。 RocksDB 合并操作的状态（例如：ListState）累积数据量大小可以超过 2^31 字节，但是会在下一次获取数据时失败。这是当前 RocksDB JNI 的限制。 EmbeddedRocksDBStateBackend 的适用场景：
状态非常大、窗口非常长、key/value 状态非常大的 Job。 所有高可用的场景。 注意，你可以保留的状态大小仅受磁盘空间的限制。与状态存储在内存中的 HashMapStateBackend 相比，EmbeddedRocksDBStateBackend 允许存储非常大的状态。 然而，这也意味着使用 EmbeddedRocksDBStateBackend 将会使应用程序的最大吞吐量降低。 所有的读写都必须序列化、反序列化操作，这个比基于堆内存的 state backend 的效率要低很多。 同时因为存在这些序列化、反序列化操作，重用放入 EmbeddedRocksDBStateBackend 的对象是安全的。
请同时参考 Task Executor 内存配置 中关于 EmbeddedRocksDBStateBackend 的建议。
EmbeddedRocksDBStateBackend 是目前唯一支持增量 CheckPoint 的 State Backend (见 这里)。
可以使用一些 RocksDB 的本地指标(metrics)，但默认是关闭的。你能在 这里 找到关于 RocksDB 本地指标的文档。
每个 slot 中的 RocksDB instance 的内存大小是有限制的，详情请见 这里。
选择合适的 State Backend # 在选择 HashMapStateBackend 和 RocksDB 的时候，其实就是在性能与可扩展性之间权衡。HashMapStateBackend 是非常快的，因为每个状态的读取和算子对于 objects 的更新都是在 Java 的 heap 上；但是状态的大小受限于集群中可用的内存。 另一方面，RocksDB 可以根据可用的 disk 空间扩展，并且只有它支持增量 snapshot。 然而，每个状态的读取和更新都需要(反)序列化，而且在 disk 上进行读操作的性能可能要比基于内存的 state backend 慢一个数量级。
在 Flink 1.13 版本中我们统一了 savepoints 的二进制格式。这意味着你可以生成 savepoint 并且之后使用另一种 state backend 读取它。 从 1.13 版本开始，所有的 state backends 都会生成一种普适的格式。因此，如果想切换 state backend 的话，那么最好先升级你的 Flink 版本，在新版本中生成 savepoint，在这之后你才可以使用一个不同的 state backend 来读取并恢复它。 设置 State Backend # 如果没有明确指定，将使用 jobmanager 做为默认的 state backend。你能在 flink-conf.yaml 中为所有 Job 设置其他默认的 State Backend。 每一个 Job 的 state backend 配置会覆盖默认的 state backend 配置，如下所示：
设置每个 Job 的 State Backend # StreamExecutionEnvironment 可以对每个 Job 的 State Backend 进行设置，如下所示：
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setStateBackend(new HashMapStateBackend()) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) 如果你想在 IDE 中使用 EmbeddedRocksDBStateBackend，或者需要在作业中通过编程方式动态配置它，必须添加以下依赖到 Flink 项目中。
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-statebackend-rocksdb\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 注意: 由于 RocksDB 是 Flink 默认分发包的一部分，所以如果你没在代码中使用 RocksDB，则不需要添加此依赖。而且可以在 flink-conf.yaml 文件中通过 state.backend 配置 State Backend，以及更多的 checkpointing 和 RocksDB 特定的 参数。 设置默认的（全局的） State Backend # 在 flink-conf.yaml 可以通过键 state.backend 设置默认的 State Backend。
可选值包括 jobmanager (HashMapStateBackend), rocksdb (EmbeddedRocksDBStateBackend)， 或使用实现了 state backend 工厂 StateBackendFactory 的类的全限定类名， 例如： EmbeddedRocksDBStateBackend 对应为 org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackendFactory。
state.checkpoints.dir 选项指定了所有 State Backend 写 CheckPoint 数据和写元数据文件的目录。 你能在 这里 找到关于 CheckPoint 目录结构的详细信息。
配置文件的部分示例如下所示：
# 用于存储 operator state 快照的 State Backend state.backend: filesystem # 存储快照的目录 state.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints RocksDB State Backend 进阶 # 该小节描述 RocksDB state backend 的更多细节
增量快照 # RocksDB 支持增量快照。不同于产生一个包含所有数据的全量备份，增量快照中只包含自上一次快照完成之后被修改的记录，因此可以显著减少快照完成的耗时。
一个增量快照是基于（通常多个）前序快照构建的。由于 RocksDB 内部存在 compaction 机制对 sst 文件进行合并，Flink 的增量快照也会定期重新设立起点（rebase），因此增量链条不会一直增长，旧快照包含的文件也会逐渐过期并被自动清理。
和基于全量快照的恢复时间相比，如果网络带宽是瓶颈，那么基于增量快照恢复可能会消耗更多时间，因为增量快照包含的 sst 文件之间可能存在数据重叠导致需要下载的数据量变大；而当 CPU 或者 IO 是瓶颈的时候，基于增量快照恢复会更快，因为从增量快照恢复不需要解析 Flink 的统一快照格式来重建本地的 RocksDB 数据表，而是可以直接基于 sst 文件加载。
虽然状态数据量很大时我们推荐使用增量快照，但这并不是默认的快照机制，您需要通过下述配置手动开启该功能：
在 flink-conf.yaml 中设置：state.backend.incremental: true 或者 在代码中按照右侧方式配置（来覆盖默认配置）：EmbeddedRocksDBStateBackend backend = new EmbeddedRocksDBStateBackend(true); 需要注意的是，一旦启用了增量快照，网页上展示的 Checkpointed Data Size 只代表增量上传的数据量，而不是一次快照的完整数据量。
内存管理 # Flink 致力于控制整个进程的内存消耗，以确保 Flink 任务管理器（TaskManager）有良好的内存使用，从而既不会在容器（Docker/Kubernetes, Yarn等）环境中由于内存超用被杀掉，也不会因为内存利用率过低导致不必要的数据落盘或是缓存命中率下降，致使性能下降。
为了达到上述目标，Flink 默认将 RocksDB 的可用内存配置为任务管理器的单槽（per-slot）托管内存量。这将为大多数应用程序提供良好的开箱即用体验，即大多数应用程序不需要调整 RocksDB 配置，简单的增加 Flink 的托管内存即可改善内存相关性能问题。
当然，您也可以选择不使用 Flink 自带的内存管理，而是手动为 RocksDB 的每个列族（ColumnFamily）分配内存（每个算子的每个 state 都对应一个列族）。这为专业用户提供了对 RocksDB 进行更细粒度控制的途径，但同时也意味着用户需要自行保证总内存消耗不会超过（尤其是容器）环境的限制。请参阅 large state tuning 了解有关大状态数据性能调优的一些指导原则。
RocksDB 使用托管内存
这个功能默认打开，并且可以通过 state.backend.rocksdb.memory.managed 配置项控制。
Flink 并不直接控制 RocksDB 的 native 内存分配，而是通过配置 RocksDB 来确保其使用的内存正好与 Flink 的托管内存预算相同。这是在任务槽（per-slot）级别上完成的（托管内存以任务槽为粒度计算）。
为了设置 RocksDB 实例的总内存使用量，Flink 对同一个任务槽上的所有 RocksDB 实例使用共享的 cache 以及 write buffer manager。 共享 cache 将对 RocksDB 中内存消耗的三个主要来源（块缓存、索引和bloom过滤器、MemTables）设置上限。
Flink还提供了两个参数来控制写路径（MemTable）和读路径（索引及过滤器，读缓存）之间的内存分配。当您看到 RocksDB 由于缺少写缓冲内存（频繁刷新）或读缓存未命中而性能不佳时，可以使用这些参数调整读写间的内存分配。
state.backend.rocksdb.memory.write-buffer-ratio，默认值 0.5，即 50% 的给定内存会分配给写缓冲区使用。 state.backend.rocksdb.memory.high-prio-pool-ratio，默认值 0.1，即 10% 的 block cache 内存会优先分配给索引及过滤器。 我们强烈建议不要将此值设置为零，以防止索引和过滤器被频繁踢出缓存而导致性能问题。此外，我们默认将L0级的过滤器和索引将被固定到缓存中以提高性能，更多详细信息请参阅 RocksDB 文档。 注意 上述机制开启时将覆盖用户在 PredefinedOptions 和 RocksDBOptionsFactory 中对 block cache 和 write buffer 进行的配置。
注意 仅面向专业用户：若要手动控制内存，可以将 state.backend.rocksdb.memory.managed 设置为 false，并通过 ColumnFamilyOptions 配置 RocksDB。 或者可以复用上述 cache/write-buffer-manager 机制，但将内存大小设置为与 Flink 的托管内存大小无关的固定大小（通过 state.backend.rocksdb.memory.fixed-per-slot 选项）。 注意在这两种情况下，用户都需要确保在 JVM 之外有足够的内存可供 RocksDB 使用。
计时器（内存 vs. RocksDB） # 计时器（Timer）用于安排稍后的操作（基于事件时间或处理时间），例如触发窗口或回调 ProcessFunction。
当选择 RocksDB 作为 State Backend 时，默认情况下计时器也存储在 RocksDB 中。这是一种健壮且可扩展的方式，允许应用程序使用很多个计时器。另一方面，在 RocksDB 中维护计时器会有一定的成本，因此 Flink 也提供了将计时器存储在 JVM 堆上而使用 RocksDB 存储其他状态的选项。当计时器数量较少时，基于堆的计时器可以有更好的性能。
您可以通过将 state.backend.rocksdb.timer-service.factory 配置项设置为 heap（而不是默认的 rocksdb）来将计时器存储在堆上。
注意 在 RocksDB state backend 中使用基于堆的计时器的组合当前不支持计时器状态的异步快照。其他状态（如 keyed state）可以被异步快照。
开启 RocksDB 原生监控指标 # 您可以选择使用 Flink 的监控指标系统来汇报 RocksDB 的原生指标，并且可以选择性的指定特定指标进行汇报。 请参阅 configuration docs 了解更多详情。
注意： 启用 RocksDB 的原生指标可能会对应用程序的性能产生负面影响。 列族（ColumnFamily）级别的预定义选项 # 注意 在引入 RocksDB 使用托管内存 功能后，此机制应限于在专家调优或故障处理中使用。
使用预定义选项，用户可以在每个 RocksDB 列族上应用一些预定义的配置，例如配置内存使用、线程、Compaction 设置等。目前每个算子的每个状态都在 RocksDB 中有专门的一个列族存储。
有两种方法可以选择要应用的预定义选项：
通过 state.backend.rocksdb.predefined-options 配置项将选项名称设置进 flink-conf.yaml 。 通过程序设置：EmbeddedRocksDBStateBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED_HIGH_MEM) 。 该选项的默认值是 DEFAULT ，对应 PredefinedOptions.DEFAULT 。
从 flink-conf.yaml 中读取列族选项 # RocksDB State Backend 会将 这里定义 的所有配置项全部加载。 因此您可以简单的通过关闭 RocksDB 使用托管内存的功能并将需要的设置选项加入配置文件来配置底层的列族选项。
通过 RocksDBOptionsFactory 配置 RocksDB 选项 # 注意 在引入 RocksDB 使用托管内存 功能后，此机制应限于在专家调优或故障处理中使用。
您也可以通过配置一个 RocksDBOptionsFactory 来手动控制 RocksDB 的选项。此机制使您可以对列族的设置进行细粒度控制，例如内存使用、线程、Compaction 设置等。目前每个算子的每个状态都在 RocksDB 中有专门的一个列族存储。
有两种方法可以将 RocksDBOptionsFactory 传递给 RocksDB State Backend：
通过 state.backend.rocksdb.options-factory 选项将工厂实现类的名称设置到flink-conf.yaml 。
通过程序设置，例如 EmbeddedRocksDBStateBackend.setRocksDBOptions(new MyOptionsFactory()); 。
注意 通过程序设置的 RocksDBOptionsFactory 将覆盖 flink-conf.yaml 配置文件的设置，且 RocksDBOptionsFactory 设置的优先级高于预定义选项（PredefinedOptions）。
注意 RocksDB是一个本地库，它直接从进程分配内存， 而不是从JVM分配内存。分配给 RocksDB 的任何内存都必须被考虑在内，通常需要将这部分内存从任务管理器（TaskManager）的JVM堆中减去。 不这样做可能会导致JVM进程由于分配的内存超过申请值而被 YARN 等资源管理框架终止。
下面是自定义 ConfigurableRocksDBOptionsFactory 的一个示例 (开发完成后，请将您的实现类全名设置到 state.backend.rocksdb.options-factory).
Java public class MyOptionsFactory implements ConfigurableRocksDBOptionsFactory { public static final ConfigOption\u0026lt;Integer\u0026gt; BLOCK_RESTART_INTERVAL = ConfigOptions .key(\u0026#34;my.custom.rocksdb.block.restart-interval\u0026#34;) .intType() .defaultValue(16) .withDescription( \u0026#34; Block restart interval. RocksDB has default block restart interval as 16. \u0026#34;); private int blockRestartInterval = BLOCK_RESTART_INTERVAL.defaultValue(); @Override public DBOptions createDBOptions(DBOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { return currentOptions .setIncreaseParallelism(4) .setUseFsync(false); } @Override public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { return currentOptions.setTableFormatConfig( new BlockBasedTableConfig() .setBlockRestartInterval(blockRestartInterval)); } @Override public RocksDBOptionsFactory configure(ReadableConfig configuration) { this.blockRestartInterval = configuration.get(BLOCK_RESTART_INTERVAL); return this; } } Python Python API 中尚不支持该特性。 Back to top
开启 Changelog # 该功能处于实验状态。 开启 Changelog 可能会给您的应用带来性能损失。（见下文） 介绍 # Changelog 是一项旨在减少 checkpointing 时间的功能，因此也可以减少 exactly-once 模式下的端到端延迟。
一般情况下 checkpoint 的持续时间受如下因素影响：
Barrier 到达和对齐时间，可以通过 Unaligned checkpoints 和 Buffer debloating 解决。
快照制作时间（所谓同步阶段）, 可以通过异步快照解决（如上文所述）。
快照上传时间（异步阶段）。
可以用增量 checkpoints 来减少上传时间。但是，大多数支持增量checkpoint的状态后端会定期执行合并类型的操作，这会导致除了新的变更之外还要重新上传旧状态。在大规模部署中，每次 checkpoint 中至少有一个 task 上传大量数据的可能性往往非常高。
开启 Changelog 功能之后，Flink 会不断上传状态变更并形成 changelog。创建 checkpoint 时，只有 changelog 中的相关部分需要上传。而配置的状态后端则会定期在后台进行快照，快照成功上传后，相关的changelog 将会被截断。
基于此，异步阶段的持续时间减少（另外因为不需要将数据刷新到磁盘，同步阶段持续时间也减少了），特别是长尾延迟得到了改善。
但是，资源使用会变得更高：
将会在 DFS 上创建更多文件 将可能在 DFS 上残留更多文件（这将在 FLINK-25511 和 FLINK-25512 之后的新版本中被解决） 将使用更多的 IO 带宽用来上传状态变更 将使用更多 CPU 资源来序列化状态变更 Task Managers 将会使用更多内存来缓存状态变更 另一项需要考虑的事情是恢复时间。取决于 state.backend.changelog.periodic-materialize.interval 的设置，changelog 可能会变得冗长，因此重放会花费更多时间。即使这样，恢复时间加上 checkpoint 持续时间仍然可能低于不开启 changelog 功能的时间，从而在故障恢复的情况下也能提供更低的端到端延迟。当然，取决于上述时间的实际比例，有效恢复时间也有可能会增加。
有关更多详细信息，请参阅 FLIP-158。
安装 # 标准的 Flink 发行版包含 Changelog 所需要的 JAR包。
请确保添加所需的文件系统插件。
配置 # 这是 YAML 中的示例配置：
state.backend.changelog.enabled: true state.backend.changelog.storage: filesystem # 当前只支持 filesystem 和 memory（仅供测试用） dstl.dfs.base-path: s3://\u0026lt;bucket-name\u0026gt; # 类似于 state.checkpoints.dir 请将如下配置保持默认值 （参见限制）:
execution.checkpointing.max-concurrent-checkpoints: 1 有关其他配置选项，请参阅配置部分。
也可以通过编程方式为每个作业开启或关闭 Changelog： Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableChangelogStateBackend(true); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.enableChangelogStateBackend(true) Python env = StreamExecutionEnvironment.get_execution_environment() env.enable_changelog_statebackend(true) 监控 # 此处列出了可用的指标。
如果 task 因写状态变更而被反压，他将在 UI 中被显示为忙碌（红色）。
升级现有作业 # 开启 Changelog
支持从 savepoint 或 checkpoint 恢复：
给定一个没有开启 Changelog 的作业 创建一个 savepoint 或一个 checkpoint 更改配置（开启 Changelog） 从创建的 snapshot 恢复 关闭 Changelog
支持从 savepoint 或 checkpoint 恢复：
给定一个开启 Changelog 的作业 创建一个 savepoint 或一个 checkpoint 更改配置（关闭 Changelog） 从创建的 snapshot 恢复 限制 # 最多同时创建一个 checkpoint 到 Flink 1.15 为止, 只有 filesystem changelog 实现可用 尚不支持 NO_CLAIM 模式 Back to top
自旧版本迁移 # 从 Flink 1.13 版本开始，社区改进了 state backend 的公开类，进而帮助用户更好理解本地状态存储和 checkpoint 存储的区分。 这个变化并不会影响 state backend 和 checkpointing 过程的运行时实现和机制，仅仅是为了更好地传达设计意图。 用户可以将现有作业迁移到新的 API，同时不会损失原有 state。
MemoryStateBackend # 旧版本的 MemoryStateBackend 等价于使用 HashMapStateBackend 和 JobManagerCheckpointStorage。
flink-conf.yaml 配置 # state.backend: hashmap # Optional, Flink will automatically default to JobManagerCheckpointStorage # when no checkpoint directory is specified. state.checkpoint-storage: jobmanager 代码配置 # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new HashMapStateBackend) env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) env.get_checkpoint_config().set_checkpoint_storage(JobManagerCheckpointStorage()) FsStateBackend # 旧版本的 FsStateBackend 等价于使用 HashMapStateBackend 和 FileSystemCheckpointStorage。
flink-conf.yaml 配置 # state.backend: hashmap state.checkpoints.dir: file:///checkpoint-dir/ # Optional, Flink will automatically default to FileSystemCheckpointStorage # when a checkpoint directory is specified. state.checkpoint-storage: filesystem 代码配置 # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;); // Advanced FsStateBackend configurations, such as write buffer size // can be set by manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new HashMapStateBackend) env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;) // Advanced FsStateBackend configurations, such as write buffer size // can be set by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(HashMapStateBackend()) env.get_checkpoint_config().set_checkpoint_storage_dir(\u0026#34;file:///checkpoint-dir\u0026#34;) # Advanced FsStateBackend configurations, such as write buffer size # can be set by manually instantiating a FileSystemCheckpointStorage object. env.get_checkpoint_config().set_checkpoint_storage(FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) RocksDBStateBackend # 旧版本的 RocksDBStateBackend 等价于使用 EmbeddedRocksDBStateBackend 和 FileSystemCheckpointStorage.
flink-conf.yaml 配置 # state.backend: rocksdb state.checkpoints.dir: file:///checkpoint-dir/ # Optional, Flink will automatically default to FileSystemCheckpointStorage # when a checkpoint directory is specified. state.checkpoint-storage: filesystem 代码配置 # Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(new EmbeddedRocksDBStateBackend()); env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;); // If you manually passed FsStateBackend into the RocksDBStateBackend constructor // to specify advanced checkpointing configurations such as write buffer size, // you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStateBackend(new EmbeddedRocksDBStateBackend) env.getCheckpointConfig().setCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;) // If you manually passed FsStateBackend into the RocksDBStateBackend constructor // to specify advanced checkpointing configurations such as write buffer size, // you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_state_backend(EmbeddedRocksDBStateBackend()) env.get_checkpoint_config().set_checkpoint_storage_dir(\u0026#34;file:///checkpoint-dir\u0026#34;) # If you manually passed FsStateBackend into the RocksDBStateBackend constructor # to specify advanced checkpointing configurations such as write buffer size, # you can achieve the same results by using manually instantiating a FileSystemCheckpointStorage object. env.get_checkpoint_config().set_checkpoint_storage(FileSystemCheckpointStorage(\u0026#34;file:///checkpoint-dir\u0026#34;)) `}),e.add({id:239,href:"/flink/flink-docs-master/zh/docs/connectors/table/datagen/",title:"DataGen",section:"Table API Connectors",content:` DataGen SQL 连接器 # Scan Source: 有界 Scan Source: 无界
DataGen 连接器允许按数据生成规则进行读取。
DataGen 连接器可以使用计算列语法。 这使您可以灵活地生成记录。
DataGen 连接器是内置的。
注意 不支持复杂类型: Array，Map，Row。 请用计算列构造这些类型。
怎么创建一个 DataGen 的表 # 表的有界性：当表中字段的数据全部生成完成后，source 就结束了。 因此，表的有界性取决于字段的有界性。
每个列，都有两种生成数据的方法：
随机生成器是默认的生成器，您可以指定随机生成的最大和最小值。char、varchar、binary、varbinary, string （类型）可以指定长度。它是无界的生成器。
序列生成器，您可以指定序列的起始和结束值。它是有界的生成器，当序列数字达到结束值，读取结束。
CREATE TABLE datagen ( f_sequence INT, f_random INT, f_random_str STRING, ts AS localtimestamp, WATERMARK FOR ts AS ts ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, -- optional options -- \u0026#39;rows-per-second\u0026#39;=\u0026#39;5\u0026#39;, \u0026#39;fields.f_sequence.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.f_sequence.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.f_sequence.end\u0026#39;=\u0026#39;1000\u0026#39;, \u0026#39;fields.f_random.min\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.f_random.max\u0026#39;=\u0026#39;1000\u0026#39;, \u0026#39;fields.f_random_str.length\u0026#39;=\u0026#39;10\u0026#39; ) 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必须 (none) String 指定要使用的连接器，这里是 'datagen'。 rows-per-second 可选 10000 Long 每秒生成的行数，用以控制数据发出速率。 fields.#.kind 可选 random String 指定 '#' 字段的生成器。可以是 'sequence' 或 'random'。 fields.#.min 可选 (Minimum value of type) (Type of field) 随机生成器的最小值，适用于数字类型。 fields.#.max 可选 (Maximum value of type) (Type of field) 随机生成器的最大值，适用于数字类型。 fields.#.max-past 可选 0 Duration 随机生成器生成相对当前时间向过去偏移的最大值，适用于 timestamp 类型。 fields.#.length 可选 100 Integer 随机生成器生成字符的长度，适用于 char、varchar、binary、varbinary、string。 fields.#.start 可选 (none) (Type of field) 序列生成器的起始值。 fields.#.end 可选 (none) (Type of field) 序列生成器的结束值。 `}),e.add({id:240,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/limit/",title:"LIMIT 语句",section:"Queries 查询",content:` LIMIT 语句 # Batch LIMIT 子句限制 SELECT 语句返回的行数。 通常，此子句与 ORDER BY 结合使用，以确保结果是确定性的。
以下示例选择 Orders 表中的前 3 行。
SELECT * FROM Orders ORDER BY orderTime LIMIT 3 Back to top
`}),e.add({id:241,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/unload/",title:"UNLOAD 语句",section:"SQL",content:` UNLOAD 语句 # UNLOAD 语句用于卸载内置的或用户自定义的模块。
执行 UNLOAD 语句 # Java 可以使用 TableEnvironment 的 executeSql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，executeSql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
Scala 可以使用 TableEnvironment 的 executeSql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，executeSql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
Python 可以使用 TableEnvironment 的 execute_sql() 方法执行 UNLOAD 语句。如果 UNLOAD 操作执行成功，execute_sql() 方法会返回 \u0026lsquo;OK\u0026rsquo;，否则会抛出异常。
以下示例展示了如何在 TableEnvironment 中执行一条 UNLOAD 语句。
SQL CLI UNLOAD 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 UNLOAD 语句。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 卸载 core 模块 tEnv.executeSql(\u0026#34;UNLOAD MODULE core\u0026#34;); tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // Empty set Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() val tEnv = StreamTableEnvironment.create(env) // 卸载 core 模块 tEnv.executeSql(\u0026#34;UNLOAD MODULE core\u0026#34;) tEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // Empty set Python table_env = StreamTableEnvironment.create(...) # 卸载 core 模块 table_env.execute_sql(\u0026#34;UNLOAD MODULE core\u0026#34;) table_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # Empty set SQL CLI Flink SQL\u0026gt; UNLOAD MODULE core; [INFO] Unload module succeeded! Flink SQL\u0026gt; SHOW MODULES; Empty set Back to top
UNLOAD MODULE # 以下语法概述了可用的语法规则：
UNLOAD MODULE module_name `}),e.add({id:242,href:"/flink/flink-docs-master/zh/docs/ops/state/large_state_tuning/",title:"大状态与 Checkpoint 调优",section:"状态与容错",content:` 大状态与 Checkpoint 调优 # 本文提供了如何配置和调整使用大状态的应用程序指南。
概述 # Flink 应用要想在大规模场景下可靠地运行，必须要满足如下两个条件：
应用程序需要能够可靠地创建 checkpoints。
在应用故障后，需要有足够的资源追赶数据输入流。
第一部分讨论如何大规模获得良好性能的 checkpoints。 后一部分解释了一些关于要规划使用多少资源的最佳实践。
监控状态和 Checkpoints # 监控 checkpoint 行为最简单的方法是通过 UI 的 checkpoint 部分。 监控 Checkpoint 的文档说明了如何查看可用的 checkpoint 指标。
这两个指标（均通过 Task 级别 Checkpointing 指标 展示） 以及在 监控 Checkpoint)中，当看 checkpoint 详细信息时，特别有趣的是:
算子收到第一个 checkpoint barrier 的时间。当触发 checkpoint 的耗费时间一直很高时，这意味着 checkpoint barrier 需要很长时间才能从 source 到达 operators。 这通常表明系统处于反压下运行。
Alignment Duration，为处理第一个和最后一个 checkpoint barrier 之间的时间。在 unaligned checkpoints 下，exactly-once 和 at-least-once checkpoints 的 subtasks 处理来自上游 subtasks 的所有数据，且没有任何中断。 然而，对于 aligned exactly-once checkpoints，已经收到 checkpoint barrier 的通道被阻止继续发送数据，直到所有剩余的通道都赶上并接收它们的 checkpoint barrier（对齐时间）。
理想情况下，这两个值都应该很低 - 较高的数值意味着 由于存在反压（没有足够的资源来处理传入的记录），导致checkpoint barriers 在作业中的移动速度较慢，这也可以通过处理记录的端到端延迟在增加来观察到。 请注意，在出现瞬态反压、数据倾斜或网络问题时，这些数值偶尔会很高。
Unaligned checkpoints 可用于加快checkpoint barriers的传播。 但是请注意，这并不能解决导致反压的根本问题(端到端记录延迟仍然很高)。
Checkpoint 调优 # 应用程序可以配置定期触发 checkpoints。 当 checkpoint 完成时间超过 checkpoint 间隔时，在正在进行的 checkpoint 完成之前，不会触发下一个 checkpoint。默认情况下，一旦正在进行的 checkpoint 完成，将立即触发下一个 checkpoint。
当 checkpoints 完成的时间经常超过 checkpoints 基本间隔时(例如，因为状态比计划的更大，或者访问 checkpoints 所在的存储系统暂时变慢)， 系统不断地进行 checkpoints（一旦完成，新的 checkpoints 就会立即启动）。这可能意味着过多的资源被不断地束缚在 checkpointing 中，并且 checkpoint 算子进行得缓慢。 此行为对使用 checkpointed 状态的流式应用程序的影响较小，但仍可能对整体应用程序性能产生影响。
为了防止这种情况，应用程序可以定义 checkpoints 之间的最小等待时间：
StreamExecutionEnvironment.getCheckpointConfig().setMinPauseBetweenCheckpoints(milliseconds)
此持续时间是指从最近一个 checkpoint 结束到下一个 checkpoint 开始之间必须经过的最小时间间隔。下图说明了这如何影响 checkpointing。
注意： 可以配置应用程序（通过CheckpointConfig）允许同时进行多个 checkpoints。 对于 Flink 中状态较大的应用程序，这通常会使用过多的资源到 checkpointing。 当手动触发 savepoint 时，它可能与正在进行的 checkpoint 同时进行。
RocksDB 调优 # 许多大型 Flink 流应用程序的状态存储主要是 RocksDB State Backend。 该backend在主内存之上提供了很好的拓展能力，并且可靠地存储了大的 keyed state。
RocksDB 的性能可能因配置而异，本节讲述了一些使用 RocksDB State Backend 调优作业的最佳实践。
增量 Checkpoint # 在减少 checkpoints 花费的时间方面，开启增量 checkpoints 应该是首要考虑因素。 与完整 checkpoints 相比，增量 checkpoints 可以显着减少 checkpointing 时间，因为增量 checkpoints 仅存储与先前完成的 checkpoint 不同的增量文件，而不是存储全量数据备份。
更多有关背景信息，请参阅 RocksDB 中的增量 Checkpoints。
RocksDB 或 JVM 堆中的计时器 # 计时器（Timer） 默认存储在 RocksDB 中，这是更健壮和可扩展的选择。
当性能调优作业只有少量计时器(没有窗口，且在 ProcessFunction 中不使用计时器)时，将这些计时器放在堆中可以提高性能。 请谨慎使用此功能，因为基于堆的计时器可能会增加 checkpointing 时间，并且自然无法扩展到内存之外。
如何配置基于堆的计时器的有关详细信息，请参阅 计时器（内存 vs. RocksDB）。
RocksDB 内存调优 # RocksDB State Backend 的性能在很大程度上取决于它可用的内存量。为了提高性能，增加内存会有很大的帮助，或者调整内存的功能。 默认情况下，RocksDB State Backend 将 Flink 的托管内存用于 RocksDB 的缓冲区和缓存（State.Backend.RocksDB.memory.managed:true）。请参考 RocksDB 内存管理 了解该机制的工作原理。 关于 RocksDB 内存调优相关的性能问题，如下步骤可能会有所帮助：
尝试提高性能的第一步应该是增加托管内存的大小。这通常会大大改善这种情况，而不是通过调整 RocksDB 底层参数引入复杂性。 尤其是在容器、进程规模较大的情况下，除非应用程序本身逻辑需要大量的 JVM 堆，否则大部分总内存通常都可以用于 RocksDB 。默认的托管内存比例 (0.4) 是保守的，当 TaskManager 进程的内存为很多 GB 时，通常是可以增加该托管内存比例。
在 RocksDB 中，写缓冲区的数量取决于应用程序中所拥有的状态数量（数据流中所有算子的状态）。每个状态对应一个列族（ColumnFamily），它需要自己写缓冲区。因此，具有多状态的应用程序通常需要更多的内存才能获得相同的性能。
你可以尝试设置 state.backend.rocksdb.memory.managed: false 来使用列族（ColumnFamily）内存的 RocksDB 与使用托管内存的 RocksDB 的性能对比。特别是针对基准测试（假设没有或适当的容器内存限制）或回归测试 Flink 早期版本时，这可能会很有用。 与使用托管内存（固定内存池）相比，不使用托管内存意味着 RocksDB 分配的内存与应用程序中的状态数成比例（内存占用随应用程序的变化而变化）。根据经验，非托管模式（除非使用列族（ColumnFamily）RocksDB）的上限约为 “140MB * 跨所有 tasks 的状态 * slots 个数”。 计时器也算作状态！
如果你的应用程序有许多状态，并且你看到频繁的 MemTable 刷新（写端瓶颈），但你不能提供更多的内存，你可以增加写缓冲区的内存比例(state.backend.rocksdb.memory.write-buffer-ratio)。有关详细信息，请参阅 RocksDB 内存管理。
一个高级选项（专家模式）是通过 RocksDBOptionFactory 来调整 RocksDB 的列族（ColumnFamily）选项（块大小、最大后台刷新线程等），以减少具有多种状态的 MemTable 刷新次数：
public class MyOptionsFactory implements ConfigurableRocksDBOptionsFactory { @Override public DBOptions createDBOptions(DBOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { // increase the max background flush threads when we have many states in one operator, // which means we would have many column families in one DB instance. return currentOptions.setMaxBackgroundFlushes(4); } @Override public ColumnFamilyOptions createColumnOptions( ColumnFamilyOptions currentOptions, Collection\u0026lt;AutoCloseable\u0026gt; handlesToClose) { // decrease the arena block size from default 8MB to 1MB. return currentOptions.setArenaBlockSize(1024 * 1024); } @Override public OptionsFactory configure(ReadableConfig configuration) { return this; } } 容量规划 # 本节讨论如何确定 Flink 作业应该使用多少资源才能可靠地运行。 容量规划的基本经验法则是：
应该有足够的资源保障正常运行时不出现反压 如何检查应用程序是否在反压下运行，详细信息请参阅 反压监控。
在无故障时间内无反压运行程序所需的资源之上能够提供一些额外的资源。 需要这些资源来“追赶”在应用程序恢复期间积累的输入数据。 这通常取决于恢复操作需要多长时间（这取决于在故障恢复时需要加载到新 TaskManager 中的状态大小）以及故障恢复的速度。
重要提示：基准点应该在开启 checkpointing 来建立，因为 checkpointing 会占用一些资源（例如网络带宽）。
临时反压通常是允许的，在负载峰值、追赶阶段或外部系统(sink 到外部系统)出现临时减速时，这是执行流控制的重要部分。
在某些操作下（如大窗口）会导致其下游算子的负载激增： 在有窗口的情况下，下游算子可能在构建窗口时几乎无事可做，而在触发窗口时有负载要做。 下游并行度的规划需要考虑窗口的输出量以及处理这种峰值的速度。
重要提示：为了方便以后增加资源，请确保将流应用程序的最大并行度设置为一个合理的数字。最大并行度定义了当扩缩容程序时（通过 savepoint ）可以设置程序并行度的上限。
Flink 的内部以键组(key groups) 的最大并行度为粒度跟踪分布式状态。 Flink 的设计力求使最大并行度的值达到很高的效率，即使执行程序时并行度很低。
压缩 # Flink 为所有 checkpoints 和 savepoints 提供可选的压缩（默认：关闭）。 目前，压缩总是使用 snappy 压缩算法（版本 1.1.4）, 但我们计划在未来支持自定义压缩算法。 压缩作用于 keyed state 下 key-groups 的粒度，即每个 key-groups 可以单独解压缩，这对于重新缩放很重要。
可以通过 ExecutionConfig 开启压缩：
ExecutionConfig executionConfig = new ExecutionConfig(); executionConfig.setUseSnapshotCompression(true); 注意： 压缩选项对增量快照没有影响，因为它们使用的是 RocksDB 的内部格式，该格式始终使用开箱即用的 snappy 压缩。
Task 本地恢复 # 问题引入 # 在 Flink 的 checkpointing 中，每个 task 都会生成其状态快照，然后将其写入分布式存储。 每个 task 通过发送一个描述分布式存储中的位置状态的句柄，向 jobmanager 确认状态的成功写入。 JobManager 反过来收集所有 tasks 的句柄并将它们捆绑到一个 checkpoint 对象中。
在恢复的情况下，jobmanager 打开最新的 checkpoint 对象并将句柄发送回相应的 tasks，然后可以从分布式存储中恢复它们的状态。 使用分布式存储来存储状态有两个重要的优势。 首先，存储是容错的，其次，分布式存储中的所有状态都可以被所有节点访问，并且可以很容易地重新分配（例如，用于重新扩缩容）。
但是，使用远程分布式存储也有一个很大的缺点：所有 tasks 都必须通过网络从远程位置读取它们的状态。 在许多场景中，恢复可能会将失败的 tasks 重新调度到与前一次运行相同的 taskmanager 中(当然也有像机器故障这样的异常)，但我们仍然必须读取远程状态。这可能导致大状态的长时间恢复，即使在一台机器上只有一个小故障。
解决办法 # Task 本地状态恢复正是针对这个恢复时间长的问题，其主要思想如下：对于每个 checkpoint ，每个 task 不仅将 task 状态写入分布式存储中， 而且还在 task 本地存储(例如本地磁盘或内存)中保存状态快照的次要副本。请注意，快照的主存储仍然必须是分布式存储，因为本地存储不能确保节点故障下的持久性，也不能为其他节点提供重新分发状态的访问，所以这个功能仍然需要主副本。
然而，对于每个 task 可以重新调度到以前的位置进行恢复的 task ，我们可以从次要本地状态副本恢复，并避免远程读取状态的成本。考虑到许多故障不是节点故障，即使节点故障通常一次只影响一个或非常少的节点， 在恢复过程中，大多数 task 很可能会重新部署到它们以前的位置，并发现它们的本地状态完好无损。这就是 task 本地恢复有效地减少恢复时间的原因。
请注意，根据所选的 state backend 和 checkpointing 策略，在每个 checkpoint 创建和存储次要本地状态副本时，可能会有一些额外的成本。 例如，在大多数情况下，实现只是简单地将对分布式存储的写操作复制到本地文件。
主要（分布式存储）和次要（task 本地）状态快照的关系 # Task 本地状态始终被视为次要副本，checkpoint 状态始终以分布式存储中的副本为主。 这对 checkpointing 和恢复期间的本地状态问题有影响：
对于 checkpointing ，主副本必须成功，并且生成次要本地副本的失败不会使 checkpoint 失败。 如果无法创建主副本，即使已成功创建次要副本，checkpoint 也会失败。
只有主副本由 jobmanager 确认和管理，次要副本属于 taskmanager ，并且它们的生命周期可以独立于它们的主副本。 例如，可以保留 3 个最新 checkpoints 的历史记录作为主副本，并且只保留最新 checkpoint 的 task 本地状态。
对于恢复，如果匹配的次要副本可用，Flink 将始终首先尝试从 task 本地状态恢复。 如果在次要副本恢复过程中出现任何问题，Flink 将透明地重试从主副本恢复 task。 仅当主副本和（可选）次要副本失败时，恢复才会失败。 在这种情况下，根据配置，Flink 仍可能回退到旧的 checkpoint。
Task 本地副本可能仅包含完整 task 状态的一部分（例如，写入一个本地文件时出现异常）。 在这种情况下，Flink 会首先尝试在本地恢复本地部分，非本地状态从主副本恢复。 主状态必须始终是完整的，并且是 task 本地状态的超集。
Task 本地状态可以具有与主状态不同的格式，它们不需要相同字节。 例如，task 本地状态甚至可能是在堆对象组成的内存中，而不是存储在任何文件中。
如果 taskmanager 丢失，则其所有 task 的本地状态都会丢失。
配置 task 本地恢复 # Task 本地恢复 默认禁用，可以通过 Flink 的 CheckpointingOptions.LOCAL_RECOVERY 配置中指定的键 state.backend.local-recovery 来启用。 此设置的值可以是 true 以启用或 false（默认）以禁用本地恢复。
注意，unaligned checkpoints 目前不支持 task 本地恢复。
不同 state backends 的 task 本地恢复的详细介绍 # 限制：目前，task 本地恢复仅涵盖 keyed state backends。 Keyed state 通常是该状态的最大部分。 在不久的将来，我们还将支持算子状态和计时器（timers）。
以下 state backends 可以支持 task 本地恢复。
FsStateBackend: keyed state 支持 task 本地恢复。 该实现会将状态复制到本地文件。 这会引入额外的写入成本并占用本地磁盘空间。 将来，我们可能还会提供一种将 task 本地状态保存在内存中的实现。
RocksDBStateBackend: 支持 keyed state 的 task 本地恢复。对于全量 checkpoints，状态被复制到本地文件。这会引入额外的写入成本并占用本地磁盘空间。对于增量快照，本地状态基于 RocksDB 的原生 checkpointing 机制。 这种机制也被用作创建主副本的第一步，这意味着在这种情况下，创建次要副本不会引入额外的成本。我们只是保留本地 checkpoint 目录， 而不是在上传到分布式存储后将其删除。这个本地副本可以与 RocksDB 的工作目录共享现有文件（通过硬链接），因此对于现有文件，增量快照的 task 本地恢复也不会消耗额外的磁盘空间。 使用硬链接还意味着 RocksDB 目录必须与所有可用于存储本地状态和本地恢复目录位于同一节点上，否则建立硬链接可能会失败（参见 FLINK-10954）。 目前，当 RocksDB 目录配置在多个物理设备上时，这也会阻止使用本地恢复。
Allocation-preserving 调度 # Task 本地恢复假设在故障下通过 allocation-preserving 调度 task ，其工作原理如下。 每个 task 都会记住其先前的分配，并请求完全相同的 slot 来重新启动恢复。 如果此 slot 不可用，task 将向 resourcemanager 请求一个 新的 slot。 这样，如果 taskmanager 不再可用，则无法返回其先前位置的 task 不会将其他正在恢复的 task 踢出其之前的 slot。 我们的理由是，只有当 taskmanager 不再可用时，前一个 slot 才会消失，在这种情况下，一些 tasks 无论如何都必须请求新的 slot 。 在我们的调度策略中，我们让绝大多数的 tasks 有机会从它们的本地状态中恢复，从而避免了从其他 tasks 处获取它们之前的 slots 的级联效应。
Back to top
`}),e.add({id:243,href:"/flink/flink-docs-master/zh/docs/dev/dataset/cluster_execution/",title:"集群执行",section:"DataSet API (Legacy)",content:` 集群执行 # Flink 程序可以分布式运行在多机器集群上。有两种方式可以将程序提交到集群上执行：
命令行界面（Interface） # 命令行界面使你可以将打包的程序（JARs）提交到集群（或单机设置）。
有关详细信息，请参阅命令行界面文档。
远程环境（Remote Environment） # 远程环境使你可以直接在集群上执行 Flink Java 程序。远程环境指向你要执行程序的集群。
Maven Dependency # 如果将程序作为 Maven 项目开发，则必须添加 flink-clients 模块的依赖：
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 示例 # 下面演示了 RemoteEnvironment 的用法：
public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment .createRemoteEnvironment(\u0026#34;flink-jobmanager\u0026#34;, 8081, \u0026#34;/home/user/udfs.jar\u0026#34;); DataSet\u0026lt;String\u0026gt; data = env.readTextFile(\u0026#34;hdfs://path/to/file\u0026#34;); data .filter(new FilterFunction\u0026lt;String\u0026gt;() { public boolean filter(String value) { return value.startsWith(\u0026#34;http://\u0026#34;); } }) .writeAsText(\u0026#34;hdfs://path/to/result\u0026#34;); env.execute(); } 请注意，该程序包含用户自定义代码，因此需要一个带有附加代码类的 JAR 文件。远程环境的构造函数使用 JAR 文件的路径进行构造。
Back to top
`}),e.add({id:244,href:"/flink/flink-docs-master/zh/docs/connectors/table/print/",title:"Print",section:"Table API Connectors",content:` Print SQL 连接器 # Sink Print 连接器允许将每一行写入标准输出流或者标准错误流。
设计目的：
简单的流作业测试。 对生产调试带来极大便利。 四种 format 选项：
打印内容 条件 1 条件 2 标识符:任务 ID\u003e 输出数据 需要提供前缀打印标识符 parallelism \u003e 1 标识符\u003e 输出数据 需要提供前缀打印标识符 parallelism == 1 任务 ID\u003e 输出数据 不需要提供前缀打印标识符 parallelism \u003e 1 输出数据 不需要提供前缀打印标识符 parallelism == 1 输出字符串格式为 \u0026ldquo;\$row_kind(f0,f1,f2\u0026hellip;)\u0026quot;，row_kind是一个 RowKind 类型的短字符串，例如：\u0026quot;+I(1,1)\u0026quot;。
Print 连接器是内置的。
注意 在任务运行时使用 Print Sinks 打印记录，你需要注意观察任务日志。
如何创建一张基于 Print 的表 # CREATE TABLE print_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) 或者，也可以通过 LIKE子句 基于已有表的结构去创建新表。
SQL CREATE TABLE print_table WITH (\u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39;) LIKE source_table (EXCLUDING ALL) 连接器参数 # 参数 是否必选 默认值 数据类型 描述 connector 必选 (none) String 指定要使用的连接器，此处应为 'print' print-identifier 可选 (none) String 配置一个标识符作为输出数据的前缀。 standard-error 可选 false Boolean 如果 format 需要打印为标准错误而不是标准输出，则为 True 。 sink.parallelism 可选 (none) Integer 为 Print sink operator 定义并行度。默认情况下，并行度由框架决定，和链在一起的上游 operator 一致。 `}),e.add({id:245,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/set/",title:"SET 语句",section:"SQL",content:` SET 语句 # SET 语句用于修改配置或展示配置。
执行 SET 语句 # SQL CLI SET 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 SET 语句。
SQL CLI Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Europe/Berlin\u0026#39;; [INFO] Session property has been set. Flink SQL\u0026gt; SET; \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Europe/Berlin\u0026#39; Syntax # SET (\u0026#39;key\u0026#39; = \u0026#39;value\u0026#39;)? 如果没有指定 key 和 value，它仅仅打印所有属性。否则，它会为 key 设置指定的 value 值。
Back to top
`}),e.add({id:246,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/topn/",title:"Top-N",section:"Queries 查询",content:` Top-N # Batch Streaming
Top-N queries ask for the N smallest or largest values ordered by columns. Both smallest and largest values sets are considered Top-N queries. Top-N queries are useful in cases where the need is to display only the N bottom-most or the N top- most records from batch/streaming table on a condition. This result set can be used for further analysis.
Flink uses the combination of a OVER window clause and a filter condition to express a Top-N query. With the power of OVER window PARTITION BY clause, Flink also supports per group Top-N. For example, the top five products per category that have the maximum sales in realtime. Top-N queries are supported for SQL on batch and streaming tables.
The following shows the syntax of the Top-N statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]] ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum FROM table_name) WHERE rownum \u0026lt;= N [AND conditions] Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one, according to the ordering of rows within the partition. Currently, we only support ROW_NUMBER as the over window function. In the future, we will support RANK() and DENSE_RANK(). PARTITION BY col1[, col2...]: Specifies the partition columns. Each partition will have a Top-N result. ORDER BY col1 [asc|desc][, col2 [asc|desc]...]: Specifies the ordering columns. The ordering directions can be different on different columns. WHERE rownum \u0026lt;= N: The rownum \u0026lt;= N is required for Flink to recognize this query is a Top-N query. The N represents the N smallest or largest records will be retained. [AND conditions]: It is free to add other conditions in the where clause, but the other conditions can only be combined with rownum \u0026lt;= N using AND conjunction. Note: the above pattern must be followed exactly, otherwise the optimizer won’t be able to translate the query. The TopN query is Result Updating. Flink SQL will sort the input data stream according to the order key, so if the top N records have been changed, the changed ones will be sent as retraction/update records to downstream. It is recommended to use a storage which supports updating as the sink of Top-N query. In addition, if the top N records need to be stored in external storage, the result table should have the same unique key with the Top-N query. The unique keys of Top-N query is the combination of partition columns and rownum column. Top-N query can also derive the unique key of upstream. Take following job as an example, say product_id is the unique key of the ShopSales, then the unique keys of the Top-N query are [category, rownum] and [product_id].
The following examples show how to specify SQL queries with Top-N on streaming tables. This is an example to get \u0026ldquo;the top five products per category that have the maximum sales in realtime\u0026rdquo; we mentioned above.
CREATE TABLE ShopSales ( product_id STRING, category STRING, product_name STRING, sales BIGINT ) WITH (...); SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num FROM ShopSales) WHERE row_num \u0026lt;= 5 No Ranking Output Optimization # As described above, the rownum field will be written into the result table as one field of the unique key, which may lead to a lot of records being written to the result table. For example, when the record (say product-1001) of ranking 9 is updated and its rank is upgraded to 1, all the records from ranking 1 ~ 9 will be output to the result table as update messages. If the result table receives too many data, it will become the bottleneck of the SQL job.
The optimization way is omitting rownum field in the outer SELECT clause of the Top-N query. This is reasonable because the number of the top N records is usually not large, thus the consumers can sort the records themselves quickly. Without rownum field, in the example above, only the changed record (product-1001) needs to be sent to downstream, which can reduce much IO to the result table.
The following example shows how to optimize the above Top-N example in this way:
CREATE TABLE ShopSales ( product_id STRING, category STRING, product_name STRING, sales BIGINT ) WITH (...); -- omit row_num field from the output SELECT product_id, category, product_name, sales FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS row_num FROM ShopSales) WHERE row_num \u0026lt;= 5 Attention in Streaming Mode In order to output the above query to an external storage and have a correct result, the external storage must have the same unique key with the Top-N query. In the above example query, if the product_id is the unique key of the query, then the external table should also has product_id as the unique key.
Back to top
`}),e.add({id:247,href:"/flink/flink-docs-master/zh/docs/connectors/table/blackhole/",title:"BlackHole",section:"Table API Connectors",content:` BlackHole SQL 连接器 # Sink: Bounded Sink: UnBounded
BlackHole 连接器允许接收所有输入记录。它被设计用于：
高性能测试。 UDF 输出，而不是实质性 sink。 就像类 Unix 操作系统上的 /dev/null。
BlackHole 连接器是内置的。
如何创建 BlackHole 表 # CREATE TABLE blackhole_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39; ); 或者，可以基于现有模式使用 LIKE 子句 创建。
CREATE TABLE blackhole_table WITH (\u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39;) LIKE source_table (EXCLUDING ALL) 连接器选项 # 选项 是否必要 默认值 类型 描述 connector 必要 (none) String 指定需要使用的连接器，此处应为‘blackhole’。 `}),e.add({id:248,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/reset/",title:"RESET 语句",section:"SQL",content:` RESET 语句 # RESET 语句用于将配置重置为默认值。
执行 RESET 语句 # SQL CLI RESET 语句可以在 SQL CLI 中执行。
以下示例展示了如何在 SQL CLI 中执行一条 RESET 语句。
SQL CLI Flink SQL\u0026gt; RESET \u0026#39;table.planner\u0026#39;; [INFO] Session property has been reset. Flink SQL\u0026gt; RESET; [INFO] All session properties have been set to their default values. Syntax # RESET (\u0026#39;key\u0026#39;)? 如果未指定 key，则将所有属性重置为默认值。否则，将指定的 key 重置为默认值。
Back to top
`}),e.add({id:249,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-topn/",title:"窗口 Top-N",section:"Queries 查询",content:` Window Top-N # Batch Streaming
Window Top-N is a special Top-N which returns the N smallest or largest values for each window and other partitioned keys.
For streaming queries, unlike regular Top-N on continuous tables, window Top-N does not emit intermediate results but only a final result, the total top N records at the end of the window. Moreover, window Top-N purges all intermediate state when no longer needed. Therefore, window Top-N queries have better performance if users don\u0026rsquo;t need results updated per record. Usually, Window Top-N is used with Windowing TVF directly. Besides, Window Top-N could be used with other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Window Top-N can be defined in the same syntax as regular Top-N, see Top-N documentation for more information. Besides that, Window Top-N requires the PARTITION BY clause contains window_start and window_end columns of the relation applied Windowing TVF or Window Aggregation. Otherwise, the optimizer won’t be able to translate the query.
The following shows the syntax of the Window Top-N statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...] ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum FROM table_name) -- relation applied windowing TVF WHERE rownum \u0026lt;= N [AND conditions] Example # Window Top-N follows after Window Aggregation # The following example shows how to calculate Top 3 suppliers who have the highest sales for every tumbling 10 minutes window.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; desc Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | | supplier_id | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+-------------+ | bidtime | price | item | supplier_id | +------------------+-------+------+-------------+ | 2020-04-15 08:05 | 4.00 | A | supplier1 | | 2020-04-15 08:06 | 4.00 | C | supplier2 | | 2020-04-15 08:07 | 2.00 | G | supplier1 | | 2020-04-15 08:08 | 2.00 | B | supplier3 | | 2020-04-15 08:09 | 5.00 | D | supplier4 | | 2020-04-15 08:11 | 2.00 | B | supplier3 | | 2020-04-15 08:13 | 1.00 | E | supplier1 | | 2020-04-15 08:15 | 3.00 | H | supplier2 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | +------------------+-------+------+-------------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum FROM ( SELECT window_start, window_end, supplier_id, SUM(price) as price, COUNT(*) as cnt FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end, supplier_id ) ) WHERE rownum \u0026lt;= 3; +------------------+------------------+-------------+-------+-----+--------+ | window_start | window_end | supplier_id | price | cnt | rownum | +------------------+------------------+-------------+-------+-----+--------+ | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier1 | 6.00 | 2 | 1 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier4 | 5.00 | 1 | 2 | | 2020-04-15 08:00 | 2020-04-15 08:10 | supplier2 | 4.00 | 1 | 3 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier5 | 6.00 | 1 | 1 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier2 | 3.00 | 1 | 2 | | 2020-04-15 08:10 | 2020-04-15 08:20 | supplier3 | 2.00 | 1 | 3 | +------------------+------------------+-------------+-------+-----+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Window Top-N follows after Windowing TVF # The following example shows how to calculate Top 3 items which have the highest price for every tumbling 10 minutes window.
Flink SQL\u0026gt; SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY price DESC) as rownum FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) ) WHERE rownum \u0026lt;= 3; +------------------+-------+------+-------------+------------------+------------------+--------+ | bidtime | price | item | supplier_id | window_start | window_end | rownum | +------------------+-------+------+-------------+------------------+------------------+--------+ | 2020-04-15 08:05 | 4.00 | A | supplier1 | 2020-04-15 08:00 | 2020-04-15 08:10 | 2 | | 2020-04-15 08:06 | 4.00 | C | supplier2 | 2020-04-15 08:00 | 2020-04-15 08:10 | 3 | | 2020-04-15 08:09 | 5.00 | D | supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 | 1 | | 2020-04-15 08:11 | 2.00 | B | supplier3 | 2020-04-15 08:10 | 2020-04-15 08:20 | 3 | | 2020-04-15 08:15 | 3.00 | H | supplier2 | 2020-04-15 08:10 | 2020-04-15 08:20 | 2 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 | 1 | +------------------+-------+------+-------------+------------------+------------------+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Currently, Flink only supports Window Top-N follows after Windowing TVF with Tumble Windows, Hop Windows and Cumulate Windows. Window Top-N follows after Windowing TVF with Session windows will be supported in the near future.
Back to top
`}),e.add({id:250,href:"/flink/flink-docs-master/zh/docs/connectors/table/hive/",title:"Hive",section:"Table API Connectors",content:""}),e.add({id:251,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/jar/",title:"JAR Statements",section:"SQL",content:` JAR 语句 # JAR 语句用于将用户 jar 添加到 classpath、或将用户 jar 从 classpath 中删除或展示运行时 classpath 中添加的 jar。
目前 Flink SQL 支持以下 JAR 语句：
ADD JAR REMOVE JAR SHOW JARS 注意 JAR 语句仅适用于 SQL CLI。
执行 JAR 语句 # SQL CLI 以下示例展示了如何在 SQL CLI 中运行 JAR 语句。 SQL CLI Flink SQL\u0026gt; ADD JAR \u0026#39;/path/hello.jar\u0026#39;; [INFO] The specified jar is added into session classloader. Flink SQL\u0026gt; SHOW JARS; /path/hello.jar Flink SQL\u0026gt; REMOVE JAR \u0026#39;/path/hello.jar\u0026#39;; [INFO] The specified jar is removed from session classloader. ADD JAR # ADD JAR \u0026#39;\u0026lt;path_to_filename\u0026gt;.jar\u0026#39; 目前只支持将本地 jar 添加到会话类类加载器（session classloader）中。
REMOVE JAR # REMOVE JAR \u0026#39;\u0026lt;path_to_filename\u0026gt;.jar\u0026#39; 目前只支持删除 ADD JAR 语句添加的 jar。
SHOW JARS # SHOW JARS 展示会话类类加载器（session classloader）中所有基于 ADD JAR 语句添加的 jar。
Back to top
`}),e.add({id:252,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-deduplication/",title:"窗口去重",section:"Queries 查询",content:` Window Deduplication # Streaming Window Deduplication is a special Deduplication which removes rows that duplicate over a set of columns, keeping the first one or the last one for each window and partitioned keys.
For streaming queries, unlike regular Deduplicate on continuous tables, Window Deduplication does not emit intermediate results but only a final result at the end of the window. Moreover, window Deduplication purges all intermediate state when no longer needed. Therefore, Window Deduplication queries have better performance if users don\u0026rsquo;t need results updated per record. Usually, Window Deduplication is used with Windowing TVF directly. Besides, Window Deduplication could be used with other operations based on Windowing TVF, such as Window Aggregation, Window TopN and Window Join.
Window Deduplication can be defined in the same syntax as regular Deduplication, see Deduplication documentation for more information. Besides that, Window Deduplication requires the PARTITION BY clause contains window_start and window_end columns of the relation. Otherwise, the optimizer won’t be able to translate the query.
Flink uses ROW_NUMBER() to remove duplicates, just like the way of Window Top-N query. In theory, Window Deduplication is a special case of Window Top-N in which the N is one and order by the processing time or event time.
The following shows the syntax of the Window Deduplication statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER (PARTITION BY window_start, window_end [, col_key1...] ORDER BY time_attr [asc|desc]) AS rownum FROM table_name) -- relation applied windowing TVF WHERE (rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2) [AND conditions] Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one. PARTITION BY window_start, window_end [, col_key1...]: Specifies the partition columns which contain window_start, window_end and other partition keys. ORDER BY time_attr [asc|desc]: Specifies the ordering column, it must be a time attribute. Currently Flink supports processing time attribute and event time attribute. Ordering by ASC means keeping the first row, ordering by DESC means keeping the last row. WHERE (rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2): The rownum = 1 | rownum \u0026lt;=1 | rownum \u0026lt; 2 is required for the optimizer to recognize the query could be translated to Window Deduplication. Note: the above pattern must be followed exactly, otherwise the optimizer won’t translate the query to Window Deduplication. Example # The following example shows how to keep last record for every 10 minutes tumbling window.
-- tables must have time attribute, e.g. \`bidtime\` in this table Flink SQL\u0026gt; DESC Bid; +-------------+------------------------+------+-----+--------+---------------------------------+ | name | type | null | key | extras | watermark | +-------------+------------------------+------+-----+--------+---------------------------------+ | bidtime | TIMESTAMP(3) *ROWTIME* | true | | | \`bidtime\` - INTERVAL \u0026#39;1\u0026#39; SECOND | | price | DECIMAL(10, 2) | true | | | | | item | STRING | true | | | | +-------------+------------------------+------+-----+--------+---------------------------------+ Flink SQL\u0026gt; SELECT * FROM Bid; +------------------+-------+------+ | bidtime | price | item | +------------------+-------+------+ | 2020-04-15 08:05 | 4.00 | C | | 2020-04-15 08:07 | 2.00 | A | | 2020-04-15 08:09 | 5.00 | D | | 2020-04-15 08:11 | 3.00 | B | | 2020-04-15 08:13 | 1.00 | E | | 2020-04-15 08:17 | 6.00 | F | +------------------+-------+------+ Flink SQL\u0026gt; SELECT * FROM ( SELECT bidtime, price, item, supplier_id, window_start, window_end, ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY bidtime DESC) AS rownum FROM TABLE( TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL \u0026#39;10\u0026#39; MINUTES)) ) WHERE rownum \u0026lt;= 1; +------------------+-------+------+-------------+------------------+------------------+--------+ | bidtime | price | item | supplier_id | window_start | window_end | rownum | +------------------+-------+------+-------------+------------------+------------------+--------+ | 2020-04-15 08:09 | 5.00 | D | supplier4 | 2020-04-15 08:00 | 2020-04-15 08:10 | 1 | | 2020-04-15 08:17 | 6.00 | F | supplier5 | 2020-04-15 08:10 | 2020-04-15 08:20 | 1 | +------------------+-------+------+-------------+------------------+------------------+--------+ Note: in order to better understand the behavior of windowing, we simplify the displaying of timestamp values to not show the trailing zeros, e.g. 2020-04-15 08:05 should be displayed as 2020-04-15 08:05:00.000 in Flink SQL Client if the type is TIMESTAMP(3).
Limitation # Limitation on Window Deduplication which follows after Windowing TVFs directly # Currently, if Window Deduplication follows after Windowing TVF, the Windowing TVF has to be with Tumble Windows, Hop Windows or Cumulate Windows instead of Session windows. Session windows will be supported in the near future.
Limitation on time attribute of order key # Currently, Window Deduplication requires order key must be event time attribute instead of processing time attribute. Ordering by processing-time would be supported in the near future.
Back to top
`}),e.add({id:253,href:"/flink/flink-docs-master/zh/docs/dev/python/installation/",title:"环境安装",section:"Python API",content:` 环境安装 # 环境要求 # 注意 PyFlink 需要 Python 3.6 以上版本（3.6, 3.7 或 3.8）。请运行以下命令，以确保 Python 版本满足要求。
\$ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 环境设置 # 你的系统也许安装了好几个版本的 Python。你可以运行下面的 ls 命令来查看当前系统中安装的 Python 版本有哪些:
\$ ls /usr/bin/python* 为了满足 Python 版本要求，你可以选择通过软链接的方式将 python 指向 python3 解释器:
ln -s /usr/bin/python3 python 除了软链接的方式，你也可以选择创建一个 Python virtual env（venv）的方式。关于如何创建一个 virtual env，你可以参考准备 Python 虚拟环境。
如果你不想使用软链接的方式改变系统 Python 解释器的路径，你也可以通过配置的方式指定 Python 解释器。 你可以参考配置python.client.executable，了解如何指定编译作业时所使用的 Python 解释器路径， 以及参考配置python.executable，了解如何指定执行 Python UDF 时所使用的 Python 解释器路径。
PyFlink 安装 # PyFlink 已经被发布到PyPi，可以通过如下方式安装 PyFlink：
\$ python -m pip install apache-flink 你也可以从源码手动构建 PyFlink，具体可以参见开发指南.
注意 从Flink 1.11版本开始, PyFlink 作业支持在 Windows 系统上运行，因此您也可以在 Windows 上开发和调试 PyFlink 作业了。
`}),e.add({id:254,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/deduplication/",title:"去重",section:"Queries 查询",content:` Deduplication # Batch Streaming
Deduplication removes rows that duplicate over a set of columns, keeping only the first one or the last one. In some cases, the upstream ETL jobs are not end-to-end exactly-once; this may result in duplicate records in the sink in case of failover. However, the duplicate records will affect the correctness of downstream analytical jobs - e.g. SUM, COUNT - so deduplication is needed before further analysis.
Flink uses ROW_NUMBER() to remove duplicates, just like the way of Top-N query. In theory, deduplication is a special case of Top-N in which the N is one and order by the processing time or event time.
The following shows the syntax of the Deduplication statement:
SELECT [column_list] FROM ( SELECT [column_list], ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]] ORDER BY time_attr [asc|desc]) AS rownum FROM table_name) WHERE rownum = 1 Parameter Specification:
ROW_NUMBER(): Assigns an unique, sequential number to each row, starting with one. PARTITION BY col1[, col2...]: Specifies the partition columns, i.e. the deduplicate key. ORDER BY time_attr [asc|desc]: Specifies the ordering column, it must be a time attribute. Currently Flink supports processing time attribute and event time attribute. Ordering by ASC means keeping the first row, ordering by DESC means keeping the last row. WHERE rownum = 1: The rownum = 1 is required for Flink to recognize this query is deduplication. Note: the above pattern must be followed exactly, otherwise the optimizer won’t be able to translate the query. The following examples show how to specify SQL queries with Deduplication on streaming tables.
CREATE TABLE Orders ( order_id STRING, user STRING, product STRING, num BIGINT, proctime AS PROCTIME() ) WITH (...); -- remove duplicate rows on order_id and keep the first occurrence row, -- because there shouldn\u0026#39;t be two orders with the same order_id. SELECT order_id, user, product, num FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY proctime ASC) AS row_num FROM Orders) WHERE row_num = 1 Back to top
`}),e.add({id:255,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/queries/match_recognize/",title:"模式检测",section:"Queries 查询",content:` 模式检测 # Streaming 搜索一组事件模式（event pattern）是一种常见的用例，尤其是在数据流情景中。Flink 提供复杂事件处理（CEP）库，该库允许在事件流中进行模式检测。此外，Flink 的 SQL API 提供了一种关系式的查询表达方式，其中包含大量内置函数和基于规则的优化，可以开箱即用。
2016 年 12 月，国际标准化组织（ISO）发布了新版本的 SQL 标准，其中包括在 SQL 中的行模式识别（Row Pattern Recognition in SQL）(ISO/IEC TR 19075-5:2016)。它允许 Flink 使用 MATCH_RECOGNIZE 子句融合 CEP 和 SQL API，以便在 SQL 中进行复杂事件处理。
MATCH_RECOGNIZE 子句启用以下任务：
使用 PARTITION BY 和 ORDER BY 子句对数据进行逻辑分区和排序。 使用 PATTERN 子句定义要查找的行模式。这些模式使用类似于正则表达式的语法。 在 DEFINE 子句中指定行模式变量的逻辑组合。 measures 是指在 MEASURES 子句中定义的表达式，这些表达式可用于 SQL 查询中的其他部分。 下面的示例演示了基本模式识别的语法：
SELECT T.aid, T.bid, T.cid FROM MyTable MATCH_RECOGNIZE ( PARTITION BY userid ORDER BY proctime MEASURES A.id AS aid, B.id AS bid, C.id AS cid PATTERN (A B C) DEFINE A AS name = \u0026#39;a\u0026#39;, B AS name = \u0026#39;b\u0026#39;, C AS name = \u0026#39;c\u0026#39; ) AS T 本页将更详细地解释每个关键字，并演示说明更复杂的示例。
Flink 的 MATCH_RECOGNIZE 子句实现是一个完整标准子集。仅支持以下部分中记录的功能。基于社区反馈，可能会支持其他功能，请查看已知的局限。 介绍和示例 # 安装指南 # 模式识别特性使用 Apache Flink 内部的 CEP 库。为了能够使用 MATCH_RECOGNIZE 子句，需要将库作为依赖项添加到 Maven 项目中。
\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-cep\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 或者，也可以将依赖项添加到集群的 classpath（查看 dependency section 获取更多相关依赖信息）。
如果你想在 SQL Client 中使用 MATCH_RECOGNIZE 子句，你无需执行任何操作，因为默认情况下包含所有依赖项。
SQL 语义 # 每个 MATCH_RECOGNIZE 查询都包含以下子句：
PARTITION BY - 定义表的逻辑分区；类似于 GROUP BY 操作。 ORDER BY - 指定传入行的排序方式；这是必须的，因为模式依赖于顺序。 MEASURES - 定义子句的输出；类似于 SELECT 子句。 ONE ROW PER MATCH - 输出方式，定义每个匹配项应产生多少行。 AFTER MATCH SKIP - 指定下一个匹配的开始位置；这也是控制单个事件可以属于多少个不同匹配项的方法。 PATTERN - 允许使用类似于 正则表达式 的语法构造搜索的模式。 DEFINE - 本部分定义了模式变量必须满足的条件。 注意 目前，MATCH_RECOGNIZE 子句只能应用于追加表。此外，它也总是生成一个追加表。
示例 # 对于我们的示例，我们假设已经注册了一个表 Ticker。该表包含特定时间点的股票价格。
这张表的 schema 如下：
Ticker |-- symbol: String # 股票的代号 |-- price: Long # 股票的价格 |-- tax: Long # 股票应纳税额 |-- rowtime: TimeIndicatorTypeInfo(rowtime) # 更改这些值的时间点 为了简化，我们只考虑单个股票 ACME 的传入数据。Ticker 可以类似于下表，其中的行是连续追加的。
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 12 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:01\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:02\u0026#39; 19 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:03\u0026#39; 21 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:04\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:05\u0026#39; 18 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:06\u0026#39; 15 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:07\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:08\u0026#39; 24 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:09\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:10\u0026#39; 19 1 现在的任务是找出一个单一股票价格不断下降的时期。为此，可以编写如下查询：
SELECT * FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES START_ROW.rowtime AS start_tstamp, LAST(PRICE_DOWN.rowtime) AS bottom_tstamp, LAST(PRICE_UP.rowtime) AS end_tstamp ONE ROW PER MATCH AFTER MATCH SKIP TO LAST PRICE_UP PATTERN (START_ROW PRICE_DOWN+ PRICE_UP) DEFINE PRICE_DOWN AS (LAST(PRICE_DOWN.price, 1) IS NULL AND PRICE_DOWN.price \u0026lt; START_ROW.price) OR PRICE_DOWN.price \u0026lt; LAST(PRICE_DOWN.price, 1), PRICE_UP AS PRICE_UP.price \u0026gt; LAST(PRICE_DOWN.price, 1) ) MR; 此查询将 Ticker 表按照 symbol 列进行分区并按照 rowtime 属性进行排序。
PATTERN 子句指定我们对以下模式感兴趣：该模式具有开始事件 START_ROW，然后是一个或多个 PRICE_DOWN 事件，并以 PRICE_UP 事件结束。如果可以找到这样的模式，如 AFTER MATCH SKIP TO LAST 子句所示，则从最后一个 PRICE_UP 事件开始寻找下一个模式匹配。
DEFINE 子句指定 PRICE_DOWN 和 PRICE_UP 事件需要满足的条件。尽管不存在 START_ROW 模式变量，但它具有一个始终被评估为 TRUE 隐式条件。
模式变量 PRICE_DOWN 定义为价格小于满足 PRICE_DOWN 条件的最后一行。对于初始情况或没有满足 PRICE_DOWN 条件的最后一行时，该行的价格应小于该模式中前一行（由 START_ROW 引用）的价格。
模式变量 PRICE_UP 定义为价格大于满足 PRICE_DOWN 条件的最后一行。
此查询为股票价格持续下跌的每个期间生成摘要行。
在查询的 MEASURES 子句部分定义确切的输出行信息。输出行数由 ONE ROW PER MATCH 输出方式定义。
symbol start_tstamp bottom_tstamp end_tstamp ========= ================== ================== ================== ACME 01-APR-11 10:00:04 01-APR-11 10:00:07 01-APR-11 10:00:08 该行结果描述了从 01-APR-11 10:00:04 开始的价格下跌期，在 01-APR-11 10:00:07 达到最低价格，到 01-APR-11 10:00:08 再次上涨。
分区 # 可以在分区数据中寻找模式，例如单个股票行情或特定用户的趋势。这可以用 PARTITION BY 子句来表示。该子句类似于对 aggregation 使用 GROUP BY。
注意 强烈建议对传入的数据进行分区，否则 MATCH_RECOGNIZE 子句将被转换为非并行算子，以确保全局排序。
事件顺序 # Apache Flink 可以根据时间（处理时间或者事件时间）进行模式搜索。
如果是事件时间，则在将事件传递到内部模式状态机之前对其进行排序。所以，无论行添加到表的顺序如何，生成的输出都是正确的。而模式是按照每行中所包含的时间指定顺序计算的。
MATCH_RECOGNIZE 子句假定升序的 时间属性 是 ORDER BY 子句的第一个参数。
对于示例 Ticker 表，诸如 ORDER BY rowtime ASC, price DESC 的定义是有效的，但 ORDER BY price, rowtime 或者 ORDER BY rowtime DESC, price ASC 是无效的。
Define \u0026amp; Measures # DEFINE 和 MEASURES 关键字与简单 SQL 查询中的 WHERE 和 SELECT 子句具有相近的含义。
MEASURES 子句定义匹配模式的输出中要包含哪些内容。它可以投影列并定义表达式进行计算。产生的行数取决于输出方式设置。
DEFINE 子句指定行必须满足的条件才能被分类到相应的模式变量。如果没有为模式变量定义条件，则将对每一行使用计算结果为 true 的默认条件。
有关在这些子句中可使用的表达式的更详细的说明，请查看事件流导航部分。
Aggregations # Aggregations 可以在 DEFINE 和 MEASURES 子句中使用。支持内置函数和用户自定义函数。
对相应匹配项的行子集可以使用 Aggregate functions。请查看事件流导航部分以了解如何计算这些子集。
下面这个示例的任务是找出股票平均价格没有低于某个阈值的最长时间段。它展示了 MATCH_RECOGNIZE 在 aggregation 中的可表达性。可以使用以下查询执行此任务：
SELECT * FROM Ticker MATCH_RECOGNIZE ( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.rowtime) AS start_tstamp, LAST(A.rowtime) AS end_tstamp, AVG(A.price) AS avgPrice ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A+ B) DEFINE A AS AVG(A.price) \u0026lt; 15 ) MR; 给定此查询和以下输入值：
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 12 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:01\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:02\u0026#39; 13 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:03\u0026#39; 16 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:04\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:05\u0026#39; 2 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:06\u0026#39; 4 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:07\u0026#39; 10 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:08\u0026#39; 15 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:09\u0026#39; 25 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:10\u0026#39; 25 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:11\u0026#39; 30 1 只要事件的平均价格不超过 15，查询就会将事件作为模式变量 A 的一部分进行累积。 例如，这种限制发生在 01-Apr-11 10：00：04。接下来的时间段在 01-Apr-11 10:00:11 再次超过平均价格 15。因此，所述查询的结果将是：
symbol start_tstamp end_tstamp avgPrice ========= ================== ================== ============ ACME 01-APR-11 10:00:00 01-APR-11 10:00:03 14.5 ACME 01-APR-11 10:00:05 01-APR-11 10:00:10 13.5 注意 Aggregation 可以应用于表达式，但前提是它们引用单个模式变量。因此，SUM(A.price * A.tax) 是有效的，而 AVG(A.price * B.tax) 则是无效的。
注意 不支持 DISTINCT aggregation。
定义模式 # MATCH_RECOGNIZE 子句允许用户在事件流中使用功能强大、表达力强的语法搜索模式，这种语法与广泛使用的正则表达式语法有些相似。
每个模式都是由基本的构建块构造的，称为 模式变量，可以应用算子（量词和其他修饰符）到这些模块中。整个模式必须用括号括起来。
示例模式如下所示：
PATTERN (A B+ C* D) 可以使用以下算子：
Concatenation - 像 (A B) 这样的模式意味着 A 和 B 之间的连接是严格的。因此，在它们之间不能存在没有映射到 A 或 B 的行。 Quantifiers - 修改可以映射到模式变量的行数。 * — 0 或者多行 + — 1 或者多行 ? — 0 或者 1 行 { n } — 严格 n 行（n \u0026gt; 0） { n, } — n 或者更多行（n ≥ 0） { n, m } — 在 n 到 m（包含）行之间（0 ≤ n ≤ m，0 \u0026lt; m） { , m } — 在 0 到 m（包含）行之间（m \u0026gt; 0） 注意 不支持可能产生空匹配的模式。此类模式的示例如 PATTERN (A*)，PATTERN (A? B*)，PATTERN (A{0,} B{0,} C*) 等。
贪婪量词和勉强量词 # 每一个量词可以是 贪婪（默认行为）的或者 勉强 的。贪婪的量词尝试匹配尽可能多的行，而勉强的量词则尝试匹配尽可能少的行。
为了说明区别，可以通过查询查看以下示例，其中贪婪量词应用于 B 变量：
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES C.price AS lastPrice ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A B* C) DEFINE A AS A.price \u0026gt; 10, B AS B.price \u0026lt; 15, C AS C.price \u0026gt; 12 ) 假设我们有以下输入：
symbol tax price rowtime ======= ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 11 2018-09-17 10:00:03 XYZ 1 12 2018-09-17 10:00:04 XYZ 2 13 2018-09-17 10:00:05 XYZ 1 14 2018-09-17 10:00:06 XYZ 2 16 2018-09-17 10:00:07 上面的模式将产生以下输出：
symbol lastPrice ======== =========== XYZ 16 将 B* 修改为 B*? 的同一查询，这意味着 B* 应该是勉强的，将产生：
symbol lastPrice ======== =========== XYZ 13 XYZ 16 模式变量 B 只匹配价格为 12 的行，而不是包含价格为 12、13 和 14 的行。
注意 模式的最后一个变量不能使用贪婪量词。因此，不允许使用类似 (A B*) 的模式。通过引入条件为 B 的人工状态（例如 C），可以轻松解决此问题。因此，你可以使用类似以下的查询：
PATTERN (A B* C) DEFINE A AS condA(), B AS condB(), C AS NOT condB() 注意 目前不支持可选的勉强量词（A?? 或者 A{0,1}?）。
时间约束 # 特别是对于流的使用场景，通常需要在给定的时间内完成模式。这要求限制住 Flink 在内部必须保持的状态总体大小（即已经过期的状态就不需要再维护了），即使在贪婪的量词的情况下也是如此。
因此，Flink SQL 支持附加的（非标准 SQL）WITHIN 子句来定义模式的时间约束。子句可以在 PATTERN 子句之后定义，并以毫秒为间隔进行解析。
如果潜在匹配的第一个和最后一个事件之间的时间长于给定值，则不会将这种匹配追加到结果表中。
注意 通常鼓励使用 WITHIN 子句，因为它有助于 Flink 进行有效的内存管理。一旦达到阈值，即可修剪基础状态。
注意 然而，WITHIN 子句不是 SQL 标准的一部分。时间约束处理的方法已被提议将来可能会改变。
下面的示例查询说明了 WITHIN 子句的用法：
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES C.rowtime AS dropTime, A.price - C.price AS dropDiff ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A B* C) WITHIN INTERVAL \u0026#39;1\u0026#39; HOUR DEFINE B AS B.price \u0026gt; A.price - 10, C AS C.price \u0026lt; A.price - 10 ) 该查询检测到在 1 小时的间隔内价格下降了 10。
假设该查询用于分析以下股票数据：
symbol rowtime price tax ====== ==================== ======= ======= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:00:00\u0026#39; 20 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:20:00\u0026#39; 17 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 10:40:00\u0026#39; 18 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:00:00\u0026#39; 11 3 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:20:00\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 11:40:00\u0026#39; 9 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:00:00\u0026#39; 15 1 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:20:00\u0026#39; 14 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 12:40:00\u0026#39; 24 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:00:00\u0026#39; 1 2 \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:20:00\u0026#39; 19 1 查询将生成以下结果：
symbol dropTime dropDiff ====== ==================== ============= \u0026#39;ACME\u0026#39; \u0026#39;01-Apr-11 13:00:00\u0026#39; 14 结果行代表价格从 15（在01-Apr-11 12:00:00）下降到 1（在01-Apr-11 13:00:00）。dropDiff 列包含价格差异。
请注意，即使价格也下降了较高的值，例如，下降了 11（在 01-Apr-11 10:00:00 和 01-Apr-11 11:40:00 之间），这两个事件之间的时间差大于 1 小时。因此，它们不会产生匹配。
输出方式 # 输出方式 描述每个找到的匹配项应该输出多少行。SQL 标准描述了两种方式：
ALL ROWS PER MATCH ONE ROW PER MATCH 目前，唯一支持的输出方式是 ONE ROW PER MATCH，它将始终为每个找到的匹配项生成一个输出摘要行。
输出行的 schema 将是按特定顺序连接 [partitioning columns] + [measures columns]。
以下示例显示了所定义的查询的输出：
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES FIRST(A.price) AS startPrice, LAST(A.price) AS topPrice, B.price AS lastPrice ONE ROW PER MATCH PATTERN (A+ B) DEFINE A AS LAST(A.price, 1) IS NULL OR A.price \u0026gt; LAST(A.price, 1), B AS B.price \u0026lt; LAST(A.price) ) 对于以下输入行：
symbol tax price rowtime ======== ===== ======== ===================== XYZ 1 10 2018-09-17 10:00:02 XYZ 2 12 2018-09-17 10:00:03 XYZ 1 13 2018-09-17 10:00:04 XYZ 2 11 2018-09-17 10:00:05 该查询将生成以下输出：
symbol startPrice topPrice lastPrice ======== ============ ========== =========== XYZ 10 13 11 该模式识别由 symbol 列分区。即使在 MEASURES 子句中未明确提及，分区列仍会添加到结果的开头。
模式导航 # DEFINE 和 MEASURES 子句允许在（可能）匹配模式的行列表中进行导航。
本节讨论用于声明条件或产生输出结果的导航。
引用模式变量 # 引用模式变量 允许引用一组映射到 DEFINE 或 MEASURES 子句中特定模式变量的行。
例如，如果我们尝试将当前行与 A 进行匹配，则表达式 A.price 描述了目前为止已映射到 A 的一组行加上当前行。如果 DEFINE/MEASURES 子句中的表达式需要一行（例如 a.price 或 a.price \u0026gt; 10），它将选择属于相应集合的最后一个值。
如果没有指定模式变量（例如 SUM(price)），则表达式引用默认模式变量 *，该变量引用模式中的所有变量。换句话说，它创建了一个列表，其中列出了迄今为止映射到任何变量的所有行以及当前行。
示例 # 对于更全面的示例，可以查看以下模式和相应的条件：
PATTERN (A B+) DEFINE A AS A.price \u0026gt;= 10, B AS B.price \u0026gt; A.price AND SUM(price) \u0026lt; 100 AND SUM(B.price) \u0026lt; 80 下表描述了如何为每个传入事件计算这些条件。
该表由以下列组成：
# - 行标识符，用于唯一标识列表中的传入行 [A.price]/[B.price]/[price]。 price - 传入行的价格。 [A.price]/[B.price]/[price] - 描述 DEFINE 子句中用于计算条件的行列表。 Classifier - 当前行的分类器，指示该行映射到的模式变量。 A.price/B.price/SUM(price)/SUM(B.price) - 描述了这些表达式求值后的结果。 # price Classifier [A.price] [B.price] [price] A.price B.price SUM(price) SUM(B.price) #1 10 -\u0026gt; A #1 - - 10 - - - #2 15 -\u0026gt; B #1 #2 #1, #2 10 15 25 15 #3 20 -\u0026gt; B #1 #2, #3 #1, #2, #3 10 20 45 35 #4 31 -\u0026gt; B #1 #2, #3, #4 #1, #2, #3, #4 10 31 76 66 #5 35 #1 #2, #3, #4, #5 #1, #2, #3, #4, #5 10 35 111 101 从表中可以看出，第一行映射到模式变量 A，随后的行映射到模式变量 B。但是，最后一行不满足 B 条件，因为所有映射行 SUM(price) 的总和与 B 中所有行的总和都超过了指定的阈值。
Logical Offsets # Logical offsets 在映射到指定模式变量的事件启用导航。这可以用两个相应的函数表示：
Offset functions 描述 LAST(variable.field, n) 返回映射到变量最后 n 个元素的事件中的字段值。计数从映射的最后一个元素开始。
FIRST(variable.field, n) 返回映射到变量的第 n 个元素的事件中的字段值。计数从映射的第一个元素开始。
示例 # 对于更全面的示例，可以参考以下模式和相应的条件：
PATTERN (A B+) DEFINE A AS A.price \u0026gt;= 10, B AS (LAST(B.price, 1) IS NULL OR B.price \u0026gt; LAST(B.price, 1)) AND (LAST(B.price, 2) IS NULL OR B.price \u0026gt; 2 * LAST(B.price, 2)) 下表描述了如何为每个传入事件计算这些条件。
该表包括以下列：
price - 传入行的价格。 Classifier - 当前行的分类器，指示该行映射到的模式变量。 LAST(B.price, 1)/LAST(B.price, 2) - 描述对这些表达式求值后的结果。 price Classifier LAST(B.price, 1) LAST(B.price, 2) Comment 10 -\u0026gt; A 15 -\u0026gt; B null null 注意 LAST(B.price, 1) 为空，因为仍然没有映射到 B。 20 -\u0026gt; B 15 null 31 -\u0026gt; B 20 15 35 31 20 因为 35 \u0026lt; 2 * 20 没有映射。 将默认模式变量与 logical offsets 一起使用也可能很有意义。
在这种情况下，offset 会包含到目前为止映射的所有行：
PATTERN (A B? C) DEFINE B AS B.price \u0026lt; 20, C AS LAST(price, 1) \u0026lt; C.price price Classifier LAST(price, 1) Comment 10 -\u0026gt; A 15 -\u0026gt; B 20 -\u0026gt; C 15 LAST(price, 1) 被计算为映射到 B 变量的行的价格。 如果第二行没有映射到 B 变量，则会得到以下结果：
price Classifier LAST(price, 1) Comment 10 -\u0026gt; A 20 -\u0026gt; C 10 LAST(price, 1) 被计算为映射到 A 变量的行的价格。 也可以在 FIRST/LAST 函数的第一个参数中使用多个模式变量引用。这样，可以编写访问多个列的表达式。但是，它们都必须使用相同的模式变量。换句话说，必须在一行中计算 LAST/FIRST 函数的值。
因此，可以使用 LAST(A.price * A.tax)，但不允许使用类似 LAST(A.price * B.tax) 的表达式。
匹配后的策略 # AFTER MATCH SKIP 子句指定在找到完全匹配后从何处开始新的匹配过程。
有四种不同的策略：
SKIP PAST LAST ROW - 在当前匹配的最后一行之后的下一行继续模式匹配。 SKIP TO NEXT ROW - 继续从匹配项开始行后的下一行开始搜索新匹配项。 SKIP TO LAST variable - 恢复映射到指定模式变量的最后一行的模式匹配。 SKIP TO FIRST variable - 在映射到指定模式变量的第一行继续模式匹配。 这也是一种指定单个事件可以属于多少个匹配项的方法。例如，使用 SKIP PAST LAST ROW 策略，每个事件最多只能属于一个匹配项。
示例 # 为了更好地理解这些策略之间的差异，我们可以看看下面的例子。
对于以下输入行：
symbol tax price rowtime ======== ===== ======= ===================== XYZ 1 7 2018-09-17 10:00:01 XYZ 2 9 2018-09-17 10:00:02 XYZ 1 10 2018-09-17 10:00:03 XYZ 2 5 2018-09-17 10:00:04 XYZ 2 10 2018-09-17 10:00:05 XYZ 2 7 2018-09-17 10:00:06 XYZ 2 14 2018-09-17 10:00:07 我们使用不同的策略评估以下查询：
SELECT * FROM Ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY rowtime MEASURES SUM(A.price) AS sumPrice, FIRST(rowtime) AS startTime, LAST(rowtime) AS endTime ONE ROW PER MATCH [AFTER MATCH STRATEGY] PATTERN (A+ C) DEFINE A AS SUM(A.price) \u0026lt; 30 ) 该查询返回映射到 A 的总体匹配的第一个和最后一个时间戳所有行的价格之和。
查询将根据使用的 AFTER MATCH 策略产生不同的结果：
AFTER MATCH SKIP PAST LAST ROW # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 第一个结果与 #1，#2，#3，#4 行匹配。
第二个结果与 #5，#6, #7 行匹配。
AFTER MATCH SKIP TO NEXT ROW # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 24 2018-09-17 10:00:02 2018-09-17 10:00:05 XYZ 25 2018-09-17 10:00:03 2018-09-17 10:00:06 XYZ 22 2018-09-17 10:00:04 2018-09-17 10:00:07 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 同样，第一个结果与 #1，#2，#3，#4 行匹配。
与上一个策略相比，下一个匹配再次包含 #2 行匹配。因此，第二个结果与 #2，#3，#4，#5 行匹配。
第三个结果与 #3，#4，#5, #6 行匹配。
第四个结果与 #4，#5，#6, #7 行匹配。
最后一个结果与 #5，#6, #7 行匹配。
AFTER MATCH SKIP TO LAST A # symbol sumPrice startTime endTime ======== ========== ===================== ===================== XYZ 26 2018-09-17 10:00:01 2018-09-17 10:00:04 XYZ 25 2018-09-17 10:00:03 2018-09-17 10:00:06 XYZ 17 2018-09-17 10:00:05 2018-09-17 10:00:07 同样，第一个结果与 #1，#2，#3，#4 行匹配。
与前一个策略相比，下一个匹配只包含 #3 行（对应 A）用于下一个匹配。因此，第二个结果与 #3，#4，#5, #6 行匹配。
最后一个结果与 #5，#6, #7 行匹配。
AFTER MATCH SKIP TO FIRST A # 这种组合将产生一个运行时异常，因为人们总是试图在上一个开始的地方开始一个新的匹配。这将产生一个无限循环，因此是禁止的。
必须记住，在 SKIP TO FIRST/LAST variable 策略的场景下，可能没有映射到该变量的行（例如，对于模式 A*）。在这种情况下，将抛出一个运行时异常，因为标准要求一个有效的行来继续匹配。
时间属性 # 为了在 MATCH_RECOGNIZE 之上应用一些后续查询，可能需要使用时间属性。有两个函数可供选择：
Function Description MATCH_ROWTIME([rowtime_field]) 返回映射到给定模式的最后一行的时间戳。
函数可以没有入参，这种情况下函数返回结果是 TIMESTAMP 类型且具有事件时间属性；也可以有一个入参，这个参数值必须是 TIMESTAMP 类型或者 TIMESTAMP_LTZ 类型，且必须有事件时间属性，这种情况下函数返回结果的数据类型和输入参数的一致，且必须有事件时间属性。
结果属性是事件时间属性，可用于后续基于时间的操作，例如 interval joins 和 group window or over window aggregations。
MATCH_PROCTIME() 返回处理时间属性，该属性可用于随后的基于时间的操作，例如 interval joins 和 group window or over window aggregations。
控制内存消耗 # 在编写 MATCH_RECOGNIZE 查询时，内存消耗是一个重要的考虑因素，因为潜在匹配的空间是以宽度优先的方式构建的。鉴于此，我们必须确保模式能够完成。最好使用映射到匹配项的合理数量的行，因为它们必须内存相适。
例如，该模式不能有没有接受每一行上限的量词。这种模式可以是这样的：
PATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 查询将每个传入行映射到 B 变量，因此永远不会完成。可以纠正此查询，例如，通过否定 C 的条件：
PATTERN (A B+ C) DEFINE A as A.price \u0026gt; 10, B as B.price \u0026lt;= 20, C as C.price \u0026gt; 20 或者使用 reluctant quantifier：
PATTERN (A B+? C) DEFINE A as A.price \u0026gt; 10, C as C.price \u0026gt; 20 注意 请注意，MATCH_RECOGNIZE 子句未使用配置的 state retention time。为此，可能需要使用 WITHIN 子句。
已知的局限 # Flink 对 MATCH_RECOGNIZE 子句实现是一项长期持续的工作，目前尚不支持 SQL 标准的某些功能。
不支持的功能包括：
模式表达式： Pattern groups - 这意味着量词不能应用于模式的子序列。因此，(A (B C)+) 不是有效的模式。 Alterations - 像 PATTERN((A B | C D) E)这样的模式，这意味着在寻找 E 行之前必须先找到子序列 A B 或者 C D。 PERMUTE operator - 这等同于它应用于所示的所有变量的排列 PATTERN (PERMUTE (A, B, C)) = PATTERN (A B C | A C B | B A C | B C A | C A B | C B A)。 Anchors - ^, \$，表示分区的开始/结束，在流上下文中没有意义，将不被支持。 Exclusion - PATTERN ({- A -} B) 表示将查找 A，但是不会参与输出。这只适用于 ALL ROWS PER MATCH 方式。 Reluctant optional quantifier - PATTERN A?? 只支持贪婪的可选量词。 ALL ROWS PER MATCH 输出方式 - 为参与创建匹配项的每一行产生一个输出行。这也意味着： MEASURES 子句唯一支持的语义是 FINAL CLASSIFIER 函数，尚不支持返回行映射到的模式变量。 SUBSET - 它允许创建模式变量的逻辑组，并在 DEFINE 和 MEASURES 子句中使用这些组。 Physical offsets - PREV/NEXT，它为所有可见事件建立索引，而不是仅将那些映射到模式变量的事件编入索引（如 logical offsets 的情况）。 提取时间属性 - 目前无法为后续基于时间的操作提取时间属性。 MATCH_RECOGNIZE 仅 SQL 支持。Table API 中没有等效项。 Aggregations: 不支持 distinct aggregations。 Back to top
`}),e.add({id:256,href:"/flink/flink-docs-master/zh/docs/dev/dataset/examples/",title:"Batch 示例",section:"DataSet API (Legacy)",content:` Batch 示例 # 以下示例展示了 Flink 从简单的WordCount到图算法的应用。示例代码展示了 Flink\u0026rsquo;s DataSet API 的使用。
完整的源代码可以在 Flink 源代码库的 flink-examples-batch 模块找到。
运行一个示例 # 在开始运行一个示例前，我们假设你已经有了 Flink 的运行示例。导航栏中的“快速开始（Quickstart）”和“安装（Setup）” 标签页提供了启动 Flink 的不同方法。
最简单的方法就是执行 ./bin/start-cluster.sh，从而启动一个只有一个 JobManager 和 TaskManager 的本地 Flink 集群。
每个 Flink 的 binary release 都会包含一个examples（示例）目录，其中可以找到这个页面上每个示例的 jar 包文件。
可以通过执行以下命令来运行WordCount 示例:
./bin/flink run ./examples/batch/WordCount.jar 其他的示例也可以通过类似的方式执行。
注意很多示例在不传递执行参数的情况下都会使用内置数据。如果需要利用 WordCount 程序计算真实数据，你需要传递存储数据的文件路径。
./bin/flink run ./examples/batch/WordCount.jar --input /path/to/some/text/data --output /path/to/result 注意非本地文件系统需要一个对应前缀，例如 hdfs://。
Word Count # WordCount 是大数据系统中的 “Hello World”。他可以计算一个文本集合中不同单词的出现频次。这个算法分两步进行： 第一步，把所有文本切割成单独的单词。第二步，把单词分组并分别统计。
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet\u0026lt;String\u0026gt; text = env.readTextFile(\u0026#34;/path/to/file\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = // 把每一行文本切割成二元组，每个二元组为: (word,1) text.flatMap(new Tokenizer()) // 根据二元组的第“0”位分组，然后对第“1”位求和 .groupBy(0) .sum(1); counts.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // 自定义函数 public static class Tokenizer implements FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { // 统一大小写并把每一行切割为单词 String[] tokens = value.toLowerCase().split(\u0026#34;\\\\W+\u0026#34;); // 消费二元组 for (String token : tokens) { if (token.length() \u0026gt; 0) { out.collect(new Tuple2\u0026lt;String, Integer\u0026gt;(token, 1)); } } } } WordCount 示例 增加如下执行参数: \`--input --output \`即可实现上述算法。 任何文本文件都可作为测试数据使用。 Scala val env = ExecutionEnvironment.getExecutionEnvironment // 获取输入数据 val text = env.readTextFile(\u0026#34;/path/to/file\u0026#34;) val counts = text.flatMap { _.toLowerCase.split(\u0026#34;\\\\W+\u0026#34;) filter { _.nonEmpty } } .map { (_, 1) } .groupBy(0) .sum(1) counts.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) WordCount 示例 增加如下执行参数: \`--input --output \`即可实现上述算法。 任何文本文件都可作为测试数据使用。 Page Rank # PageRank算法可以计算互联网中一个网页的重要性，这个重要性通过由一个页面指向其他页面的链接定义。PageRank 算法是一个重复执行相同运算的迭代图算法。在每一次迭代中，每个页面把他当前的 rank 值分发给他所有的邻居节点，并且通过他收到邻居节点的 rank 值更新自身的 rank 值。PageRank 算法因 Google 搜索引擎的使用而流行，它根据网页的重要性来对搜索结果进行排名。
在这个简单的示例中，PageRank 算法由一个批量迭代和一些固定次数的迭代实现。
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 通过解析一个CSV文件来获取每个页面原始的rank值 DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; pagesWithRanks = env.readCsvFile(pagesInputPath) .types(Long.class, Double.class); // 链接被编码为邻接表: (page-id, Array(neighbor-ids)) DataSet\u0026lt;Tuple2\u0026lt;Long, Long[]\u0026gt;\u0026gt; pageLinkLists = getLinksDataSet(env); // 设置迭代数据集合 IterativeDataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; iteration = pagesWithRanks.iterate(maxIterations); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; newRanks = iteration // 为每个页面匹配其对应的出边，并发送rank值 .join(pageLinkLists).where(0).equalTo(0).flatMap(new JoinVertexWithEdgesMatch()) // 收集并计算新的rank值 .groupBy(0).sum(1) // 施加阻尼系数 .map(new Dampener(DAMPENING_FACTOR, numPages)); DataSet\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; finalPageRanks = iteration.closeWith( newRanks, newRanks.join(iteration).where(0).equalTo(0) // 结束条件 .filter(new EpsilonFilter())); finalPageRanks.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // 自定义函数 public static final class JoinVertexWithEdgesMatch implements FlatJoinFunction\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Long[]\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; { @Override public void join(\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt; page, Tuple2\u0026lt;Long, Long[]\u0026gt; adj, Collector\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; out) { Long[] neighbors = adj.f1; double rank = page.f1; double rankToDistribute = rank / ((double) neigbors.length); for (int i = 0; i \u0026lt; neighbors.length; i++) { out.collect(new Tuple2\u0026lt;Long, Double\u0026gt;(neighbors[i], rankToDistribute)); } } } public static final class Dampener implements MapFunction\u0026lt;Tuple2\u0026lt;Long,Double\u0026gt;, Tuple2\u0026lt;Long,Double\u0026gt;\u0026gt; { private final double dampening, randomJump; public Dampener(double dampening, double numVertices) { this.dampening = dampening; this.randomJump = (1 - dampening) / numVertices; } @Override public Tuple2\u0026lt;Long, Double\u0026gt; map(Tuple2\u0026lt;Long, Double\u0026gt; value) { value.f1 = (value.f1 * dampening) + randomJump; return value; } } public static final class EpsilonFilter implements FilterFunction\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt;\u0026gt; { @Override public boolean filter(Tuple2\u0026lt;Tuple2\u0026lt;Long, Double\u0026gt;, Tuple2\u0026lt;Long, Double\u0026gt;\u0026gt; value) { return Math.abs(value.f0.f1 - value.f1.f1) \u0026gt; EPSILON; } } PageRank代码 实现了以上示例。 他需要以下参数来运行: \`--pages --links --output --numPages --iterations \`。 Scala // 自定义类型 case class Link(sourceId: Long, targetId: Long) case class Page(pageId: Long, rank: Double) case class AdjacencyList(sourceId: Long, targetIds: Array[Long]) // 初始化执行环境 val env = ExecutionEnvironment.getExecutionEnvironment // 通过解析一个CSV文件来获取每个页面原始的rank值 val pages = env.readCsvFile[Page](pagesInputPath) // 链接被编码为邻接表: (page-id, Array(neighbor-ids)) val links = env.readCsvFile[Link](linksInputPath) // 将原始rank值赋给每个页面 val pagesWithRanks = pages.map(p =\u0026gt; Page(p, 1.0 / numPages)) // 通过输入链接建立邻接表 val adjacencyLists = links // initialize lists .map(e =\u0026gt; AdjacencyList(e.sourceId, Array(e.targetId))) // concatenate lists .groupBy(\u0026#34;sourceId\u0026#34;).reduce { (l1, l2) =\u0026gt; AdjacencyList(l1.sourceId, l1.targetIds ++ l2.targetIds) } // 开始迭代 val finalRanks = pagesWithRanks.iterateWithTermination(maxIterations) { currentRanks =\u0026gt; val newRanks = currentRanks // 发送rank值给目标页面 .join(adjacencyLists).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;sourceId\u0026#34;) { (page, adjacent, out: Collector[Page]) =\u0026gt; for (targetId \u0026lt;- adjacent.targetIds) { out.collect(Page(targetId, page.rank / adjacent.targetIds.length)) } } // 收集rank值并求和更新 .groupBy(\u0026#34;pageId\u0026#34;).aggregate(SUM, \u0026#34;rank\u0026#34;) // 施加阻尼系数 .map { p =\u0026gt; Page(p.pageId, (p.rank * DAMPENING_FACTOR) + ((1 - DAMPENING_FACTOR) / numPages)) } // 如果没有明显的rank更新则停止迭代 val termination = currentRanks.join(newRanks).where(\u0026#34;pageId\u0026#34;).equalTo(\u0026#34;pageId\u0026#34;) { (current, next, out: Collector[Int]) =\u0026gt; // check for significant update if (math.abs(current.rank - next.rank) \u0026gt; EPSILON) out.collect(1) } (newRanks, termination) } val result = finalRanks // 输出结果 result.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) PageRank代码 实现了以上示例。 他需要以下参数来执行： \`--pages --links --output --numPages --iterations \`。 输入文件是纯文本文件，并且必须存为以下格式：
页面被表示为一个长整型（long）ID并由换行符分割 例如 \u0026quot;1\\n2\\n12\\n42\\n63\\n\u0026quot; 给出了ID为 1, 2, 12, 42和63的五个页面。 链接由空格分割的两个页面ID来表示。每个链接由换行符来分割。 例如 \u0026quot;1 2\\n2 12\\n1 12\\n42 63\\n\u0026quot; 表示了以下四个有向链接： (1)-\u0026gt;(2), (2)-\u0026gt;(12), (1)-\u0026gt;(12) 和 (42)-\u0026gt;(63). 这个简单的实现版本要求每个页面至少有一个入链接和一个出链接（一个页面可以指向自己）。
Connected Components（连通组件算法） # Connected Components 通过给相连的顶点相同的组件ID来标示出一个较大的图中的连通部分。类似PageRank，Connected Components 也是一个迭代算法。在每一次迭代中，每个顶点把他现在的组件ID传播给所有邻居顶点。当一个顶点接收到的组件ID小于他自身的组件ID时，这个顶点也更新其组件ID为这个新组件ID。
这个代码实现使用了增量迭代： 没有改变其组件 ID 的顶点不会参与下一轮迭代。这种方法会带来更好的性能，因为后面的迭代可以只处理少量的需要计算的顶点。
Java // 读取顶点和边的数据 DataSet\u0026lt;Long\u0026gt; vertices = getVertexDataSet(env); DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; edges = getEdgeDataSet(env).flatMap(new UndirectEdge()); // 分配初始的组件ID（等于每个顶点的ID） DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; verticesWithInitialId = vertices.map(new DuplicateValue\u0026lt;Long\u0026gt;()); // 开始一个增量迭代 DeltaIteration\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; iteration = verticesWithInitialId.iterateDelta(verticesWithInitialId, maxIterations, 0); // 应用迭代计算逻辑: DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; changes = iteration.getWorkset() // 链接相应的边 .join(edges).where(0).equalTo(0).with(new NeighborWithComponentIDJoin()) // 选出最小的邻居组件ID .groupBy(0).aggregate(Aggregations.MIN, 1) // 如果邻居的组件ID更小则进行更新 .join(iteration.getSolutionSet()).where(0).equalTo(0) .flatMap(new ComponentIdFilter()); // 停止增量迭代 （增量和新的数据集是相同的） DataSet\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; result = iteration.closeWith(changes, changes); // 输出结果 result.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;); // 自定义函数 public static final class DuplicateValue\u0026lt;T\u0026gt; implements MapFunction\u0026lt;T, Tuple2\u0026lt;T, T\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;T, T\u0026gt; map(T vertex) { return new Tuple2\u0026lt;T, T\u0026gt;(vertex, vertex); } } public static final class UndirectEdge implements FlatMapFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { Tuple2\u0026lt;Long, Long\u0026gt; invertedEdge = new Tuple2\u0026lt;Long, Long\u0026gt;(); @Override public void flatMap(Tuple2\u0026lt;Long, Long\u0026gt; edge, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) { invertedEdge.f0 = edge.f1; invertedEdge.f1 = edge.f0; out.collect(edge); out.collect(invertedEdge); } } public static final class NeighborWithComponentIDJoin implements JoinFunction\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Long, Long\u0026gt; join(Tuple2\u0026lt;Long, Long\u0026gt; vertexWithComponent, Tuple2\u0026lt;Long, Long\u0026gt; edge) { return new Tuple2\u0026lt;Long, Long\u0026gt;(edge.f1, vertexWithComponent.f1); } } public static final class ComponentIdFilter implements FlatMapFunction\u0026lt;Tuple2\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; { @Override public void flatMap(Tuple2\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;, Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; value, Collector\u0026lt;Tuple2\u0026lt;Long, Long\u0026gt;\u0026gt; out) { if (value.f0.f1 \u0026lt; value.f1.f1) { out.collect(value.f0); } } } 实现了以上示例。他需要以下参数来运行: \`--vertices --edges --output --iterations \`。 Scala // 初始化运行环境 val env = ExecutionEnvironment.getExecutionEnvironment // 读顶点和边的数据 // 分配初始的组件ID（等于每个顶点的ID） val vertices = getVerticesDataSet(env).map { id =\u0026gt; (id, id) } // 通过发出每条输入边自身和他的反向边得到无向边 val edges = getEdgesDataSet(env).flatMap { edge =\u0026gt; Seq(edge, (edge._2, edge._1)) } // 开始增量迭代 val verticesWithComponents = vertices.iterateDelta(vertices, maxIterations, Array(0)) { (s, ws) =\u0026gt; // 开始迭代逻辑： 链接相应的边 val allNeighbors = ws.join(edges).where(0).equalTo(0) { (vertex, edge) =\u0026gt; (edge._2, vertex._2) } // 选择组件ID最小的邻居节点 val minNeighbors = allNeighbors.groupBy(0).min(1) // 如果邻居的ID更小则更新 val updatedComponents = minNeighbors.join(s).where(0).equalTo(0) { (newVertex, oldVertex, out: Collector[(Long, Long)]) =\u0026gt; if (newVertex._2 \u0026lt; oldVertex._2) out.collect(newVertex) } // 增量和新的数据集是一致的 (updatedComponents, updatedComponents) } verticesWithComponents.writeAsCsv(outputPath, \u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) ConnectedComponents代码 实现了以上示例。他需要以下参数来运行: \`--vertices --edges --output --iterations \`。 输入文件是纯文本文件并且必须被存储为如下格式：
顶点被表示为 ID，并且由换行符分隔。 例如 \u0026quot;1\\n2\\n12\\n42\\n63\\n\u0026quot; 表示 (1), (2), (12), (42) 和 (63)五个顶点。 边被表示为空格分隔的顶点对。边由换行符分隔: 例如 \u0026quot;1 2\\n2 12\\n1 12\\n42 63\\n\u0026quot; 表示四条无向边： (1)-(2), (2)-(12), (1)-(12), and (42)-(63)。 Back to top
`}),e.add({id:257,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/operators/",title:"Operators",section:"DataStream API",content:" "}),e.add({id:258,href:"/flink/flink-docs-master/zh/docs/dev/python/table_api_tutorial/",title:"Table API 教程",section:"Python API",content:` Table API 教程 # Apache Flink 提供 Table API 关系型 API 来统一处理流和批，即查询在无边界的实时流或有边界的批处理数据集上以相同的语义执行，并产生相同的结果。 Flink 的 Table API 易于编写，通常能简化数据分析，数据管道和ETL应用的编码。
概要 # 在该教程中，我们会从零开始，介绍如何创建一个 Flink Python 项目及运行 Python Table API 作业。该作业读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
先决条件 # 本练习假定你对 Python 有一定的了解，但是即使你来自其他编程语言，也应该能够继续学习。 它还假定你熟悉基本的关系操作，例如 SELECT 和 GROUP BY 子句。
如何寻求帮助 # 如果你遇到问题，可以访问 社区信息页面。 与此同时，Apache Flink 的用户邮件列表 一直被列为 Apache 项目中最活跃的项目邮件列表之一，也是快速获得帮助的好方法。
继续我们的旅程 # 如果要继续我们的旅程，你需要一台具有以下功能的计算机：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python Table API 需要安装 PyFlink，它已经被发布到 PyPi，你可以通过如下方式安装 PyFlink：
\$ python -m pip install apache-flink 安装 PyFlink 后，你便可以编写 Python Table API 作业了。
编写一个 Flink Python Table API 程序 # 编写 Flink Python Table API 程序的第一步是创建 TableEnvironment。这是 Python Table API 作业的入口类。
t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode()) t_env.get_config().set(\u0026#34;parallelism.default\u0026#34;, \u0026#34;1\u0026#34;) 接下来，我们将介绍如何创建源表和结果表。
t_env.create_temporary_table( \u0026#39;source\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .build()) .option(\u0026#39;path\u0026#39;, input_path) .format(\u0026#39;csv\u0026#39;) .build()) tab = t_env.from_path(\u0026#39;source\u0026#39;) t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .option(\u0026#39;path\u0026#39;, output_path) .format(FormatDescriptor.for_format(\u0026#39;canal-json\u0026#39;) .build()) .build()) 你也可以使用 TableEnvironment.execute_sql() 方法，通过 DDL 语句来注册源表和结果表:
my_source_ddl = \u0026#34;\u0026#34;\u0026#34; create table source ( word STRING ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;.format(input_path) my_sink_ddl = \u0026#34;\u0026#34;\u0026#34; create table sink ( word STRING, \`count\` BIGINT ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;canal-json\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;.format(output_path) t_env.execute_sql(my_source_ddl) t_env.execute_sql(my_sink_ddl) 上面的程序展示了如何创建及注册表名分别为 source 和 sink 的表。 其中，源表 source 有一列: word，该表代表了从 input_path 所指定的输入文件中读取的单词； 结果表 sink 有两列: word 和 count，该表的结果会输出到 output_path 所指定的输出文件中。
接下来，我们介绍如何创建一个作业：该作业读取表 source 中的数据，进行一些变换，然后将结果写入表 sink。
最后，需要做的就是启动 Flink Python Table API 作业。上面所有的操作，比如创建源表 进行变换以及写入结果表的操作都只是构建作业逻辑图，只有当 execute_insert(sink_name) 被调用的时候， 作业才会被真正提交到集群或者本地进行执行。
@udtf(result_types=[DataTypes.STRING()]) def split(line: Row): for s in line[0].split(): yield Row(s) # 计算 word count tab.flat_map(split).alias(\u0026#39;word\u0026#39;) \\ .group_by(col(\u0026#39;word\u0026#39;)) \\ .select(col(\u0026#39;word\u0026#39;), lit(1).count) \\ .execute_insert(\u0026#39;sink\u0026#39;) \\ .wait() 该教程的完整代码如下:
import argparse import logging import sys from pyflink.common import Row from pyflink.table import (EnvironmentSettings, TableEnvironment, TableDescriptor, Schema, DataTypes, FormatDescriptor) from pyflink.table.expressions import lit, col from pyflink.table.udf import udtf word_count_data = [\u0026#34;To be, or not to be,--that is the question:--\u0026#34;, \u0026#34;Whether \u0026#39;tis nobler in the mind to suffer\u0026#34;, \u0026#34;The slings and arrows of outrageous fortune\u0026#34;, \u0026#34;Or to take arms against a sea of troubles,\u0026#34;, \u0026#34;And by opposing end them?--To die,--to sleep,--\u0026#34;, \u0026#34;No more; and by a sleep to say we end\u0026#34;, \u0026#34;The heartache, and the thousand natural shocks\u0026#34;, \u0026#34;That flesh is heir to,--\u0026#39;tis a consummation\u0026#34;, \u0026#34;Devoutly to be wish\u0026#39;d. To die,--to sleep;--\u0026#34;, \u0026#34;To sleep! perchance to dream:--ay, there\u0026#39;s the rub;\u0026#34;, \u0026#34;For in that sleep of death what dreams may come,\u0026#34;, \u0026#34;When we have shuffled off this mortal coil,\u0026#34;, \u0026#34;Must give us pause: there\u0026#39;s the respect\u0026#34;, \u0026#34;That makes calamity of so long life;\u0026#34;, \u0026#34;For who would bear the whips and scorns of time,\u0026#34;, \u0026#34;The oppressor\u0026#39;s wrong, the proud man\u0026#39;s contumely,\u0026#34;, \u0026#34;The pangs of despis\u0026#39;d love, the law\u0026#39;s delay,\u0026#34;, \u0026#34;The insolence of office, and the spurns\u0026#34;, \u0026#34;That patient merit of the unworthy takes,\u0026#34;, \u0026#34;When he himself might his quietus make\u0026#34;, \u0026#34;With a bare bodkin? who would these fardels bear,\u0026#34;, \u0026#34;To grunt and sweat under a weary life,\u0026#34;, \u0026#34;But that the dread of something after death,--\u0026#34;, \u0026#34;The undiscover\u0026#39;d country, from whose bourn\u0026#34;, \u0026#34;No traveller returns,--puzzles the will,\u0026#34;, \u0026#34;And makes us rather bear those ills we have\u0026#34;, \u0026#34;Than fly to others that we know not of?\u0026#34;, \u0026#34;Thus conscience does make cowards of us all;\u0026#34;, \u0026#34;And thus the native hue of resolution\u0026#34;, \u0026#34;Is sicklied o\u0026#39;er with the pale cast of thought;\u0026#34;, \u0026#34;And enterprises of great pith and moment,\u0026#34;, \u0026#34;With this regard, their currents turn awry,\u0026#34;, \u0026#34;And lose the name of action.--Soft you now!\u0026#34;, \u0026#34;The fair Ophelia!--Nymph, in thy orisons\u0026#34;, \u0026#34;Be all my sins remember\u0026#39;d.\u0026#34;] def word_count(input_path, output_path): t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode()) # write all the data to one file t_env.get_config().set(\u0026#34;parallelism.default\u0026#34;, \u0026#34;1\u0026#34;) # define the source if input_path is not None: t_env.create_temporary_table( \u0026#39;source\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .build()) .option(\u0026#39;path\u0026#39;, input_path) .format(\u0026#39;csv\u0026#39;) .build()) tab = t_env.from_path(\u0026#39;source\u0026#39;) else: print(\u0026#34;Executing word_count example with default input data set.\u0026#34;) print(\u0026#34;Use --input to specify file input.\u0026#34;) tab = t_env.from_elements(map(lambda i: (i,), word_count_data), DataTypes.ROW([DataTypes.FIELD(\u0026#39;line\u0026#39;, DataTypes.STRING())])) # define the sink if output_path is not None: t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;filesystem\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .option(\u0026#39;path\u0026#39;, output_path) .format(FormatDescriptor.for_format(\u0026#39;canal-json\u0026#39;) .build()) .build()) else: print(\u0026#34;Printing result to stdout. Use --output to specify output path.\u0026#34;) t_env.create_temporary_table( \u0026#39;sink\u0026#39;, TableDescriptor.for_connector(\u0026#39;print\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;word\u0026#39;, DataTypes.STRING()) .column(\u0026#39;count\u0026#39;, DataTypes.BIGINT()) .build()) .build()) @udtf(result_types=[DataTypes.STRING()]) def split(line: Row): for s in line[0].split(): yield Row(s) # compute word count tab.flat_map(split).alias(\u0026#39;word\u0026#39;) \\ .group_by(col(\u0026#39;word\u0026#39;)) \\ .select(col(\u0026#39;word\u0026#39;), lit(1).count) \\ .execute_insert(\u0026#39;sink\u0026#39;) \\ .wait() # remove .wait if submitting to a remote cluster, refer to # https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/faq/#wait-for-jobs-to-finish-when-executing-jobs-in-mini-cluster # for more details if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\u0026#34;%(message)s\u0026#34;) parser = argparse.ArgumentParser() parser.add_argument( \u0026#39;--input\u0026#39;, dest=\u0026#39;input\u0026#39;, required=False, help=\u0026#39;Input file to process.\u0026#39;) parser.add_argument( \u0026#39;--output\u0026#39;, dest=\u0026#39;output\u0026#39;, required=False, help=\u0026#39;Output file to write results to.\u0026#39;) argv = sys.argv[1:] known_args, _ = parser.parse_known_args(argv) word_count(known_args.input, known_args.output) 执行一个 Flink Python Table API 程序 # 接下来，可以在命令行中运行作业（假设作业名为 word_count.py）：
\$ python word_count.py 上述命令会构建 Python Table API 程序，并在本地 mini cluster 中运行。如果想将作业提交到远端集群执行， 可以参考作业提交示例。
最后，你可以得到如下运行结果：
+I[To, 1] +I[be,, 1] +I[or, 1] +I[not, 1] ... 上述教程介绍了如何编写并运行一个 Flink Python Table API 程序，你也可以访问 PyFlink 示例 ，了解更多关于 PyFlink 的示例。 如果想了解 Flink Python Table API 的更多信息，可以参考 Flink Python API 文档 。
`}),e.add({id:259,href:"/flink/flink-docs-master/zh/docs/dev/datastream/execution/packaging/",title:"程序打包",section:"管理执行",content:` 程序打包和分布式运行 # 正如之前所描述的，Flink 程序可以使用 remote environment 在集群上执行。或者，程序可以被打包成 JAR 文件（Java Archives）执行。如果使用命令行的方式执行程序，将程序打包是必需的。
打包程序 # 为了能够通过命令行或 web 界面执行打包的 JAR 文件，程序必须使用通过 StreamExecutionEnvironment.getExecutionEnvironment() 获取的 environment。当 JAR 被提交到命令行或 web 界面后，该 environment 会扮演集群环境的角色。如果调用 Flink 程序的方式与上述接口不同，该 environment 会扮演本地环境的角色。
打包程序只要简单地将所有相关的类导出为 JAR 文件，JAR 文件的 manifest 必须指向包含程序入口点（拥有公共 main 方法）的类。实现的最简单方法是将 main-class 写入 manifest 中（比如 main-class: org.apache.flinkexample.MyProgram）。main-class 属性与 Java 虚拟机通过指令 java -jar pathToTheJarFile 执行 JAR 文件时寻找 main 方法的类是相同的。大多数 IDE 提供了在导出 JAR 文件时自动包含该属性的功能。
总结 # 调用打包后程序的完整流程包括两步：
搜索 JAR 文件 manifest 中的 main-class 或 program-class 属性。如果两个属性同时存在，program-class 属性会优先于 main-class 属性。对于 JAR manifest 中两个属性都不存在的情况，命令行和 web 界面支持手动传入入口点类名参数。
系统接着调用该类的 main 方法。
Back to top
`}),e.add({id:260,href:"/flink/flink-docs-master/zh/docs/flinkdev/building/",title:"从源码构建 Flink",section:"Flink 开发",content:` 从源码构建 Flink # 本篇主题是如何从版本 1.16-SNAPSHOT 的源码构建 Flink。
构建 Flink # 首先需要准备源码。可以从发布版本下载源码 或者从 Git 库克隆 Flink 源码。
还需要准备 Maven 3 和 JDK (Java开发套件)。Flink 依赖 Java 11 或更新的版本来进行构建。
*注意：Maven 3.3.x 可以构建 Flink，但是不能正确地屏蔽掉指定的依赖。Maven 3.2.5 可以正确地构建库文件。
输入以下命令从 Git 克隆代码
git clone https://github.com/apache/flink.git 最简单的构建 Flink 的方法是执行如下命令：
mvn clean install -DskipTests 上面的 Maven 指令（mvn）首先删除（clean）所有存在的构建，然后构建一个新的 Flink 运行包（install）。
为了加速构建，可以：
使用 \u0026rsquo; -DskipTests\u0026rsquo; 跳过测试 使用 fast Maven profile 跳过 QA 的插件和 JavaDocs 的生成 使用 skip-webui-build Maven profile 跳过 WebUI 编译 使用 Maven 并行构建功能，比如 \u0026lsquo;mvn package -T 1C\u0026rsquo; 会尝试并行使用多核 CPU，同时让每一个 CPU 核构建1个模块。 maven-shade-plugin 现存的 bug 可能会在并行构建时产生死锁。建议分2步进行构建：首先使用并行方式运行 mvn validate/test-compile/test，然后使用单线程方式运行 mvn package/verify/install。 构建脚本如下：
mvn clean install -DskipTests -Dfast -Pskip-webui-build -T 1C fast 和 skip-webui-build 这两个 Maven profiles 对整体构建时间影响比较大，特别是在存储设备比较慢的机器上，因为对应的任务会读写很多小文件。
构建 PyFlink # 先决条件 # 构建 Flink
如果想构建一个可用于 pip 安装的 PyFlink 包，需要先构建 Flink 工程，如 构建 Flink 中所述。
Python 的版本为 3.6, 3.7 或者 3.8.
\$ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 构建 PyFlink 的 Cython 扩展模块（可选的）
为了构建 PyFlink 的 Cython 扩展模块，需要 C 编译器。在不同操作系统上安装 C 编译器的方式略有不同：
Linux Linux 操作系统通常预装有 GCC。否则，需要手动安装。例如，可以在 Ubuntu 或 Debian 上使用命令sudo apt-get install build-essential安装。
Mac OS X 要在 Mac OS X 上安装 GCC，你需要下载并安装 Xcode 命令行工具，该工具可在 Apple 的开发人员页面中找到。
还需要使用以下命令安装依赖项：
\$ python -m pip install -r flink-python/dev/dev-requirements.txt 安装 # 进入 Flink 源码根目录，并执行以下命令，构建 apache-flink 和 apache-flink-libraries 的源码发布包和 wheel 包：
cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..; 构建好的 apache-flink-libraries 的源码发布包位于 ./flink-python/apache-flink-libraries/dist/ 目录下。可使用 pip 安装，比如:
python -m pip install apache-flink-libraries/dist/*.tar.gz 构建好的 apache-flink 的源码发布包和 wheel 包位于 ./flink-python/dist/ 目录下。它们均可使用 pip 安装，比如:
python -m pip install dist/*.whl 依赖屏蔽 # Flink 屏蔽了一些它使用的包，这样做是为了避免与程序员自己引入的包的存在的可能的版本冲突。屏蔽掉的包包括 Google Guava,Asm,Apache Curator,Apache HTTP Components,Netty 等。
这种依赖屏蔽机制最近在 Maven 中有所改变。需要用户根据 Maven 的的不同版本来执行不同的命令。
对于Maven 3.1.x and 3.2.x 直接在 Flink 源码根目录执行命令 mvn clean install -DskipTests 就足够了。
Maven 3.3.x 如下的构建需要两步走：第一步需要在基础目录下执行编译构建；第二步需要在编译后的 flink-dist 目录下执行：
mvn clean install -DskipTests cd flink-dist mvn clean install 注意: 运行 mvn --version 以查看Maven的版本。
Back to top
Scala 版本 # 只是用 Java 库和 API 的用户可以忽略这一部分。 Flink 有使用 Scala 来写的 API，库和运行时模块。使用 Scala API 和库的同学必须配置 Flink 的 Scala 版本和自己的 Flink 版本（因为 Scala 并不严格的向后兼容）。
从 1.7 版本开始，Flink 可以使用 Scala 2.11（默认）和 2.12 来构建。
如果使用 Scala 2.12 来进行构建，执行如下命令：
mvn clean install -DskipTests -Dscala-2.12 要针对特定的二进制 Scala 版本进行构建，可以使用
mvn clean install -DskipTests -Dscala-2.12 -Dscala.version=\u0026lt;scala version\u0026gt; Back to top
加密的文件系统 # 如果你的 home 目录是加密的，可能遇到如下异常 java.io.IOException: File name too long。一些像 Ubuntu 的 enfs 这样的加密文件系统因为不支持长文件名会产生这个异常。
解决方法是添加如下内容到 pom.xml 文件中出现这个错误的模块的编译器配置项下。
\u0026lt;args\u0026gt; \u0026lt;arg\u0026gt;-Xmax-classfile-name\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;128\u0026lt;/arg\u0026gt; \u0026lt;/args\u0026gt; 例如，如果错误出现在 flink-yarn 模块下，上述的代码需要添加到 scala-maven-plugin 的 \u0026lt;configuration\u0026gt; 项下。请查看这个问题的链接获取更多信息。
Back to top
`}),e.add({id:261,href:"/flink/flink-docs-master/zh/docs/dev/python/table/udfs/python_udfs/",title:"普通自定义函数",section:"自定义函数",content:` 普通自定义函数（UDF） # 用户自定义函数是重要的功能，因为它们极大地扩展了 Python Table API 程序的表达能力。
注意: 要执行 Python 用户自定义函数，客户端和集群端都需要安装 Python 3.6 以上版本(3.6、3.7 或 3.8)，并安装 PyFlink。
标量函数（ScalarFunction） # PyFlink 支持在 Python Table API 程序中使用 Python 标量函数。 如果要定义 Python 标量函数， 可以继承 pyflink.table.udf 中的基类 ScalarFunction，并实现 eval 方法。 Python 标量函数的行为由名为 eval 的方法定义，eval 方法支持可变长参数，例如 eval(* args)。
以下示例显示了如何定义自己的 Python 哈希函数、如何在 TableEnvironment 中注册它以及如何在作业中使用它。
from pyflink.table.expressions import call, col from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.udf import ScalarFunction, udf class HashCode(ScalarFunction): def __init__(self): self.factor = 12 def eval(self, s): return hash(s) * self.factor settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) hash_code = udf(HashCode(), result_type=DataTypes.BIGINT()) # 在 Python Table API 中使用 Python 自定义函数 my_table.select(col(\u0026#34;string\u0026#34;), col(\u0026#34;bigint\u0026#34;), hash_code(col(\u0026#34;bigint\u0026#34;)), call(hash_code, col(\u0026#34;bigint\u0026#34;))) # 在 SQL API 中使用 Python 自定义函数 table_env.create_temporary_function(\u0026#34;hash_code\u0026#34;, udf(HashCode(), result_type=DataTypes.BIGINT())) table_env.sql_query(\u0026#34;SELECT string, bigint, hash_code(bigint) FROM MyTable\u0026#34;) 除此之外，还支持在Python Table API程序中使用 Java / Scala 标量函数。
\u0026#39;\u0026#39;\u0026#39; Java code: // Java 类必须具有公共的无参数构造函数，并且可以在当前的Java类加载器中可以加载到。 public class HashCode extends ScalarFunction { private int factor = 12; public int eval(String s) { return s.hashCode() * factor; } } \u0026#39;\u0026#39;\u0026#39; from pyflink.table.expressions import call, col from pyflink.table import TableEnvironment, EnvironmentSettings settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) # 注册 Java 函数 table_env.create_java_temporary_function(\u0026#34;hash_code\u0026#34;, \u0026#34;my.java.function.HashCode\u0026#34;) # 在 Python Table API 中使用 Java 函数 my_table.select(call(\u0026#39;hash_code\u0026#39;, col(\u0026#34;string\u0026#34;))) # 在 SQL API 中使用 Java 函数 table_env.sql_query(\u0026#34;SELECT string, bigint, hash_code(string) FROM MyTable\u0026#34;) 除了扩展基类 ScalarFunction 之外，还支持多种方式来定义 Python 标量函数。 以下示例显示了多种定义 Python 标量函数的方式。该函数需要两个类型为 bigint 的参数作为输入参数，并返回它们的总和作为结果。
# 方式一：扩展基类 calarFunction class Add(ScalarFunction): def eval(self, i, j): return i + j add = udf(Add(), result_type=DataTypes.BIGINT()) # 方式二：普通 Python 函数 @udf(result_type=DataTypes.BIGINT()) def add(i, j): return i + j # 方式三：lambda 函数 add = udf(lambda i, j: i + j, result_type=DataTypes.BIGINT()) # 方式四：callable 函数 class CallableAdd(object): def __call__(self, i, j): return i + j add = udf(CallableAdd(), result_type=DataTypes.BIGINT()) # 方式五：partial 函数 def partial_add(i, j, k): return i + j + k add = udf(functools.partial(partial_add, k=1), result_type=DataTypes.BIGINT()) # 注册 Python 自定义函数 table_env.create_temporary_function(\u0026#34;add\u0026#34;, add) # 在 Python Table API 中使用 Python 自定义函数 my_table.select(call(\u0026#39;add\u0026#39;, col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;))) # 也可以在 Python Table API 中直接使用 Python 自定义函数 my_table.select(add(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;))) 表值函数（TableFunction） # 与 Python 用户自定义标量函数类似，Python 用户自定义表值函数以零个，一个或者多个列作为输入参数。但是，与标量函数不同的是，表值函数可以返回 任意数量的行作为输出而不是单个值。Python 用户自定义表值函数的返回类型可以是 Iterable，Iterator 或 generator 类型。
以下示例说明了如何定义自己的 Python 自定义表值函数，将其注册到 TableEnvironment 中，并在作业中使用它。
from pyflink.table.expressions import col from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.udf import TableFunction, udtf class Split(TableFunction): def eval(self, string): for s in string.split(\u0026#34; \u0026#34;): yield s, len(s) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) my_table = ... # type: Table, table schema: [a: String] # 注册 Python 表值函数 split = udtf(Split(), result_types=[DataTypes.STRING(), DataTypes.INT()]) # 在 Python Table API 中使用 Python 表值函数 my_table.join_lateral(split(col(\u0026#34;a\u0026#34;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)) my_table.left_outer_join_lateral(split(col(\u0026#34;a\u0026#34;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)) # 在 SQL API 中使用 Python 表值函数 table_env.create_temporary_function(\u0026#34;split\u0026#34;, udtf(Split(), result_types=[DataTypes.STRING(), DataTypes.INT()])) table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)\u0026#34;) table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE\u0026#34;) 除此之外，还支持在 Python Table API 程序中使用 Java / Scala 表值函数。
\u0026#39;\u0026#39;\u0026#39; Java code: // 类型\u0026#34;Tuple2 \u0026lt;String，Integer\u0026gt;\u0026#34;代表，表值函数的输出类型为（String，Integer）。 // Java类必须具有公共的无参数构造函数，并且可以在当前的Java类加载器中加载到。 public class Split extends TableFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { private String separator = \u0026#34; \u0026#34;; public void eval(String str) { for (String s : str.split(separator)) { // use collect(...) to emit a row collect(new Tuple2\u0026lt;String, Integer\u0026gt;(s, s.length())); } } } \u0026#39;\u0026#39;\u0026#39; from pyflink.table.expressions import call, col from pyflink.table import TableEnvironment, EnvironmentSettings env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) my_table = ... # type: Table, table schema: [a: String] # 注册 Java 自定义函数。 table_env.create_java_temporary_function(\u0026#34;split\u0026#34;, \u0026#34;my.java.function.Split\u0026#34;) # 在 Python Table API 中使用表值函数。 \u0026#34;alias\u0026#34;指定表的字段名称。 my_table.join_lateral(call(\u0026#39;split\u0026#39;, col(\u0026#39;a\u0026#39;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;word\u0026#39;), col(\u0026#39;length\u0026#39;)) my_table.left_outer_join_lateral(call(\u0026#39;split\u0026#39;, col(\u0026#39;a\u0026#39;)).alias(\u0026#34;word\u0026#34;, \u0026#34;length\u0026#34;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;word\u0026#39;), col(\u0026#39;length\u0026#39;)) # 注册 Python 函数。 # 在SQL中将table函数与LATERAL和TABLE关键字一起使用。 # CROSS JOIN表值函数（等效于Table API中的\u0026#34;join\u0026#34;）。 table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable, LATERAL TABLE(split(a)) as T(word, length)\u0026#34;) # LEFT JOIN一个表值函数（等同于Table API中的\u0026#34;left_outer_join\u0026#34;）。 table_env.sql_query(\u0026#34;SELECT a, word, length FROM MyTable LEFT JOIN LATERAL TABLE(split(a)) as T(word, length) ON TRUE\u0026#34;) 像 Python 标量函数一样，您可以使用上述五种方式来定义 Python 表值函数。
注意 唯一的区别是，Python 表值函数的返回类型必须是 iterable（可迭代子类）, iterator（迭代器） or generator（生成器）。
# 方式一：生成器函数 @udtf(result_types=DataTypes.BIGINT()) def generator_func(x): yield 1 yield 2 # 方式二：返回迭代器 @udtf(result_types=DataTypes.BIGINT()) def iterator_func(x): return range(5) # 方式三：返回可迭代子类 @udtf(result_types=DataTypes.BIGINT()) def iterable_func(x): result = [1, 2, 3] return result 聚合函数（AggregateFunction） # A user-defined aggregate function (UDAGG) maps scalar values of multiple rows to a new scalar value.
NOTE: Currently the general user-defined aggregate function is only supported in the GroupBy aggregation and Group Window Aggregation in streaming mode. For batch mode, it\u0026rsquo;s currently not supported and it is recommended to use the Vectorized Aggregate Functions.
The behavior of an aggregate function is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.
For each set of rows that need to be aggregated, the runtime will create an empty accumulator by calling create_accumulator(). Subsequently, the accumulate(...) method of the aggregate function will be called for each input row to update the accumulator. Currently after each row has been processed, the get_value(...) method of the aggregate function will be called to compute the aggregated result.
The following example illustrates the aggregation process:
In the above example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the highest price of all beverages in the table, i.e., perform a max() aggregation.
In order to define an aggregate function, one has to extend the base class AggregateFunction in pyflink.table and implement the evaluation method named accumulate(...). The result type and accumulator type of the aggregate function can be specified by one of the following two approaches:
Implement the method named get_result_type() and get_accumulator_type(). Wrap the function instance with the decorator udaf in pyflink.table.udf and specify the parameters result_type and accumulator_type. The following example shows how to define your own aggregate function and call it in a query.
from pyflink.common import Row from pyflink.table import AggregateFunction, DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import call from pyflink.table.udf import udaf from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble class WeightedAvg(AggregateFunction): def create_accumulator(self): # Row(sum, count) return Row(0, 0) def get_value(self, accumulator): if accumulator[1] == 0: return None else: return accumulator[0] / accumulator[1] def accumulate(self, accumulator, value, weight): accumulator[0] += value * weight accumulator[1] += weight def retract(self, accumulator, value, weight): accumulator[0] -= value * weight accumulator[1] -= weight def get_result_type(self): return DataTypes.BIGINT() def get_accumulator_type(self): return DataTypes.ROW([ DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udaf decorator: # weighted_avg = udaf(WeightedAvg(), result_type=DataTypes.BIGINT(), accumulator_type=...) weighted_avg = udaf(WeightedAvg()) t = table_env.from_elements([(1, 2, \u0026#34;Lee\u0026#34;), (3, 4, \u0026#34;Jay\u0026#34;), (5, 6, \u0026#34;Jay\u0026#34;), (7, 8, \u0026#34;Lee\u0026#34;)]).alias(\u0026#34;value\u0026#34;, \u0026#34;count\u0026#34;, \u0026#34;name\u0026#34;) # call function \u0026#34;inline\u0026#34; without registration in Table API result = t.group_by(col(\u0026#34;name\u0026#34;)).select(weighted_avg(col(\u0026#34;value\u0026#34;), col(\u0026#34;count\u0026#34;)).alias(\u0026#34;avg\u0026#34;)).execute() result.print() # register function table_env.create_temporary_function(\u0026#34;weighted_avg\u0026#34;, WeightedAvg()) # call registered function in Table API result = t.group_by(col(\u0026#34;name\u0026#34;)).select(call(\u0026#34;weighted_avg\u0026#34;, col(\u0026#34;value\u0026#34;), col(\u0026#34;count\u0026#34;)).alias(\u0026#34;avg\u0026#34;)).execute() result.print() # register table table_env.create_temporary_view(\u0026#34;source\u0026#34;, t) # call registered function in SQL result = table_env.sql_query( \u0026#34;SELECT weighted_avg(\`value\`, \`count\`) AS avg FROM source GROUP BY name\u0026#34;).execute() result.print() # use the general Python aggregate function in GroupBy Window Aggregation tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) result = t.window(tumble_window) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;name\u0026#39;)) \\ .select(col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, weighted_avg(col(\u0026#39;value\u0026#39;), col(\u0026#39;count\u0026#39;))) \\ .execute() result.print() The accumulate(...) method of our WeightedAvg class takes three input arguments. The first one is the accumulator and the other two are user-defined inputs. In order to calculate a weighted average value, the accumulator needs to store the weighted sum and count of all the data that have already been accumulated. In our example, we use a Row object as the accumulator. Accumulators will be managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of failover to ensure exactly-once semantics.
Mandatory and Optional Methods # The following methods are mandatory for each AggregateFunction:
create_accumulator() accumulate(...) get_value(...) The following methods of AggregateFunction are required depending on the use case:
retract(...) is required when there are operations that could generate retraction messages before the current aggregation operation, e.g. group aggregate, outer join. This method is optional, but it is strongly recommended to be implemented to ensure the UDAF can be used in any use case. merge(...) is required for session window ang hop window aggregations. get_result_type() and get_accumulator_type() is required if the result type and accumulator type would not be specified in the udaf decorator. ListView and MapView # If an accumulator needs to store large amounts of data, pyflink.table.ListView and pyflink.table.MapView could be used instead of list and dict. These two data structures provide the similar functionalities as list and dict, however usually having better performance by leveraging Flink\u0026rsquo;s state backend to eliminate unnecessary state access. You can use them by declaring DataTypes.LIST_VIEW(...) and DataTypes.MAP_VIEW(...) in the accumulator type, e.g.:
from pyflink.table import ListView class ListViewConcatAggregateFunction(AggregateFunction): def get_value(self, accumulator): # the ListView is iterable return accumulator[1].join(accumulator[0]) def create_accumulator(self): return Row(ListView(), \u0026#39;\u0026#39;) def accumulate(self, accumulator, *args): accumulator[1] = args[1] # the ListView support add, clear and iterate operations. accumulator[0].add(args[0]) def get_accumulator_type(self): return DataTypes.ROW([ # declare the first column of the accumulator as a string ListView. DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.LIST_VIEW(DataTypes.STRING())), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.STRING() Currently, there are 2 limitations to use the ListView and MapView:
The accumulator must be a Row. The ListView and MapView must be the first level children of the Row accumulator. Please refer to the documentation of the corresponding classes for more information about this advanced feature.
NOTE: For reducing the data transmission cost between Python UDF worker and Java process caused by accessing the data in Flink states(e.g. accumulators and data views), there is a cached layer between the raw state handler and the Python state backend. You can adjust the values of these configuration options to change the behavior of the cache layer for best performance: python.state.cache-size, python.map-state.read-cache-size, python.map-state.write-cache-size, python.map-state.iterate-response-batch-size. For more details please refer to the Python Configuration Documentation.
Table Aggregate Functions # A user-defined table aggregate function (UDTAGG) maps scalar values of multiple rows to zero, one, or multiple rows (or structured types). The returned record may consist of one or more fields. If an output record consists of only a single field, the structured record can be omitted, and a scalar value can be emitted that will be implicitly wrapped into a row by the runtime.
NOTE: Currently the general user-defined table aggregate function is only supported in the GroupBy aggregation in streaming mode.
Similar to an aggregate function, the behavior of a table aggregate is centered around the concept of an accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed.
For each set of rows that needs to be aggregated, the runtime will create an empty accumulator by calling create_accumulator(). Subsequently, the accumulate(...) method of the function is called for each input row to update the accumulator. Once all rows have been processed, the emit_value(...) method of the function is called to compute and return the final result.
The following example illustrates the aggregation process:
In the example, we assume a table that contains data about beverages. The table consists of three columns (id, name, and price) and 5 rows. We would like to find the 2 highest prices of all beverages in the table, i.e., perform a TOP2() table aggregation. We need to consider each of the 5 rows. The result is a table with the top 2 values.
In order to define a table aggregate function, one has to extend the base class TableAggregateFunction in pyflink.table and implement one or more evaluation methods named accumulate(...).
The result type and accumulator type of the aggregate function can be specified by one of the following two approaches:
Implement the method named get_result_type() and get_accumulator_type(). Wrap the function instance with the decorator udtaf in pyflink.table.udf and specify the parameters result_type and accumulator_type. The following example shows how to define your own aggregate function and call it in a query.
from pyflink.common import Row from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udtaf, TableAggregateFunction class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row): if row[0] is not None: if accumulator[0] is None or row[0] \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row[0] elif accumulator[1] is None or row[0] \u0026gt; accumulator[1]: accumulator[1] = row[0] def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udtaf decorator: # top2 = udtaf(Top2(), result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]), accumulator_type=DataTypes.ARRAY(DataTypes.BIGINT())) top2 = udtaf(Top2()) t = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # call function \u0026#34;inline\u0026#34; without registration in Table API t.group_by(col(\u0026#39;b\u0026#39;)).flat_aggregate(top2).select(col(\u0026#39;*\u0026#39;)).execute().print() # the result is: +----+--------------------------------+----------------------+ | op | b | a | +----+--------------------------------+----------------------+ | +I | Hi | 1 | | +I | Hi | \u0026lt;NULL\u0026gt; | | -D | Hi | 1 | | -D | Hi | \u0026lt;NULL\u0026gt; | | +I | Hi | 7 | | +I | Hi | 3 | | +I | Hi2 | 5 | | +I | Hi2 | \u0026lt;NULL\u0026gt; | +----+--------------------------------+----------------------+ The accumulate(...) method of our Top2 class takes two inputs. The first one is the accumulator and the second one is the user-defined input. In order to calculate a result, the accumulator needs to store the 2 highest values of all the data that has been accumulated. Accumulators are automatically managed by Flink\u0026rsquo;s checkpointing mechanism and are restored in case of a failure to ensure exactly-once semantics. The result values are emitted together with a ranking index.
Mandatory and Optional Methods # The following methods are mandatory for each TableAggregateFunction:
create_accumulator() accumulate(...) emit_value(...) The following methods of TableAggregateFunction are required depending on the use case:
retract(...) is required when there are operations that could generate retraction messages before the current aggregation operation, e.g. group aggregate, outer join. This method is optional, but it is strongly recommended to be implemented to ensure the UDTAF can be used in any use case. get_result_type() and get_accumulator_type() is required if the result type and accumulator type would not be specified in the udtaf decorator. ListView and MapView # Similar to Aggregation function, we can also use ListView and MapView in Table Aggregate Function.
from pyflink.common import Row from pyflink.table import ListView from pyflink.table.types import DataTypes from pyflink.table.udf import TableAggregateFunction class ListViewConcatTableAggregateFunction(TableAggregateFunction): def emit_value(self, accumulator): result = accumulator[1].join(accumulator[0]) yield Row(result) yield Row(result) def create_accumulator(self): return Row(ListView(), \u0026#39;\u0026#39;) def accumulate(self, accumulator, *args): accumulator[1] = args[1] accumulator[0].add(args[0]) def get_accumulator_type(self): return DataTypes.ROW([ DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.LIST_VIEW(DataTypes.STRING())), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.STRING())]) `}),e.add({id:262,href:"/flink/flink-docs-master/zh/docs/dev/table/types/",title:"数据类型",section:"Table API \u0026 SQL",content:` 数据类型 # Flink SQL 为用户提供了一系列丰富的原始数据类型。
数据类型 # 在 Flink 的 Table 生态系统中，数据类型 描述了数据的逻辑类型，可以用来表示转换过程中输入、输出的类型。
Flink 的数据类型类似于 SQL 标准中的术语数据类型，但包含了值的可空性，以便于更好地处理标量表达式。
以下是一些数据类型的例子：
INT INT NOT NULL INTERVAL DAY TO SECOND(3) ROW\u0026lt;myField ARRAY\u0026lt;BOOLEAN\u0026gt;, myOtherField TIMESTAMP(3)\u0026gt; 可在下文中找到所有预先定义好的数据类型。
Table API 中的数据类型 # Java/Scala 在定义 connector、catalog、用户自定义函数时，使用 JVM 相关 API 的用户可能会使用到 Table API 中基于 org.apache.flink.table.types.DataType 的一些实例。
数据类型 实例有两个职责：
作为逻辑类型的表现形式，定义 JVM 类语言或 Python 语言与 Table 生态系统的边界，而不是以具体的物理表现形式存在于数据的传输过程或存储中。 可选的: 在与其他 API 进行数据交换时，为 Planner 提供这些数据物理层面的相关提示。 对于基于 JVM 的语言，所有预定义的数据类型都可以在 org.apache.flink.table.api.DataTypes 下找到。
Python 在 Python 语言定义用户自定义函数时，使用 Python API 的用户 可能会使用到 Python API 中基于 pyflink.table.types.DataType 的一些实例。
数据类型 实例有如下职责：
作为逻辑类型的表现形式，定义 JVM 类语言或 Python 语言与 Table 生态系统的边界，而不是以具体的物理表现形式存在于数据的传输过程或存储中。 对于 Python 语言，这些类型可以在 pyflink.table.types.DataTypes 下找到。
Java 使用 Table API 编程时，建议使用星号引入所有相关依赖，以获得更流畅的 API 使用体验：
import static org.apache.flink.table.api.DataTypes.*; DataType t = INTERVAL(DAY(), SECOND(3)); Scala 使用 Table API 编程时，建议使用星号引入所有相关依赖，以获得更流畅的 API 使用体验：
import org.apache.flink.table.api.DataTypes._ val t: DataType = INTERVAL(DAY(), SECOND(3)) Python from pyflink.table.types import DataTypes t = DataTypes.INTERVAL(DataTypes.DAY(), DataTypes.SECOND(3)) 物理提示 # 在Table 生态系统中，当需要将 SQL 中的数据类型对应到实际编程语言中的数据类型时，就需要有物理提示。物理提示明确了对应过程中应该使用哪种数据格式。
比如，在 source 端产生数据时，可以规定：TIMESTAMP 的逻辑类型，在底层要使用 java.sql.Timestamp 这个类表示，而不是使用默认的 java.time.LocalDateTime 类。有了物理提示，可以帮助 Flink 运行时根据提供的类将数据转换为其内部数据格式。同样在 sink 端，定义好数据格式，以便能从 Flink 运行时获取、转换数据。
下面的例子展示了如何声明一个桥接转换类：
Java // 告诉 Flink 运行时使用 java.sql.Timestamp 处理数据，而不是 java.time.LocalDateTime DataType t = DataTypes.TIMESTAMP(3).bridgedTo(java.sql.Timestamp.class); // 告诉 Flink 运行时使用基本的 int 数组来处理数据，而不是用包装类 Integer 数组 DataType t = DataTypes.ARRAY(DataTypes.INT().notNull()).bridgedTo(int[].class); Scala // 告诉 Flink 运行时使用 java.sql.Timestamp 处理数据，而不是 java.time.LocalDateTime val t: DataType = DataTypes.TIMESTAMP(3).bridgedTo(classOf[java.sql.Timestamp]) // 告诉 Flink 运行时使用基本的 int 数组来处理数据，而不是用包装类 Integer 数组 val t: DataType = DataTypes.ARRAY(DataTypes.INT().notNull()).bridgedTo(classOf[Array[Int]]) 注意 请记住，只有在扩展 API 时才需要使用到物理提示。使用预定义的 source、sink 以及 Flink 函数时，不需要用到物理提示。在使用 Table API 编写程序时，Flink 会忽略物理提示（例如 field.cast(TIMESTAMP(3).bridgedTo(Timestamp.class))）。
List of Data Types # This section lists all pre-defined data types. Java/Scala For the JVM-based Table API those types are also available in org.apache.flink.table.api.DataTypes. Python For the Python Table API, those types are available in pyflink.table.types.DataTypes. The default planner supports the following set of SQL types:
Data Type Remarks for Data Type CHAR VARCHAR STRING BOOLEAN BINARY VARBINARY BYTES DECIMAL Supports fixed precision and scale. TINYINT SMALLINT INTEGER BIGINT FLOAT DOUBLE DATE TIME Supports only a precision of 0. TIMESTAMP TIMESTAMP_LTZ INTERVAL Supports only interval of MONTH and SECOND(3). ARRAY MULTISET MAP ROW RAW Structured types Only exposed in user-defined functions yet. Character Strings # CHAR # Data type of a fixed-length character string.
Declaration
SQL CHAR CHAR(n) Java/Scala DataTypes.CHAR(n) Bridging to JVM Types
Java Type Input Output Remarks java.lang.String X X Default byte[] X X Assumes UTF-8 encoding. org.apache.flink.table.data.StringData X X Internal data structure. Python Not supported. The type can be declared using CHAR(n) where n is the number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
VARCHAR / STRING # Data type of a variable-length character string.
Declaration
SQL VARCHAR VARCHAR(n) STRING Java/Scala DataTypes.VARCHAR(n) DataTypes.STRING() Bridging to JVM Types
Java Type Input Output Remarks java.lang.String X X Default byte[] X X Assumes UTF-8 encoding. org.apache.flink.table.data.StringData X X Internal data structure. Python DataTypes.VARCHAR(n) DataTypes.STRING() Attention The specified maximum number of code points n in DataTypes.VARCHAR(n) must be 2,147,483,647 currently.
The type can be declared using VARCHAR(n) where n is the maximum number of code points. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
STRING is a synonym for VARCHAR(2147483647).
Binary Strings # BINARY # Data type of a fixed-length binary string (=a sequence of bytes).
Declaration
SQL BINARY BINARY(n) Java/Scala DataTypes.BINARY(n) Bridging to JVM Types
Java Type Input Output Remarks byte[] X X Default Python Not supported. The type can be declared using BINARY(n) where n is the number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
VARBINARY / BYTES # Data type of a variable-length binary string (=a sequence of bytes).
Declaration
SQL VARBINARY VARBINARY(n) BYTES Java/Scala DataTypes.VARBINARY(n) DataTypes.BYTES() Bridging to JVM Types
Java Type Input Output Remarks byte[] X X Default Python DataTypes.VARBINARY(n) DataTypes.BYTES() Attention The specified maximum number of bytes n in DataTypes.VARBINARY(n) must be 2,147,483,647 currently.
The type can be declared using VARBINARY(n) where n is the maximum number of bytes. n must have a value between 1 and 2,147,483,647 (both inclusive). If no length is specified, n is equal to 1.
BYTES is a synonym for VARBINARY(2147483647).
Exact Numerics # DECIMAL # Data type of a decimal number with fixed precision and scale.
Declaration
SQL DECIMAL DECIMAL(p) DECIMAL(p, s) DEC DEC(p) DEC(p, s) NUMERIC NUMERIC(p) NUMERIC(p, s) Java/Scala DataTypes.DECIMAL(p, s) Bridging to JVM Types
Java Type Input Output Remarks java.math.BigDecimal X X Default org.apache.flink.table.data.DecimalData X X Internal data structure. Python DataTypes.DECIMAL(p, s) Attention The precision and scale specified in DataTypes.DECIMAL(p, s) must be 38 and 18 separately currently.
The type can be declared using DECIMAL(p, s) where p is the number of digits in a number (precision) and s is the number of digits to the right of the decimal point in a number (scale). p must have a value between 1 and 38 (both inclusive). s must have a value between 0 and p (both inclusive). The default value for p is 10. The default value for s is 0.
NUMERIC(p, s) and DEC(p, s) are synonyms for this type.
TINYINT # Data type of a 1-byte signed integer with values from -128 to 127.
Declaration
SQL TINYINT Java/Scala DataTypes.TINYINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Byte X X Default byte X (X) Output only if type is not nullable. Python DataTypes.TINYINT() SMALLINT # Data type of a 2-byte signed integer with values from -32,768 to 32,767.
Declaration
SQL SMALLINT Java/Scala DataTypes.SMALLINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Short X X Default short X (X) Output only if type is not nullable. Python DataTypes.SMALLINT() INT # Data type of a 4-byte signed integer with values from -2,147,483,648 to 2,147,483,647.
Declaration
SQL INT INTEGER Java/Scala DataTypes.INT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Integer X X Default int X (X) Output only if type is not nullable. Python DataTypes.INT() INTEGER is a synonym for this type.
BIGINT # Data type of an 8-byte signed integer with values from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.
Declaration
SQL BIGINT Java/Scala DataTypes.BIGINT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Long X X Default long X (X) Output only if type is not nullable. Python DataTypes.BIGINT() Approximate Numerics # FLOAT # Data type of a 4-byte single precision floating point number.
Compared to the SQL standard, the type does not take parameters.
Declaration
SQL FLOAT Java/Scala DataTypes.FLOAT() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Float X X Default float X (X) Output only if type is not nullable. Python DataTypes.FLOAT() DOUBLE # Data type of an 8-byte double precision floating point number.
Declaration
SQL DOUBLE DOUBLE PRECISION Java/Scala DataTypes.DOUBLE() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Double X X Default double X (X) Output only if type is not nullable. Python DataTypes.DOUBLE() DOUBLE PRECISION is a synonym for this type.
Date and Time # DATE # Data type of a date consisting of year-month-day with values ranging from 0000-01-01 to 9999-12-31.
Compared to the SQL standard, the range starts at year 0000.
Declaration
SQL DATE Java/Scala DataTypes.DATE() Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalDate X X Default java.sql.Date X X java.lang.Integer X X Describes the number of days since epoch. int X (X) Describes the number of days since epoch.
Output only if type is not nullable. Python DataTypes.DATE() TIME # Data type of a time without time zone consisting of hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 00:00:00.000000000 to 23:59:59.999999999.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.LocalTime. A time with time zone is not provided. Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported. A time with time zone is not provided. Declaration
SQL TIME TIME(p) Java/Scala DataTypes.TIME(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalTime X X Default java.sql.Time X X java.lang.Integer X X Describes the number of milliseconds of the day. int X (X) Describes the number of milliseconds of the day.
Output only if type is not nullable. java.lang.Long X X Describes the number of nanoseconds of the day. long X (X) Describes the number of nanoseconds of the day.
Output only if type is not nullable. Python DataTypes.TIME(p) Attention The precision specified in DataTypes.TIME(p) must be 0 currently.
The type can be declared using TIME(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 0.
TIMESTAMP # Data type of a timestamp without time zone consisting of year-month-day hour:minute:second[.fractional] with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 to 9999-12-31 23:59:59.999999999.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.LocalDateTime.
A conversion from and to BIGINT (a JVM long type) is not supported as this would imply a time zone. However, this type is time zone free. For more java.time.Instant-like semantics use TIMESTAMP_LTZ.
Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported.
A conversion from and to BIGINT is not supported as this would imply a time zone. However, this type is time zone free. If you have such a requirement please use TIMESTAMP_LTZ.
Declaration
SQL TIMESTAMP TIMESTAMP(p) TIMESTAMP WITHOUT TIME ZONE TIMESTAMP(p) WITHOUT TIME ZONE Java/Scala DataTypes.TIMESTAMP(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.LocalDateTime X X Default java.sql.Timestamp X X org.apache.flink.table.data.TimestampData X X Internal data structure. Python DataTypes.TIMESTAMP(p) Attention The precision specified in DataTypes.TIMESTAMP(p) must be 3 currently.
The type can be declared using TIMESTAMP(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.
TIMESTAMP(p) WITHOUT TIME ZONE is a synonym for this type.
TIMESTAMP WITH TIME ZONE # Data type of a timestamp with time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.
SQL/Java/Scala Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.OffsetDateTime. Python Compared to the SQL standard, leap seconds (23:59:60 and 23:59:61) are not supported. Compared to TIMESTAMP_LTZ, the time zone offset information is physically stored in every datum. It is used individually for every computation, visualization, or communication to external systems.
Declaration
SQL TIMESTAMP WITH TIME ZONE TIMESTAMP(p) WITH TIME ZONE Java/Scala DataTypes.TIMESTAMP_WITH_TIME_ZONE(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.OffsetDateTime X X Default java.time.ZonedDateTime X Ignores the zone ID. Python Not supported. SQL/Java/Scala The type can be declared using TIMESTAMP(p) WITH TIME ZONE where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6. Python TIMESTAMP_LTZ # Data type of a timestamp with local time zone consisting of year-month-day hour:minute:second[.fractional] zone with up to nanosecond precision and values ranging from 0000-01-01 00:00:00.000000000 +14:59 to 9999-12-31 23:59:59.999999999 -14:59.
SQL/Java/Scala Leap seconds (23:59:60 and 23:59:61) are not supported as the semantics are closer to java.time.OffsetDateTime.
Compared to TIMESTAMP WITH TIME ZONE, the time zone offset information is not stored physically in every datum. Instead, the type assumes java.time.Instant semantics in UTC time zone at the edges of the table ecosystem. Every datum is interpreted in the local time zone configured in the current session for computation and visualization.
Python Leap seconds (23:59:60 and 23:59:61) are not supported.
Compared to TIMESTAMP WITH TIME ZONE, the time zone offset information is not stored physically in every datum. Every datum is interpreted in the local time zone configured in the current session for computation and visualization.
This type fills the gap between time zone free and time zone mandatory timestamp types by allowing the interpretation of UTC timestamps according to the configured session time zone.
Declaration
SQL TIMESTAMP_LTZ TIMESTAMP_LTZ(p) TIMESTAMP WITH LOCAL TIME ZONE TIMESTAMP(p) WITH LOCAL TIME ZONE Java/Scala DataTypes.TIMESTAMP_LTZ(p) DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(p) Bridging to JVM Types
Java Type Input Output Remarks java.time.Instant X X Default java.lang.Integer X X Describes the number of seconds since epoch. int X (X) Describes the number of seconds since epoch.
Output only if type is not nullable. java.lang.Long X X Describes the number of milliseconds since epoch. long X (X) Describes the number of milliseconds since epoch.
Output only if type is not nullable. java.sql.Timestamp X X Describes the number of milliseconds since epoch. org.apache.flink.table.data.TimestampData X X Internal data structure. Python DataTypes.TIMESTAMP_LTZ(p) DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(p) Attention The precision specified in DataTypes.TIMESTAMP_LTZ(p) must be 3 currently.
The type can be declared using TIMESTAMP_LTZ(p) where p is the number of digits of fractional seconds (precision). p must have a value between 0 and 9 (both inclusive). If no precision is specified, p is equal to 6.
TIMESTAMP(p) WITH LOCAL TIME ZONE is a synonym for this type.
INTERVAL YEAR TO MONTH # Data type for a group of year-month interval types.
The type must be parameterized to one of the following resolutions:
interval of years, interval of years to months, or interval of months. An interval of year-month consists of +years-months with values ranging from -9999-11 to +9999-11.
The value representation is the same for all types of resolutions. For example, an interval of months of 50 is always represented in an interval-of-years-to-months format (with default year precision): +04-02.
Declaration
SQL INTERVAL YEAR INTERVAL YEAR(p) INTERVAL YEAR(p) TO MONTH INTERVAL MONTH Java/Scala DataTypes.INTERVAL(DataTypes.YEAR()) DataTypes.INTERVAL(DataTypes.YEAR(p)) DataTypes.INTERVAL(DataTypes.YEAR(p), DataTypes.MONTH()) DataTypes.INTERVAL(DataTypes.MONTH()) Bridging to JVM Types
Java Type Input Output Remarks java.time.Period X X Ignores the days part. Default java.lang.Integer X X Describes the number of months. int X (X) Describes the number of months.
Output only if type is not nullable. Python DataTypes.INTERVAL(DataTypes.YEAR()) DataTypes.INTERVAL(DataTypes.YEAR(p)) DataTypes.INTERVAL(DataTypes.YEAR(p), DataTypes.MONTH()) DataTypes.INTERVAL(DataTypes.MONTH()) The type can be declared using the above combinations where p is the number of digits of years (year precision). p must have a value between 1 and 4 (both inclusive). If no year precision is specified, p is equal to 2.
INTERVAL DAY TO SECOND # Data type for a group of day-time interval types.
The type must be parameterized to one of the following resolutions with up to nanosecond precision:
interval of days, interval of days to hours, interval of days to minutes, interval of days to seconds, interval of hours, interval of hours to minutes, interval of hours to seconds, interval of minutes, interval of minutes to seconds, or interval of seconds. An interval of day-time consists of +days hours:months:seconds.fractional with values ranging from -999999 23:59:59.999999999 to +999999 23:59:59.999999999. The value representation is the same for all types of resolutions. For example, an interval of seconds of 70 is always represented in an interval-of-days-to-seconds format (with default precisions): +00 00:01:10.000000.
Declaration
SQL INTERVAL DAY INTERVAL DAY(p1) INTERVAL DAY(p1) TO HOUR INTERVAL DAY(p1) TO MINUTE INTERVAL DAY(p1) TO SECOND(p2) INTERVAL HOUR INTERVAL HOUR TO MINUTE INTERVAL HOUR TO SECOND(p2) INTERVAL MINUTE INTERVAL MINUTE TO SECOND(p2) INTERVAL SECOND INTERVAL SECOND(p2) Java/Scala DataTypes.INTERVAL(DataTypes.DAY()) DataTypes.INTERVAL(DataTypes.DAY(p1)) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.MINUTE(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.SECOND()) DataTypes.INTERVAL(DataTypes.SECOND(p2)) Bridging to JVM Types
Java Type Input Output Remarks java.time.Duration X X Default java.lang.Long X X Describes the number of milliseconds. long X (X) Describes the number of milliseconds.
Output only if type is not nullable. Python DataTypes.INTERVAL(DataTypes.DAY()) DataTypes.INTERVAL(DataTypes.DAY(p1)) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.DAY(p1), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.HOUR()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.HOUR(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.MINUTE()) DataTypes.INTERVAL(DataTypes.MINUTE(), DataTypes.SECOND(p2)) DataTypes.INTERVAL(DataTypes.SECOND()) DataTypes.INTERVAL(DataTypes.SECOND(p2)) The type can be declared using the above combinations where p1 is the number of digits of days (day precision) and p2 is the number of digits of fractional seconds (fractional precision). p1 must have a value between 1 and 6 (both inclusive). p2 must have a value between 0 and 9 (both inclusive). If no p1 is specified, it is equal to 2 by default. If no p2 is specified, it is equal to 6 by default.
Constructured Data Types # ARRAY # Data type of an array of elements with same subtype.
Compared to the SQL standard, the maximum cardinality of an array cannot be specified but is fixed at 2,147,483,647. Also, any valid type is supported as a subtype.
Declaration
SQL ARRAY\u0026lt;t\u0026gt; t ARRAY Java/Scala DataTypes.ARRAY(t) Bridging to JVM Types
Java Type Input Output Remarks t[] (X) (X) Depends on the subtype. Default java.util.List\u0026lt;t\u0026gt; X X subclass of java.util.List\u0026lt;t\u0026gt; X org.apache.flink.table.data.ArrayData X X Internal data structure. Python DataTypes.ARRAY(t) The type can be declared using ARRAY\u0026lt;t\u0026gt; where t is the data type of the contained elements.
t ARRAY is a synonym for being closer to the SQL standard. For example, INT ARRAY is equivalent to ARRAY\u0026lt;INT\u0026gt;.
MAP # Data type of an associative array that maps keys (including NULL) to values (including NULL). A map cannot contain duplicate keys; each key can map to at most one value.
There is no restriction of element types; it is the responsibility of the user to ensure uniqueness.
The map type is an extension to the SQL standard.
Declaration
SQL MAP\u0026lt;kt, vt\u0026gt; Java/Scala DataTypes.MAP(kt, vt) Bridging to JVM Types
Java Type Input Output Remarks java.util.Map\u0026lt;kt, vt\u0026gt; X X Default subclass of java.util.Map\u0026lt;kt, vt\u0026gt; X org.apache.flink.table.data.MapData X X Internal data structure. Python DataTypes.MAP(kt, vt) The type can be declared using MAP\u0026lt;kt, vt\u0026gt; where kt is the data type of the key elements and vt is the data type of the value elements.
MULTISET # Data type of a multiset (=bag). Unlike a set, it allows for multiple instances for each of its elements with a common subtype. Each unique value (including NULL) is mapped to some multiplicity.
There is no restriction of element types; it is the responsibility of the user to ensure uniqueness.
Declaration
SQL MULTISET\u0026lt;t\u0026gt; t MULTISET Java/Scala DataTypes.MULTISET(t) Bridging to JVM Types
Java Type Input Output Remarks java.util.Map\u0026lt;t, java.lang.Integer\u0026gt; X X Assigns each value to an integer multiplicity. Default subclass of java.util.Map\u0026lt;t, java.lang.Integer\u0026gt;\u0026gt; X org.apache.flink.table.data.MapData X X Internal data structure. Python DataTypes.MULTISET(t) The type can be declared using MULTISET\u0026lt;t\u0026gt; where t is the data type of the contained elements.
t MULTISET is a synonym for being closer to the SQL standard. For example, INT MULTISET is equivalent to MULTISET\u0026lt;INT\u0026gt;.
ROW # Data type of a sequence of fields.
A field consists of a field name, field type, and an optional description. The most specific type of a row of a table is a row type. In this case, each column of the row corresponds to the field of the row type that has the same ordinal position as the column.
Compared to the SQL standard, an optional field description simplifies the handling with complex structures.
A row type is similar to the STRUCT type known from other non-standard-compliant frameworks.
Declaration
SQL ROW\u0026lt;n0 t0, n1 t1, ...\u0026gt; ROW\u0026lt;n0 t0 \u0026#39;d0\u0026#39;, n1 t1 \u0026#39;d1\u0026#39;, ...\u0026gt; ROW(n0 t0, n1 t1, ...\u0026gt; ROW(n0 t0 \u0026#39;d0\u0026#39;, n1 t1 \u0026#39;d1\u0026#39;, ...) Java/Scala DataTypes.ROW(DataTypes.FIELD(n0, t0), DataTypes.FIELD(n1, t1), ...) DataTypes.ROW(DataTypes.FIELD(n0, t0, d0), DataTypes.FIELD(n1, t1, d1), ...) Bridging to JVM Types
Java Type Input Output Remarks org.apache.flink.types.Row X X Default org.apache.flink.table.data.RowData X X Internal data structure. Python DataTypes.ROW([DataTypes.FIELD(n0, t0), DataTypes.FIELD(n1, t1), ...]) DataTypes.ROW([DataTypes.FIELD(n0, t0, d0), DataTypes.FIELD(n1, t1, d1), ...]) The type can be declared using ROW\u0026lt;n0 t0 'd0', n1 t1 'd1', ...\u0026gt; where n is the unique name of a field, t is the logical type of a field, d is the description of a field.
ROW(...) is a synonym for being closer to the SQL standard. For example, ROW(myField INT, myOtherField BOOLEAN) is equivalent to ROW\u0026lt;myField INT, myOtherField BOOLEAN\u0026gt;.
User-Defined Data Types # Java/Scala Attention User-defined data types are not fully supported yet. They are currently (as of Flink 1.11) only exposed as unregistered structured types in parameters and return types of functions.
A structured type is similar to an object in an object-oriented programming language. It contains zero, one or more attributes. Each attribute consists of a name and a type.
There are two kinds of structured types:
Types that are stored in a catalog and are identified by a catalog identifier (like cat.db.MyType). Those are equal to the SQL standard definition of structured types.
Anonymously defined, unregistered types (usually reflectively extracted) that are identified by an implementation class (like com.myorg.model.MyType). Those are useful when programmatically defining a table program. They enable reusing existing JVM classes without manually defining the schema of a data type again.
Registered Structured Types # Currently, registered structured types are not supported. Thus, they cannot be stored in a catalog or referenced in a CREATE TABLE DDL.
Unregistered Structured Types # Unregistered structured types can be created from regular POJOs (Plain Old Java Objects) using automatic reflective extraction.
The implementation class of a structured type must meet the following requirements:
The class must be globally accessible which means it must be declared public, static, and not abstract. The class must offer a default constructor with zero arguments or a full constructor that assigns all fields. All fields of the class must be readable by either public declaration or a getter that follows common coding style such as getField(), isField(), field(). All fields of the class must be writable by either public declaration, fully assigning constructor, or a setter that follows common coding style such as setField(...), field(...). All fields must be mapped to a data type either implicitly via reflective extraction or explicitly using the @DataTypeHint annotations. Fields that are declared static or transient are ignored. The reflective extraction supports arbitrary nesting of fields as long as a field type does not (transitively) refer to itself.
The declared field class (e.g. public int age;) must be contained in the list of supported JVM bridging classes defined for every data type in this document (e.g. java.lang.Integer or int for INT).
For some classes an annotation is required in order to map the class to a data type (e.g. @DataTypeHint(\u0026quot;DECIMAL(10, 2)\u0026quot;) to assign a fixed precision and scale for java.math.BigDecimal).
Python Declaration
Java class User { // extract fields automatically public int age; public String name; // enrich the extraction with precision information public @DataTypeHint(\u0026#34;DECIMAL(10, 2)\u0026#34;) BigDecimal totalBalance; // enrich the extraction with forcing using RAW types public @DataTypeHint(\u0026#34;RAW\u0026#34;) Class\u0026lt;?\u0026gt; modelClass; } DataTypes.of(User.class); Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default org.apache.flink.types.Row X X Represent the structured type as a row. org.apache.flink.table.data.RowData X X Internal data structure. Scala case class User( // extract fields automatically age: Int, name: String, // enrich the extraction with precision information @DataTypeHint(\u0026#34;DECIMAL(10, 2)\u0026#34;) totalBalance: java.math.BigDecimal, // enrich the extraction with forcing using a RAW type @DataTypeHint(\u0026#34;RAW\u0026#34;) modelClass: Class[_] ) DataTypes.of(classOf[User]) Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default org.apache.flink.types.Row X X Represent the structured type as a row. org.apache.flink.table.data.RowData X X Internal data structure. Python Not supported. Other Data Types # BOOLEAN # Data type of a boolean with a (possibly) three-valued logic of TRUE, FALSE, and UNKNOWN.
Declaration
SQL BOOLEAN Java/Scala DataTypes.BOOLEAN() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Boolean X X Default boolean X (X) Output only if type is not nullable. Python DataTypes.BOOLEAN() RAW # Data type of an arbitrary serialized type. This type is a black box within the table ecosystem and is only deserialized at the edges.
The raw type is an extension to the SQL standard.
Declaration
SQL RAW(\u0026#39;class\u0026#39;, \u0026#39;snapshot\u0026#39;) Java/Scala DataTypes.RAW(class, serializer) DataTypes.RAW(class) Bridging to JVM Types
Java Type Input Output Remarks class X X Originating class or subclasses (for input) or superclasses (for output). Default byte[] X org.apache.flink.table.data.RawValueData X X Internal data structure. Python Not supported. SQL/Java/Scala The type can be declared using RAW('class', 'snapshot') where class is the originating class and snapshot is the serialized TypeSerializerSnapshot in Base64 encoding. Usually, the type string is not declared directly but is generated while persisting the type.
In the API, the RAW type can be declared either by directly supplying a Class + TypeSerializer or by passing Class and letting the framework extract Class + TypeSerializer from there.
Python NULL # Data type for representing untyped NULL values.
The null type is an extension to the SQL standard. A null type has no other value except NULL, thus, it can be cast to any nullable type similar to JVM semantics.
This type helps in representing unknown types in API calls that use a NULL literal as well as bridging to formats such as JSON or Avro that define such a type as well.
This type is not very useful in practice and is just mentioned here for completeness.
Declaration
SQL NULL Java/Scala DataTypes.NULL() Bridging to JVM Types
Java Type Input Output Remarks java.lang.Object X X Default any class (X) Any non-primitive type. Python Not supported. CAST 方法 # Flink Table API 和 Flink SQL 支持从 输入 数据类型 到 目标 数据类型的转换。有的转换 无论输入值是什么都能保证转换成功，而有些转换则会在运行时失败（即不可能转换为 目标 数据类型对应的值）。 例如，将 INT 数据类型的值转换为 STRING 数据类型一定能转换成功，但无法保证将 STRING 数据类型转换为 INT 数据类型。
在生成执行计划时，Flink 的 SQL 检查器会拒绝提交那些不可能直接转换为 目标 数据类型的SQL，并抛出 ValidationException 异常， 例如从 TIMESTAMP 类型转化到 INTERVAL 类型。 然而有些查询即使通过了 SQL 检查器的验证，依旧可能会在运行期间转换失败，这就需要用户正确处理这些失败了。
在 Flink Table API 和 Flink SQL 中，可以用下面两个内置方法来进行转换操作：
CAST：定义在 SQL 标准的 CAST 方法。在某些容易发生转换失败的查询场景中，当实际输入数据不合法时，作业便会运行失败。类型推导会保留输入类型的可空性。 TRY_CAST：常规 CAST 方法的扩展，当转换失败时返回 NULL。该方法的返回值允许为空。 例如：
CAST(\u0026#39;42\u0026#39; AS INT) --- 结果返回数字 42 的 INT 格式（非空） CAST(NULL AS VARCHAR) --- 结果返回 VARCHAR 类型的空值 CAST(\u0026#39;non-number\u0026#39; AS INT) --- 抛出异常，并停止作业 TRY_CAST(\u0026#39;42\u0026#39; AS INT) --- 结果返回数字 42 的 INT 格式 TRY_CAST(NULL AS VARCHAR) --- 结果返回 VARCHAR 类型的空值 TRY_CAST(\u0026#39;non-number\u0026#39; AS INT) --- 结果返回 INT 类型的空值 COALESCE(TRY_CAST(\u0026#39;non-number\u0026#39; AS INT), 0) --- 结果返回数字 0 的 INT 格式（非空） 下表展示了各个类型的转换程度，\u0026ldquo;Y\u0026rdquo; 表示支持，\u0026quot;!\u0026quot; 表示转换可能会失败，\u0026ldquo;N\u0026rdquo; 表示不支持：
输入类型\\目标类型 CHAR¹/VARCHAR¹/STRING BINARY¹/VARBINARY¹/BYTES BOOLEAN DECIMAL TINYINT SMALLINT INTEGER BIGINT FLOAT DOUBLE DATE TIME TIMESTAMP TIMESTAMP_LTZ INTERVAL ARRAY MULTISET MAP ROW STRUCTURED RAW CHAR/VARCHAR/STRING Y ! ! ! ! ! ! ! ! ! ! ! ! ! N N N N N N N BINARY/VARBINARY/BYTES Y Y N N N N N N N N N N N N N N N N N N N BOOLEAN Y N Y Y Y Y Y Y Y Y N N N N N N N N N N N DECIMAL Y N N Y Y Y Y Y Y Y N N N N N N N N N N N TINYINT Y N Y Y Y Y Y Y Y Y N N N² N² N N N N N N N SMALLINT Y N Y Y Y Y Y Y Y Y N N N² N² N N N N N N N INTEGER Y N Y Y Y Y Y Y Y Y N N N² N² Y⁵ N N N N N N BIGINT Y N Y Y Y Y Y Y Y Y N N N² N² Y⁶ N N N N N N FLOAT Y N N Y Y Y Y Y Y Y N N N N N N N N N N N DOUBLE Y N N Y Y Y Y Y Y Y N N N N N N N N N N N DATE Y N N N N N N N N N Y N Y Y N N N N N N N TIME Y N N N N N N N N N N Y Y Y N N N N N N N TIMESTAMP Y N N N N N N N N N Y Y Y Y N N N N N N N TIMESTAMP_LTZ Y N N N N N N N N N Y Y Y Y N N N N N N N INTERVAL Y N N N N N Y⁵ Y⁶ N N N N N N Y N N N N N N ARRAY Y N N N N N N N N N N N N N N !³ N N N N N MULTISET Y N N N N N N N N N N N N N N N !³ N N N N MAP Y N N N N N N N N N N N N N N N N !³ N N N ROW Y N N N N N N N N N N N N N N N N N !³ N N STRUCTURED Y N N N N N N N N N N N N N N N N N N !³ N RAW Y ! N N N N N N N N N N N N N N N N N N Y⁴ 备注：
所有转化到具有固长或变长的类型时会根据类型的定义来裁剪或填充数据。 使用 TO_TIMESTAMP 方法和 TO_TIMESTAMP_LTZ 方法的场景，不要使用 CAST 或 TRY_CAST。 支持转换，当且仅当用其内部数据结构也支持转化时。转换可能会失败，当且仅当用其内部数据结构也可能会转换失败。 支持转换，当且仅当用使用 RAW 的类和类的序列化器一样。 支持转换，当且仅当用使用 INTERVAL 做“月”到“年”的转换。 支持转换，当且仅当用使用 INTERVAL 做“天”到“时间”的转换。 请注意：无论是 CAST 还是 TRY_CAST，当输入为 NULL ，输出也为 NULL。
旧版本 CAST 方法 # 用户可以通过将参数 table.exec.legacy-cast-behaviour 设置为 enabled 来启用 1.15 版本之前的 CAST 行为。 在 Flink 1.15 版本此参数默认为 disabled。
如果设置为 enabled，请注意以下问题：
转换为 CHAR/VARCHAR/BINARY/VARBINARY 数据类型时，不再自动修剪（trim）或填充（pad）。 使用 CAST 时不再会因为转化失败而停止作业，只会返回 NULL，但不会像 TRY_CAST 那样推断正确的类型。 CHAR/VARCHAR/STRING 的转换结果会有一些细微的差别。 我们 不建议 配置此参数，而是 强烈建议 在新项目中保持这个参数为默认禁用，以使用最新版本的 CAST 方法。 在下一个版本，这个参数会被移除。 数据类型提取 # Java/Scala 在 API 中的很多地方，Flink 都尝试利用反射机制从类信息中自动提取数据类型，以避免重复地手动定义 schema。但是，通过反射提取数据类型并不总是有效的，因为有可能会缺失逻辑信息。因此，可能需要在类或字段声明的附近添加额外信息以支持提取逻辑。
下表列出了无需更多信息即可隐式映射到数据类型的类。
如果你打算在 Scala 中实现类，建议使用包装类型（例如 java.lang.Integer）而不是 Scala 的基本类型。如下表所示，Scala 的基本类型（例如 Int 或 Double）会被编译为 JVM 基本类型（例如 int/double）并产生 NOT NULL 语义。此外，在泛型中使用的 Scala 基本类型（例如 java.util.Map[Int, Double]）在编译期间会被擦除，导致类信息类似于 java.util.Map[java.lang.Object, java.lang.Object]。
类 数据类型 java.lang.String STRING java.lang.Boolean BOOLEAN boolean BOOLEAN NOT NULL java.lang.Byte TINYINT byte TINYINT NOT NULL java.lang.Short SMALLINT short SMALLINT NOT NULL java.lang.Integer INT int INT NOT NULL java.lang.Long BIGINT long BIGINT NOT NULL java.lang.Float FLOAT float FLOAT NOT NULL java.lang.Double DOUBLE double DOUBLE NOT NULL java.sql.Date DATE java.time.LocalDate DATE java.sql.Time TIME(0) java.time.LocalTime TIME(9) java.sql.Timestamp TIMESTAMP(9) java.time.LocalDateTime TIMESTAMP(9) java.time.OffsetDateTime TIMESTAMP(9) WITH TIME ZONE java.time.Instant TIMESTAMP_LTZ(9) java.time.Duration INTERVAL SECOND(9) java.time.Period INTERVAL YEAR(4) TO MONTH byte[] BYTES T[] ARRAY\u0026lt;T\u0026gt; java.util.Map\u0026lt;K, V\u0026gt; MAP\u0026lt;K, V\u0026gt; 结构化类型 T 匿名结构化类型 T 本文档中提到的其他 JVM 桥接类需要 @DataTypeHint 注释。
数据类型 hints 可以参数化或替换单个函数参数和返回类型、结构化类或结构化类的字段的默认提取逻辑。实现者可以通过声明 @DataTypeHint 注解来选择默认提取逻辑的修改程度。
@DataTypeHint 注解提供了一组可选的 hint 参数。其中一些参数如以下示例所示。更多信息可以在注解类的文档中找到。
Python Java import org.apache.flink.table.annotation.DataTypeHint; class User { // 使用默认转换类 \`java.lang.Integer\` 定义 INT 数据类型 public @DataTypeHint(\u0026#34;INT\u0026#34;) Object o; // 使用显式转换类定义毫秒精度的 TIMESTAMP 数据类型 public @DataTypeHint(value = \u0026#34;TIMESTAMP(3)\u0026#34;, bridgedTo = java.sql.Timestamp.class) Object o; // 通过强制使用 RAW 类型来丰富提取 public @DataTypeHint(\u0026#34;RAW\u0026#34;) Class\u0026lt;?\u0026gt; modelClass; // 定义所有出现的 java.math.BigDecimal（包含嵌套字段）都将被提取为 DECIMAL(12, 2) public @DataTypeHint(defaultDecimalPrecision = 12, defaultDecimalScale = 2) AccountStatement stmt; // 定义当类型不能映射到数据类型时，总是将其视为 RAW 类型，而不是抛出异常 public @DataTypeHint(allowRawGlobally = HintFlag.TRUE) ComplexModel model; } Scala import org.apache.flink.table.annotation.DataTypeHint class User { // 使用默认转换类 \`java.lang.Integer\` 定义 INT 数据类型 @DataTypeHint(\u0026#34;INT\u0026#34;) var o: AnyRef // 使用显式转换类定义毫秒精度的 TIMESTAMP 数据类型 @DataTypeHint(value = \u0026#34;TIMESTAMP(3)\u0026#34;, bridgedTo = java.sql.Timestamp.class) var o: AnyRef // 通过强制使用 RAW 类型来丰富提取 @DataTypeHint(\u0026#34;RAW\u0026#34;) var modelClass: Class[_] // 定义所有出现的 java.math.BigDecimal（包含嵌套字段）都将被提取为 DECIMAL(12, 2) @DataTypeHint(defaultDecimalPrecision = 12, defaultDecimalScale = 2) var stmt: AccountStatement // 定义当类型不能映射到数据类型时，总是将其视为 RAW 类型，而不是抛出异常 @DataTypeHint(allowRawGlobally = HintFlag.TRUE) var model: ComplexModel } Python 不支持。 Back to top
`}),e.add({id:263,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream_tutorial/",title:"DataStream API 教程",section:"Python API",content:` DataStream API 教程 # Apache Flink 提供了 DataStream API，用于构建健壮的、有状态的流式应用程序。它提供了对状态和时间细粒度控制，从而允许实现高级事件驱动系统。 在这篇教程中，你将学习如何使用 PyFlink 和 DataStream API 构建一个简单的流式应用程序。
你要搭建一个什么系统 # 在本教程中，你将学习如何编写一个简单的 Python DataStream 作业。 该程序读取一个 csv 文件，计算词频，并将结果写到一个结果文件中。
准备条件 # 本教程假设你对 Python 有一定的了解，但是即使你使用的是其它编程语言，你也应该能够学会。
困难求助 # 如果你有疑惑，可以查阅 社区支持资源。 特别是，Apache Flink 用户邮件列表 一直被评为所有 Apache 项目中最活跃的一个，也是快速获得帮助的好方法。
怎样跟着教程练习 # 首先，你需要在你的电脑上准备以下环境：
Java 11 Python 3.6, 3.7, 3.8 or 3.9 使用 Python DataStream API 需要安装 PyFlink，PyFlink 发布在 PyPI上，可以通过 pip 快速安装。
\$ python -m pip install apache-flink 一旦 PyFlink 安装完成之后，你就可以开始编写 Python DataStream 作业了。
编写一个 Flink Python DataStream API 程序 # DataStream API 应用程序首先需要声明一个执行环境（StreamExecutionEnvironment），这是流式程序执行的上下文。你将通过它来设置作业的属性（例如默认并发度、重启策略等）、创建源、并最终触发作业的执行。
env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) env.set_parallelism(1) 一旦创建了 StreamExecutionEnvironment 之后，你可以使用它来声明数据源。数据源从外部系统（如 Apache Kafka、Rabbit MQ 或 Apache Pulsar）拉取数据到 Flink 作业里。
为了简单起见，本教程读取文件作为数据源。
ds = env.from_source( source=FileSource.for_record_stream_format(StreamFormat.text_line_format(), input_path) .process_static_file_set().build(), watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;file_source\u0026#34; ) 你现在可以在这个数据流上执行转换操作，或者使用 sink 将数据写入外部系统。本教程使用 FileSink 将结果数据写入文件中。
ds.sink_to( sink=FileSink.for_row_format( base_path=output_path, encoder=Encoder.simple_string_encoder()) .with_output_file_config( OutputFileConfig.builder() .with_part_prefix(\u0026#34;prefix\u0026#34;) .with_part_suffix(\u0026#34;.ext\u0026#34;) .build()) .with_rolling_policy(RollingPolicy.default_rolling_policy()) .build() ) def split(line): yield from line.split() # compute word count ds = ds.flat_map(split) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) 最后一步是执行 PyFlink DataStream API 作业。PyFlink applications 是懒加载的，并且只有在完全构建之后才会提交给集群上执行。要执行一个应用程序，你只需简单地调用 env.execute()。
env.execute() 完整的代码如下:
import argparse import logging import sys from pyflink.common import WatermarkStrategy, Encoder, Types from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode from pyflink.datastream.connectors.file_system import FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy word_count_data = [\u0026#34;To be, or not to be,--that is the question:--\u0026#34;, \u0026#34;Whether \u0026#39;tis nobler in the mind to suffer\u0026#34;, \u0026#34;The slings and arrows of outrageous fortune\u0026#34;, \u0026#34;Or to take arms against a sea of troubles,\u0026#34;, \u0026#34;And by opposing end them?--To die,--to sleep,--\u0026#34;, \u0026#34;No more; and by a sleep to say we end\u0026#34;, \u0026#34;The heartache, and the thousand natural shocks\u0026#34;, \u0026#34;That flesh is heir to,--\u0026#39;tis a consummation\u0026#34;, \u0026#34;Devoutly to be wish\u0026#39;d. To die,--to sleep;--\u0026#34;, \u0026#34;To sleep! perchance to dream:--ay, there\u0026#39;s the rub;\u0026#34;, \u0026#34;For in that sleep of death what dreams may come,\u0026#34;, \u0026#34;When we have shuffled off this mortal coil,\u0026#34;, \u0026#34;Must give us pause: there\u0026#39;s the respect\u0026#34;, \u0026#34;That makes calamity of so long life;\u0026#34;, \u0026#34;For who would bear the whips and scorns of time,\u0026#34;, \u0026#34;The oppressor\u0026#39;s wrong, the proud man\u0026#39;s contumely,\u0026#34;, \u0026#34;The pangs of despis\u0026#39;d love, the law\u0026#39;s delay,\u0026#34;, \u0026#34;The insolence of office, and the spurns\u0026#34;, \u0026#34;That patient merit of the unworthy takes,\u0026#34;, \u0026#34;When he himself might his quietus make\u0026#34;, \u0026#34;With a bare bodkin? who would these fardels bear,\u0026#34;, \u0026#34;To grunt and sweat under a weary life,\u0026#34;, \u0026#34;But that the dread of something after death,--\u0026#34;, \u0026#34;The undiscover\u0026#39;d country, from whose bourn\u0026#34;, \u0026#34;No traveller returns,--puzzles the will,\u0026#34;, \u0026#34;And makes us rather bear those ills we have\u0026#34;, \u0026#34;Than fly to others that we know not of?\u0026#34;, \u0026#34;Thus conscience does make cowards of us all;\u0026#34;, \u0026#34;And thus the native hue of resolution\u0026#34;, \u0026#34;Is sicklied o\u0026#39;er with the pale cast of thought;\u0026#34;, \u0026#34;And enterprises of great pith and moment,\u0026#34;, \u0026#34;With this regard, their currents turn awry,\u0026#34;, \u0026#34;And lose the name of action.--Soft you now!\u0026#34;, \u0026#34;The fair Ophelia!--Nymph, in thy orisons\u0026#34;, \u0026#34;Be all my sins remember\u0026#39;d.\u0026#34;] def word_count(input_path, output_path): env = StreamExecutionEnvironment.get_execution_environment() env.set_runtime_mode(RuntimeExecutionMode.BATCH) # write all the data to one file env.set_parallelism(1) # define the source if input_path is not None: ds = env.from_source( source=FileSource.for_record_stream_format(StreamFormat.text_line_format(), input_path) .process_static_file_set().build(), watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(), source_name=\u0026#34;file_source\u0026#34; ) else: print(\u0026#34;Executing word_count example with default input data set.\u0026#34;) print(\u0026#34;Use --input to specify file input.\u0026#34;) ds = env.from_collection(word_count_data) def split(line): yield from line.split() # compute word count ds = ds.flat_map(split) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) # define the sink if output_path is not None: ds.sink_to( sink=FileSink.for_row_format( base_path=output_path, encoder=Encoder.simple_string_encoder()) .with_output_file_config( OutputFileConfig.builder() .with_part_prefix(\u0026#34;prefix\u0026#34;) .with_part_suffix(\u0026#34;.ext\u0026#34;) .build()) .with_rolling_policy(RollingPolicy.default_rolling_policy()) .build() ) else: print(\u0026#34;Printing result to stdout. Use --output to specify output path.\u0026#34;) ds.print() # submit for execution env.execute() if __name__ == \u0026#39;__main__\u0026#39;: logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\u0026#34;%(message)s\u0026#34;) parser = argparse.ArgumentParser() parser.add_argument( \u0026#39;--input\u0026#39;, dest=\u0026#39;input\u0026#39;, required=False, help=\u0026#39;Input file to process.\u0026#39;) parser.add_argument( \u0026#39;--output\u0026#39;, dest=\u0026#39;output\u0026#39;, required=False, help=\u0026#39;Output file to write results to.\u0026#39;) argv = sys.argv[1:] known_args, _ = parser.parse_known_args(argv) word_count(known_args.input, known_args.output) 执行一个 Flink Python DataStream API 程序 # 现在你已经编写好 PyFlink 程序，可以通过如下命令执行它:
\$ python word_count.py 这个命令会在本地集群中构建并运行 PyFlink 程序。你也可以使用 Job Submission Examples 中描述的命令将其提交到远程集群。
最后，你可以得到如下运行结果:
(a,5) (Be,1) (Is,1) (No,2) ... 本教程为你开始编写自己的 PyFlink DataStream API 程序提供了基础。你也可以访问 PyFlink 示例 ，了解更多关于 PyFlink 的示例。 如果需要了解更多关于 Python DataStream API 的使用，请查阅 Flink Python API 文档 。
`}),e.add({id:264,href:"/flink/flink-docs-master/zh/docs/dev/table/timezone/",title:"时区",section:"Table API \u0026 SQL",content:` 时区 # Flink 为日期和时间提供了丰富的数据类型， 包括 DATE， TIME， TIMESTAMP， TIMESTAMP_LTZ， INTERVAL YEAR TO MONTH， INTERVAL DAY TO SECOND (更多详情请参考 Date and Time)。 Flink 支持在 session （会话）级别设置时区（更多详情请参考 table.local-time-zone）。 Flink 对多种时间类型和时区的支持使得跨时区的数据处理变得非常容易。
TIMESTAMP vs TIMESTAMP_LTZ # TIMESTAMP 类型 # TIMESTAMP(p) 是 TIMESTAMP(p) WITHOUT TIME ZONE 的简写， 精度 p 支持的范围是0-9， 默认是6。 TIMESTAMP 用于描述年， 月， 日， 小时， 分钟， 秒 和 小数秒对应的时间戳。 TIMESTAMP 可以通过一个字符串来指定，例如： Flink SQL\u0026gt; SELECT TIMESTAMP \u0026#39;1970-01-01 00:00:04.001\u0026#39;; +-------------------------+ | 1970-01-01 00:00:04.001 | +-------------------------+ TIMESTAMP_LTZ 类型 # TIMESTAMP_LTZ(p) 是 TIMESTAMP(p) WITH LOCAL TIME ZONE 的简写， 精度 p 支持的范围是0-9， 默认是6。 TIMESTAMP_LTZ 用于描述时间线上的绝对时间点， 使用 long 保存从 epoch 至今的毫秒数， 使用int保存毫秒中的纳秒数。 epoch 时间是从 java 的标准 epoch 时间 1970-01-01T00:00:00Z 开始计算。 在计算和可视化时， 每个 TIMESTAMP_LTZ 类型的数据都是使用的 session （会话）中配置的时区。 TIMESTAMP_LTZ 没有字符串表达形式因此无法通过字符串来指定， 可以通过一个 long 类型的 epoch 时间来转化(例如: 通过 Java 来产生一个 long 类型的 epoch 时间 System.currentTimeMillis()) Flink SQL\u0026gt; CREATE VIEW T1 AS SELECT TO_TIMESTAMP_LTZ(4001, 3); Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM T1; +---------------------------+ | TO_TIMESTAMP_LTZ(4001, 3) | +---------------------------+ | 1970-01-01 00:00:04.001 | +---------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM T1; +---------------------------+ | TO_TIMESTAMP_LTZ(4001, 3) | +---------------------------+ | 1970-01-01 08:00:04.001 | +---------------------------+ TIMESTAMP_LTZ 可以用于跨时区的计算，因为它是一个基于 epoch 的绝对时间点（比如上例中的 4001 毫秒）代表的就是不同时区的同一个绝对时间点。 补充一个背景知识：在同一个时间点， 全世界所有的机器上执行 System.currentTimeMillis() 都会返回同样的值。 (比如上例中的 4001 milliseconds), 这就是绝对时间的定义。 时区的作用 # 本地时区定义了当前 session（会话）所在的时区， 你可以在 Sql client 或者应用程序中配置。
SQL Client -- 设置为 UTC 时区 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; -- 设置为上海时区 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; -- 设置为Los_Angeles时区 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;America/Los_Angeles\u0026#39;; Java EnvironmentSettings envSetting = EnvironmentSettings.inStreamingMode(); TableEnvironment tEnv = TableEnvironment.create(envSetting); // 设置为 UTC 时区 tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;UTC\u0026#34;)); // 设置为上海时区 tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;Asia/Shanghai\u0026#34;)); // 设置为 Los_Angeles 时区 tEnv.getConfig().setLocalTimeZone(ZoneId.of(\u0026#34;America/Los_Angeles\u0026#34;)); Scala val envSetting = EnvironmentSettings.inStreamingMode() val tEnv = TableEnvironment.create(envSetting) // 设置为 UTC 时区 tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;UTC\u0026#34;)) // 设置为上海时区 tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;Asia/Shanghai\u0026#34;)) // 设置为 Los_Angeles 时区 tEnv.getConfig.setLocalTimeZone(ZoneId.of(\u0026#34;America/Los_Angeles\u0026#34;)) Python env_setting = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_setting) # set to UTC time zone t_env.get_config().set_local_timezone(\u0026#34;UTC\u0026#34;) # set to Shanghai time zone t_env.get_config().set_local_timezone(\u0026#34;Asia/Shanghai\u0026#34;) # set to Los_Angeles time zone t_env.get_config().set_local_timezone(\u0026#34;America/Los_Angeles\u0026#34;) session（会话）的时区设置在 Flink SQL 中非常有用， 它的主要用法如下:
确定时间函数的返回值 # session （会话）中配置的时区会对以下函数生效。
LOCALTIME LOCALTIMESTAMP CURRENT_DATE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_ROW_TIMESTAMP() NOW() PROCTIME() Flink SQL\u0026gt; SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; Flink SQL\u0026gt; CREATE VIEW MyView1 AS SELECT LOCALTIME, LOCALTIMESTAMP, CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_ROW_TIMESTAMP(), NOW(), PROCTIME(); Flink SQL\u0026gt; DESC MyView1; +------------------------+-----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +------------------------+-----------------------------+-------+-----+--------+-----------+ | LOCALTIME | TIME(0) | false | | | | | LOCALTIMESTAMP | TIMESTAMP(3) | false | | | | | CURRENT_DATE | DATE | false | | | | | CURRENT_TIME | TIME(0) | false | | | | | CURRENT_TIMESTAMP | TIMESTAMP_LTZ(3) | false | | | | |CURRENT_ROW_TIMESTAMP() | TIMESTAMP_LTZ(3) | false | | | | | NOW() | TIMESTAMP_LTZ(3) | false | | | | | PROCTIME() | TIMESTAMP_LTZ(3) *PROCTIME* | false | | | | +------------------------+-----------------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView1; +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | LOCALTIME | LOCALTIMESTAMP | CURRENT_DATE | CURRENT_TIME | CURRENT_TIMESTAMP | CURRENT_ROW_TIMESTAMP() | NOW() | PROCTIME() | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | 15:18:36 | 2021-04-15 15:18:36.384 | 2021-04-15 | 15:18:36 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | 2021-04-15 15:18:36.384 | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView1; +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | LOCALTIME | LOCALTIMESTAMP | CURRENT_DATE | CURRENT_TIME | CURRENT_TIMESTAMP | CURRENT_ROW_TIMESTAMP() | NOW() | PROCTIME() | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ | 23:18:36 | 2021-04-15 23:18:36.384 | 2021-04-15 | 23:18:36 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | 2021-04-15 23:18:36.384 | +-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+ TIMESTAMP_LTZ 字符串表示 # 当一个 TIMESTAMP_LTZ 值转为 string 格式时， session 中配置的时区会生效。 例如打印这个值，将类型强制转化为 STRING 类型， 将类型强制转换为 TIMESTAMP ，将 TIMESTAMP 的值转化为 TIMESTAMP_LTZ 类型：
Flink SQL\u0026gt; CREATE VIEW MyView2 AS SELECT TO_TIMESTAMP_LTZ(4001, 3) AS ltz, TIMESTAMP \u0026#39;1970-01-01 00:00:01.001\u0026#39; AS ntz; Flink SQL\u0026gt; DESC MyView2; +------+------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +------+------------------+-------+-----+--------+-----------+ | ltz | TIMESTAMP_LTZ(3) | true | | | | | ntz | TIMESTAMP(3) | false | | | | +------+------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView2; +-------------------------+-------------------------+ | ltz | ntz | +-------------------------+-------------------------+ | 1970-01-01 00:00:04.001 | 1970-01-01 00:00:01.001 | +-------------------------+-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView2; +-------------------------+-------------------------+ | ltz | ntz | +-------------------------+-------------------------+ | 1970-01-01 08:00:04.001 | 1970-01-01 00:00:01.001 | +-------------------------+-------------------------+ Flink SQL\u0026gt; CREATE VIEW MyView3 AS SELECT ltz, CAST(ltz AS TIMESTAMP(3)), CAST(ltz AS STRING), ntz, CAST(ntz AS TIMESTAMP_LTZ(3)) FROM MyView2; Flink SQL\u0026gt; DESC MyView3; +-------------------------------+------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +-------------------------------+------------------+-------+-----+--------+-----------+ | ltz | TIMESTAMP_LTZ(3) | true | | | | | CAST(ltz AS TIMESTAMP(3)) | TIMESTAMP(3) | true | | | | | CAST(ltz AS STRING) | STRING | true | | | | | ntz | TIMESTAMP(3) | false | | | | | CAST(ntz AS TIMESTAMP_LTZ(3)) | TIMESTAMP_LTZ(3) | false | | | | +-------------------------------+------------------+-------+-----+--------+-----------+ Flink SQL\u0026gt; SELECT * FROM MyView3; +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ | ltz | CAST(ltz AS TIMESTAMP(3)) | CAST(ltz AS STRING) | ntz | CAST(ntz AS TIMESTAMP_LTZ(3)) | +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ | 1970-01-01 08:00:04.001 | 1970-01-01 08:00:04.001 | 1970-01-01 08:00:04.001 | 1970-01-01 00:00:01.001 | 1970-01-01 00:00:01.001 | +-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+ 时间属性和时区 # 更多时间属性相关的详细介绍， 请参考 Time Attribute 。
处理时间和时区 # Flink SQL 使用函数 PROCTIME() 来定义处理时间属性， 该函数返回的类型是 TIMESTAMP_LTZ 。
在 Flink1.13 之前， PROCTIME() 函数返回的类型是 TIMESTAMP ， 返回值是UTC时区下的 TIMESTAMP 。 例如： 当上海的时间为 2021-03-01 12:00:00 时， PROCTIME() 显示的时间却是错误的 2021-03-01 04:00:00 。 这个问题在 Flink 1.13 中修复了， 因此用户不用再去处理时区的问题了。 PROCTIME() 返回的是本地时区的时间， 使用 TIMESTAMP_LTZ 类型也可以支持夏令时时间。
Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT PROCTIME(); +-------------------------+ | PROCTIME() | +-------------------------+ | 2021-04-15 14:48:31.387 | +-------------------------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT PROCTIME(); +-------------------------+ | PROCTIME() | +-------------------------+ | 2021-04-15 22:48:31.387 | +-------------------------+ Flink SQL\u0026gt; CREATE TABLE MyTable1 ( item STRING, price DOUBLE, proctime as PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView3 AS SELECT TUMBLE_START(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_PROCTIME(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_proctime, item, MAX(price) as max_price FROM MyTable1 GROUP BY TUMBLE(proctime, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView3; +-----------------+-----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +-----------------+-----------------------------+-------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | false | | | | | window_end | TIMESTAMP(3) | false | | | | | window_proctime | TIMESTAMP_LTZ(3) *PROCTIME* | false | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +-----------------+-----------------------------+-------+-----+--------+-----------+ 在终端执行以下命令写入数据到 MyTable1 ：
\u0026gt; nc -lk 9999 A,1.1 B,1.2 A,1.8 B,2.5 C,3.8 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView3; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_procime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.005 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.007 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:10:00.007 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView3; 相比在 UTC 时区下的计算结果， 在 Asia/Shanghai 时区下计算的窗口开始时间， 窗口结束时间和窗口处理时间是不同的。
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_procime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.005 | A | 1.8 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.007 | B | 2.5 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:10:00.007 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ 处理时间窗口是不确定的， 每次运行都会返回不同的窗口和聚合结果。 以上的示例只用于说明时区如何影响处理时间窗口。 事件时间和时区 # Flink 支持在 TIMESTAMP 列和 TIMESTAMP_LTZ 列上定义时间属性。
TIMESTAMP 上的事件时间属性 # 如果 source 中的时间用于表示年-月-日-小时-分钟-秒， 通常是一个不带时区的字符串， 例如: 2020-04-15 20:13:40.564。 推荐在 TIMESTAMP 列上定义事件时间属性。
Flink SQL\u0026gt; CREATE TABLE MyTable2 ( item STRING, price DOUBLE, ts TIMESTAMP(3), -- TIMESTAMP data type WATERMARK FOR ts AS ts - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView4 AS SELECT TUMBLE_START(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_ROWTIME(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_rowtime, item, MAX(price) as max_price FROM MyTable2 GROUP BY TUMBLE(ts, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView4; +----------------+------------------------+------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +----------------+------------------------+------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | true | | | | | window_end | TIMESTAMP(3) | true | | | | | window_rowtime | TIMESTAMP(3) *ROWTIME* | true | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +----------------+------------------------+------+-----+--------+-----------+ 在终端执行以下命令用于写入数据到 MyTable2 ：
\u0026gt; nc -lk 9999 A,1.1,2021-04-15 14:01:00 B,1.2,2021-04-15 14:02:00 A,1.8,2021-04-15 14:03:00 B,2.5,2021-04-15 14:04:00 C,3.8,2021-04-15 14:05:00 C,3.8,2021-04-15 14:11:00 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView4; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView4; 相比在 UTC 时区下的计算结果， 在 Asia/Shanghai 时区下计算的窗口开始时间， 窗口结束时间和窗口的 rowtime 是相同的。
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ TIMESTAMP_LTZ 上的事件时间属性 # 如果源数据中的时间为一个 epoch 时间， 通常是一个 long 值， 例如: 1618989564564 ，推荐将事件时间属性定义在 TIMESTAMP_LTZ 列上。
Flink SQL\u0026gt; CREATE TABLE MyTable3 ( item STRING, price DOUBLE, ts BIGINT, -- long time value in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL \u0026#39;10\u0026#39; SECOND ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;127.0.0.1\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); Flink SQL\u0026gt; CREATE VIEW MyView5 AS SELECT TUMBLE_START(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_start, TUMBLE_END(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) AS window_end, TUMBLE_ROWTIME(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES) as window_rowtime, item, MAX(price) as max_price FROM MyTable3 GROUP BY TUMBLE(ts_ltz, INTERVAL \u0026#39;10\u0026#39; MINUTES), item; Flink SQL\u0026gt; DESC MyView5; +----------------+----------------------------+-------+-----+--------+-----------+ | name | type | null | key | extras | watermark | +----------------+----------------------------+-------+-----+--------+-----------+ | window_start | TIMESTAMP(3) | false | | | | | window_end | TIMESTAMP(3) | false | | | | | window_rowtime | TIMESTAMP_LTZ(3) *ROWTIME* | true | | | | | item | STRING | true | | | | | max_price | DOUBLE | true | | | | +----------------+----------------------------+-------+-----+--------+-----------+ MyTable3 的输入数据为：
A,1.1,1618495260000 # The corresponding utc timestamp is 2021-04-15 14:01:00 B,1.2,1618495320000 # The corresponding utc timestamp is 2021-04-15 14:02:00 A,1.8,1618495380000 # The corresponding utc timestamp is 2021-04-15 14:03:00 B,2.5,1618495440000 # The corresponding utc timestamp is 2021-04-15 14:04:00 C,3.8,1618495500000 # The corresponding utc timestamp is 2021-04-15 14:05:00 C,3.8,1618495860000 # The corresponding utc timestamp is 2021-04-15 14:11:00 Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView5; +-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | A | 1.8 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | B | 2.5 | | 2021-04-15 14:00:00.000 | 2021-04-15 14:10:00.000 | 2021-04-15 14:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ Flink SQL\u0026gt; SET \u0026#39;table.local-time-zone\u0026#39; = \u0026#39;Asia/Shanghai\u0026#39;; Flink SQL\u0026gt; SELECT * FROM MyView5; 相比在 UTC 时区下的计算结果， 在 Asia/Shanghai 时区下计算的窗口开始时间， 窗口结束时间和窗口的 rowtime 是不同的。
+-------------------------+-------------------------+-------------------------+------+-----------+ | window_start | window_end | window_rowtime | item | max_price | +-------------------------+-------------------------+-------------------------+------+-----------+ | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | A | 1.8 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | B | 2.5 | | 2021-04-15 22:00:00.000 | 2021-04-15 22:10:00.000 | 2021-04-15 22:09:59.999 | C | 3.8 | +-------------------------+-------------------------+-------------------------+------+-----------+ 夏令时支持 # Flink SQL支持在 TIMESTAMP_LTZ列上定义时间属性， 基于这一特征，Flink SQL 在窗口中使用 TIMESTAMP 和 TIMESTAMP_LTZ 类型优雅地支持了夏令时。
Flink 使用时间戳的字符格式来分割窗口并通过每条记录对应的 epoch 时间来分配窗口。 这意味着 Flink 窗口开始时间和窗口结束时间使用的是 TIMESTAMP 类型（例如: TUMBLE_START 和 TUMBLE_END）， 窗口的时间属性使用的是 TIMESTAMP_LTZ 类型（例如: TUMBLE_PROCTIME， TUMBLE_ROWTIME）。 给定一个 tumble window示例， 在 Los_Angeles 时区下夏令时从 2021-03-14 02:00:00 开始：
long epoch1 = 1615708800000L; // 2021-03-14 00:00:00 long epoch2 = 1615712400000L; // 2021-03-14 01:00:00 long epoch3 = 1615716000000L; // 2021-03-14 03:00:00, 手表往前拨一小时，跳过 (2021-03-14 02:00:00) long epoch4 = 1615719600000L; // 2021-03-14 04:00:00 在 Los_angele 时区下， tumble window [2021-03-14 00:00:00, 2021-03-14 00:04:00] 将会收集3个小时的数据， 在其他非夏令时的时区下将会收集4个小时的数据，用户只需要在 TIMESTAMP_LTZ 列上声明时间属性即可。
Flink 的所有窗口（如 Hop window， Session window， Cumulative window）都会遵循这种方式， Flink SQL 中的所有操作都很好地支持了 TIMESTAMP_LTZ 类型，因此Flink可以非常优雅的支持夏令时。 Batch 模式和 Streaming 模式的区别 # 以下函数：
LOCALTIME LOCALTIMESTAMP CURRENT_DATE CURRENT_TIME CURRENT_TIMESTAMP NOW() Flink 会根据执行模式来进行不同计算，在 Streaming 模式下这些函数是每条记录都会计算一次，但在 Batch 模式下，只会在 query 开始时计算一次，所有记录都使用相同的结果。
以下时间函数无论是在 Streaming 模式还是 Batch 模式下，都会为每条记录计算一次结果：
CURRENT_ROW_TIMESTAMP() PROCTIME() Back to top
`}),e.add({id:265,href:"/flink/flink-docs-master/zh/docs/dev/python/table/",title:"Table API",section:"Python API",content:" "}),e.add({id:266,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/data_types/",title:"Data Types",section:"DataStream API",content:` Data Types # In Apache Flink\u0026rsquo;s Python DataStream API, a data type describes the type of a value in the DataStream ecosystem. It can be used to declare input and output types of operations and informs the system how to serailize elements.
Pickle Serialization # If the type has not been declared, data would be serialized or deserialized using Pickle. For example, the program below specifies no data types.
from pyflink.datastream import StreamExecutionEnvironment def processing(): env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(1) env.from_collection(collection=[(1, \u0026#39;aaa\u0026#39;), (2, \u0026#39;bbb\u0026#39;)]) \\ .map(lambda record: (record[0]+1, record[1].upper())) \\ .print() # note: print to stdout on the worker machine env.execute() if __name__ == \u0026#39;__main__\u0026#39;: processing() However, types need to be specified when:
Passing Python records to Java operations. Improve serialization and deserialization performance. Passing Python records to Java operations # Since Java operators or functions can not identify Python data, types need to be provided to help to convert Python types to Java types for processing. For example, types need to be provided if you want to output data using the FileSink which is implemented in Java.
from pyflink.common.serialization import Encoder from pyflink.common.typeinfo import Types from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.connectors.file_system import FileSink def file_sink(): env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(1) env.from_collection(collection=[(1, \u0026#39;aaa\u0026#39;), (2, \u0026#39;bbb\u0026#39;)]) \\ .map(lambda record: (record[0] + 1, record[1].upper()), output_type=Types.ROW([Types.INT(), Types.STRING()])) \\ .add_sink(FileSink .for_row_format(\u0026#39;/tmp/output\u0026#39;, Encoder.simple_string_encoder()) .build()) env.execute() if __name__ == \u0026#39;__main__\u0026#39;: file_sink() Improve serialization and deserialization performance # Even though data can be serialized and deserialized through Pickle, performance will be better if types are provided. Explicit types allow PyFlink to use efficient serializers when moving records through the pipeline.
Supported Data Types # You can use pyflink.common.typeinfo.Types to define types in Python DataStream API. The table below shows the types supported now and how to define them:
PyFlink Type Python Type Java Type Types.BOOLEAN() bool java.lang.Boolean Types.BYTE() int java.lang.Byte Types.SHORT() int java.lang.Short Types.INT() int java.lang.Integer Types.LONG() int java.lang.Long Types.FLOAT() float java.lang.Float Types.DOUBLE() float java.lang.Double Types.CHAR() str java.lang.Character Types.STRING() str java.lang.String Types.BIG_INT() int java.math.BigInteger Types.BIG_DEC() decimal.Decimal java.math.BigDecimal Types.INSTANT() pyflink.common.time.Instant java.time.Instant Types.TUPLE() tuple org.apache.flink.api.java.tuple.Tuple0 ~ org.apache.flink.api.java.tuple.Tuple25 Types.ROW() pyflink.common.Row org.apache.flink.types.Row Types.ROW_NAMED() pyflink.common.Row org.apache.flink.types.Row Types.MAP() dict java.util.Map Types.PICKLED_BYTE_ARRAY() The actual unpickled Python object byte[] Types.SQL_DATE() datetime.date java.sql.Date Types.SQL_TIME() datetime.time java.sql.Time Types.SQL_TIMESTAMP() datetime.datetime java.sql.Timestamp Types.LIST() list of Python object java.util.List The table below shows the array types supported:
PyFlink Array Type Python Type Java Type Types.PRIMITIVE_ARRAY(Types.BYTE()) bytes byte[] Types.PRIMITIVE_ARRAY(Types.BOOLEAN()) list of bool boolean[] Types.PRIMITIVE_ARRAY(Types.SHORT()) list of int short[] Types.PRIMITIVE_ARRAY(Types.INT()) list of int int[] Types.PRIMITIVE_ARRAY(Types.LONG()) list of int long[] Types.PRIMITIVE_ARRAY(Types.FLOAT()) list of float float[] Types.PRIMITIVE_ARRAY(Types.DOUBLE()) list of float double[] Types.PRIMITIVE_ARRAY(Types.CHAR()) list of str char[] Types.BASIC_ARRAY(Types.BYTE()) list of int java.lang.Byte[] Types.BASIC_ARRAY(Types.BOOLEAN()) list of bool java.lang.Boolean[] Types.BASIC_ARRAY(Types.SHORT()) list of int java.lang.Short[] Types.BASIC_ARRAY(Types.INT()) list of int java.lang.Integer[] Types.BASIC_ARRAY(Types.LONG()) list of int java.lang.Long[] Types.BASIC_ARRAY(Types.FLOAT()) list of float java.lang.Float[] Types.BASIC_ARRAY(Types.DOUBLE()) list of float java.lang.Double[] Types.BASIC_ARRAY(Types.CHAR()) list of str java.lang.Character[] Types.BASIC_ARRAY(Types.STRING()) list of str java.lang.String[] Types.OBJECT_ARRAY() list of Python object Array `}),e.add({id:267,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/",title:"DataStream API",section:"Python API",content:" "}),e.add({id:268,href:"/flink/flink-docs-master/zh/docs/dev/python/table/intro_to_table_api/",title:"Python Table API 简介",section:"Table API",content:` Python Table API 简介 # 本文档是对 PyFlink Table API 的简要介绍，用于帮助新手用户快速理解 PyFlink Table API 的基本用法。 关于高级用法，请参阅用户指南中的其他文档。
Python Table API 程序的基本结构 # 所有的 Table API 和 SQL 程序，不管批模式，还是流模式，都遵循相同的结构。下面代码示例展示了 Table API 和 SQL 程序的基本结构。
from pyflink.table import EnvironmentSettings, TableEnvironment # 1. 创建 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # 2. 创建 source 表 table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE datagen ( id INT, data STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39; = \u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39; = \u0026#39;10\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 3. 创建 sink 表 table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print ( id INT, data STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 4. 查询 source 表，同时执行计算 # 通过 Table API 创建一张表： source_table = table_env.from_path(\u0026#34;datagen\u0026#34;) # 或者通过 SQL 查询语句创建一张表： # source_table = table_env.sql_query(\u0026#34;SELECT * FROM datagen\u0026#34;) result_table = source_table.select(source_table.id + 1, source_table.data) # 5. 将计算结果写入给 sink 表 # 将 Table API 结果表数据写入 sink 表： result_table.execute_insert(\u0026#34;print\u0026#34;).wait() # 或者通过 SQL 查询语句来写入 sink 表： # table_env.execute_sql(\u0026#34;INSERT INTO print SELECT * FROM datagen\u0026#34;).wait() Back to top
创建 TableEnvironment # TableEnvironment 是 Table API 和 SQL 集成的核心概念。下面代码示例展示了如何创建一个 TableEnvironment:
from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # or create a batch TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) 关于创建 TableEnvironment 的更多细节，请查阅 TableEnvironment 文档。
TableEnvironment 可以用来:
Table 管理：创建表、列举表、Table 和 DataStream 互转等。 自定义函数管理：自定义函数的注册、删除、列举等。 关于 Python 自定义函数的更多细节，请参考普通自定义函数 和向量化自定义函数章节的介绍。 执行 SQL 语句：更多细节可查阅SQL 查询章节的介绍。 作业配置管理：更多细节可查阅Python 配置章节的介绍。 Python 依赖管理：更多细节可查阅依赖管理章节的介绍。 作业提交：更多细节可查阅作业提交章节的介绍。 Back to top
创建表 # Table 是 Python Table API 的核心组件。Table 对象由一系列数据转换操作构成，但是它不包含数据本身。 相反，它描述了如何从数据源中读取数据，以及如何将最终结果写出到外部存储等。表可以被打印、优化并最终在集群中执行。 表也可以是有限流或无限流，以支持流式处理和批处理场景。
一个 Table 实例总是与一个特定的 TableEnvironment 相绑定。不支持在同一个查询中合并来自不同 TableEnvironments 的表，例如 join 或者 union 它们。
通过列表类型的对象创建 # 你可以使用一个列表对象创建一张表：
from pyflink.table import EnvironmentSettings, TableEnvironment # 创建 批 TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) table.execute().print() 结果为：
+----------------------+--------------------------------+ | _1 | _2 | +----------------------+--------------------------------+ | 1 | Hi | | 2 | Hello | +----------------------+--------------------------------+ 你也可以创建具有指定列名的表：
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table.execute().print() 结果为：
+----------------------+--------------------------------+ | id | data | +----------------------+--------------------------------+ | 1 | Hi | | 2 | Hello | +----------------------+--------------------------------+ 默认情况下，表结构是从数据中自动提取的。 如果自动生成的表模式不符合你的预期，你也可以手动指定：
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) # 默认情况下，“id” 列的类型是 64 位整型 print(\u0026#39;By default the type of the \u0026#34;id\u0026#34; column is %s.\u0026#39; % table.get_schema().get_field_data_type(\u0026#34;id\u0026#34;)) from pyflink.table import DataTypes table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.TINYINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) # 现在 “id” 列的类型是 8 位整型 print(\u0026#39;Now the type of the \u0026#34;id\u0026#34; column is %s.\u0026#39; % table.get_schema().get_field_data_type(\u0026#34;id\u0026#34;)) 结果为：
By default the type of the \u0026#34;id\u0026#34; column is BIGINT. Now the type of the \u0026#34;id\u0026#34; column is TINYINT. 通过 DDL 创建 # 你可以通过 DDL 语句创建表，它代表一张从指定的外部存储读取数据的表：
from pyflink.table import EnvironmentSettings, TableEnvironment # 创建 流 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE random_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;3\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;6\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = table_env.from_path(\u0026#34;random_source\u0026#34;) table.execute().print() 结果为：
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | +----+----------------------+--------+ 通过 TableDescriptor 创建 # 你也可以通过 TableDescriptor 来创建表. 这种方式等价于通过 SQL DDL 语句的方式.
from pyflink.table import EnvironmentSettings, TableEnvironment, TableDescriptor, Schema, DataTypes # create a stream TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.create_temporary_table( \u0026#39;random_source\u0026#39;, TableDescriptor.for_connector(\u0026#39;datagen\u0026#39;) .schema(Schema.new_builder() .column(\u0026#39;id\u0026#39;, DataTypes.BIGINT()) .column(\u0026#39;data\u0026#39;, DataTypes.TINYINT()) .build()) .option(\u0026#39;fields.id.kind\u0026#39;, \u0026#39;sequence\u0026#39;) .option(\u0026#39;fields.id.start\u0026#39;, \u0026#39;1\u0026#39;) .option(\u0026#39;fields.id.end\u0026#39;, \u0026#39;3\u0026#39;) .option(\u0026#39;fields.data.kind\u0026#39;, \u0026#39;sequence\u0026#39;) .option(\u0026#39;fields.data.start\u0026#39;, \u0026#39;4\u0026#39;) .option(\u0026#39;fields.data.end\u0026#39;, \u0026#39;6\u0026#39;) .build()) table = table_env.from_path(\u0026#34;random_source\u0026#34;) table.execute().print() The results are as following:
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | +----+----------------------+--------+ 通过 Catalog 创建 # TableEnvironment 维护了一个使用标识符创建的表的 catalogs 映射。
Catalog 中的表既可以是临时的，并与单个 Flink 会话生命周期相关联，也可以是永久的，跨多个 Flink 会话可见。
通过 SQL DDL 创建的表和视图， 例如 \u0026ldquo;create table \u0026hellip;\u0026rdquo; 和 \u0026ldquo;create view \u0026hellip;\u0026quot;，都存储在 catalog 中。
你可以通过 SQL 直接访问 catalog 中的表。
如果你要用 Table API 来使用 catalog 中的表，可以使用 \u0026ldquo;from_path\u0026rdquo; 方法来创建 Table API 对象：
# 准备 catalog # 将 Table API 表注册到 catalog 中 table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#39;source_table\u0026#39;, table) # 从 catalog 中获取 Table API 表 new_table = table_env.from_path(\u0026#39;source_table\u0026#39;) new_table.execute().print() 结果为：
+----+----------------------+--------------------------------+ | op | id | data | +----+----------------------+--------------------------------+ | +I | 1 | Hi | | +I | 2 | Hello | +----+----------------------+--------------------------------+ Back to top
查询 # Table API 查询 # Table 对象有许多方法，可以用于进行关系操作。 这些方法返回新的 Table 对象，表示对输入 Table 应用关系操作之后的结果。 这些关系操作可以由多个方法调用组成，例如 table.group_by(...).select(...)。
Table API 文档描述了流和批处理上所有支持的 Table API 操作。
以下示例展示了一个简单的 Table API 聚合查询：
from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col # 通过 batch table environment 来执行查询 env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) orders = table_env.from_elements([(\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 10), (\u0026#39;Rose\u0026#39;, \u0026#39;ENGLAND\u0026#39;, 30), (\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 20)], [\u0026#39;name\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;revenue\u0026#39;]) # 计算所有来自法国客户的收入 revenue = orders \\ .select(col(\u0026#34;name\u0026#34;), col(\u0026#34;country\u0026#34;), col(\u0026#34;revenue\u0026#34;)) \\ .where(col(\u0026#34;country\u0026#34;) == \u0026#39;FRANCE\u0026#39;) \\ .group_by(col(\u0026#34;name\u0026#34;)) \\ .select(col(\u0026#34;name\u0026#34;), col(\u0026#34;country\u0026#34;).sum.alias(\u0026#39;rev_sum\u0026#39;)) revenue.execute().print() 结果为：
+--------------------------------+----------------------+ | name | rev_sum | +--------------------------------+----------------------+ | Jack | 30 | +--------------------------------+----------------------+ Table API 也支持 行操作的 API, 这些行操作包括 Map Operation, FlatMap Operation, Aggregate Operation 和 FlatAggregate Operation.
以下示例展示了一个简单的 Table API 基于行操作的查询
from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table import DataTypes from pyflink.table.udf import udf import pandas as pd # 通过 batch table environment 来执行查询 env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) orders = table_env.from_elements([(\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 10), (\u0026#39;Rose\u0026#39;, \u0026#39;ENGLAND\u0026#39;, 30), (\u0026#39;Jack\u0026#39;, \u0026#39;FRANCE\u0026#39;, 20)], [\u0026#39;name\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;revenue\u0026#39;]) map_function = udf(lambda x: pd.concat([x.name, x.revenue * 10], axis=1), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()), DataTypes.FIELD(\u0026#34;revenue\u0026#34;, DataTypes.BIGINT())]), func_type=\u0026#34;pandas\u0026#34;) orders.map(map_function).execute().print() 结果为：
+--------------------------------+----------------------+ | name | revenue | +--------------------------------+----------------------+ | Jack | 100 | | Rose | 300 | | Jack | 200 | +--------------------------------+----------------------+ SQL 查询 # Flink 的 SQL 基于 Apache Calcite，它实现了标准的 SQL。SQL 查询语句使用字符串来表达。
SQL 文档描述了 Flink 对流和批处理所支持的 SQL。
下面示例展示了一个简单的 SQL 聚合查询：
from pyflink.table import EnvironmentSettings, TableEnvironment # 通过 stream table environment 来执行查询 env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE random_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;8\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;11\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print_sink ( id BIGINT, data_sum TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; INSERT INTO print_sink SELECT id, sum(data) as data_sum FROM (SELECT id / 2 as id, data FROM random_source) WHERE id \u0026gt; 1 GROUP BY id \u0026#34;\u0026#34;\u0026#34;).wait() 结果为：
2\u0026gt; +I(4,11) 6\u0026gt; +I(2,8) 8\u0026gt; +I(3,10) 6\u0026gt; -U(2,8) 8\u0026gt; -U(3,10) 6\u0026gt; +U(2,15) 8\u0026gt; +U(3,19) 实际上，上述输出展示了 print 结果表所接收到的 change log。 change log 的格式为:
{subtask id}\u0026gt; {消息类型}{值的字符串格式} 例如，\u0026ldquo;2\u0026gt; +I(4,11)\u0026rdquo; 表示这条消息来自第二个 subtask，其中 \u0026ldquo;+I\u0026rdquo; 表示这是一条插入的消息，\u0026quot;(4, 11)\u0026rdquo; 是这条消息的内容。 另外，\u0026quot;-U\u0026quot; 表示这是一条撤回消息 (即更新前)，这意味着应该在 sink 中删除或撤回该消息。 \u0026ldquo;+U\u0026rdquo; 表示这是一条更新的记录 (即更新后)，这意味着应该在 sink 中更新或插入该消息。
所以，从上面的 change log，我们可以得到如下结果：
(4, 11) (2, 15) (3, 19) Table API 和 SQL 的混合使用 # Table API 中的 Table 对象和 SQL 中的 Table 可以自由地相互转换。
下面例子展示了如何在 SQL 中使用 Table 对象：
# 创建一张 sink 表来接收结果数据 table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE table_sink ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 将 Table API 表转换成 SQL 中的视图 table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#39;table_api_table\u0026#39;, table) # 将 Table API 表的数据写入结果表 table_env.execute_sql(\u0026#34;INSERT INTO table_sink SELECT * FROM table_api_table\u0026#34;).wait() 结果为：
6\u0026gt; +I(1,Hi) 6\u0026gt; +I(2,Hello) 下面例子展示了如何在 Table API 中使用 SQL 表：
# 创建一张 SQL source 表 table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE sql_source ( id BIGINT, data TINYINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.id.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.id.start\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;fields.id.end\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.kind\u0026#39;=\u0026#39;sequence\u0026#39;, \u0026#39;fields.data.start\u0026#39;=\u0026#39;4\u0026#39;, \u0026#39;fields.data.end\u0026#39;=\u0026#39;7\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 将 SQL 表转换成 Table API 表 table = table_env.from_path(\u0026#34;sql_source\u0026#34;) # 或者通过 SQL 查询语句创建表 # table = table_env.sql_query(\u0026#34;SELECT * FROM sql_source\u0026#34;) # 将表中的数据写出 table.execute().print() 结果为：
+----+----------------------+--------+ | op | id | data | +----+----------------------+--------+ | +I | 1 | 4 | | +I | 2 | 5 | | +I | 3 | 6 | | +I | 4 | 7 | +----+----------------------+--------+ Back to top
将结果写出 # 打印结果 # 你可以通过 TableResult.print 方法，将表的结果打印到标准输出中。该方法通常用于预览表的中间结果。
# prepare source tables source = table_env.from_elements([(1, \u0026#34;Hi\u0026#34;, \u0026#34;Hello\u0026#34;), (2, \u0026#34;Hello\u0026#34;, \u0026#34;Hello\u0026#34;)], [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]) # Get TableResult table_result = table_env.execute_sql(\u0026#34;select a + 1, b, c from %s\u0026#34; % source) # Print the table table_result.print() 结果为：
+----+----------------------+--------------------------------+--------------------------------+ | op | EXPR\$0 | b | c | +----+----------------------+--------------------------------+--------------------------------+ | +I | 2 | Hi | Hello | | +I | 3 | Hello | Hello | +----+----------------------+--------------------------------+--------------------------------+ Note 该方式会触发表的物化，同时将表的内容收集到客户端内存中，所以通过 Table.limit 来限制收集数据的条数是一种很好的做法。
将结果数据收集到客户端 # 你可以使用 TableResult.collect 将 Table 的结果收集到客户端，结果的类型为迭代器类型。
以下代码展示了如何使用 TableResult.collect() 方法：
# 准备 source 表 source = table_env.from_elements([(1, \u0026#34;Hi\u0026#34;, \u0026#34;Hello\u0026#34;), (2, \u0026#34;Hello\u0026#34;, \u0026#34;Hello\u0026#34;)], [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;]) # 得到 TableResult table_result = table_env.execute_sql(\u0026#34;select a + 1, b, c from %s\u0026#34; % source) # 遍历结果 with table_result.collect() as results: for result in results: print(result) 结果为：
\u0026lt;Row(2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)\u0026gt; \u0026lt;Row(3, \u0026#39;Hello\u0026#39;, \u0026#39;Hello\u0026#39;)\u0026gt; Note 该方式会触发表的物化，同时将表的内容收集到客户端内存中，所以通过 Table.limit 来限制收集数据的条数是一种很好的做法。
将结果数据转换为Pandas DataFrame，并收集到客户端 # 你可以调用 \u0026ldquo;to_pandas\u0026rdquo; 方法来 将一个 Table 对象转化成 pandas DataFrame:
table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) print(table.to_pandas()) 结果为：
id data 0 1 Hi 1 2 Hello Note 该方式会触发表的物化，同时将表的内容收集到客户端内存中，所以通过 Table.limit 来限制收集数据的条数是一种很好的做法。
Note 并不是所有的数据类型都可以转换为 pandas DataFrames。
将结果写入到一张 Sink 表中 # 你可以调用 \u0026ldquo;execute_insert\u0026rdquo; 方法来将 Table 对象中的数据写入到一张 sink 表中：
table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table.execute_insert(\u0026#34;sink_table\u0026#34;).wait() 结果为：
6\u0026gt; +I(1,Hi) 6\u0026gt; +I(2,Hello) 也可以通过 SQL 来完成:
table_env.create_temporary_view(\u0026#34;table_source\u0026#34;, table) table_env.execute_sql(\u0026#34;INSERT INTO sink_table SELECT * FROM table_source\u0026#34;).wait() 将结果写入多张 Sink 表中 # 你也可以使用 StatementSet 在一个作业中将 Table 中的数据写入到多张 sink 表中：
# 准备 source 表和 sink 表 table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.create_temporary_view(\u0026#34;simple_source\u0026#34;, table) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE first_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE second_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) # 创建 statement set statement_set = table_env.create_statement_set() # 将 \u0026#34;table\u0026#34; 的数据写入 \u0026#34;first_sink_table\u0026#34; statement_set.add_insert(\u0026#34;first_sink_table\u0026#34;, table) # 通过一条 sql 插入语句将数据从 \u0026#34;simple_source\u0026#34; 写入到 \u0026#34;second_sink_table\u0026#34; statement_set.add_insert_sql(\u0026#34;INSERT INTO second_sink_table SELECT * FROM simple_source\u0026#34;) # 执行 statement set statement_set.execute().wait() 结果为：
7\u0026gt; +I(1,Hi) 7\u0026gt; +I(1,Hi) 7\u0026gt; +I(2,Hello) 7\u0026gt; +I(2,Hello) Explain 表 # Table API 提供了一种机制来查看 Table 的逻辑查询计划和优化后的查询计划。 这是通过 Table.explain() 或者 StatementSet.explain() 方法来完成的。Table.explain() 可以返回一个 Table 的执行计划。StatementSet.explain() 则可以返回含有多个 sink 的作业的执行计划。这些方法会返回一个字符串，字符串描述了以下三个方面的信息：
关系查询的抽象语法树，即未经优化的逻辑查询计划， 优化后的逻辑查询计划， 物理执行计划。 TableEnvironment.explain_sql() 和 TableEnvironment.execute_sql() 支持执行 EXPLAIN 语句获得执行计划。更多细节请查阅 EXPLAIN。
以下代码展示了如何使用 Table.explain() 方法：
# 使用流模式 TableEnvironment from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table1 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table2 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table = table1 \\ .where(col(\u0026#34;data\u0026#34;).like(\u0026#39;H%\u0026#39;)) \\ .union_all(table2) print(table.explain()) 结果为：
== 抽象语法树 == LogicalUnion(all=[true]) :- LogicalFilter(condition=[LIKE(\$1, _UTF-16LE\u0026#39;H%\u0026#39;)]) : +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]]]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]]]) == 优化后的逻辑计划 == Union(all=[true], union=[id, data]) :- Calc(select=[id, data], where=[LIKE(data, _UTF-16LE\u0026#39;H%\u0026#39;)]) : +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) == 物理执行计划 == Stage 133 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 134 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_201907291, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 135 : Operator content : Calc(select=[id, data], where=[(data LIKE _UTF-16LE\u0026#39;H%\u0026#39;)]) ship_strategy : FORWARD Stage 136 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 137 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_1709623525, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD 以下代码展示了如何使用 StatementSet.explain() 方法：
# 使用流模式 TableEnvironment from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(environment_settings=env_settings) table1 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table2 = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE print_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;print\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) table_env.execute_sql(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE black_hole_sink_table ( id BIGINT, data VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;blackhole\u0026#39; ) \u0026#34;\u0026#34;\u0026#34;) statement_set = table_env.create_statement_set() statement_set.add_insert(\u0026#34;print_sink_table\u0026#34;, table1.where(col(\u0026#34;data\u0026#34;).like(\u0026#39;H%\u0026#39;))) statement_set.add_insert(\u0026#34;black_hole_sink_table\u0026#34;, table2) print(statement_set.explain()) 结果为
== 抽象语法树 == LogicalSink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) +- LogicalFilter(condition=[LIKE(\$1, _UTF-16LE\u0026#39;H%\u0026#39;)]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]]]) LogicalSink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]]]) == 优化后的逻辑计划 == Sink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) +- Calc(select=[id, data], where=[LIKE(data, _UTF-16LE\u0026#39;H%\u0026#39;)]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) Sink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data]) == 物理执行计划 == Stage 139 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 140 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_541737614, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 141 : Operator content : Calc(select=[id, data], where=[(data LIKE _UTF-16LE\u0026#39;H%\u0026#39;)]) ship_strategy : FORWARD Stage 143 : Data Source content : Source: PythonInputFormatTableSource(id, data) Stage 144 : Operator content : SourceConversion(table=[default_catalog.default_database.Unregistered_TableSource_1437429083, source: [PythonInputFormatTableSource(id, data)]], fields=[id, data]) ship_strategy : FORWARD Stage 142 : Data Sink content : Sink: Sink(table=[default_catalog.default_database.print_sink_table], fields=[id, data]) ship_strategy : FORWARD Stage 145 : Data Sink content : Sink: Sink(table=[default_catalog.default_database.black_hole_sink_table], fields=[id, data]) ship_strategy : FORWARD `}),e.add({id:269,href:"/flink/flink-docs-master/zh/docs/dev/python/table/table_environment/",title:"TableEnvironment",section:"Table API",content:' TableEnvironment # 本篇文档是对 PyFlink TableEnvironment 的介绍。 文档包括对 TableEnvironment 类中每个公共接口的详细描述。\n创建 TableEnvironment # 创建 TableEnvironment 的推荐方式是通过 EnvironmentSettings 对象创建:\nfrom pyflink.common import Configuration from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment config = Configuration() config.set_string(\u0026#39;execution.buffer-timeout\u0026#39;, \u0026#39;1 min\u0026#39;) env_settings = EnvironmentSettings \\ .new_instance() \\ .in_streaming_mode() \\ .with_configuration(config) \\ .build() table_env = TableEnvironment.create(env_settings) 或者，用户可以从现有的 StreamExecutionEnvironment 创建 StreamTableEnvironment，以与 DataStream API 进行互操作。\nfrom pyflink.datastream import StreamExecutionEnvironment from pyflink.table import StreamTableEnvironment # create a streaming TableEnvironment from a StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment() table_env = StreamTableEnvironment.create(env) TableEnvironment API # Table/SQL 操作 # 这些 APIs 用来创建或者删除 Table API/SQL 表和写查询：\nAPIs 描述 文档 from_elements(elements, schema=None, verify_schema=True) 通过元素集合来创建表。 链接 from_pandas(pdf, schema=None, split_num=1) 通过 pandas DataFrame 来创建表。 链接 from_path(path) 通过指定路径下已注册的表来创建一个表，例如通过 create_temporary_view 注册表。 链接 create_temporary_view(view_path, table) 将一个 `Table` 对象注册为一张临时表，类似于 SQL 的临时表。 链接 drop_temporary_view(view_path) 删除指定路径下已注册的临时表。 链接 drop_temporary_table(table_path) 删除指定路径下已注册的临时表。 你可以使用这个接口来删除临时 source 表和临时 sink 表。 链接 execute_sql(stmt) 执行指定的语句并返回执行结果。 执行语句可以是 DDL/DML/DQL/SHOW/DESCRIBE/EXPLAIN/USE。 注意，对于 "INSERT INTO" 语句，这是一个异步操作，通常在向远程集群提交作业时才需要使用。 但是，如果在本地集群或者 IDE 中执行作业时，你需要等待作业执行完成，这时你可以查阅 这里 来获取更多细节。 更多关于 SQL 语句的细节，可查阅 SQL 文档。 链接 sql_query(query) 执行一条 SQL 查询，并将查询的结果作为一个 `Table` 对象。 链接 废弃的 APIs\nAPIs 描述 文档 from_table_source(table_source) 通过 table source 创建一张表。 链接 scan(*table_path) 从 catalog 中扫描已注册的表并且返回结果表。 它可以使用 from_path 来替换。 链接 register_table(name, table) 在 TableEnvironment 的 catalog 中用唯一名称注册一个 “Table” 对象。 可以在 SQL 查询中引用已注册的表。 它可以使用 create_temporary_view 替换。 链接 register_table_source(name, table_source) 在 TableEnvironment 的 catalog 中注册一个外部 `TableSource`。 链接 register_table_sink(name, table_sink) 在 TableEnvironment 的 catalog 中注册一个外部 `TableSink`。 链接 insert_into(target_path, table) 将 `Table` 对象的内容写到指定的 sink 表中。 注意，这个接口不会触发作业的执行。 你需要调用 `execute` 方法来执行你的作业。 链接 sql_update(stmt) 计算 INSERT, UPDATE 或者 DELETE 等 SQL 语句或者一个 DDL 语句。 它可以使用 execute_sql 来替换。 链接 执行/解释作业 # 这些 APIs 是用来执行/解释作业。注意，execute_sql API 也可以用于执行作业。\nAPIs 描述 文档 explain_sql(stmt, *extra_details) 返回指定语句的抽象语法树和执行计划。 链接 create_statement_set() 创建一个可接受 DML 语句或表的 StatementSet 实例。 它可用于执行包含多个 sink 的作业。 链接 废弃的 APIs\nAPIs 描述 文档 explain(table=None, extended=False) 返回指定 Table API 和 SQL 查询的抽象语法树，以及用来计算给定 `Table` 对象或者多个 sink 计划结果的执行计划。 如果你使用 "insert_into" 或者 "sql_update" 方法将数据发送到多个 sinks，你可以通过这个方法来得到执行计划。 它也可以用 TableEnvironment.explain_sql，Table.explain 或者 StatementSet.explain 来替换。 链接 execute(job_name) 触发程序执行。执行环境将执行程序的所有部分。 如果你想要使用 insert_into 或者 sql_update 方法将数据发送到结果表，你可以使用这个方法触发程序的执行。 这个方法将阻塞客户端程序，直到任务完成/取消/失败。 链接 创建/删除用户自定义函数 # 这些 APIs 用来注册 UDFs 或者 删除已注册的 UDFs。 注意，execute_sql API 也可以用于注册/删除 UDFs。 关于不同类型 UDFs 的详细信息，可查阅 用户自定义函数。\nAPIs 描述 文档 create_temporary_function(path, function) 将一个 Python 用户自定义函数注册为临时 catalog 函数。 链接 create_temporary_system_function(name, function) 将一个 Python 用户自定义函数注册为临时系统函数。 如果临时系统函数的名称与临时 catalog 函数名称相同，优先使用临时系统函数。 链接 create_java_function(path, function_class_name, ignore_if_exists=None) 将 Java 用户自定义函数注册为指定路径下的 catalog 函数。 如果 catalog 是持久化的，则可以跨多个 Flink 会话和集群使用已注册的 catalog 函数。 链接 create_java_temporary_function(path, function_class_name) 将 Java 用户自定义函数注册为临时 catalog 函数。 链接 create_java_temporary_system_function(name, function_class_name) 将 Java 用户定义的函数注册为临时系统函数。 链接 drop_function(path) 删除指定路径下已注册的 catalog 函数。 链接 drop_temporary_function(path) 删除指定名称下已注册的临时系统函数。 链接 drop_temporary_system_function(name) 删除指定名称下已注册的临时系统函数。 链接 废弃的 APIs\nAPIs 描述 文档 register_function(name, function) 注册一个 Python 用户自定义函数，并为其指定一个唯一的名称。 若已有与该名称相同的用户自定义函数，则替换之。 它可以通过 create_temporary_system_function 来替换。 链接 register_java_function(name, function_class_name) 注册一个 Java 用户自定义函数，并为其指定一个唯一的名称。 若已有与该名称相同的用户自定义函数，则替换之。 它可以通过 create_java_temporary_system_function 来替换。 链接 依赖管理 # 这些 APIs 用来管理 Python UDFs 所需要的 Python 依赖。 更多细节可查阅依赖管理。\nAPIs 描述 文档 add_python_file(file_path) 添加 Python 依赖，可以是 Python 文件，Python 包或者本地目录。 它们将会被添加到 Python UDF 工作程序的 PYTHONPATH 中。 链接 set_python_requirements(requirements_file_path, requirements_cache_dir=None) 指定一个 requirements.txt 文件，该文件定义了第三方依赖关系。 这些依赖项将安装到一个临时 catalog 中，并添加到 Python UDF 工作程序的 PYTHONPATH 中。 链接 add_python_archive(archive_path, target_dir=None) 添加 Python 归档文件。该文件将被解压到 Python UDF 程序的工作目录中。 链接 配置 # APIs 描述 文档 get_config() 返回 table config，可以通过 table config 来定义 Table API 的运行时行为。 你可以在 配置 和 Python 配置 中找到所有可用的配置选项。 下面的代码示例展示了如何通过这个 API 来设置配置选项：\n# set the parallelism to 8 table_env.get_config().set("parallelism.default", "8")\n# set the job name table_env.get_config().set("pipeline.name", "my_first_job") 链接 Catalog APIs # 这些 APIs 用于访问 catalog 和模块。你可以在 模块 和 catalog 文档中找到更详细的介绍。\nAPIs 描述 文档 register_catalog(catalog_name, catalog) 注册具有唯一名称的 `Catalog`。 链接 get_catalog(catalog_name) 通过指定的名称来获得已注册的 `Catalog` 。 链接 use_catalog(catalog_name) 将当前目录设置为所指定的 catalog。 它也将默认数据库设置为所指定 catalog 的默认数据库。 链接 get_current_catalog() 获取当前会话默认的 catalog 名称。 链接 get_current_database() 获取正在运行会话中的当前默认数据库名称。 链接 use_database(database_name) 设置当前默认的数据库。 它必须存在当前 catalog 中。 当寻找未限定的对象名称时，该路径将被用作默认路径。 链接 load_module(module_name, module) 加载给定名称的 `Module`。 模块将按照加载的顺序进行保存。 链接 unload_module(module_name) 卸载给定名称的 `Module`。 链接 use_modules(*module_names) 按指定列表激活在这个环境中加载的 `Module`。 链接 list_catalogs() 获取在这个环境中注册的所有 catalog 目录名称。 链接 list_modules() 获取在这个环境中注册的所有激活的 `Module` 名称。 链接 list_full_modules() 获取在这个环境中注册的所有加载的 `Module` 名称及激活状态。 链接 list_databases() 获取当前 catalog 中所有数据库的名称。 链接 list_tables() 获取当前 catalog 的当前数据库下的所有表和临时表的名称。 它可以返回永久和临时的表和视图。 链接 list_views() 获取当前 catalog 的当前数据库中的所有临时表名称。 它既可以返回永久的也可以返回临时的临时表。 链接 list_user_defined_functions() 获取在该环境中已注册的所有用户自定义函数的名称。 链接 list_functions() 获取该环境中所有函数的名称。 链接 list_temporary_tables() 获取当前命名空间（当前 catalog 的当前数据库）中所有可用的表和临时表名称。 链接 list_temporary_views() 获取当前命名空间（当前 catalog 的当前数据库）中所有可用的临时表名称。 链接 Statebackend，Checkpoint 以及重启策略 # 在 Flink 1.10 之前，你可以通过 StreamExecutionEnvironment 来配置 statebackend，checkpointing 以及重启策略。 现在你可以通过在 TableConfig 中，通过设置键值选项来配置它们，更多详情可查阅 容错，State Backends 以及 Checkpointing。\n下面代码示例展示了如何通过 Table API 来配置 statebackend，checkpoint 以及重启策略：\n# 设置重启策略为 \u0026#34;fixed-delay\u0026#34; table_env.get_config().set(\u0026#34;restart-strategy\u0026#34;, \u0026#34;fixed-delay\u0026#34;) table_env.get_config().set(\u0026#34;restart-strategy.fixed-delay.attempts\u0026#34;, \u0026#34;3\u0026#34;) table_env.get_config().set(\u0026#34;restart-strategy.fixed-delay.delay\u0026#34;, \u0026#34;30s\u0026#34;) # 设置 checkpoint 模式为 EXACTLY_ONCE table_env.get_config().set(\u0026#34;execution.checkpointing.mode\u0026#34;, \u0026#34;EXACTLY_ONCE\u0026#34;) table_env.get_config().set(\u0026#34;execution.checkpointing.interval\u0026#34;, \u0026#34;3min\u0026#34;) # 设置 statebackend 类型为 \u0026#34;rocksdb\u0026#34;，其他可选项有 \u0026#34;filesystem\u0026#34; 和 \u0026#34;jobmanager\u0026#34; # 你也可以将这个属性设置为 StateBackendFactory 的完整类名 # e.g. org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory table_env.get_config().set(\u0026#34;state.backend\u0026#34;, \u0026#34;rocksdb\u0026#34;) # 设置 RocksDB statebackend 所需要的 checkpoint 目录 table_env.get_config().set(\u0026#34;state.checkpoints.dir\u0026#34;, \u0026#34;file:///tmp/checkpoints/\u0026#34;) '}),e.add({id:270,href:"/flink/flink-docs-master/zh/docs/dev/python/table/operations/",title:"Operations",section:"Table API",content:" "}),e.add({id:271,href:"/flink/flink-docs-master/zh/docs/dev/python/table/operations/operations/",title:"Overview",section:"Operations",content:" "}),e.add({id:272,href:"/flink/flink-docs-master/zh/docs/dev/python/dependency_management/",title:"依赖管理",section:"Python API",content:` Dependency Management # There are requirements to use dependencies inside the Python API programs. For example, users may need to use third-party Python libraries in Python user-defined functions. In addition, in scenarios such as machine learning prediction, users may want to load a machine learning model inside the Python user-defined functions.
When the PyFlink job is executed locally, users could install the third-party Python libraries into the local Python environment, download the machine learning model to local, etc. However, this approach doesn\u0026rsquo;t work well when users want to submit the PyFlink jobs to remote clusters. In the following sections, we will introduce the options provided in PyFlink for these requirements.
Note Both Python DataStream API and Python Table API have provided APIs for each kind of dependency. If you are mixing use of Python DataStream API and Python Table API in a single job, you should specify the dependencies via Python DataStream API to make them work for both the Python DataStream API and Python Table API.
JAR Dependencies # If third-party JARs are used, you can specify the JARs in the Python Table API as following:
# Specify a list of jar URLs via \u0026#34;pipeline.jars\u0026#34;. The jars are separated by \u0026#34;;\u0026#34; # and will be uploaded to the cluster. # NOTE: Only local file URLs (start with \u0026#34;file://\u0026#34;) are supported. table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) # It looks like the following on Windows: table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///E:/my/jar/path/connector.jar;file:///E:/my/jar/path/udf.jar\u0026#34;) # Specify a list of URLs via \u0026#34;pipeline.classpaths\u0026#34;. The URLs are separated by \u0026#34;;\u0026#34; # and will be added to the classpath during job execution. # NOTE: The paths must specify a protocol (e.g. file://) and users should ensure that the URLs are accessible on both the client and the cluster. table_env.get_config().set(\u0026#34;pipeline.classpaths\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) or in the Python DataStream API as following:
# Use the add_jars() to add local jars and the jars will be uploaded to the cluster. # NOTE: Only local file URLs (start with \u0026#34;file://\u0026#34;) are supported. stream_execution_environment.add_jars(\u0026#34;file:///my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///my/jar/path/connector2.jar\u0026#34;) # It looks like the following on Windows: stream_execution_environment.add_jars(\u0026#34;file:///E:/my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///E:/my/jar/path/connector2.jar\u0026#34;) # Use the add_classpaths() to add the dependent jars URLs into the classpath. # The URLs will also be added to the classpath of both the client and the cluster. # NOTE: The paths must specify a protocol (e.g. file://) and users should ensure that the # URLs are accessible on both the client and the cluster. stream_execution_environment.add_classpaths(\u0026#34;file:///my/jar/path/connector1.jar\u0026#34;, \u0026#34;file:///my/jar/path/connector2.jar\u0026#34;) or through the command line arguments --jarfile when submitting the job.
Note It only supports to specify one jar file with the command line argument --jarfile and so you need to build a fat jar if there are multiple jar files.
Python Dependencies # Python libraries # You may want to use third-part Python libraries in Python user-defined functions. There are multiple ways to specify the Python libraries.
You could specify them inside the code using Python Table API as following:
table_env.add_python_file(file_path) or using Python DataStream API as following:
stream_execution_environment.add_python_file(file_path) You could also specify the Python libraries using configuration python.files or via command line arguments -pyfs or --pyFiles when submitting the job.
Note The Python libraries could be local files or local directories. They will be added to the PYTHONPATH of the Python UDF worker.
requirements.txt # It also allows to specify a requirements.txt file which defines the third-party Python dependencies. These Python dependencies will be installed into the working directory and added to the PYTHONPATH of the Python UDF worker.
You could prepare the requirements.txt manually as following:
echo numpy==1.16.5 \u0026gt;\u0026gt; requirements.txt echo pandas==1.0.0 \u0026gt;\u0026gt; requirements.txt or using pip freeze which lists all the packages installed in the current Python environment:
pip freeze \u0026gt; requirements.txt The content of the requirements.txt file may look like the following:
numpy==1.16.5 pandas==1.0.0 You could manually edit it by removing unnecessary entries or adding extra entries, etc.
The requirements.txt file could then be specified inside the code using Python Table API as following:
# requirements_cache_dir is optional table_env.set_python_requirements( requirements_file_path=\u0026#34;/path/to/requirements.txt\u0026#34;, requirements_cache_dir=\u0026#34;cached_dir\u0026#34;) or using Python DataStream API as following:
# requirements_cache_dir is optional stream_execution_environment.set_python_requirements( requirements_file_path=\u0026#34;/path/to/requirements.txt\u0026#34;, requirements_cache_dir=\u0026#34;cached_dir\u0026#34;) Note For the dependencies which could not be accessed in the cluster, a directory which contains the installation packages of these dependencies could be specified using the parameter requirements_cached_dir. It will be uploaded to the cluster to support offline installation. You could prepare the requirements_cache_dir as following:
pip download -d cached_dir -r requirements.txt --no-binary :all: Note Please make sure that the prepared packages match the platform of the cluster, and the Python version used.
You could also specify the requirements.txt file using configuration python.requirements or via command line arguments -pyreq or --pyRequirements when submitting the job.
Note It will install the packages specified in the requirements.txt file using pip, so please make sure that pip (version \u0026gt;= 20.3) and setuptools (version \u0026gt;= 37.0.0) are available.
Archives # You may also want to specify archive files. The archive files could be used to specify custom Python virtual environments, data files, etc.
You could specify the archive files inside the code using Python Table API as following:
table_env.add_python_archive(archive_path=\u0026#34;/path/to/archive_file\u0026#34;, target_dir=None) or using Python DataStream API as following:
stream_execution_environment.add_python_archive(archive_path=\u0026#34;/path/to/archive_file\u0026#34;, target_dir=None) Note The parameter target_dir is optional. If specified, the archive file will be extracted to a directory with the specified name of target_dir during execution. Otherwise, the archive file will be extracted to a directory with the same name as the archive file.
Suppose you have specified the archive file as following:
table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;myenv\u0026#34;) Then, you could access the content of the archive file in Python user-defined functions as following:
def my_udf(): with open(\u0026#34;myenv/py_env/data/data.txt\u0026#34;) as f: ... If you have not specified the parameter target_dir:
table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;) You could then access the content of the archive file in Python user-defined functions as following:
def my_udf(): with open(\u0026#34;py_env.zip/py_env/data/data.txt\u0026#34;) as f: ... Note The archive file will be extracted to the working directory of Python UDF worker and so you could access the files inside the archive file using relative path.
You could also specify the archive files using configuration python.archives or via command line arguments -pyarch or --pyArchives when submitting the job.
Note If the archive file contains a Python virtual environment, please make sure that the Python virtual environment matches the platform that the cluster is running on.
Note Currently, only zip files (i.e., zip, jar, whl, egg, etc) and tar files (i.e., tar, tar.gz, tgz) are supported.
Python interpreter # It supports to specify the path of the Python interpreter to execute Python worker.
You could specify the Python interpreter inside the code using Python Table API as following:
table_env.get_config().set_python_executable(\u0026#34;/path/to/python\u0026#34;) or using Python DataStream API as following:
stream_execution_environment.set_python_executable(\u0026#34;/path/to/python\u0026#34;) It also supports to use the Python interpreter inside an archive file.
# Python Table API table_env.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;venv\u0026#34;) table_env.get_config().set_python_executable(\u0026#34;venv/py_env/bin/python\u0026#34;) # Python DataStream API stream_execution_environment.add_python_archive(\u0026#34;/path/to/py_env.zip\u0026#34;, \u0026#34;venv\u0026#34;) stream_execution_environment.set_python_executable(\u0026#34;venv/py_env/bin/python\u0026#34;) You could also specify the Python interpreter using configuration python.executable or via command line arguments -pyexec or --pyExecutable when submitting the job.
Note If the path of the Python interpreter refers to the Python archive file, relative path should be used instead of absolute path.
Python interpreter of client # Python is needed at the client side to parse the Python user-defined functions during compiling the job.
You could specify the custom Python interpreter used at the client side by activating it in the current session.
source my_env/bin/activate or specify it using configuration python.client.executable, command line arguments -pyclientexec or --pyClientExecutable, environment variable PYFLINK_CLIENT_EXECUTABLE
How to specify Python Dependencies in Java/Scala Program # It also supports to use Python user-defined functions in the Java Table API programs or pure SQL programs. The following code shows a simple example on how to use the Python user-defined functions in a Java Table API program:
import org.apache.flink.configuration.CoreOptions; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; TableEnvironment tEnv = TableEnvironment.create( EnvironmentSettings.inBatchMode()); tEnv.getConfig().set(CoreOptions.DEFAULT_PARALLELISM, 1); // register the Python UDF tEnv.executeSql(\u0026#34;create temporary system function add_one as \u0026#39;add_one.add_one\u0026#39; language python\u0026#34;); tEnv.createTemporaryView(\u0026#34;source\u0026#34;, tEnv.fromValues(1L, 2L, 3L).as(\u0026#34;a\u0026#34;)); // use Python UDF in the Java Table API program tEnv.executeSql(\u0026#34;select add_one(a) as a from source\u0026#34;).collect(); You can refer to the SQL statement about CREATE FUNCTION for more details on how to create Python user-defined functions using SQL statements.
The Python dependencies could then be specified via the Python config options, such as python.archives, python.files, python.requirements, python.client.executable, python.executable. etc or through command line arguments when submitting the job.
`}),e.add({id:273,href:"/flink/flink-docs-master/zh/docs/dev/python/table/operations/row_based_operations/",title:"Row-based Operations",section:"Operations",content:` Row-based Operations # This page describes how to use row-based operations in PyFlink Table API.
Map # Performs a map operation with a python general scalar function or vectorized scalar function. The output will be flattened if the output type is a composite type.
from pyflink.common import Row from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.types import DataTypes from pyflink.table.udf import udf env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) @udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) def func1(id: int, data: str) -\u0026gt; Row: return Row(id, data * 2) # the input columns are specified as the inputs table.map(func1(col(\u0026#39;id\u0026#39;), col(\u0026#39;data\u0026#39;))).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ It also supports to take a Row object (containing all the columns of the input table) as input.
@udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())])) def func2(data: Row) -\u0026gt; Row: return Row(data.id, data.data * 2) # specify the function without the input columns table.map(func2).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ Note The input columns should not be specified when using func2 in the map operation.
It also supports to use vectorized scalar function in the map operation. It should be noted that the input type and output type should be pandas.DataFrame instead of Row in this case.
import pandas as pd @udf(result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;data\u0026#34;, DataTypes.STRING())]), func_type=\u0026#39;pandas\u0026#39;) def func3(data: pd.DataFrame) -\u0026gt; pd.DataFrame: res = pd.concat([data.id, data.data * 2], axis=1) return res table.map(func3).execute().print() # result is #+----------------------+--------------------------------+ #| id | data | #+----------------------+--------------------------------+ #| 1 | HiHi | #| 2 | HelloHello | #+----------------------+--------------------------------+ FlatMap # Performs a flat_map operation with a python table function.
from pyflink.common import Row from pyflink.table.udf import udtf from pyflink.table import DataTypes, EnvironmentSettings, TableEnvironment env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi,Flink\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;id\u0026#39;, \u0026#39;data\u0026#39;]) @udtf(result_types=[DataTypes.INT(), DataTypes.STRING()]) def split(x: Row) -\u0026gt; Row: for s in x.data.split(\u0026#34;,\u0026#34;): yield x.id, s # use split in \`flat_map\` table.flat_map(split).execute().print() # result is #+-------------+--------------------------------+ #| f0 | f1 | #+-------------+--------------------------------+ #| 1 | Hi | #| 1 | Flink | #| 2 | Hello | #+-------------+--------------------------------+ The python table function could also be used in join_lateral and left_outer_join_lateral.
# use table function in \`join_lateral\` or \`left_outer_join_lateral\` table.join_lateral(split.alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;)).execute().print() # result is #+----------------------+--------------------------------+-------------+--------------------------------+ #| id | data | a | b | #+----------------------+--------------------------------+-------------+--------------------------------+ #| 1 | Hi,Flink | 1 | Hi | #| 1 | Hi,Flink | 1 | Flink | #| 2 | Hello | 2 | Hello | #+----------------------+--------------------------------+-------------+--------------------------------+ Aggregate # Performs an aggregate operation with a python general aggregate function or vectorized aggregate function.
from pyflink.common import Row from pyflink.table import DataTypes, EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.udf import AggregateFunction, udaf class CountAndSumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return Row(accumulator[0], accumulator[1]) def create_accumulator(self): return Row(0, 0) def accumulate(self, accumulator, row): accumulator[0] += 1 accumulator[1] += row.b def retract(self, accumulator, row): accumulator[0] -= 1 accumulator[1] -= row.b def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] += other_acc[0] accumulator[1] += other_acc[1] def get_accumulator_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) function = CountAndSumAggregateFunction() agg = udaf(function, result_type=function.get_result_type(), accumulator_type=function.get_accumulator_type(), name=str(function.__class__.__name__)) # aggregate with a python general aggregate function env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) t = table_env.from_elements([(1, 2), (2, 1), (1, 3)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) result = t.group_by(col(\u0026#39;a\u0026#39;)) \\ .aggregate(agg.alias(\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;d\u0026#39;)) result.execute().print() # the result is #+----+----------------------+----------------------+----------------------+ #| op | a | c | d | #+----+----------------------+----------------------+----------------------+ #| +I | 1 | 2 | 5 | #| +I | 2 | 1 | 1 | #+----+----------------------+----------------------+----------------------+ # aggregate with a python vectorized aggregate function env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(env_settings) t = table_env.from_elements([(1, 2), (2, 1), (1, 3)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) t.aggregate(pandas_udaf.alias(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;)).execute().print() # the result is #+--------------------------------+-------------+ #| a | b | #+--------------------------------+-------------+ #| 2.0 | 3 | #+--------------------------------+-------------+ Note Similar to map operation, if you specify the aggregate function without the input columns in aggregate operation, it will take Row or Pandas.DataFrame as input which contains all the columns of the input table including the grouping keys. Note You have to close the \u0026ldquo;aggregate\u0026rdquo; with a select statement and it should not contain aggregate functions in the select statement. Besides, the output of aggregate will be flattened if it is a composite type.
FlatAggregate # Performs a flat_aggregate operation with a python general Table Aggregate Function
Similar to GroupBy Aggregation, FlatAggregate groups the inputs on the grouping keys. Different from AggregateFunction, TableAggregateFunction could return 0, 1, or more records for a grouping key. Similar to aggregate, you have to close the flat_aggregate with a select statement and the select statement should not contain aggregate functions.
from pyflink.common import Row from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udtaf, TableAggregateFunction class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row): if row.a is not None: if accumulator[0] is None or row.a \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row.a elif accumulator[1] is None or row.a \u0026gt; accumulator[1]: accumulator[1] = row.a def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) # the result type and accumulator type can also be specified in the udtaf decorator: # top2 = udtaf(Top2(), result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]), accumulator_type=DataTypes.ARRAY(DataTypes.BIGINT())) top2 = udtaf(Top2()) t = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # call function \u0026#34;inline\u0026#34; without registration in Table API result = t.group_by(col(\u0026#39;b\u0026#39;)).flat_aggregate(top2).select(col(\u0026#39;*\u0026#39;)).to_pandas() # the result is: #+----+--------------------------------+----------------------+ #| op | b | a | #+----+--------------------------------+----------------------+ #| +I | Hi2 | 5 | #| +I | Hi2 | \u0026lt;NULL\u0026gt; | #| +I | Hi | 7 | #| +I | Hi | 3 | #+----+--------------------------------+----------------------+ `}),e.add({id:274,href:"/flink/flink-docs-master/zh/docs/dev/python/datastream/state/",title:"State",section:"DataStream API",content:" "}),e.add({id:275,href:"/flink/flink-docs-master/zh/docs/dev/table/tableapi/",title:"Table API",section:"Table API \u0026 SQL",content:" Table API # Table API 是批处理和流处理的统一的关系型 API。Table API 的查询不需要修改代码就可以采用批输入或流输入来运行。Table API 是 SQL 语言的超集，并且是针对 Apache Flink 专门设计的。Table API 集成了 Scala，Java 和 Python 语言的 API。Table API 的查询是使用 Java，Scala 或 Python 语言嵌入的风格定义的，有诸如自动补全和语法校验的 IDE 支持，而不是像普通 SQL 一样使用字符串类型的值来指定查询。\nTable API 和 Flink SQL 共享许多概念以及部分集成的 API。通过查看公共概念 \u0026amp; API来学习如何注册表或如何创建一个表对象。流概念页面讨论了诸如动态表和时间属性等流特有的概念。\n下面的例子中假定有一张叫 Orders 的表，表中有属性 (a, b, c, rowtime) 。rowtime 字段是流任务中的逻辑时间属性或是批任务中的普通时间戳字段。\n概述 \u0026amp; 示例 # Table API 支持 Scala, Java 和 Python 语言。Scala 语言的 Table API 利用了 Scala 表达式，Java 语言的 Table API 支持 DSL 表达式和解析并转换为等价表达式的字符串，Python 语言的 Table API 仅支持解析并转换为等价表达式的字符串。\n下面的例子展示了 Scala、Java 和 Python 语言的 Table API 的不同之处。表程序是在批环境下执行的。程序扫描了 Orders 表，通过字段 a 进行分组，并计算了每组结果的行数。\nJava Java 的 Table API 通过引入 org.apache.flink.table.api.java.* 来使用。下面的例子展示了如何创建一个 Java 的 Table API 程序，以及表达式是如何指定为字符串的。 使用DSL表达式时也需要引入静态的 org.apache.flink.table.api.Expressions.*。\nimport org.apache.flink.table.api.*; import static org.apache.flink.table.api.Expressions.*; EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() .build(); TableEnvironment tEnv = TableEnvironment.create(settings); // 在表环境中注册 Orders 表 // ... // 指定表程序 Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // schema (a, b, c, rowtime) Table counts = orders .groupBy($(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).count().as(\u0026#34;cnt\u0026#34;)); // 打印 counts.execute().print(); Scala Scala 的 Table API 通过引入 org.apache.flink.table.api._、org.apache.flink.api.scala._ 和 org.apache.flink.table.api.bridge.scala._（开启数据流的桥接支持）来使用。\n下面的例子展示了如何创建一个 Scala 的 Table API 程序。通过 Scala 的带美元符号（$）的字符串插值来实现表字段引用。\nimport org.apache.flink.api.scala._ import org.apache.flink.table.api._ import org.apache.flink.table.api.bridge.scala._ // 环境配置 val settings = EnvironmentSettings .newInstance() .inStreamingMode() .build() val tEnv = TableEnvironment.create(settings) // 在表环境中注册 Orders 表 // ... // 指定表程序 val orders = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime) val result = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.count as \u0026#34;cnt\u0026#34;) .execute() .print() Python 下面的例子展示了如何创建一个 Python 的 Table API 程序，以及表达式是如何指定为字符串的。\nfrom pyflink.table import * from pyflink.table.expressions import col # 环境配置 t_env = TableEnvironment.create( environment_settings=EnvironmentSettings.in_batch_mode()) # 在表环境中注册 Orders 表和结果 sink 表 source_data_path = \u0026#34;/path/to/source/directory/\u0026#34; result_data_path = \u0026#34;/path/to/result/directory/\u0026#34; source_ddl = f\u0026#34;\u0026#34;\u0026#34; create table Orders( a VARCHAR, b BIGINT, c BIGINT, rowtime TIMESTAMP(3), WATERMARK FOR rowtime AS rowtime - INTERVAL \u0026#39;1\u0026#39; SECOND ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{source_data_path}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) sink_ddl = f\u0026#34;\u0026#34;\u0026#34; create table `Result`( a VARCHAR, cnt BIGINT ) with ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;{result_data_path}\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(sink_ddl) # 指定表程序 orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # schema (a, b, c, rowtime) orders.group_by(col(\u0026#34;a\u0026#34;)).select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).count.alias(\u0026#39;cnt\u0026#39;)).execute_insert(\u0026#34;result\u0026#34;).wait() 下一个例子展示了一个更加复杂的 Table API 程序。这个程序也扫描 Orders 表。程序过滤了空值，使字符串类型的字段 a 标准化，并且每个小时进行一次计算并返回 a 的平均账单金额 b。\nJava // 环境配置 // ... // 指定表程序 Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // schema (a, b, c, rowtime) Table result = orders .filter( and( $(\u0026#34;a\u0026#34;).isNotNull(), $(\u0026#34;b\u0026#34;).isNotNull(), $(\u0026#34;c\u0026#34;).isNotNull() )) .select($(\u0026#34;a\u0026#34;).lowerCase().as(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;rowtime\u0026#34;)) .window(Tumble.over(lit(1).hours()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;hourlyWindow\u0026#34;)) .groupBy($(\u0026#34;hourlyWindow\u0026#34;), $(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;hourlyWindow\u0026#34;).end().as(\u0026#34;hour\u0026#34;), $(\u0026#34;b\u0026#34;).avg().as(\u0026#34;avgBillingAmount\u0026#34;)); Scala // 环境配置 // ... // 指定表程序 val orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;) // schema (a, b, c, rowtime) val result: Table = orders .filter($\u0026#34;a\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;b\u0026#34;.isNotNull \u0026amp;\u0026amp; $\u0026#34;c\u0026#34;.isNotNull) .select($\u0026#34;a\u0026#34;.lowerCase() as \u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;rowtime\u0026#34;) .window(Tumble over 1.hour on $\u0026#34;rowtime\u0026#34; as \u0026#34;hourlyWindow\u0026#34;) .groupBy($\u0026#34;hourlyWindow\u0026#34;, $\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;hourlyWindow\u0026#34;.end as \u0026#34;hour\u0026#34;, $\u0026#34;b\u0026#34;.avg as \u0026#34;avgBillingAmount\u0026#34;) Python # 指定表程序 from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # schema (a, b, c, rowtime) result = orders.filter(col(\u0026#34;a\u0026#34;).is_not_null \u0026amp; col(\u0026#34;b\u0026#34;).is_not_null \u0026amp; col(\u0026#34;c\u0026#34;).is_not_null) \\ .select(col(\u0026#34;a\u0026#34;).lower_case.alias(\u0026#39;a\u0026#39;), col(\u0026#34;b\u0026#34;), col(\u0026#34;rowtime\u0026#34;)) \\ .window(Tumble.over(lit(1).hour).on(col(\u0026#34;rowtime\u0026#34;)).alias(\u0026#34;hourly_window\u0026#34;)) \\ .group_by(col(\u0026#39;hourly_window\u0026#39;), col(\u0026#39;a\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;hourly_window\u0026#39;).end.alias(\u0026#39;hour\u0026#39;), col(\u0026#34;b\u0026#34;).avg.alias(\u0026#39;avg_billing_amount\u0026#39;)) 因为 Table API 的批数据 API 和流数据 API 是统一的，所以这两个例子程序不需要修改代码就可以运行在流输入或批输入上。在这两种情况下，只要流任务没有数据延时，程序将会输出相同的结果（查看流概念获取详情)。\nBack to top\nOperations # Table API支持如下操作。请注意不是所有的操作都可以既支持流也支持批；这些操作都具有相应的标记。\nScan, Projection, and Filter # From # Batch Streaming\n和 SQL 查询的 FROM 子句类似。 执行一个注册过的表的扫描。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) FromValues # Batch Streaming\n和 SQL 查询中的 VALUES 子句类似。 基于提供的行生成一张内联表。\n你可以使用 row(...) 表达式创建复合行：\nJava Table table = tEnv.fromValues( row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ); Scala table = tEnv.fromValues( row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) Python table = t_env.from_elements([(1, \u0026#39;ABC\u0026#39;), (2, \u0026#39;ABCDE\u0026#39;)]) 这将生成一张结构如下的表：\nroot |-- f0: BIGINT NOT NULL // original types INT and BIGINT are generalized to BIGINT |-- f1: VARCHAR(5) NOT NULL // original types CHAR(3) and CHAR(5) are generalized // to VARCHAR(5). VARCHAR is used instead of CHAR so that // no padding is applied 这个方法会根据输入的表达式自动获取类型。如果在某一个特定位置的类型不一致，该方法会尝试寻找一个所有类型的公共超类型。如果公共超类型不存在，则会抛出异常。\n你也可以明确指定所需的类型。指定如 DECIMAL 这样的一般类型或者给列命名可能是有帮助的。\nJava Table table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()) ), row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ); Scala val table = tEnv.fromValues( DataTypes.ROW( DataTypes.FIELD(\u0026#34;id\u0026#34;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#34;name\u0026#34;, DataTypes.STRING()) ), row(1, \u0026#34;ABC\u0026#34;), row(2L, \u0026#34;ABCDE\u0026#34;) ) Python table = t_env.from_elements( [(1, \u0026#39;ABC\u0026#39;), (2, \u0026#39;ABCDE\u0026#39;)], schema=DataTypes.Row([DataTypes.FIELD(\u0026#39;id\u0026#39;, DataTypes.DECIMAL(10, 2)), DataTypes.FIELD(\u0026#39;name\u0026#39;, DataTypes.STRING())])) 这将生成一张结构如下的表：\nroot |-- id: DECIMAL(10, 2) |-- name: STRING Select # Batch Streaming\n和 SQL 的 SELECT 子句类似。 执行一个 select 操作。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.select($(\u0026#34;a\u0026#34;), $(\u0026#34;c\u0026#34;).as(\u0026#34;d\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) Table result = orders.select($\u0026#34;a\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;d\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.select(col(\u0026#34;a\u0026#34;), col(\u0026#34;c\u0026#34;).alias(\u0026#39;d\u0026#39;)) 你可以选择星号（*）作为通配符，select 表中的所有列。\nJava Table result = orders.select($(\u0026#34;*\u0026#34;)); Scala Table result = orders.select($\u0026#34;*\u0026#34;) Python from pyflink.table.expressions import col result = orders.select(col(\u0026#34;*\u0026#34;)) As # Batch Streaming\n重命名字段。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.as(\u0026#34;x, y, z, t\u0026#34;); scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;, \u0026#34;t\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.alias(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;, \u0026#34;t\u0026#34;) Where / Filter # Batch Streaming\n和 SQL 的 WHERE 子句类似。 过滤掉未验证通过过滤谓词的行。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.where($(\u0026#34;b\u0026#34;).isEqual(\u0026#34;red\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.filter($\u0026#34;a\u0026#34; % 2 === 0) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.where(col(\u0026#34;a\u0026#34;) == \u0026#39;red\u0026#39;) 或者\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.filter($(\u0026#34;b\u0026#34;).isEqual(\u0026#34;red\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.filter($\u0026#34;a\u0026#34; % 2 === 0) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.filter(col(\u0026#34;a\u0026#34;) == \u0026#39;red\u0026#39;) 列操作 # AddColumns # Batch Streaming\n执行字段添加操作。 如果所添加的字段已经存在，将抛出异常。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.addColumns(concat($(\u0026#34;c\u0026#34;), \u0026#34;sunny\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.addColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;)) Python from pyflink.table.expressions import concat orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.add_columns(concat(col(\u0026#34;c\u0026#34;), \u0026#39;sunny\u0026#39;)) AddOrReplaceColumns # Batch Streaming\n执行字段添加操作。 如果添加的列名称和已存在的列名称相同，则已存在的字段将被替换。 此外，如果添加的字段里面有重复的字段名，则会使用最后一个字段。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.addOrReplaceColumns(concat($(\u0026#34;c\u0026#34;), \u0026#34;sunny\u0026#34;).as(\u0026#34;desc\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.addOrReplaceColumns(concat($\u0026#34;c\u0026#34;, \u0026#34;Sunny\u0026#34;) as \u0026#34;desc\u0026#34;) Python from pyflink.table.expressions import concat orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.add_or_replace_columns(concat(col(\u0026#34;c\u0026#34;), \u0026#39;sunny\u0026#39;).alias(\u0026#39;desc\u0026#39;)) DropColumns # Batch Streaming\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.dropColumns($(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.dropColumns($\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.drop_columns(col(\u0026#34;b\u0026#34;), col(\u0026#34;c\u0026#34;)) RenameColumns # Batch Streaming\n执行字段重命名操作。 字段表达式应该是别名表达式，并且仅当字段已存在时才能被重命名。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.renameColumns($(\u0026#34;b\u0026#34;).as(\u0026#34;b2\u0026#34;), $(\u0026#34;c\u0026#34;).as(\u0026#34;c2\u0026#34;)); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.renameColumns($\u0026#34;b\u0026#34; as \u0026#34;b2\u0026#34;, $\u0026#34;c\u0026#34; as \u0026#34;c2\u0026#34;) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.rename_columns(col(\u0026#34;b\u0026#34;).alias(\u0026#39;b2\u0026#39;), col(\u0026#34;c\u0026#34;).alias(\u0026#39;c2\u0026#39;)) Back to top\nAggregations # GroupBy Aggregation # Batch Streaming Result Updating\n和 SQL 的 GROUP BY 子句类似。 使用分组键对行进行分组，使用伴随的聚合算子来按照组进行聚合行。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.groupBy($(\u0026#34;a\u0026#34;)).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().as(\u0026#34;d\u0026#34;)); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.groupBy($\u0026#34;a\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum().as(\u0026#34;d\u0026#34;)) Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.group_by(col(\u0026#34;a\u0026#34;)).select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.alias(\u0026#39;d\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. GroupBy Window Aggregation # Batch Streaming\n使用分组窗口结合单个或者多个分组键对表进行分组和聚合。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .window(Tumble.over(lit(5).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)) // 定义窗口 .groupBy($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;)) // 按窗口和键分组 // 访问窗口属性并聚合 .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end(), $(\u0026#34;w\u0026#34;).rowtime(), $(\u0026#34;b\u0026#34;).sum().as(\u0026#34;d\u0026#34;) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // 定义窗口 .groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) // 按窗口和键分组 .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.sum as \u0026#34;d\u0026#34;) // 访问窗口属性并聚合 Python from pyflink.table.window import Tumble from pyflink.table.expressions import lit, col orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.window(Tumble.over(lit(5).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, col(\u0026#39;b\u0026#39;).sum.alias(\u0026#39;d\u0026#39;)) Over Window Aggregation # 和 SQL 的 OVER 子句类似。 更多细节详见 over windows section\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders // 定义窗口 .window( Over .partitionBy($(\u0026#34;a\u0026#34;)) .orderBy($(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .following(CURRENT_RANGE) .as(\u0026#34;w\u0026#34;)) // 滑动聚合 .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).avg().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).max().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).min().over($(\u0026#34;w\u0026#34;)) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result: Table = orders // 定义窗口 .window( Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE following CURRENT_RANGE as \u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max().over($\u0026#34;w\u0026#34;), $\u0026#34;b\u0026#34;.min().over($\u0026#34;w\u0026#34;)) // 滑动聚合 Python from pyflink.table.window import Over from pyflink.table.expressions import col, UNBOUNDED_RANGE, CURRENT_RANGE orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.over_window(Over.partition_by(col(\u0026#34;a\u0026#34;)).order_by(col(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE).following(CURRENT_RANGE) .alias(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).avg.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).max.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).min.over(col(\u0026#39;w\u0026#39;))) 所有的聚合必须定义在同一个窗口上，比如同一个分区、排序和范围内。目前只支持 PRECEDING 到当前行范围（无界或有界）的窗口。尚不支持 FOLLOWING 范围的窗口。ORDER BY 操作必须指定一个单一的时间属性。\nDistinct Aggregation # Batch Streaming Result Updating\n和 SQL DISTINCT 聚合子句类似，例如 COUNT(DISTINCT a)。 Distinct 聚合声明的聚合函数（内置或用户定义的）仅应用于互不相同的输入值。 Distinct 可以应用于 GroupBy Aggregation、GroupBy Window Aggregation 和 Over Window Aggregation。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); // 按属性分组后的的互异（互不相同、去重）聚合 Table groupByDistinctResult = orders .groupBy($(\u0026#34;a\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().distinct().as(\u0026#34;d\u0026#34;)); // 按属性、时间窗口分组后的互异（互不相同、去重）聚合 Table groupByWindowDistinctResult = orders .window(Tumble .over(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;) ) .groupBy($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().distinct().as(\u0026#34;d\u0026#34;)); // over window 上的互异（互不相同、去重）聚合 Table result = orders .window(Over .partitionBy($(\u0026#34;a\u0026#34;)) .orderBy($(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .as(\u0026#34;w\u0026#34;)) .select( $(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).avg().distinct().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).max().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;b\u0026#34;).min().over($(\u0026#34;w\u0026#34;)) ); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) // 按属性分组后的的互异（互不相同、去重）聚合 val groupByDistinctResult = orders .groupBy($\u0026#34;a\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // 按属性、时间窗口分组后的互异（互不相同、去重）聚合 val groupByWindowDistinctResult = orders .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;).groupBy($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum.distinct as \u0026#34;d\u0026#34;) // over window 上的互异（互不相同、去重）聚合 val result = orders .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as $\u0026#34;w\u0026#34;) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.avg.distinct over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.max over $\u0026#34;w\u0026#34;, $\u0026#34;b\u0026#34;.min over $\u0026#34;w\u0026#34;) Python from pyflink.table.expressions import col, lit, UNBOUNDED_RANGE from pyflink.table.window import Over, Tumble orders = t_env.from_path(\u0026#34;Orders\u0026#34;) # 按属性分组后的的互异（互不相同、去重）聚合 group_by_distinct_result = orders.group_by(col(\u0026#34;a\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.distinct.alias(\u0026#39;d\u0026#39;)) # 按属性、时间窗口分组后的互异（互不相同、去重）聚合 group_by_window_distinct_result = orders.window(Tumble.over(lit(5).minutes).on(col(\u0026#34;rowtime\u0026#34;)).alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#34;a\u0026#34;), col(\u0026#39;w\u0026#39;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).sum.distinct.alias(\u0026#39;d\u0026#39;)) # over window 上的互异（互不相同、去重）聚合 result = orders.over_window(Over .partition_by(col(\u0026#34;a\u0026#34;)) .order_by(col(\u0026#34;rowtime\u0026#34;)) .preceding(UNBOUNDED_RANGE) .alias(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#34;a\u0026#34;), col(\u0026#34;b\u0026#34;).avg.distinct.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).max.over(col(\u0026#39;w\u0026#39;)), col(\u0026#34;b\u0026#34;).min.over(col(\u0026#39;w\u0026#39;))) 用户定义的聚合函数也可以与 DISTINCT 修饰符一起使用。如果计算不同（互异、去重的）值的聚合结果，则只需向聚合函数添加 distinct 修饰符即可。\nJava Table orders = tEnv.from(\u0026#34;Orders\u0026#34;); // 对 user-defined aggregate functions 使用互异（互不相同、去重）聚合 tEnv.registerFunction(\u0026#34;myUdagg\u0026#34;, new MyUdagg()); orders.groupBy(\u0026#34;users\u0026#34;) .select( $(\u0026#34;users\u0026#34;), call(\u0026#34;myUdagg\u0026#34;, $(\u0026#34;points\u0026#34;)).distinct().as(\u0026#34;myDistinctResult\u0026#34;) ); Scala val orders: Table = tEnv.from(\u0026#34;Orders\u0026#34;) // 对 user-defined aggregate functions 使用互异（互不相同、去重）聚合 val myUdagg = new MyUdagg() orders.groupBy($\u0026#34;users\u0026#34;).select($\u0026#34;users\u0026#34;, myUdagg.distinct($\u0026#34;points\u0026#34;) as \u0026#34;myDistinctResult\u0026#34;) Python Unsupported For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Distinct # Batch Streaming Result Updating\n和 SQL 的 DISTINCT 子句类似。 返回具有不同组合值的记录。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders.distinct(); Scala val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders.distinct() Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) result = orders.distinct() For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Back to top\nJoins # Inner Join # Batch Streaming\n和 SQL 的 JOIN 子句类似。关联两张表。两张表必须有不同的字段名，并且必须通过 join 算子或者使用 where 或 filter 算子定义至少一个 join 等式连接谓词。\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;)); Table result = left.join(right) .where($(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val result = left.join(right).where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;)) result = left.join(right).where(col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Outer Join # Batch Streaming Result Updating\n和 SQL LEFT/RIGHT/FULL OUTER JOIN 子句类似。 关联两张表。 两张表必须有不同的字段名，并且必须定义至少一个等式连接谓词。\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;)); Table leftOuterResult = left.leftOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Table rightOuterResult = left.rightOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Table fullOuterResult = left.fullOuterJoin(right, $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;))) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;) val leftOuterResult = left.leftOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val rightOuterResult = left.rightOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) val fullOuterResult = left.fullOuterJoin(right, $\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;)) left_outer_result = left.left_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) right_outer_result = left.right_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) full_outer_result = left.full_outer_join(right, col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Interval Join # Batch Streaming\nInterval join 是可以通过流模式处理的常规 join 的子集。\nInterval join 至少需要一个 equi-join 谓词和一个限制双方时间界限的 join 条件。这种条件可以由两个合适的范围谓词（\u0026lt;、\u0026lt;=、\u0026gt;=、\u0026gt;）或一个比较两个输入表相同时间属性（即处理时间或事件时间）的等值谓词来定义。\nJava Table left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;), $(\u0026#34;ltime\u0026#34;)); Table right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($(\u0026#34;d\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;f\u0026#34;), $(\u0026#34;rtime\u0026#34;)); Table result = left.join(right) .where( and( $(\u0026#34;a\u0026#34;).isEqual($(\u0026#34;d\u0026#34;)), $(\u0026#34;ltime\u0026#34;).isGreaterOrEqual($(\u0026#34;rtime\u0026#34;).minus(lit(5).minutes())), $(\u0026#34;ltime\u0026#34;).isLess($(\u0026#34;rtime\u0026#34;).plus(lit(10).minutes())) )) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;e\u0026#34;), $(\u0026#34;ltime\u0026#34;)); Scala val left = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;, $\u0026#34;ltime\u0026#34;) val right = tableEnv.from(\u0026#34;MyTable\u0026#34;).select($\u0026#34;d\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;f\u0026#34;, $\u0026#34;rtime\u0026#34;) val result = left.join(right) .where($\u0026#34;a\u0026#34; === $\u0026#34;d\u0026#34; \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026gt;= $\u0026#34;rtime\u0026#34; - 5.minutes \u0026amp;\u0026amp; $\u0026#34;ltime\u0026#34; \u0026lt; $\u0026#34;rtime\u0026#34; + 10.minutes) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;e\u0026#34;, $\u0026#34;ltime\u0026#34;) Python from pyflink.table.expressions import col left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;rowtime1\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;f\u0026#39;), col(\u0026#39;rowtime2\u0026#39;)) joined_table = left.join(right).where((col(\u0026#39;a\u0026#39;) == col(\u0026#39;d\u0026#39;)) \u0026amp; (col(\u0026#39;rowtime1\u0026#39;) \u0026gt;= col(\u0026#39;rowtime2\u0026#39;) - lit(1).second) \u0026amp; (col(\u0026#39;rowtime1\u0026#39;) \u0026lt;= col(\u0026#39;rowtime2\u0026#39;) + lit(2).seconds)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;e\u0026#39;), col(\u0026#39;rowtime1\u0026#39;)) Inner Join with Table Function (UDTF) # Batch Streaming\njoin 表和表函数的结果。左（外部）表的每一行都会 join 表函数相应调用产生的所有行。 如果表函数调用返回空结果，则删除左侧（外部）表的一行。\nJava // 注册 User-Defined Table Function TableFunction\u0026lt;Tuple3\u0026lt;String,String,String\u0026gt;\u0026gt; split = new MySplitUDTF(); tableEnv.registerFunction(\u0026#34;split\u0026#34;, split); // join Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .joinLateral(call(\u0026#34;split\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;s\u0026#34;), $(\u0026#34;t\u0026#34;), $(\u0026#34;v\u0026#34;)); Scala // 实例化 User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .joinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;) Python # 注册 User-Defined Table Function @udtf(result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()]) def split(x): return [Row(1, 2, 3)] # join orders = t_env.from_path(\u0026#34;Orders\u0026#34;) joined_table = orders.join_lateral(split(col(\u0026#39;c\u0026#39;)).alias(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;s\u0026#39;), col(\u0026#39;t\u0026#39;), col(\u0026#39;v\u0026#39;)) Left Outer Join with Table Function (UDTF) # Batch Streaming\njoin 表和表函数的结果。左（外部）表的每一行都会 join 表函数相应调用产生的所有行。如果表函数调用返回空结果，则保留相应的 outer（外部连接）行并用空值填充右侧结果。\n目前，表函数左外连接的谓词只能为空或字面（常量）真。\nJava // 注册 User-Defined Table Function TableFunction\u0026lt;Tuple3\u0026lt;String,String,String\u0026gt;\u0026gt; split = new MySplitUDTF(); tableEnv.registerFunction(\u0026#34;split\u0026#34;, split); // join Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .leftOuterJoinLateral(call(\u0026#34;split\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;s\u0026#34;), $(\u0026#34;t\u0026#34;), $(\u0026#34;v\u0026#34;)); Scala // 实例化 User-Defined Table Function val split: TableFunction[_] = new MySplitUDTF() // join val result: Table = table .leftOuterJoinLateral(split($\u0026#34;c\u0026#34;) as (\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;s\u0026#34;, $\u0026#34;t\u0026#34;, $\u0026#34;v\u0026#34;) Python # 注册 User-Defined Table Function @udtf(result_types=[DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()]) def split(x): return [Row(1, 2, 3)] # join orders = t_env.from_path(\u0026#34;Orders\u0026#34;) joined_table = orders.left_outer_join_lateral(split(col(\u0026#39;c\u0026#39;)).alias(\u0026#34;s\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;v\u0026#34;)) result = joined_table.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;s\u0026#39;), col(\u0026#39;t\u0026#39;), col(\u0026#39;v\u0026#39;)) Join with Temporal Table # Temporal table 是跟踪随时间变化的表。\nTemporal table 函数提供对特定时间点 temporal table 状态的访问。表与 temporal table 函数进行 join 的语法和使用表函数进行 inner join 的语法相同。\n目前仅支持与 temporal table 的 inner join。\nJava Table ratesHistory = tableEnv.from(\u0026#34;RatesHistory\u0026#34;); // 注册带有时间属性和主键的 temporal table function TemporalTableFunction rates = ratesHistory.createTemporalTableFunction( \u0026#34;r_proctime\u0026#34;, \u0026#34;r_currency\u0026#34;); tableEnv.registerFunction(\u0026#34;rates\u0026#34;, rates); // 基于时间属性和键与“Orders”表关联 Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .joinLateral(call(\u0026#34;rates\u0026#34;, $(\u0026#34;o_proctime\u0026#34;)), $(\u0026#34;o_currency\u0026#34;).isEqual($(\u0026#34;r_currency\u0026#34;))); Python 目前不支持 Python 的 Table API。 Back to top\nSet Operations # Union # Batch 和 SQL UNION 子句类似。Union 两张表会删除重复记录。两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.union(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.union(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.union(right) UnionAll # Batch Streaming\n和 SQL UNION ALL 子句类似。Union 两张表。 两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.unionAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.unionAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.union_all(right) Intersect # Batch 和 SQL INTERSECT 子句类似。Intersect 返回两个表中都存在的记录。如果一条记录在一张或两张表中存在多次，则只返回一条记录，也就是说，结果表中不存在重复的记录。两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.intersect(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.intersect(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.intersect(right) IntersectAll # Batch 和 SQL INTERSECT ALL 子句类似。IntersectAll 返回两个表中都存在的记录。如果一条记录在两张表中出现多次，那么该记录返回的次数同该记录在两个表中都出现的次数一致，也就是说，结果表可能存在重复记录。两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.intersectAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.intersectAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.intersect_all(right) Minus # Batch 和 SQL EXCEPT 子句类似。Minus 返回左表中存在且右表中不存在的记录。左表中的重复记录只返回一次，换句话说，结果表中没有重复记录。两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.minus(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.minus(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.minus(right) MinusAll # Batch 和 SQL EXCEPT ALL 子句类似。MinusAll 返回右表中不存在的记录。在左表中出现 n 次且在右表中出现 m 次的记录，在结果表中出现 (n - m) 次，例如，也就是说结果中删掉了在右表中存在重复记录的条数的记录。两张表必须具有相同的字段类型。\nJava Table left = tableEnv.from(\u0026#34;orders1\u0026#34;); Table right = tableEnv.from(\u0026#34;orders2\u0026#34;); left.minusAll(right); Scala val left = tableEnv.from(\u0026#34;orders1\u0026#34;) val right = tableEnv.from(\u0026#34;orders2\u0026#34;) left.minusAll(right) Python left = t_env.from_path(\u0026#34;orders1\u0026#34;) right = t_env.from_path(\u0026#34;orders2\u0026#34;) left.minus_all(right) In # Batch Streaming\n和 SQL IN 子句类似。如果表达式的值存在于给定表的子查询中，那么 In 子句返回 true。子查询表必须由一列组成。这个列必须与表达式具有相同的数据类型。\nJava Table left = tableEnv.from(\u0026#34;Orders1\u0026#34;) Table right = tableEnv.from(\u0026#34;Orders2\u0026#34;); Table result = left.select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;), $(\u0026#34;c\u0026#34;)).where($(\u0026#34;a\u0026#34;).in(right)); Scala val left = tableEnv.from(\u0026#34;Orders1\u0026#34;) val right = tableEnv.from(\u0026#34;Orders2\u0026#34;) val result = left.select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;, $\u0026#34;c\u0026#34;).where($\u0026#34;a\u0026#34;.in(right)) Python left = t_env.from_path(\u0026#34;Source1\u0026#34;).select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)) right = t_env.from_path(\u0026#34;Source2\u0026#34;).select(col(\u0026#39;a\u0026#39;)) result = left.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;), col(\u0026#39;c\u0026#39;)).where(col(\u0026#39;a\u0026#39;).in_(right)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. Back to top\nOrderBy, Offset \u0026amp; Fetch # Order By # Batch Streaming\n和 SQL ORDER BY 子句类似。返回跨所有并行分区的全局有序记录。对于无界表，该操作需要对时间属性进行排序或进行后续的 fetch 操作。\nJava Table result = tab.orderBy($(\u0026#34;a\u0026#34;).asc()); Scala val result = tab.orderBy($\u0026#34;a\u0026#34;.asc) Python result = tab.order_by(col(\u0026#39;a\u0026#39;).asc) Offset \u0026amp; Fetch # Batch Streaming\n和 SQL 的 OFFSET 和 FETCH 子句类似。Offset 操作根据偏移位置来限定（可能是已排序的）结果集。Fetch 操作将（可能已排序的）结果集限制为前 n 行。通常，这两个操作前面都有一个排序操作。对于无界表，offset 操作需要 fetch 操作。\nJava // 从已排序的结果集中返回前5条记录 Table result1 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).fetch(5); // 从已排序的结果集中返回跳过3条记录之后的所有记录 Table result2 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).offset(3); // 从已排序的结果集中返回跳过10条记录之后的前5条记录 Table result3 = in.orderBy($(\u0026#34;a\u0026#34;).asc()).offset(10).fetch(5); Scala // 从已排序的结果集中返回前5条记录 val result1: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).fetch(5) // 从已排序的结果集中返回跳过3条记录之后的所有记录 val result2: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(3) // 从已排序的结果集中返回跳过10条记录之后的前5条记录 val result3: Table = in.orderBy($\u0026#34;a\u0026#34;.asc).offset(10).fetch(5) Python # 从已排序的结果集中返回前5条记录 result1 = table.order_by(col(\u0026#39;a\u0026#39;).asc).fetch(5) # 从已排序的结果集中返回跳过3条记录之后的所有记录 result2 = table.order_by(col(\u0026#39;a\u0026#39;).asc).offset(3) # 从已排序的结果集中返回跳过10条记录之后的前5条记录 result3 = table.order_by(col(\u0026#39;a\u0026#39;).asc).offset(10).fetch(5) Insert # Batch Streaming\n和 SQL 查询中的 INSERT INTO 子句类似，该方法执行对已注册的输出表的插入操作。 insertInto() 方法会将 INSERT INTO 转换为一个 TablePipeline。 该数据流可以用 TablePipeline.explain() 来解释，用 TablePipeline.execute() 来执行。\n输出表必须已注册在 TableEnvironment（详见表连接器）中。此外，已注册表的 schema 必须与查询中的 schema 相匹配。\nJava Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); orders.insertInto(\u0026#34;OutOrders\u0026#34;).execute(); Scala val orders = tableEnv.from(\u0026#34;Orders\u0026#34;) orders.insertInto(\u0026#34;OutOrders\u0026#34;).execute() Python orders = t_env.from_path(\u0026#34;Orders\u0026#34;) orders.execute_insert(\u0026#34;OutOrders\u0026#34;) Back to top\nGroup Windows # Group window 聚合根据时间或行计数间隔将行分为有限组，并为每个分组进行一次聚合函数计算。对于批处理表，窗口是按时间间隔对记录进行分组的便捷方式。\nJava 窗口是使用 window(GroupWindow w) 子句定义的，并且需要使用 as 子句来指定别名。为了按窗口对表进行分组，窗口别名的引用必须像常规分组属性一样在 groupBy(...) 子句中。 以下示例展示了如何在表上定义窗口聚合。\nTable table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // 定义窗口并指定别名为 w .groupBy($(\u0026#34;w\u0026#34;)) // 以窗口 w 对表进行分组 .select($(\u0026#34;b\u0026#34;).sum()); // 聚合 Scala 窗口是使用 window(GroupWindow w) 子句定义的，并且需要使用 as 子句来指定别名。为了按窗口对表进行分组，窗口别名必须像常规分组属性一样在 groupBy(...) 子句中引用。 以下示例展示了如何在表上定义窗口聚合。\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // 定义窗口并指定别名为 w .groupBy($\u0026#34;w\u0026#34;) // 以窗口 w 对表进行分组 .select($\u0026#34;b\u0026#34;.sum) // 聚合 Python 窗口是使用 window(GroupWindow w) 子句定义的，并且需要使用 alias 子句来指定别名。为了按窗口对表进行分组，窗口别名必须像常规分组属性一样在 group_by(...) 子句中引用。 以下示例展示了如何在表上定义窗口聚合。\n# 定义窗口并指定别名为 w，以窗口 w 对表进行分组，然后再聚合 table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;)).select(col(\u0026#39;b\u0026#39;).sum) Java 在流环境中，如果窗口聚合除了窗口之外还根据一个或多个属性进行分组，则它们只能并行计算，例如，groupBy(...) 子句引用了一个窗口别名和至少一个附加属性。仅引用窗口别名（例如在上面的示例中）的 groupBy(...) 子句只能由单个非并行任务进行计算。 以下示例展示了如何定义有附加分组属性的窗口聚合。\nTable table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // 定义窗口并指定别名为 w .groupBy($(\u0026#34;w\u0026#34;), $(\u0026#34;a\u0026#34;)) // 以属性 a 和窗口 w 对表进行分组 .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum()); // 聚合 Scala 在流环境中，如果窗口聚合除了窗口之外还根据一个或多个属性进行分组，则它们只能并行计算，例如，groupBy(...) 子句引用了一个窗口别名和至少一个附加属性。仅引用窗口别名（例如在上面的示例中）的 groupBy(...) 子句只能由单个非并行任务进行计算。 以下示例展示了如何定义有附加分组属性的窗口聚合。\nval table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // 定义窗口并指定别名为 w .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // 以属性 a 和窗口 w 对表进行分组 .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum) // 聚合 Python 在流环境中，如果窗口聚合除了窗口之外还根据一个或多个属性进行分组，则它们只能并行计算，例如，group_by(...) 子句引用了一个窗口别名和至少一个附加属性。仅引用窗口别名（例如在上面的示例中）的 group_by(...) 子句只能由单个非并行任务进行计算。 以下示例展示了如何定义有附加分组属性的窗口聚合。\n# 定义窗口并指定别名为 w，以属性 a 和窗口 w 对表进行分组， # 然后再聚合 table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;a\u0026#39;)).select(col(\u0026#39;b\u0026#39;).sum) 时间窗口的开始、结束或行时间戳等窗口属性可以作为窗口别名的属性添加到 select 子句中，如 w.start、w.end 和 w.rowtime。窗口开始和行时间戳是包含的上下窗口边界。相反，窗口结束时间戳是唯一的上窗口边界。例如，从下午 2 点开始的 30 分钟滚动窗口将 “14:00:00.000” 作为开始时间戳，“14:29:59.999” 作为行时间时间戳，“14:30:00.000” 作为结束时间戳。\nJava Table table = input .window([GroupWindow w].as(\u0026#34;w\u0026#34;)) // 定义窗口并指定别名为 w .groupBy($(\u0026#34;w\u0026#34;), $(\u0026#34;a\u0026#34;)) // 以属性 a 和窗口 w 对表进行分组 .select($(\u0026#34;a\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end(), $(\u0026#34;w\u0026#34;).rowtime(), $(\u0026#34;b\u0026#34;).count()); // 聚合并添加窗口开始、结束和 rowtime 时间戳 Scala val table = input .window([w: GroupWindow] as $\u0026#34;w\u0026#34;) // 定义窗口并指定别名为 w .groupBy($\u0026#34;w\u0026#34;, $\u0026#34;a\u0026#34;) // 以属性 a 和窗口 w 对表进行分组 .select($\u0026#34;a\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end, $\u0026#34;w\u0026#34;.rowtime, $\u0026#34;b\u0026#34;.count) // 聚合并添加窗口开始、结束和 rowtime 时间戳 Python # 定义窗口并指定别名为 w，以属性 a 和窗口 w 对表进行分组， # 然后再聚合并添加窗口开始、结束和 rowtime 时间戳 table = input.window([w: GroupWindow].alias(\u0026#34;w\u0026#34;)) \\ .group_by(col(\u0026#39;w\u0026#39;), col(\u0026#39;a\u0026#39;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, col(\u0026#39;w\u0026#39;).rowtime, col(\u0026#39;b\u0026#39;).count) Window 参数定义了如何将行映射到窗口。 Window 不是用户可以实现的接口。相反，Table API 提供了一组具有特定语义的预定义 Window 类。下面列出了支持的窗口定义。\nTumble (Tumbling Windows) # 滚动窗口将行分配给固定长度的非重叠连续窗口。例如，一个 5 分钟的滚动窗口以 5 分钟的间隔对行进行分组。滚动窗口可以定义在事件时间、处理时间或行数上。\nJava 滚动窗口是通过 Tumble 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Tumbling Event-time Window .window(Tumble.over(lit(10).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(lit(10).minutes()).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(rowInterval(10)).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala 滚动窗口是通过 Tumble 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Tumbling Event-time Window .window(Tumble over 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble over 10.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python 滚动窗口是通过 Tumble 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 alias 指定窗口的别名。别名用于在 group_by() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 # Tumbling Event-time Window .window(Tumble.over(lit(10).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Tumbling Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(lit(10).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Tumbling Row-count Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Tumble.over(row_interval(10)).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Slide (Sliding Windows) # 滑动窗口具有固定大小并按指定的滑动间隔滑动。如果滑动间隔小于窗口大小，则滑动窗口重叠。因此，行可能分配给多个窗口。例如，15 分钟大小和 5 分钟滑动间隔的滑动窗口将每一行分配给 3 个不同的 15 分钟大小的窗口，以 5 分钟的间隔进行一次计算。滑动窗口可以定义在事件时间、处理时间或行数上。\nJava 滑动窗口是通过 Slide 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 every 将窗口的长度定义为时间或行计数间隔。滑动间隔的类型必须与窗口长度的类型相同。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Sliding Event-time Window .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;)); // Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(lit(10).minutes()) .every(lit(5).minutes()) .on($(\u0026#34;proctime\u0026#34;)) .as(\u0026#34;w\u0026#34;)); // Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(rowInterval(10)).every(rowInterval(5)).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala 滑动窗口是通过 Slide 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 every 将窗口的长度定义为时间或行计数间隔。滑动间隔的类型必须与窗口长度的类型相同。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Sliding Event-time Window .window(Slide over 10.minutes every 5.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.minutes every 5.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) // Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide over 10.rows every 5.rows on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python 滑动窗口是通过 Slide 类定义的，具体如下：\nMethod Description over 将窗口的长度定义为时间或行计数间隔。 every 将窗口的长度定义为时间或行计数间隔。滑动间隔的类型必须与窗口长度的类型相同。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 alias 指定窗口的别名。别名用于在 group_by() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 # Sliding Event-time Window .window(Slide.over(lit(10).minutes).every(lit(5).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Sliding Processing-time window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(lit(10).minutes).every(lit(5).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Sliding Row-count window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Slide.over(row_interval(10)).every(row_interval(5)).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Session (Session Windows) # 会话窗口没有固定的大小，其边界是由不活动的间隔定义的，例如，如果在定义的间隔期内没有事件出现，则会话窗口将关闭。例如，定义30 分钟间隔的会话窗口，当观察到一行在 30 分钟内不活动（否则该行将被添加到现有窗口中）且30 分钟内没有添加新行，窗口会关闭。会话窗口支持事件时间和处理时间。\nJava 会话窗口是通过 Session 类定义的，具体如下：\nMethod Description withGap 将两个窗口之间的间隙定义为时间间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Session Event-time Window .window(Session.withGap(lit(10).minutes()).on($(\u0026#34;rowtime\u0026#34;)).as(\u0026#34;w\u0026#34;)); // Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session.withGap(lit(10).minutes()).on($(\u0026#34;proctime\u0026#34;)).as(\u0026#34;w\u0026#34;)); Scala 会话窗口是通过 Session 类定义的，具体如下：\nMethod Description withGap 将两个窗口之间的间隙定义为时间间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 as 指定窗口的别名。别名用于在 groupBy() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 // Session Event-time Window .window(Session withGap 10.minutes on $\u0026#34;rowtime\u0026#34; as $\u0026#34;w\u0026#34;) // Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session withGap 10.minutes on $\u0026#34;proctime\u0026#34; as $\u0026#34;w\u0026#34;) Python 会话窗口是通过 Session 类定义的，具体如下：\nMethod Description with_gap 将两个窗口之间的间隙定义为时间间隔。 on 要对数据进行分组（时间间隔）或排序（行计数）的时间属性。批处理查询支持任意 Long 或 Timestamp 类型的属性。流处理查询仅支持声明的事件时间或处理时间属性。 alias 指定窗口的别名。别名用于在 group_by() 子句中引用窗口，并可以在 select() 子句中选择如窗口开始、结束或行时间戳的窗口属性。 # Session Event-time Window .window(Session.with_gap(lit(10).minutes).on(col(\u0026#39;rowtime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) # Session Processing-time Window (assuming a processing-time attribute \u0026#34;proctime\u0026#34;) .window(Session.with_gap(lit(10).minutes).on(col(\u0026#39;proctime\u0026#39;)).alias(\u0026#34;w\u0026#34;)) Back to top\nOver Windows # Over window 聚合聚合来自在标准的 SQL（OVER 子句），可以在 SELECT 查询子句中定义。与在“GROUP BY”子句中指定的 group window 不同， over window 不会折叠行。相反，over window 聚合为每个输入行在其相邻行的范围内计算聚合。\nOver windows 使用 window(w: OverWindow*) 子句（在 Python API 中使用 over_window(*OverWindow)）定义，并通过 select() 方法中的别名引用。以下示例显示如何在表上定义 over window 聚合。\nJava Table table = input .window([OverWindow w].as(\u0026#34;w\u0026#34;)) // define over window with alias w .select($(\u0026#34;a\u0026#34;), $(\u0026#34;b\u0026#34;).sum().over($(\u0026#34;w\u0026#34;)), $(\u0026#34;c\u0026#34;).min().over($(\u0026#34;w\u0026#34;))); // aggregate over the over window w Scala val table = input .window([w: OverWindow] as $\u0026#34;w\u0026#34;) // define over window with alias w .select($\u0026#34;a\u0026#34;, $\u0026#34;b\u0026#34;.sum over $\u0026#34;w\u0026#34;, $\u0026#34;c\u0026#34;.min over $\u0026#34;w\u0026#34;) // aggregate over the over window w Python # define over window with alias w and aggregate over the over window w table = input.over_window([w: OverWindow].alias(\u0026#34;w\u0026#34;)) .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;).sum.over(col(\u0026#39;w\u0026#39;)), col(\u0026#39;c\u0026#39;).min.over(col(\u0026#39;w\u0026#39;))) OverWindow 定义了计算聚合的行范围。OverWindow 不是用户可以实现的接口。相反，Table API 提供了Over 类来配置 over window 的属性。可以在事件时间或处理时间以及指定为时间间隔或行计数的范围内定义 over window 。可以通过 Over 类（和其他类）上的方法来定义 over window，具体如下：\nPartition By # 可选的\n在一个或多个属性上定义输入的分区。每个分区单独排序，聚合函数分别应用于每个分区。\n注意：在流环境中，如果窗口包含 partition by 子句，则只能并行计算 over window 聚合。如果没有 partitionBy(\u0026hellip;)，数据流将由单个非并行任务处理。\nOrder By # 必须的\n定义每个分区内行的顺序，从而定义聚合函数应用于行的顺序。\n注意：对于流处理查询，必须声明事件时间或处理时间属性。目前，仅支持单个排序属性。\nPreceding # 可选的\n定义了包含在窗口中并位于当前行之前的行的间隔。间隔可以是时间或行计数间隔。\n有界 over window 用间隔的大小指定，例如，时间间隔为10分钟或行计数间隔为10行。\n无界 over window 通过常量来指定，例如，用UNBOUNDED_RANGE指定时间间隔或用 UNBOUNDED_ROW 指定行计数间隔。无界 over windows 从分区的第一行开始。\n如果省略前面的子句，则使用 UNBOUNDED_RANGE 和 CURRENT_RANGE 作为窗口前后的默认值。\nFollowing # 可选的\n定义包含在窗口中并在当前行之后的行的窗口间隔。间隔必须以与前一个间隔（时间或行计数）相同的单位指定。\n目前，不支持在当前行之后有行的 over window。相反，你可以指定两个常量之一：\nCURRENT_ROW 将窗口的上限设置为当前行。 CURRENT_RANGE 将窗口的上限设置为当前行的排序键，例如，与当前行具有相同排序键的所有行都包含在窗口中。 如果省略后面的子句，则时间间隔窗口的上限定义为 CURRENT_RANGE，行计数间隔窗口的上限定义为CURRENT_ROW。\nAs # 必须的\n为 over window 指定别名。别名用于在之后的 select() 子句中引用该 over window。\n注意：目前，同一个 select() 调用中的所有聚合函数必须在同一个 over window 上计算。\nUnbounded Over Windows # Java // 无界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(UNBOUNDED_RANGE).as(\u0026#34;w\u0026#34;)); // 无界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy(\u0026#34;proctime\u0026#34;).preceding(UNBOUNDED_RANGE).as(\u0026#34;w\u0026#34;)); // 无界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(UNBOUNDED_ROW).as(\u0026#34;w\u0026#34;)); // 无界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(UNBOUNDED_ROW).as(\u0026#34;w\u0026#34;)); Scala // 无界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // 无界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_RANGE as \u0026#34;w\u0026#34;) // 无界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) // 无界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding UNBOUNDED_ROW as \u0026#34;w\u0026#34;) Python # 无界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(UNBOUNDED_RANGE).alias(\u0026#34;w\u0026#34;)) # 无界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(UNBOUNDED_RANGE).alias(\u0026#34;w\u0026#34;)) # 无界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(UNBOUNDED_ROW).alias(\u0026#34;w\u0026#34;)) # 无界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(UNBOUNDED_ROW).alias(\u0026#34;w\u0026#34;)) Bounded Over Windows # Java // 有界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(lit(1).minutes()).as(\u0026#34;w\u0026#34;)); // 有界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(lit(1).minutes()).as(\u0026#34;w\u0026#34;)); // 有界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;rowtime\u0026#34;)).preceding(rowInterval(10)).as(\u0026#34;w\u0026#34;)); // 有界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .window(Over.partitionBy($(\u0026#34;a\u0026#34;)).orderBy($(\u0026#34;proctime\u0026#34;)).preceding(rowInterval(10)).as(\u0026#34;w\u0026#34;)); Scala // 有界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // 有界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 1.minutes as \u0026#34;w\u0026#34;) // 有界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;rowtime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) // 有界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .window(Over partitionBy $\u0026#34;a\u0026#34; orderBy $\u0026#34;proctime\u0026#34; preceding 10.rows as \u0026#34;w\u0026#34;) Python # 有界的事件时间 over window（假定有一个叫“rowtime”的事件时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(lit(1).minutes).alias(\u0026#34;w\u0026#34;)) # 有界的处理时间 over window（假定有一个叫“proctime”的处理时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(lit(1).minutes).alias(\u0026#34;w\u0026#34;)) # 有界的事件时间行数 over window（假定有一个叫“rowtime”的事件时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;rowtime\u0026#39;)).preceding(row_interval(10)).alias(\u0026#34;w\u0026#34;)) # 有界的处理时间行数 over window（假定有一个叫“proctime”的处理时间属性） .over_window(Over.partition_by(col(\u0026#39;a\u0026#39;)).order_by(col(\u0026#39;proctime\u0026#39;)).preceding(row_interval(10)).alias(\u0026#34;w\u0026#34;)) Back to top\nRow-based Operations # 基于行生成多列输出的操作。\nMap # Batch Streaming\nJava 使用用户定义的标量函数或内置标量函数执行 map 操作。如果输出类型是复合类型，则输出将被展平。\npublic class MyMapFunction extends ScalarFunction { public Row eval(String a) { return Row.of(a, \u0026#34;pre-\u0026#34; + a); } @Override public TypeInformation\u0026lt;?\u0026gt; getResultType(Class\u0026lt;?\u0026gt;[] signature) { return Types.ROW(Types.STRING(), Types.STRING()); } } ScalarFunction func = new MyMapFunction(); tableEnv.registerFunction(\u0026#34;func\u0026#34;, func); Table table = input .map(call(\u0026#34;func\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)); Scala 使用用户定义的标量函数或内置标量函数执行 map 操作。如果输出类型是复合类型，则输出将被展平。\nclass MyMapFunction extends ScalarFunction { def eval(a: String): Row = { Row.of(a, \u0026#34;pre-\u0026#34; + a) } override def getResultType(signature: Array[Class[_]]): TypeInformation[_] = Types.ROW(Types.STRING, Types.STRING) } val func = new MyMapFunction() val table = input .map(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) Python 使用 python 的通用标量函数或向量化标量函数执行 map 操作。如果输出类型是复合类型，则输出将被展平。\nfrom pyflink.common import Row from pyflink.table import DataTypes from pyflink.table.udf import udf def map_function(a: Row) -\u0026gt; Row: return Row(a.a + 1, a.b * a.b) # 使用 python 通用标量函数进行 map 操作 func = udf(map_function, result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())])) table = input.map(func).alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) # 使用 python 向量化标量函数进行 map 操作 pandas_func = udf(lambda x: x * 2, result_type=DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]), func_type=\u0026#39;pandas\u0026#39;) table = input.map(pandas_func).alias(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) FlatMap # Batch Streaming\nJava 使用表函数执行 flatMap 操作。\npublic class MyFlatMapFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(String str) { if (str.contains(\u0026#34;#\u0026#34;)) { String[] array = str.split(\u0026#34;#\u0026#34;); for (int i = 0; i \u0026lt; array.length; ++i) { collect(Row.of(array[i], array[i].length())); } } } @Override public TypeInformation\u0026lt;Row\u0026gt; getResultType() { return Types.ROW(Types.STRING(), Types.INT()); } } TableFunction func = new MyFlatMapFunction(); tableEnv.registerFunction(\u0026#34;func\u0026#34;, func); Table table = input .flatMap(call(\u0026#34;func\u0026#34;, $(\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)); Scala 使用表函数执行 flatMap 操作。\nclass MyFlatMapFunction extends TableFunction[Row] { def eval(str: String): Unit = { if (str.contains(\u0026#34;#\u0026#34;)) { str.split(\u0026#34;#\u0026#34;).foreach({ s =\u0026gt; val row = new Row(2) row.setField(0, s) row.setField(1, s.length) collect(row) }) } } override def getResultType: TypeInformation[Row] = { Types.ROW(Types.STRING, Types.INT) } } val func = new MyFlatMapFunction val table = input .flatMap(func($\u0026#34;c\u0026#34;)).as(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) Python 通过 python 表函数执行 flat_map 操作。\nfrom pyflink.table.udf import udtf from pyflink.table import DataTypes from pyflink.common import Row @udtf(result_types=[DataTypes.INT(), DataTypes.STRING()]) def split(x: Row) -\u0026gt; Row: for s in x.b.split(\u0026#34;,\u0026#34;): yield x.a, s input.flat_map(split) Aggregate # Batch Streaming Result\nJava 使用聚合函数来执行聚合操作。你必须使用 select 子句关闭 aggregate，并且 select 子句不支持聚合函数。如果输出类型是复合类型，则聚合的输出将被展平。\npublic class MyMinMaxAcc { public int min = 0; public int max = 0; } public class MyMinMax extends AggregateFunction\u0026lt;Row, MyMinMaxAcc\u0026gt; { public void accumulate(MyMinMaxAcc acc, int value) { if (value \u0026lt; acc.min) { acc.min = value; } if (value \u0026gt; acc.max) { acc.max = value; } } @Override public MyMinMaxAcc createAccumulator() { return new MyMinMaxAcc(); } public void resetAccumulator(MyMinMaxAcc acc) { acc.min = 0; acc.max = 0; } @Override public Row getValue(MyMinMaxAcc acc) { return Row.of(acc.min, acc.max); } @Override public TypeInformation\u0026lt;Row\u0026gt; getResultType() { return new RowTypeInfo(Types.INT, Types.INT); } } AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(\u0026#34;myAggFunc\u0026#34;, myAggFunc); Table table = input .groupBy($(\u0026#34;key\u0026#34;)) .aggregate(call(\u0026#34;myAggFunc\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;x\u0026#34;), $(\u0026#34;y\u0026#34;)); Scala 使用聚合函数来执行聚合操作。你必须使用 select 子句关闭 aggregate，并且 select 子句不支持聚合函数。如果输出类型是复合类型，则聚合的输出将被展平。\ncase class MyMinMaxAcc(var min: Int, var max: Int) class MyMinMax extends AggregateFunction[Row, MyMinMaxAcc] { def accumulate(acc: MyMinMaxAcc, value: Int): Unit = { if (value \u0026lt; acc.min) { acc.min = value } if (value \u0026gt; acc.max) { acc.max = value } } override def createAccumulator(): MyMinMaxAcc = MyMinMaxAcc(0, 0) def resetAccumulator(acc: MyMinMaxAcc): Unit = { acc.min = 0 acc.max = 0 } override def getValue(acc: MyMinMaxAcc): Row = { Row.of(Integer.valueOf(acc.min), Integer.valueOf(acc.max)) } override def getResultType: TypeInformation[Row] = { new RowTypeInfo(Types.INT, Types.INT) } } val myAggFunc = new MyMinMax val table = input .groupBy($\u0026#34;key\u0026#34;) .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;) Python 使用 python 的通用聚合函数或 向量化聚合函数来执行聚合操作。你必须使用 select 子句关闭 aggregate ，并且 select 子句不支持聚合函数。如果输出类型是复合类型，则聚合的输出将被展平。\nfrom pyflink.common import Row from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf class CountAndSumAggregateFunction(AggregateFunction): def get_value(self, accumulator): return Row(accumulator[0], accumulator[1]) def create_accumulator(self): return Row(0, 0) def accumulate(self, accumulator, row: Row): accumulator[0] += 1 accumulator[1] += row.b def retract(self, accumulator, row: Row): accumulator[0] -= 1 accumulator[1] -= row.b def merge(self, accumulator, accumulators): for other_acc in accumulators: accumulator[0] += other_acc[0] accumulator[1] += other_acc[1] def get_accumulator_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.BIGINT())]) function = CountAndSumAggregateFunction() agg = udaf(function, result_type=function.get_result_type(), accumulator_type=function.get_accumulator_type(), name=str(function.__class__.__name__)) # 使用 python 通用聚合函数进行聚合 result = t.group_by(col(\u0026#39;a\u0026#39;)) \\ .aggregate(agg.alias(\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;), col(\u0026#39;d\u0026#39;)) # 使用 python 向量化聚合函数进行聚合 pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) t.aggregate(pandas_udaf.alias(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)) \\ .select(col(\u0026#39;a\u0026#39;), col(\u0026#39;b\u0026#39;)) Group Window Aggregate # Batch Streaming\n在 group window 和可能的一个或多个分组键上对表进行分组和聚合。你必须使用 select 子句关闭 aggregate。并且 select 子句不支持“*\u0026ldquo;或聚合函数。\nJava AggregateFunction myAggFunc = new MyMinMax(); tableEnv.registerFunction(\u0026#34;myAggFunc\u0026#34;, myAggFunc); Table table = input .window(Tumble.over(lit(5).minutes()) .on($(\u0026#34;rowtime\u0026#34;)) .as(\u0026#34;w\u0026#34;)) // 定义窗口 .groupBy($(\u0026#34;key\u0026#34;), $(\u0026#34;w\u0026#34;)) // 以键和窗口分组 .aggregate(call(\u0026#34;myAggFunc\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;x\u0026#34;), $(\u0026#34;y\u0026#34;), $(\u0026#34;w\u0026#34;).start(), $(\u0026#34;w\u0026#34;).end()); // 访问窗口属性与聚合结果 Scala val myAggFunc = new MyMinMax val table = input .window(Tumble over 5.minutes on $\u0026#34;rowtime\u0026#34; as \u0026#34;w\u0026#34;) // 定义窗口 .groupBy($\u0026#34;key\u0026#34;, $\u0026#34;w\u0026#34;) // 以键和窗口分组 .aggregate(myAggFunc($\u0026#34;a\u0026#34;) as (\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;x\u0026#34;, $\u0026#34;y\u0026#34;, $\u0026#34;w\u0026#34;.start, $\u0026#34;w\u0026#34;.end) // 访问窗口属性与聚合结果 Python from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf from pyflink.table.expressions import col, lit from pyflink.table.window import Tumble pandas_udaf = udaf(lambda pd: (pd.b.mean(), pd.b.max()), result_type=DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.FLOAT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.INT())]), func_type=\u0026#34;pandas\u0026#34;) tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) t.select(col(\u0026#39;b\u0026#39;), col(\u0026#39;rowtime\u0026#39;)) \\ .window(tumble_window) \\ .group_by(col(\u0026#34;w\u0026#34;)) \\ .aggregate(pandas_udaf.alias(\u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;)) \\ .select(col(\u0026#39;w\u0026#39;).rowtime, col(\u0026#39;d\u0026#39;), col(\u0026#39;e\u0026#39;)) FlatAggregate # Java 和 GroupBy Aggregation 类似。使用运行中的表之后的聚合算子对分组键上的行进行分组，以按组聚合行。和 AggregateFunction 的不同之处在于，TableAggregateFunction 的每个分组可能返回0或多条记录。你必须使用 select 子句关闭 flatAggregate。并且 select 子句不支持聚合函数。\n除了使用 emitValue 输出结果，你还可以使用 emitUpdateWithRetract 方法。和 emitValue 不同的是，emitUpdateWithRetract 用于下发已更新的值。此方法在retract 模式下增量输出数据，例如，一旦有更新，我们必须在发送新的更新记录之前收回旧记录。如果在表聚合函数中定义了这两个方法，则将优先使用 emitUpdateWithRetract 方法而不是 emitValue 方法，这是因为该方法可以增量输出值，因此被视为比 emitValue 方法更有效。\n/** * Top2 Accumulator。 */ public class Top2Accum { public Integer first; public Integer second; } /** * 用户定义的聚合函数 top2。 */ public class Top2 extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2Accum\u0026gt; { @Override public Top2Accum createAccumulator() { Top2Accum acc = new Top2Accum(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accum acc, Integer v) { if (v \u0026gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v \u0026gt; acc.second) { acc.second = v; } } public void merge(Top2Accum acc, java.lang.Iterable\u0026lt;Top2Accum\u0026gt; iterable) { for (Top2Accum otherAcc : iterable) { accumulate(acc, otherAcc.first); accumulate(acc, otherAcc.second); } } public void emitValue(Top2Accum acc, Collector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { // 下发 value 与 rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)); } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)); } } } tEnv.registerFunction(\u0026#34;top2\u0026#34;, new Top2()); Table orders = tableEnv.from(\u0026#34;Orders\u0026#34;); Table result = orders .groupBy($(\u0026#34;key\u0026#34;)) .flatAggregate(call(\u0026#34;top2\u0026#34;, $(\u0026#34;a\u0026#34;)).as(\u0026#34;v\u0026#34;, \u0026#34;rank\u0026#34;)) .select($(\u0026#34;key\u0026#34;), $(\u0026#34;v\u0026#34;), $(\u0026#34;rank\u0026#34;); Scala 和 GroupBy Aggregation 类似。使用运行中的表之后的聚合运算符对分组键上的行进行分组，以按组聚合行。和 AggregateFunction 的不同之处在于，TableAggregateFunction 的每个分组可能返回0或多条记录。你必须使用 select 子句关闭 flatAggregate。并且 select 子句不支持聚合函数。\n除了使用 emitValue 输出结果，你还可以使用 emitUpdateWithRetract 方法。和 emitValue 不同的是，emitUpdateWithRetract 用于发出已更新的值。此方法在retract 模式下增量输出数据，例如，一旦有更新，我们必须在发送新的更新记录之前收回旧记录。如果在表聚合函数中定义了这两个方法，则将优先使用 emitUpdateWithRetract 方法而不是 emitValue 方法，这是因为该方法可以增量输出值，因此被视为比 emitValue 方法更有效。\nimport java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Top2 Accumulator。 */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ } /** * 用户定义的聚合函数 top2。 */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = { val iter = its.iterator() while (iter.hasNext) { val top2 = iter.next() accumulate(acc, top2.first) accumulate(acc, top2.second) } } def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = { // 下发 value 与 rank if (acc.first != Int.MinValue) { out.collect(JTuple2.of(acc.first, 1)) } if (acc.second != Int.MinValue) { out.collect(JTuple2.of(acc.second, 2)) } } } val top2 = new Top2 val orders: Table = tableEnv.from(\u0026#34;Orders\u0026#34;) val result = orders .groupBy($\u0026#34;key\u0026#34;) .flatAggregate(top2($\u0026#34;a\u0026#34;) as ($\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;)) .select($\u0026#34;key\u0026#34;, $\u0026#34;v\u0026#34;, $\u0026#34;rank\u0026#34;) Python 使用 python 通用 Table Aggregate Function 执行 flat_aggregate 操作。\n和 GroupBy Aggregation 类似。使用运行中的表之后的聚合运算符对分组键上的行进行分组，以按组聚合行。和 AggregateFunction 的不同之处在于，TableAggregateFunction 的每个分组可能返回0或多条记录。你必须使用 select 子句关闭 flat_aggregate。并且 select 子句不支持聚合函数。\nfrom pyflink.common import Row from pyflink.table.udf import TableAggregateFunction, udtaf from pyflink.table import DataTypes from pyflink.table.expressions import col class Top2(TableAggregateFunction): def emit_value(self, accumulator): yield Row(accumulator[0]) yield Row(accumulator[1]) def create_accumulator(self): return [None, None] def accumulate(self, accumulator, row: Row): if row.a is not None: if accumulator[0] is None or row.a \u0026gt; accumulator[0]: accumulator[1] = accumulator[0] accumulator[0] = row.a elif accumulator[1] is None or row.a \u0026gt; accumulator[1]: accumulator[1] = row.a def merge(self, accumulator, accumulators): for other_acc in accumulators: self.accumulate(accumulator, other_acc[0]) self.accumulate(accumulator, other_acc[1]) def get_accumulator_type(self): return DataTypes.ARRAY(DataTypes.BIGINT()) def get_result_type(self): return DataTypes.ROW( [DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.BIGINT())]) mytop = udtaf(Top2()) t = t_env.from_elements([(1, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (3, \u0026#39;Hi\u0026#39;, \u0026#39;hi\u0026#39;), (5, \u0026#39;Hi2\u0026#39;, \u0026#39;hi\u0026#39;), (7, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;), (2, \u0026#39;Hi\u0026#39;, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) result = t.select(col(\u0026#39;a\u0026#39;), col(\u0026#39;c\u0026#39;)) \\ .group_by(col(\u0026#39;c\u0026#39;)) \\ .flat_aggregate(mytop) \\ .select(col(\u0026#39;a\u0026#39;)) \\ .flat_aggregate(mytop.alias(\u0026#34;b\u0026#34;)) For streaming queries the required state to compute the query result might grow infinitely depending on the type of aggregation and the number of distinct grouping keys. Please provide an idle state retention time to prevent excessive state size. See Idle State Retention Time for details. 数据类型 # 请查看数据类型的专门页面。\n行中的字段可以是一般类型和(嵌套)复合类型(比如 POJO、元组、行、 Scala 案例类)。\n任意嵌套的复合类型的字段都可以通过值访问函数来访问。\n用户自定义函数可以将泛型当作黑匣子一样传输和处理。\nBack to top\n"}),e.add({id:276,href:"/flink/flink-docs-master/zh/docs/dev/datastream/execution/parallel/",title:"并行执行",section:"管理执行",content:` 并行执行 # 本节描述了在 Flink 中配置程序的并行执行。一个 Flink 程序由多个任务 task 组成（转换/算子、数据源和数据接收器）。一个 task 包括多个并行执行的实例，且每一个实例都处理 task 输入数据的一个子集。一个 task 的并行实例数被称为该 task 的 并行度 (parallelism)。
使用 savepoints 时，应该考虑设置最大并行度。当作业从一个 savepoint 恢复时，你可以改变特定算子或着整个程序的并行度，并且此设置会限定整个程序的并行度的上限。由于在 Flink 内部将状态划分为了 key-groups，且性能所限不能无限制地增加 key-groups，因此设定最大并行度是有必要的。
toc 设置并行度 # 一个 task 的并行度可以从多个层次指定：
算子层次 # 单个算子、数据源和数据接收器的并行度可以通过调用 setParallelism()方法来指定。如下所示：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;String\u0026gt; text = [...]; DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(value -\u0026gt; value.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1).setParallelism(5); wordCounts.print(); env.execute(\u0026#34;Word Count Example\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment val text = [...] val wordCounts = text .flatMap{ _.split(\u0026#34; \u0026#34;) map { (_, 1) } } .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1).setParallelism(5) wordCounts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() text = [...] word_counts = text .flat_map(lambda x: x.split(\u0026#34; \u0026#34;)) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) \\ .set_parallelism(5) word_counts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) 执行环境层次 # 如此节所描述，Flink 程序运行在执行环境的上下文中。执行环境为所有执行的算子、数据源、数据接收器 (data sink) 定义了一个默认的并行度。可以显式配置算子层次的并行度去覆盖执行环境的并行度。
可以通过调用 setParallelism() 方法指定执行环境的默认并行度。如果想以并行度3来执行所有的算子、数据源和数据接收器。可以在执行环境上设置默认并行度，如下所示：
Java final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(3); DataStream\u0026lt;String\u0026gt; text = [...]; DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = [...]; wordCounts.print(); env.execute(\u0026#34;Word Count Example\u0026#34;); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(3) val text = [...] val wordCounts = text .flatMap{ _.split(\u0026#34; \u0026#34;) map { (_, 1) } } .keyBy(_._1) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1) wordCounts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_parallelism(3) text = [...] word_counts = text .flat_map(lambda x: x.split(\u0026#34; \u0026#34;)) \\ .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \\ .key_by(lambda i: i[0]) \\ .window(TumblingEventTimeWindows.of(Time.seconds(5))) \\ .reduce(lambda i, j: (i[0], i[1] + j[1])) word_counts.print() env.execute(\u0026#34;Word Count Example\u0026#34;) 客户端层次 # 将作业提交到 Flink 时可在客户端设定其并行度。客户端可以是 Java 或 Scala 程序，Flink 的命令行接口（CLI）就是一种典型的客户端。
在 CLI 客户端中，可以通过 -p 参数指定并行度，例如：
./bin/flink run -p 10 ../examples/*WordCount-java*.jar 在 Java/Scala 程序中，可以通过如下方式指定并行度：
Java try { PackagedProgram program = new PackagedProgram(file, args); InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\u0026#34;localhost:6123\u0026#34;); Configuration config = new Configuration(); Client client = new Client(jobManagerAddress, config, program.getUserCodeClassLoader()); // set the parallelism to 10 here client.run(program, 10, true); } catch (ProgramInvocationException e) { e.printStackTrace(); } Scala try { PackagedProgram program = new PackagedProgram(file, args) InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport(\u0026#34;localhost:6123\u0026#34;) Configuration config = new Configuration() Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader()) // set the parallelism to 10 here client.run(program, 10, true) } catch { case e: Exception =\u0026gt; e.printStackTrace } Python Python API 中尚不支持该特性。 系统层次 # 可以通过设置 ./conf/flink-conf.yaml 文件中的 parallelism.default 参数，在系统层次来指定所有执行环境的默认并行度。你可以通过查阅配置文档获取更多细节。
设置最大并行度 # 最大并行度可以在所有设置并行度的地方进行设定（客户端和系统层次除外）。与调用 setParallelism() 方法修改并行度相似，你可以通过调用 setMaxParallelism() 方法来设定最大并行度。
默认的最大并行度等于将 operatorParallelism + (operatorParallelism / 2) 值四舍五入到大于等于该值的一个整型值，并且这个整型值是 2 的幂次方，注意默认最大并行度下限为 128，上限为 32768。
注意 为最大并行度设置一个非常大的值将会降低性能，因为一些 state backends 需要维持内部的数据结构，而这些数据结构将会随着 key-groups 的数目而扩张（key-group 是状态重新分配的最小单元）。
Back to top
`}),e.add({id:277,href:"/flink/flink-docs-master/zh/docs/dev/python/table/udfs/vectorized_python_udfs/",title:"向量化自定义函数",section:"自定义函数",content:` 向量化自定义函数 # 向量化 Python 用户自定义函数，是在执行时，通过在 JVM 和 Python VM 之间以 Arrow 列存格式批量传输数据，来执行的函数。 向量化 Python 用户自定义函数的性能通常比非向量化 Python 用户自定义函数要高得多， 因为向量化 Python 用户自定义函数可以大大减少序列化/反序列化的开销和调用开销。 此外，用户可以利用流行的 Python 库（例如 Pandas，Numpy 等）来实现向量化 Python 用户自定义函数的逻辑。 这些 Python 库通常经过高度优化，并提供了高性能的数据结构和功能。 向量化用户自定义函数的定义，与非向量化用户自定义函数具有相似的方式， 用户只需要在调用 udf 或者 udaf 装饰器时添加一个额外的参数 func_type=\u0026quot;pandas\u0026quot;，将其标记为一个向量化用户自定义函数即可。
注意: 要执行 Python 向量化自定义函数，客户端和集群端都需要安装 Python 3.6 以上版本(3.6、3.7 或 3.8)，并安装 PyFlink。
向量化标量函数 # 向量化 Python 标量函数以 pandas.Series 类型的参数作为输入，并返回与输入长度相同的 pandas.Series。 在内部实现中，Flink 会将输入数据拆分为多个批次，并将每一批次的输入数据转换为 Pandas.Series 类型， 然后为每一批输入数据调用用户自定义的向量化 Python 标量函数。请参阅配置选项 python.fn-execution.arrow.batch.size， 以获取有关如何配置批次大小的更多详细信息。
向量化 Python 标量函数可以在任何可以使用非向量化 Python 标量函数的地方使用。
以下示例显示了如何定义自己的向量化 Python 标量函数，该函数计算两列的总和，并在查询中使用它：
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col from pyflink.table.udf import udf @udf(result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) def add(i, j): return i + j settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) # use the vectorized Python scalar function in Python Table API my_table.select(add(col(\u0026#34;bigint\u0026#34;), col(\u0026#34;bigint\u0026#34;))) # 在SQL API中使用Python向量化标量函数 table_env.create_temporary_function(\u0026#34;add\u0026#34;, add) table_env.sql_query(\u0026#34;SELECT add(bigint, bigint) FROM MyTable\u0026#34;) 向量化聚合函数 # 向量化 Python 聚合函数以一个或多个 pandas.Series 类型的参数作为输入，并返回一个标量值作为输出。
注意 现在返回类型还不支持 RowType 和 MapType。
向量化 Python 聚合函数能够用在 GroupBy Aggregation（Batch），GroupBy Window Aggregation(Batch and Stream) 和 Over Window Aggregation(Batch and Stream bounded over window)。关于聚合的更多使用细节，你可以参考 相关文档.
注意 向量化聚合函数不支持部分聚合，而且一个组或者窗口内的所有数据， 在执行的过程中，会被同时加载到内存，所以需要确保所配置的内存大小足够容纳这些数据。
以下示例显示了如何定一个自己的向量化聚合函数，该函数计算一列的平均值，并在 GroupBy Aggregation, GroupBy Window Aggregation and Over Window Aggregation 使用它:
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings from pyflink.table.expressions import col, lit from pyflink.table.udf import udaf from pyflink.table.window import Tumble @udaf(result_type=DataTypes.FLOAT(), func_type=\u0026#34;pandas\u0026#34;) def mean_udaf(v): return v.mean() settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.create(settings) my_table = ... # type: Table, table schema: [a: String, b: BigInt, c: BigInt] # 在 GroupBy Aggregation 中使用向量化聚合函数 my_table.group_by(col(\u0026#39;a\u0026#39;)).select(col(\u0026#39;a\u0026#39;), mean_udaf(col(\u0026#39;b\u0026#39;))) # 在 GroupBy Window Aggregation 中使用向量化聚合函数 tumble_window = Tumble.over(lit(1).hours) \\ .on(col(\u0026#34;rowtime\u0026#34;)) \\ .alias(\u0026#34;w\u0026#34;) my_table.window(tumble_window) \\ .group_by(col(\u0026#34;w\u0026#34;)) \\ .select(col(\u0026#39;w\u0026#39;).start, col(\u0026#39;w\u0026#39;).end, mean_udaf(col(\u0026#39;b\u0026#39;))) # 在 Over Window Aggregation 中使用向量化聚合函数 table_env.create_temporary_function(\u0026#34;mean_udaf\u0026#34;, mean_udaf) table_env.sql_query(\u0026#34;\u0026#34;\u0026#34; SELECT a, mean_udaf(b) over (PARTITION BY a ORDER BY rowtime ROWS BETWEEN UNBOUNDED preceding AND UNBOUNDED FOLLOWING) FROM MyTable\u0026#34;\u0026#34;\u0026#34;) 除了直接定义一个 Python 函数之外，还支持多种方式来定义向量化 Python 聚合函数。 以下示例显示了多种定义向量化 Python 聚合函数的方式。该函数需要两个类型为 bigint 的参数作为输入参数，并返回它们的最大值的和作为结果。
from pyflink.table import DataTypes from pyflink.table.udf import AggregateFunction, udaf # 方式一：扩展基类 \`AggregateFunction\` class MaxAdd(AggregateFunction): def open(self, function_context): mg = function_context.get_metric_group() self.counter = mg.add_group(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;).counter(\u0026#34;my_counter\u0026#34;) self.counter_sum = 0 def get_value(self, accumulator): # counter self.counter.inc(10) self.counter_sum += 10 return accumulator[0] def create_accumulator(self): return [] def accumulate(self, accumulator, *args): result = 0 for arg in args: result += arg.max() accumulator.append(result) max_add = udaf(MaxAdd(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # 方式二：普通 Python 函数 @udaf(result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) def max_add(i, j): return i.max() + j.max() # 方式三：lambda 函数 max_add = udaf(lambda i, j: i.max() + j.max(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # 方式四：callable 函数 class CallableMaxAdd(object): def __call__(self, i, j): return i.max() + j.max() max_add = udaf(CallableMaxAdd(), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) # 方式五：partial 函数 def partial_max_add(i, j, k): return i.max() + j.max() + k max_add = udaf(functools.partial(partial_max_add, k=1), result_type=DataTypes.BIGINT(), func_type=\u0026#34;pandas\u0026#34;) `}),e.add({id:278,href:"/flink/flink-docs-master/zh/docs/dev/table/sql/",title:"SQL",section:"Table API \u0026 SQL",content:""}),e.add({id:279,href:"/flink/flink-docs-master/zh/docs/dev/python/table/python_types/",title:"数据类型",section:"Table API",content:` 数据类型 # 本节描述PyFlink Table API中所支持的数据类型.
Data Type # 在Table生态系统中，数据类型用于描述值的逻辑类型。它可以用来声明Python用户自定义函数的输入／输出类型。 Python Table API的用户可以在Python Table API中，或者定义Python用户自定义函数时，使用pyflink.table.types.DataType实例。
DataType实例声明了数据的逻辑类型，这并不能用于推断数据在进行传输或存储时的具体物理表示形式。 所有预定义的数据类型都位于pyflink.table.types中，并且可以通过类pyflink.table.types.DataTypes中所定义的方法创建。
可以在下面找到所有预定义数据类型的列表。
数据类型（Data Type）和Python类型的映射关系 # 数据类型可用于声明Python用户自定义函数的输入/输出类型。输入数据将被转换为与所定义的数据类型相对应的Python对象，用户自定义函数的执行结果的类型也必须与所定义的数据类型匹配。
对于向量化Python UDF，输入类型和输出类型都为pandas.Series。pandas.Series中的元素类型对应于指定的数据类型。
Data Type Python Type Pandas Type BOOLEAN bool numpy.bool_ TINYINT int numpy.int8 SMALLINT int numpy.int16 INT int numpy.int32 BIGINT int numpy.int64 FLOAT float numpy.float32 DOUBLE float numpy.float64 VARCHAR str str VARBINARY bytes bytes DECIMAL decimal.Decimal decimal.Decimal DATE datetime.date datetime.date TIME datetime.time datetime.time TimestampType datetime.datetime datetime.datetime LocalZonedTimestampType datetime.datetime datetime.datetime INTERVAL YEAR TO MONTH int Not Supported Yet INTERVAL DAY TO SECOND datetime.timedelta Not Supported Yet ARRAY list numpy.ndarray MULTISET list Not Supported Yet MAP dict Not Supported Yet ROW Row dict `}),e.add({id:280,href:"/flink/flink-docs-master/zh/docs/dev/python/table/system_functions/",title:"系统（内置）函数",section:"Table API",content:" "}),e.add({id:281,href:"/flink/flink-docs-master/zh/docs/dev/table/functions/systemfunctions/",title:"系统（内置）函数",section:"函数",content:" 系统（内置）函数 # Flink Table API \u0026amp; SQL 为用户提供了一组内置的数据转换函数。本页简要介绍了它们。如果你需要的函数尚不支持，你可以实现 用户自定义函数。如果你觉得这个函数够通用，请 创建一个 Jira issue并详细 说明。\n标量函数 # 标量函数将零、一个或多个值作为输入并返回单个值作为结果。\n比较函数 # SQL 函数 Table 函数 描述 value1 = value2 value1 === value2 如果 value1 等于 value2 返回 TRUE；如果 value1 或者 value2 为 NULL 返回 UNKNOW。 value1 \u0026lt;\u0026gt; value2 value1 !== value2 如果 value1 不等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 \u0026gt; value2 value1 \u0026gt; value2 如果 value1 大于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 \u0026gt;= value2 value1 \u0026gt;= value2 如果 value1 大于或等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 \u0026lt; value2 value1 \u0026lt; value2 如果 value1 小于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value1 \u0026lt;= value2 value1 \u0026lt;= value2 如果 value1 小于或等于 value2 返回 TRUE；如果 value1 或 value2 为 NULL 返回 UNKNOWN。 value IS NULL value.isNull 如果值为 NULL 返回 TRUE。 value IS NOT NULL value.isNotNull 如果值不为 NULL 返回 TRUE。 value1 IS DISTINCT FROM value2 不适用 A 和 B 的数据类型和值不完全相同返回 TRUE。A 和 B 的数据类型和值都相同返回 FALSE。将 NULL 视为相同。 例如 1 IS DISTINCT FROM NULL 返回 TRUE；NULL IS DISTINCT FROM NULL 返回 FALSE。 value1 IS NOT DISTINCT FROM value2 不适用 A 和 B 的数据类型和值都相同返回 TRUE。A 和 B 的数据类型和值不完全相同则返回 FALSE。将 NULL 视为相同。 例如 1 IS NOT DISTINCT FROM NULL 返回 FALSE；NULL IS NOT DISTINCT FROM NULL 返回 TRUE。 value1 BETWEEN [ ASYMMETRIC | SYMMETRIC ] value2 AND value3 不适用 默认或使用 ASYMMETRIC 关键字的情况下，如果 value1 大于等于 value2 且小于等于 value3 返回 TRUE。 使用 SYMMETRIC 关键字则 value1 在 value2 和 value3 之间返回 TRUE。 当 value2 或 value3 为 NULL 时，返回 FALSE 或 UNKNOWN。 例如 12 BETWEEN 15 AND 12 返回 FALSE； 12 BETWEEN SYMMETRIC 15 AND 12 返回 TRUE； 12 BETWEEN 10 AND NULL 返回 UNKNOWN； 12 BETWEEN NULL AND 10 返回 FALSE； 12 BETWEEN SYMMETRIC NULL AND 12 返回 UNKNOWN。 value1 NOT BETWEEN [ ASYMMETRIC | SYMMETRIC ] value2 AND value3 不适用 默认或使用 ASYMMETRIC 关键字的情况下，如果 value1 小于 value2 或大于 value3，则返回 TRUE。 使用 SYMMETRIC 关键字则 value1 不在 value2 和 value3 之间返回 TRUE。 当 value2 或 value3 为 NULL 时，返回 TRUE 或 UNKNOWN。 例如 12 NOT BETWEEN 15 AND 12 返回 TRUE； 12 NOT BETWEEN SYMMETRIC 15 AND 12 返回 FALSE； 12 NOT BETWEEN NULL AND 15 返回 UNKNOWN； 12 NOT BETWEEN 15 AND NULL 返回 TRUE； 12 NOT BETWEEN SYMMETRIC 12 AND NULL 返回 UNKNOWN。 string1 LIKE string2 [ ESCAPE char ] string1.like(string2) 如果 string1 匹配 string2 返回 TRUE；如果 string1 或 string2 为 NULL 返回 UNKNOWN。 如果需要可以定义转义字符。尚不支持转义字符。 string1 NOT LIKE string2 [ ESCAPE char ] 不适用 如果 string1 与 string2 不匹配返回 TRUE；如果 string1 或 string2 为 NULL 返回 UNKNOWN。 如果需要可以定义转义字符。尚不支持转义字符。 string1 SIMILAR TO string2 [ ESCAPE char ] string1.similar(string2) 如果 string1 匹配 SQL 正则表达式 string2 返回 TRUE；如果 string1 或 string2 为 NULL 返回 UNKNOWN。如果需要可以定义转义字符。尚不支持转义字符。 string1 NOT SIMILAR TO string2 [ ESCAPE char ] 不适用 如果 string1 与 SQL 正则表达式 string2 不匹配返回 TRUE；如果 string1 或 string2 为 NULL 返回 UNKNOWN。如果需要可以定义转义字符。尚不支持转义字符。 value1 IN (value2 [, value3]* ) value1.in(valu2) 在给定列表 (value2, value3, \u0026hellip;) 中存在 value1 返回 TRUE。当列表包含 NULL，如果可以找到 value1 则返回 TRUE，否则返回 UNKNOWN。如果 value1 为 NULL 则始终返回 UNKNOWN。例如 4 IN (1, 2, 3) 返回 FALSE；1 IN (1, 2, NULL) 返回 TRUE；4 IN (1, 2, NULL) 返回 UNKNOWN。 value1 NOT IN (value2 [, value3]* ) 不适用 在给定列表 (value2, value3, \u0026hellip;) 中不存在 value1 返回 TRUE。当列表包含 NULL，如果可以找到 value1 则 返回 FALSE，否则返回 UNKNOWN。如果 value1 为 NULL，则始终返回 UNKNOWN。例如 4 NOT IN (1, 2, 3) 返回 TRUE；1 NOT IN (1, 2, NULL) 返回 FALSE；4 NOT IN (1, 2, NULL) 返回 UNKNOWN。 EXISTS (sub-query) 不适用 如果子查询至少返回一行则返回 TRUE。 仅支持可以在 join 和分组操作中可以被重写的操作。对于流式查询，该操作在 join 和分组操作中被重写。根据输入行的数量计算查询结果所需的状态可能会无限增长。请提供具有有效保留间隔的查询配置，以防止状态过大。 value IN (sub-query) value1.in(TABLE) 如果 value 等于子查询结果集中的一行则返回 TRUE。 value NOT IN (sub-query) 不适用 如果 value 不包含于子查询返回的行则返回 TRUE。 不适用 value1.between(value2, value3) 如果 value1 大于或等于 value2 且小于或等于 value3 返回 TRUE。当 value2 或 value3 为 NULL 时， 返回 FALSE 或 UNKNOWN。 不适用 value1.notBetween(value2, value3) 如果 value1 大于或等于 value2 且小于或等于 value3 返回 FALSE。当 value2 或 value3 为 NULL 时， 返回 TRUE 或 UNKNOWN。 逻辑函数 # SQL 函数 Table 函数 描述 boolean1 OR boolean2 BOOLEAN1 || BOOLEAN2 如果 boolean1 为 TRUE 或 boolean2 为 TRUE 返回 TRUE。支持三值逻辑。 例如 true || Null(BOOLEAN) 返回 TRUE。 boolean1 AND boolean2 BOOLEAN1 \u0026amp;\u0026amp; BOOLEAN2 如果 boolean1 和 boolean2 都为 TRUE 返回 TRUE。支持三值逻辑。 例如 true \u0026amp;\u0026amp; Null(BOOLEAN) 返回 UNKNOWN。 NOT boolean BOOLEAN.not(), not(BOOLEAN), or \u0026#39;!BOOLEAN\u0026#39; (Scala only) 如果布尔值为 FALSE 返回 TRUE；如果布尔值为 TRUE 返回 FALSE；如果布尔值为 UNKNOWN 返回 UNKNOWN。 boolean IS FALSE BOOLEAN.isFalse 如果布尔值为 FALSE 返回 TRUE；如果 boolean 为 TRUE 或 UNKNOWN 返回 FALSE。 boolean IS NOT FALSE BOOLEAN.isNotFalse 如果 boolean 为 TRUE 或 UNKNOWN 返回 TRUE；如果 boolean 为 FALSE 返回 FALSE。 boolean IS TRUE BOOLEAN.isTrue 如果 boolean 为 TRUE 返回 TRUE；如果 boolean 为 FALSE 或 UNKNOWN 返回 FALSE。 boolean IS NOT TRUE BOOLEAN.isNotTrue 如果 boolean 为 FALSE 或 UNKNOWN 返回 TRUE；如果布尔值为 TRUE 返回 FALSE。 boolean IS UNKNOWN 不适用 如果布尔值为 UNKNOWN 返回 TRUE；如果 boolean 为 TRUE 或 FALSE 返回 FALSE。 boolean IS NOT UNKNOWN 不适用 如果 boolean 为 TRUE 或 FALSE 返回 TRUE；如果布尔值为 UNKNOWN 返回 FALSE。 算术函数 # SQL 函数 Table 函数 描述 \u0026#43; numeric \u0026#43; NUMERIC 返回 numeric。 - numeric - NUMERIC 返回 numeric 的相反数。 numeric1 \u0026#43; numeric2 NUMERIC1 \u0026#43; NUMERIC2 返回 numeric1 加 numeric2。 numeric1 - numeric2 NUMERIC1 - NUMERIC2 返回 numeric1 减 numeric2。 numeric1 * numberic2 NUMERIC1 * NUMERIC2 返回 numeric1 乘以 numeric2。 numeric1 / numeric2 NUMERIC1 / NUMERIC2 返回 numeric1 除以 numeric2。 numeric1 % numeric2 MOD(NUMERIC1, NUMERIC2) 返回 numeric1 除以 numeric2 的余数（模数）。仅当 numeric1 为负时，结果才为负。 POWER(numeric1, numeric2) NUMERIC1.power(NUMERIC2) 返回 numeric1 的 numeric2 次方。 ABS(numeric) NUMERIC.abs() 返回 numeric 的绝对值。 SQRT(numeric) NUMERIC.sqrt() 返回 numeric 的平方根。 LN(numeric) NUMERIC.ln() 返回 numeric 的自然对数（以 e 为底）。 LOG10(numeric) NUMERIC.log10() 返回以 10 为底的 numeric 的对数。 LOG2(numeric) NUMERIC.log2() 返回以 2 为底的 numeric 的对数。 LOG(numeric2) LOG(numeric1, numeric2) NUMERIC1.log() NUMERIC1.log(NUMERIC2) 当用一个参数调用时，返回 numeric2 的自然对数。当使用两个参数调用时，此函数返回 numeric2 以 numeric1 为底的对数。numeric2 必须大于 0，numeric1 必须大于 1。 EXP(numeric) NUMERIC.exp() 返回 e 的 numeric 次幂。 CEIL(numeric) CEILING(numeric) NUMERIC.ceil() NUMERIC.ceiling() 向上取整，并返回大于或等于 numeric 的最小整数。 FLOOR(numeric) NUMERIC.floor() 向下取整，并返回小于或等于 numeric 的最大整数。 SIN(numeric) NUMERIC.sin() 返回 numeric 的正弦值。 SINH(numeric) NUMERIC.sinh() 返回 numeric 的双曲正弦值。返回类型为 DOUBLE。 COS(numeric) NUMERIC.cos() 返回 numeric 的余弦值。 TAN(numeric) NUMERIC.tan() 返回 numeric 的正切值。 TANH(numeric) NUMERIC.tanh() 返回 numeric 的双曲正切值。返回类型为 DOUBLE。 COT(numeric) NUMERIC.cot() 返回 numeric 的余切值。 ASIN(numeric) NUMERIC.asin() 返回 numeric 的反正弦值。 ACOS(numeric) NUMERIC.acos() 返回 numeric 的反余弦值。 ATAN(numeric) NUMERIC.atan() 返回 numeric 的反正切值。 ATAN2(numeric1, numeric2) atan2(NUMERIC1, NUMERIC2) 返回坐标 (numeric1, numeric2) 的反正切。 COSH(numeric) NUMERIC.cosh() 返回 numeric 的双曲余弦值。返回值类型为 DOUBLE。 DEGREES(numeric) NUMERIC.degrees() 返回弧度 numeric 的度数表示。 RADIANS(numeric) NUMERIC.radians() 返回度数 numeric 的弧度表示。 SIGN(numeric) NUMERIC.sign() 返回 numeric 的符号。 ROUND(numeric, INT) NUMERIC.round(INT) 返回 numeric 四舍五入保留 INT 小数位的值。 PI() pi() 返回无比接近 pi 的值。 E() e() 返回无比接近 e 的值。 RAND() rand() 返回 [0.0, 1.0) 范围内的伪随机双精度值。 RAND(INT) rand(INT) 返回范围为 [0.0, 1.0) 的伪随机双精度值，初始种子为 INT。 如果两个 RAND 函数具有相同的初始种子，它们将返回相同的数字序列。 RAND_INTEGER(INT) randInteger(INT) 返回 [0, INT) 范围内的伪随机整数。 RAND_INTEGER(INT1, INT2) randInteger(INT1, INT2) 返回范围为 [0, INT2) 的伪随机整数，初始种子为 INT1。 如果两个 RAND_INTGER 函数具有相同的初始种子和边界，它们将返回相同的数字序列。 UUID() uuid() 根据 RFC 4122 类型 4（伪随机生成）UUID，返回 UUID（通用唯一标识符）字符串。 例如“3d3c68f7-f608-473f-b60c-b0c44ad4cc4e”，UUID 是使用加密强的伪随机数生成器生成的。 BIN(INT) INT.bin() 以二进制格式返回 INTEGER 的字符串表示形式。如果 INTEGER 为 NULL，则返回 NULL。 例如 4.bin() 返回“100”，12.bin() 返回“1100”。 HEX(numeric) HEX(string) NUMERIC.hex() string.hex() 以十六进制格式返回整数 numeric 值或 STRING 的字符串表示形式。如果参数为 NULL，则返回 NULL。 例如数字 20 返回“14”，数字 100 返回“64”，字符串“hello,world” 返回“68656C6C6F2C776F726C64”。 TRUNCATE(numeric1, integer2) NUMERIC1.truncate(INTEGER2) 返回截取 integer2 位小数的数字。如果 numeric1 或 integer2 为 NULL，则返回 NULL。 如果 integer2 为 0，则结果没有小数点或小数部分。integer2 可以为负数，使值的小数点左边的 integer2 位变为零。 此函数也可以传入只有一个 numeric1 参数且不设置 Integer2 以使用。如果未设置 Integer2 则 Integer2 为 0。 例如 42.324.truncate(2) 为 42.32，42.324.truncate() 为 42.0。 字符串函数 # SQL 函数 Table 函数 描述 string1 || string2 STRING1 \u0026#43; STRING2 返回 STRING1 和 STRING2 的连接。 CHAR_LENGTH(string) CHARACTER_LENGTH(string) STRING.charLength() 返回字符串中的字符数。 UPPER(string) STRING.upperCase() 以大写形式返回字符串。 LOWER(string) STRING.lowerCase() 以小写形式返回字符串。 POSITION(string1 IN string2) STRING1.position(STRING2) 返回 STRING2 中第一次出现 STRING1 的位置（从 1 开始）；如果在 STRING2 中找不到 STRING1 返回 0。 TRIM([ BOTH | LEADING | TRAILING ] string1 FROM string2) STRING1.trim(LEADING, STRING2) STRING1.trim(TRAILING, STRING2) STRING1.trim(BOTH, STRING2) STRING1.trim(BOTH) STRING1.trim() 返回从 STRING1 中删除以字符串 STRING2 开头/结尾/开头且结尾的字符串的结果。默认情况下，两边的空格都会被删除。 LTRIM(string) STRING.ltrim() 返回从 STRING 中删除左边空格的字符串。 例如 ' This is a test String.'.ltrim() 返回 'This is a test String.'。 RTRIM(string) STRING.rtrim() 返回从 STRING 中删除右边空格的字符串。 例如 'This is a test String. '.ltrim() 返回 'This is a test String.'。 REPEAT(string, int) STRING.repeat(INT) 返回 INT 个 string 连接的字符串。 例如 REPEAT('This is a test String.', 2) 返回 \u0026quot;This is a test String.This is a test String.\u0026quot;。 REGEXP_REPLACE(string1, string2, string3) STRING1.regexpReplace(STRING2, STRING3) 返回 STRING1 所有与正则表达式 STRING2 匹配的子字符串被 STRING3 替换后的字符串。 例如 'foobar'.regexpReplace('oo|ar', '') 返回 \u0026quot;fb\u0026quot;。 OVERLAY(string1 PLACING string2 FROM integer1 [ FOR integer2 ]) STRING1.overlay(STRING2, INT1) STRING1.overlay(STRING2, INT1, INT2) 返回一个字符串，该字符串从位置 INT1 用 STRING2 替换 STRING1 的 INT2（默认为 STRING2 的长度）字符。 例如 'xxxxxtest'.overlay('xxxx', 6) 返回 \u0026quot;xxxxxxxxx\u0026quot;； 'xxxxxtest'.overlay('xxxx', 6, 2) 返回 \u0026quot;xxxxxxxxxst\u0026quot;。 SUBSTRING(string FROM integer1 [ FOR integer2 ]) STRING.substring(INT1) STRING.substring(INT1, INT2) 返回 STRING 从位置 INT1 开始，长度为 INT2（默认到结尾）的子字符串。 REPLACE(string1, string2, string3) STRING1.replace(STRING2, STRING3) 返回一个新字符串，它用 STRING1 中的 STRING3（非重叠）替换所有出现的 STRING2。 例如 'hello world'.replace('world', 'flink') 返回 'hello flink'； 'ababab'.replace('abab', 'z') 返回 'zab'。 REGEXP_EXTRACT(string1, string2[, integer]) STRING1.regexpExtract(STRING2[, INTEGER1]) 将字符串 STRING1 按照 STRING2 正则表达式的规则拆分，返回指定 INTEGER1 处位置的字符串。正则表达式匹配组索引从 1 开始， 0 表示匹配整个正则表达式。此外，正则表达式匹配组索引不应超过定义的组数。 例如 REGEXP_EXTRACT('foothebar', 'foo(.*?)(bar)', 2) 返回 \u0026quot;bar\u0026quot;。 INITCAP(string) STRING.initCap() 返回新形式的 STRING，其中每个单词的第一个字符转换为大写，其余字符转换为小写。这里的单词表示字母数字的字符序列。 CONCAT(string1, string2, ...) concat(STRING1, STRING2, ...) 返回连接 string1，string2， \u0026hellip; 的字符串。如果有任一参数为 NULL，则返回 NULL。 例如 CONCAT('AA', 'BB', 'CC') 返回 \u0026quot;AABBCC\u0026quot;。 CONCAT_WS(string1, string2, string3, ...) concat_ws(STRING1, STRING2, STRING3, ...) 返回将 STRING2， STRING3， \u0026hellip; 与分隔符 STRING1 连接起来的字符串。在要连接的字符串之间添加分隔符。 如果 STRING1 为 NULL，则返回 NULL。与 concat() 相比，concat_ws() 会自动跳过 NULL 参数。 例如 concat_ws('~', 'AA', Null(STRING), 'BB', '', 'CC') 返回 \u0026quot;AA~BB~~CC\u0026quot;. LPAD(string1, integer, string2) STRING1.lpad(INT, STRING2) 返回从 string1 靠左填充 string2 到 INT 长度的新字符串。如果 string1 的长度小于 INT 值，则返回 string1 缩 短为整数字符。例如 LPAD('hi', 4, '??') 返回 \u0026quot;??hi\u0026quot;；LPAD('hi', 1, '??') 返回 `\u0026ldquo;h\u0026rdquo;。 RPAD(string1, integer, string2) STRING1.rpad(INT, STRING2) 返回从 string1 靠右边填充 string2 到 INT 长度的新字符串。如果 string1 的长度小于 INT 值，则返回 string1 缩 短为长度为 INT 的新字符串。例如 RPAD('hi', 4, '??') 返回 \u0026quot;hi??\u0026quot;, RPAD('hi', 1, '??') 返回 \u0026quot;h\u0026quot;。 FROM_BASE64(string) STRING.fromBase64() 返回字符串 string1 的 base64 解码的结果；如果字符串为 NULL，则返回 NULL。 例如 FROM_BASE64('aGVsbG8gd29ybGQ=') 返回 \u0026quot;hello world\u0026quot;。 TO_BASE64(string) STRING.toBase64() 返回字符串 string 的 base64 编码的结果；如果字符串为 NULL，则返回 NULL。 例如 TO_BASE64('hello world') 返回 \u0026quot;aGVsbG8gd29ybGQ=\u0026quot;。 ASCII(string) STRING.ascii() 返回字符串 string 第一个字符的数值。如果字符串为 NULL 则返回 NULL。 例如 ascii('abc') 返回 97，ascii(CAST(NULL AS VARCHAR)) 返回 NULL。 CHR(integer) INT.chr() 返回二进制等于 integer 的 ASCII 字符。如果整数 integer 大于 255，我们先将得到整数对 255 取模数， 并返回模数的 CHR。如果整数为 NULL，则返回 NULL。例如 chr(97) 返回 a，chr(353) 返回 a， ascii(CAST(NULL AS VARCHAR)) 返回 NULL。 DECODE(binary, string) BINARY.decode(STRING) 使用提供的字符集（\u0026lsquo;US-ASCII\u0026rsquo;，\u0026lsquo;ISO-8859-1\u0026rsquo;，\u0026lsquo;UTF-8\u0026rsquo;，\u0026lsquo;UTF-16BE\u0026rsquo;，\u0026lsquo;UTF-16LE\u0026rsquo;，\u0026lsquo;UTF-16\u0026rsquo;）解码。 如果任一参数为空，则结果也将为空。 ENCODE(string1, string2) STRING1.encode(STRING2) 使用提供的字符集（\u0026lsquo;US-ASCII\u0026rsquo;，\u0026lsquo;ISO-8859-1\u0026rsquo;，\u0026lsquo;UTF-8\u0026rsquo;，\u0026lsquo;UTF-16BE\u0026rsquo;，\u0026lsquo;UTF-16LE\u0026rsquo;，\u0026lsquo;UTF-16\u0026rsquo;）编码。 如果任一参数为空，则结果也将为空。 INSTR(string1, string2) STRING1.instr(STRING2) 返回 string2 在 string1 中第一次出现的位置。如果有任一参数为 NULL，则返回 NULL。 LEFT(string, integer) STRING.LEFT(INT) 返回字符串中最左边的长度为 integer 值的字符串。如果 integer 为负，则返回 EMPTY 字符串。如果有任一参数 为 NULL 则返回 NULL。 RIGHT(string, integer) STRING.RIGHT(INT) 返回字符串中最右边的长度为 integer 值的字符串。如果 integer 为负，则返回 EMPTY 字符串。如果有任一参数 为 NULL 则返回 NULL。 LOCATE(string1, string2[, integer]) STRING1.locate(STRING2[, INTEGER]) 返回 string2 中 string1 在位置 integer 之后第一次出现的位置。未找到返回 0。如果有任一参数为 NULL 则返回 NULL。 PARSE_URL(string1, string2[, string3]) STRING1.parseUrl(STRING2[, STRING3]) 从 URL 返回指定的部分。string2 的有效值包括“HOST”，“PATH”，“QUERY”，“REF”，“PROTOCOL”，“AUTHORITY”，“FILE”和“USERINFO”。 如果有任一参数为 NULL，则返回 NULL。例如 parse_url('http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1', 'HOST') 返回 'facebook.com'。 还可以通过提供关键词 string3 作为第三个参数来提取 QUERY 中特定键的值。例如 parse_url('http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1', 'QUERY', 'k1') 返回 'v1'。 REGEXP(string1, string2) STRING1.regexp(STRING2) 如果 string1 的任何（可能为空）子字符串与 Java 正则表达式 string2 匹配，则返回 TRUE，否则返回 FALSE。 如果有任一参数为 NULL，则返回 NULL。 REVERSE(string) STRING.reverse() 返回反转的字符串。如果字符串为 NULL，则返回 NULL。 SPLIT_INDEX(string1, string2, integer1) STRING1.splitIndex(STRING2, INTEGER1) 通过分隔符 string2 拆分 string1，返回分隔后这组字符串的第 integer（从零开始）个字符串。如果整数为负，则返回 NULL。 如果有任一参数为 NULL，则返回 NULL。 STR_TO_MAP(string1[, string2, string3]) STRING1.strToMap([STRING2, STRING3]) 使用分隔符将 string1 拆分为键值对后返回一个 map。string2 是 pair 分隔符，默认为 \u0026lsquo;,\u0026rsquo;。string3 是键值分隔符，默认为 \u0026lsquo;=\u0026rsquo;。 pair 分隔符与键值分隔符均为正则表达式，当使用特殊字符作为分隔符时请提前进行转义，例如 \u0026lt;([{\\^-=$!|]})?*+.\u0026gt;。 SUBSTR(string, integer1[, integer2]) STRING.substr(INTEGER1[, INTEGER2]) 返回字符串的子字符串，从位置 integer1 开始，长度为 integer2（默认到末尾）。 时间函数 # SQL 函数 Table 函数 描述 DATE string STRING.toDate() 以“yyyy-MM-dd”的形式返回从字符串解析的 SQL 日期。 TIME string STRING.toTime() 以“HH:mm:ss”的形式返回从字符串解析的 SQL 时间。 TIMESTAMP string STRING.toTimestamp() 以“yyyy-MM-dd HH:mm:ss[.SSS]”的形式返回从字符串解析的 SQL 时间戳。 INTERVAL string range 不适用 从“dd hh:mm:ss.fff”形式的字符串解析 SQL 毫秒间隔或者从“yyyy-mm”形式的字符串解析 SQL 月数间隔。间隔范围可以 是 DAY，MINUTE，DAY TO HOUR 或 DAY TO SECOND，以毫秒为间隔；YEAR 或 YEAR TO MONTH 表示几个月的间隔。例 如 INTERVAL '10 00:00:00.004' DAY TO SECOND，INTERVAL '10' DAY 或 INTERVAL '2-10' YEAR TO MONTH 返回间隔。 不适用 numeric.year | numeric.years 创建 numeric 年的月间隔。 不适用 numeric.quarter | numeric.quarters 为 numeric 季度创建月间隔。例如 2.quarters 返回 6。 不适用 numeric.month | numeric.months 创建 numeric 个月的间隔。 不适用 numeric.week | numeric.weeks 为 numeric 周创建毫秒间隔。例如 2.weeks 返回 1209600000。 不适用 numeric.day | numeric.days 创建 numeric 天的毫秒间隔。 不适用 numeric.hour | numeric.hours 创建 numeric 小时的毫秒间隔。 不适用 numeric.minute | numeric.minutes 创建 numeric 分钟的毫秒间隔。 不适用 numeric.second | numeric.seconds 创建 numeric 秒的毫秒间隔。 不适用 numeric.milli | numeric.millis 创建 numeric 毫秒的毫秒间隔。 LOCALTIME localTime() 返回本地时区的当前 SQL 时间，返回类型为 TIME(0)。在流模式下为每条记录进行取值。 但在批处理模式下，它在查询开始时计算一次，并对每一行使用相同的结果。 LOCALTIMESTAMP localTimestamp() 返回本地时区的当前 SQL 时间，返回类型为 TIMESTAMP(3)。在流模式下为每条记录进行取值。 但在批处理模式下，它在查询开始时计算一次，并对每一行使用相同的结果。 CURRENT_TIME currentTime() 返回本地时区的当前 SQL 时间，这是 LOCAL_TIME 的同义词。 CURRENT_DATE currentDate() 返回本地时区中的当前 SQL 日期。在流模式下为每条记录进行取值。 但在批处理模式下，它在查询开始时计算一次，并对每一行使用相同的结果。 CURRENT_TIMESTAMP currentTimestamp() 返回本地时区的当前 SQL 时间戳，返回类型为 TIMESTAMP_LTZ(3)。在流模式下为每条记录进行取值。 但在批处理模式下，它在查询开始时计算一次，并对每一行使用相同的结果。 NOW() 不适用 返回本地时区的当前 SQL 时间戳，这是 CURRENT_TIMESTAMP 的同义词。 CURRENT_ROW_TIMESTAMP() 不适用 返回本地时区的当前 SQL 时间戳，返回类型为 TIMESTAMP_LTZ(3)。无论是在批处理模式还是流模式下，都会为每条记录进行取值。 EXTRACT(timeinteravlunit FROM temporal) TEMPORAL.extract(TIMEINTERVALUNIT) 返回从时间的时间间隔单位部分提取的 long 值。例如 EXTRACT(DAY FROM DATE '2006-06-05') 返回 5。 YEAR(date) 不适用 从 SQL 日期 date 返回年份。相当于 EXTRACT(YEAR FROM date)。例如 YEAR(DATE '1994-09-27') 返回 1994。 QUARTER(date) 不适用 从 SQL 日期 date 返回一年中的季度（1 到 4 之间的整数）。相当于 EXTRACT(QUARTER FROM date)。 例如 QUARTER(DATE '1994-09-27') 返回 3。 MONTH(date) 不适用 从 SQL 日期 date 返回一年中的月份（1 到 12 之间的整数）。相当于 EXTRACT(MONTH FROM date)。 例如 MONTH(DATE '1994-09-27') 返回 9。 WEEK(date) 不适用 从 SQL 日期 date 返回一年中的第几周（1 到 53 之间的整数）。相当于 EXTRACT(WEEK FROM date)。 例如 WEEK(DATE '1994-09-27') 返回 39。 DAYOFYEAR(date) 不适用 从 SQL 日期 date 返回一年中的第几天（1 到 366 之间的整数）。相当于 EXTRACT(DOY FROM date)。 例如 DAYOFYEAR(DATE '1994-09-27') 返回 270。 DAYOFMONTH 不适用 从 SQL 日期 date 返回一个月中的第几天（1 到 31 之间的整数）。相当于 EXTRACT(DAY FROM date)。 例如 DAYOFMONTH(DATE '1994-09-27') 返回 27。 HOUR(timestamp) 不适用 从 SQL 时间戳 timestamp 返回小时单位部分的小时（0 到 23 之间的整数）数。相当于 EXTRACT(HOUR FROM timestamp)。 例如 MINUTE(TIMESTAMP '1994-09-27 13:14:15') 返回 14。 MINUTE(timestamp) 不适用 从 SQL 时间戳 timestamp 返回分钟单位的分钟数（0 到 59 之间的整数）。相当于 EXTRACT(MINUTE FROM timestamp)。 例如 MINUTE(TIMESTAMP '1994-09-27 13:14:15') 返回 14。 SECOND(timestamp) 不适用 从 SQL 时间戳 timestamp 返回秒单位部分的秒数（0 到 59 之间的整数）。相当于 EXTRACT(SECOND FROM timestamp)。 例如 SECOND(TIMESTAMP '1994-09-27 13:14:15') 返回 15。 FLOOR(timepoint TO timeintervalunit) TIMEPOINT.floor(TIMEINTERVALUNIT) 返回将时间点 timepoint 向下取值到时间单位 timeintervalunit 的值。例如 FLOOR(TIME '12:44:31' TO MINUTE) 返回 12:44:00。 CEIL(timespoint TO timeintervaluntit) TIMEPOINT.ceil(TIMEINTERVALUNIT) 返回将时间点 timespoint 向上取值到时间单位 TIMEINTERVALUNIT 的值。例如 CEIL(TIME '12:44:31' TO MINUTE) 返回 12:45:00。 (timepoint1, temporal1) OVERLAPS (timepoint2, temporal2) temporalOverlaps(TIMEPOINT1, TEMPORAL1, TIMEPOINT2, TEMPORAL2) 如果由 (timepoint1, temporal1) 和 (timepoint2, temporal2) 定义的两个时间间隔重叠，则返回 TRUE。 时间值可以是时间点或时间间隔。例如 (TIME '2:55:00', INTERVAL '1' HOUR) OVERLAPS (TIME '3:30:00', INTERVAL '2' HOUR) 返回 TRUE； (TIME '9:00:00', TIME '10:00:00') OVERLAPS (TIME '10:15:00', INTERVAL '3' HOUR) 返回 FALSE。 DATE_FORMAT(timestamp, string) 不适用 将时间戳 timestamp 转换为日期格式字符串 string 指定格式的字符串值。格式字符串与 Java 的 SimpleDateFormat 兼容。 TIMESTAMPADD(timeintervalunit, interval, timepoint) 不适用 TIMESTAMPDIFF(timepointunit, timepoint1, timepoint2) timestampDiff(TIMEPOINTUNIT, TIMEPOINT1, TIMEPOINT2) 返回 timepoint1 和 timepoint2 之间时间间隔。间隔的单位由第一个参数给出，它应该是以下值之一： SECOND，MINUTE，HOUR，DAY，MONTH 或 YEAR。 CONVERT_TZ(string1, string2, string3) 不适用 将日期时间 string1（具有默认 ISO 时间戳格式 \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;）从时区 string2 转换为时区 string3 的值。 时区的格式应该是缩写如“PST”，全名如“America/Los_Angeles”，或自定义 ID 如“GMT-08:00”。例如 CONVERT_TZ('1970-01-01 00:00:00', 'UTC', 'America/Los_Angeles') 返回 '1969-12-31 16:00:00\u0026rsquo;。 FROM_UNIXTIME(numeric[, string]) fromUnixtime(NUMERIC[, STRING]) 以字符串格式 string 返回数字参数 numberic 的表示形式（默认为 \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;）。numeric 是一个内部 时间戳值，表示自'1970-01-01 00:00:00\u0026rsquo; UTC 以来的秒数，由 UNIX_TIMESTAMP() 函数生成。返回值以会话时区表示 （在 TableConfig 中指定）。例如，如果在 UTC 时区，FROM_UNIXTIME(44) 返回 \u0026lsquo;1970-01-01 00:00:44\u0026rsquo;，如果在 \u0026lsquo;Asia/Tokyo\u0026rsquo; 时区，则返回 \u0026lsquo;1970-01-01 09:00:44\u0026rsquo;。 UNIX_TIMESTAMP() 不适用 以秒为单位获取当前的 Unix 时间戳。此函数不是确定性的，这意味着将为每个记录重新计算该值。 UNIX_TIMESTAMP(string1[, string2]) 不适用 使用表配置中指定的时区将格式为 string2 的日期时间字符串 string1（如果未指定默认情况下：yyyy-MM-dd HH:mm:ss） 转换为 Unix 时间戳（以秒为单位）。 TO_DATE(string1[, string2]) 不适用 将格式为 string2（默认为 \u0026lsquo;yyyy-MM-dd\u0026rsquo;）的字符串 string1 转换为日期。 TO_TIMESTAMP_LTZ(numeric, precision) toTimestampLtz(numeric, PRECISION) 将纪元秒或纪元毫秒转换为 TIMESTAMP_LTZ，有效精度为 0 或 3，0 代表 TO_TIMESTAMP_LTZ(epochSeconds, 0)， 3 代表 TO_TIMESTAMP_LTZ(epochMilliseconds, 3)。 TO_TIMESTAMP(string1[, string2]) 不适用 将 \u0026lsquo;UTC+0\u0026rsquo; 时区下格式为 string2（默认为：\u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;）的字符串 string1 转换为时间戳。 CURRENT_WATERMARK(rowtime) 不适用 返回给定时间列属性 rowtime 的当前水印，如果管道中的当前操作没有可用的上游操作的公共水印时则为 NULL。 函数的返回类型被推断为与提供的时间列属性匹配，但调整后的精度为 3。例如时间列属性为 TIMESTAMP_LTZ(9)，则函数将返回 TIMESTAMP_LTZ(3)。\n请注意，此函数可以返回 NULL，您可能必须考虑这种情况。例如，如果您想过滤掉后期数据，您可以使用：\nWHERE CURRENT_WATERMARK(ts) IS NULL OR ts \u0026gt; CURRENT_WATERMARK(ts) 条件函数 # SQL 函数 Table 函数 描述 CASE value WHEN value1_1 [, value1_2]* THEN RESULT1 (WHEN value2_1 [, value2_2 ]* THEN result_2)* (ELSE result_z) END 不适用 当第一个时间值包含在 (valueX_1, valueX_2, \u0026hellip;) 中时，返回 resultX。当没有值匹配时，如果提供则返回 result_z， 否则返回 NULL。 CASE WHEN condition1 THEN result1 (WHEN condition2 THEN result2)* (ELSE result_z) END 不适用 满足第一个条件 X 时返回 resultX。当不满足任何条件时，如果提供则返回 result_z，否则返回 NULL。 NULLIF(value1, value2) 不适用 如果 value1 等于 value2 返回 NULL；否则返回 value1。例如 NULLIF(5, 5) 返回 NULL；NULLIF(5, 0) 返回 5。 COALESCE(value1, value2 [, value3]*) 不适用 从 value1, value2, \u0026hellip; 返回第一个不为 NULL 的值。例如 COALESCE(3, 5, 3) 返回 3。 IF(condition, true_value, false_value) 不适用 如果满足条件，则返回 true_value，否则返回 false_value。例如 IF(5 \u0026gt; 3, 5, 3) 返回 5。 IFNULL(input, null_replacement) input.ifNull(nullReplacement) 如果输入为 NULL，则返回 null_replacement；否则返回输入。与 COALESCE 或 CASE WHEN 相比，此函数返回的数据类型 在是否为空方面非常明确。。返回的类型是两个参数的公共类型，但只有在 null_replacement 可为空时才能为空。该函数允许将可 为空的列传递到使用 NOT NULL 约束声明的函数或表中。例如 IFNULL(nullable_column, 5) 一定不返回 NULL。 IS_ALPHA(string) 不适用 如果字符串中的所有字符都是字母，则返回 true，否则返回 false。 IS_DECIMAL(string) 不适用 如果 string 可以解析为有效数字，则返回 true，否则返回 false。 IS_DIGIT(string) 不适用 如果字符串中的所有字符都是数字，则返回 true，否则返回 false。 不适用 BOOLEAN.?(VALUE1, VALUE2) 如果 BOOLEAN 计算结果为 TRUE，则返回 VALUE1；否则返回 VALUE2。例如 (42 \u0026gt; 5).?('A', 'B') 返回 \u0026quot;A\u0026quot;。 GREATEST(value1[, value2]*) 不适用 返回所有输入参数的最大值，如果输入参数中包含 NULL，则返回 NULL。 LEAST(value1[, value2]*) 不适用 返回所有输入参数的最小值，如果输入参数中包含 NULL，则返回 NULL。 类型转换函数 # SQL 函数 Table 函数 描述 CAST(value AS type) ANY.cast(TYPE) 返回被强制转换为类型 type 的新值。例如 CAST('42' AS INT) 返回 42； CAST(NULL AS VARCHAR) 返回 VARCHAR 类型的 NULL。 TYPEOF(input) | TYPEOF(input, force_serializable) call(\u0026#34;TYPEOF\u0026#34;, input) | call(\u0026#34;TYPEOF\u0026#34;, input, force_serializable) 返回输入表达式的数据类型的字符串表示形式。默认情况下返回的字符串是一个摘要字符串，可能会为了可读性而省略某些细节。 如果 force_serializable 设置为 TRUE，则字符串表示可以保留在目录中的完整数据类型。请注意， 特别是匿名的内联数据类型没有可序列化的字符串表示。在这种情况下返回 NULL。 集合函数 # SQL 函数 Table 函数 描述 CARDINALITY(array) ARRAY.cardinality() 返回数组中元素的数量。 array \u0026#39;[\u0026#39; INT \u0026#39;]\u0026#39; ARRAY.at(INT) 返回数组中 INT 位置的元素。索引从 1 开始。 ELEMENT(array) ARRAY.element() 返回数组的唯一元素（其基数应为 1）；如果数组为空，则返回 NULL。如果数组有多个元素，则抛出异常。 CARDINALITY(map) MAP.cardinality() 返回 map 中的 entries 数量。 map ‘[’ value ‘]’ MAP.at(ANY) 返回 map 中指定 key 对应的值。 JSON Functions # JSON functions make use of JSON path expressions as described in ISO/IEC TR 19075-6 of the SQL standard. Their syntax is inspired by and adopts many features of ECMAScript, but is neither a subset nor superset thereof.\nPath expressions come in two flavors, lax and strict. When omitted, it defaults to the strict mode. Strict mode is intended to examine data from a schema perspective and will throw errors whenever data does not adhere to the path expression. However, functions like JSON_VALUE allow defining fallback behavior if an error is encountered. Lax mode, on the other hand, is more forgiving and converts errors to empty sequences.\nThe special character $ denotes the root node in a JSON path. Paths can access properties ($.a), array elements ($.a[0].b), or branch over all elements in an array ($.a[*].b).\nKnown Limitations:\nNot all features of Lax mode are currently supported correctly. This is an upstream bug (CALCITE-4717). Non-standard behavior is not guaranteed. SQL Function Table Function Description IS JSON [ { VALUE | SCALAR | ARRAY | OBJECT } ] STRING.isJson([JsonType type]) Determine whether a given string is valid JSON.\nSpecifying the optional type argument puts a constraint on which type of JSON object is allowed. If the string is valid JSON, but not that type, false is returned. The default is VALUE.\n-- TRUE \u0026#39;1\u0026#39; IS JSON \u0026#39;[]\u0026#39; IS JSON \u0026#39;{}\u0026#39; IS JSON -- TRUE \u0026#39;\u0026#34;abc\u0026#34;\u0026#39; IS JSON -- FALSE \u0026#39;abc\u0026#39; IS JSON NULL IS JSON -- TRUE \u0026#39;1\u0026#39; IS JSON SCALAR -- FALSE \u0026#39;1\u0026#39; IS JSON ARRAY -- FALSE \u0026#39;1\u0026#39; IS JSON OBJECT -- FALSE \u0026#39;{}\u0026#39; IS JSON SCALAR -- FALSE \u0026#39;{}\u0026#39; IS JSON ARRAY -- TRUE \u0026#39;{}\u0026#39; IS JSON OBJECT JSON_EXISTS(jsonValue, path [ { TRUE | FALSE | UNKNOWN | ERROR } ON ERROR ]) STRING.jsonExists(STRING path [, JsonExistsOnError onError]) Determines whether a JSON string satisfies a given path search criterion.\nIf the error behavior is omitted, FALSE ON ERROR is assumed as the default.\n-- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;$.a\u0026#39;); -- FALSE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;$.b\u0026#39;); -- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: [{ \u0026#34;b\u0026#34;: 1 }]}\u0026#39;, \u0026#39;$.a[0].b\u0026#39;); -- TRUE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict $.b\u0026#39; TRUE ON ERROR); -- FALSE SELECT JSON_EXISTS(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict $.b\u0026#39; FALSE ON ERROR); JSON_STRING(value) jsonString(value) Serializes a value into JSON.\nThis function returns a JSON string containing the serialized value. If the value is NULL, the function returns NULL.\n-- NULL JSON_STRING(CAST(NULL AS INT)) -- \u0026#39;1\u0026#39; JSON_STRING(1) -- \u0026#39;true\u0026#39; JSON_STRING(TRUE) -- \u0026#39;\u0026#34;Hello, World!\u0026#34;\u0026#39; JSON_STRING(\u0026#39;Hello, World!\u0026#39;) -- \u0026#39;[1,2]\u0026#39; JSON_STRING(ARRAY[1, 2]) JSON_VALUE(jsonValue, path [RETURNING \u0026lt;dataType\u0026gt;] [ { NULL | ERROR | DEFAULT \u0026lt;defaultExpr\u0026gt; } ON EMPTY ] [ { NULL | ERROR | DEFAULT \u0026lt;defaultExpr\u0026gt; } ON ERROR ]) STRING.jsonValue(STRING path [, returnType, onEmpty, defaultOnEmpty, onError, defaultOnError]) Extracts a scalar from a JSON string.\nThis method searches a JSON string for a given path expression and returns the value if the value at that path is scalar. Non-scalar values cannot be returned. By default, the value is returned as STRING. Using returningType a different type can be chosen, with the following types being supported:\nVARCHAR / STRING BOOLEAN INTEGER DOUBLE For empty path expressions or errors a behavior can be defined to either return null, raise an error or return a defined default value instead. When omitted, the default is NULL ON EMPTY or NULL ON ERROR, respectively. The default value may be a literal or an expression. If the default value itself raises an error, it falls through to the error behavior for ON EMPTY, and raises an error for ON ERROR.\nFor path contains special characters such as spaces, you can use ['property'] or [\u0026quot;property\u0026quot;] to select the specified property in a parent object. Be sure to put single or double quotes around the property name. When using JSON_VALUE in SQL, the path is a character parameter which is already single quoted, so you have to escape the single quotes around property name, such as JSON_VALUE('{\u0026quot;a b\u0026quot;: \u0026quot;true\u0026quot;}', '$.[''a b'']').\n-- \u0026#34;true\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;$.a\u0026#39;) -- TRUE JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;$.a\u0026#39; RETURNING BOOLEAN) -- \u0026#34;false\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;lax $.b\u0026#39; DEFAULT FALSE ON EMPTY) -- \u0026#34;false\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;a\u0026#34;: true}\u0026#39;, \u0026#39;strict $.b\u0026#39; DEFAULT FALSE ON ERROR) -- 0.998D JSON_VALUE(\u0026#39;{\u0026#34;a.b\u0026#34;: [0.998,0.996]}\u0026#39;,\u0026#39;$.[\u0026#34;a.b\u0026#34;][0]\u0026#39; RETURNING DOUBLE) -- \u0026#34;right\u0026#34; JSON_VALUE(\u0026#39;{\u0026#34;contains blank\u0026#34;: \u0026#34;right\u0026#34;}\u0026#39;, \u0026#39;strict $.[\u0026#39;\u0026#39;contains blank\u0026#39;\u0026#39;]\u0026#39; NULL ON EMPTY DEFAULT \u0026#39;wrong\u0026#39; ON ERROR) JSON_QUERY(jsonValue, path [ { WITHOUT | WITH CONDITIONAL | WITH UNCONDITIONAL } [ ARRAY ] WRAPPER ] [ { NULL | EMPTY ARRAY | EMPTY OBJECT | ERROR } ON EMPTY ] [ { NULL | EMPTY ARRAY | EMPTY OBJECT | ERROR } ON ERROR ]) STRING.jsonQuery(path [, JsonQueryWrapper [, JsonQueryOnEmptyOrError, JsonQueryOnEmptyOrError ] ]) Extracts JSON values from a JSON string.\nThe result is always returned as a STRING. The RETURNING clause is currently not supported.\nThe wrappingBehavior determines whether the extracted value should be wrapped into an array, and whether to do so unconditionally or only if the value itself isn\u0026rsquo;t an array already.\nonEmpty and onError determine the behavior in case the path expression is empty, or in case an error was raised, respectively. By default, in both cases null is returned. Other choices are to use an empty array, an empty object, or to raise an error.\n-- \u0026#39;{ \u0026#34;b\u0026#34;: 1 }\u0026#39; JSON_QUERY(\u0026#39;{ \u0026#34;a\u0026#34;: { \u0026#34;b\u0026#34;: 1 } }\u0026#39;, \u0026#39;$.a\u0026#39;) -- \u0026#39;[1, 2]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;$\u0026#39;) -- NULL JSON_QUERY(CAST(NULL AS STRING), \u0026#39;$\u0026#39;) -- \u0026#39;[\u0026#34;c1\u0026#34;,\u0026#34;c2\u0026#34;]\u0026#39; JSON_QUERY(\u0026#39;{\u0026#34;a\u0026#34;:[{\u0026#34;c\u0026#34;:\u0026#34;c1\u0026#34;},{\u0026#34;c\u0026#34;:\u0026#34;c2\u0026#34;}]}\u0026#39;, \u0026#39;lax $.a[*].c\u0026#39;) -- Wrap result into an array -- \u0026#39;[{}]\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- \u0026#39;[1, 2]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- \u0026#39;[[1, 2]]\u0026#39; JSON_QUERY(\u0026#39;[1, 2]\u0026#39;, \u0026#39;$\u0026#39; WITH UNCONDITIONAL ARRAY WRAPPER) -- Scalars must be wrapped to be returned -- NULL JSON_QUERY(1, \u0026#39;$\u0026#39;) -- \u0026#39;[1]\u0026#39; JSON_QUERY(1, \u0026#39;$\u0026#39; WITH CONDITIONAL ARRAY WRAPPER) -- Behavior if path expression is empty / there is an error -- \u0026#39;{}\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;lax $.invalid\u0026#39; EMPTY OBJECT ON EMPTY) -- \u0026#39;[]\u0026#39; JSON_QUERY(\u0026#39;{}\u0026#39;, \u0026#39;strict $.invalid\u0026#39; EMPTY ARRAY ON ERROR) JSON_OBJECT([[KEY] key VALUE value]* [ { NULL | ABSENT } ON NULL ]) jsonObject(JsonOnNull, keyValues...) Builds a JSON object string from a list of key-value pairs.\nNote that keys must be non-NULL string literals, while values may be arbitrary expressions.\nThis function returns a JSON string. The ON NULL behavior defines how to treat NULL values. If omitted, NULL ON NULL is assumed by default.\nValues which are created from another JSON construction function call (JSON_OBJECT, JSON_ARRAY) are inserted directly rather than as a string. This allows building nested JSON structures.\n-- \u0026#39;{}\u0026#39; JSON_OBJECT() -- \u0026#39;{\u0026#34;K1\u0026#34;:\u0026#34;V1\u0026#34;,\u0026#34;K2\u0026#34;:\u0026#34;V2\u0026#34;}\u0026#39; JSON_OBJECT(\u0026#39;K1\u0026#39; VALUE \u0026#39;V1\u0026#39;, \u0026#39;K2\u0026#39; VALUE \u0026#39;V2\u0026#39;) -- Expressions as values JSON_OBJECT(\u0026#39;orderNo\u0026#39; VALUE orders.orderId) -- ON NULL JSON_OBJECT(KEY \u0026#39;K1\u0026#39; VALUE CAST(NULL AS STRING) NULL ON NULL) -- \u0026#39;{\u0026#34;K1\u0026#34;:null}\u0026#39; JSON_OBJECT(KEY \u0026#39;K1\u0026#39; VALUE CAST(NULL AS STRING) ABSENT ON NULL) -- \u0026#39;{}\u0026#39; -- \u0026#39;{\u0026#34;K1\u0026#34;:{\u0026#34;K2\u0026#34;:\u0026#34;V\u0026#34;}}\u0026#39; JSON_OBJECT( KEY \u0026#39;K1\u0026#39; VALUE JSON_OBJECT( KEY \u0026#39;K2\u0026#39; VALUE \u0026#39;V\u0026#39; ) ) JSON_OBJECTAGG([KEY] key VALUE value [ { NULL | ABSENT } ON NULL ]) jsonObjectAgg(JsonOnNull, keyExpression, valueExpression) Builds a JSON object string by aggregating key-value expressions into a single JSON object.\nThe key expression must return a non-nullable character string. Value expressions can be arbitrary, including other JSON functions. If a value is NULL, the ON NULL behavior defines what to do. If omitted, NULL ON NULL is assumed by default.\nNote that keys must be unique. If a key occurs multiple times, an error will be thrown.\nThis function is currently not supported in OVER windows.\n-- \u0026#39;{\u0026#34;Apple\u0026#34;:2,\u0026#34;Banana\u0026#34;:17,\u0026#34;Orange\u0026#34;:0}\u0026#39; SELECT JSON_OBJECTAGG(KEY product VALUE cnt) FROM orders JSON_ARRAY([value]* [ { NULL | ABSENT } ON NULL ]) jsonArray(JsonOnNull, values...) Builds a JSON array string from a list of values.\nThis function returns a JSON string. The values can be arbitrary expressions. The ON NULL behavior defines how to treat NULL values. If omitted, ABSENT ON NULL is assumed by default.\nElements which are created from another JSON construction function call (JSON_OBJECT, JSON_ARRAY) are inserted directly rather than as a string. This allows building nested JSON structures.\n-- \u0026#39;[]\u0026#39; JSON_ARRAY() -- \u0026#39;[1,\u0026#34;2\u0026#34;]\u0026#39; JSON_ARRAY(1, \u0026#39;2\u0026#39;) -- Expressions as values JSON_ARRAY(orders.orderId) -- ON NULL JSON_ARRAY(CAST(NULL AS STRING) NULL ON NULL) -- \u0026#39;[null]\u0026#39; JSON_ARRAY(CAST(NULL AS STRING) ABSENT ON NULL) -- \u0026#39;[]\u0026#39; -- \u0026#39;[[1]]\u0026#39; JSON_ARRAY(JSON_ARRAY(1)) JSON_ARRAYAGG(items [ { NULL | ABSENT } ON NULL ]) jsonArrayAgg(JsonOnNull, itemExpression) Builds a JSON object string by aggregating items into an array.\nItem expressions can be arbitrary, including other JSON functions. If a value is NULL, the ON NULL behavior defines what to do. If omitted, ABSENT ON NULL is assumed by default.\nThis function is currently not supported in OVER windows, unbounded session windows, or hop windows.\n-- \u0026#39;[\u0026#34;Apple\u0026#34;,\u0026#34;Banana\u0026#34;,\u0026#34;Orange\u0026#34;]\u0026#39; SELECT JSON_ARRAYAGG(product) FROM orders 值构建函数 # SQL 函数 Table 函数 描述 -- implicit constructor with parenthesis (value1 [, value2]*) row(ANY1, ANY2, ...) 返回从值列表 (value1, value2, \u0026hellip;) 创建的行。隐式行构造函数支持任意表达式作为字段，但至少需要两个字段。 显式行构造函数可以处理任意数量的字段，但目前还不能很好地支持所有类型的字段表达式。 ARRAY ‘[’ value1 [, value2 ]* ‘]’ array(ANY1, ANY2, ...) 返回从值列表 (value1, value2, \u0026hellip;) 创建的数组。 MAP ‘[’ value1, value2 [, value3, value4 ]* ‘]’ map(ANY1, ANY2, ANY3, ANY4, ...) 返回从键值对列表 ((value1, value2), (value3, value4), \u0026hellip;) 创建的 map。 不适用 numeric.rows 创建一个 numeric 行间隔（通常用于窗口创建）。 值获取函数 # SQL 函数 Table 函数 描述 tableName.compositeType.field COMPOSITE.get(STRING) | COMPOSITE.get(INT) 按名称从 Flink 复合类型（例如，Tuple，POJO）返回字段的值。 tableName.compositeType.* ANY.flatten() 返回 Flink 复合类型（例如，Tuple，POJO）的平面表示，将其每个直接子类型转换为单独的字段。在大多数情况下，平面表示 的字段与原始字段的命名类似，但使用 $ 分隔符（例如 mypojo$mytuple$f0）。 分组函数 # SQL 函数 Table 函数 描述 GROUP_ID() 不适用 返回唯一标识分组键组合的整数。 GROUPING(expression1 [, expression2]* ) | GROUPING_ID(expression1 [, expression2]* ) 不适用 返回给定分组表达式的位向量。 哈希函数 # SQL 函数 Table 函数 描述 MD5(string) STRING.md5() 以 32 个十六进制数字的字符串形式返回 string 的 MD5 哈希值；如果字符串为 NULL，则返回 NULL。 SHA1(string) STRING.sha1() 以 40 个十六进制数字的字符串形式返回 string 的 SHA-1 哈希值；如果字符串为 NULL，则返回 NULL。 SHA224(string) STRING.sha224() 以 56 个十六进制数字的字符串形式返回 string 的 SHA-224 哈希值；如果字符串为 NULL，则返回 NULL。 SHA256(string) STRING.sha256() 以 64 个十六进制数字的字符串形式返回 string 的 SHA-256 哈希值；如果字符串为 NULL，则返回 NULL。 SHA384(string) STRING.sha384() 以 96 个十六进制数字的字符串形式返回 string 的 SHA-384 哈希值；如果字符串为 NULL，则返回 NULL。 SHA512(string) STRING.sha512() 以 128 个十六进制数字的字符串形式返回 string 的 SHA-512 哈希值；如果字符串为 NULL，则返回 NULL。 SHA2(string, hashLength) STRING.sha2(INT) 使用 SHA-2 系列散列函数（SHA-224，SHA-256，SHA-384 或 SHA-512）返回散列值。第一个参数字符串是要散列的字符串， 第二个参数 hashLength 是结果的位长（224，256，384 或 512）。如果 string 或 hashLength 为 NULL，则返回 NULL。 辅助函数 # SQL 函数 Table 函数 描述 聚合函数 # 聚合函数将所有的行作为输入，并返回单个聚合值作为结果。\nSQL 函数 Table 函数 描述 COUNT([ ALL ] expression | DISTINCT expression1 [, expression2]*) 不适用 默认情况下或使用关键字 ALL，返回不为 NULL 的表达式的输入行数。使用 DISTINCT 则对所有值去重后计算。 COUNT(*) | COUNT(1) FIELD.count 返回输入行数。 AVG([ ALL | DISTINCT ] expression) FIELD.avg 默认情况下或使用关键字 ALL，返回所有输入行中表达式的平均值（算术平均值）。使用 DISTINCT 则对所有值去重后计算。 SUM([ ALL | DISTINCT ] expression) FIELD.sum 默认情况下或使用关键字 ALL，返回所有输入行的表达式总和。使用 DISTINCT 则对所有值去重后计算。 不适用 FIELD.sum0 返回所有输入行的数字字段的总和。如果所有值都为 NULL，则返回 0。 MAX([ ALL | DISTINCT ] expression) FIELD.max 默认情况下或使用关键字 ALL，返回所有输入行中表达式的最大值。使用 DISTINCT 则对所有值去重后计算。 MIN([ ALL | DISTINCT ] expression ) FIELD.min 默认情况下或使用关键字 ALL，返回所有输入行中表达式的最小值。使用 DISTINCT 则对所有值去重后计算。 STDDEV_POP([ ALL | DISTINCT ] expression) FIELD.stddevPop 默认情况下或使用关键字 ALL，返回所有输入行中表达式的总体标准偏差。使用 DISTINCT 则对所有值去重后计算。 STDDEV_SAMP([ ALL | DISTINCT ] expression) FIELD.stddevSamp 默认情况下或使用关键字 ALL，返回所有输入行中表达式的样本标准偏差。使用 DISTINCT 则对所有值去重后计算。 VAR_POP([ ALL | DISTINCT ] expression) FIELD.varPop 默认情况下或使用关键字 ALL，返回所有输入行中表达式的总体方差（总体标准差的平方）。使用 DISTINCT 则对所有值去重后计算。 VAR_SAMP([ ALL | DISTINCT ] expression) FIELD.varSamp 默认情况下或使用关键字 ALL，返回所有输入行中表达式的样本方差（样本标准差的平方）。使用 DISTINCT 则对所有值去重后计算。 COLLECT([ ALL | DISTINCT ] expression) FIELD.collect 默认情况下或使用关键字 ALL，返回跨所有输入行的多组表达式。NULL 值将被忽略。使用 DISTINCT 则对所有值去重后计算。 VARIANCE([ ALL | DISTINCT ] expression) 不适用 VAR_SAMP() 的同义方法。 RANK() 不适用 返回值在一组值中的排名。结果是 1 加上分区顺序中当前行之前或等于当前行的行数。排名在序列中不一定连续。 DENSE_RANK() 不适用 返回值在一组值中的排名。结果是一加先前分配的等级值。与函数 rank 不同，dense_rank 不会在排名序列中产生间隙。 ROW_NUMBER() 不适用 在窗口分区内根据 rows 的排序为每一行分配一个唯一的序列号，从一开始。ROW_NUMBER 和 RANK 相似。ROW_NUMBER 按 顺序对所有行进行编号（例如 1，2，3，4，5）。RANK 为等值 row 提供相同的序列值（例如 1，2，2，4，5）。 LEAD(expression [, offset] [, default]) 不适用 返回窗口中当前行之后第 offset 行处的表达式值。offset 的默认值为 1，default 的默认值为 NULL。 LAG(expression [, offset] [, default]) 不适用 返回窗口中当前行之前第 offset 行处的表达式值。offset 的默认值为 1，default 的默认值为 NULL。 FIRST_VALUE(expression) FIELD.firstValue 返回一组有序值中的第一个值。 LAST_VALUE(expression) FIELD.lastValue 返回一组有序值中的最后一个值。 LISTAGG(expression [, separator]) 不适用 连接字符串表达式的值并在它们之间放置分隔符值。字符串末尾不添加分隔符时则分隔符的默认值为“,”。 CUME_DIST() 不适用 返回值在一组值的累积分布。结果是小于或等于当前行的值的行数除以窗口分区的总行数。 PERCENT_RANK() 不适用 返回值在一组值的百分比排名。结果是当前行在窗口分区中的排名减 1，然后除以窗口分区的总行数减 1。如果窗口分区的总行数为 1，则该函数返回 0。 NTILE(n) 不适用 将窗口分区中的所有数据按照顺序划分为 n 个分组，返回分配给各行数据的分组编号（从 1 开始）。 如果不能均匀划分为 n 个分组，则从第 1 个分组开始，为每一分组分配一个剩余值。 比如某个窗口分区有 6 行数据，划分为 4 个分组，则各行的分组编号为：1，1，2，2，3，4。 时间间隔单位和时间点单位标识符 # 下表列出了时间间隔单位和时间点单位标识符。\n对于 Table API，请使用 _ 代替空格（例如 DAY_TO_HOUR）。\n时间间隔单位 时间点单位 MILLENNIUM CENTURY DECADE YEAR YEAR YEAR TO MONTH QUARTER QUARTER MONTH MONTH WEEK WEEK DAY DAY DAY TO HOUR DAY TO MINUTE DAY TO SECOND HOUR HOUR HOUR TO MINUTE HOUR TO SECOND MINUTE MINUTE MINUTE TO SECOND SECOND SECOND MILLISECOND MILLISECOND MICROSECOND MICROSECOND NANOSECOND EPOCH DOY （仅适用SQL） DOW （仅适用SQL） ISODOW （仅适用SQL） ISOYEAR （仅适用SQL） SQL_TSI_YEAR （仅适用SQL） SQL_TSI_QUARTER （仅适用SQL） SQL_TSI_MONTH （仅适用SQL） SQL_TSI_WEEK （仅适用SQL） SQL_TSI_DAY （仅适用SQL） SQL_TSI_HOUR （仅适用SQL） SQL_TSI_MINUTE （仅适用SQL） SQL_TSI_SECOND （仅适用SQL） Back to top\n列函数 # 列函数用于选择或丢弃表的列。\n列函数仅在 Table API 中使用。 语法 描述 withColumns(\u0026hellip;) 选择指定的列 withoutColumns(\u0026hellip;) 选择除指定列以外的列 详细语法如下：\n列函数: withColumns(columnExprs) withoutColumns(columnExprs) 多列表达式: columnExpr [, columnExpr]* 单列表达式: columnRef | columnIndex to columnIndex | columnName to columnName 列引用: columnName(The field name that exists in the table) | columnIndex(a positive integer starting from 1) 列函数的用法如下表所示（假设我们有一个包含 5 列的表：(a: Int, b: Long, c: String, d:String, e: String)）：\n接口 用法举例 描述 withColumns($(*)) select(withColumns($(\u0026quot;*\u0026quot;))) = select($(\u0026ldquo;a\u0026rdquo;), $(\u0026ldquo;b\u0026rdquo;), $(\u0026ldquo;c\u0026rdquo;), $(\u0026ldquo;d\u0026rdquo;), $(\u0026ldquo;e\u0026rdquo;)) 全部列 withColumns(m to n) select(withColumns(range(2, 4))) = select($(\u0026ldquo;b\u0026rdquo;), $(\u0026ldquo;c\u0026rdquo;), $(\u0026ldquo;d\u0026rdquo;)) 第 m 到第 n 列 withColumns(m, n, k) select(withColumns(lit(1), lit(3), $(\u0026ldquo;e\u0026rdquo;))) = select($(\u0026ldquo;a\u0026rdquo;), $(\u0026ldquo;c\u0026rdquo;), $(\u0026ldquo;e\u0026rdquo;)) 第 m、n、k 列 withColumns(m, n to k) select(withColumns(lit(1), range(3, 5))) = select($(\u0026ldquo;a\u0026rdquo;), $(\u0026ldquo;c\u0026rdquo;), $(\u0026ldquo;d\u0026rdquo;), $(\u0026ldquo;e\u0026rdquo;)) 以上两种用法的混合 withoutColumns(m to n) select(withoutColumns(range(2, 4))) = select($(\u0026ldquo;a\u0026rdquo;), $(\u0026ldquo;e\u0026rdquo;)) 不选从第 m 到第 n 列 withoutColumns(m, n, k) select(withoutColumns(lit(1), lit(3), lit(5))) = select($(\u0026ldquo;b\u0026rdquo;), $(\u0026ldquo;d\u0026rdquo;)) 不选第 m、n、k 列 withoutColumns(m, n to k) select(withoutColumns(lit(1), range(3, 5))) = select($(\u0026ldquo;b\u0026rdquo;)) 以上两种用法的混合 列函数可用于所有需要列字段的地方，例如 select、groupBy、orderBy、UDFs 等函数，例如：\nJava table .groupBy(withColumns(range(1, 3))) .select(withColumns(range(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;)), myUDAgg(myUDF(withColumns(range(5, 20))))); Scala table .groupBy(withColumns(range(1, 3))) .select(withColumns(\u0026#39;a to \u0026#39;b), myUDAgg(myUDF(withColumns(5 to 20)))) Python table .group_by(with_columns(range_(1, 3))) .select(with_columns(range_(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;)), myUDAgg(myUDF(with_columns(range_(5, 20))))) Back to top\n"}),e.add({id:282,href:"/flink/flink-docs-master/zh/docs/dev/python/table/udfs/",title:"自定义函数",section:"Table API",content:""}),e.add({id:283,href:"/flink/flink-docs-master/zh/docs/dev/table/functions/",title:"函数",section:"Table API \u0026 SQL",content:""}),e.add({id:284,href:"/flink/flink-docs-master/zh/docs/dev/datastream/side_output/",title:"旁路输出",section:"DataStream API",content:` 旁路输出 # 除了由 DataStream 操作产生的主要流之外，你还可以产生任意数量的旁路输出结果流。结果流中的数据类型不必与主要流中的数据类型相匹配，并且不同旁路输出的类型也可以不同。当你需要拆分数据流时，通常必须复制该数据流，然后从每个流中过滤掉不需要的数据，这个操作十分有用。
使用旁路输出时，首先需要定义用于标识旁路输出流的 OutputTag：
Java // 这需要是一个匿名的内部类，以便我们分析类型 OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;) {}; Scala val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) Python output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) 注意 OutputTag 是如何根据旁路输出流所包含的元素类型进行类型化的。
可以通过以下方法将数据发送到旁路输出：
ProcessFunction KeyedProcessFunction CoProcessFunction KeyedCoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 你可以使用在上述方法中向用户暴露的 Context 参数，将数据发送到由 OutputTag 标识的旁路输出。这是从 ProcessFunction 发送数据到旁路输出的示例：
Java DataStream\u0026lt;Integer\u0026gt; input = ...; final OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Integer\u0026gt; mainDataStream = input .process(new ProcessFunction\u0026lt;Integer, Integer\u0026gt;() { @Override public void processElement( Integer value, Context ctx, Collector\u0026lt;Integer\u0026gt; out) throws Exception { // 发送数据到主要的输出 out.collect(value); // 发送数据到旁路输出 ctx.output(outputTag, \u0026#34;sideout-\u0026#34; + String.valueOf(value)); } }); Scala val input: DataStream[Int] = ... val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = input .process(new ProcessFunction[Int, Int] { override def processElement( value: Int, ctx: ProcessFunction[Int, Int]#Context, out: Collector[Int]): Unit = { // 发送数据到主要的输出 out.collect(value) // 发送数据到旁路输出 ctx.output(outputTag, \u0026#34;sideout-\u0026#34; + String.valueOf(value)) } }) Python input = ... # type: DataStream output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) class MyProcessFunction(ProcessFunction): def process_element(self, value: int, ctx: ProcessFunction.Context): # emit data to regular output yield value # emit data to side output yield output_tag, \u0026#34;sideout-\u0026#34; + str(value) main_data_stream = input \\ .process(MyProcessFunction(), Types.INT()) 你可以在 DataStream 运算结果上使用 getSideOutput(OutputTag) 方法获取旁路输出流。这将产生一个与旁路输出流结果类型一致的 DataStream：
Java final OutputTag\u0026lt;String\u0026gt; outputTag = new OutputTag\u0026lt;String\u0026gt;(\u0026#34;side-output\u0026#34;){}; SingleOutputStreamOperator\u0026lt;Integer\u0026gt; mainDataStream = ...; DataStream\u0026lt;String\u0026gt; sideOutputStream = mainDataStream.getSideOutput(outputTag); Scala val outputTag = OutputTag[String](\u0026#34;side-output\u0026#34;) val mainDataStream = ... val sideOutputStream: DataStream[String] = mainDataStream.getSideOutput(outputTag) Python output_tag = OutputTag(\u0026#34;side-output\u0026#34;, Types.STRING()) main_data_stream = ... # type: DataStream side_output_stream = main_data_stream.get_side_output(output_tag) # type: DataStream Back to top
`}),e.add({id:285,href:"/flink/flink-docs-master/zh/docs/dev/python/python_execution_mode/",title:"执行模式",section:"Python API",content:` Execution Mode # The Python API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job. The Python runtime execution mode defines how the Python user-defined functions will be executed.
Prior to release-1.15, there is the only execution mode called PROCESS execution mode. The PROCESS mode means that the Python user-defined functions will be executed in separate Python processes.
In release-1.15, it has introduced a new execution mode called THREAD execution mode. The THREAD mode means that the Python user-defined functions will be executed in JVM.
NOTE: Multiple Python user-defined functions running in the same JVM are still affected by GIL.
When can/should I use THREAD execution mode? # The purpose of the introduction of THREAD mode is to overcome the overhead of serialization/deserialization and network communication introduced of inter-process communication in the PROCESS mode. So if performance is not your concern, or the computing logic of your Python user-defined functions is the performance bottleneck of the job, PROCESS mode will be the best choice as PROCESS mode provides the best isolation compared to THREAD mode.
Configuring Python execution mode # The execution mode can be configured via the python.execution-mode setting. There are two possible values:
PROCESS: The Python user-defined functions will be executed in separate Python process. (default) THREAD: The Python user-defined functions will be executed in JVM. You could specify the execution mode in Python Table API or Python DataStream API jobs as following:
## Python Table API # Specify \`PROCESS\` mode table_env.get_config().set(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;process\u0026#34;) # Specify \`THREAD\` mode table_env.get_config().set(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;thread\u0026#34;) ## Python DataStream API config = Configuration() # Specify \`PROCESS\` mode config.set_string(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;process\u0026#34;) # Specify \`THREAD\` mode config.set_string(\u0026#34;python.execution-mode\u0026#34;, \u0026#34;thread\u0026#34;) # Create the corresponding StreamExecutionEnvironment env = StreamExecutionEnvironment.get_execution_environment(config) Supported Cases # Python Table API # The following table shows where the THREAD execution mode is supported in Python Table API.
UDFs PROCESS THREAD Python UDF Yes Yes Python UDTF Yes Yes Python UDAF Yes No Pandas UDF \u0026amp; Pandas UDAF Yes No Python DataStream API # The following Table shows the supported cases in Python DataStream API.
Operators PROCESS THREAD Map Yes Yes FlatMap Yes Yes Filter Yes Yes Reduce Yes Yes Union Yes Yes Connect Yes Yes CoMap Yes Yes CoFlatMap Yes Yes Process Function Yes Yes Window Apply Yes Yes Window Aggregate Yes Yes Window Reduce Yes Yes Window Process Yes Yes Side Output Yes Yes State Yes Yes Iterate No No Window CoGroup No No Window Join No No Interval Join No No Async I/O No No Currently, it still doesn\u0026rsquo;t support to execute Python UDFs in THREAD execution mode in all places. It will fall back to PROCESS execution mode in these cases. So it may happen that you configure a job to execute in THREAD execution mode, however, it\u0026rsquo;s actually executed in PROCESS execution mode. THREAD execution mode is only supported in Python 3.7+. Execution Behavior # This section provides an overview of the execution behavior of THREAD execution mode and contrasts they with PROCESS execution mode. For more details, please refer to the FLIP that introduced this feature: FLIP-206.
PROCESS Execution Mode # In PROCESS execution mode, the Python user-defined functions will be executed in separate Python Worker process. The Java operator process communicates with the Python worker process using various Grpc services.
THREAD Execution Mode # In THREAD execution mode, the Python user-defined functions will be executed in the same process as Java operators. PyFlink takes use of third part library PEMJA to embed Python in Java Application.
`}),e.add({id:286,href:"/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_pandas/",title:"PyFlink Table 和 Pandas DataFrame 互转",section:"Table API",content:` PyFlink Table 和 Pandas DataFrame 互转 # PyFlink 支持 PyFlink Table 和 Pandas DataFrame 之间进行互转。
将 Pandas DataFrame 转换为 PyFlink Table # PyFlink 支持将 Pandas DataFrame 转换成 PyFlink Table。在内部实现上，会在客户端将 Pandas DataFrame 序列化成 Arrow 列存格式，序列化后的数据 在作业执行期间，在 Arrow 源中会被反序列化，并进行处理。Arrow 源除了可以用在批作业中外，还可以用于流作业，它将正确处理检查点并提供恰好一次的保证。
以下示例显示如何从 Pandas DataFrame 创建 PyFlink Table：
from pyflink.table import DataTypes import pandas as pd import numpy as np # 创建一个Pandas DataFrame pdf = pd.DataFrame(np.random.rand(1000, 2)) # 由Pandas DataFrame创建PyFlink表 table = t_env.from_pandas(pdf) # 由Pandas DataFrame创建指定列名的PyFlink表 table = t_env.from_pandas(pdf, [\u0026#39;f0\u0026#39;, \u0026#39;f1\u0026#39;]) # 由Pandas DataFrame创建指定列类型的PyFlink表 table = t_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()]) # 由Pandas DataFrame创建列名和列类型的PyFlink表 table = t_env.from_pandas(pdf, DataTypes.ROW([DataTypes.FIELD(\u0026#34;f0\u0026#34;, DataTypes.DOUBLE()), DataTypes.FIELD(\u0026#34;f1\u0026#34;, DataTypes.DOUBLE())])) 将 PyFlink Table 转换为 Pandas DataFrame # 除此之外，还支持将 PyFlink Table 转换为 Pandas DataFrame。在内部实现上，它将执行表的计算逻辑，得到物化之后的表的执行结果，并 在客户端将其序列化为 Arrow 列存格式，最大 Arrow 批处理大小 由配置选项python.fn-execution.arrow.batch.size 确定。 序列化后的数据将被转换为 Pandas DataFrame。这意味着需要把表的内容收集到客户端，因此在调用此函数之前，请确保表的内容可以容纳在内存中。 可以通过 Table.limit ，设置收集到客户端的数据的条数。
以下示例显示了如何将 PyFlink Table 转换为 Pandas DataFrame：
from pyflink.table.expressions import col import pandas as pd import numpy as np # 创建PyFlink Table pdf = pd.DataFrame(np.random.rand(1000, 2)) table = t_env.from_pandas(pdf, [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).filter(col(\u0026#39;a\u0026#39;) \u0026gt; 0.5) # 转换PyFlink Table为Pandas DataFrame pdf = table.limit(100).to_pandas() `}),e.add({id:287,href:"/flink/flink-docs-master/zh/docs/dev/python/table/conversion_of_data_stream/",title:"Table 和 DataStream 互转",section:"Table API",content:" "}),e.add({id:288,href:"/flink/flink-docs-master/zh/docs/dev/python/table/sql/",title:"SQL",section:"Table API",content:" "}),e.add({id:289,href:"/flink/flink-docs-master/zh/docs/dev/datastream/application_parameters/",title:"Handling Application Parameters",section:"DataStream API",content:` 应用程序参数处理 # 应用程序参数处理 # 几乎所有的批和流的 Flink 应用程序，都依赖于外部配置参数。这些配置参数可以用于指定输入和输出源（如路径或地址）、系统参数（并行度，运行时配置）和特定的应用程序参数（通常使用在用户自定义函数）。
为解决以上问题，Flink 提供一个名为 Parametertool 的简单公共类，其中包含了一些基本的工具。请注意，这里说的 Parametertool 并不是必须使用的。Commons CLI 和 argparse4j 等其他框架也可以非常好地兼容 Flink。
用 ParameterTool 读取配置值 # ParameterTool 定义了一组静态方法，用于读取配置信息。该工具类内部使用了 Map\u0026lt;string，string\u0026gt; 类型，这样使得它可以很容易地与你的配置集成在一起。
配置值来自 .properties 文件 # 以下方法可以读取 Properties 文件并解析出键/值对：
String propertiesFilePath = \u0026#34;/home/sam/flink/myjob.properties\u0026#34;; ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath); File propertiesFile = new File(propertiesFilePath); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile); InputStream propertiesFileInputStream = new FileInputStream(file); ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFileInputStream); 配置值来自命令行 # 以下方法可以从命令行中获取参数，如 --input hdfs:///mydata --elements 42。
public static void main(String[] args) { ParameterTool parameter = ParameterTool.fromArgs(args); // .. regular code .. 配置值来自系统属性 # 启动 JVM 时，可以将系统属性传递给 JVM：-Dinput=hdfs:///mydata。你也可以从这些系统属性初始化 ParameterTool：
ParameterTool parameter = ParameterTool.fromSystemProperties(); 在 Flink 程序中使用参数 # 现在我们已经从某处获取了参数（见上文），可以以各种不同的方式使用它们。
直接从 ParameterTool 获取
ParameterTool 本身具有访问配置值的方法。
ParameterTool parameters = // ... parameter.getRequired(\u0026#34;input\u0026#34;); parameter.get(\u0026#34;output\u0026#34;, \u0026#34;myDefaultValue\u0026#34;); parameter.getLong(\u0026#34;expectedCount\u0026#34;, -1L); parameter.getNumberOfParameters(); // .. there are more methods available. 你可以在提交应用程序时直接在客户端的 main() 方法中使用这些方法的返回值。例如，你可以这样设置算子的并行度：
ParameterTool parameters = ParameterTool.fromArgs(args); int parallelism = parameters.get(\u0026#34;mapParallelism\u0026#34;, 2); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer()).setParallelism(parallelism); 由于 ParameterTool 是序列化的，你可以将其传递给函数本身：
ParameterTool parameters = ParameterTool.fromArgs(args); DataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; counts = text.flatMap(new Tokenizer(parameters)); 然后在函数内使用它以获取命令行的传递的参数。
注册全局参数 # 从 JobManager web 界面和用户定义的所有函数中可以以配置值的方式访问在 ExecutionConfig 中注册的全局作业参数。
注册全局参数：
ParameterTool parameters = ParameterTool.fromArgs(args); // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(parameters); 在任意富函数中访问参数：
public static final class Tokenizer extends RichFlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; { @Override public void flatMap(String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) { ParameterTool parameters = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); parameters.getRequired(\u0026#34;input\u0026#34;); // .. do more .. Back to top
`}),e.add({id:290,href:"/flink/flink-docs-master/zh/docs/ops/state/task_failure_recovery/",title:"Task 故障恢复",section:"状态与容错",content:` Task 故障恢复 # 当 Task 发生故障时，Flink 需要重启出错的 Task 以及其他受到影响的 Task ，以使得作业恢复到正常执行状态。
Flink 通过重启策略和故障恢复策略来控制 Task 重启：重启策略决定是否可以重启以及重启的间隔；故障恢复策略决定哪些 Task 需要重启。
Restart Strategies # Flink 作业如果没有定义重启策略，则会遵循集群启动时加载的默认重启策略。 如果提交作业时设置了重启策略，该策略将覆盖掉集群的默认策略。
通过 Flink 的配置文件 flink-conf.yaml 来设置默认的重启策略。配置参数 restart-strategy 定义了采取何种策略。 如果没有启用 checkpoint，就采用“不重启”策略。如果启用了 checkpoint 且没有配置重启策略，那么就采用固定延时重启策略， 此时最大尝试重启次数由 Integer.MAX_VALUE 参数设置。下表列出了可用的重启策略和与其对应的配置值。
每个重启策略都有自己的一组配置参数来控制其行为。 这些参数也在配置文件中设置。 后文的描述中会详细介绍每种重启策略的配置项。
Key Default Type Description restart-strategy (none) String Defines the restart strategy to use in case of job failures.
Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy. More details can be found here.failurerate, failure-rate: Failure rate restart strategy. More details can be found here.exponentialdelay, exponential-delay: Exponential delay restart strategy. More details can be found here.If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay. 除了定义默认的重启策略以外，还可以为每个 Flink 作业单独定义重启策略。 这个重启策略通过在程序中的 StreamExecutionEnvironment 对象上调用 setRestartStrategy 方法来设置。 当然，对于 StreamExecutionEnvironment 也同样适用。
下例展示了如何给我们的作业设置固定延时重启策略。 如果发生故障，系统会重启作业 3 次，每两次连续的重启尝试之间等待 10 秒钟。
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 延时 )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 延时 )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.fixed_delay_restart( 3, # 尝试重启的次数 10000 # 延时(毫秒) )) 以下部分详细描述重启策略的配置项。
Fixed Delay Restart Strategy # 固定延时重启策略按照给定的次数尝试重启作业。 如果尝试超过了给定的最大次数，作业将最终失败。 在连续的两次重启尝试之间，重启策略等待一段固定长度的时间。
通过在 flink-conf.yaml 中设置如下配置参数，默认启用此策略。
restart-strategy: fixed-delay Key Default Type Description restart-strategy.fixed-delay.attempts 1 Integer The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay. restart-strategy.fixed-delay.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s" 例如：
restart-strategy.fixed-delay.attempts: 3 restart-strategy.fixed-delay.delay: 10 s 固定延迟重启策略也可以在程序中设置：
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 延时 )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 延时 )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.fixed_delay_restart( 3, # 尝试重启的次数 10000 # 延时(毫秒) )) Failure Rate Restart Strategy # 故障率重启策略在故障发生之后重启作业，但是当故障率（每个时间间隔发生故障的次数）超过设定的限制时，作业会最终失败。 在连续的两次重启尝试之间，重启策略等待一段固定长度的时间。
通过在 flink-conf.yaml 中设置如下配置参数，默认启用此策略。
restart-strategy: failure-rate Key Default Type Description restart-strategy.failure-rate.delay 1 s Duration Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.failure-rate-interval 1 min Duration Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s" restart-strategy.failure-rate.max-failures-per-interval 1 Integer Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate. 例如：
restart-strategy.failure-rate.max-failures-per-interval: 3 restart-strategy.failure-rate.failure-rate-interval: 5 min restart-strategy.failure-rate.delay: 10 s 故障率重启策略也可以在程序中设置：
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // 每个时间间隔的最大故障次数 Time.of(5, TimeUnit.MINUTES), // 测量故障率的时间间隔 Time.of(10, TimeUnit.SECONDS) // 延时 )); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // 每个时间间隔的最大故障次数 Time.of(5, TimeUnit.MINUTES), // 测量故障率的时间间隔 Time.of(10, TimeUnit.SECONDS) // 延时 )) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.failure_rate_restart( 3, # 每个时间间隔的最大故障次数 300000, # 测量故障率的时间间隔 10000 # 延时(毫秒) )) No Restart Strategy # 作业直接失败，不尝试重启。
restart-strategy: none 不重启策略也可以在程序中设置：
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.noRestart()); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment() env.setRestartStrategy(RestartStrategies.noRestart()) Python env = StreamExecutionEnvironment.get_execution_environment() env.set_restart_strategy(RestartStrategies.no_restart()) Fallback Restart Strategy # 使用群集定义的重启策略。 这对于启用了 checkpoint 的流处理程序很有帮助。 如果没有定义其他重启策略，默认选择固定延时重启策略。
Failover Strategies # Flink 支持多种不同的故障恢复策略，该策略需要通过 Flink 配置文件 flink-conf.yaml 中的 jobmanager.execution.failover-strategy 配置项进行配置。
故障恢复策略 jobmanager.execution.failover-strategy 配置值 全图重启 full 基于 Region 的局部重启 region Restart All Failover Strategy # 在全图重启故障恢复策略下，Task 发生故障时会重启作业中的所有 Task 进行故障恢复。
Restart Pipelined Region Failover Strategy # 该策略会将作业中的所有 Task 划分为数个 Region。当有 Task 发生故障时，它会尝试找出进行故障恢复需要重启的最小 Region 集合。 相比于全局重启故障恢复策略，这种策略在一些场景下的故障恢复需要重启的 Task 会更少。
此处 Region 指以 Pipelined 形式进行数据交换的 Task 集合。也就是说，Batch 形式的数据交换会构成 Region 的边界。
DataStream 和 流式 Table/SQL 作业的所有数据交换都是 Pipelined 形式的。 批处理式 Table/SQL 作业的所有数据交换默认都是 Batch 形式的。 DataSet 作业中的数据交换形式会根据 ExecutionConfig 中配置的 ExecutionMode 决定。 需要重启的 Region 的判断逻辑如下：
出错 Task 所在 Region 需要重启。 如果要重启的 Region 需要消费的数据有部分无法访问（丢失或损坏），产出该部分数据的 Region 也需要重启。 需要重启的 Region 的下游 Region 也需要重启。这是出于保障数据一致性的考虑，因为一些非确定性的计算或者分发会导致同一个 Result Partition 每次产生时包含的数据都不相同。 Back to top
`}),e.add({id:291,href:"/flink/flink-docs-master/zh/docs/dev/table/functions/udfs/",title:"自定义函数",section:"函数",content:` 自定义函数 # 自定义函数（UDF）是一种扩展开发机制，可以用来在查询语句里调用难以用其他方式表达的频繁使用或自定义的逻辑。
自定义函数可以用 JVM 语言（例如 Java 或 Scala）或 Python 实现，实现者可以在 UDF 中使用任意第三方库，本文聚焦于使用 JVM 语言开发自定义函数。
概述 # 当前 Flink 有如下几种函数：
标量函数 将标量值转换成一个新标量值； 表值函数 将标量值转换成新的行数据； 聚合函数 将多行数据里的标量值转换成一个新标量值； 表值聚合函数 将多行数据里的标量值转换成新的行数据； 异步表值函数 是异步查询外部数据系统的特殊函数。 注意 标量和表值函数已经使用了新的基于数据类型的类型系统，聚合函数仍然使用基于 TypeInformation 的旧类型系统。
以下示例展示了如何创建一个基本的标量函数，以及如何在 Table API 和 SQL 里调用这个函数。
函数用于 SQL 查询前要先经过注册；而在用于 Table API 时，函数可以先注册后调用，也可以 内联 后直接使用。
Java import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; // 定义函数逻辑 public static class SubstringFunction extends ScalarFunction { public String eval(String s, Integer begin, Integer end) { return s.substring(begin, end); } } TableEnvironment env = TableEnvironment.create(...); // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(SubstringFunction.class, \$(\u0026#34;myField\u0026#34;), 5, 12)); // 注册函数 env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, SubstringFunction.class); // 在 Table API 里调用注册好的函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;SubstringFunction\u0026#34;, \$(\u0026#34;myField\u0026#34;), 5, 12)); // 在 SQL 里调用注册好的函数 env.sqlQuery(\u0026#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // define function logic class SubstringFunction extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(begin, end) } } val env = TableEnvironment.create(...) // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[SubstringFunction], \$\u0026#34;myField\u0026#34;, 5, 12)) // 注册函数 env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, classOf[SubstringFunction]) // 在 Table API 里调用注册好的函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;SubstringFunction\u0026#34;, \$\u0026#34;myField\u0026#34;, 5, 12)) // 在 SQL 里调用注册好的函数 env.sqlQuery(\u0026#34;SELECT SubstringFunction(myField, 5, 12) FROM MyTable\u0026#34;) 对于交互式会话，还可以在使用或注册函数之前对其进行参数化，这样可以把函数 实例 而不是函数 类 用作临时函数。
为确保函数实例可应用于集群环境，参数必须是可序列化的。
Java import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; // 定义可参数化的函数逻辑 public static class SubstringFunction extends ScalarFunction { private boolean endInclusive; public SubstringFunction(boolean endInclusive) { this.endInclusive = endInclusive; } public String eval(String s, Integer begin, Integer end) { return s.substring(begin, endInclusive ? end + 1 : end); } } TableEnvironment env = TableEnvironment.create(...); // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(new SubstringFunction(true), \$(\u0026#34;myField\u0026#34;), 5, 12)); // 注册函数 env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, new SubstringFunction(true)); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction // 定义可参数化的函数逻辑 class SubstringFunction(val endInclusive) extends ScalarFunction { def eval(s: String, begin: Integer, end: Integer): String = { s.substring(endInclusive ? end + 1 : end) } } val env = TableEnvironment.create(...) // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(new SubstringFunction(true), \$\u0026#34;myField\u0026#34;, 5, 12)) // 注册函数 env.createTemporarySystemFunction(\u0026#34;SubstringFunction\u0026#34;, new SubstringFunction(true)) 你可以在 Table API 中使用 * 表达式作为函数的一个参数，它将被扩展为该表所有的列作为函数对应位置的参数。
Java import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; public static class MyConcatFunction extends ScalarFunction { public String eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object... fields) { return Arrays.stream(fields) .map(Object::toString) .collect(Collectors.joining(\u0026#34;,\u0026#34;)); } } TableEnvironment env = TableEnvironment.create(...); // 使用 \$(\u0026#34;*\u0026#34;) 作为函数的参数，如果 MyTable 有 3 列 (a, b, c)， // 它们都将会被传给 MyConcatFunction。 env.from(\u0026#34;MyTable\u0026#34;).select(call(MyConcatFunction.class, \$(\u0026#34;*\u0026#34;))); // 它等价于显式地将所有列传给 MyConcatFunction。 env.from(\u0026#34;MyTable\u0026#34;).select(call(MyConcatFunction.class, \$(\u0026#34;a\u0026#34;), \`(\u0026#34;b\u0026#34;), \`(\u0026#34;c\u0026#34;))); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction import scala.annotation.varargs class MyConcatFunction extends ScalarFunction { @varargs def eval(@DataTypeHint(inputGroup = InputGroup.ANY) row: AnyRef*): String = { row.map(f =\u0026gt; f.toString).mkString(\u0026#34;,\u0026#34;) } } val env = TableEnvironment.create(...) // 使用 \`\u0026#34;*\u0026#34; 作为函数的参数，如果 MyTable 有 3 个列 (a, b, c)， // 它们都将会被传给 MyConcatFunction。 env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[MyConcatFunction], \`\u0026#34;*\u0026#34;)); // 它等价于显式地将所有列传给 MyConcatFunction。 env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[MyConcatFunction], \`\u0026#34;a\u0026#34;, \`\u0026#34;b\u0026#34;, \`\u0026#34;c\u0026#34;)); Back to top
开发指南 # 注意在聚合函数使用新的类型系统前，本节仅适用于标量和表值函数。
所有的自定义函数都遵循一些基本的实现原则。
函数类 # 实现类必须继承自合适的基类之一（例如 org.apache.flink.table.functions.ScalarFunction ）。
该类必须声明为 public ，而不是 abstract ，并且可以被全局访问。不允许使用非静态内部类或匿名类。
为了将自定义函数存储在持久化的 catalog 中，该类必须具有默认构造器，且在运行时可实例化。
Anonymous functions in Table API can only be persisted if the function is not stateful (i.e. containing only transient and static fields).
求值方法 # 基类提供了一组可以被重写的方法，例如 open()、 close() 或 isDeterministic() 。
但是，除了上述方法之外，作用于每条传入记录的主要逻辑还必须通过专门的 求值方法 来实现。
根据函数的种类，后台生成的运算符会在运行时调用诸如 eval()、accumulate() 或 retract() 之类的求值方法。
这些方法必须声明为 public ，并带有一组定义明确的参数。
常规的 JVM 方法调用语义是适用的。因此可以：
实现重载的方法，例如 eval(Integer) 和 eval(LocalDateTime)； 使用变长参数，例如 eval(Integer...); 使用对象继承，例如 eval(Object) 可接受 LocalDateTime 和 Integer 作为参数； 也可组合使用，例如 eval(Object...) 可接受所有类型的参数。 以下代码片段展示了一个重载函数的示例：
Java import org.apache.flink.table.functions.ScalarFunction; // 有多个重载求值方法的函数 public static class SumFunction extends ScalarFunction { public Integer eval(Integer a, Integer b) { return a + b; } public Integer eval(String a, String b) { return Integer.valueOf(a) + Integer.valueOf(b); } public Integer eval(Double... d) { double result = 0; for (double value : d) result += value; return (int) result; } } Scala import org.apache.flink.table.functions.ScalarFunction import scala.annotation.varargs // 有多个重载求值方法的函数 class SumFunction extends ScalarFunction { def eval(a: Integer, b: Integer): Integer = { a + b } def eval(a: String, b: String): Integer = { Integer.valueOf(a) + Integer.valueOf(b) } @varargs // generate var-args like Java def eval(d: Double*): Integer = { d.sum.toInt } } 类型推导 # Table（类似于 SQL 标准）是一种强类型的 API。因此，函数的参数和返回类型都必须映射到数据类型。
从逻辑角度看，Planner 需要知道数据类型、精度和小数位数；从 JVM 角度来看，Planner 在调用自定义函数时需要知道如何将内部数据结构表示为 JVM 对象。
术语 类型推导 概括了意在验证输入值、派生出参数/返回值数据类型的逻辑。
Flink 自定义函数实现了自动的类型推导提取，通过反射从函数的类及其求值方法中派生数据类型。如果这种隐式的反射提取方法不成功，则可以通过使用 @DataTypeHint 和 @FunctionHint 注解相关参数、类或方法来支持提取过程，下面展示了有关如何注解函数的例子。
如果需要更高级的类型推导逻辑，实现者可以在每个自定义函数中显式重写 getTypeInference() 方法。但是，建议使用注解方式，因为它可使自定义类型推导逻辑保持在受影响位置附近，而在其他位置则保持默认状态。
自动类型推导 # 自动类型推导会检查函数的类和求值方法，派生出函数参数和结果的数据类型， @DataTypeHint 和 @FunctionHint 注解支持自动类型推导。
有关可以隐式映射到数据类型的类的完整列表，请参阅数据类型。
@DataTypeHint
在许多情况下，需要支持以 内联 方式自动提取出函数参数、返回值的类型。
以下例子展示了如何使用 @DataTypeHint，详情可参考该注解类的文档。
Java import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.InputGroup; import org.apache.flink.table.functions.ScalarFunction; import org.apache.flink.types.Row; // 有多个重载求值方法的函数 public static class OverloadedFunction extends ScalarFunction { // no hint required public Long eval(long a, long b) { return a + b; } // 定义 decimal 的精度和小数位 public @DataTypeHint(\u0026#34;DECIMAL(12, 3)\u0026#34;) BigDecimal eval(double a, double b) { return BigDecimal.valueOf(a + b); } // 定义嵌套数据类型 @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, t TIMESTAMP_LTZ(3)\u0026gt;\u0026#34;) public Row eval(int i) { return Row.of(String.valueOf(i), Instant.ofEpochSecond(i)); } // 允许任意类型的符入，并输出序列化定制后的值 @DataTypeHint(value = \u0026#34;RAW\u0026#34;, bridgedTo = ByteBuffer.class) public ByteBuffer eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o) { return MyUtils.serializeToByteBuffer(o); } } Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.functions.ScalarFunction import org.apache.flink.types.Row import scala.annotation.varargs // function with overloaded evaluation methods class OverloadedFunction extends ScalarFunction { // no hint required def eval(a: Long, b: Long): Long = { a + b } // 定义 decimal 的精度和小数位 @DataTypeHint(\u0026#34;DECIMAL(12, 3)\u0026#34;) def eval(double a, double b): BigDecimal = { java.lang.BigDecimal.valueOf(a + b) } // 定义嵌套数据类型 @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, t TIMESTAMP_LTZ(3)\u0026gt;\u0026#34;) def eval(Int i): Row = { Row.of(java.lang.String.valueOf(i), java.time.Instant.ofEpochSecond(i)) } // 允许任意类型的符入，并输出定制序列化后的值 @DataTypeHint(value = \u0026#34;RAW\u0026#34;, bridgedTo = classOf[java.nio.ByteBuffer]) def eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o): java.nio.ByteBuffer = { MyUtils.serializeToByteBuffer(o) } } @FunctionHint
有时我们希望一种求值方法可以同时处理多种数据类型，有时又要求对重载的多个求值方法仅声明一次通用的结果类型。
@FunctionHint 注解可以提供从入参数据类型到结果数据类型的映射，它可以在整个函数类或求值方法上注解输入、累加器和结果的数据类型。可以在类顶部声明一个或多个注解，也可以为类的所有求值方法分别声明一个或多个注解。所有的 hint 参数都是可选的，如果未定义参数，则使用默认的基于反射的类型提取。在函数类顶部定义的 hint 参数被所有求值方法继承。
以下例子展示了如何使用 @FunctionHint，详情可参考该注解类的文档。
Java import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.FunctionHint; import org.apache.flink.table.functions.TableFunction; import org.apache.flink.types.Row; // 为函数类的所有求值方法指定同一个输出类型 @FunctionHint(output = @DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, i INT\u0026gt;\u0026#34;)) public static class OverloadedFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(int a, int b) { collect(Row.of(\u0026#34;Sum\u0026#34;, a + b)); } // overloading of arguments is still possible public void eval() { collect(Row.of(\u0026#34;Empty args\u0026#34;, -1)); } } // 解耦类型推导与求值方法，类型推导完全取决于 FunctionHint @FunctionHint( input = {@DataTypeHint(\u0026#34;INT\u0026#34;), @DataTypeHint(\u0026#34;INT\u0026#34;)}, output = @DataTypeHint(\u0026#34;INT\u0026#34;) ) @FunctionHint( input = {@DataTypeHint(\u0026#34;BIGINT\u0026#34;), @DataTypeHint(\u0026#34;BIGINT\u0026#34;)}, output = @DataTypeHint(\u0026#34;BIGINT\u0026#34;) ) @FunctionHint( input = {}, output = @DataTypeHint(\u0026#34;BOOLEAN\u0026#34;) ) public static class OverloadedFunction extends TableFunction\u0026lt;Object\u0026gt; { // an implementer just needs to make sure that a method exists // that can be called by the JVM public void eval(Object... o) { if (o.length == 0) { collect(false); } collect(o[0]); } } Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row // 为函数类的所有求值方法指定同一个输出类型 @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;s STRING, i INT\u0026gt;\u0026#34;)) class OverloadedFunction extends TableFunction[Row] { def eval(a: Int, b: Int): Unit = { collect(Row.of(\u0026#34;Sum\u0026#34;, Int.box(a + b))) } // overloading of arguments is still possible def eval(): Unit = { collect(Row.of(\u0026#34;Empty args\u0026#34;, Int.box(-1))) } } // 解耦类型推导与求值方法，类型推导完全取决于 @FunctionHint @FunctionHint( input = Array(new DataTypeHint(\u0026#34;INT\u0026#34;), new DataTypeHint(\u0026#34;INT\u0026#34;)), output = new DataTypeHint(\u0026#34;INT\u0026#34;) ) @FunctionHint( input = Array(new DataTypeHint(\u0026#34;BIGINT\u0026#34;), new DataTypeHint(\u0026#34;BIGINT\u0026#34;)), output = new DataTypeHint(\u0026#34;BIGINT\u0026#34;) ) @FunctionHint( input = Array(), output = new DataTypeHint(\u0026#34;BOOLEAN\u0026#34;) ) class OverloadedFunction extends TableFunction[AnyRef] { // an implementer just needs to make sure that a method exists // that can be called by the JVM @varargs def eval(o: AnyRef*) = { if (o.length == 0) { collect(Boolean.box(false)) } collect(o(0)) } } 定制类型推导 # 在大多数情况下，@DataTypeHint 和 @FunctionHint 足以构建自定义函数，然而通过重写 getTypeInference() 定制自动类型推导逻辑，实现者可以创建任意像系统内置函数那样有用的函数。
以下用 Java 实现的例子展示了定制类型推导的潜力，它根据字符串参数来确定函数的结果类型。该函数带有两个字符串参数：第一个参数表示要分析的字符串，第二个参数表示目标类型。
Java import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.catalog.DataTypeFactory; import org.apache.flink.table.functions.ScalarFunction; import org.apache.flink.table.types.inference.TypeInference; import org.apache.flink.types.Row; public static class LiteralFunction extends ScalarFunction { public Object eval(String s, String type) { switch (type) { case \u0026#34;INT\u0026#34;: return Integer.valueOf(s); case \u0026#34;DOUBLE\u0026#34;: return Double.valueOf(s); case \u0026#34;STRING\u0026#34;: default: return s; } } // 禁用自动的反射式类型推导，使用如下逻辑进行类型推导 @Override public TypeInference getTypeInference(DataTypeFactory typeFactory) { return TypeInference.newBuilder() // 指定输入参数的类型，必要时参数会被隐式转换 .typedArguments(DataTypes.STRING(), DataTypes.STRING()) // specify a strategy for the result data type of the function .outputTypeStrategy(callContext -\u0026gt; { if (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) { throw callContext.newValidationError(\u0026#34;Literal expected for second argument.\u0026#34;); } // 基于字符串值返回数据类型 final String literal = callContext.getArgumentValue(1, String.class).orElse(\u0026#34;STRING\u0026#34;); switch (literal) { case \u0026#34;INT\u0026#34;: return Optional.of(DataTypes.INT().notNull()); case \u0026#34;DOUBLE\u0026#34;: return Optional.of(DataTypes.DOUBLE().notNull()); case \u0026#34;STRING\u0026#34;: default: return Optional.of(DataTypes.STRING()); } }) .build(); } } For more examples of custom type inference, see also the flink-examples-table module with advanced function implementation .
运行时集成 # 有时候自定义函数需要获取一些全局信息，或者在真正被调用之前做一些配置（setup）/清理（clean-up）的工作。自定义函数也提供了 open() 和 close() 方法，你可以重写这两个方法做到类似于 DataStream API 中 RichFunction 的功能。
open() 方法在求值方法被调用之前先调用。close() 方法在求值方法调用完之后被调用。
open() 方法提供了一个 FunctionContext，它包含了一些自定义函数被执行时的上下文信息，比如 metric group、分布式文件缓存，或者是全局的作业参数等。
下面的信息可以通过调用 FunctionContext 的对应的方法来获得：
方法 描述 getMetricGroup() 执行该函数的 subtask 的 Metric Group。 getCachedFile(name) 分布式文件缓存的本地临时文件副本。 getJobParameter(name, defaultValue) 跟对应的 key 关联的全局参数值。 下面的例子展示了如何在一个标量函数中通过 FunctionContext 来获取一个全局的任务参数：
Java import org.apache.flink.table.api.*; import org.apache.flink.table.functions.FunctionContext; import org.apache.flink.table.functions.ScalarFunction; public static class HashCodeFunction extends ScalarFunction { private int factor = 0; @Override public void open(FunctionContext context) throws Exception { // 获取参数 \u0026#34;hashcode_factor\u0026#34; // 如果不存在，则使用默认值 \u0026#34;12\u0026#34; factor = Integer.parseInt(context.getJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;12\u0026#34;)); } public int eval(String s) { return s.hashCode() * factor; } } TableEnvironment env = TableEnvironment.create(...); // 设置任务参数 env.getConfig().addJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;31\u0026#34;); // 注册函数 env.createTemporarySystemFunction(\u0026#34;hashCode\u0026#34;, HashCodeFunction.class); // 调用函数 env.sqlQuery(\u0026#34;SELECT myField, hashCode(myField) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.api._ import org.apache.flink.table.functions.FunctionContext import org.apache.flink.table.functions.ScalarFunction class HashCodeFunction extends ScalarFunction { private var factor: Int = 0 override def open(context: FunctionContext): Unit = { // 获取参数 \u0026#34;hashcode_factor\u0026#34; // 如果不存在，则使用默认值 \u0026#34;12\u0026#34; factor = context.getJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;12\u0026#34;).toInt } def eval(s: String): Int = { s.hashCode * factor } } val env = TableEnvironment.create(...) // 设置任务参数 env.getConfig.addJobParameter(\u0026#34;hashcode_factor\u0026#34;, \u0026#34;31\u0026#34;) // 注册函数 env.createTemporarySystemFunction(\u0026#34;hashCode\u0026#34;, classOf[HashCodeFunction]) // 调用函数 env.sqlQuery(\u0026#34;SELECT myField, hashCode(myField) FROM MyTable\u0026#34;) Back to top
标量函数 # 自定义标量函数可以把 0 到多个标量值映射成 1 个标量值，数据类型里列出的任何数据类型都可作为求值方法的参数和返回值类型。
想要实现自定义标量函数，你需要扩展 org.apache.flink.table.functions 里面的 ScalarFunction 并且实现一个或者多个求值方法。标量函数的行为取决于你写的求值方法。求值方法必须是 public 的，而且名字必须是 eval。
下面的例子展示了如何实现一个求哈希值的函数并在查询里调用它，详情可参考开发指南：
Java import org.apache.flink.table.annotation.InputGroup; import org.apache.flink.table.api.*; import org.apache.flink.table.functions.ScalarFunction; import static org.apache.flink.table.api.Expressions.*; public static class HashFunction extends ScalarFunction { // 接受任意类型输入，返回 INT 型输出 public int eval(@DataTypeHint(inputGroup = InputGroup.ANY) Object o) { return o.hashCode(); } } TableEnvironment env = TableEnvironment.create(...); // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(HashFunction.class, \$(\u0026#34;myField\u0026#34;))); // 注册函数 env.createTemporarySystemFunction(\u0026#34;HashFunction\u0026#34;, HashFunction.class); // 在 Table API 里调用注册好的函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;HashFunction\u0026#34;, \$(\u0026#34;myField\u0026#34;))); // 在 SQL 里调用注册好的函数 env.sqlQuery(\u0026#34;SELECT HashFunction(myField) FROM MyTable\u0026#34;); Scala import org.apache.flink.table.annotation.InputGroup import org.apache.flink.table.api._ import org.apache.flink.table.functions.ScalarFunction class HashFunction extends ScalarFunction { // 接受任意类型输入，返回 INT 型输出 def eval(@DataTypeHint(inputGroup = InputGroup.ANY) o: AnyRef): Int { return o.hashCode(); } } val env = TableEnvironment.create(...) // 在 Table API 里不经注册直接“内联”调用函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(classOf[HashFunction], \$\u0026#34;myField\u0026#34;)) // 注册函数 env.createTemporarySystemFunction(\u0026#34;HashFunction\u0026#34;, classOf[HashFunction]) // 在 Table API 里调用注册好的函数 env.from(\u0026#34;MyTable\u0026#34;).select(call(\u0026#34;HashFunction\u0026#34;, \$\u0026#34;myField\u0026#34;)) // 在 SQL 里调用注册好的函数 env.sqlQuery(\u0026#34;SELECT HashFunction(myField) FROM MyTable\u0026#34;) 如果你打算使用 Python 实现或调用标量函数，详情可参考 Python 标量函数。
Back to top
表值函数 # 跟自定义标量函数一样，自定义表值函数的输入参数也可以是 0 到多个标量。但是跟标量函数只能返回一个值不同的是，它可以返回任意多行。返回的每一行可以包含 1 到多列，如果输出行只包含 1 列，会省略结构化信息并生成标量值，这个标量值在运行阶段会隐式地包装进行里。
要定义一个表值函数，你需要扩展 org.apache.flink.table.functions 下的 TableFunction，可以通过实现多个名为 eval 的方法对求值方法进行重载。像其他函数一样，输入和输出类型也可以通过反射自动提取出来。表值函数返回的表的类型取决于 TableFunction 类的泛型参数 T，不同于标量函数，表值函数的求值方法本身不包含返回类型，而是通过 collect(T) 方法来发送要输出的行。
在 Table API 中，表值函数是通过 .joinLateral(...) 或者 .leftOuterJoinLateral(...) 来使用的。joinLateral 算子会把外表（算子左侧的表）的每一行跟跟表值函数返回的所有行（位于算子右侧）进行 （cross）join。leftOuterJoinLateral 算子也是把外表（算子左侧的表）的每一行跟表值函数返回的所有行（位于算子右侧）进行（cross）join，并且如果表值函数返回 0 行也会保留外表的这一行。
在 SQL 里面用 JOIN 或者 以 ON TRUE 为条件的 LEFT JOIN 来配合 LATERAL TABLE(\u0026lt;TableFunction\u0026gt;) 的使用。
下面的例子展示了如何实现一个分隔函数并在查询里调用它，详情可参考开发指南：
Java import org.apache.flink.table.annotation.DataTypeHint; import org.apache.flink.table.annotation.FunctionHint; import org.apache.flink.table.api.*; import org.apache.flink.table.functions.TableFunction; import org.apache.flink.types.Row; import static org.apache.flink.table.api.Expressions.*; @FunctionHint(output = @DataTypeHint(\u0026#34;ROW\u0026lt;word STRING, length INT\u0026gt;\u0026#34;)) public static class SplitFunction extends TableFunction\u0026lt;Row\u0026gt; { public void eval(String str) { for (String s : str.split(\u0026#34; \u0026#34;)) { // use collect(...) to emit a row collect(Row.of(s, s.length())); } } } TableEnvironment env = TableEnvironment.create(...); // 在 Table API 里不经注册直接“内联”调用函数 env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(SplitFunction.class, \$(\u0026#34;myField\u0026#34;))) .select(\$(\u0026#34;myField\u0026#34;), \$(\u0026#34;word\u0026#34;), \$(\u0026#34;length\u0026#34;)); env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(SplitFunction.class, \$(\u0026#34;myField\u0026#34;))) .select(\$(\u0026#34;myField\u0026#34;), \$(\u0026#34;word\u0026#34;), \$(\u0026#34;length\u0026#34;)); // 在 Table API 里重命名函数字段 env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(SplitFunction.class, \$(\u0026#34;myField\u0026#34;)).as(\u0026#34;newWord\u0026#34;, \u0026#34;newLength\u0026#34;)) .select(\$(\u0026#34;myField\u0026#34;), \$(\u0026#34;newWord\u0026#34;), \$(\u0026#34;newLength\u0026#34;)); // 注册函数 env.createTemporarySystemFunction(\u0026#34;SplitFunction\u0026#34;, SplitFunction.class); // 在 Table API 里调用注册好的函数 env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(\u0026#34;SplitFunction\u0026#34;, \$(\u0026#34;myField\u0026#34;))) .select(\$(\u0026#34;myField\u0026#34;), \$(\u0026#34;word\u0026#34;), \$(\u0026#34;length\u0026#34;)); env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(\u0026#34;SplitFunction\u0026#34;, \$(\u0026#34;myField\u0026#34;))) .select(\$(\u0026#34;myField\u0026#34;), \$(\u0026#34;word\u0026#34;), \$(\u0026#34;length\u0026#34;)); // 在 SQL 里调用注册好的函数 env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))\u0026#34;); env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE\u0026#34;); // 在 SQL 里重命名函数字段 env.sqlQuery( \u0026#34;SELECT myField, newWord, newLength \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE\u0026#34;); Scala import org.apache.flink.table.annotation.DataTypeHint import org.apache.flink.table.annotation.FunctionHint import org.apache.flink.table.api._ import org.apache.flink.table.functions.TableFunction import org.apache.flink.types.Row @FunctionHint(output = new DataTypeHint(\u0026#34;ROW\u0026lt;word STRING, length INT\u0026gt;\u0026#34;)) class SplitFunction extends TableFunction[Row] { def eval(str: String): Unit = { // use collect(...) to emit a row str.split(\u0026#34; \u0026#34;).foreach(s =\u0026gt; collect(Row.of(s, Int.box(s.length)))) } } val env = TableEnvironment.create(...) // 在 Table API 里不经注册直接“内联”调用函数 env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(classOf[SplitFunction], \$\u0026#34;myField\u0026#34;) .select(\$\u0026#34;myField\u0026#34;, \$\u0026#34;word\u0026#34;, \$\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], \$\u0026#34;myField\u0026#34;)) .select(\$\u0026#34;myField\u0026#34;, \$\u0026#34;word\u0026#34;, \$\u0026#34;length\u0026#34;) // 在 Table API 里重命名函数字段 env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(classOf[SplitFunction], \$\u0026#34;myField\u0026#34;).as(\u0026#34;newWord\u0026#34;, \u0026#34;newLength\u0026#34;)) .select(\$\u0026#34;myField\u0026#34;, \$\u0026#34;newWord\u0026#34;, \$\u0026#34;newLength\u0026#34;) // 注册函数 env.createTemporarySystemFunction(\u0026#34;SplitFunction\u0026#34;, classOf[SplitFunction]) // 在 Table API 里调用注册好的函数 env .from(\u0026#34;MyTable\u0026#34;) .joinLateral(call(\u0026#34;SplitFunction\u0026#34;, \$\u0026#34;myField\u0026#34;)) .select(\$\u0026#34;myField\u0026#34;, \$\u0026#34;word\u0026#34;, \$\u0026#34;length\u0026#34;) env .from(\u0026#34;MyTable\u0026#34;) .leftOuterJoinLateral(call(\u0026#34;SplitFunction\u0026#34;, \$\u0026#34;myField\u0026#34;)) .select(\$\u0026#34;myField\u0026#34;, \$\u0026#34;word\u0026#34;, \$\u0026#34;length\u0026#34;) // 在 SQL 里调用注册好的函数 env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable, LATERAL TABLE(SplitFunction(myField))\u0026#34;); env.sqlQuery( \u0026#34;SELECT myField, word, length \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE\u0026#34;) // 在 SQL 里重命名函数字段 env.sqlQuery( \u0026#34;SELECT myField, newWord, newLength \u0026#34; + \u0026#34;FROM MyTable \u0026#34; + \u0026#34;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE\u0026#34;) 如果你打算使用 Scala，不要把表值函数声明为 Scala object，Scala object 是单例对象，将导致并发问题。
如果你打算使用 Python 实现或调用表值函数，详情可参考 Python 表值函数。
Back to top
聚合函数 # 自定义聚合函数（UDAGG）是把一个表（一行或者多行，每行可以有一列或者多列）聚合成一个标量值。
上面的图片展示了一个聚合的例子。假设你有一个关于饮料的表。表里面有三个字段，分别是 id、name、price，表里有 5 行数据。假设你需要找到所有饮料里最贵的饮料的价格，即执行一个 max() 聚合。你需要遍历所有 5 行数据，而结果就只有一个数值。
自定义聚合函数是通过扩展 AggregateFunction 来实现的。AggregateFunction 的工作过程如下。首先，它需要一个 accumulator，它是一个数据结构，存储了聚合的中间结果。通过调用 AggregateFunction 的 createAccumulator() 方法创建一个空的 accumulator。接下来，对于每一行数据，会调用 accumulate() 方法来更新 accumulator。当所有的数据都处理完了之后，通过调用 getValue 方法来计算和返回最终的结果。
下面几个方法是每个 AggregateFunction 必须要实现的：
createAccumulator() accumulate() getValue() Flink 的类型推导在遇到复杂类型的时候可能会推导出错误的结果，比如那些非基本类型和普通的 POJO 类型的复杂类型。所以跟 ScalarFunction 和 TableFunction 一样，AggregateFunction 也提供了 AggregateFunction#getResultType() 和 AggregateFunction#getAccumulatorType() 来分别指定返回值类型和 accumulator 的类型，两个函数的返回值类型也都是 TypeInformation。
除了上面的方法，还有几个方法可以选择实现。这些方法有些可以让查询更加高效，而有些是在某些特定场景下必须要实现的。例如，如果聚合函数用在会话窗口（当两个会话窗口合并的时候需要 merge 他们的 accumulator）的话，merge() 方法就是必须要实现的。
AggregateFunction 的以下方法在某些场景下是必须实现的：
retract() 在 bounded OVER 窗口中是必须实现的。 merge() 在许多批式聚合和会话以及滚动窗口聚合中是必须实现的。除此之外，这个方法对于优化也很多帮助。例如，两阶段聚合优化就需要所有的 AggregateFunction 都实现 merge 方法。 resetAccumulator() 在许多批式聚合中是必须实现的。 AggregateFunction 的所有方法都必须是 public 的，不能是 static 的，而且名字必须跟上面写的一样。createAccumulator、getValue、getResultType 以及 getAccumulatorType 这几个函数是在抽象类 AggregateFunction 中定义的，而其他函数都是约定的方法。如果要定义一个聚合函数，你需要扩展 org.apache.flink.table.functions.AggregateFunction，并且实现一个（或者多个）accumulate 方法。accumulate 方法可以重载，每个方法的参数类型不同，并且支持变长参数。
AggregateFunction 的所有方法的详细文档如下。
Java /** * Base class for user-defined aggregates and table aggregates. * * @param \u0026lt;T\u0026gt; the type of the aggregation result. * @param \u0026lt;ACC\u0026gt; the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ public abstract class UserDefinedAggregateFunction\u0026lt;T, ACC\u0026gt; extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ public ACC createAccumulator(); // MANDATORY /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ public TypeInformation\u0026lt;T\u0026gt; getResultType = null; // PRE-DEFINED /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ public TypeInformation\u0026lt;ACC\u0026gt; getAccumulatorType = null; // PRE-DEFINED } /** * Base class for aggregation functions. * * @param \u0026lt;T\u0026gt; the type of the aggregation result * @param \u0026lt;ACC\u0026gt; the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. * AggregateFunction represents its state using accumulator, thereby the state of the * AggregateFunction must be put into the accumulator. */ public abstract class AggregateFunction\u0026lt;T, ACC\u0026gt; extends UserDefinedAggregateFunction\u0026lt;T, ACC\u0026gt; { /** Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An AggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ public void accumulate(ACC accumulator, [user defined inputs]); // MANDATORY /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ public void retract(ACC accumulator, [user defined inputs]); // OPTIONAL /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and bounded grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an {@link java.lang.Iterable} pointed to a group of accumulators that will be * merged. */ public void merge(ACC accumulator, java.lang.Iterable\u0026lt;ACC\u0026gt; its); // OPTIONAL /** * Called every time when an aggregation result should be materialized. * The returned value could be either an early and incomplete result * (periodically emitted as data arrive) or the final result of the * aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @return the aggregation result */ public T getValue(ACC accumulator); // MANDATORY /** * Resets the accumulator for this [[AggregateFunction]]. This function must be implemented for * bounded grouping aggregate. * * @param accumulator the accumulator which needs to be reset */ public void resetAccumulator(ACC accumulator); // OPTIONAL /** * Returns true if this AggregateFunction can only be applied in an OVER window. * * @return true if the AggregateFunction requires an OVER window, false otherwise. */ public Boolean requiresOver = false; // PRE-DEFINED } Scala /** * Base class for user-defined aggregates and table aggregates. * * @tparam T the type of the aggregation result. * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ abstract class UserDefinedAggregateFunction[T, ACC] extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ def createAccumulator(): ACC // MANDATORY /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ def getResultType: TypeInformation[T] = null // PRE-DEFINED /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ def getAccumulatorType: TypeInformation[ACC] = null // PRE-DEFINED } /** * Base class for aggregation functions. * * @tparam T the type of the aggregation result * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. * AggregateFunction represents its state using accumulator, thereby the state of the * AggregateFunction must be put into the accumulator. */ abstract class AggregateFunction[T, ACC] extends UserDefinedAggregateFunction[T, ACC] { /** * Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. An AggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit // MANDATORY /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit // OPTIONAL /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and bounded grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an [[java.lang.Iterable]] pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, its: java.lang.Iterable[ACC]): Unit // OPTIONAL /** * Called every time when an aggregation result should be materialized. * The returned value could be either an early and incomplete result * (periodically emitted as data arrive) or the final result of the * aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @return the aggregation result */ def getValue(accumulator: ACC): T // MANDATORY /** * Resets the accumulator for this [[AggregateFunction]]. This function must be implemented for * bounded grouping aggregate. * * @param accumulator the accumulator which needs to be reset */ def resetAccumulator(accumulator: ACC): Unit // OPTIONAL /** * Returns true if this AggregateFunction can only be applied in an OVER window. * * @return true if the AggregateFunction requires an OVER window, false otherwise. */ def requiresOver: Boolean = false // PRE-DEFINED } 下面的例子展示了如何：
定义一个聚合函数来计算某一列的加权平均， 在 TableEnvironment 中注册函数， 在查询中使用函数。 为了计算加权平均值，accumulator 需要存储加权总和以及数据的条数。在我们的例子里，我们定义了一个类 WeightedAvgAccum 来作为 accumulator。Flink 的 checkpoint 机制会自动保存 accumulator，在失败时进行恢复，以此来保证精确一次的语义。
我们的 WeightedAvg（聚合函数）的 accumulate 方法有三个输入参数。第一个是 WeightedAvgAccum accumulator，另外两个是用户自定义的输入：输入的值 ivalue 和 输入的权重 iweight。尽管 retract()、merge()、resetAccumulator() 这几个方法在大多数聚合类型中都不是必须实现的，我们也在样例中提供了他们的实现。请注意我们在 Scala 样例中也是用的是 Java 的基础类型，并且定义了 getResultType() 和 getAccumulatorType()，因为 Flink 的类型推导对于 Scala 的类型推导做的不是很好。
Java /** * Accumulator for WeightedAvg. */ public static class WeightedAvgAccum { public long sum = 0; public int count = 0; } /** * Weighted Average user-defined aggregate function. */ public static class WeightedAvg extends AggregateFunction\u0026lt;Long, WeightedAvgAccum\u0026gt; { @Override public WeightedAvgAccum createAccumulator() { return new WeightedAvgAccum(); } @Override public Long getValue(WeightedAvgAccum acc) { if (acc.count == 0) { return null; } else { return acc.sum / acc.count; } } public void accumulate(WeightedAvgAccum acc, long iValue, int iWeight) { acc.sum += iValue * iWeight; acc.count += iWeight; } public void retract(WeightedAvgAccum acc, long iValue, int iWeight) { acc.sum -= iValue * iWeight; acc.count -= iWeight; } public void merge(WeightedAvgAccum acc, Iterable\u0026lt;WeightedAvgAccum\u0026gt; it) { Iterator\u0026lt;WeightedAvgAccum\u0026gt; iter = it.iterator(); while (iter.hasNext()) { WeightedAvgAccum a = iter.next(); acc.count += a.count; acc.sum += a.sum; } } public void resetAccumulator(WeightedAvgAccum acc) { acc.count = 0; acc.sum = 0L; } } // 注册函数 StreamTableEnvironment tEnv = ... tEnv.registerFunction(\u0026#34;wAvg\u0026#34;, new WeightedAvg()); // 使用函数 tEnv.sqlQuery(\u0026#34;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user\u0026#34;); Scala import java.lang.{Long =\u0026gt; JLong, Integer =\u0026gt; JInteger} import org.apache.flink.api.java.tuple.{Tuple1 =\u0026gt; JTuple1} import org.apache.flink.api.java.typeutils.TupleTypeInfo import org.apache.flink.table.api.Types import org.apache.flink.table.functions.AggregateFunction /** * Accumulator for WeightedAvg. */ class WeightedAvgAccum extends JTuple1[JLong, JInteger] { sum = 0L count = 0 } /** * Weighted Average user-defined aggregate function. */ class WeightedAvg extends AggregateFunction[JLong, CountAccumulator] { override def createAccumulator(): WeightedAvgAccum = { new WeightedAvgAccum } override def getValue(acc: WeightedAvgAccum): JLong = { if (acc.count == 0) { null } else { acc.sum / acc.count } } def accumulate(acc: WeightedAvgAccum, iValue: JLong, iWeight: JInteger): Unit = { acc.sum += iValue * iWeight acc.count += iWeight } def retract(acc: WeightedAvgAccum, iValue: JLong, iWeight: JInteger): Unit = { acc.sum -= iValue * iWeight acc.count -= iWeight } def merge(acc: WeightedAvgAccum, it: java.lang.Iterable[WeightedAvgAccum]): Unit = { val iter = it.iterator() while (iter.hasNext) { val a = iter.next() acc.count += a.count acc.sum += a.sum } } def resetAccumulator(acc: WeightedAvgAccum): Unit = { acc.count = 0 acc.sum = 0L } override def getAccumulatorType: TypeInformation[WeightedAvgAccum] = { new TupleTypeInfo(classOf[WeightedAvgAccum], Types.LONG, Types.INT) } override def getResultType: TypeInformation[JLong] = Types.LONG } // 注册函数 val tEnv: StreamTableEnvironment = ??? tEnv.registerFunction(\u0026#34;wAvg\u0026#34;, new WeightedAvg()) // 使用函数 tEnv.sqlQuery(\u0026#34;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user\u0026#34;) Python \u0026#39;\u0026#39;\u0026#39; Java code: /** * Accumulator for WeightedAvg. */ public static class WeightedAvgAccum { public long sum = 0; public int count = 0; } // The java class must have a public no-argument constructor and can be founded in current java classloader. // Java 类必须有一个 public 的无参构造函数，并且可以在当前类加载器中加载到。 /** * Weighted Average user-defined aggregate function. */ public static class WeightedAvg extends AggregateFunction\u0026lt;Long, WeightedAvgAccum\u0026gt; { @Override public WeightedAvgAccum createAccumulator() { return new WeightedAvgAccum(); } @Override public Long getValue(WeightedAvgAccum acc) { if (acc.count == 0) { return null; } else { return acc.sum / acc.count; } } public void accumulate(WeightedAvgAccum acc, long iValue, int iWeight) { acc.sum += iValue * iWeight; acc.count += iWeight; } public void retract(WeightedAvgAccum acc, long iValue, int iWeight) { acc.sum -= iValue * iWeight; acc.count -= iWeight; } public void merge(WeightedAvgAccum acc, Iterable\u0026lt;WeightedAvgAccum\u0026gt; it) { Iterator\u0026lt;WeightedAvgAccum\u0026gt; iter = it.iterator(); while (iter.hasNext()) { WeightedAvgAccum a = iter.next(); acc.count += a.count; acc.sum += a.sum; } } public void resetAccumulator(WeightedAvgAccum acc) { acc.count = 0; acc.sum = 0L; } } \u0026#39;\u0026#39;\u0026#39; # 注册函数 t_env = ... # type: StreamTableEnvironment t_env.register_java_function(\u0026#34;wAvg\u0026#34;, \u0026#34;my.java.function.WeightedAvg\u0026#34;) # 使用函数 t_env.sql_query(\u0026#34;SELECT user, wAvg(points, level) AS avgPoints FROM userScores GROUP BY user\u0026#34;) 如果你打算使用 Python 实现或调用聚合函数，详情可参考 Python 聚合函数。
Back to top
表值聚合函数 # 自定义表值聚合函数（UDTAGG）可以把一个表（一行或者多行，每行有一列或者多列）聚合成另一张表，结果中可以有多行多列。
上图展示了一个表值聚合函数的例子。假设你有一个饮料的表，这个表有 3 列，分别是 id、name 和 price，一共有 5 行。假设你需要找到价格最高的两个饮料，类似于 top2() 表值聚合函数。你需要遍历所有 5 行数据，结果是有 2 行数据的一个表。
用户自定义表值聚合函数是通过扩展 TableAggregateFunction 类来实现的。一个 TableAggregateFunction 的工作过程如下。首先，它需要一个 accumulator，这个 accumulator 负责存储聚合的中间结果。 通过调用 TableAggregateFunction 的 createAccumulator 方法来构造一个空的 accumulator。接下来，对于每一行数据，会调用 accumulate 方法来更新 accumulator。当所有数据都处理完之后，调用 emitValue 方法来计算和返回最终的结果。
下面几个 TableAggregateFunction 的方法是必须要实现的：
createAccumulator() accumulate() Flink 的类型推导在遇到复杂类型的时候可能会推导出错误的结果，比如那些非基本类型和普通的 POJO 类型的复杂类型。所以类似于 ScalarFunction 和 TableFunction，TableAggregateFunction 也提供了 TableAggregateFunction#getResultType() 和 TableAggregateFunction#getAccumulatorType() 方法来指定返回值类型和 accumulator 的类型，这两个方法都需要返回 TypeInformation。
除了上面的方法，还有几个其他的方法可以选择性的实现。有些方法可以让查询更加高效，而有些方法对于某些特定场景是必须要实现的。比如，在会话窗口（当两个会话窗口合并时会合并两个 accumulator）中使用聚合函数时，必须要实现merge() 方法。
下面几个 TableAggregateFunction 的方法在某些特定场景下是必须要实现的：
retract() 在 bounded OVER 窗口中的聚合函数必须要实现。 merge() 在许多批式聚合和以及流式会话和滑动窗口聚合中是必须要实现的。 resetAccumulator() 在许多批式聚合中是必须要实现的。 emitValue() 在批式聚合以及窗口聚合中是必须要实现的。 下面的 TableAggregateFunction 的方法可以提升流式任务的效率：
emitUpdateWithRetract() 在 retract 模式下，该方法负责发送被更新的值。 emitValue 方法会发送所有 accumulator 给出的结果。拿 TopN 来说，emitValue 每次都会发送所有的最大的 n 个值。这在流式任务中可能会有一些性能问题。为了提升性能，用户可以实现 emitUpdateWithRetract 方法。这个方法在 retract 模式下会增量的输出结果，比如有数据更新了，我们必须要撤回老的数据，然后再发送新的数据。如果定义了 emitUpdateWithRetract 方法，那它会优先于 emitValue 方法被使用，因为一般认为 emitUpdateWithRetract 会更加高效，因为它的输出是增量的。
TableAggregateFunction 的所有方法都必须是 public 的、非 static 的，而且名字必须跟上面提到的一样。createAccumulator、getResultType 和 getAccumulatorType 这三个方法是在抽象父类 TableAggregateFunction 中定义的，而其他的方法都是约定的方法。要实现一个表值聚合函数，你必须扩展 org.apache.flink.table.functions.TableAggregateFunction，并且实现一个（或者多个）accumulate 方法。accumulate 方法可以有多个重载的方法，也可以支持变长参数。
TableAggregateFunction 的所有方法的详细文档如下。
Java /** * Base class for user-defined aggregates and table aggregates. * * @param \u0026lt;T\u0026gt; the type of the aggregation result. * @param \u0026lt;ACC\u0026gt; the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ public abstract class UserDefinedAggregateFunction\u0026lt;T, ACC\u0026gt; extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ public ACC createAccumulator(); // MANDATORY /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ public TypeInformation\u0026lt;T\u0026gt; getResultType = null; // PRE-DEFINED /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ public TypeInformation\u0026lt;ACC\u0026gt; getAccumulatorType = null; // PRE-DEFINED } /** * Base class for table aggregation functions. * * @param \u0026lt;T\u0026gt; the type of the aggregation result * @param \u0026lt;ACC\u0026gt; the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute a table aggregation result. * TableAggregateFunction represents its state using accumulator, thereby the state of * the TableAggregateFunction must be put into the accumulator. */ public abstract class TableAggregateFunction\u0026lt;T, ACC\u0026gt; extends UserDefinedAggregateFunction\u0026lt;T, ACC\u0026gt; { /** Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. A TableAggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ public void accumulate(ACC accumulator, [user defined inputs]); // MANDATORY /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ public void retract(ACC accumulator, [user defined inputs]); // OPTIONAL /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and bounded grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an {@link java.lang.Iterable} pointed to a group of accumulators that will be * merged. */ public void merge(ACC accumulator, java.lang.Iterable\u0026lt;ACC\u0026gt; its); // OPTIONAL /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the collector used to output data */ public void emitValue(ACC accumulator, Collector\u0026lt;T\u0026gt; out); // OPTIONAL /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated. * This method outputs data incrementally in retract mode, i.e., once there is an update, we * have to retract old records before sending new updated ones. The emitUpdateWithRetract * method will be used in preference to the emitValue method if both methods are defined in the * table aggregate function, because the method is treated to be more efficient than emitValue * as it can outputvalues incrementally. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the retractable collector used to output data. Use collect method * to output(add) records and use retract method to retract(delete) * records. */ public void emitUpdateWithRetract(ACC accumulator, RetractableCollector\u0026lt;T\u0026gt; out); // OPTIONAL /** * Collects a record and forwards it. The collector can output retract messages with the retract * method. Note: only use it in {@code emitRetractValueIncrementally}. */ public interface RetractableCollector\u0026lt;T\u0026gt; extends Collector\u0026lt;T\u0026gt; { /** * Retract a record. * * @param record The record to retract. */ void retract(T record); } } Scala /** * Base class for user-defined aggregates and table aggregates. * * @tparam T the type of the aggregation result. * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. */ abstract class UserDefinedAggregateFunction[T, ACC] extends UserDefinedFunction { /** * Creates and init the Accumulator for this (table)aggregate function. * * @return the accumulator with the initial value */ def createAccumulator(): ACC // MANDATORY /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s result. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s result or null if the result * type should be automatically inferred. */ def getResultType: TypeInformation[T] = null // PRE-DEFINED /** * Returns the TypeInformation of the (table)aggregate function\u0026#39;s accumulator. * * @return The TypeInformation of the (table)aggregate function\u0026#39;s accumulator or null if the * accumulator type should be automatically inferred. */ def getAccumulatorType: TypeInformation[ACC] = null // PRE-DEFINED } /** * Base class for table aggregation functions. * * @tparam T the type of the aggregation result * @tparam ACC the type of the aggregation accumulator. The accumulator is used to keep the * aggregated values which are needed to compute an aggregation result. * TableAggregateFunction represents its state using accumulator, thereby the state of * the TableAggregateFunction must be put into the accumulator. */ abstract class TableAggregateFunction[T, ACC] extends UserDefinedAggregateFunction[T, ACC] { /** * Processes the input values and update the provided accumulator instance. The method * accumulate can be overloaded with different custom types and arguments. A TableAggregateFunction * requires at least one accumulate() method. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def accumulate(accumulator: ACC, [user defined inputs]): Unit // MANDATORY /** * Retracts the input values from the accumulator instance. The current design assumes the * inputs are the values that have been previously accumulated. The method retract can be * overloaded with different custom types and arguments. This function must be implemented for * datastream bounded over aggregate. * * @param accumulator the accumulator which contains the current aggregated results * @param [user defined inputs] the input value (usually obtained from a new arrived data). */ def retract(accumulator: ACC, [user defined inputs]): Unit // OPTIONAL /** * Merges a group of accumulator instances into one accumulator instance. This function must be * implemented for datastream session window grouping aggregate and bounded grouping aggregate. * * @param accumulator the accumulator which will keep the merged aggregate results. It should * be noted that the accumulator may contain the previous aggregated * results. Therefore user should not replace or clean this instance in the * custom merge method. * @param its an [[java.lang.Iterable]] pointed to a group of accumulators that will be * merged. */ def merge(accumulator: ACC, its: java.lang.Iterable[ACC]): Unit // OPTIONAL /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the collector used to output data */ def emitValue(accumulator: ACC, out: Collector[T]): Unit // OPTIONAL /** * Called every time when an aggregation result should be materialized. The returned value * could be either an early and incomplete result (periodically emitted as data arrive) or * the final result of the aggregation. * * Different from emitValue, emitUpdateWithRetract is used to emit values that have been updated. * This method outputs data incrementally in retract mode, i.e., once there is an update, we * have to retract old records before sending new updated ones. The emitUpdateWithRetract * method will be used in preference to the emitValue method if both methods are defined in the * table aggregate function, because the method is treated to be more efficient than emitValue * as it can outputvalues incrementally. * * @param accumulator the accumulator which contains the current * aggregated results * @param out the retractable collector used to output data. Use collect method * to output(add) records and use retract method to retract(delete) * records. */ def emitUpdateWithRetract(accumulator: ACC, out: RetractableCollector[T]): Unit // OPTIONAL /** * Collects a record and forwards it. The collector can output retract messages with the retract * method. Note: only use it in \`emitRetractValueIncrementally\`. */ trait RetractableCollector[T] extends Collector[T] { /** * Retract a record. * * @param record The record to retract. */ def retract(record: T): Unit } } 下面的例子展示了如何
定义一个 TableAggregateFunction 来计算给定列的最大的 2 个值， 在 TableEnvironment 中注册函数， 在 Table API 查询中使用函数（当前只在 Table API 中支持 TableAggregateFunction）。 为了计算最大的 2 个值，accumulator 需要保存当前看到的最大的 2 个值。在我们的例子中，我们定义了类 Top2Accum 来作为 accumulator。Flink 的 checkpoint 机制会自动保存 accumulator，并且在失败时进行恢复，来保证精确一次的语义。
我们的 Top2 表值聚合函数（TableAggregateFunction）的 accumulate() 方法有两个输入，第一个是 Top2Accum accumulator，另一个是用户定义的输入：输入的值 v。尽管 merge() 方法在大多数聚合类型中不是必须的，我们也在样例中提供了它的实现。请注意，我们在 Scala 样例中也使用的是 Java 的基础类型，并且定义了 getResultType() 和 getAccumulatorType() 方法，因为 Flink 的类型推导对于 Scala 的类型推导支持的不是很好。
Java /** * Accumulator for Top2. */ public class Top2Accum { public Integer first; public Integer second; } /** * The top2 user-defined table aggregate function. */ public static class Top2 extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2Accum\u0026gt; { @Override public Top2Accum createAccumulator() { Top2Accum acc = new Top2Accum(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accum acc, Integer v) { if (v \u0026gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v \u0026gt; acc.second) { acc.second = v; } } public void merge(Top2Accum acc, java.lang.Iterable\u0026lt;Top2Accum\u0026gt; iterable) { for (Top2Accum otherAcc : iterable) { accumulate(acc, otherAcc.first); accumulate(acc, otherAcc.second); } } public void emitValue(Top2Accum acc, Collector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { // emit the value and rank if (acc.first != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.first, 1)); } if (acc.second != Integer.MIN_VALUE) { out.collect(Tuple2.of(acc.second, 2)); } } } // 注册函数 StreamTableEnvironment tEnv = ... tEnv.registerFunction(\u0026#34;top2\u0026#34;, new Top2()); // 初始化表 Table tab = ...; // 使用函数 tab.groupBy(\u0026#34;key\u0026#34;) .flatAggregate(\u0026#34;top2(a) as (v, rank)\u0026#34;) .select(\u0026#34;key, v, rank\u0026#34;); Scala import java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = { val iter = its.iterator() while (iter.hasNext) { val top2 = iter.next() accumulate(acc, top2.first) accumulate(acc, top2.second) } } def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = { // emit the value and rank if (acc.first != Int.MinValue) { out.collect(JTuple2.of(acc.first, 1)) } if (acc.second != Int.MinValue) { out.collect(JTuple2.of(acc.second, 2)) } } } // 初始化表 val tab = ... // 使用函数 tab .groupBy(\u0026#39;key) .flatAggregate(top2(\u0026#39;a) as (\u0026#39;v, \u0026#39;rank)) .select(\u0026#39;key, \u0026#39;v, \u0026#39;rank) 下面的例子展示了如何使用 emitUpdateWithRetract 方法来只发送更新的数据。为了只发送更新的结果，accumulator 保存了上一次的最大的2个值，也保存了当前最大的2个值。注意：如果 TopN 中的 n 非常大，这种既保存上次的结果，也保存当前的结果的方式不太高效。一种解决这种问题的方式是把输入数据直接存储到 accumulator 中，然后在调用 emitUpdateWithRetract 方法时再进行计算。
Java /** * Accumulator for Top2. */ public class Top2Accum { public Integer first; public Integer second; public Integer oldFirst; public Integer oldSecond; } /** * The top2 user-defined table aggregate function. */ public static class Top2 extends TableAggregateFunction\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;, Top2Accum\u0026gt; { @Override public Top2Accum createAccumulator() { Top2Accum acc = new Top2Accum(); acc.first = Integer.MIN_VALUE; acc.second = Integer.MIN_VALUE; acc.oldFirst = Integer.MIN_VALUE; acc.oldSecond = Integer.MIN_VALUE; return acc; } public void accumulate(Top2Accum acc, Integer v) { if (v \u0026gt; acc.first) { acc.second = acc.first; acc.first = v; } else if (v \u0026gt; acc.second) { acc.second = v; } } public void emitUpdateWithRetract(Top2Accum acc, RetractableCollector\u0026lt;Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; out) { if (!acc.first.equals(acc.oldFirst)) { // if there is an update, retract old value then emit new value. if (acc.oldFirst != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldFirst, 1)); } out.collect(Tuple2.of(acc.first, 1)); acc.oldFirst = acc.first; } if (!acc.second.equals(acc.oldSecond)) { // if there is an update, retract old value then emit new value. if (acc.oldSecond != Integer.MIN_VALUE) { out.retract(Tuple2.of(acc.oldSecond, 2)); } out.collect(Tuple2.of(acc.second, 2)); acc.oldSecond = acc.second; } } } // 注册函数 StreamTableEnvironment tEnv = ... tEnv.registerFunction(\u0026#34;top2\u0026#34;, new Top2()); // 初始化表 Table tab = ...; // 使用函数 tab.groupBy(\u0026#34;key\u0026#34;) .flatAggregate(\u0026#34;top2(a) as (v, rank)\u0026#34;) .select(\u0026#34;key, v, rank\u0026#34;); Scala import java.lang.{Integer =\u0026gt; JInteger} import org.apache.flink.table.api.Types import org.apache.flink.table.functions.TableAggregateFunction /** * Accumulator for top2. */ class Top2Accum { var first: JInteger = _ var second: JInteger = _ var oldFirst: JInteger = _ var oldSecond: JInteger = _ } /** * The top2 user-defined table aggregate function. */ class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] { override def createAccumulator(): Top2Accum = { val acc = new Top2Accum acc.first = Int.MinValue acc.second = Int.MinValue acc.oldFirst = Int.MinValue acc.oldSecond = Int.MinValue acc } def accumulate(acc: Top2Accum, v: Int) { if (v \u0026gt; acc.first) { acc.second = acc.first acc.first = v } else if (v \u0026gt; acc.second) { acc.second = v } } def emitUpdateWithRetract( acc: Top2Accum, out: RetractableCollector[JTuple2[JInteger, JInteger]]) : Unit = { if (acc.first != acc.oldFirst) { // if there is an update, retract old value then emit new value. if (acc.oldFirst != Int.MinValue) { out.retract(JTuple2.of(acc.oldFirst, 1)) } out.collect(JTuple2.of(acc.first, 1)) acc.oldFirst = acc.first } if (acc.second != acc.oldSecond) { // if there is an update, retract old value then emit new value. if (acc.oldSecond != Int.MinValue) { out.retract(JTuple2.of(acc.oldSecond, 2)) } out.collect(JTuple2.of(acc.second, 2)) acc.oldSecond = acc.second } } } // 初始化表 val tab = ... // 使用函数 tab .groupBy(\u0026#39;key) .flatAggregate(top2(\u0026#39;a) as (\u0026#39;v, \u0026#39;rank)) .select(\u0026#39;key, \u0026#39;v, \u0026#39;rank) Back to top
`}),e.add({id:292,href:"/flink/flink-docs-master/zh/docs/dev/python/table/catalogs/",title:"Catalogs",section:"Table API",content:" "}),e.add({id:293,href:"/flink/flink-docs-master/zh/docs/dev/table/modules/",title:"模块",section:"Table API \u0026 SQL",content:` Modules # Modules allow users to extend Flink\u0026rsquo;s built-in objects, such as defining functions that behave like Flink built-in functions. They are pluggable, and while Flink provides a few pre-built modules, users can write their own.
For example, users can define their own geo functions and plug them into Flink as built-in functions to be used in Flink SQL and Table APIs. Another example is users can load an out-of-shelf Hive module to use Hive built-in functions as Flink built-in functions.
Furthermore, a module can provide built-in table source and sink factories which disable Flink\u0026rsquo;s default discovery mechanism based on Java’s Service Provider Interfaces (SPI), or influence how connectors of temporary tables should be created without a corresponding catalog.
Module Types # CoreModule # CoreModule contains all of Flink\u0026rsquo;s system (built-in) functions and is loaded and enabled by default.
HiveModule # The HiveModule provides Hive built-in functions as Flink\u0026rsquo;s system functions to SQL and Table API users. Flink\u0026rsquo;s Hive documentation provides full details on setting up the module.
User-Defined Module # Users can develop custom modules by implementing the Module interface. To use custom modules in SQL CLI, users should develop both a module and its corresponding module factory by implementing the ModuleFactory interface.
A module factory defines a set of properties for configuring the module when the SQL CLI bootstraps. Properties are passed to a discovery service where the service tries to match the properties to a ModuleFactory and instantiate a corresponding module instance.
Module Lifecycle and Resolution Order # A module can be loaded, enabled, disabled and unloaded. When TableEnvironment loads a module initially, it enables the module by default. Flink supports multiple modules and keeps track of the loading order to resolve metadata. Besides, Flink only resolves the functions among enabled modules. E.g., when there are two functions of the same name residing in two modules, there will be three conditions.
If both of the modules are enabled, then Flink resolves the function according to the resolution order of the modules. If one of them is disabled, then Flink resolves the function to the enabled module. If both of the modules are disabled, then Flink cannot resolve the function. Users can change the resolution order by using modules in a different declared order. E.g., users can specify Flink to find functions first in Hive by USE MODULES hive, core.
Besides, users can also disable modules by not declaring them. E.g., users can specify Flink to disable core module by USE MODULES hive (However, it is strongly not recommended disabling core module). Disable a module does not unload it, and users can enable it again by using it. E.g., users can bring back core module and place it in the first by USE MODULES core, hive. A module can be enabled only when it is loaded already. Using an unloaded module will throw an Exception. Eventually, users can unload a module.
The difference between disabling and unloading a module is that TableEnvironment still keeps the disabled modules, and users can list all loaded modules to view the disabled modules.
Namespace # Objects provided by modules are considered part of Flink\u0026rsquo;s system (built-in) objects; thus, they don\u0026rsquo;t have any namespaces.
How to Load, Unload, Use and List Modules # Using SQL # Users can use SQL to load/unload/use/list modules in both Table API and SQL CLI.
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // Show initially loaded and enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;); // Show all enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.executeSql(\u0026#34;USE MODULES hive, core\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.executeSql(\u0026#34;UNLOAD MODULE hive\u0026#34;); tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print(); // Empty set tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(setting) // Show initially loaded and enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.executeSql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;) // Show all enabled modules tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;) // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.executeSql(\u0026#34;USE MODULES hive, core\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.executeSql(\u0026#34;USE MODULES hive\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.executeSql(\u0026#34;UNLOAD MODULE hive\u0026#34;) tableEnv.executeSql(\u0026#34;SHOW MODULES\u0026#34;).print() // Empty set tableEnv.executeSql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Python from pyflink.table import * # environment configuration settings = EnvironmentSettings.inStreamingMode() t_env = TableEnvironment.create(settings) # Show initially loaded and enabled modules t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # +-------------+------+ # Load a hive module t_env.execute_sql(\u0026#34;LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;)\u0026#34;) # Show all enabled modules t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ # Show all loaded modules with both name and use status t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # | hive | true | # +-------------+------+ # Change resolution order t_env.execute_sql(\u0026#34;USE MODULES hive, core\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | hive | # | core | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+------+ # | module name | used | # +-------------+------+ # | hive | true | # | core | true | # +-------------+------+ # Disable core module t_env.execute_sql(\u0026#34;USE MODULES hive\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # +-------------+ # | module name | # +-------------+ # | hive | # +-------------+ t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ # Unload hive module t_env.execute_sql(\u0026#34;UNLOAD MODULE hive\u0026#34;) t_env.execute_sql(\u0026#34;SHOW MODULES\u0026#34;).print() # Empty set t_env.execute_sql(\u0026#34;SHOW FULL MODULES\u0026#34;).print() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | false | # +-------------+-------+ SQL Client -- Show initially loaded and enabled modules Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | +-------------+ 1 row in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | +-------------+------+ 1 row in set -- Load a hive module Flink SQL\u0026gt; LOAD MODULE hive WITH (\u0026#39;hive-version\u0026#39; = \u0026#39;...\u0026#39;); -- Show all enabled modules Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | core | | hive | +-------------+ 2 rows in set -- Show all loaded modules with both name and use status Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | core | true | | hive | true | +-------------+------+ 2 rows in set -- Change resolution order Flink SQL\u0026gt; USE MODULES hive, core ; Flink SQL\u0026gt; SHOW MODULES; +-------------+ | module name | +-------------+ | hive | | core | +-------------+ 2 rows in set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+------+ | module name | used | +-------------+------+ | hive | true | | core | true | +-------------+------+ 2 rows in set -- Unload hive module Flink SQL\u0026gt; UNLOAD MODULE hive; Flink SQL\u0026gt; SHOW MODULES; Empty set Flink SQL\u0026gt; SHOW FULL MODULES; +-------------+-------+ | module name | used | +-------------+-------+ | hive | false | +-------------+-------+ 1 row in set YAML All modules defined using YAML must provide a type property that specifies the type. The following types are supported out of the box.
Module Type Value CoreModule core HiveModule hive modules: - name: core type: core - name: hive type: hive When using SQL, module name is used to perform the module discovery. It is parsed as a simple identifier and case-sensitive. Using Java, Scala or Python # Users can use Java, Scala or Python to load/unload/use/list modules programmatically.
Java EnvironmentSettings settings = EnvironmentSettings.inStreamingMode(); TableEnvironment tableEnv = TableEnvironment.create(settings); // Show initially loaded and enabled modules tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.loadModule(\u0026#34;hive\u0026#34;, new HiveModule()); // Show all enabled modules tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.useModules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;); tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.listFullModules(); // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.useModules(\u0026#34;hive\u0026#34;); tableEnv.listModules(); // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.listFullModules(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.unloadModule(\u0026#34;hive\u0026#34;); tableEnv.listModules(); // Empty set tableEnv.listFullModules(); // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Scala val settings = EnvironmentSettings.inStreamingMode() val tableEnv = TableEnvironment.create(setting) // Show initially loaded and enabled modules tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | core | // +-------------+ tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // +-------------+------+ // Load a hive module tableEnv.loadModule(\u0026#34;hive\u0026#34;, new HiveModule()) // Show all enabled modules tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | core | // | hive | // +-------------+ // Show all loaded modules with both name and use status tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | core | true | // | hive | true | // +-------------+------+ // Change resolution order tableEnv.useModules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;) tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | hive | // | core | // +-------------+ tableEnv.listFullModules() // +-------------+------+ // | module name | used | // +-------------+------+ // | hive | true | // | core | true | // +-------------+------+ // Disable core module tableEnv.useModules(\u0026#34;hive\u0026#34;) tableEnv.listModules() // +-------------+ // | module name | // +-------------+ // | hive | // +-------------+ tableEnv.listFullModules() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | true | // | core | false | // +-------------+-------+ // Unload hive module tableEnv.unloadModule(\u0026#34;hive\u0026#34;) tableEnv.listModules() // Empty set tableEnv.listFullModules() // +-------------+-------+ // | module name | used | // +-------------+-------+ // | hive | false | // +-------------+-------+ Python from pyflink.table import * # environment configuration settings = EnvironmentSettings.inStreamingMode() t_env = TableEnvironment.create(settings) # Show initially loaded and enabled modules t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | core | # +-------------+ t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # +-------------+------+ # Load a hive module t_env.load_module(\u0026#34;hive\u0026#34;, HiveModule()) # Show all enabled modules t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | core | # | hive | # +-------------+ # Show all loaded modules with both name and use status t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | core | true | # | hive | true | # +-------------+------+ # Change resolution order t_env.use_modules(\u0026#34;hive\u0026#34;, \u0026#34;core\u0026#34;) t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | hive | # | core | # +-------------+ t_env.list_full_modules() # +-------------+------+ # | module name | used | # +-------------+------+ # | hive | true | # | core | true | # +-------------+------+ # Disable core module t_env.use_modules(\u0026#34;hive\u0026#34;) t_env.list_modules() # +-------------+ # | module name | # +-------------+ # | hive | # +-------------+ t_env.list_full_modules() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | true | # | core | false | # +-------------+-------+ # Unload hive module t_env.unload_module(\u0026#34;hive\u0026#34;) t_env.list_modules() # Empty set t_env.list_full_modules() # +-------------+-------+ # | module name | used | # +-------------+-------+ # | hive | false | # +-------------+-------+ Back to top
`}),e.add({id:294,href:"/flink/flink-docs-master/zh/docs/dev/table/catalogs/",title:"Catalogs",section:"Table API \u0026 SQL",content:` Catalogs # Catalog 提供了元数据信息，例如数据库、表、分区、视图以及数据库或其他外部系统中存储的函数和信息。
数据处理最关键的方面之一是管理元数据。 元数据可以是临时的，例如临时表、或者通过 TableEnvironment 注册的 UDF。 元数据也可以是持久化的，例如 Hive Metastore 中的元数据。Catalog 提供了一个统一的API，用于管理元数据，并使其可以从 Table API 和 SQL 查询语句中来访问。
Catalog 类型 # GenericInMemoryCatalog # GenericInMemoryCatalog 是基于内存实现的 Catalog，所有元数据只在 session 的生命周期内可用。
JdbcCatalog # JdbcCatalog 使得用户可以将 Flink 通过 JDBC 协议连接到关系数据库。Postgres Catalog 和 MySQL Catalog 是目前 JDBC Catalog 仅有的两种实现。 参考 JdbcCatalog 文档 获取关于配置 JDBC catalog 的详细信息。
HiveCatalog # HiveCatalog 有两个用途：作为原生 Flink 元数据的持久化存储，以及作为读写现有 Hive 元数据的接口。 Flink 的 Hive 文档 提供了有关设置 HiveCatalog 以及访问现有 Hive 元数据的详细信息。
警告 Hive Metastore 以小写形式存储所有元数据对象名称。而 GenericInMemoryCatalog 区分大小写。
用户自定义 Catalog # Catalog 是可扩展的，用户可以通过实现 Catalog 接口来开发自定义 Catalog。 想要在 SQL CLI 中使用自定义 Catalog，用户除了需要实现自定义的 Catalog 之外，还需要为这个 Catalog 实现对应的 CatalogFactory 接口。
CatalogFactory 定义了一组属性，用于 SQL CLI 启动时配置 Catalog。 这组属性集将传递给发现服务，在该服务中，服务会尝试将属性关联到 CatalogFactory 并初始化相应的 Catalog 实例。
如何创建 Flink 表并将其注册到 Catalog # 使用 SQL DDL # 用户可以使用 DDL 通过 Table API 或者 SQL Client 在 Catalog 中创建表。
Java TableEnvironment tableEnv = ...; // Create a HiveCatalog Catalog catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;); // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog); // Create a catalog database tableEnv.executeSql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;); // Create a catalog table tableEnv.executeSql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;); tableEnv.listTables(); // should return the tables in current catalog and database. Scala val tableEnv = ... // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;); // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog); // Create a catalog database tableEnv.executeSql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;); // Create a catalog table tableEnv.executeSql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;); tableEnv.listTables(); // should return the tables in current catalog and database. Python from pyflink.table.catalog import HiveCatalog # Create a HiveCatalog catalog = HiveCatalog(\u0026#34;myhive\u0026#34;, None, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) # Register the catalog t_env.register_catalog(\u0026#34;myhive\u0026#34;, catalog) # Create a catalog database t_env.execute_sql(\u0026#34;CREATE DATABASE mydb WITH (...)\u0026#34;) # Create a catalog table t_env.execute_sql(\u0026#34;CREATE TABLE mytable (name STRING, age INT) WITH (...)\u0026#34;) # should return the tables in current catalog and database. t_env.list_tables() SQL Client // the catalog should have been registered via yaml file Flink SQL\u0026gt; CREATE DATABASE mydb WITH (...); Flink SQL\u0026gt; CREATE TABLE mytable (name STRING, age INT) WITH (...); Flink SQL\u0026gt; SHOW TABLES; mytable 更多详细信息，请参考Flink SQL CREATE DDL。
使用 Java/Scala # 用户可以用编程的方式使用Java 或者 Scala 来创建 Catalog 表。
Java import org.apache.flink.table.api.*; import org.apache.flink.table.catalog.*; import org.apache.flink.table.catalog.hive.HiveCatalog; TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.inStreamingMode()); // Create a HiveCatalog Catalog catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;); // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog); // Create a catalog database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...)); // Create a catalog table final Schema schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build(); tableEnv.createTable(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.forConnector(\u0026#34;kafka\u0026#34;) .schema(schema) // … .build()); List\u0026lt;String\u0026gt; tables = catalog.listTables(\u0026#34;mydb\u0026#34;); // tables should contain \u0026#34;mytable\u0026#34; Scala import org.apache.flink.table.api._ import org.apache.flink.table.catalog._ import org.apache.flink.table.catalog.hive.HiveCatalog val tableEnv = TableEnvironment.create(EnvironmentSettings.inStreamingMode()) // Create a HiveCatalog val catalog = new HiveCatalog(\u0026#34;myhive\u0026#34;, null, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) // Register the catalog tableEnv.registerCatalog(\u0026#34;myhive\u0026#34;, catalog) // Create a catalog database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...)) // Create a catalog table val schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build() tableEnv.createTable(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.forConnector(\u0026#34;kafka\u0026#34;) .schema(schema) // … .build()) val tables = catalog.listTables(\u0026#34;mydb\u0026#34;) // tables should contain \u0026#34;mytable\u0026#34; Python from pyflink.table import * from pyflink.table.catalog import HiveCatalog, CatalogDatabase, ObjectPath, CatalogBaseTable from pyflink.table.descriptors import Kafka settings = EnvironmentSettings.in_batch_mode() t_env = TableEnvironment.create(settings) # Create a HiveCatalog catalog = HiveCatalog(\u0026#34;myhive\u0026#34;, None, \u0026#34;\u0026lt;path_of_hive_conf\u0026gt;\u0026#34;) # Register the catalog t_env.register_catalog(\u0026#34;myhive\u0026#34;, catalog) # Create a catalog database database = CatalogDatabase.create_instance({\u0026#34;k1\u0026#34;: \u0026#34;v1\u0026#34;}, None) catalog.create_database(\u0026#34;mydb\u0026#34;, database) # Create a catalog table schema = Schema.new_builder() \\ .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .column(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() catalog_table = t_env.create_table(\u0026#34;myhive.mydb.mytable\u0026#34;, TableDescriptor.for_connector(\u0026#34;kafka\u0026#34;) .schema(schema) # … .build()) # tables should contain \u0026#34;mytable\u0026#34; tables = catalog.list_tables(\u0026#34;mydb\u0026#34;) Catalog API # 注意：这里只列出了编程方式的 Catalog API，用户可以使用 SQL DDL 实现许多相同的功能。 关于 DDL 的详细信息请参考 SQL CREATE DDL。
数据库操作 # Java/Scala // create database catalog.createDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // drop database catalog.dropDatabase(\u0026#34;mydb\u0026#34;, false); // alter database catalog.alterDatabase(\u0026#34;mydb\u0026#34;, new CatalogDatabaseImpl(...), false); // get database catalog.getDatabase(\u0026#34;mydb\u0026#34;); // check if a database exist catalog.databaseExists(\u0026#34;mydb\u0026#34;); // list databases in a catalog catalog.listDatabases(\u0026#34;mycatalog\u0026#34;); Python from pyflink.table.catalog import CatalogDatabase # create database catalog_database = CatalogDatabase.create_instance({\u0026#34;k1\u0026#34;: \u0026#34;v1\u0026#34;}, None) catalog.create_database(\u0026#34;mydb\u0026#34;, catalog_database, False) # drop database catalog.drop_database(\u0026#34;mydb\u0026#34;, False) # alter database catalog.alter_database(\u0026#34;mydb\u0026#34;, catalog_database, False) # get database catalog.get_database(\u0026#34;mydb\u0026#34;) # check if a database exist catalog.database_exists(\u0026#34;mydb\u0026#34;) # list databases in a catalog catalog.list_databases() 表操作 # Java/Scala // create table catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // drop table catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), false); // alter table catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogTableImpl(...), false); // rename table catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), \u0026#34;my_new_table\u0026#34;); // get table catalog.getTable(\u0026#34;mytable\u0026#34;); // check if a table exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list tables in a database catalog.listTables(\u0026#34;mydb\u0026#34;); Python from pyflink.table import * from pyflink.table.catalog import CatalogBaseTable, ObjectPath from pyflink.table.descriptors import Kafka table_schema = TableSchema.builder() \\ .field(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .field(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() table_properties = Kafka() \\ .version(\u0026#34;0.11\u0026#34;) \\ .start_from_earlist() \\ .to_properties() catalog_table = CatalogBaseTable.create_table(schema=table_schema, properties=table_properties, comment=\u0026#34;my comment\u0026#34;) # create table catalog.create_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # drop table catalog.drop_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), False) # alter table catalog.alter_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # rename table catalog.rename_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), \u0026#34;my_new_table\u0026#34;) # get table catalog.get_table(\u0026#34;mytable\u0026#34;) # check if a table exist or not catalog.table_exists(\u0026#34;mytable\u0026#34;) # list tables in a database catalog.list_tables(\u0026#34;mydb\u0026#34;) 视图操作 # Java/Scala // create view catalog.createTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), new CatalogViewImpl(...), false); // drop view catalog.dropTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), false); // alter view catalog.alterTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogViewImpl(...), false); // rename view catalog.renameTable(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), \u0026#34;my_new_view\u0026#34;, false); // get view catalog.getTable(\u0026#34;myview\u0026#34;); // check if a view exist or not catalog.tableExists(\u0026#34;mytable\u0026#34;); // list views in a database catalog.listViews(\u0026#34;mydb\u0026#34;); Python from pyflink.table import * from pyflink.table.catalog import CatalogBaseTable, ObjectPath table_schema = TableSchema.builder() \\ .field(\u0026#34;name\u0026#34;, DataTypes.STRING()) \\ .field(\u0026#34;age\u0026#34;, DataTypes.INT()) \\ .build() catalog_table = CatalogBaseTable.create_view( original_query=\u0026#34;select * from t1\u0026#34;, expanded_query=\u0026#34;select * from test-catalog.db1.t1\u0026#34;, schema=table_schema, properties={}, comment=\u0026#34;This is a view\u0026#34; ) catalog.create_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), catalog_table, False) # drop view catalog.drop_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), False) # alter view catalog.alter_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_table, False) # rename view catalog.rename_table(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myview\u0026#34;), \u0026#34;my_new_view\u0026#34;, False) # get view catalog.get_table(\u0026#34;myview\u0026#34;) # check if a view exist or not catalog.table_exists(\u0026#34;mytable\u0026#34;) # list views in a database catalog.list_views(\u0026#34;mydb\u0026#34;) 分区操作 # Java/Scala // create view catalog.createPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // drop partition catalog.dropPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), false); // alter partition catalog.alterPartition( new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...), new CatalogPartitionImpl(...), false); // get partition catalog.getPartition(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // check if a partition exist or not catalog.partitionExists(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;)); // list partitions of a table under a give partition spec catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), new CatalogPartitionSpec(...)); // list partitions of a table by expression filter catalog.listPartitions(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), Arrays.asList(epr1, ...)); Python from pyflink.table.catalog import ObjectPath, CatalogPartitionSpec, CatalogPartition catalog_partition = CatalogPartition.create_instance({}, \u0026#34;my partition\u0026#34;) catalog_partition_spec = CatalogPartitionSpec({\u0026#34;third\u0026#34;: \u0026#34;2010\u0026#34;, \u0026#34;second\u0026#34;: \u0026#34;bob\u0026#34;}) catalog.create_partition( ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec, catalog_partition, False) # drop partition catalog.drop_partition(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec, False) # alter partition catalog.alter_partition( ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), CatalogPartitionSpec(...), catalog_partition, False) # get partition catalog.get_partition(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) # check if a partition exist or not catalog.partition_exists(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) # list partitions of a table catalog.list_partitions(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;)) # list partitions of a table under a give partition spec catalog.list_partitions(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;mytable\u0026#34;), catalog_partition_spec) 函数操作 # Java/Scala // create function catalog.createFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // drop function catalog.dropFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), false); // alter function catalog.alterFunction(new ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), new CatalogFunctionImpl(...), false); // get function catalog.getFunction(\u0026#34;myfunc\u0026#34;); // check if a function exist or not catalog.functionExists(\u0026#34;myfunc\u0026#34;); // list functions in a database catalog.listFunctions(\u0026#34;mydb\u0026#34;); Python from pyflink.table.catalog import ObjectPath, CatalogFunction catalog_function = CatalogFunction.create_instance(class_name=\u0026#34;my.python.udf\u0026#34;) # create function catalog.create_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), catalog_function, False) # drop function catalog.drop_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), False) # alter function catalog.alter_function(ObjectPath(\u0026#34;mydb\u0026#34;, \u0026#34;myfunc\u0026#34;), catalog_function, False) # get function catalog.get_function(\u0026#34;myfunc\u0026#34;) # check if a function exist or not catalog.function_exists(\u0026#34;myfunc\u0026#34;) # list functions in a database catalog.list_functions(\u0026#34;mydb\u0026#34;) 通过 Table API 和 SQL Client 操作 Catalog # 注册 Catalog # 用户可以访问默认创建的内存 Catalog default_catalog，这个 Catalog 默认拥有一个默认数据库 default_database。 用户也可以注册其他的 Catalog 到现有的 Flink 会话中。
Java/Scala tableEnv.registerCatalog(new CustomCatalog(\u0026#34;myCatalog\u0026#34;)); Python t_env.register_catalog(catalog) YAML 使用 YAML 定义的 Catalog 必须提供 type 属性，以表示指定的 Catalog 类型。 以下几种类型可以直接使用。
Catalog Type Value GenericInMemory generic_in_memory Hive hive catalogs: - name: myCatalog type: custom_catalog hive-conf-dir: ... 修改当前的 Catalog 和数据库 # Flink 始终在当前的 Catalog 和数据库中寻找表、视图和 UDF。
Java/Scala tableEnv.useCatalog(\u0026#34;myCatalog\u0026#34;); tableEnv.useDatabase(\u0026#34;myDb\u0026#34;); Python t_env.use_catalog(\u0026#34;myCatalog\u0026#34;) t_env.use_database(\u0026#34;myDb\u0026#34;) SQL Flink SQL\u0026gt; USE CATALOG myCatalog; Flink SQL\u0026gt; USE myDB; 通过提供全限定名 catalog.database.object 来访问不在当前 Catalog 中的元数据信息。
Java/Scala tableEnv.from(\u0026#34;not_the_current_catalog.not_the_current_db.my_table\u0026#34;); Python t_env.from_path(\u0026#34;not_the_current_catalog.not_the_current_db.my_table\u0026#34;) SQL Flink SQL\u0026gt; SELECT * FROM not_the_current_catalog.not_the_current_db.my_table; 列出可用的 Catalog # Java/Scala tableEnv.listCatalogs(); Python t_env.list_catalogs() SQL Flink SQL\u0026gt; show catalogs; 列出可用的数据库 # Java/Scala tableEnv.listDatabases(); Python t_env.list_databases() SQL Flink SQL\u0026gt; show databases; 列出可用的表 # Java/Scala tableEnv.listTables(); Python t_env.list_tables() SQL Flink SQL\u0026gt; show tables; `}),e.add({id:295,href:"/flink/flink-docs-master/zh/docs/dev/table/sqlclient/",title:"SQL 客户端",section:"Table API \u0026 SQL",content:` SQL 客户端 # Flink 的 Table \u0026amp; SQL API 可以处理 SQL 语言编写的查询语句，但是这些查询需要嵌入用 Java 或 Scala 编写的表程序中。此外，这些程序在提交到集群前需要用构建工具打包。这或多或少限制了 Java/Scala 程序员对 Flink 的使用。
SQL 客户端 的目的是提供一种简单的方式来编写、调试和提交表程序到 Flink 集群上，而无需写一行 Java 或 Scala 代码。SQL 客户端命令行界面（CLI） 能够在命令行中检索和可视化分布式应用中实时产生的结果。
入门 # 本节介绍如何在命令行里启动（setup）和运行你的第一个 Flink SQL 程序。
SQL 客户端捆绑在常规 Flink 发行版中，因此可以直接运行。它仅需要一个正在运行的 Flink 集群就可以在其中执行表程序。有关设置 Flink 群集的更多信息，请参见集群和部署部分。如果仅想试用 SQL 客户端，也可以使用以下命令启动本地集群：
./bin/start-cluster.sh 启动 SQL 客户端命令行界面 # SQL Client 脚本也位于 Flink 的 bin 目录中。将来，用户可以通过启动嵌入式 standalone 进程或通过连接到远程 SQL 客户端网关来启动 SQL 客户端命令行界面。目前仅支持 embedded，模式默认值embedded。可以通过以下方式启动 CLI：
./bin/sql-client.sh 或者显式使用 embedded 模式:
./bin/sql-client.sh embedded 执行 SQL 查询 # 命令行界面启动后，你可以使用 HELP 命令列出所有可用的 SQL 语句。输入第一条 SQL 查询语句并按 Enter 键执行，可以验证你的设置及集群连接是否正确：
SELECT \u0026#39;Hello World\u0026#39;; 该查询不需要 table source，并且只产生一行结果。CLI 将从集群中检索结果并将其可视化。按 Q 键退出结果视图。
CLI 为维护和可视化结果提供三种模式。
表格模式（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;table\u0026#39;; 变更日志模式（changelog mode）不会实体化和可视化结果，而是由插入（+）和撤销（-）组成的持续查询产生结果流。
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;changelog\u0026#39;; Tableau模式（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(execution.type)：
SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; 注意当你使用这个模式运行一个流式查询的时候，Flink 会将结果持续的打印在当前的屏幕之上。如果这个流式查询的输入是有限的数据集， 那么Flink在处理完所有的数据之后，会自动的停止作业，同时屏幕上的打印也会相应的停止。如果你想提前结束这个查询，那么可以直接使用 CTRL-C 按键，这个会停掉作业同时停止屏幕上的打印。
你可以用如下查询来查看三种结果模式的运行情况：
SELECT name, COUNT(*) AS cnt FROM (VALUES (\u0026#39;Bob\u0026#39;), (\u0026#39;Alice\u0026#39;), (\u0026#39;Greg\u0026#39;), (\u0026#39;Bob\u0026#39;)) AS NameTable(name) GROUP BY name; 此查询执行一个有限字数示例：
变更日志模式 下，看到的结果应该类似于：
+ Bob, 1 + Alice, 1 + Greg, 1 - Bob, 1 + Bob, 2 表格模式 下，可视化结果表将不断更新，直到表程序以如下内容结束：
Bob, 2 Alice, 1 Greg, 1 Tableau模式 下，如果这个查询以流的方式执行，那么将显示以下内容：
+-----+----------------------+----------------------+ | +/- | name | cnt | +-----+----------------------+----------------------+ | + | Bob | 1 | | + | Alice | 1 | | + | Greg | 1 | | - | Bob | 1 | | + | Bob | 2 | +-----+----------------------+----------------------+ Received a total of 5 rows 如果这个查询以批的方式执行，显示的内容如下：
+-------+-----+ | name | cnt | +-------+-----+ | Alice | 1 | | Bob | 2 | | Greg | 1 | +-------+-----+ 3 rows in set 这几种结果模式在 SQL 查询的原型设计过程中都非常有用。这些模式的结果都存储在 SQL 客户端 的 Java 堆内存中。为了保持 CLI 界面及时响应，变更日志模式仅显示最近的 1000 个更改。表格模式支持浏览更大的结果，这些结果仅受可用主内存和配置的最大行数（sql-client.execution.max-table-result.rows）的限制。
注意 在批处理环境下执行的查询只能用表格模式或者Tableau模式进行检索。
定义查询语句后，可以将其作为长时间运行的独立 Flink 作业提交给集群。配置部分解释如何声明读取数据的 table source，写入数据的 sink 以及配置其他表程序属性的方法。
Back to top
Configuration # SQL Client startup options # The SQL Client can be started with the following optional CLI commands. They are discussed in detail in the subsequent paragraphs.
./bin/sql-client.sh --help Mode \u0026#34;embedded\u0026#34; (default) submits Flink jobs from the local machine. Syntax: [embedded] [OPTIONS] \u0026#34;embedded\u0026#34; mode options: -f,--file \u0026lt;script file\u0026gt; Script file that should be executed. In this mode, the client will not open an interactive terminal. -h,--help Show the help message with descriptions of all options. -hist,--history \u0026lt;History file path\u0026gt; The file which you want to save the command history into. If not specified, we will auto-generate one under your user\u0026#39;s home directory. -i,--init \u0026lt;initialization file\u0026gt; Script file that used to init the session context. If get error in execution, the sql client will exit. Notice it\u0026#39;s not allowed to add query or insert into the init file. -j,--jar \u0026lt;JAR file\u0026gt; A JAR file to be imported into the session. The file might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -l,--library \u0026lt;JAR directory\u0026gt; A JAR file directory with which every new session is initialized. The files might contain user-defined classes needed for the execution of statements such as functions, table sources, or sinks. Can be used multiple times. -pyarch,--pyArchives \u0026lt;arg\u0026gt; Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory be specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. \u0026#39;#\u0026#39; could be used as the separator of the archive file path and the target directory name. Comma (\u0026#39;,\u0026#39;) could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF (e.g.: --pyArchives file:///tmp/py37.zip,file:///tmp/data .zip#data --pyExecutable py37.zip/py37/bin/python). The data files could be accessed in Python UDF, e.g.: f = open(\u0026#39;data/data.txt\u0026#39;, \u0026#39;r\u0026#39;). -pyexec,--pyExecutable \u0026lt;arg\u0026gt; Specify the path of the python interpreter used to execute the python UDF worker (e.g.: --pyExecutable /usr/local/bin/python3). The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u0026gt;= 20.3) and SetupTools (version \u0026gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. -pyfs,--pyFiles \u0026lt;pythonFiles\u0026gt; Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (\u0026#39;,\u0026#39;) could be used as the separator to specify multiple files (e.g.: --pyFiles file:///tmp/myresource.zip,hdfs:///\$n amenode_address/myresource2.zip). -pyreq,--pyRequirements \u0026lt;arg\u0026gt; Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use \u0026#39;#\u0026#39; as the separator if the optional parameter exists (e.g.: --pyRequirements file:///tmp/requirements.txt#file:/// tmp/cached_dir). -s,--session \u0026lt;session identifier\u0026gt; The identifier for a session. \u0026#39;default\u0026#39; is the default identifier. -u,--update \u0026lt;SQL update statement\u0026gt; Deprecated Experimental (for testing only!) feature: Instructs the SQL Client to immediately execute the given update statement after starting up. The process is shut down after the statement has been submitted to the cluster and returns an appropriate return code. Currently, this feature is only supported for INSERT INTO statements that declare the target sink table.Please use option -f to submit update statement. SQL Client Configuration # Key Default Type Description sql-client.display.max-column-width
Streaming 30 Integer When printing the query results, this parameter determines the number of characters shown on screen before truncating.This only applies to columns with variable-length types (e.g. STRING) in streaming mode.Fixed-length types and all types in batch mode are printed using a deterministic column width sql-client.execution.max-table-result.rows
Batch Streaming 1000000 Integer The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style. sql-client.execution.result-mode
Batch Streaming TABLE Enum
Determines how the query result should be displayed.
Possible values:"TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation."CHANGELOG": Visualizes the result stream that is produced by a continuous query."TABLEAU": Display results in the screen directly in a tableau format. sql-client.verbose
Batch Streaming false Boolean Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause. Initialize Session Using SQL Files # A SQL query needs a configuration environment in which it is executed. SQL Client supports the -i startup option to execute an initialization SQL file to setup environment when starting up the SQL Client. The so-called initialization SQL file can use DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment.
An example of such a file is presented below.
-- Define available catalogs CREATE CATALOG MyCatalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;hive\u0026#39; ); USE CATALOG MyCatalog; -- Define available database CREATE DATABASE MyDatabase; USE MyDatabase; -- Define TABLE CREATE TABLE MyTable( MyField1 INT, MyField2 STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/something\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); -- Define VIEW CREATE VIEW MyCustomView AS SELECT MyField2 FROM MyTable; -- Define user-defined functions here. CREATE FUNCTION foo.bar.AggregateUDF AS myUDF; -- Properties that change the fundamental execution behavior of a table program. SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- execution mode either \u0026#39;batch\u0026#39; or \u0026#39;streaming\u0026#39; SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;table\u0026#39;; -- available values: \u0026#39;table\u0026#39;, \u0026#39;changelog\u0026#39; and \u0026#39;tableau\u0026#39; SET \u0026#39;sql-client.execution.max-table-result.rows\u0026#39; = \u0026#39;10000\u0026#39;; -- optional: maximum number of maintained rows SET \u0026#39;parallelism.default\u0026#39; = \u0026#39;1\u0026#39;; -- optional: Flink\u0026#39;s parallelism (1 by default) SET \u0026#39;pipeline.auto-watermark-interval\u0026#39; = \u0026#39;200\u0026#39;; --optional: interval for periodic watermarks SET \u0026#39;pipeline.max-parallelism\u0026#39; = \u0026#39;10\u0026#39;; -- optional: Flink\u0026#39;s maximum parallelism SET \u0026#39;table.exec.state.ttl\u0026#39; = \u0026#39;1000\u0026#39;; -- optional: table program\u0026#39;s idle state time SET \u0026#39;restart-strategy\u0026#39; = \u0026#39;fixed-delay\u0026#39;; -- Configuration options for adjusting and tuning table programs. SET \u0026#39;table.optimizer.join-reorder-enabled\u0026#39; = \u0026#39;true\u0026#39;; SET \u0026#39;table.exec.spill-compression.enabled\u0026#39; = \u0026#39;true\u0026#39;; SET \u0026#39;table.exec.spill-compression.block-size\u0026#39; = \u0026#39;128kb\u0026#39;; This configuration:
connects to Hive catalogs and uses MyCatalog as the current catalog with MyDatabase as the current database of the catalog, defines a table MyTable that can read data from a CSV file, defines a view MyCustomView that declares a virtual table using a SQL query, defines a user-defined function myUDF that can be instantiated using the class name, uses streaming mode for running statements and a parallelism of 1, runs exploratory queries in the table result mode, and makes some planner adjustments around join reordering and spilling via configuration options. When using -i \u0026lt;init.sql\u0026gt; option to initialize SQL Client session, the following statements are allowed in an initialization SQL file:
DDL(CREATE/DROP/ALTER), USE CATALOG/DATABASE, LOAD/UNLOAD MODULE, SET command, RESET command. When execute queries or insert statements, please enter the interactive mode or use the -f option to submit the SQL statements.
Attention If SQL Client receives errors during initialization, SQL Client will exit with error messages.
Dependencies # The SQL Client does not require setting up a Java project using Maven, Gradle, or sbt. Instead, you can pass the dependencies as regular JAR files that get submitted to the cluster. You can either specify each JAR file separately (using --jar) or define entire library directories (using --library). For connectors to external systems (such as Apache Kafka) and corresponding data formats (such as JSON), Flink provides ready-to-use JAR bundles. These JAR files can be downloaded for each release from the Maven central repository.
The full list of offered SQL JARs can be found on the connection to external systems page.
You can refer to the configuration section for information on how to configure connector and format dependencies.
Use SQL Client to submit job # SQL Client allows users to submit jobs either within the interactive command line or using -f option to execute sql file.
In both modes, SQL Client supports to parse and execute all types of the Flink supported SQL statements.
Interactive Command Line # In interactive Command Line, the SQL Client reads user inputs and executes the statement terminated by semicolon (;).
SQL Client will print success message if the statement is executed successfully. When getting errors, SQL Client will also print error messages. By default, the error message only contains the error cause. In order to print the full exception stack for debugging, please set the sql-client.verbose to true through command SET 'sql-client.verbose' = 'true';.
Execute SQL Files # SQL Client supports to execute a SQL script file with the -f option. SQL Client will execute statements one by one in the SQL script file and print execution messages for each executed statements. Once a statement fails, the SQL Client will exit and all the remaining statements will not be executed.
An example of such a file is presented below.
CREATE TEMPORARY TABLE users ( user_id BIGINT, user_name STRING, user_level STRING, region STRING, PRIMARY KEY (user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;upsert-kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;users\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;key.format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;value.format\u0026#39; = \u0026#39;avro\u0026#39; ); -- set sync mode SET \u0026#39;table.dml-sync\u0026#39; = \u0026#39;true\u0026#39;; -- set the job name SET \u0026#39;pipeline.name\u0026#39; = \u0026#39;SqlJob\u0026#39;; -- set the queue that the job submit to SET \u0026#39;yarn.application.queue\u0026#39; = \u0026#39;root\u0026#39;; -- set the job parallelism SET \u0026#39;parallelism.default\u0026#39; = \u0026#39;100\u0026#39;; -- restore from the specific savepoint path SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;; INSERT INTO pageviews_enriched SELECT * FROM pageviews AS p LEFT JOIN users FOR SYSTEM_TIME AS OF p.proctime AS u ON p.user_id = u.user_id; This configuration:
defines a temporal table source users that reads from a CSV file, set the properties, e.g job name, set the savepoint path, submit a sql job that load the savepoint from the specified savepoint path. Attention Compared to the interactive mode, SQL Client will stop execution and exit when there are errors.
Execute a set of SQL statements # SQL Client execute each INSERT INTO statement as a single Flink job. However, this is sometimes not optimal because some part of the pipeline can be reused. SQL Client supports STATEMENT SET syntax to execute a set of SQL statements. This is an equivalent feature with StatementSet in Table API. The STATEMENT SET syntax encloses one or more INSERT INTO statements. All statements in a STATEMENT SET block are holistically optimized and executed as a single Flink job. Joint optimization and execution allows for reusing common intermediate results and can therefore significantly improve the efficiency of executing multiple queries.
Syntax # EXECUTE STATEMENT SET BEGIN -- one or more INSERT INTO statements { INSERT INTO|OVERWRITE \u0026lt;select_statement\u0026gt;; }+ END; Attention The statements of enclosed in the STATEMENT SET must be separated by a semicolon (;). The old syntax BEGIN STATEMENT SET; ... END; is deprecated, may be removed in the future version.
SQL CLI Flink SQL\u0026gt; CREATE TABLE pageviews ( \u0026gt; user_id BIGINT, \u0026gt; page_id BIGINT, \u0026gt; viewtime TIMESTAMP, \u0026gt; proctime AS PROCTIME() \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026gt; \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026gt; \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026gt; \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; CREATE TABLE pageview ( \u0026gt; page_id BIGINT, \u0026gt; cnt BIGINT \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026gt; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026gt; \u0026#39;table-name\u0026#39; = \u0026#39;pageview\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; CREATE TABLE uniqueview ( \u0026gt; page_id BIGINT, \u0026gt; cnt BIGINT \u0026gt; ) WITH ( \u0026gt; \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026gt; \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026gt; \u0026#39;table-name\u0026#39; = \u0026#39;uniqueview\u0026#39; \u0026gt; ); [INFO] Execute statement succeed. Flink SQL\u0026gt; EXECUTE STATEMENT SET \u0026gt; BEGIN \u0026gt; \u0026gt; INSERT INTO pageview \u0026gt; SELECT page_id, count(1) \u0026gt; FROM pageviews \u0026gt; GROUP BY page_id; \u0026gt; \u0026gt; INSERT INTO uniqueview \u0026gt; SELECT page_id, count(distinct user_id) \u0026gt; FROM pageviews \u0026gt; GROUP BY page_id; \u0026gt; \u0026gt; END; [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 6b1af540c0c0bb3fcfcad50ac037c862 SQL File CREATE TABLE pageviews ( user_id BIGINT, page_id BIGINT, viewtime TIMESTAMP, proctime AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;pageviews\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;avro\u0026#39; ); CREATE TABLE pageview ( page_id BIGINT, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;pageview\u0026#39; ); CREATE TABLE uniqueview ( page_id BIGINT, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;jdbc\u0026#39;, \u0026#39;url\u0026#39; = \u0026#39;jdbc:mysql://localhost:3306/mydatabase\u0026#39;, \u0026#39;table-name\u0026#39; = \u0026#39;uniqueview\u0026#39; ); EXECUTE STATEMENT SET BEGIN INSERT INTO pageview SELECT page_id, count(1) FROM pageviews GROUP BY page_id; INSERT INTO uniqueview SELECT page_id, count(distinct user_id) FROM pageviews GROUP BY page_id; END; Execute DML statements sync/async # By default, SQL Client executes DML statements asynchronously. That means, SQL Client will submit a job for the DML statement to a Flink cluster, and not wait for the job to finish. So SQL Client can submit multiple jobs at the same time. This is useful for streaming jobs, which are long-running in general.
SQL Client makes sure that a statement is successfully submitted to the cluster. Once the statement is submitted, the CLI will show information about the Flink job.
Flink SQL\u0026gt; INSERT INTO MyTableSink SELECT * FROM MyTableSource; [INFO] Table update statement has been successfully submitted to the cluster: Cluster ID: StandaloneClusterId Job ID: 6f922fe5cba87406ff23ae4a7bb79044 Attention The SQL Client does not track the status of the running Flink job after submission. The CLI process can be shutdown after the submission without affecting the detached query. Flink\u0026rsquo;s restart strategy takes care of the fault-tolerance. A query can be cancelled using Flink\u0026rsquo;s web interface, command-line, or REST API.
However, for batch users, it\u0026rsquo;s more common that the next DML statement requires waiting until the previous DML statement finishes. In order to execute DML statements synchronously, you can set table.dml-sync option true in SQL Client.
Flink SQL\u0026gt; SET \u0026#39;table.dml-sync\u0026#39; = \u0026#39;true\u0026#39;; [INFO] Session property has been set. Flink SQL\u0026gt; INSERT INTO MyTableSink SELECT * FROM MyTableSource; [INFO] Submitting SQL update statement to the cluster... [INFO] Execute statement in sync mode. Please wait for the execution finish... [INFO] Complete execution of the SQL update statement. Attention If you want to terminate the job, just type CTRL-C to cancel the execution.
Start a SQL Job from a savepoint # Flink supports to start the job with specified savepoint. In SQL Client, it\u0026rsquo;s allowed to use SET command to specify the path of the savepoint.
Flink SQL\u0026gt; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026#39;/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab\u0026#39;; [INFO] Session property has been set. -- all the following DML statements will be restroed from the specified savepoint path Flink SQL\u0026gt; INSERT INTO ... When the path to savepoint is specified, Flink will try to restore the state from the savepoint when executing all the following DML statements.
Because the specified savepoint path will affect all the following DML statements, you can use RESET command to reset this config option, i.e. disable restoring from savepoint.
Flink SQL\u0026gt; RESET execution.savepoint.path; [INFO] Session property has been reset. For more details about creating and managing savepoints, please refer to Job Lifecycle Management.
Define a Custom Job Name # SQL Client supports to define job name for queries and DML statements through SET command.
Flink SQL\u0026gt; SET \u0026#39;pipeline.name\u0026#39; = \u0026#39;kafka-to-hive\u0026#39;; [INFO] Session property has been set. -- all the following DML statements will use the specified job name. Flink SQL\u0026gt; INSERT INTO ... Because the specified job name will affect all the following queries and DML statements, you can also use RESET command to reset this configuration, i.e. use default job names.
Flink SQL\u0026gt; RESET pipeline.name; [INFO] Session property has been reset. If the option pipeline.name is not specified, SQL Client will generate a default name for the submitted job, e.g. insert-into_\u0026lt;sink_table_name\u0026gt; for INSERT INTO statements.
Back to top
局限与未来 # 当前的 SQL 客户端仅支持嵌入式模式。在将来，社区计划提供基于 REST 的 SQL 客户端网关（Gateway) 的功能，详见 FLIP-24 和 FLIP-91。
Back to top
`}),e.add({id:296,href:"/flink/flink-docs-master/zh/docs/dev/datastream/testing/",title:"测试",section:"DataStream API",content:` 测试 # 测试是每个软件开发过程中不可或缺的一部分， Apache Flink 同样提供了在测试金字塔的多个级别上测试应用程序代码的工具。
测试用户自定义函数 # 通常，我们可以假设 Flink 在用户自定义函数之外产生了正确的结果。因此，建议尽可能多的用单元测试来测试那些包含主要业务逻辑的类。
单元测试无状态、无时间限制的 UDF # 例如，让我们以以下无状态的 MapFunction 为例。
Java public class IncrementMapFunction implements MapFunction\u0026lt;Long, Long\u0026gt; { @Override public Long map(Long record) throws Exception { return record + 1; } } Scala class IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } 通过传递合适地参数并验证输出，你可以很容易的使用你喜欢的测试框架对这样的函数进行单元测试。
Java public class IncrementMapFunctionTest { @Test public void testIncrement() throws Exception { // instantiate your function IncrementMapFunction incrementer = new IncrementMapFunction(); // call the methods that you have implemented assertEquals(3L, incrementer.map(2L)); } } Scala class IncrementMapFunctionTest extends FlatSpec with Matchers { \u0026#34;IncrementMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function val incrementer: IncrementMapFunction = new IncrementMapFunction() // call the methods that you have implemented incremeter.map(2) should be (3) } } 类似地，对于使用 org.apache.flink.util.Collector 的用户自定义函数（例如FlatMapFunction 或者 ProcessFunction），可以通过提供模拟对象而不是真正的 collector 来轻松测试。具有与 IncrementMapFunction 相同功能的 FlatMapFunction 可以按照以下方式进行单元测试。
Java public class IncrementFlatMapFunctionTest { @Test public void testIncrement() throws Exception { // instantiate your function IncrementFlatMapFunction incrementer = new IncrementFlatMapFunction(); Collector\u0026lt;Integer\u0026gt; collector = mock(Collector.class); // call the methods that you have implemented incrementer.flatMap(2L, collector); //verify collector was called with the right output Mockito.verify(collector, times(1)).collect(3L); } } Scala class IncrementFlatMapFunctionTest extends FlatSpec with MockFactory { \u0026#34;IncrementFlatMapFunction\u0026#34; should \u0026#34;increment values\u0026#34; in { // instantiate your function val incrementer : IncrementFlatMapFunction = new IncrementFlatMapFunction() val collector = mock[Collector[Integer]] //verify collector was called with the right output (collector.collect _).expects(3) // call the methods that you have implemented flattenFunction.flatMap(2, collector) } } 对有状态或及时 UDF 和自定义算子进行单元测试 # 对使用管理状态或定时器的用户自定义函数的功能测试会更加困难，因为它涉及到测试用户代码和 Flink 运行时的交互。 为此，Flink 提供了一组所谓的测试工具，可用于测试用户自定义函数和自定义算子：
OneInputStreamOperatorTestHarness (适用于 DataStream 上的算子) KeyedOneInputStreamOperatorTestHarness (适用于 KeyedStream 上的算子) TwoInputStreamOperatorTestHarness (f适用于两个 DataStream 的 ConnectedStreams 算子) KeyedTwoInputStreamOperatorTestHarness (适用于两个 KeyedStream 上的 ConnectedStreams 算子) 要使用测试工具，还需要一组其他的依赖项，请查阅配置小节了解更多细节。
现在，可以使用测试工具将记录和 watermark 推送到用户自定义函数或自定义算子中，控制处理时间，最后对算子的输出（包括旁路输出）进行校验。
Java public class StatefulFlatMapTest { private OneInputStreamOperatorTestHarness\u0026lt;Long, Long\u0026gt; testHarness; private StatefulFlatMap statefulFlatMapFunction; @Before public void setupTestHarness() throws Exception { //instantiate user-defined function statefulFlatMapFunction = new StatefulFlatMapFunction(); // wrap user defined function into a the corresponding operator testHarness = new OneInputStreamOperatorTestHarness\u0026lt;\u0026gt;(new StreamFlatMap\u0026lt;\u0026gt;(statefulFlatMapFunction)); // optionally configured the execution environment testHarness.getExecutionConfig().setAutoWatermarkInterval(50); // open the test harness (will also call open() on RichFunctions) testHarness.open(); } @Test public void testingStatefulFlatMapFunction() throws Exception { //push (timestamped) elements into the operator (and hence user defined function) testHarness.processElement(2L, 100L); //trigger event time timers by advancing the event time of the operator with a watermark testHarness.processWatermark(100L); //trigger processing time timers by advancing the processing time of the operator directly testHarness.setProcessingTime(100L); //retrieve list of emitted records for assertions assertThat(testHarness.getOutput(), containsInExactlyThisOrder(3L)); //retrieve list of records emitted to a specific side output for assertions (ProcessFunction only) //assertThat(testHarness.getSideOutput(new OutputTag\u0026lt;\u0026gt;(\u0026#34;invalidRecords\u0026#34;)), hasSize(0)) } } Scala class StatefulFlatMapFunctionTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[Long, Long] = null private var statefulFlatMap: StatefulFlatMapFunction = null before { //instantiate user-defined function statefulFlatMap = new StatefulFlatMap // wrap user defined function into a the corresponding operator testHarness = new OneInputStreamOperatorTestHarness[Long, Long](new StreamFlatMap(statefulFlatMap)) // optionally configured the execution environment testHarness.getExecutionConfig().setAutoWatermarkInterval(50) // open the test harness (will also call open() on RichFunctions) testHarness.open() } \u0026#34;StatefulFlatMap\u0026#34; should \u0026#34;do some fancy stuff with timers and state\u0026#34; in { //push (timestamped) elements into the operator (and hence user defined function) testHarness.processElement(2, 100) //trigger event time timers by advancing the event time of the operator with a watermark testHarness.processWatermark(100) //trigger proccesign time timers by advancing the processing time of the operator directly testHarness.setProcessingTime(100) //retrieve list of emitted records for assertions testHarness.getOutput should contain (3) //retrieve list of records emitted to a specific side output for assertions (ProcessFunction only) //testHarness.getSideOutput(new OutputTag[Int](\u0026#34;invalidRecords\u0026#34;)) should have size 0 } } KeyedOneInputStreamOperatorTestHarness 和 KeyedTwoInputStreamOperatorTestHarness 可以通过为键的类另外提供一个包含 TypeInformation 的 KeySelector 来实例化。
Java public class StatefulFlatMapFunctionTest { private OneInputStreamOperatorTestHarness\u0026lt;String, Long, Long\u0026gt; testHarness; private StatefulFlatMap statefulFlatMapFunction; @Before public void setupTestHarness() throws Exception { //instantiate user-defined function statefulFlatMapFunction = new StatefulFlatMapFunction(); // wrap user defined function into a the corresponding operator testHarness = new KeyedOneInputStreamOperatorTestHarness\u0026lt;\u0026gt;(new StreamFlatMap\u0026lt;\u0026gt;(statefulFlatMapFunction), new MyStringKeySelector(), Types.STRING); // open the test harness (will also call open() on RichFunctions) testHarness.open(); } //tests } Scala class StatefulFlatMapTest extends FlatSpec with Matchers with BeforeAndAfter { private var testHarness: OneInputStreamOperatorTestHarness[String, Long, Long] = null private var statefulFlatMapFunction: FlattenFunction = null before { //instantiate user-defined function statefulFlatMapFunction = new StateFulFlatMap // wrap user defined function into a the corresponding operator testHarness = new KeyedOneInputStreamOperatorTestHarness(new StreamFlatMap(statefulFlatMapFunction),new MyStringKeySelector(), Types.STRING()) // open the test harness (will also call open() on RichFunctions) testHarness.open() } //tests } 在 Flink 代码库里可以找到更多使用这些测试工具的示例，例如：
org.apache.flink.streaming.runtime.operators.windowing.WindowOperatorTest 是测试算子和用户自定义函数（取决于处理时间和事件时间）的一个很好的例子。 注意 AbstractStreamOperatorTestHarness 及其派生类目前不属于公共 API，可以进行更改。
单元测试 Process Function # 考虑到它的重要性，除了之前可以直接用于测试 ProcessFunction 的测试工具之外，Flink 还提供了一个名为 ProcessFunctionTestHarnesses 的测试工具工厂类，可以简化测试工具的实例化。考虑以下示例：
注意 要使用此测试工具，还需要引入上一节中介绍的依赖项。
Java public static class PassThroughProcessFunction extends ProcessFunction\u0026lt;Integer, Integer\u0026gt; { @Override public void processElement(Integer value, Context ctx, Collector\u0026lt;Integer\u0026gt; out) throws Exception { out.collect(value); } } Scala class PassThroughProcessFunction extends ProcessFunction[Integer, Integer] { @throws[Exception] override def processElement(value: Integer, ctx: ProcessFunction[Integer, Integer]#Context, out: Collector[Integer]): Unit = { out.collect(value) } } 通过传递合适的参数并验证输出，对使用 ProcessFunctionTestHarnesses 是很容易进行单元测试并验证输出。
Java public class PassThroughProcessFunctionTest { @Test public void testPassThrough() throws Exception { //instantiate user-defined function PassThroughProcessFunction processFunction = new PassThroughProcessFunction(); // wrap user defined function into a the corresponding operator OneInputStreamOperatorTestHarness\u0026lt;Integer, Integer\u0026gt; harness = ProcessFunctionTestHarnesses .forProcessFunction(processFunction); //push (timestamped) elements into the operator (and hence user defined function) harness.processElement(1, 10); //retrieve list of emitted records for assertions assertEquals(harness.extractOutputValues(), Collections.singletonList(1)); } } Scala class PassThroughProcessFunctionTest extends FlatSpec with Matchers { \u0026#34;PassThroughProcessFunction\u0026#34; should \u0026#34;forward values\u0026#34; in { //instantiate user-defined function val processFunction = new PassThroughProcessFunction // wrap user defined function into a the corresponding operator val harness = ProcessFunctionTestHarnesses.forProcessFunction(processFunction) //push (timestamped) elements into the operator (and hence user defined function) harness.processElement(1, 10) //retrieve list of emitted records for assertions harness.extractOutputValues() should contain (1) } } 有关如何使用 ProcessFunctionTestHarnesses 来测试 ProcessFunction 不同风格的更多示例，, 例如 KeyedProcessFunction，KeyedCoProcessFunction，BroadcastProcessFunction等，鼓励用户自行查看 ProcessFunctionTestHarnessesTest。
测试 Flink 作业 # JUnit 规则 MiniClusterWithClientResource # Apache Flink 提供了一个名为 MiniClusterWithClientResource 的 Junit 规则，用于针对本地嵌入式小型集群测试完整的作业。 叫做 MiniClusterWithClientResource.
要使用 MiniClusterWithClientResource，需要添加一个额外的依赖项（测试范围）。
\u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-test-utils\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gttest\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! 让我们采用与前面几节相同的简单 MapFunction来做示例。
Java public class IncrementMapFunction implements MapFunction\u0026lt;Long, Long\u0026gt; { @Override public Long map(Long record) throws Exception { return record + 1; } } Scala class IncrementMapFunction extends MapFunction[Long, Long] { override def map(record: Long): Long = { record + 1 } } 现在，可以在本地 Flink 集群使用这个 MapFunction 的简单 pipeline，如下所示。
Java public class ExampleIntegrationTest { @ClassRule public static MiniClusterWithClientResource flinkCluster = new MiniClusterWithClientResource( new MiniClusterResourceConfiguration.Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build()); @Test public void testIncrementPipeline() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // configure your test environment env.setParallelism(2); // values are collected in a static variable CollectSink.values.clear(); // create a stream of custom elements and apply transformations env.fromElements(1L, 21L, 22L) .map(new IncrementMapFunction()) .addSink(new CollectSink()); // execute env.execute(); // verify your results assertTrue(CollectSink.values.containsAll(2L, 22L, 23L)); } // create a testing sink private static class CollectSink implements SinkFunction\u0026lt;Long\u0026gt; { // must be static public static final List\u0026lt;Long\u0026gt; values = Collections.synchronizedList(new ArrayList\u0026lt;\u0026gt;()); @Override public void invoke(Long value, SinkFunction.Context context) throws Exception { values.add(value); } } } Scala class StreamingJobIntegrationTest extends FlatSpec with Matchers with BeforeAndAfter { val flinkCluster = new MiniClusterWithClientResource(new MiniClusterResourceConfiguration.Builder() .setNumberSlotsPerTaskManager(2) .setNumberTaskManagers(1) .build) before { flinkCluster.before() } after { flinkCluster.after() } \u0026#34;IncrementFlatMapFunction pipeline\u0026#34; should \u0026#34;incrementValues\u0026#34; in { val env = StreamExecutionEnvironment.getExecutionEnvironment // configure your test environment env.setParallelism(2) // values are collected in a static variable CollectSink.values.clear() // create a stream of custom elements and apply transformations env.fromElements(1L, 21L, 22L) .map(new IncrementMapFunction()) .addSink(new CollectSink()) // execute env.execute() // verify your results CollectSink.values should contain allOf (2, 22, 23) } } // create a testing sink class CollectSink extends SinkFunction[Long] { override def invoke(value: Long, context: SinkFunction.Context): Unit = { CollectSink.values.add(value) } } object CollectSink { // must be static val values: util.List[Long] = Collections.synchronizedList(new util.ArrayList()) } 关于使用 MiniClusterWithClientResource 进行集成测试的几点备注：
为了不将整个 pipeline 代码从生产复制到测试，请将你的 source 和 sink 在生产代码中设置成可插拔的，并在测试中注入特殊的测试 source 和测试 sink。
这里使用 CollectSink 中的静态变量，是因为Flink 在将所有算子分布到整个集群之前先对其进行了序列化。 解决此问题的一种方法是与本地 Flink 小型集群通过实例化算子的静态变量进行通信。 或者，你可以使用测试的 sink 将数据写入临时目录的文件中。
如果你的作业使用事件时间计时器，则可以实现自定义的 并行 源函数来发出 watermark。
建议始终以 parallelism \u0026gt; 1 的方式在本地测试 pipeline，以识别只有在并行执行 pipeline 时才会出现的 bug。
优先使用 @ClassRule 而不是 @Rule，这样多个测试可以共享同一个 Flink 集群。这样做可以节省大量的时间，因为 Flink 集群的启动和关闭通常会占用实际测试的执行时间。
如果你的 pipeline 包含自定义状态处理，则可以通过启用 checkpoint 并在小型集群中重新启动作业来测试其正确性。为此，你需要在 pipeline 中（仅测试）抛出用户自定义函数的异常来触发失败。
Back to top
`}),e.add({id:297,href:"/flink/flink-docs-master/zh/docs/deployment/memory/network_mem_tuning/",title:"网络缓冲调优",section:"内存配置",content:` 网络内存调优指南 # 概述 # Flink 中每条消息都会被放到网络缓冲（network buffer） 中，并以此为最小单位发送到下一个 subtask。 为了维持连续的高吞吐，Flink 在传输过程的输入端和输出端使用了网络缓冲队列。
每个 subtask 都有一个输入队列来接收数据和一个输出队列来发送数据到下一个 subtask。 在 pipeline 场景，拥有更多的中间缓存数据可以使 Flink 提供更高、更富有弹性的吞吐量，但是也会增加快照时间。
只有所有的 subtask 都收到了全部注入的 checkpoint barrier 才能完成快照。 在对齐的 checkpoints 中，checkpoint barrier 会跟着网络缓冲数据在 job graph 中流动。 缓冲数据越多，checkpoint barrier 流动的时间就越长。在非对齐的 checkpoints 中，缓冲数据越多，checkpoint 就会越大，因为这些数据都会被持久化到 checkpoint 中。
缓冲消胀机制（Buffer Debloating） # 之前，配置缓冲数据量的唯一方法是指定缓冲区的数量和大小。然而，因为每次部署的不同很难配置一组完美的参数。 Flink 1.14 新引入的缓冲消胀机制尝试通过自动调整缓冲数据量到一个合理值来解决这个问题。
缓冲消胀功能计算 subtask 可能达到的最大吞吐（始终保持繁忙状态时）并且通过调整缓冲数据量来使得数据的消费时间达到配置值。
可以通过设置 taskmanager.network.memory.buffer-debloat.enabled 为 true 来开启缓冲消胀机制。 通过设置 taskmanager.network.memory.buffer-debloat.target 为 duration 类型的值来指定消费缓冲数据的目标时间。 默认值应该能满足大多数场景。
这个功能使用过去的吞吐数据来预测消费剩余缓冲数据的时间。如果预测不准，缓冲消胀机制会导致以下问题：
没有足够的缓存数据来提供全量吞吐。 有太多缓冲数据对 checkpoint barrier 推进或者非对齐的 checkpoint 的大小造成不良影响。 如果您的作业负载经常变化（即，突如其来的数据尖峰，定期的窗口聚合触发或者 join ），您可能需要调整以下设置：
taskmanager.network.memory.buffer-debloat.period：这是缓冲区大小重算的最小时间周期。周期越小，缓冲消胀机制的反应时间就越快，但是必要的计算会消耗更多的CPU。
taskmanager.network.memory.buffer-debloat.samples：调整用于计算平均吞吐量的采样数。采集样本的频率可以通过 taskmanager.network.memory.buffer-debloat.period 来设置。样本数越少，缓冲消胀机制的反应时间就越快，但是当吞吐量突然飙升或者下降时，缓冲消胀机制计算的最佳缓冲数据量会更容易出错。
taskmanager.network.memory.buffer-debloat.threshold-percentages：防止缓冲区大小频繁改变的优化（比如，新的大小跟旧的大小相差不大）。
更多详细和额外的参数配置，请参考配置参数。
您可以使用以下指标来监控当前的缓冲区大小：
estimatedTimeToConsumeBuffersMs：消费所有输入通道（input channel）中数据的总时间。 debloatedBufferSize：当前的缓冲区大小。 限制 # 当前，有一些场景还没有自动地被缓冲消胀机制处理。
多个输入和合并 # 当前，吞吐计算和缓冲消胀发生在 subtask 层面。
如果您的 subtask 有很多不同的输入或者有一个合并的输入，缓冲消胀可能会导致低吞吐的输入有太多缓冲数据，而高吞吐输入的缓冲区数量可能太少而不够维持当前吞吐。当不同的输入吞吐差别比较大时，这种现象会更加的明显。我们推荐您在测试这个功能时重点关注这种 subtask。
缓冲区的尺寸和个数 # 当前，缓冲消胀仅在使用的缓冲区大小上设置上限。实际的缓冲区大小和个数保持不变。这意味着缓冲消胀机制不会减少作业的内存使用。您应该手动减少缓冲区的大小或者个数。
此外，如果您想减少缓冲数据量使其低于缓冲消胀当前允许的量，您可能需要手动的设置缓冲区的个数。
High parallelism # Currently, the buffer debloating mechanism might not perform correctly with high parallelism (above ~200) using the default configuration. If you observe reduced throughput or higher than expected checkpointing times we suggest increasing the number of floating buffers (taskmanager.network.memory.floating-buffers-per-gate) from the default value to at least the number equal to the parallelism.
The actual value of parallelism from which the problem occurs is various from job to job but normally it should be more than a couple of hundreds.
网络缓冲生命周期 # Flink 有多个本地缓冲区池 —— 每个输出和输入流对应一个。 每个缓冲区池的大小被限制为
#channels * taskmanager.network.memory.buffers-per-channel + taskmanager.network.memory.floating-buffers-per-gate
缓冲区的大小可以通过 taskmanager.memory.segment-size 来设置。
输入网络缓冲 # 输入通道中的缓冲区被分为独占缓冲区（exclusive buffer）和流动缓冲区（floating buffer）。每个独占缓冲区只能被一个特定的通道使用。 一个通道可以从输入流的共享缓冲区池中申请额外的流动缓冲区。剩余的流动缓冲区是可选的并且只有资源足够的时候才能获取。
在初始阶段：
Flink 会为每一个输入通道获取配置数量的独占缓冲区。 所有的独占缓冲区都必须被满足，否则作业会抛异常失败。 Flink 至少要有一个流动缓冲区才能运行。 输出网络缓冲 # 不像输入缓冲区池，输出缓冲区池只有一种类型的缓冲区被所有的 subpartitions 共享。
为了避免过多的数据倾斜，每个 subpartition 的缓冲区数量可以通过 taskmanager.network.memory.max-buffers-per-channel 来限制。
不同于输入缓冲区池，这里配置的独占缓冲区和流动缓冲区只被当作推荐值。如果没有足够的缓冲区，每个输出 subpartition 可以只使用一个独占缓冲区而没有流动缓冲区。
透支缓冲区（Overdraft buffers） # 另外，每个 subtask 输出数据时可以至多请求 taskmanager.network.memory.max-overdraft-buffers-per-gate （默认 5）个额外的透支缓冲区（overdraft buffers）。只有当前 subtask 被下游 subtasks 反压且当前 subtask 需要 请求超过 1 个网络缓冲区（network buffer）才能完成当前的操作时，透支缓冲区才会被使用。可能发生在以下情况：
序列化非常大的 records，不能放到单个网络缓冲区中。 类似 flat map 的算子，即：处理单个 record 时可能会生产多个 records。 周期性地或某些事件触发产生大量 records 的算子（例如：WindowOperator 的触发）。 在这些情况下，如果没有透支缓冲区，Flink 的 subtask 线程会被阻塞在反压，从而阻止例如 Unaligned Checkpoint 的完成。 为了缓解这种情况，增加了透支缓冲区的概念。这些透支缓冲区是可选的，Flink 可以仅仅使用常规的缓冲区逐渐取得进展，也就是 说 0 是 taskmanager.network.memory.max-overdraft-buffers-per-gate 可以接受的配置值。
缓冲区的数量 # 独占缓冲区和流动缓冲区的默认配置应该足以应对最大吞吐。如果想要最小化缓冲数据量，那么可以将独占缓冲区设置为 0，同时减小内存段的大小。
选择缓冲区的大小 # 在往下游 subtask 发送数据部分时，缓冲区通过汇集 record 来优化网络开销。下游 subtask 应该在接收到完整的 record 后才开始处理它。
If the buffer size is too small, or the buffers are flushed too frequently (execution.buffer-timeout configuration parameter), this can lead to decreased throughput since the per-buffer overhead are significantly higher then per-record overheads in the Flink\u0026rsquo;s runtime.
As a rule of thumb, we don\u0026rsquo;t recommend thinking about increasing the buffer size, or the buffer timeout unless you can observe a network bottleneck in your real life workload (downstream operator idling, upstream backpressured, output buffer queue is full, downstream input queue is empty).
如果缓冲区太大，会导致：
内存使用高 大量的 checkpoint 数据量（针对非对齐的 checkpoints） 漫长的 checkpoint 时间（针对对齐的 checkpoints） execution.buffer-timeout 较小时内存分配使用率会比较低，因为缓冲区还没被塞满数据就被发送下去了。 选择缓冲区的数量 # 缓冲区的数量是通过 taskmanager.network.memory.buffers-per-channel 和 taskmanager.network.memory.floating-buffers-per-gate 来配置的。
为了最好的吞吐率，我们建议使用独占缓冲区和流动缓冲区的默认值(except you have one of limit cases)。如果缓冲数据量存在问题，更建议打开缓冲消胀。
您可以人工地调整网络缓冲区的个数，但是需要注意：
您应该根据期待的吞吐量（单位 bytes/second）来调整缓冲区的数量。协调数据传输量（大约两个节点之间的两个往返消息）。延迟也取决于您的网络。 使用 buffer 往返时间（大概 1ms 在正常的本地网络中），缓冲区大小和期待的吞吐，您可以通过下面的公式计算维持吞吐所需要的缓冲区数量：
number_of_buffers = expected_throughput * buffer_roundtrip / buffer_size 比如，期待吞吐为 320MB/s，往返延迟为 1ms，内存段为默认大小，为了维持吞吐需要使用10个活跃的缓冲区：
number_of_buffers = 320MB/s * 1ms / 32KB = 10 流动缓冲区的目的是为了处理数据倾斜。理想情况下，流动缓冲区的数量（默认8个）和每个通道独占缓冲区的数量（默认2个）能够使网络吞吐量饱和。但这并不总是可行和必要的。所有 subtask 中只有一个通道被使用也是非常罕见的。
独占缓冲区的目的是提供一个流畅的吞吐量。当一个缓冲区在传输数据时，另一个缓冲区被填充。当吞吐量比较高时，独占缓冲区的数量是决定 Flink 中缓冲数据的主要因素。
当低吞吐量下出现反压时，您应该考虑减少独占缓冲区。
总结 # 可以通过开启缓冲消胀机制来简化 Flink 网络的内存配置调整。您也可能需要调整它。
如果这不起作用，您可以关闭缓冲消胀机制并且人工地配置内存段的大小和缓冲区个数。针对第二种场景，我们推荐：
使用默认值以获得最大吞吐 减少内存段大小、独占缓冲区的数量来加快 checkpoint 并减少网络栈消耗的内存量 Back to top
`}),e.add({id:298,href:"/flink/flink-docs-master/zh/docs/connectors/table/downloads/",title:"下载页面",section:"Table API Connectors",content:` SQL Connectors 下载页面 # Download links are available only for stable releases. The page contains links to optional sql-client connectors and formats that are not part of the binary distribution.
可选的 SQL formats # Name Download link Avro Only available for stable versions. Avro Schema Registry Only available for stable versions. Debezium Only available for stable versions. ORC Only available for stable versions. Parquet Only available for stable versions. Protobuf Only available for stable versions. 可选的 SQL 连接器 # Name Version Download Link AWS Kinesis Firehose Only available for stable versions. Elasticsearch 6.x Only available for stable versions. Elasticsearch 7.x and later versions Only available for stable versions. Files Only available for stable versions. HBase 1.4.x Only available for stable versions. HBase 2.2.x Only available for stable versions. JDBC Only available for stable versions. Kafka universal Only available for stable versions. Kinesis Only available for stable versions. Pulsar Only available for stable versions. RabbitMQ Only available for stable versions. Upsert Kafka universal Only available for stable versions. `}),e.add({id:299,href:"/flink/flink-docs-master/zh/docs/dev/datastream/experimental/",title:"实验功能",section:"DataStream API",content:` 实验功能 # This section describes experimental features in the DataStream API. Experimental features are still evolving and can be either unstable, incomplete, or subject to heavy change in future versions.
Reinterpreting a pre-partitioned data stream as keyed stream # We can re-interpret a pre-partitioned data stream as a keyed stream to avoid shuffling.
WARNING: The re-interpreted data stream MUST already be pre-partitioned in EXACTLY the same way Flink\u0026rsquo;s keyBy would partition the data in a shuffle w.r.t. key-group assignment. One use-case for this could be a materialized shuffle between two jobs: the first job performs a keyBy shuffle and materializes each output into a partition. A second job has sources that, for each parallel instance, reads from the corresponding partitions created by the first job. Those sources can now be re-interpreted as keyed streams, e.g. to apply windowing. Notice that this trick makes the second job embarrassingly parallel, which can be helpful for a fine-grained recovery scheme.
This re-interpretation functionality is exposed through DataStreamUtils:
static \u0026lt;T, K\u0026gt; KeyedStream\u0026lt;T, K\u0026gt; reinterpretAsKeyedStream( DataStream\u0026lt;T\u0026gt; stream, KeySelector\u0026lt;T, K\u0026gt; keySelector, TypeInformation\u0026lt;K\u0026gt; typeInfo) Given a base stream, a key selector, and type information, the method creates a keyed stream from the base stream.
Code example:
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource\u0026lt;Integer\u0026gt; source = ...; DataStreamUtils.reinterpretAsKeyedStream(source, (in) -\u0026gt; in, TypeInformation.of(Integer.class)) .window(TumblingEventTimeWindows.of(Time.seconds(1))) .reduce((a, b) -\u0026gt; a + b) .addSink(new DiscardingSink\u0026lt;\u0026gt;()); env.execute(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) val source = ... new DataStreamUtils(source).reinterpretAsKeyedStream((in) =\u0026gt; in) .window(TumblingEventTimeWindows.of(Time.seconds(1))) .reduce((a, b) =\u0026gt; a + b) .addSink(new DiscardingSink[Int]) env.execute() Back to top
`}),e.add({id:300,href:"/flink/flink-docs-master/zh/docs/dev/table/config/",title:"配置",section:"Table API \u0026 SQL",content:` 配置 # Table 和 SQL API 的默认配置能够确保结果准确，同时也提供可接受的性能。
根据 Table 程序的需求，可能需要调整特定的参数用于优化。例如，无界流程序可能需要保证所需的状态是有限的(请参阅 流式概念).
概览 # 当实例化一个 TableEnvironment 时，可以使用 EnvironmentSettings 来传递用于当前会话的所期望的配置项 —— 传递一个 Configuration 对象到 EnvironmentSettings。
此外，在每个 TableEnvironment 中，TableConfig 提供用于当前会话的配置项。
对于常见或者重要的配置项，TableConfig 提供带有详细注释的 getters 和 setters 方法。
对于更加高级的配置，用户可以直接访问底层的 key-value 配置项。以下章节列举了所有可用于调整 Flink Table 和 SQL API 程序的配置项。
注意 因为配置项会在执行操作的不同时间点被读取，所以推荐在实例化 TableEnvironment 后尽早地设置配置项。
Java // instantiate table environment Configuration configuration = new Configuration(); // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); EnvironmentSettings settings = EnvironmentSettings.newInstance() .inStreamingMode().withConfiguration(configuration).build(); TableEnvironment tEnv = TableEnvironment.create(settings); // access flink configuration after table environment instantiation TableConfig tableConfig = tEnv.getConfig(); // set low-level key-value options tableConfig.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;); tableConfig.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;); tableConfig.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;); Scala // instantiate table environment val configuration = new Configuration; // set low-level key-value options configuration.setString(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.setString(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) val settings = EnvironmentSettings.newInstance .inStreamingMode.withConfiguration(configuration).build val tEnv: TableEnvironment = TableEnvironment.create(settings) // access flink configuration after table environment instantiation val tableConfig = tEnv.getConfig() // set low-level key-value options tableConfig.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) tableConfig.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) tableConfig.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) Python # instantiate table environment configuration = Configuration() configuration.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) configuration.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) settings = EnvironmentSettings.new_instance() \\ ... .in_streaming_mode() \\ ... .with_configuration(configuration) \\ ... .build() t_env = TableEnvironment.create(settings) # access flink configuration after table environment instantiation table_config = t_env.get_config() # set low-level key-value options table_config.set(\u0026#34;table.exec.mini-batch.enabled\u0026#34;, \u0026#34;true\u0026#34;) table_config.set(\u0026#34;table.exec.mini-batch.allow-latency\u0026#34;, \u0026#34;5 s\u0026#34;) table_config.set(\u0026#34;table.exec.mini-batch.size\u0026#34;, \u0026#34;5000\u0026#34;) SQL CLI Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.enabled\u0026#39; = \u0026#39;true\u0026#39;; Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.allow-latency\u0026#39; = \u0026#39;5s\u0026#39;; Flink SQL\u0026gt; SET \u0026#39;table.exec.mini-batch.size\u0026#39; = \u0026#39;5000\u0026#39;; 执行配置 # 以下选项可用于优化查询执行的性能。
Key Default Type Description table.exec.async-lookup.buffer-capacity
Batch Streaming 100 Integer The max number of async i/o operation that the async lookup join can trigger. table.exec.async-lookup.output-mode
Batch Streaming ORDERED Enum
Output mode for asynchronous operations which will convert to {@see AsyncDataStream.OutputMode}, ORDERED by default. If set to ALLOW_UNORDERED, will attempt to use {@see AsyncDataStream.OutputMode.UNORDERED} when it does not affect the correctness of the result, otherwise ORDERED will be still used.
Possible values:"ORDERED""ALLOW_UNORDERED" table.exec.async-lookup.timeout
Batch Streaming 3 min Duration The async timeout for the asynchronous operation to complete. table.exec.deduplicate.insert-update-after-sensitive-enabled
Streaming true Boolean Set whether the job (especially the sinks) is sensitive to INSERT messages and UPDATE_AFTER messages. If false, Flink may, sometimes (e.g. deduplication for last row), send UPDATE_AFTER instead of INSERT for the first row. If true, Flink will guarantee to send INSERT for the first row, in that case there will be additional overhead. Default is true. table.exec.deduplicate.mini-batch.compact-changes-enabled
Streaming false Boolean Set whether to compact the changes sent downstream in row-time mini-batch. If true, Flink will compact changes and send only the latest change downstream. Note that if the downstream needs the details of versioned data, this optimization cannot be applied. If false, Flink will send all changes to downstream just like when the mini-batch is not enabled. table.exec.disabled-operators
Batch (none) String Mainly for testing. A comma-separated list of operator names, each name represents a kind of disabled operator. Operators that can be disabled include "NestedLoopJoin", "ShuffleHashJoin", "BroadcastHashJoin", "SortMergeJoin", "HashAgg", "SortAgg". By default no operator is disabled. table.exec.legacy-cast-behaviour
Batch Streaming DISABLED Enum
Determines whether CAST will operate following the legacy behaviour or the new one that introduces various fixes and improvements.
Possible values:"ENABLED": CAST will operate following the legacy behaviour."DISABLED": CAST will operate following the new correct behaviour. table.exec.mini-batch.allow-latency
Streaming 0 ms Duration The maximum latency can be used for MiniBatch to buffer input records. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: If table.exec.mini-batch.enabled is set true, its value must be greater than zero. table.exec.mini-batch.enabled
Streaming false Boolean Specifies whether to enable MiniBatch optimization. MiniBatch is an optimization to buffer input records to reduce state access. This is disabled by default. To enable this, users should set this config to true. NOTE: If mini-batch is enabled, 'table.exec.mini-batch.allow-latency' and 'table.exec.mini-batch.size' must be set. table.exec.mini-batch.size
Streaming -1 Long The maximum number of input records can be buffered for MiniBatch. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: MiniBatch only works for non-windowed aggregations currently. If table.exec.mini-batch.enabled is set true, its value must be positive. table.exec.rank.topn-cache-size
Streaming 10000 Long Rank operators have a cache which caches partial state contents to reduce state access. Cache size is the number of records in each ranking task. table.exec.resource.default-parallelism
Batch Streaming -1 Integer Sets default parallelism for all operators (such as aggregate, join, filter) to run with parallel instances. This config has a higher priority than parallelism of StreamExecutionEnvironment (actually, this config overrides the parallelism of StreamExecutionEnvironment). A value of -1 indicates that no default parallelism is set, then it will fallback to use the parallelism of StreamExecutionEnvironment. table.exec.simplify-operator-name-enabled
Batch Streaming true Boolean When it is true, the optimizer will simplify the operator name with id and type of ExecNode and keep detail in description. Default value is true. table.exec.sink.keyed-shuffle
Streaming AUTO Enum
In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink's parallelism differs from upstream operator and upstream is append only. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).
By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).
Possible values:"NONE""AUTO""FORCE" table.exec.sink.not-null-enforcer
Batch Streaming ERROR Enum
Determines how Flink enforces NOT NULL column constraints when inserting null values.
Possible values:"ERROR": Throw a runtime exception when writing null values into NOT NULL column."DROP": Drop records silently if a null value would have to be inserted into a NOT NULL column. table.exec.sink.type-length-enforcer
Batch Streaming IGNORE Enum
Determines whether values for columns with CHAR(\u0026lt;length\u0026gt;)/VARCHAR(\u0026lt;length\u0026gt;)/BINARY(\u0026lt;length\u0026gt;)/VARBINARY(\u0026lt;length\u0026gt;) types will be trimmed or padded (only for CHAR(\u0026lt;length\u0026gt;)/BINARY(\u0026lt;length\u0026gt;)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.
Possible values:"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive."TRIM_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length. table.exec.sink.upsert-materialize
Streaming AUTO Enum
Because of the disorder of ChangeLog data caused by Shuffle in distributed system, the data received by Sink may not be the order of global upsert. So add upsert materialize operator before upsert sink. It receives the upstream changelog records and generate an upsert view for the downstream.
By default, the materialize operator will be added when a distributed disorder occurs on unique keys. You can also choose no materialization(NONE) or force materialization(FORCE).
Possible values:"NONE""AUTO""FORCE" table.exec.sort.async-merge-enabled
Batch true Boolean Whether to asynchronously merge sorted spill files. table.exec.sort.default-limit
Batch -1 Integer Default limit when user don't set a limit after order by. -1 indicates that this configuration is ignored. table.exec.sort.max-num-file-handles
Batch 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles per operator. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading. table.exec.source.cdc-events-duplicate
Streaming false Boolean Indicates whether the CDC (Change Data Capture) sources in the job will produce duplicate change events that requires the framework to deduplicate and get consistent result. CDC source refers to the source that produces full change events, including INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE, for example Kafka source with Debezium format. The value of this configuration is false by default.
However, it's a common case that there are duplicate change events. Because usually the CDC tools (e.g. Debezium) work in at-least-once delivery when failover happens. Thus, in the abnormal situations Debezium may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions.
Therefore, it is recommended to turn on this configuration if your CDC tool is at-least-once delivery. Enabling this configuration requires to define PRIMARY KEY on the CDC sources. The primary key will be used to deduplicate change events and generate normalized changelog stream at the cost of an additional stateful operator. table.exec.source.idle-timeout
Streaming 0 ms Duration When a source do not receive any elements for the timeout time, it will be marked as temporarily idle. This allows downstream tasks to advance their watermarks without the need to wait for watermarks from this source while it is idle. Default value is 0, which means detecting source idleness is not enabled. table.exec.spill-compression.block-size
Batch 64 kb MemorySize The memory size used to do compress when spilling data. The larger the memory, the higher the compression ratio, but more memory resource will be consumed by the job. table.exec.spill-compression.enabled
Batch true Boolean Whether to compress spilled data. Currently we only support compress spilled data for sort and hash-agg and hash-join operators. table.exec.state.ttl
Streaming 0 ms Duration Specifies a minimum time interval for how long idle state (i.e. state which was not updated), will be retained. State will never be cleared until it was idle for less than the minimum time, and will be cleared at some time after it was idle. Default is never clean-up the state. NOTE: Cleaning up state requires additional overhead for bookkeeping. Default value is 0, which means that it will never clean up state. table.exec.uid.format
Streaming "\u0026lt;id\u0026gt;_\u0026lt;transformation\u0026gt;" String Defines the format pattern for generating the UID of an ExecNode streaming transformation. The pattern can be defined globally or per-ExecNode in the compiled plan. Supported arguments are: \u0026lt;id\u0026gt; (from static counter), \u0026lt;type\u0026gt; (e.g. 'stream-exec-sink'), \u0026lt;version\u0026gt;, and \u0026lt;transformation\u0026gt; (e.g. 'constraint-validator' for a sink). In Flink 1.15.x the pattern was wrongly defined as '\u0026lt;id\u0026gt;_\u0026lt;type\u0026gt;_\u0026lt;version\u0026gt;_\u0026lt;transformation\u0026gt;' which would prevent migrations in the future. table.exec.uid.generation
Streaming PLAN_ONLY Enum
In order to remap state to operators during a restore, it is required that the pipeline's streaming transformations get a UID assigned.
The planner can generate and assign explicit UIDs. If no UIDs have been set by the planner, the UIDs will be auto-generated by lower layers that can take the complete topology into account for uniqueness of the IDs. See the DataStream API for more information.
This configuration option is for experts only and the default should be sufficient for most use cases. By default, only pipelines created from a persisted compiled plan will get UIDs assigned explicitly. Thus, these pipelines can be arbitrarily moved around within the same topology without affecting the stable UIDs.
Possible values:"PLAN_ONLY": Sets UIDs on streaming transformations if and only if the pipeline definition comes from a compiled plan. Pipelines that have been constructed in the API without a compilation step will not set an explicit UID as it might not be stable across multiple translations."ALWAYS": Always sets UIDs on streaming transformations. This strategy is for experts only! Pipelines that have been constructed in the API without a compilation step might not be able to be restored properly. The UID generation depends on previously declared pipelines (potentially across jobs if the same JVM is used). Thus, a stable environment must be ensured. Pipeline definitions that come from a compiled plan are safe to use."DISABLED": No explicit UIDs will be set. table.exec.window-agg.buffer-size-limit
Batch 100000 Integer Sets the window elements buffer size limit used in group window agg operator. 优化器配置 # 以下配置可以用于调整查询优化器的行为以获得更好的执行计划。
Key Default Type Description table.optimizer.agg-phase-strategy
Batch Streaming "AUTO" String Strategy for aggregate phase. Only AUTO, TWO_PHASE or ONE_PHASE can be set. AUTO: No special enforcer for aggregate stage. Whether to choose two stage aggregate or one stage aggregate depends on cost. TWO_PHASE: Enforce to use two stage aggregate which has localAggregate and globalAggregate. Note that if aggregate call does not support optimize into two phase, we will still use one stage aggregate. ONE_PHASE: Enforce to use one stage aggregate which only has CompleteGlobalAggregate. table.optimizer.distinct-agg.split.bucket-num
Streaming 1024 Integer Configure the number of buckets when splitting distinct aggregation. The number is used in the first level aggregation to calculate a bucket key 'hash_code(distinct_key) % BUCKET_NUM' which is used as an additional group key after splitting. table.optimizer.distinct-agg.split.enabled
Streaming false Boolean Tells the optimizer whether to split distinct aggregation (e.g. COUNT(DISTINCT col), SUM(DISTINCT col)) into two level. The first aggregation is shuffled by an additional key which is calculated using the hashcode of distinct_key and number of buckets. This optimization is very useful when there is data skew in distinct aggregation and gives the ability to scale-up the job. Default is false. table.optimizer.dynamic-filtering.enabled
Batch Streaming true Boolean When it is true, the optimizer will try to push dynamic filtering into scan table source, the irrelevant partitions or input data will be filtered to reduce scan I/O in runtime. table.optimizer.join-reorder-enabled
Batch Streaming false Boolean Enables join reorder in optimizer. Default is disabled. table.optimizer.join.broadcast-threshold
Batch 1048576 Long Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 to disable broadcasting. table.optimizer.multiple-input-enabled
Batch true Boolean When it is true, the optimizer will merge the operators with pipelined shuffling into a multiple input operator to reduce shuffling and improve performance. Default value is true. table.optimizer.non-deterministic-update.strategy
Streaming IGNORE Enum
When it is \`TRY_RESOLVE\`, the optimizer tries to resolve the correctness issue caused by 'Non-Deterministic Updates' (NDU) in a changelog pipeline. Changelog may contain kinds of message types: Insert (I), Delete (D), Update_Before (UB), Update_After (UA). There's no NDU problem in an insert only changelog pipeline. For updates, there are three main NDU problems:
1. Non-deterministic functions, include scalar, table, aggregate functions, both builtin and custom ones.
2. LookupJoin on an evolving source
3. Cdc-source carries metadata fields which are system columns, not belongs to the entity data itself.
For the first step, the optimizer automatically enables the materialization for No.2(LookupJoin) if needed, and gives the detailed error message for No.1(Non-deterministic functions) and No.3(Cdc-source with metadata) which is relatively easier to solve by changing the SQL.
Default value is \`IGNORE\`, the optimizer does no changes.
Possible values:"TRY_RESOLVE""IGNORE" table.optimizer.reuse-source-enabled
Batch Streaming true Boolean When it is true, the optimizer will try to find out duplicated table sources and reuse them. This works only when table.optimizer.reuse-sub-plan-enabled is true. table.optimizer.reuse-sub-plan-enabled
Batch Streaming true Boolean When it is true, the optimizer will try to find out duplicated sub-plans and reuse them. table.optimizer.source.aggregate-pushdown-enabled
Batch true Boolean When it is true, the optimizer will push down the local aggregates into the TableSource which implements SupportsAggregatePushDown. table.optimizer.source.predicate-pushdown-enabled
Batch Streaming true Boolean When it is true, the optimizer will push down predicates into the FilterableTableSource. Default value is true. table.optimizer.source.report-statistics-enabled
Batch Streaming true Boolean When it is true, the optimizer will collect and use the statistics from source connectors if the source extends from SupportsStatisticReport and the statistics from catalog is UNKNOWN.Default value is true. Planner 配置 # 以下配置可以用于调整 planner 的行为。
Key Default Type Description table.builtin-catalog-name
Batch Streaming "default_catalog" String The name of the initial catalog to be created when instantiating a TableEnvironment. table.builtin-database-name
Batch Streaming "default_database" String The name of the default database in the initial catalog to be created when instantiating TableEnvironment. table.dml-sync
Batch Streaming false Boolean Specifies if the DML job (i.e. the insert operation) is executed asynchronously or synchronously. By default, the execution is async, so you can submit multiple DML jobs at the same time. If set this option to true, the insert operation will wait for the job to finish. table.dynamic-table-options.enabled
Batch Streaming true Boolean Enable or disable the OPTIONS hint used to specify table options dynamically, if disabled, an exception would be thrown if any OPTIONS hint is specified table.generated-code.max-length
Batch Streaming 4000 Integer Specifies a threshold where generated code will be split into sub-function calls. Java has a maximum method length of 64 KB. This setting allows for finer granularity if necessary. Default value is 4000 instead of 64KB as by default JIT refuses to work on methods with more than 8K byte code. table.local-time-zone
Batch Streaming "default" String The local time zone defines current session time zone id. It is used when converting to/from \u0026lt;code\u0026gt;TIMESTAMP WITH LOCAL TIME ZONE\u0026lt;/code\u0026gt;. Internally, timestamps with local time zone are always represented in the UTC time zone. However, when converting to data types that don't include a time zone (e.g. TIMESTAMP, TIME, or simply STRING), the session time zone is used during conversion. The input of option is either a full name such as "America/Los_Angeles", or a custom timezone id such as "GMT-08:00". table.plan.compile.catalog-objects
Batch Streaming ALL Enum
Strategy how to persist catalog objects such as tables, functions, or data types into a plan during compilation.
It influences the need for catalog metadata to be present during a restore operation and affects the plan size.
This configuration option does not affect anonymous/inline or temporary objects. Anonymous/inline objects will be persisted entirely (including schema and options) if possible or fail the compilation otherwise. Temporary objects will be persisted only by their identifier and the object needs to be present in the session context during a restore.
Possible values:"ALL": All metadata about catalog tables, functions, or data types will be persisted into the plan during compilation. For catalog tables, this includes the table's identifier, schema, and options. For catalog functions, this includes the function's identifier and class. For catalog data types, this includes the identifier and entire type structure. With this strategy, the catalog's metadata doesn't have to be available anymore during a restore operation."SCHEMA": In addition to an identifier, schema information about catalog tables, functions, or data types will be persisted into the plan during compilation. A schema allows for detecting incompatible changes in the catalog during a plan restore operation. However, all other metadata will still be retrieved from the catalog."IDENTIFIER": Only the identifier of catalog tables, functions, or data types will be persisted into the plan during compilation. All metadata will be retrieved from the catalog during a restore operation. With this strategy, plans become less verbose. table.plan.force-recompile
Streaming false Boolean When false COMPILE PLAN statement will fail if the output plan file is already existing, unless the clause IF NOT EXISTS is used. When true COMPILE PLAN will overwrite the existing output plan file. We strongly suggest to enable this flag only for debugging purpose. table.plan.restore.catalog-objects
Batch Streaming ALL Enum
Strategy how to restore catalog objects such as tables, functions, or data types using a given plan and performing catalog lookups if necessary. It influences the need for catalog metadata to bepresent and enables partial enrichment of plan information.
Possible values:"ALL": Reads all metadata about catalog tables, functions, or data types that has been persisted in the plan. The strategy performs a catalog lookup by identifier to fill in missing information or enrich mutable options. If the original object is not available in the catalog anymore, pipelines can still be restored if all information necessary is contained in the plan."ALL_ENFORCED": Requires that all metadata about catalog tables, functions, or data types has been persisted in the plan. The strategy will neither perform a catalog lookup by identifier nor enrich mutable options with catalog information. A restore will fail if not all information necessary is contained in the plan."IDENTIFIER": Uses only the identifier of catalog tables, functions, or data types and always performs a catalog lookup. A restore will fail if the original object is not available in the catalog anymore. Additional metadata that might be contained in the plan will be ignored. table.resources.download-dir
Batch Streaming System.getProperty("java.io.tmpdir") String Local directory that is used by planner for storing downloaded resources. table.sql-dialect
Batch Streaming "default" String The SQL dialect defines how to parse a SQL query. A different SQL dialect may support different SQL grammar. Currently supported dialects are: default and hive SQL Client 配置 # 以下配置可以用于调整 sql client 的行为。
Key Default Type Description sql-client.display.max-column-width
Streaming 30 Integer When printing the query results, this parameter determines the number of characters shown on screen before truncating.This only applies to columns with variable-length types (e.g. STRING) in streaming mode.Fixed-length types and all types in batch mode are printed using a deterministic column width sql-client.execution.max-table-result.rows
Batch Streaming 1000000 Integer The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style. sql-client.execution.result-mode
Batch Streaming TABLE Enum
Determines how the query result should be displayed.
Possible values:"TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation."CHANGELOG": Visualizes the result stream that is produced by a continuous query."TABLEAU": Display results in the screen directly in a tableau format. sql-client.verbose
Batch Streaming false Boolean Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause. `}),e.add({id:301,href:"/flink/flink-docs-master/zh/docs/dev/python/table/metrics/",title:"指标",section:"Table API",content:` 指标 # PyFlink 支持指标系统，该指标系统允许收集指标并将其暴露给外部系统。
注册指标 # 您可以通过在Python 用户自定义函数的 open 方法中调用 function_context.get_metric_group() 来访问指标系统。 get_metric_group() 方法返回一个 MetricGroup 对象，您可以在该对象上创建和注册新指标。
指标类型 # PyFlink 支持计数器 Counters，量表 Gauges ，分布 Distribution 和仪表 Meters。
计数器 Counter # Counter 用于计算某个东西的出现次数。可以通过 inc()/inc(n: int) 或 dec()/dec(n: int) 增加或减少当前值。 您可以通过在 MetricGroup 上调用 counter(name: str) 来创建和注册 Counter。
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.counter = None def open(self, function_context): self.counter = function_context.get_metric_group().counter(\u0026#34;my_counter\u0026#34;) def eval(self, i): self.counter.inc(i) return i 量表 # Gauge 可按需返回数值。您可以通过在 MetricGroup 上调用 gauge(name: str, obj: Callable[[], int]) 来注册一个量表。Callable 对象将用于汇报数值。量表指标(Gauge metrics)只能用于汇报整数值。
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.length = 0 def open(self, function_context): function_context.get_metric_group().gauge(\u0026#34;my_gauge\u0026#34;, lambda : self.length) def eval(self, i): self.length = i return i - 1 分布（Distribution） # Distribution 用于报告关于所报告值分布的信息（总和，计数，最小，最大和平均值）的指标。可以通过 update(n: int) 来更新当前值。您可以通过在 MetricGroup 上调用 distribution(name: str) 来注册该指标。分布指标(Distribution metrics)只能用于汇报整数指标。
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.distribution = None def open(self, function_context): self.distribution = function_context.get_metric_group().distribution(\u0026#34;my_distribution\u0026#34;) def eval(self, i): self.distribution.update(i) return i - 1 仪表 # 仪表用于汇报平均吞吐量。可以使用 mark_event() 函数来注册事件的发生，使用 mark_event(n: int) 函数来注册同时发生的多个事件。 您可以通过在 MetricGroup 上调用 meter(self, name: str, time_span_in_seconds: int = 60) 来注册仪表。time_span_in_seconds的默认值为60。
Python from pyflink.table.udf import ScalarFunction class MyUDF(ScalarFunction): def __init__(self): self.meter = None def open(self, function_context): # 120秒内统计的平均每秒事件数，默认是60秒 self.meter = function_context.get_metric_group().meter(\u0026#34;my_meter\u0026#34;, time_span_in_seconds=120) def eval(self, i): self.meter.mark_event(i) return i - 1 范围（Scope） # 您可以参考 Java 指标文档以获取有关范围定义的更多详细信息。
用户范围（User Scope） # 您可以通过调用 MetricGroup.add_group(key: str, value: str = None) 来定义用户范围。如果 value 不为 None，则创建一个新的键值 MetricGroup对。 其中，键组被添加到该组的子组中，而值组又被添加到键组的子组中。在这种情况下，值组将作为结果返回，与此同时，创建一个用户变量。
Python function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) 系统范围（System Scope） # 您可以参考 Java 指标文档以获取有关系统范围的更多详细信息。
所有变量列表 # 您可以参考 Java 指标文档以获取有关“所有变量列表”的更多详细信息。
用户变量（User Variables） # 您可以通过调用 MetricGroup.addGroup(key: str, value: str = None) 并指定 value 参数来定义用户变量。
**重要提示：**用户变量不能在以 scope format 中使用。
Python function_context \\ .get_metric_group() \\ .add_group(\u0026#34;my_metrics_key\u0026#34;, \u0026#34;my_metrics_value\u0026#34;) \\ .counter(\u0026#34;my_counter\u0026#34;) PyFlink 和 Flink 的共通部分 # 您可以参考 Java 的指标文档，以获取关于以下部分的更多详细信息：
Reporter 。 系统指标 。 延迟跟踪 。 REST API 集成 。 仪表板集成 。 Back to top
`}),e.add({id:302,href:"/flink/flink-docs-master/zh/docs/dev/python/python_config/",title:"配置",section:"Python API",content:` 配置 # Depending on the requirements of a Python API program, it might be necessary to adjust certain parameters for optimization.
For Python DataStream API program, the config options could be set as following:
from pyflink.common import Configuration from pyflink.datastream import StreamExecutionEnvironment config = Configuration() config.set_integer(\u0026#34;python.fn-execution.bundle.size\u0026#34;, 1000) env = StreamExecutionEnvironment.get_execution_environment(config) For Python Table API program, all the config options available for Java/Scala Table API program could also be used in the Python Table API program. You could refer to the Table API Configuration for more details on all the available config options for Table API programs. The config options could be set as following in a Table API program:
from pyflink.table import TableEnvironment, EnvironmentSettings env_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_settings) t_env.get_config().set(\u0026#34;python.fn-execution.bundle.size\u0026#34;, \u0026#34;1000\u0026#34;) The config options could also be set when creating EnvironmentSettings:
from pyflink.common import Configuration from pyflink.table import TableEnvironment, EnvironmentSettings # create a streaming TableEnvironment config = Configuration() config.set_string(\u0026#34;python.fn-execution.bundle.size\u0026#34;, \u0026#34;1000\u0026#34;) env_settings = EnvironmentSettings \\ .new_instance() \\ .in_streaming_mode() \\ .with_configuration(config) \\ .build() table_env = TableEnvironment.create(env_settings) # or directly pass config into create method table_env = TableEnvironment.create(config) Python Options # Key Default Type Description python.archives (none) String Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory is specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF. The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). The option is equivalent to the command line option "-pyarch". python.client.executable "python" String The path of the Python interpreter used to launch the Python process when submitting the Python jobs via "flink run" or compiling the Java/Scala jobs containing Python UDFs. Equivalent to the command line option "-pyclientexec" or the environment variable PYFLINK_CLIENT_EXECUTABLE. The priority is as following: 1. the configuration 'python.client.executable' defined in the source code(Only used in Flink Java SQL/Table API job call Python UDF);
2. the command line option "-pyclientexec";
3. the configuration 'python.client.executable' defined in flink-conf.yaml
4. the environment variable PYFLINK_CLIENT_EXECUTABLE; python.executable "python" String Specify the path of the python interpreter used to execute the python UDF worker. The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version \u0026gt;= 20.3) and SetupTools (version \u0026gt;= 37.0.0). Please ensure that the specified environment meets the above requirements. The option is equivalent to the command line option "-pyexec". python.execution-mode "process" String Specify the python runtime execution mode. The optional values are \`process\` and \`thread\`. The \`process\` mode means that the Python user-defined functions will be executed in separate Python process. The \`thread\` mode means that the Python user-defined functions will be executed in the same process of the Java operator. Note that currently it still doesn't support to execute Python user-defined functions in \`thread\` mode in all places. It will fall back to \`process\` mode in these cases. python.files (none) String Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (',') could be used as the separator to specify multiple files. The option is equivalent to the command line option "-pyfs". python.fn-execution.arrow.batch.size 10000 Integer The maximum number of elements to include in an arrow batch for Python user-defined function execution. The arrow batch size should not exceed the bundle size. Otherwise, the bundle size will be used as the arrow batch size. python.fn-execution.bundle.size 100000 Integer The maximum number of elements to include in a bundle for Python user-defined function execution. The elements are processed asynchronously. One bundle of elements are processed before processing the next bundle of elements. A larger value can improve the throughput, but at the cost of more memory usage and higher latency. python.fn-execution.bundle.time 1000 Long Sets the waiting timeout(in milliseconds) before processing a bundle for Python user-defined function execution. The timeout defines how long the elements of a bundle will be buffered before being processed. Lower timeouts lead to lower tail latencies, but may affect throughput. python.fn-execution.memory.managed true Boolean If set, the Python worker will configure itself to use the managed memory budget of the task slot. Otherwise, it will use the Off-Heap Memory of the task slot. In this case, users should set the Task Off-Heap Memory using the configuration key taskmanager.memory.task.off-heap.size. python.map-state.iterate-response-batch-size 1000 Integer The maximum number of the MapState keys/entries sent to Python UDF worker in each batch when iterating a Python MapState. Note that this is an experimental flag and might not be available in future releases. python.map-state.read-cache-size 1000 Integer The maximum number of cached entries for a single Python MapState. Note that this is an experimental flag and might not be available in future releases. python.map-state.write-cache-size 1000 Integer The maximum number of cached write requests for a single Python MapState. The write requests will be flushed to the state backend (managed in the Java operator) when the number of cached write requests exceed this limit. Note that this is an experimental flag and might not be available in future releases. python.metric.enabled true Boolean When it is false, metric for Python will be disabled. You can disable the metric to achieve better performance at some circumstance. python.operator-chaining.enabled true Boolean Python operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization. python.profile.enabled false Boolean Specifies whether to enable Python worker profiling. The profile result will be displayed in the log file of the TaskManager periodically. The interval between each profiling is determined by the config options python.fn-execution.bundle.size and python.fn-execution.bundle.time. python.requirements (none) String Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists. The option is equivalent to the command line option "-pyreq". python.state.cache-size 1000 Integer The maximum number of states cached in a Python UDF worker. Note that this is an experimental flag and might not be available in future releases. `}),e.add({id:303,href:"/flink/flink-docs-master/zh/docs/dev/python/debugging/",title:"调试",section:"Python API",content:` 调试 # 本页介绍如何在PyFlink进行调试
打印日志信息 # 客户端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 PyFlink 作业中，Python UDF 之外的地方打印上下文和调试信息。 在提交作业时，日志信息会打印在客户端的日志文件中。
from pyflink.table import EnvironmentSettings, TableEnvironment # 创建 TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.create(env_settings) table = table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) # 使用 logging 模块 import logging logging.warning(table.get_schema()) # 使用 print 函数 print(table.get_schema()) 注意: 客户端缺省的日志级别是 WARNING，因此，只有日志级别在 WARNING 及以上的日志信息才会打印在客户端的日志文件中。
服务器端日志 # 你可以通过 print 或者标准的 Python logging 模块，在 Python UDF 中打印上下文和调试信息。 在作业运行的过程中，日志信息会打印在 TaskManager 的日志文件中。
from pyflink.table import DataTypes from pyflink.table.udf import udf import logging @udf(result_type=DataTypes.BIGINT()) def add(i, j): # 使用 logging 模块 logging.info(\u0026#34;debug\u0026#34;) # 使用 print 函数 print(\u0026#39;debug\u0026#39;) return i + j 注意: 服务器端缺省的日志级别是 INFO，因此，只有日志级别在 INFO 及以上的日志信息才会打印在 TaskManager 的日志文件中。
查看日志 # 如果设置了环境变量FLINK_HOME，日志将会放置在FLINK_HOME指向目录的log目录之下。否则，日志将会放在安装的Pyflink模块的 log目录下。你可以通过执行下面的命令来查找PyFlink模块的log目录的路径：
\$ python -c \u0026#34;import pyflink;import os;print(os.path.dirname(os.path.abspath(pyflink.__file__))+\u0026#39;/log\u0026#39;)\u0026#34; 调试Python UDFs # 本地调试 # 你可以直接在 PyCharm 等 IDE 调试你的 Python 函数。
远程调试 # 你可以利用PyCharm提供的pydevd_pycharm工具进行Python UDF的调试
在PyCharm里创建一个Python Remote Debug
run -\u0026gt; Python Remote Debug -\u0026gt; + -\u0026gt; 选择一个port (e.g. 6789)
安装pydevd-pycharm工具
\$ pip install pydevd-pycharm 在你的Python UDF里面添加如下的代码
import pydevd_pycharm pydevd_pycharm.settrace(\u0026#39;localhost\u0026#39;, port=6789, stdoutToServer=True, stderrToServer=True) 启动刚刚创建的Python Remote Dubug Server
运行你的Python代码
Profiling Python UDFs # 你可以打开profile来分析性能瓶颈
t_env.get_config().set(\u0026#34;python.profile.enabled\u0026#34;, \u0026#34;true\u0026#34;) 你可以在日志里面查看profile的结果
`}),e.add({id:304,href:"/flink/flink-docs-master/zh/docs/dev/table/sourcessinks/",title:"User-defined Sources \u0026 Sinks",section:"Table API \u0026 SQL",content:` User-defined Sources \u0026amp; Sinks # Dynamic tables are the core concept of Flink\u0026rsquo;s Table \u0026amp; SQL API for processing both bounded and unbounded data in a unified fashion.
Because dynamic tables are only a logical concept, Flink does not own the data itself. Instead, the content of a dynamic table is stored in external systems (such as databases, key-value stores, message queues) or files.
Dynamic sources and dynamic sinks can be used to read and write data from and to an external system. In the documentation, sources and sinks are often summarized under the term connector.
Flink provides pre-defined connectors for Kafka, Hive, and different file systems. See the connector section for more information about built-in table sources and sinks.
This page focuses on how to develop a custom, user-defined connector.
Overview # In many cases, implementers don\u0026rsquo;t need to create a new connector from scratch but would like to slightly modify existing connectors or hook into the existing stack. In other cases, implementers would like to create specialized connectors.
This section helps for both kinds of use cases. It explains the general architecture of table connectors from pure declaration in the API to runtime code that will be executed on the cluster.
The filled arrows show how objects are transformed to other objects from one stage to the next stage during the translation process.
Metadata # Both Table API and SQL are declarative APIs. This includes the declaration of tables. Thus, executing a CREATE TABLE statement results in updated metadata in the target catalog.
For most catalog implementations, physical data in the external system is not modified for such an operation. Connector-specific dependencies don\u0026rsquo;t have to be present in the classpath yet. The options declared in the WITH clause are neither validated nor otherwise interpreted.
The metadata for dynamic tables (created via DDL or provided by the catalog) is represented as instances of CatalogTable. A table name will be resolved into a CatalogTable internally when necessary.
Planning # When it comes to planning and optimization of the table program, a CatalogTable needs to be resolved into a DynamicTableSource (for reading in a SELECT query) and DynamicTableSink (for writing in an INSERT INTO statement).
DynamicTableSourceFactory and DynamicTableSinkFactory provide connector-specific logic for translating the metadata of a CatalogTable into instances of DynamicTableSource and DynamicTableSink. In most of the cases, a factory\u0026rsquo;s purpose is to validate options (such as 'port' = '5022' in the example), configure encoding/decoding formats (if required), and create a parameterized instance of the table connector.
By default, instances of DynamicTableSourceFactory and DynamicTableSinkFactory are discovered using Java\u0026rsquo;s Service Provider Interfaces (SPI). The connector option (such as 'connector' = 'custom' in the example) must correspond to a valid factory identifier.
Although it might not be apparent in the class naming, DynamicTableSource and DynamicTableSink can also be seen as stateful factories that eventually produce concrete runtime implementation for reading/writing the actual data.
The planner uses the source and sink instances to perform connector-specific bidirectional communication until an optimal logical plan could be found. Depending on the optionally declared ability interfaces (e.g. SupportsProjectionPushDown or SupportsOverwrite), the planner might apply changes to an instance and thus mutate the produced runtime implementation.
Runtime # Once the logical planning is complete, the planner will obtain the runtime implementation from the table connector. Runtime logic is implemented in Flink\u0026rsquo;s core connector interfaces such as InputFormat or SourceFunction.
Those interfaces are grouped by another level of abstraction as subclasses of ScanRuntimeProvider, LookupRuntimeProvider, and SinkRuntimeProvider.
For example, both OutputFormatProvider (providing org.apache.flink.api.common.io.OutputFormat) and SinkFunctionProvider (providing org.apache.flink.streaming.api.functions.sink.SinkFunction) are concrete instances of SinkRuntimeProvider that the planner can handle.
Back to top
Project Configuration # If you want to implement a custom connector or a custom format, the following dependency is usually sufficient:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-common\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gtprovided\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. runtime "org.apache.flink:flink-table-common:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. If you want to develop a connector that needs to bridge with DataStream APIs (i.e. if you want to adapt a DataStream connector to the Table API), you need to add this dependency:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. \u0026ltdependency\u0026gt \u0026ltgroupId\u0026gtorg.apache.flink\u0026lt/groupId\u0026gt \u0026ltartifactId\u0026gtflink-table-api-java-bridge\u0026lt/artifactId\u0026gt \u0026ltversion\u0026gt1.16-SNAPSHOT\u0026lt/version\u0026gt \u0026ltscope\u0026gtprovided\u0026lt/scope\u0026gt \u0026lt/dependency\u0026gt Copied to clipboard! Check out Project configuration for more details. Gradle Open the build.gradle file in your project directory and add the following in the dependencies block. runtime "org.apache.flink:flink-table-api-java-bridge:1.16-SNAPSHOT" Copied to clipboard! Note: This assumes that you have created your project using our Gradle build script or quickstart script. Check out Project configuration for more details. When developing the connector/format, we suggest shipping both a thin JAR and an uber JAR, so users can easily load the uber JAR in the SQL client or in the Flink distribution and start using it. The uber JAR should include all the third-party dependencies of the connector, excluding the table dependencies listed above.
You should not depend on flink-table-planner_2.12 in production code. With the new module flink-table-planner-loader introduced in Flink 1.15, the application\u0026rsquo;s classpath will not have direct access to org.apache.flink.table.planner classes anymore. If you need a feature available only internally within the org.apache.flink.table.planner package and subpackages, please open an issue. To learn more, check out Anatomy of Table Dependencies. Extension Points # This section explains the available interfaces for extending Flink\u0026rsquo;s table connectors.
Dynamic Table Factories # Dynamic table factories are used to configure a dynamic table connector for an external storage system from catalog and session information.
org.apache.flink.table.factories.DynamicTableSourceFactory can be implemented to construct a DynamicTableSource.
org.apache.flink.table.factories.DynamicTableSinkFactory can be implemented to construct a DynamicTableSink.
By default, the factory is discovered using the value of the connector option as the factory identifier and Java\u0026rsquo;s Service Provider Interface.
In JAR files, references to new implementations can be added to the service file:
META-INF/services/org.apache.flink.table.factories.Factory
The framework will check for a single matching factory that is uniquely identified by factory identifier and requested base class (e.g. DynamicTableSourceFactory).
The factory discovery process can be bypassed by the catalog implementation if necessary. For this, a catalog needs to return an instance that implements the requested base class in org.apache.flink.table.catalog.Catalog#getFactory.
Dynamic Table Source # By definition, a dynamic table can change over time.
When reading a dynamic table, the content can either be considered as:
A changelog (finite or infinite) for which all changes are consumed continuously until the changelog is exhausted. This is represented by the ScanTableSource interface. A continuously changing or very large external table whose content is usually never read entirely but queried for individual values when necessary. This is represented by the LookupTableSource interface. A class can implement both of these interfaces at the same time. The planner decides about their usage depending on the specified query.
Scan Table Source # A ScanTableSource scans all rows from an external storage system during runtime.
The scanned rows don\u0026rsquo;t have to contain only insertions but can also contain updates and deletions. Thus, the table source can be used to read a (finite or infinite) changelog. The returned changelog mode indicates the set of changes that the planner can expect during runtime.
For regular batch scenarios, the source can emit a bounded stream of insert-only rows.
For regular streaming scenarios, the source can emit an unbounded stream of insert-only rows.
For change data capture (CDC) scenarios, the source can emit bounded or unbounded streams with insert, update, and delete rows.
A table source can implement further ability interfaces such as SupportsProjectionPushDown that might mutate an instance during planning. All abilities can be found in the org.apache.flink.table.connector.source.abilities package and are listed in the source abilities table.
The runtime implementation of a ScanTableSource must produce internal data structures. Thus, records must be emitted as org.apache.flink.table.data.RowData. The framework provides runtime converters such that a source can still work on common data structures and perform a conversion at the end.
Lookup Table Source # A LookupTableSource looks up rows of an external storage system by one or more keys during runtime.
Compared to ScanTableSource, the source does not have to read the entire table and can lazily fetch individual values from a (possibly continuously changing) external table when necessary.
Compared to ScanTableSource, a LookupTableSource does only support emitting insert-only changes currently.
Further abilities are not supported. See the documentation of org.apache.flink.table.connector.source.LookupTableSource for more information.
The runtime implementation of a LookupTableSource is a TableFunction or AsyncTableFunction. The function will be called with values for the given lookup keys during runtime.
Source Abilities # Interface Description SupportsFilterPushDown Enables to push down the filter into the DynamicTableSource. For efficiency, a source can push filters further down in order to be close to the actual data generation. SupportsLimitPushDown Enables to push down a limit (the expected maximum number of produced records) into a DynamicTableSource. SupportsPartitionPushDown Enables to pass available partitions to the planner and push down partitions into a DynamicTableSource. During the runtime, the source will only read data from the passed partition list for efficiency. SupportsProjectionPushDown Enables to push down a (possibly nested) projection into a DynamicTableSource. For efficiency, a source can push a projection further down in order to be close to the actual data generation. If the source also implements SupportsReadingMetadata, the source will also read the required metadata only. SupportsReadingMetadata Enables to read metadata columns from a DynamicTableSource. The source is responsible to add the required metadata at the end of the produced rows. This includes potentially forwarding metadata column from contained formats. SupportsWatermarkPushDown Enables to push down a watermark strategy into a DynamicTableSource. The watermark strategy is a builder/factory for timestamp extraction and watermark generation. During the runtime, the watermark generator is located inside the source and is able to generate per-partition watermarks. SupportsSourceWatermark Enables to fully rely on the watermark strategy provided by the ScanTableSource itself. Thus, a CREATE TABLE DDL is able to use SOURCE_WATERMARK() which is a built-in marker function that will be detected by the planner and translated into a call to this interface if available. Attention The interfaces above are currently only available for ScanTableSource, not for LookupTableSource.
Dynamic Table Sink # By definition, a dynamic table can change over time.
When writing a dynamic table, the content can always be considered as a changelog (finite or infinite) for which all changes are written out continuously until the changelog is exhausted. The returned changelog mode indicates the set of changes that the sink accepts during runtime.
For regular batch scenarios, the sink can solely accept insert-only rows and write out bounded streams.
For regular streaming scenarios, the sink can solely accept insert-only rows and can write out unbounded streams.
For change data capture (CDC) scenarios, the sink can write out bounded or unbounded streams with insert, update, and delete rows.
A table sink can implement further ability interfaces such as SupportsOverwrite that might mutate an instance during planning. All abilities can be found in the org.apache.flink.table.connector.sink.abilities package and are listed in the sink abilities table.
The runtime implementation of a DynamicTableSink must consume internal data structures. Thus, records must be accepted as org.apache.flink.table.data.RowData. The framework provides runtime converters such that a sink can still work on common data structures and perform a conversion at the beginning.
Sink Abilities # Interface Description SupportsOverwrite Enables to overwrite existing data in a DynamicTableSink. By default, if this interface is not implemented, existing tables or partitions cannot be overwritten using e.g. the SQL INSERT OVERWRITE clause. SupportsPartitioning Enables to write partitioned data in a DynamicTableSink. SupportsWritingMetadata Enables to write metadata columns into a DynamicTableSource. A table sink is responsible for accepting requested metadata columns at the end of consumed rows and persist them. This includes potentially forwarding metadata columns to contained formats. Encoding / Decoding Formats # Some table connectors accept different formats that encode and decode keys and/or values.
Formats work similar to the pattern DynamicTableSourceFactory -\u0026gt; DynamicTableSource -\u0026gt; ScanRuntimeProvider, where the factory is responsible for translating options and the source is responsible for creating runtime logic.
Because formats might be located in different modules, they are discovered using Java\u0026rsquo;s Service Provider Interface similar to table factories. In order to discover a format factory, the dynamic table factory searches for a factory that corresponds to a factory identifier and connector-specific base class.
For example, the Kafka table source requires a DeserializationSchema as runtime interface for a decoding format. Therefore, the Kafka table source factory uses the value of the value.format option to discover a DeserializationFormatFactory.
The following format factories are currently supported:
org.apache.flink.table.factories.DeserializationFormatFactory org.apache.flink.table.factories.SerializationFormatFactory The format factory translates the options into an EncodingFormat or a DecodingFormat. Those interfaces are another kind of factory that produce specialized format runtime logic for the given data type.
For example, for a Kafka table source factory, the DeserializationFormatFactory would return an EncodingFormat\u0026lt;DeserializationSchema\u0026gt; that can be passed into the Kafka table source.
Back to top
Full Stack Example # This section sketches how to implement a scan table source with a decoding format that supports changelog semantics. The example illustrates how all of the mentioned components play together. It can serve as a reference implementation.
In particular, it shows how to
create factories that parse and validate options, implement table connectors, implement and discover custom formats, and use provided utilities such as data structure converters and the FactoryUtil. The table source uses a simple single-threaded SourceFunction to open a socket that listens for incoming bytes. The raw bytes are decoded into rows by a pluggable format. The format expects a changelog flag as the first column.
We will use most of the interfaces mentioned above to enable the following DDL:
CREATE TABLE UserScores (name STRING, score INT) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;socket\u0026#39;, \u0026#39;hostname\u0026#39; = \u0026#39;localhost\u0026#39;, \u0026#39;port\u0026#39; = \u0026#39;9999\u0026#39;, \u0026#39;byte-delimiter\u0026#39; = \u0026#39;10\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;changelog-csv\u0026#39;, \u0026#39;changelog-csv.column-delimiter\u0026#39; = \u0026#39;|\u0026#39; ); Because the format supports changelog semantics, we are able to ingest updates during runtime and create an updating view that can continuously evaluate changing data:
SELECT name, SUM(score) FROM UserScores GROUP BY name; Use the following command to ingest data in a terminal:
\u0026gt; nc -lk 9999 INSERT|Alice|12 INSERT|Bob|5 DELETE|Alice|12 INSERT|Alice|18 Factories # This section illustrates how to translate metadata coming from the catalog to concrete connector instances.
Both factories have been added to the META-INF/services directory.
SocketDynamicTableFactory
The SocketDynamicTableFactory translates the catalog table to a table source. Because the table source requires a decoding format, we are discovering the format using the provided FactoryUtil for convenience.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableSourceFactory; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.types.DataType; public class SocketDynamicTableFactory implements DynamicTableSourceFactory { // define all options statically public static final ConfigOption\u0026lt;String\u0026gt; HOSTNAME = ConfigOptions.key(\u0026#34;hostname\u0026#34;) .stringType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; PORT = ConfigOptions.key(\u0026#34;port\u0026#34;) .intType() .noDefaultValue(); public static final ConfigOption\u0026lt;Integer\u0026gt; BYTE_DELIMITER = ConfigOptions.key(\u0026#34;byte-delimiter\u0026#34;) .intType() .defaultValue(10); // corresponds to \u0026#39;\\n\u0026#39; @Override public String factoryIdentifier() { return \u0026#34;socket\u0026#34;; // used for matching to \`connector = \u0026#39;...\u0026#39;\` } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(HOSTNAME); options.add(PORT); options.add(FactoryUtil.FORMAT); // use pre-defined option for format return options; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(BYTE_DELIMITER); return options; } @Override public DynamicTableSource createDynamicTableSource(Context context) { // either implement your custom validation logic here ... // or use the provided helper utility final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context); // discover a suitable decoding format final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat = helper.discoverDecodingFormat( DeserializationFormatFactory.class, FactoryUtil.FORMAT); // validate all options helper.validate(); // get the validated options final ReadableConfig options = helper.getOptions(); final String hostname = options.get(HOSTNAME); final int port = options.get(PORT); final byte byteDelimiter = (byte) (int) options.get(BYTE_DELIMITER); // derive the produced data type (excluding computed columns) from the catalog table final DataType producedDataType = context.getCatalogTable().getResolvedSchema().toPhysicalRowDataType(); // create and return dynamic table source return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } } ChangelogCsvFormatFactory
The ChangelogCsvFormatFactory translates format-specific options to a format. The FactoryUtil in SocketDynamicTableFactory takes care of adapting the option keys accordingly and handles the prefixing like changelog-csv.column-delimiter.
Because this factory implements DeserializationFormatFactory, it could also be used for other connectors that support deserialization formats such as the Kafka connector.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.configuration.ConfigOption; import org.apache.flink.configuration.ConfigOptions; import org.apache.flink.configuration.ReadableConfig; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.data.RowData; import org.apache.flink.table.factories.FactoryUtil; import org.apache.flink.table.factories.DeserializationFormatFactory; import org.apache.flink.table.factories.DynamicTableFactory; public class ChangelogCsvFormatFactory implements DeserializationFormatFactory { // define all options statically public static final ConfigOption\u0026lt;String\u0026gt; COLUMN_DELIMITER = ConfigOptions.key(\u0026#34;column-delimiter\u0026#34;) .stringType() .defaultValue(\u0026#34;|\u0026#34;); @Override public String factoryIdentifier() { return \u0026#34;changelog-csv\u0026#34;; } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; requiredOptions() { return Collections.emptySet(); } @Override public Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; optionalOptions() { final Set\u0026lt;ConfigOption\u0026lt;?\u0026gt;\u0026gt; options = new HashSet\u0026lt;\u0026gt;(); options.add(COLUMN_DELIMITER); return options; } @Override public DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; createDecodingFormat( DynamicTableFactory.Context context, ReadableConfig formatOptions) { // either implement your custom validation logic here ... // or use the provided helper method FactoryUtil.validateFactoryOptions(this, formatOptions); // get the validated options final String columnDelimiter = formatOptions.get(COLUMN_DELIMITER); // create and return the format return new ChangelogCsvFormat(columnDelimiter); } } Table Source and Decoding Format # This section illustrates how to translate from instances of the planning layer to runtime instances that are shipped to the cluster.
SocketDynamicTableSource
The SocketDynamicTableSource is used during planning. In our example, we don\u0026rsquo;t implement any of the available ability interfaces. Therefore, the main logic can be found in getScanRuntimeProvider(...) where we instantiate the required SourceFunction and its DeserializationSchema for runtime. Both instances are parameterized to return internal data structures (i.e. RowData).
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.ScanTableSource; import org.apache.flink.table.connector.source.SourceFunctionProvider; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; public class SocketDynamicTableSource implements ScanTableSource { private final String hostname; private final int port; private final byte byteDelimiter; private final DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat; private final DataType producedDataType; public SocketDynamicTableSource( String hostname, int port, byte byteDelimiter, DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; decodingFormat, DataType producedDataType) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.decodingFormat = decodingFormat; this.producedDataType = producedDataType; } @Override public ChangelogMode getChangelogMode() { // in our example the format decides about the changelog mode // but it could also be the source itself return decodingFormat.getChangelogMode(); } @Override public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) { // create runtime classes that are shipped to the cluster final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer = decodingFormat.createRuntimeDecoder( runtimeProviderContext, producedDataType); final SourceFunction\u0026lt;RowData\u0026gt; sourceFunction = new SocketSourceFunction( hostname, port, byteDelimiter, deserializer); return SourceFunctionProvider.of(sourceFunction, false); } @Override public DynamicTableSource copy() { return new SocketDynamicTableSource(hostname, port, byteDelimiter, decodingFormat, producedDataType); } @Override public String asSummaryString() { return \u0026#34;Socket Table Source\u0026#34;; } } ChangelogCsvFormat
The ChangelogCsvFormat is a decoding format that uses a DeserializationSchema during runtime. It supports emitting INSERT and DELETE changes.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.ChangelogMode; import org.apache.flink.table.connector.format.DecodingFormat; import org.apache.flink.table.connector.source.DynamicTableSource; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.DataType; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.types.RowKind; public class ChangelogCsvFormat implements DecodingFormat\u0026lt;DeserializationSchema\u0026lt;RowData\u0026gt;\u0026gt; { private final String columnDelimiter; public ChangelogCsvFormat(String columnDelimiter) { this.columnDelimiter = columnDelimiter; } @Override @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public DeserializationSchema\u0026lt;RowData\u0026gt; createRuntimeDecoder( DynamicTableSource.Context context, DataType producedDataType) { // create type information for the DeserializationSchema final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo = (TypeInformation\u0026lt;RowData\u0026gt;) context.createTypeInformation( producedDataType); // most of the code in DeserializationSchema will not work on internal data structures // create a converter for conversion at the end final DataStructureConverter converter = context.createDataStructureConverter(producedDataType); // use logical types during runtime for parsing final List\u0026lt;LogicalType\u0026gt; parsingTypes = producedDataType.getLogicalType().getChildren(); // create runtime class return new ChangelogCsvDeserializer(parsingTypes, converter, producedTypeInfo, columnDelimiter); } @Override public ChangelogMode getChangelogMode() { // define that this format can produce INSERT and DELETE rows return ChangelogMode.newBuilder() .addContainedKind(RowKind.INSERT) .addContainedKind(RowKind.DELETE) .build(); } } Runtime # For completeness, this section illustrates the runtime logic for both SourceFunction and DeserializationSchema.
ChangelogCsvDeserializer
The ChangelogCsvDeserializer contains a simple parsing logic for converting bytes into Row of Integer and String with a row kind. The final conversion step converts those into internal data structures.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.table.connector.RuntimeConverter.Context; import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter; import org.apache.flink.table.data.RowData; import org.apache.flink.table.types.logical.LogicalType; import org.apache.flink.table.types.logical.LogicalTypeRoot; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class ChangelogCsvDeserializer implements DeserializationSchema\u0026lt;RowData\u0026gt; { private final List\u0026lt;LogicalType\u0026gt; parsingTypes; private final DataStructureConverter converter; private final TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo; private final String columnDelimiter; public ChangelogCsvDeserializer( List\u0026lt;LogicalType\u0026gt; parsingTypes, DataStructureConverter converter, TypeInformation\u0026lt;RowData\u0026gt; producedTypeInfo, String columnDelimiter) { this.parsingTypes = parsingTypes; this.converter = converter; this.producedTypeInfo = producedTypeInfo; this.columnDelimiter = columnDelimiter; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { // return the type information required by Flink\u0026#39;s core interfaces return producedTypeInfo; } @Override public void open(InitializationContext context) { // converters must be open converter.open(Context.create(ChangelogCsvDeserializer.class.getClassLoader())); } @Override public RowData deserialize(byte[] message) { // parse the columns including a changelog flag final String[] columns = new String(message).split(Pattern.quote(columnDelimiter)); final RowKind kind = RowKind.valueOf(columns[0]); final Row row = new Row(kind, parsingTypes.size()); for (int i = 0; i \u0026lt; parsingTypes.size(); i++) { row.setField(i, parse(parsingTypes.get(i).getTypeRoot(), columns[i + 1])); } // convert to internal data structure return (RowData) converter.toInternal(row); } private static Object parse(LogicalTypeRoot root, String value) { switch (root) { case INTEGER: return Integer.parseInt(value); case VARCHAR: return value; default: throw new IllegalArgumentException(); } } @Override public boolean isEndOfStream(RowData nextElement) { return false; } } SocketSourceFunction
The SocketSourceFunction opens a socket and consumes bytes. It splits records by the given byte delimiter (\\n by default) and delegates the decoding to a pluggable DeserializationSchema. The source function can only work with a parallelism of 1.
import org.apache.flink.api.common.serialization.DeserializationSchema; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.api.java.typeutils.ResultTypeQueryable; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.source.RichSourceFunction; import org.apache.flink.table.data.RowData; public class SocketSourceFunction extends RichSourceFunction\u0026lt;RowData\u0026gt; implements ResultTypeQueryable\u0026lt;RowData\u0026gt; { private final String hostname; private final int port; private final byte byteDelimiter; private final DeserializationSchema\u0026lt;RowData\u0026gt; deserializer; private volatile boolean isRunning = true; private Socket currentSocket; public SocketSourceFunction(String hostname, int port, byte byteDelimiter, DeserializationSchema\u0026lt;RowData\u0026gt; deserializer) { this.hostname = hostname; this.port = port; this.byteDelimiter = byteDelimiter; this.deserializer = deserializer; } @Override public TypeInformation\u0026lt;RowData\u0026gt; getProducedType() { return deserializer.getProducedType(); } @Override public void open(Configuration parameters) throws Exception { deserializer.open(() -\u0026gt; getRuntimeContext().getMetricGroup()); } @Override public void run(SourceContext\u0026lt;RowData\u0026gt; ctx) throws Exception { while (isRunning) { // open and consume from socket try (final Socket socket = new Socket()) { currentSocket = socket; socket.connect(new InetSocketAddress(hostname, port), 0); try (InputStream stream = socket.getInputStream()) { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); int b; while ((b = stream.read()) \u0026gt;= 0) { // buffer until delimiter if (b != byteDelimiter) { buffer.write(b); } // decode and emit record else { ctx.collect(deserializer.deserialize(buffer.toByteArray())); buffer.reset(); } } } } catch (Throwable t) { t.printStackTrace(); // print and continue } Thread.sleep(1000); } } @Override public void cancel() { isRunning = false; try { currentSocket.close(); } catch (Throwable t) { // ignore } } } Back to top
`}),e.add({id:305,href:"/flink/flink-docs-master/zh/docs/dev/python/environment_variables/",title:"环境变量",section:"Python API",content:` Environment Variables # These environment variables will affect the behavior of PyFlink:
Environment Variable Description FLINK_HOME PyFlink job will be compiled before submitting and it requires Flink's distribution to compile the job. PyFlink's installation package already contains Flink's distribution and it's used by default. This environment allows you to specify a custom Flink's distribution. PYFLINK_CLIENT_EXECUTABLE The path of the Python interpreter used to launch the Python process when submitting the Python jobs via "flink run" or compiling the Java/Scala jobs containing Python UDFs. Equivalent to the configuration option 'python.client.executable'. The priority is as following: The configuration 'python.client.executable' defined in the source code; The environment variable PYFLINK_CLIENT_EXECUTABLE; The configuration 'python.client.executable' defined in flink-conf.yaml If none of above is set, the default Python interpreter 'python' will be used. `}),e.add({id:306,href:"/flink/flink-docs-master/zh/docs/dev/python/table/python_table_api_connectors/",title:"连接器",section:"Table API",content:` 连接器 # 本篇描述了如何在 PyFlink 中使用连接器，并着重介绍了在 Python 程序中使用 Flink 连接器时需要注意的细节。
Note 想要了解常见的连接器信息和通用配置，请查阅相关的 Java/Scala 文档。
下载连接器（connector）和格式（format）jar 包 # 由于 Flink 是一个基于 Java/Scala 的项目，连接器（connector）和格式（format）的实现是作为 jar 包存在的， 要在 PyFlink 作业中使用，首先需要将其指定为作业的 依赖。
table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/json.jar\u0026#34;) 如何使用连接器 # 在 PyFlink Table API 中，DDL 是定义 source 和 sink 比较推荐的方式，这可以通过 TableEnvironment 中的 execute_sql() 方法来完成，然后就可以在作业中使用这张表了。
source_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE source_table( a VARCHAR, b INT ) WITH ( \u0026#39;connector\u0026#39; =\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;source_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;test_3\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; sink_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table( a VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;sink_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) t_env.execute_sql(sink_ddl) t_env.sql_query(\u0026#34;SELECT a FROM source_table\u0026#34;) \\ .execute_insert(\u0026#34;sink_table\u0026#34;).wait() 下面是如何在 PyFlink 中使用 Kafka source/sink 和 JSON 格式的完整示例。
from pyflink.table import TableEnvironment, EnvironmentSettings def log_processing(): env_settings = EnvironmentSettings.in_streaming_mode() t_env = TableEnvironment.create(env_settings) # specify connector and format jars t_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/json.jar\u0026#34;) source_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE source_table( a VARCHAR, b INT ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;source_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;properties.group.id\u0026#39; = \u0026#39;test_3\u0026#39;, \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; sink_ddl = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE sink_table( a VARCHAR ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;sink_topic\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;kafka:9092\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;json\u0026#39; ) \u0026#34;\u0026#34;\u0026#34; t_env.execute_sql(source_ddl) t_env.execute_sql(sink_ddl) t_env.sql_query(\u0026#34;SELECT a FROM source_table\u0026#34;) \\ .execute_insert(\u0026#34;sink_table\u0026#34;).wait() if __name__ == \u0026#39;__main__\u0026#39;: log_processing() 内置的 Sources 和 Sinks # 有些 source 和 sink 被内置在 Flink 中，可以直接使用。这些内置的 source 包括将 Pandas DataFrame 作为数据源， 或者将一个元素集合作为数据源。内置的 sink 包括将数据转换为 Pandas DataFrame 等。
和 Pandas 之间互转 # PyFlink 表支持与 Pandas DataFrame 之间互相转换。
from pyflink.table.expressions import col import pandas as pd import numpy as np # 创建一个 PyFlink 表 pdf = pd.DataFrame(np.random.rand(1000, 2)) table = t_env.from_pandas(pdf, [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;]).filter(col(\u0026#39;a\u0026#39;) \u0026gt; 0.5) # 将 PyFlink 表转换成 Pandas DataFrame pdf = table.to_pandas() from_elements() # from_elements() 用于从一个元素集合中创建一张表。元素类型必须是可支持的原子类型或者复杂类型。
from pyflink.table import DataTypes table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)]) # 使用第二个参数指定自定义字段名 table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) # 使用第二个参数指定自定义表结构 table_env.from_elements([(1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;)], DataTypes.ROW([DataTypes.FIELD(\u0026#34;a\u0026#34;, DataTypes.INT()), DataTypes.FIELD(\u0026#34;b\u0026#34;, DataTypes.STRING())])) 以上查询返回的表如下:
+----+-------+ | a | b | +====+=======+ | 1 | Hi | +----+-------+ | 2 | Hello | +----+-------+ 用户自定义的 source 和 sink # 在某些情况下，你可能想要自定义 source 或 sink。目前，source 和 sink 必须使用 Java/Scala 实现，你可以定义一个 TableFactory ， 然后通过 DDL 在 PyFlink 作业中来使用它们。更多详情，可查阅 Java/Scala 文档。
`}),e.add({id:307,href:"/flink/flink-docs-master/zh/docs/dev/python/faq/",title:"常见问题",section:"Python API",content:` 常见问题 # 本页介绍了针对PyFlink用户的一些常见问题的解决方案。
准备Python虚拟环境 # 您可以下载[便捷脚本]({% link downloads/setup-pyflink-virtual-env.sh %})，以准备可在Mac OS和大多数Linux发行版上使用的Python虚拟环境包(virtual env zip)。 您可以指定PyFlink的版本，来生成对应的PyFlink版本所需的Python虚拟环境，否则将安装最新版本的PyFlink所对应的Python虚拟环境。
\$ sh setup-pyflink-virtual-env.sh 集群（Cluster） # \$ # 指定Python虚拟环境 \` table_env.add_python_archive(\u0026#34;venv.zip\u0026#34;) \` # 指定用于执行python UDF workers (用户自定义函数工作者) 的python解释器的路径 \` table_env.get_config().set_python_executable(\u0026#34;venv.zip/venv/bin/python\u0026#34;) 如果需要了解add_python_archive和set_python_executable用法的详细信息，请参阅相关文档。
添加Jar文件 # PyFlink作业可能依赖jar文件，比如connector，Java UDF等。 您可以在提交作业时使用以下Python Table API或通过命令行参数来指定依赖项。
# 注意：仅支持本地文件URL（以\u0026#34;file:\u0026#34;开头）。 table_env.get_config().set(\u0026#34;pipeline.jars\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) # 注意：路径必须指定协议（例如：文件——\u0026#34;file\u0026#34;），并且用户应确保在客户端和群集上都可以访问这些URL。 table_env.get_config().set(\u0026#34;pipeline.classpaths\u0026#34;, \u0026#34;file:///my/jar/path/connector.jar;file:///my/jar/path/udf.jar\u0026#34;) 有关添加Java依赖项的API的详细信息，请参阅相关文档。
添加Python文件 # 您可以使用命令行参数pyfs或TableEnvironment的API add_python_file添加python文件依赖，这些依赖可以是python文件，python软件包或本地目录。 例如，如果您有一个名为myDir的目录，该目录具有以下层次结构：
myDir ├──utils ├──__init__.py ├──my_util.py 您可以将添加目录myDir添加到Python依赖中，如下所示：
table_env.add_python_file(\u0026#39;myDir\u0026#39;) def my_udf(): from utils import my_util 当在 mini cluster 环境执行作业时，显式等待作业执行结束 # 当在 mini cluster 环境执行作业（比如，在IDE中执行作业）且在作业中使用了如下API（比如 Python Table API 的 TableEnvironment.execute_sql, StatementSet.execute 和 Python DataStream API 的 StreamExecutionEnvironment.execute_async） 的时候，因为这些API是异步的，请记得显式地等待作业执行结束。否则程序会在已提交的作业执行结束之前退出，以致无法观测到已提交作业的执行结果。 请参考如下示例代码，了解如何显式地等待作业执行结束：
# 异步执行 SQL / Table API 作业 t_result = table_env.execute_sql(...) t_result.wait() # 异步执行 DataStream 作业 job_client = stream_execution_env.execute_async(\u0026#39;My DataStream Job\u0026#39;) job_client.get_job_execution_result().result() 注意: 当往远程集群提交作业时，无需显式地等待作业执行结束，所以当往远程集群提交作业之前，请记得移除这些等待作业执行结束的代码逻辑。
`}),e.add({id:308,href:"/flink/flink-docs-master/zh/docs/dev/datastream/scala_api_extensions/",title:"Scala API 扩展",section:"DataStream API",content:` Scala API 扩展 # 为了在 Scala 和 Java API 之间保持大致相同的使用体验，在批处理和流处理的标准 API 中省略了一些允许 Scala 高级表达的特性。
如果你想拥有完整的 Scala 体验，可以选择通过隐式转换增强 Scala API 的扩展。
要使用所有可用的扩展，你只需为 DataStream API 添加一个简单的引入
import org.apache.flink.streaming.api.scala.extensions._ 或者，您可以引入单个扩展 a-là-carte 来使用您喜欢的扩展。
Accept partial functions # 通常，DataStream API 不接受匿名模式匹配函数来解构元组、case 类或集合，如下所示：
val data: DataStream[(Int, String, Double)] = // [...] data.map { case (id, name, temperature) =\u0026gt; // [...] // The previous line causes the following compilation error: // \u0026#34;The argument types of an anonymous function must be fully known. (SLS 8.5)\u0026#34; } 这个扩展在 DataStream Scala API 中引入了新的方法，这些方法在扩展 API 中具有一对一的对应关系。这些委托方法支持匿名模式匹配函数。
DataStream API # Method Original Example mapWith map (DataStream) data.mapWith { case (_, value) =\u0026gt; value.toString } flatMapWith flatMap (DataStream) data.flatMapWith { case (_, name, visits) =\u0026gt; visits.map(name -\u0026gt; _) } filterWith filter (DataStream) data.filterWith { case Train(_, isOnTime) =\u0026gt; isOnTime } keyingBy keyBy (DataStream) data.keyingBy { case (id, _, _) =\u0026gt; id } mapWith map (ConnectedDataStream) data.mapWith( map1 = case (_, value) =\u0026gt; value.toString, map2 = case (_, _, value, _) =\u0026gt; value + 1 ) flatMapWith flatMap (ConnectedDataStream) data.flatMapWith( flatMap1 = case (_, json) =\u0026gt; parse(json), flatMap2 = case (_, _, json, _) =\u0026gt; parse(json) ) keyingBy keyBy (ConnectedDataStream) data.keyingBy( key1 = case (_, timestamp) =\u0026gt; timestamp, key2 = case (id, _, _) =\u0026gt; id ) reduceWith reduce (KeyedStream, WindowedStream) data.reduceWith { case ((_, sum1), (_, sum2) =\u0026gt; sum1 + sum2 } projecting apply (JoinedStream) data1.join(data2). whereClause(case (pk, _) =\u0026gt; pk). isEqualTo(case (_, fk) =\u0026gt; fk). projecting { case ((pk, tx), (products, fk)) =\u0026gt; tx -\u0026gt; products } 有关每个方法语义的更多信息, 请参考 DataStream API 文档。
要单独使用此扩展，你可以添加以下引入：
import org.apache.flink.api.scala.extensions.acceptPartialFunctions 用于 DataSet 扩展
import org.apache.flink.streaming.api.scala.extensions.acceptPartialFunctions 下面的代码片段展示了如何一起使用这些扩展方法 (以及 DataSet API) 的最小示例:
object Main { import org.apache.flink.streaming.api.scala.extensions._ case class Point(x: Double, y: Double) def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val ds = env.fromElements(Point(1, 2), Point(3, 4), Point(5, 6)) ds.filterWith { case Point(x, _) =\u0026gt; x \u0026gt; 1 }.reduceWith { case (Point(x1, y1), (Point(x2, y2))) =\u0026gt; Point(x1 + y1, x2 + y2) }.mapWith { case Point(x, y) =\u0026gt; (x, y) }.flatMapWith { case (x, y) =\u0026gt; Seq(\u0026#34;x\u0026#34; -\u0026gt; x, \u0026#34;y\u0026#34; -\u0026gt; y) }.keyingBy { case (id, value) =\u0026gt; id } } } Back to top
`}),e.add({id:309,href:"/flink/flink-docs-master/zh/docs/dev/datastream/java_lambdas/",title:"Java Lambda Expressions",section:"DataStream API",content:` Java Lambda 表达式 # Java 8 引入了几种新的语言特性，旨在实现更快、更清晰的编码。作为最重要的特性，即所谓的“Lambda 表达式”，它开启了函数式编程的大门。Lambda 表达式允许以简捷的方式实现和传递函数，而无需声明额外的（匿名）类。
Flink 支持对 Java API 的所有算子使用 Lambda 表达式，但是，当 Lambda 表达式使用 Java 泛型时，你需要 显式 地声明类型信息。 本文档介绍如何使用 Lambda 表达式并描述了其（Lambda 表达式）当前的限制。有关 Flink API 的通用介绍，请参阅 DataStream API 编程指南。
示例和限制 # 下面的这个示例演示了如何实现一个简单的内联 map() 函数，它使用 Lambda 表达式计算输入值的平方。
不需要声明 map() 函数的输入 i 和输出参数的数据类型，因为 Java 编译器会对它们做出推断。
env.fromElements(1, 2, 3) // 返回 i 的平方 .map(i -\u0026gt; i*i) .print(); 由于 OUT 是 Integer 而不是泛型，所以 Flink 可以从方法签名 OUT map(IN value) 的实现中自动提取出结果的类型信息。
不幸的是，像 flatMap() 这样的函数，它的签名 void flatMap(IN value, Collector\u0026lt;OUT\u0026gt; out) 被 Java 编译器编译为 void flatMap(IN value, Collector out)。这样 Flink 就无法自动推断输出的类型信息了。
Flink 很可能抛出如下异常：
org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of \u0026#39;Collector\u0026#39; are missing. In many cases lambda methods don\u0026#39;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the \u0026#39;org.apache.flink.api.common.functions.FlatMapFunction\u0026#39; interface. Otherwise the type has to be specified explicitly using type information. 在这种情况下，需要 显式 指定类型信息，否则输出将被视为 Object 类型，这会导致低效的序列化。
import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.DataSet; import org.apache.flink.util.Collector; DataSet\u0026lt;Integer\u0026gt; input = env.fromElements(1, 2, 3); // 必须声明 collector 类型 input.flatMap((Integer number, Collector\u0026lt;String\u0026gt; out) -\u0026gt; { StringBuilder builder = new StringBuilder(); for(int i = 0; i \u0026lt; number; i++) { builder.append(\u0026#34;a\u0026#34;); out.collect(builder.toString()); } }) // 显式提供类型信息 .returns(Types.STRING) // 打印 \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aaa\u0026#34; .print(); 当使用 map() 函数返回泛型类型的时候也会发生类似的问题。下面示例中的方法签名 Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer value) 被擦除为 Tuple2 map(Integer value)。
import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple2; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) // 没有关于 Tuple2 字段的信息 .print(); 一般来说，这些问题可以通过多种方式解决：
import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.api.java.tuple.Tuple2; // 使用显式的 \u0026#34;.returns(...)\u0026#34; env.fromElements(1, 2, 3) .map(i -\u0026gt; Tuple2.of(i, i)) .returns(Types.TUPLE(Types.INT, Types.INT)) .print(); // 使用类来替代 env.fromElements(1, 2, 3) .map(new MyTuple2Mapper()) .print(); public static class MyTuple2Mapper extends MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } } // 使用匿名类来替代 env.fromElements(1, 2, 3) .map(new MapFunction\u0026lt;Integer, Tuple2\u0026lt;Integer, Integer\u0026gt;\u0026gt; { @Override public Tuple2\u0026lt;Integer, Integer\u0026gt; map(Integer i) { return Tuple2.of(i, i); } }) .print(); // 也可以像这个示例中使用 Tuple 的子类来替代 env.fromElements(1, 2, 3) .map(i -\u0026gt; new DoubleTuple(i, i)) .print(); public static class DoubleTuple extends Tuple2\u0026lt;Integer, Integer\u0026gt; { public DoubleTuple(int f0, int f1) { this.f0 = f0; this.f1 = f1; } } Back to top
`}),e.add({id:310,href:"/flink/flink-docs-master/zh/docs/dev/datastream/execution/",title:"管理执行",section:"DataStream API",content:""}),e.add({id:311,href:"/flink/flink-docs-master/zh/docs/dev/table/concepts/temporal_table_function/",title:"Temporal Table Function",section:"流式概念",content:` Temporal Table Function # A Temporal table function provides access to the version of a temporal table at a specific point in time. In order to access the data in a temporal table, one must pass a time attribute that determines the version of the table that will be returned. Flink uses the SQL syntax of table functions to provide a way to express it.
Unlike a versioned table, temporal table functions can only be defined on top of append-only streams — it does not support changelog inputs. Additionally, a temporal table function cannot be defined in pure SQL DDL.
Defining a Temporal Table Function # Temporal table functions can be defined on top of append-only streams using the Table API. The table is registered with one or more key columns, and a time attribute used for versioning.
Suppose we have an append-only table of currency rates that we would like to register as a temporal table function.
SELECT * FROM currency_rates; update_time currency rate ============= ========= ==== 09:00:00 Yen 102 09:00:00 Euro 114 09:00:00 USD 1 11:15:00 Euro 119 11:49:00 Pounds 108 Using the Table API, we can register this stream using currency for the key and update_time as the versioning time attribute.
Java TemporalTableFunction rates = tEnv .from(\u0026#34;currency_rates\u0026#34;) .createTemporalTableFunction(\u0026#34;update_time\u0026#34;, \u0026#34;currency\u0026#34;); tEnv.registerFunction(\u0026#34;rates\u0026#34;, rates); Scala rates = tEnv .from(\u0026#34;currency_rates\u0026#34;) .createTemporalTableFunction(\u0026#34;update_time\u0026#34;, \u0026#34;currency\u0026#34;) tEnv.registerFunction(\u0026#34;rates\u0026#34;, rates) Python Still not supported in Python Table API. Temporal Table Function Join # Once defined, a temporal table function is used as a standard table function. Append-only tables (left input/probe side) can join with a temporal table (right input/build side), i.e., a table that changes over time and tracks its changes, to retrieve the value for a key as it was at a particular point in time.
Consider an append-only table orders that tracks customers\u0026rsquo; orders in different currencies.
SELECT * FROM orders; order_time amount currency ========== ====== ========= 10:15 2 Euro 10:30 1 USD 10:32 50 Yen 10:52 3 Euro 11:04 5 USD Given these tables, we would like to convert orders to a common currency — USD.
SQL SELECT SUM(amount * rate) AS amount FROM orders, LATERAL TABLE (rates(order_time)) WHERE rates.currency = orders.currency Java Table result = orders .joinLateral(\$(\u0026#34;rates(order_time)\u0026#34;), \$(\u0026#34;orders.currency = rates.currency\u0026#34;)) .select(\$(\u0026#34;(o_amount * r_rate).sum as amount\u0026#34;)); Scala val result = orders .joinLateral(\$\u0026#34;rates(order_time)\u0026#34;, \$\u0026#34;orders.currency = rates.currency\u0026#34;) .select(\$\u0026#34;(o_amount * r_rate).sum as amount\u0026#34;)) Python Still not supported in Python API. Back to top
`}),e.add({id:312,href:"/flink/flink-docs-master/zh/release-notes/flink-1.10/",title:"Release Notes - Flink 1.10",section:"Release-notes",content:` Release Notes - Flink 1.10 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.9 and Flink 1.10. Please read these notes carefully if you are planning to upgrade your Flink version to 1.10.
Clusters \u0026amp; Deployment # FileSystems should be loaded via Plugin Architecture # FLINK-11956 # s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through plugins but now seamlessly integrate with all credential providers. Other filesystems are strongly recommended to be only used as plugins as we will continue to remove relocations.
Flink Client respects Classloading Policy # FLINK-13749 # The Flink client now also respects the configured classloading policy, i.e., parent-first or child-first classloading. Previously, only cluster components such as the job manager or task manager supported this setting. This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicitly to use parent-first classloading, which was the previous (hard-coded) behaviour.
Enable spreading out Tasks evenly across all TaskManagers # FLINK-12122 # When FLIP-6 was rolled out with Flink 1.5.0, we changed how slots are allocated from TaskManagers (TMs). Instead of evenly allocating the slots from all registered TMs, we had the tendency to exhaust a TM before using another one. To use a scheduling strategy that is more similar to the pre-FLIP-6 behaviour, where Flink tries to spread out the workload across all currently available TMs, one can set cluster.evenly-spread-out-slots: true in the flink-conf.yaml.
Directory Structure Change for highly available Artifacts # FLINK-13633 # All highly available artifacts stored by Flink will now be stored under HA_STORAGE_DIR/HA_CLUSTER_ID with HA_STORAGE_DIR configured by high-availability.storageDir and HA_CLUSTER_ID configured by high-availability.cluster-id.
Resources and JARs shipped via \u0026ndash;yarnship will be ordered in the Classpath # FLINK-13127 # When using the --yarnship command line option, resource directories and jar files will be added to the classpath in lexicographical order with resources directories appearing first.
Removal of \u0026ndash;yn/\u0026ndash;yarncontainer Command Line Options # FLINK-12362 # The Flink CLI no longer supports the deprecated command line options -yn/--yarncontainer, which were used to specify the number of containers to start on YARN. This option has been deprecated since the introduction of FLIP-6. All Flink users are advised to remove this command line option.
Removal of \u0026ndash;yst/\u0026ndash;yarnstreaming Command Line Options # FLINK-14957 # The Flink CLI no longer supports the deprecated command line options -yst/--yarnstreaming, which were used to disable eager pre-allocation of memory. All Flink users are advised to remove this command line option.
Mesos Integration will reject expired Offers faster # FLINK-14029 # Flink\u0026rsquo;s Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on to a lot of expired offers without giving them back to the Mesos resource manager.
Scheduler Rearchitecture # FLINK-14651 # Flink\u0026rsquo;s scheduler was refactored with the goal of making scheduling strategies customizable in the future. Using the legacy scheduler is discouraged as it will be removed in a future release. However, users that experience issues related to scheduling can fallback to the legacy scheduler by setting jobmanager.scheduler to legacy in their flink-conf.yaml for the time being. Note, however, that using the legacy scheduler with the Pipelined Region Failover Strategy enabled has the following caveats:
Exceptions that caused a job to restart will not be shown on the job overview page of the Web UI FLINK-15917 # . However, exceptions that cause a job to fail (e.g., when all restart attempts exhausted) will still be shown.
The uptime metric will not be reset after restarting a job due to task failure FLINK-15918 # .
Note that in the default flink-conf.yaml, the Pipelined Region Failover Strategy is already enabled. That is, users that want to use the legacy scheduler and cannot accept aforementioned caveats should make sure that jobmanager.execution.failover-strategy is set to full or not set at all.
Java 11 Support # FLINK-10725 # Beginning from this release, Flink can be compiled and run with Java 11. All Java 8 artifacts can be also used with Java 11. This means that users that want to run Flink with Java 11 do not have to compile Flink themselves.
When starting Flink with Java 11, the following warnings may be logged:
WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.flink.core.memory.MemoryUtils (file:/opt/flink/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar) to constructor java.nio.DirectByteBuffer(long,int) WARNING: Please consider reporting this to the maintainers of org.apache.flink.core.memory.MemoryUtils WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/flinkuser/.m2/repository/org/apache/flink/flink-core/1.10.0/flink-core-1.10.0.jar) to field java.lang.String.value WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/home/flinkuser/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar) to method java.nio.DirectByteBuffer.cleaner() WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by com.esotericsoftware.kryo.util.UnsafeUtil (file:/home/flinkuser/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar) to constructor java.nio.DirectByteBuffer(long,int,java.lang.Object) WARNING: Please consider reporting this to the maintainers of com.esotericsoftware.kryo.util.UnsafeUtil WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release These warnings are considered harmless and will be addressed in future Flink releases.
Lastly, note that the connectors for Cassandra, Hive, HBase, and Kafka 0.8\u0026ndash;0.11 have not been tested with Java 11 because the respective projects did not provide Java 11 support at the time of the Flink 1.10.0 release.
Memory Management # New Task Executor Memory Model # FLINK-13980 # With FLIP-49, a new memory model has been introduced for the task executor. New configuration options have been introduced to control the memory consumption of the task executor process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration. The memory model of the job manager process has not been changed yet but it is planned to be updated as well.
If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes.
Please, check the user documentation for more details.
Deprecation and breaking changes # The following options have been removed and have no effect anymore:
Deprecated/removed config option Note taskmanager.memory.fraction Check also the description of the new option taskmanager.memory.managed.fraction but it has different semantics and the value of the deprecated option usually has to be adjusted taskmanager.memory.off-heap Support for on-heap managed memory has been removed, leaving off-heap managed memory as the only possibility taskmanager.memory.preallocate Pre-allocation is no longer supported, and managed memory is always allocated lazily The following options, if used, are interpreted as other new options in order to maintain backwards compatibility where it makes sense:
Deprecated config option Interpreted as taskmanager.heap.size taskmanager.memory.flink.size for standalone deployment taskmanager.memory.process.size for containerized deployments taskmanager.memory.size taskmanager.memory.managed.size taskmanager.network.memory.min taskmanager.memory.network.min taskmanager.network.memory.max taskmanager.memory.network.max taskmanager.network.memory.fraction taskmanager.memory.network.fraction The container cut-off configuration options, containerized.heap-cutoff-ratio and containerized.heap-cutoff-min, have no effect for task executor processes anymore but they still have the same semantics for the JobManager process.
RocksDB State Backend Memory Control # FLINK-7289 # Together with the introduction of the new Task Executor Memory Model, the memory consumption of the RocksDB state backend will be limited by the total amount of Flink Managed Memory, which can be configured via taskmanager.memory.managed.size or taskmanager.memory.managed.fraction. Furthermore, users can tune RocksDB\u0026rsquo;s write/read memory ratio (state.backend.rocksdb.memory.write-buffer-ratio, by default 0.5) and the reserved memory fraction for indices/filters (state.backend.rocksdb.memory.high-prio-pool-ratio, by default 0.1). More details and advanced configuration options can be found in the Flink user documentation.
Fine-grained Operator Resource Management # FLINK-14058 # Config options table.exec.resource.external-buffer-memory, table.exec.resource.hash-agg.memory, table.exec.resource.hash-join.memory, and table.exec.resource.sort.memory have been deprecated. Beginning from Flink 1.10, these config options are interpreted as weight hints instead of absolute memory requirements. Flink choses sensible default weight hints which should not be adjustment by users.
Table API \u0026amp; SQL # Rename of ANY Type to RAW Type # FLINK-14904 # The identifier raw is a reserved keyword now and must be escaped with backticks when used as a SQL field or function name.
Rename of Table Connector Properties # FLINK-14649 # Some indexed properties for table connectors have been flattened and renamed for a better user experience when writing DDL statements. This affects the Kafka Connector properties connector.properties and connector.specific-offsets. Furthermore, the Elasticsearch Connector property connector.hosts is affected. The aforementioned, old properties are deprecated and will be removed in future versions. Please consult the Table Connectors documentation for the new property names.
Methods for interacting with temporary Tables \u0026amp; Views # FLINK-14490 # Methods registerTable()/registerDataStream()/registerDataSet() have been deprecated in favor of createTemporaryView(), which better adheres to the corresponding SQL term.
The scan() method has been deprecated in favor of the from() method.
Methods registerTableSource()/registerTableSink() become deprecated in favor of ConnectTableDescriptor#createTemporaryTable(). The ConnectTableDescriptor approach expects only a set of string properties as a description of a TableSource or TableSink instead of an instance of a class in case of the deprecated methods. This in return makes it possible to reliably store those definitions in catalogs.
Method insertInto(String path, String... pathContinued) has been removed in favor of in insertInto(String path).
All the newly introduced methods accept a String identifier which will be parsed into a 3-part identifier. The parser supports quoting the identifier. It also requires escaping any reserved SQL keywords.
Removal of ExternalCatalog API # FLINK-13697 # The deprecated ExternalCatalog API has been dropped. This includes:
ExternalCatalog (and all dependent classes, e.g., ExternalTable) SchematicDescriptor, MetadataDescriptor, StatisticsDescriptor Users are advised to use the new Catalog API.
Configuration # Introduction of Type Information for ConfigOptions # FLINK-14493 # Getters of org.apache.flink.configuration.Configuration throw IllegalArgumentException now if the configured value cannot be parsed into the required type. In previous Flink releases the default value was returned in such cases.
Increase of default Restart Delay # FLINK-13884 # The default restart delay for all shipped restart strategies, i.e., fixed-delay and failure-rate, has been raised to 1 s (from originally 0 s).
Simplification of Cluster-Level Restart Strategy Configuration # FLINK-13921 # Previously, if the user had set restart-strategy.fixed-delay.attempts or restart-strategy.fixed-delay.delay but had not configured the option restart-strategy, the cluster-level restart strategy would have been fixed-delay. Now the cluster-level restart strategy is only determined by the config option restart-strategy and whether checkpointing is enabled. See \u0026ldquo;Task Failure Recovery\u0026rdquo; for details.
Disable memory-mapped BoundedBlockingSubpartition by default # FLINK-14952 # The config option taskmanager.network.bounded-blocking-subpartition-type has been renamed to taskmanager.network.blocking-shuffle.type. Moreover, the default value of the aforementioned config option has been changed from auto to file. The reason is that TaskManagers running on YARN with auto, could easily exceed the memory budget of their container, due to incorrectly accounted memory-mapped files memory usage.
Removal of non-credit-based Network Flow Control # FLINK-14516 # The non-credit-based network flow control code was removed alongside of the configuration option taskmanager.network.credit-model. Flink will now always use credit-based flow control.
Removal of HighAvailabilityOptions#HA_JOB_DELAY # FLINK-13885 # The configuration option high-availability.job.delay has been removed since it is no longer used.
State # Enable Background Cleanup of State with TTL by default # FLINK-14898 # Background cleanup of expired state with TTL is activated by default now for all state backends shipped with Flink. Note that the RocksDB state backend implements background cleanup by employing a compaction filter. This has the caveat that even if a Flink job does not store state with TTL, a minor performance penalty during compaction is incurred. Users that experience noticeable performance degradation during RocksDB compaction can disable the TTL compaction filter by setting the config option state.backend.rocksdb.ttl.compaction.filter.enabled to false.
Deprecation of StateTtlConfig#Builder#cleanupInBackground() # FLINK-15606 # StateTtlConfig#Builder#cleanupInBackground() has been deprecated because the background cleanup of state with TTL is already enabled by default.
Timers are stored in RocksDB by default when using RocksDBStateBackend # FLINK-15637 # The default timer store has been changed from Heap to RocksDB for the RocksDB state backend to support asynchronous snapshots for timer state and better scalability, with less than 5% performance cost. Users that find the performance decline critical can set state.backend.rocksdb.timer-service.factory to HEAP in flink-conf.yaml to restore the old behavior.
Removal of StateTtlConfig#TimeCharacteristic # FLINK-15605 # StateTtlConfig#TimeCharacteristic has been removed in favor of StateTtlConfig#TtlTimeCharacteristic.
New efficient Method to check if MapState is empty # FLINK-13034 # We have added a new method MapState#isEmpty() which enables users to check whether a map state is empty. The new method is 40% faster than mapState.keys().iterator().hasNext() when using the RocksDB state backend.
RocksDB Upgrade # FLINK-14483 # We have again released our own RocksDB build (FRocksDB) which is based on RocksDB version 5.17.2 with several feature backports for the Write Buffer Manager to enable limiting RocksDB\u0026rsquo;s memory usage. The decision to release our own RocksDB build was made because later RocksDB versions suffer from a performance regression under certain workloads.
RocksDB Logging disabled by default # FLINK-15068 # Logging in RocksDB (e.g., logging related to flush, compaction, memtable creation, etc.) has been disabled by default to prevent disk space from being filled up unexpectedly. Users that need to enable logging should implement their own RocksDBOptionsFactory that creates DBOptions instances with InfoLogLevel set to INFO_LEVEL.
Improved RocksDB Savepoint Recovery # FLINK-12785 # In previous Flink releases users may encounter an OutOfMemoryError when restoring from a RocksDB savepoint containing large KV pairs. For that reason we introduced a configurable memory limit in the RocksDBWriteBatchWrapper with a default value of 2 MB. RocksDB\u0026rsquo;s WriteBatch will flush before the consumed memory limit is reached. If needed, the limit can be tuned via the state.backend.rocksdb.write-batch-size config option in flink-conf.yaml.
PyFlink # Python 2 Support dropped # FLINK-14469 # Beginning from this release, PyFlink does not support Python 2. This is because Python 2 has reached end of life on January 1, 2020, and several third-party projects that PyFlink depends on are also dropping Python 2 support.
Monitoring # InfluxdbReporter skips Inf and NaN # FLINK-12147 # The InfluxdbReporter now silently skips values that are unsupported by InfluxDB, such as Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, Double.NaN, etc.
Connectors # Kinesis Connector License Change # FLINK-12847 # flink-connector-kinesis is now licensed under the Apache License, Version 2.0, and its artifacts will be deployed to Maven central as part of the Flink releases. Users no longer need to build the Kinesis connector from source themselves.
Miscellaneous Interface Changes # ExecutionConfig#getGlobalJobParameters() cannot return null anymore # FLINK-9787 # ExecutionConfig#getGlobalJobParameters has been changed to never return null. Conversely, ExecutionConfig#setGlobalJobParameters(GlobalJobParameters) will not accept null values anymore.
Change of contract in MasterTriggerRestoreHook interface # FLINK-14344 # Implementations of MasterTriggerRestoreHook#triggerCheckpoint(long, long, Executor) must be non-blocking now. Any blocking operation should be executed asynchronously, e.g., using the given executor.
Client-/ and Server-Side Separation of HA Services # FLINK-13750 # The HighAvailabilityServices have been split up into client-side ClientHighAvailabilityServices and cluster-side HighAvailabilityServices. When implementing custom high availability services, users should follow this separation by overriding the factory method HighAvailabilityServicesFactory#createClientHAServices(Configuration). Moreover, HighAvailabilityServices#getWebMonitorLeaderRetriever() should no longer be implemented since it has been deprecated.
Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() # FLINK-13977 # Implementations of HighAvailabilityServices should implement HighAvailabilityServices#getClusterRestEndpointLeaderElectionService() instead of HighAvailabilityServices#getWebMonitorLeaderElectionService().
Interface Change in LeaderElectionService # FLINK-14287 # LeaderElectionService#confirmLeadership(UUID, String) now takes an additional second argument, which is the address under which the leader will be reachable. All custom LeaderElectionService implementations will need to be updated accordingly.
Deprecation of Checkpoint Lock # FLINK-14857 # The method org.apache.flink.streaming.runtime.tasks.StreamTask#getCheckpointLock() is deprecated now. Users should use MailboxExecutor to run actions that require synchronization with the task\u0026rsquo;s thread (e.g. collecting output produced by an external thread). The methods MailboxExecutor#yield() or MailboxExecutor#tryYield() can be used for actions that need to give up control to other actions temporarily, e.g., if the current operator is blocked. The MailboxExecutor can be accessed by using YieldingOperatorFactory (see AsyncWaitOperator for an example usage).
Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces # FLINK-14926 # Interfaces OptionsFactory and ConfigurableOptionsFactory have been deprecated in favor of RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory, respectively.
Incompatibility of serialized JobGraphs # FLINK-14594 # Serialized JobGraphs which set the ResourceSpec created by Flink versions \u0026lt; 1.10 are no longer compatible with Flink \u0026gt;= 1.10. If you want to migrate these jobs to Flink \u0026gt;= 1.10 you will have to stop the job with a savepoint and then resume it from this savepoint on the Flink \u0026gt;= 1.10 cluster.
`}),e.add({id:313,href:"/flink/flink-docs-master/zh/release-notes/flink-1.11/",title:"Release Notes - Flink 1.11",section:"Release-notes",content:` Release Notes - Flink 1.11 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read these notes carefully if you are planning to upgrade your Flink version to 1.11.
Clusters \u0026amp; Deployment # Support for Application Mode # FLIP-85 # The user can now submit applications and choose to execute their main() method on the cluster rather than the client. This allows for more light-weight application submission. For more details, see the Application Mode documentation.
Web Submission behaves the same as detached mode. # With FLINK-16657 the web submission logic changes and it exposes the same behavior as submitting a job through the CLI in detached mode. This implies that, for instance, jobs based on the DataSet API that were using sinks like print(), count() or collect() will now throw an exception while before the output was simply never printed. See also comments on related PR.
Support for Hadoop 3.0.0 and higher # FLINK-11086 # Flink project does not provide any updated \u0026ldquo;flink-shaded-hadoop-*\u0026rdquo; jars. Users need to provide Hadoop dependencies through the HADOOP_CLASSPATH environment variable (recommended) or via lib/ folder. Also, the include-hadoop Maven profile has been removed.
flink-csv and flink-json are bundled in lib folder # FLINK-18173 # There is no need to download manually jar files for flink-csv and flink-json formats as they are now bundled in the lib folder.
Removal of LegacyScheduler # FLINK-15629 # Flink no longer supports the legacy scheduler. Hence, setting jobmanager.scheduler: legacy will no longer work and fail with an IllegalArgumentException. The only valid option for jobmanager.scheduler is the default value ng.
Bind user code class loader to lifetime of a slot # FLINK-16408 # The user code class loader is being reused by the TaskExecutor as long as there is at least a single slot allocated for the respective job. This changes Flink\u0026rsquo;s recovery behaviour slightly so that it will not reload static fields. The benefit is that this change drastically reduces pressure on the JVM\u0026rsquo;s metaspace.
Replaced slave file name with workers # FLINK-18307 # For Standalone Setups, the file with the worker nodes is no longer called slaves but workers. Previous setups that use the start-cluster.sh and stop-cluster.sh scripts need to rename that file.
Flink Docker Integration Improvements # The examples of Dockerfiles and docker image build.sh scripts have been removed from the Flink Github repository. The examples will no longer be maintained by community in the Flink Github repository, including the examples of integration with Bluemix. Therefore, the following modules have been deleted from the Flink Github repository:
flink-contrib/docker-flink flink-container/docker flink-container/kubernetes Check the updated user documentation for Flink Docker integration instead. It now describes in detail how to use and customize the Flink official docker image: configuration options, logging, plugins, adding more dependencies and installing software. The documentation also includes examples for Session and Job cluster deployments with:
docker run docker compose docker swarm standalone Kubernetes Memory Management # New JobManager Memory Model # Overview # With FLIP-116, a new memory model has been introduced for the JobManager. New configuration options have been introduced to control the memory consumption of the JobManager process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration.
Please, check the user documentation for more details.
If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes or even failures. In order to start the JobManager process, you have to specify at least one of the following options jobmanager.memory.flink.size, jobmanager.memory.process.size or jobmanager.memory.heap.size. See also the migration guide for more information.
Deprecation and breaking changes # The following options are deprecated:
jobmanager.heap.size jobmanager.heap.mb If these deprecated options are still used, they will be interpreted as one of the following new options in order to maintain backwards compatibility:
JVM Heap (jobmanager.memory.heap.size) for standalone and Mesos deployments Total Process Memory (jobmanager.memory.process.size) for containerized deployments (Kubernetes and Yarn) The following options have been removed and have no effect anymore:
containerized.heap-cutoff-ratio containerized.heap-cutoff-min There is no container cut-off anymore.
JVM arguments # The direct and metaspace memory of the JobManager\u0026rsquo;s JVM process are now limited by configurable values:
jobmanager.memory.off-heap.size jobmanager.memory.jvm-metaspace.size See also JVM Parameters.
These new limits can produce the respective OutOfMemoryError exceptions if they are not configured properly or there is a respective memory leak. See also the troubleshooting guide. Removal of deprecated mesos.resourcemanager.tasks.mem # FLINK-15198 # The mesos.resourcemanager.tasks.mem option, deprecated in 1.10 in favour of taskmanager.memory.process.size, has been completely removed and will have no effect anymore in 1.11+.
Table API \u0026amp; SQL # Blink is now the default planner # FLINK-16934 # The default table planner has been changed to blink.
Changed package structure for Table API # FLINK-15947 # Due to various issues with packages org.apache.flink.table.api.scala/java all classes from those packages were relocated. Moreover the scala expressions were moved to org.apache.flink.table.api as announced in Flink 1.9.
If you used one of:
org.apache.flink.table.api.java.StreamTableEnvironment org.apache.flink.table.api.scala.StreamTableEnvironment org.apache.flink.table.api.java.BatchTableEnvironment org.apache.flink.table.api.scala.BatchTableEnvironment And you do not convert to/from DataStream, switch to:
org.apache.flink.table.api.TableEnvironment If you do convert to/from DataStream/DataSet, change your imports to one of:
org.apache.flink.table.api.bridge.java.StreamTableEnvironment org.apache.flink.table.api.bridge.scala.StreamTableEnvironment org.apache.flink.table.api.bridge.java.BatchTableEnvironment org.apache.flink.table.api.bridge.scala.BatchTableEnvironment For the Scala expressions use the import:
org.apache.flink.table.api._ instead of org.apache.flink.table.api.bridge.scala._ Additionally, if you use Scala\u0026rsquo;s implicit conversions to/from DataStream/DataSet, import org.apache.flink.table.api.bridge.scala._ instead of org.apache.flink.table.api.scala._
Removal of deprecated StreamTableSink # FLINK-16362 # The existing StreamTableSink implementations should remove emitDataStream method.
Removal of BatchTableSink#emitDataSet # FLINK-16535 # The existing BatchTableSink implementations should rename emitDataSet to consumeDataSet and return DataSink.
Corrected execution behavior of TableEnvironment.execute() and StreamTableEnvironment.execute() # FLINK-16363 # In previous versions, TableEnvironment.execute() and StreamExecutionEnvironment.execute() can both trigger table and DataStream programs. Since Flink 1.11.0, table programs can only be triggered by TableEnvironment.execute(). Once table program is converted into DataStream program (through toAppendStream() or toRetractStream() method), it can only be triggered by StreamExecutionEnvironment.execute().
Corrected execution behavior of ExecutionEnvironment.execute() and BatchTableEnvironment.execute() # FLINK-17126 # In previous versions, BatchTableEnvironment.execute() and ExecutionEnvironment.execute() can both trigger table and DataSet programs for legacy batch planner. Since Flink 1.11.0, batch table programs can only be triggered by BatchEnvironment.execute(). Once table program is converted into DataSet program (through toDataSet() method), it can only be triggered by ExecutionEnvironment.execute().
Added a changeflag to Row type # FLINK-16998 # An additional change flag called RowKind was added to the Row type. This changed the serialization format and will trigger a state migration.
Configuration # Renamed log4j-yarn-session.properties and logback-yarn.xml properties files # FLINK-17527 # The logging properties files log4j-yarn-session.properties and logback-yarn.xml have been renamed to log4j-session.properties and logback-session.xml. Moreover, yarn-session.sh and kubernetes-session.sh use these logging properties files.
State # Removal of deprecated background cleanup toggle (State TTL) # FLINK-15620 # The StateTtlConfig#cleanupInBackground has been removed, because the method was deprecated and the background TTL was enabled by default in 1.10.
Removal of deprecated option to disable TTL compaction filter # FLINK-15621 # The TTL compaction filter in RocksDB has been enabled in 1.10 by default and it is now always enabled in 1.11+. Because of that the following option and methods have been removed in 1.11:
state.backend.rocksdb.ttl.compaction.filter.enabled StateTtlConfig#cleanupInRocksdbCompactFilter() RocksDBStateBackend#isTtlCompactionFilterEnabled RocksDBStateBackend#enableTtlCompactionFilter RocksDBStateBackend#disableTtlCompactionFilter (state_backend.py) is_ttl_compaction_filter_enabled (state_backend.py) enable_ttl_compaction_filter (state_backend.py) disable_ttl_compaction_filter Changed argument type of StateBackendFactory#createFromConfig # FLINK-16913 # Starting from Flink 1.11 the StateBackendFactory#createFromConfig interface now takes ReadableConfig instead of Configuration. A Configuration class is still a valid argument to that method, as it implements the ReadableConfig interface. Implementors of custom StateBackend should adjust their implementations.
Removal of deprecated OptionsFactory and ConfigurableOptionsFactory classes # FLINK-18242 # The deprecated OptionsFactory and ConfigurableOptionsFactory classes have been removed. Please use RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory instead. Please also recompile your application codes if any class extends DefaultConfigurableOptionsFactory.
Enabled by default setTotalOrderSeek # FLINK-17800 # Since Flink-1.11 the option setTotalOrderSeek will be enabled by default for RocksDB\u0026rsquo;s ReadOptions. This is in order to prevent user from miss using optimizeForPointLookup. For backward compatibility we support customizing ReadOptions through RocksDBOptionsFactory. Please set setTotalOrderSeek back to false if any performance regression observed (it shouldn\u0026rsquo;t happen according to our testing).
Increased default size of state.backend.fs.memory-threshold # FLINK-17865 # The default value of state.backend.fs.memory-threshold has been increased from 1K to 20K to prevent too many small files created on remote FS for small states. Jobs with large parallelism on source or stateful operators may have \u0026ldquo;JM OOM\u0026rdquo; or \u0026ldquo;RPC message exceeding maximum frame size\u0026rdquo; problem with this change. If you encounter such issues please manually set the configuration back to 1K.
PyFlink # Throw exceptions for the unsupported data types # FLINK-16606 # DataTypes can be configured with some parameters, e.g., precision. However in previous releases, the precision provided by users was not taking any effect and default value for the precision was being used. To avoid confusion since Flink 1.11 exceptions will be thrown if the value is not supported to make it more visible to users. Changes include:
the precision for TimeType can only be 0 the length for VarBinaryType/VarCharType can only be 0x7fffffff the precision/scale for DecimalType can only be 38/18 the precision for TimestampType/LocalZonedTimestampType can only be 3 the resolution for DayTimeIntervalType can only be SECOND and the fractionalPrecision can only be 3 the resolution for YearMonthIntervalType can only be MONTH and the yearPrecision can only be 2 the CharType/BinaryType/ZonedTimestampType is not supported Monitoring # Converted all MetricReporters to plugins # FLINK-16963 # All MetricReporters that come with Flink have been converted to plugins. They should no longer be placed into /lib directory (doing so may result in dependency conflicts!), but /plugins/\u0026lt;some_directory\u0026gt; instead.
Changed of DataDog\u0026rsquo;s metric reporter Counter metrics # FLINK-15438 # The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. This aligns the count semantics with the DataDog documentation.
Switch to Log4j 2 by default # FLINK-15672 # Flink now uses Log4j2 by default. Users who wish to revert back to Log4j1 can find instructions to do so in the logging documentation.
Changed behaviour of JobManager API\u0026rsquo;s log request # FLINK-16303 # Requesting an unavailable log or stdout file from the JobManager\u0026rsquo;s HTTP server returns status code 404 now. In previous releases, the HTTP server would return a file with (file unavailable) as its content.
Removal of lastCheckpointAlignmentBuffered metric # FLINK-16404 # Note that the metric lastCheckpointAlignmentBuffered has been removed, because the upstream task will not send any data after emitting a checkpoint barrier until the alignment has been completed on the downstream side. The web UI still displays this value but it is always 0 now.
Connectors # Dropped Kafka 0.8/0.9 connectors # FLINK-15115 # The Kafka 0.8 and 0.9 connectors are no longer under active development and were removed.
Dropped Elasticsearch 2.x connector # FLINK-16046 # The Elasticsearch 2 connector is no longer under active development and was removed. Prior version of these connectors will continue to work with Flink.
Removal of deprecated KafkaPartitioner # FLINK-15862 # Deprecated KafkaPartitioner was removed. Please see the release notes of Flink 1.3.0 how to migrate from that interface.
Refined fallback filesystems to only handle specific filesystems # FLINK-16015 # By default, if there is an official filesystem plugin for a given schema, it will not be allowed to use fallback filesystem factories (like HADOOP libraries on the classpath) to load it. Added fs.allowed-fallback-filesystems configuration option to override this behaviour.
Deprecation of FileSystem#getKind # FLINK-16400 # org.apache.flink.core.fs.FileSystem#getKind method has been formally deprecated, as it was not used by Flink.
Runtime # Streaming jobs will always fail immediately on failures in synchronous part of a checkpoint # FLINK-17350 # Failures in synchronous part of checkpointing (like an exceptions thrown by an operator) will fail its Task (and job) immediately, regardless of the configuration parameters. Since Flink 1.5 such failures could be ignored by setting setTolerableCheckpointFailureNumber(...) or its deprecated setFailTaskOnCheckpointError(...) predecessor. Now both options will only affect asynchronous failures.
Checkpoint timeouts are no longer ignored by CheckpointConfig#setTolerableCheckpointFailureNumber (FLINK-17351) # Checkpoint timeouts will now be treated as normal checkpoint failures and checked against value configured by CheckpointConfig#setTolerableCheckpointFailureNumber(...).
Miscellaneous Interface Changes # Removal of deprecated StreamTask#getCheckpointLock() # FLINK-12484 # DataStream API no longer provides StreamTask#getCheckpointLock method, which was deprecated in Flink 1.10. Users should use MailboxExecutor to run actions that require synchronization with the task\u0026rsquo;s thread (e.g. collecting output produced by an external thread). MailboxExecutor#yield or MailboxExecutor#tryYield methods can be used for actions that should give control to other actions temporarily (equivalent of StreamTask#getCheckpointLock().wait()), if the current operator is blocked. MailboxExecutor can be accessed by using YieldingOperatorFactory. Example usage can be found in the AsyncWaitOperator.
Note, SourceFunction.SourceContext.getCheckpointLock is still available for custom implementations of SourceFunction interface.
Reversed dependency from flink-streaming-java to flink-client # FLINK-15090 # Starting from Flink 1.11.0, the flink-streaming-java module does not have a dependency on flink-clients anymore. If your project was depending on this transitive dependency you now have to add flink-clients as an explicit dependency.
AsyncWaitOperator is chainable again # FLINK-16219 # AsyncWaitOperator will be allowed to be chained by default with all operators, except of tasks with SourceFunction. This mostly revert limitation introduced as a bug fix for FLINK-13063.
Changed argument types of ShuffleEnvironment#createInputGates and #createResultPartitionWriters methods # FLINK-16586 # The argument type of methods ShuffleEnvironment#createInputGates and #createResultPartitionWriters are adjusted from Collection to List for satisfying the order guarantee requirement in unaligned checkpoint. It will break the compatibility if users already implemented a custom ShuffleService based on ShuffleServiceFactory interface.
Deprecation of CompositeTypeSerializerSnapshot#isOuterSnapshotCompatible # FLINK-17520 # The boolean isOuterSnapshotCompatible(TypeSerializer) on the CompositeTypeSerializerSnapshot class has been deprecated, in favor of a new OuterSchemaCompatibility resolveOuterSchemaCompatibility(TypeSerializer) method. Please implement that instead. Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.
Removal of deprecated TimestampExtractor # FLINK-17655 # The long-deprecated TimestampExtractor was removed along with API methods in the DataStream API. Please use the new TimestampAssigner and WatermarkStrategies for working with timestamps and watermarks in the DataStream API.
Deprecation of ListCheckpointed interface # FLINK-6258 # The ListCheckpointed interface has been deprecated because it uses Java Serialization for checkpointing state which is problematic for savepoint compatibility. Use the CheckpointedFunction interface instead, which gives more control over state serialization.
Removal of deprecated state access methods # FLINK-17376 # We removed deprecated state access methods RuntimeContext#getFoldingState(), OperatorStateStore#getSerializableListState() and OperatorStateStore#getOperatorState(). This means that some code that was compiled against Flink 1.10 will not work with a Flink 1.11 cluster. An example of this is our Kafka connector which internally used OperatorStateStore.getSerializableListState.
`}),e.add({id:314,href:"/flink/flink-docs-master/zh/release-notes/flink-1.12/",title:"Release Notes - Flink 1.12",section:"Release-notes",content:` Release Notes - Flink 1.12 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.11 and Flink 1.12. Please read these notes carefully if you are planning to upgrade your Flink version to 1.12.
Known Issues # Unaligned checkpoint recovery may lead to corrupted data stream # FLINK-20654 # Using unaligned checkpoints in Flink 1.12.0 combined with two/multiple inputs tasks or with union inputs for single input tasks can result in corrupted state.
This can happen if a new checkpoint is triggered before recovery is fully completed. For state to be corrupted a task with two or more input gates must receive a checkpoint barrier exactly at the same time this tasks finishes recovering spilled in-flight data. In such case this new checkpoint can succeed, with corrupted/missing in-flight data, which will result in various deserialisation/corrupted data stream errors when someone attempts to recover from such corrupted checkpoint.
Using unaligned checkpoints in Flink 1.12.1, a corruption may occur in the checkpoint following a declined checkpoint.
A late barrier of a canceled checkpoint may lead to buffers being not written into the successive checkpoint, such that recovery is not possible. This happens, when the next checkpoint barrier arrives at a given operator before all previous barriers arrived, which can only happen after cancellation in unaligned checkpoints.
APIs # Remove deprecated methods in ExecutionConfig # FLINK-19084 # Deprecated method ExecutionConfig#isLatencyTrackingEnabled was removed, you can use ExecutionConfig#getLatencyTrackingInterval instead.
Deprecated and methods without effect were removed: ExecutionConfig#enable/disableSysoutLogging, ExecutionConfig#set/isFailTaskOnCheckpointError.
Removed -q flag from cli. The option had no effect.
Remove deprecated RuntimeContext#getAllAccumulators # FLINK-19032 # The deprecated method RuntimeContext#getAllAccumulators was removed. Please use RuntimeContext#getAccumulator instead.
Deprecated CheckpointConfig#setPreferCheckpointForRecovery due to risk of data loss # FLINK-20441 # The CheckpointConfig#setPreferCheckpointForRecovery method has been deprecated, because preferring older checkpoints over newer savepoints for recovery can lead to data loss.
FLIP-134: Batch execution for the DataStream API # Allow explicitly configuring time behaviour on KeyedStream.intervalJoin() FLINK-19479
Before Flink 1.12 the KeyedStream.intervalJoin() operation was changing behavior based on the globally set Stream TimeCharacteristic. In Flink 1.12 we introduced explicit inProcessingTime() and inEventTime() methods on IntervalJoin and the join no longer changes behaviour based on the global characteristic.
Deprecate timeWindow() operations in DataStream API FLINK-19318
In Flink 1.12 we deprecated the timeWindow() operations in the DataStream API. Please use window(WindowAssigner) with either a TumblingEventTimeWindows, SlidingEventTimeWindows, TumblingProcessingTimeWindows, or SlidingProcessingTimeWindows. For more information, see the deprecation description of TimeCharacteristic/setStreamTimeCharacteristic.
Deprecate StreamExecutionEnvironment.setStreamTimeCharacteristic() and TimeCharacteristic FLINK-19319
In Flink 1.12 the default stream time characteristic has been changed to EventTime, thus you don\u0026rsquo;t need to call this method for enabling event-time support anymore. Explicitly using processing-time windows and timers works in event-time mode. If you need to disable watermarks, please use ExecutionConfig.setAutoWatermarkInterval(long). If you are using IngestionTime, please manually set an appropriate WatermarkStrategy. If you are using generic \u0026ldquo;time window\u0026rdquo; operations (for example KeyedStream.timeWindow()) that change behaviour based on the time characteristic, please use equivalent operations that explicitly specify processing time or event time.
Allow explicitly configuring time behaviour on CEP PatternStream FLINK-19326
Before Flink 1.12 the CEP operations were changing their behavior based on the globally set Stream TimeCharacteristic. In Flink 1.12 we introduced explicit inProcessingTime() and inEventTime() methods on PatternStream and the CEP operations no longer change their behaviour based on the global characteristic.
API cleanups # Remove remaining UdfAnalyzer configurations FLINK-13857
The ExecutionConfig#get/setCodeAnalysisMode method and SkipCodeAnalysis class were removed. They took no effect even before that change, therefore there is no need to use any of these.
Remove deprecated DataStream#split FLINK-19083
The DataStream#split() operation has been removed after being marked as deprecated for a couple of versions. Please use Side Outputs) instead.
Remove deprecated DataStream#fold() method and all related classes FLINK-19035
The long deprecated (Windowed)DataStream#fold was removed in 1.12. Please use other operations such as e.g. (Windowed)DataStream#reduce that perform better in distributed systems.
Extend CompositeTypeSerializerSnapshot to allow composite serializers to signal migration based on outer configuration # FLINK-17520 # The boolean isOuterSnapshotCompatible(TypeSerializer) on the CompositeTypeSerializerSnapshot class has been deprecated, in favor of a new OuterSchemaCompatibility#resolveOuterSchemaCompatibility(TypeSerializer) method. Please implement that instead. Compared to the old method, the new method allows composite serializers to signal state schema migration based on outer schema and configuration.
Bump Scala Macros Version to 2.1.1 # FLINK-19278 # Flink now relies on Scala Macros 2.1.1. This means that we no longer support Scala \u0026lt; 2.11.11.
SQL # Use new type inference for SQL DDL of aggregate functions # FLINK-18901 # The CREATE FUNCTION DDL for aggregate functions uses the new type inference now. It might be necessary to update existing implementations to the new reflective type extraction logic. Use StreamTableEnvironment.registerFunction for the old stack.
Update parser module for FLIP-107 # FLINK-19273 # The term METADATA is a reserved keyword now. Use backticks to escape column names and other identifiers with this name.
Update internal aggregate functions to new type system # FLINK-18809 # SQL queries that use the COLLECT function might need to be updated to the new type system.
Connectors and Formats # Remove Kafka 0.10.x and 0.11.x connectors # FLINK-19152 # In Flink 1.12 we removed the Kafka 0.10.x and 0.11.x connectors. Please use the universal Kafka connector which works with any Kafka cluster version after 0.10.2.x.
Please refer to the documentation to learn about how to upgrade the Flink Kafka Connector version.
Csv Serialization schema contains line delimiter # FLINK-19868 # The csv.line-delimiter option has been removed from CSV format. Because the line delimiter should be defined by the connector instead of format. If users have been using this option in previous Flink version, they should alter such table to remove this option when upgrading to Flink 1.12. There should not much users using this option.
Upgrade to Kafka Schema Registry Client 5.5.0 # FLINK-18546 # The flink-avro-confluent-schema-registry module is no longer provided as a fat-jar. You should include its dependencies in your job\u0026rsquo;s fat-jar. Sql-client users can use flink-sql-avro-confluent-schema-registry fat jar.
Upgrade to Avro version 1.10.0 from 1.8.2 # FLINK-18192 # The default version of Avro in flink-avro module was upgraded to 1.10. If for some reason you need an older version (you have Avro coming from Hadoop, or you use classes generated from an older Avro version), please explicitly downgrade the Avro version in your project.
NOTE: We observed a decreased performance of the Avro 1.10 version compared to 1.8.2. If you are concerned with the performance and you are fine working with an older version of Avro, consider downgrading the Avro version.
Create an uber jar when packaging flink-avro for SQL Client # FLINK-18802 # The SQL Client jar was renamed to flink-sql-avro-1.16-SNAPSHOT.jar, previously flink-avro-1.16-SNAPSHOT-sql-jar.jar. Moreover it is no longer needed to add Avro dependencies manually.
Deployment # Default log4j configuration rolls logs after reaching 100 megabytes # FLINK-8357 # The default log4j configuration has changed: Besides the existing rolling of log files on startup of Flink, they also roll once they\u0026rsquo;ve reached a size of 100MB. Flink keeps a total of 10 log files, effectively limiting the total size of the log directory to 1GB (per Flink service logging to that directory).
Use jemalloc by default in the Flink docker image # FLINK-19125 # jemalloc is adopted as the default memory allocator in Flink\u0026rsquo;s docker image to reduce issues with memory fragmentation. Users can roll back to using glibc by passing the \u0026lsquo;disable-jemalloc\u0026rsquo; flag to the docker-entrypoint.sh script. For more details, please refer to the Flink on Docker documentation.
Upgrade Mesos version to 1.7 # FLINK-19783 # The Mesos dependency has been bumped from 1.0.1 to 1.7.0.
Send SIGKILL if Flink process doesn\u0026rsquo;t stop after a timeout # FLINK-17470 # In Flink 1.12 we changed the behavior of the standalone scripts to issue a SIGKILL if a SIGTERM did not succeed in shutting down a Flink process.
Introduce non-blocking job submission # FLINK-16866 # The semantics of submitting a job have slightly changed. The submission call returns almost immediately, with the job being in a new INITIALIZING state. Operations such as triggering a savepoint or retrieving the full job details are not available while the job is in that state.
Once the JobManager for that job has been created, the job is in CREATED state and all calls are available.
Runtime # FLIP-141: Intra-Slot Managed Memory Sharing # The configuration python.fn-execution.buffer.memory.size and python.fn-execution.framework.memory.size have been removed and so will not take effect any more. Besides, the default value of python.fn-execution.memory.managed has been changed to true and so managed memory will be used by default for Python workers. In cases where Python UDFs are used together with the RocksDB state backend in streaming or built-in batch algorithms in batch, the user can control how managed memory should be shared between data processing (RocksDB state backend or batch algorithms) and Python, by overwriting managed memory consumer weights.
FLIP-119 Pipelined Region Scheduling # FLINK-16430 # Beginning from Flink 1.12, jobs will be scheduled in the unit of pipelined regions. A pipelined region is a set of pipelined connected tasks. This means that, for streaming jobs which consist of multiple regions, it no longer waits for all tasks to acquire slots before starting to deploy tasks. Instead, any region can be deployed once it has acquired enough slots for within tasks. For batch jobs, tasks will not be assigned slots and get deployed individually. Instead, a task will be deployed together with all other tasks in the same region, once the region has acquired enough slots.
The old scheduler can be enabled using the jobmanager.scheduler.scheduling-strategy: legacy setting.
`}),e.add({id:315,href:"/flink/flink-docs-master/zh/release-notes/flink-1.13/",title:"Release Notes - Flink 1.13",section:"Release-notes",content:` Release Notes - Flink 1.13 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.12 and Flink 1.13. Please read these notes carefully if you are planning to upgrade your Flink version to 1.13.
Failover # Remove state.backend.async option. # FLINK-21935 # The state.backend.async option is deprecated. Snapshots are always asynchronous now (as they were by default before) and there is no option to configure a synchronous snapshot any more.
The constructors of FsStateBackend and MemoryStateBackend that take a flag for sync/async snapshots are kept for API compatibility, but the flags are ignored now.
FLINK-19463 # Flink has always separated local state storage from fault tolerance. Keyed state is maintained locally in state backends, either on the JVM heap or in embedded RocksDB instances. Fault tolerance comes from checkpoints and savepoints - periodic snapshots of a job\u0026rsquo;s internal state to some durable file system - such as Amazon S3 or HDFS.
Historically, Flink\u0026rsquo;s StateBackend interface intermixed these concepts in a way that confused many users. In 1.13, checkpointing configurations have been extracted into their own interface, CheckpointStorage.
This change does not affect the runtime behavior and simply provides a better mental model to users. Pipelines can be updated to use the new the new abstractions without losing state, consistency, or change in semantics.
Please follow the migration guide or the JavaDoc on the deprecated state backend classes - MemoryStateBackend, FsStateBackend and RocksDBStateBackend for migration details.
Unify binary format for Keyed State savepoints # FLINK-20976 # Flink’s savepoint binary format is unified across all state backends. That means you can take a savepoint with one state backend and then restore it using another.
If you want to switch the state backend you should first upgrade your Flink version to 1.13, then take a savepoint with the new version, and only after that, you can restore it with a different state backend.
FailureRateRestartBackoffTimeStrategy allows one less restart than configured # FLINK-20752 # The Failure Rate Restart Strategy was allowing 1 less restart per interval than configured. Users wishing to keep the current behavior should reduce the maximum number of allowed failures per interval by 1.
Support rescaling for Unaligned Checkpoints # FLINK-17979 # While recovering from unaligned checkpoints, users can now change the parallelism of the job. This change allows users to quickly upscale the job under backpressure.
SQL # Officially deprecate the legacy planner # FLINK-21709 # The old planner of the Table \u0026amp; SQL API is deprecated and will be dropped in Flink 1.14. This means that both the BatchTableEnvironment and DataSet API interop are reaching end of life. Use the unified TableEnvironment for batch and stream processing with the new planner, or the DataStream API in batch execution mode.
Use TIMESTAMP_LTZ as return type for function PROCTIME() # FLINK-21714 # Before Flink 1.13, the function return type of PROCTIME() is TIMESTAMP, and the return value is the TIMESTAMP in UTC time zone, e.g. the wall-clock shows 2021-03-01 12:00:00 at Shanghai, however the PROCTIME() displays 2021-03-01 04:00:00 which is wrong. Flink 1.13 fixes this issue and uses TIMESTAMP_LTZ type as return type of PROCTIME(), users don\u0026rsquo;t need to deal time zone problems anymore.
Support defining event time attribute on TIMESTAMP_LTZ column # FLINK-20387 # Support defining event time attribute on TIMESTAMP_LTZ column, base on this, Flink SQL gracefully support the Daylight Saving Time.
Correct function CURRENT_TIMESTAMP/CURRENT_TIME/CURRENT_DATE/LOCALTIME/LOCALTIMESTAMP/NOW() # FLINK-21713 # The value of time function CURRENT_TIMESTAMP and NOW() are corrected from UTC time with TIMESTAMP type to epoch time with TIMESTAMP_LTZ type. Time function LOCALTIME, LOCALTIMESTAMP, CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP and NOW() are corrected from evaluates for per record in batch mode to evaluate once at query-start for batch job.
Disable problematic cast conversion between NUMERIC type and TIMESTAMP type # FLINK-21698 # The CAST operation between NUMERIC type and TIMESTAMP type is problematic and is disabled now, e.g. CAST(numeric AS TIMESTAMP(3)) is disabled and should use TO_TIMESTAMP(FROM_UNIXTIME(numeric)) instead.
Support USE MODULES syntax # FLINK-21298 # The term MODULES is a reserved keyword now. Use backticks to escape column names and other identifiers with this name.
Update TableResult.collect()/TableResult.print() to the new type system # FLINK-20613 # Table.execute().collect() might return slightly different results for column types and row kind. The most important differences include:
Structured types are represented as POJOs of the original class and not Row anymore. Raw types are serialized according to the configuration in TableConfig. Add new StreamTableEnvironment.fromDataStream # FLINK-19977 # StreamTableEnvironment.fromDataStream has slightly different semantics now because it has been integrated into the new type system. Esp. row fields derived from composite type information might be in a different order compared to 1.12. The old behavior is still available via the overloaded method that takes expressions like fromDataStream(ds, \$(\u0026quot;field1\u0026quot;), \$(\u0026quot;field2\u0026quot;)).
Update the Row.toString method # FLINK-18090 # The Row.toSting() method has been reworked. This is an incompatible change. If the legacy representation is still required for tests, the old behavior can be restored via the flag RowUtils.USE_LEGACY_TO_STRING for the local JVM. However, relying on the row\u0026rsquo;s string representation for tests is not a good idea in general as field data types are not verified.
Support start SQL Client with an initialization SQL file # FLINK-20320 # The sql-client-defaults.yaml YAML file is deprecated and not provided in the release package. To be compatible, it\u0026rsquo;s still supported to initialize the SQL Client with the YAML file if manually provided. But it\u0026rsquo;s recommend to use the new introduced -i startup option to execute an initialization SQL file to setup the SQL Client session. The so-called initialization SQL file can use Flink DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment. The support of legacy SQL Client YAML file will be totally dropped in Flink 1.14.
Hive dialect no longer supports Flink syntax for DML and DQL # FLINK-21808 # Hive dialect supports HiveQL for DML and DQL. Please switch to default dialect in order to write in Flink syntax.
Runtime # BoundedOneInput.endInput is called when taking synchronous savepoint # FLINK-21132 # endInput() is not called anymore (on BoundedOneInput and BoundedMultiInput) when the job is stopping with savepoint.
Remove JobManagerOptions.SCHEDULING_STRATEGY # FLINK-20591 # The configuration parameter jobmanager.scheduler.scheduling-strategy has been removed, because the legacy scheduler has been removed from Flink 1.13.0.
Warn user if System.exit() is called in user code # FLINK-15156 # A new configuration value cluster.intercept-user-system-exit allows to log a warning, or throw an exception if user code calls System.exit().
This feature is not covering all locations in Flink where user code is executed. It just adds the infrastructure for such an interception. We are tracking this improvement in FLINK-21307.
MiniClusterJobClient#getAccumulators was infinitely blocking in local environment for a streaming job # FLINK-18685 # The semantics for accumulators have now changed in MiniClusterJobClient to fix this bug and comply with other JobClient implementations: Previously MiniClusterJobClient assumed that getAccumulator() was called on a bounded pipeline and that the user wanted to acquire the final accumulator values after the job is finished. But now it returns the current value of accumulators immediately to be compatible with unbounded pipelines.
If it is run on a bounded pipeline, then to get the final accumulator values after the job is finished, one needs to call
getJobExecutionResult().thenApply(JobExecutionResult::getAllAccumulatorResults)
Docker # Consider removing automatic configuration fo number of slots from docker # FLINK-21036 # The docker images no longer set the default number of taskmanager slots to the number of CPU cores. This behavior was inconsistent with all other deployment methods and ignored any limits on the CPU usage set via docker.
Rework jemalloc switch to use an environment variable # FLINK-21034 # The docker switch for disabling the jemalloc memory allocator has been reworked from a script argument to an environment variable called DISABLE_JEMALLOC. If set to \u0026ldquo;true\u0026rdquo; jemalloc will not be enabled.
Connectors # Remove swift FS filesystem # FLINK-21819 # The Swift filesystem is no longer being actively developed and has been removed from the project and distribution.
FLINK-22133 # The unified source API for connectors has a minor breaking change. The SplitEnumerator.snapshotState() method was adjusted to accept the Checkpoint ID of the checkpoint for which the snapshot is created.
Monitoring \u0026amp; debugging # Introduce latency tracking state # FLINK-21736 # State access latency metrics are introduced to track all kinds of keyed state access to help debug state performance. This feature is not enabled by default and can be turned on by setting state.backend.latency-track.keyed-state-enabled to true.
Support for CPU flame graphs in web UI # FLINK-13550 # Flink now offers flame graphs for each node in the job graph. Please enable this experimental feature by setting the respective configuration flag rest.flamegraph.enabled.
Display last n exceptions/causes for job restarts in Web UI # FLINK-6042 # Flink exposes the exception history now through the REST API and the UI. The amount of most-recently handled exceptions that shall be tracked can be defined through web.exception-history-size. Some values of the exception history\u0026rsquo;s REST API Json response are deprecated as part of this effort.
Create backPressuredTimeMsPerSecond metric # FLINK-20717 # Previously idleTimeMsPerSecond was defined as the time task spent waiting for either the input or the back pressure. Now idleTimeMsPerSecond excludes back pressured time, so if the task is back pressured it is not idle. The back pressured time is now measured separately as backPressuredTimeMsPerSecond.
Enable log4j2 monitor interval by default # FLINK-20510 # The Log4j support for updating the Log4j configuration at runtime has been enabled by default. The configuration files are checked for changes every 30 seconds.
ZooKeeper quorum fails to start due to missing log4j library # FLINK-20404 # The Zookeeper scripts in the Flink distribution have been modified to disable the Log4j JMX integration due to an incompatibility between Zookeeper 3.4 and Log4j 2. To re-enable this feature, remove the line in the zookeeper.sh file that sets zookeeper.jmx.log4j.disable.
Expose stage of task initialization # FLINK-17012 # Task\u0026rsquo;s RUNNING state was split into two states: INITIALIZING and RUNNING. Task is INITIALIZING while state is initialising and in case of unaligned checkpoints, until all the in-flight data has been recovered.
Deployment # Officially deprecate Mesos support # FLINK-22352 # The community decided to deprecate the Apache Mesos support for Apache Flink. It is subject to removal in the future. Users are encouraged to switch to a different resource manager.
`}),e.add({id:316,href:"/flink/flink-docs-master/zh/release-notes/flink-1.14/",title:"Release Notes - Flink 1.14",section:"Release-notes",content:` Release notes - Flink 1.14 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.13 and Flink 1.14. Please read these notes carefully if you are planning to upgrade your Flink version to 1.14.
Known issues # Memory leak with Pulsar connector on Java 11 # Netty, which the Pulsar client uses underneath, allocates memory differently on Java 11 and Java 8. On Java 11, it will allocate memory from the pool of Java Direct Memory and is affected by the MaxDirectMemory limit. The current Pulsar client has no configuration options for controlling the memory limits, which can lead to OOM(s).
Users are advised to use the Pulsar connector with Java 8 or overprovision memory for Flink. Read the memory setup guide on how to configure memory for Flink and track the proper solution in FLINK-24302.
Summary of changed dependency names # There are two changes in Flink 1.14 that require updating dependency names when upgrading from earlier versions.
The removal of the Blink planner (FLINK-22879) requires the removal of the blink infix. Due to FLINK-14105, if you have a dependency on flink-runtime, flink-optimizer and/or flink-queryable-state-runtime, the Scala suffix (_2.11/_2.12) needs to be removed from the artifactId. Table API \u0026amp; SQL # Use pipeline name consistently across DataStream API and Table API # FLINK-23646 # The default job name for DataStream API programs in batch mode has changed from \u0026quot;Flink Streaming Job\u0026quot; to \u0026quot;Flink Batch Job\u0026quot;. A custom name can be set with the config option pipeline.name.
Propagate unique keys for fromChangelogStream # FLINK-24033 # Compared to 1.13.2, StreamTableEnvironment.fromChangelogStream might produce a different stream because primary keys were not properly considered before.
Support new type inference for Table#flatMap # FLINK-16769 # Table.flatMap() supports the new type system now. Users are requested to upgrade their functions.
Add Scala implicit conversions for new API methods # FLINK-22590 # The Scala implicits that convert between DataStream API and Table API have been updated to the new methods of FLIP-136.
The changes might require an update of pipelines that used toTable or implicit conversions from Table to DataStream[Row].
Remove YAML environment file support in SQL Client # FLINK-22540 # The sql-client-defaults.yaml file was deprecated in the 1.13 release and is now completely removed. As an alternative, you can use the -i startup option to execute an SQL initialization file to set up the SQL Client session. The SQL initialization file can use Flink DDLs to define available catalogs, table sources and sinks, user-defined functions, and other properties required for execution and deployment.
See more: https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/sqlclient/#initialize-session-using-sql-files
Remove the legacy planner code base # FLINK-22864 # The old Table/SQL planner has been removed. BatchTableEnvironment and DataSet API interoperability with Table API are not supported anymore. Use the unified TableEnvironment for batch and stream processing with the new planner or the DataStream API in batch execution mode.
Users are encouraged to update their pipelines. Otherwise Flink 1.13 is the last version that offers the old functionality.
Remove the \u0026ldquo;blink\u0026rdquo; suffix from table modules # FLINK-22879 # The following Maven modules have been renamed:
flink-table-planner-blink -\u0026gt; flink-table-planner flink-table-runtime-blink -\u0026gt; flink-table-runtime flink-table-uber-blink -\u0026gt; flink-table-uber It might be required to update job JAR dependencies. Note that flink-table-planner and flink-table-uber used to contain the legacy planner before Flink 1.14 and now they contain the only officially supported planner (i.e. previously known as \u0026lsquo;Blink\u0026rsquo; planner).
Remove BatchTableEnvironment and related API classes # FLINK-22877 # Due to the removal of BatchTableEnvironment, BatchTableSource and BatchTableSink have been removed as well. Use DynamicTableSource and DynamicTableSink instead. They support the old InputFormat and OutputFormat interfaces as runtime providers if necessary.
Remove TableEnvironment#connect # FLINK-23063 # The deprecated TableEnvironment.connect() method has been removed. Use the new TableEnvironment.createTemporaryTable(String, TableDescriptor) to create tables programmatically. Please note that this method only supports sources and sinks that comply with FLIP-95. This is also indicated by the new property design 'connector'='kafka' instead of 'connector.type'='kafka'.
Deprecate toAppendStream and toRetractStream # FLINK-23330 # The outdated variants of StreamTableEnvironment.{fromDataStream|toAppendStream|toRetractStream) have been deprecated. Use the (from|to)(Data|Changelog)Stream alternatives introduced in 1.13.
Remove old connectors and formats stack around descriptors # FLINK-23513 # The legacy versions of the SQL Kafka connector and SQL Elasticsearch connector have been removed together with their corresponding legacy formats. DDL or descriptors that still use 'connector.type=' or 'format.type=' options need to be updated to the new connector and formats available via the 'connector=' option.
Drop BatchTableSource/Sink HBaseTableSource/Sink and related classes # FLINK-22623 # The HBaseTableSource/Sink and related classes including various HBaseInputFormats and HBaseSinkFunction have been removed. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API (or vice versa) if necessary. The DataSet API is not supported anymore.
Drop BatchTableSource ParquetTableSource and related classes # FLINK-22622 # The ParquetTableSource and related classes including various ParquetInputFormats have been removed. Use the FileSystem connector with a Parquet format as a replacement. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API if necessary. The DataSet API is not supported anymore.
Drop BatchTableSource OrcTableSource and related classes # FLINK-22620 # The OrcTableSource and related classes (including OrcInputFormat) have been removed. Use the FileSystem connector with an ORC format as a replacement. It is possible to read via the Table \u0026amp; SQL API and convert the Table to DataStream API if necessary. The DataSet API is not supported anymore.
Drop usages of BatchTableEnvironment and the old planner in Python # FLINK-22619 # The Python API does not offer a dedicated BatchTableEnvironment anymore. Instead, users can switch to the unified TableEnvironment for both batch and stream processing. Only the Blink planner (the only remaining planner in 1.14) is supported.
Migrate ModuleFactory to the new factory stack # FLINK-23720 # The LOAD/UNLOAD MODULE architecture for table modules has been updated to the new factory stack of FLIP-95. Users of this feature should update their ModuleFactory implementations.
Migrate Table API to new KafkaSink # FLINK-23639 # Table API/SQL now writes to Kafka with the new KafkaSink. When migrating from a query writing to Kafka in exactly-once mode from an earlier Flink version, make sure to terminate the old application with stop-with-savepoint to avoid lingering Kafka transactions. To run in exactly-once processing mode, the sink needs a user-configured and unique transaction prefix, such that transactions of different applications do not interfere with each other.
DataStream API # Fixed idleness handling for two/multi input operators # FLINK-18934 # FLINK-23767 # We added processWatermarkStatusX method to classes such as AbstractStreamOperator, Input etc. It allows to take the WatermarkStatus into account when combining watermarks in two/multi input operators.
Note that with this release, we renamed the previously internal StreamStatus to WatermarkStatus in order to better reflect its purpose.
Allow @TypeInfo annotation on POJO field declarations # FLINK-12141 # @TypeInfo annotations can now also be used on POJO fields which, for example, can help to define custom serializers for third-party classes that can otherwise not be annotated themselves.
Clarify SourceFunction#cancel() contract about interruptions # FLINK-23527 # Contract of the SourceFunction.cancel() method with respect to interruptions has been clarified:
source itself should not be interrupting the source thread interrupt should not be expected in the clean cancellation case Expose a consistent GlobalDataExchangeMode # FLINK-23402 # The default DataStream API shuffle mode for batch executions has been changed to blocking exchanges for all edges of the stream graph. A new option execution.batch-shuffle-mode allows you to change it to pipelined behavior if necessary.
Python API # Support loopback mode to allow Python UDF worker and client to reuse the same Python VM # FLINK-21222 # Instead of launching a separate Python process, the Python UDF worker will reuse the Python process of the client side when running jobs locally. This makes it easier to debug Python UDFs.
Support Python UDF chaining in Python DataStream API # FLINK-22913 # The job graph of Python DataStream API jobs may be different from before as Python functions will be chained as much as possible to optimize performance. You could disable Python functions chaining by explicitly setting python.operator-chaining.enabled as false.
Connectors # Expose standardized operator metrics (FLIP-179) # FLINK-23652 # Connectors using the unified Source and Sink interface will expose certain standardized metrics automatically. Applications that use RuntimeContext#getMetricGroup need to be rebuild against 1.14 before being submitted to a 1.14 cluster.
Port KafkaSink to new Unified Sink API (FLIP-143) # FLINK-22902 # KafkaSink supersedes FlinkKafkaProducer and provides efficient exactly-once and at-least-once writing with the new unified sink interface, supporting both batch and streaming mode of DataStream API. To upgrade, please stop with savepoint. To run in exactly-once processing mode, KafkaSink needs a user-configured and unique transaction prefix, such that transactions of different applications do not interfere with each other.
Deprecate FlinkKafkaConsumer # FLINK-24055 # FlinkKafkaConsumer has been deprecated in favor of KafkaSource. To upgrade to the new version, please store the offsets in Kafka with setCommitOffsetsOnCheckpoints in the old FlinkKafkaConsumer and then stop with a savepoint. When resuming from the savepoint, please use setStartingOffsets(OffsetsInitializer.committedOffsets()) in the new KafkaSourceBuilder to transfer the offsets to the new source.
InputStatus should not contain END_OF_RECOVERY # FLINK-23474 # InputStatus.END_OF_RECOVERY was removed. It was an internal flag that should never be returned from SourceReaders. Returning that value in earlier versions might lead to misbehavior.
Connector-base exposes dependency to flink-core. # FLINK-22964 # Connectors do not transitively hold a reference to flink-core anymore. That means that a fat JAR with a connector does not include flink-core with this fix.
Runtime \u0026amp; Coordination # Increase akka.ask.timeout for tests using the MiniCluster # FLINK-23906 # The default akka.ask.timeout used by the MiniCluster has been increased to 5 minutes. If you want to use a smaller value, then you have to set it explicitly in the passed configuration.
The change is due to the fact that messages can not get lost in a single-process MiniCluster, so this timeout (which otherwise helps to detect message loss in distributed setups) has no benefit here.
The increased timeout reduces the number of false-positive timeouts, for example, during heavy tests on loaded CI/CD workers or during debugging.
The node IP obtained in NodePort mode is a VIP # FLINK-23507 # When using kubernetes.rest-service.exposed.type=NodePort, the connection string for the REST gateway is now correctly constructed in the form \u0026lt;nodeIp\u0026gt;:\u0026lt;nodePort\u0026gt; instead of \u0026lt;kubernetesApiServerUrl\u0026gt;:\u0026lt;nodePort\u0026gt;. This may be a breaking change for some users.
This also introduces a new config option kubernetes.rest-service.exposed.node-port-address-type that lets you select \u0026lt;nodeIp\u0026gt; from a desired range.
Timeout heartbeat if the heartbeat target is no longer reachable # FLINK-23209 # Flink now supports detecting dead TaskManagers via the number of consecutive failed heartbeat RPCs. The threshold until a TaskManager is marked as unreachable can be configured via heartbeat.rpc-failure-threshold. This can speed up the detection of dead TaskManagers significantly.
RPCs fail faster when target is unreachable # FLINK-23202 # The same way Flink detects unreachable heartbeat targets faster, Flink now also immediately fails RPCs where the target is known by the OS to be unreachable on a network level, instead of waiting for a timeout (akka.ask.timeout).
This creates faster task failovers because cancelling tasks on a dead TaskExecutor no longer gets delayed by the RPC timeout.
If this faster failover is a problem in certain setups (which might rely on the fact that external systems hit timeouts), we recommend configuring the application\u0026rsquo;s restart strategy with a restart delay.
Changes in accounting of IOExceptions when triggering checkpoints on JobManager # FLINK-23189 # In previous versions, IOExceptions thrown from the JobManager would not fail the entire Job. We changed the way we track those exceptions and now they do increase the number of checkpoint failures.
The number of tolerable checkpoint failures can be adjusted or disabled via: org.apache.flink.streaming.api.environment.CheckpointConfig#setTolerableCheckpointFailureNumber (which is set to 0 by default).
Refine ShuffleMaster lifecycle management for pluggable shuffle service framework # FLINK-22910 # We improved the ShuffleMaster interface by adding some lifecycle methods, including open, close, registerJob and unregisterJob. Besides, the ShuffleMaster now becomes a cluster level service which can be shared by multiple jobs. This is a breaking change to the pluggable shuffle service framework and the customized shuffle plugin needs to adapt to the new interface accordingly.
Group job specific ZooKeeper HA services under common jobs/ zNode # FLINK-22636 # The ZooKeeper job-specific HA services are now grouped under a zNode with the respective JobID. Moreover, the config options high-availability.zookeeper.path.latch, high-availability.zookeeper.path.leader, high-availability.zookeeper.path.checkpoints, and high-availability.zookeeper.path.checkpoint-counter have been removed and, thus, no longer have an effect.
Fallback value for taskmanager.slot.timeout # FLINK-22002 # The config option taskmanager.slot.timeout now falls back to akka.ask.timeout if no value has been configured. Previously, the default value for taskmanager.slot.timeout was 10 s.
DuplicateJobSubmissionException after JobManager failover # FLINK-21928 # The fix for this problem only works if the ApplicationMode is used with a single job submission and if the user code does not access the JobExecutionResult. If any of these conditions is violated, then Flink cannot guarantee that the whole Flink application is executed.
Additionally, it is still required that the user cleans up the corresponding HA entries for the running jobs registry because these entries won\u0026rsquo;t be reliably cleaned up when encountering the situation described by FLINK-21928.
Zookeeper node under leader and leaderlatch is not deleted after job finished # FLINK-20695 # The HighAvailabilityServices interface has received a new method cleanupJobData which can be implemented in order to clean up job-related HA data after a given job has terminated.
Optimize scheduler performance for large-scale jobs # FLINK-21110 # The performance of the scheduler has been improved to reduce the time of execution graph creation, task deployment, and task failover. This improvement is significant to large scale jobs which currently may spend minutes on the processes mentioned above. This improvement also helps to avoid cases when the job manager main thread gets blocked for too long and leads to heartbeat timeout.
Checkpoints # The semantic of alignmentTimeout configuration has changed meaning # FLINK-23041 # The semantic of alignmentTimeout configuration has changed meaning and now it is measured as the time between the start of a checkpoint (on the checkpoint coordinator) and the time when the checkpoint barrier is received by a task.
Disable unaligned checkpoints for BROADCAST exchanges # FLINK-22815 # Broadcast partitioning can not work with unaligned checkpointing. There are no guarantees that records are consumed at the same rate in all channels. This can result in some tasks applying state changes corresponding to a certain broadcasted event while others do not. Upon restore, it may lead to an inconsistent state.
DefaultCompletedCheckpointStore drops unrecoverable checkpoints silently # FLINK-22502 # On recovery, if a failure occurs during retrieval of a checkpoint, the job is restarted (instead of skipping the checkpoint in some circumstances). This prevents potential consistency violations.
Remove the CompletedCheckpointRecover#recover() method. # FLINK-22483 # Flink no longer reloads checkpoint metadata from the external storage before restoring the task state after failover (except when the JobManager fails over / changes leadership). This results in less external I/O and faster failover.
Please note that this changes public interfaces around CompletedCheckpointStore, that we allow overriding by providing custom implementation of HA Services.
Remove the deprecated CheckpointConfig#setPreferCheckpointForRecovery method. # FLINK-20427 # The deprecated method CheckpointConfig#setPreferCheckpointForRecovery was removed, because preferring older checkpoints over newer savepoints for recovery can lead to data loss.
Dependency upgrades # Bump up RocksDb version to 6.20.3 # FLINK-14482 # RocksDB has been upgraded to 6.20.3. The new version contains lots of bug fixes, ARM platform support, musl library support, and more attractive features. However, the new version can entail at most 8% performance regression according to our tests.
See the corresponding ticket for more information.
Support configuration of the RocksDB info logging via configuration # FLINK-23812 # With RocksDB upgraded to 6.20.3 (FLINK-14482), you can now also configure a rolling info logging strategy by configuring it accordingly via the newly added state.backend.rocksdb.log.* settings. This can be helpful for debugging RocksDB (performance) issues in containerized environments where the local data directory is volatile but the logs should be retained on a separate volume mount.
Make flink-runtime scala-free # FLINK-14105 # Flink\u0026rsquo;s Akka dependency is now loaded with a separate classloader and no longer accessible from the outside.
As a result, various modules (most prominently, flink-runtime) no longer have a scala suffix in their artifactId.
Drop Mesos support # FLINK-23118 # The support for Apache Mesos has been removed.
`}),e.add({id:317,href:"/flink/flink-docs-master/zh/release-notes/flink-1.5/",title:"Release Notes - Flink 1.5",section:"Release-notes",content:` Release Notes - Flink 1.5 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.
Update Configuration for Reworked Job Deployment # Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos. Flink will automatically determine the number of containers from the parallelism of the application.
Although the deployment logic was completely reworked, we aimed to not unnecessarily change the previous behavior to enable a smooth transition. Nonetheless, there are a few options that you should update in your conf/flink-conf.yaml or know about.
The allocation of TaskManagers with multiple slots is not fully supported yet. Therefore, we recommend to configure TaskManagers with a single slot, i.e., set taskmanager.numberOfTaskSlots: 1 If you observed any problems with the new deployment mode, you can always switch back to the pre-1.5 behavior by configuring mode: legacy. Please report any problems or possible improvements that you notice to the Flink community, either by posting to a mailing list or by opening a JIRA issue.
Note: We plan to remove the legacy mode in the next release.
Update Configuration for Reworked Network Stack # The changes on the networking stack for credit-based flow control and improved latency affect the configuration of network buffers. In a nutshell, the networking stack can require more memory to run applications. Hence, you might need to adjust the network configuration of your Flink setup.
There are two ways to address problems of job submissions that fail due to lack of network buffers.
Reduce the number of buffers per channel, i.e., taskmanager.network.memory.buffers-per-channel or Increase the amount of TaskManager memory that is used by the network stack, i.e., increase taskmanager.network.memory.fraction and/or taskmanager.network.memory.max. Please consult the section about network buffer configuration in the Flink documentation for details. In case you experience issues with the new credit-based flow control mode, you can disable flow control by setting taskmanager.network.credit-model: false.
Note: We plan to remove the old model and this configuration in the next release.
Hadoop Classpath Discovery # We removed the automatic Hadoop classpath discovery via the Hadoop binary. If you want Flink to pick up the Hadoop classpath you have to export HADOOP_CLASSPATH. On cloud environments and most Hadoop distributions you would do
export HADOOP_CLASSPATH=\`hadoop classpath\`. Breaking Changes of the REST API # In an effort to harmonize, extend, and improve the REST API, a few handlers and return values were changed.
The jobs overview handler is now registered under /jobs/overview (before /joboverview) and returns a list of job details instead of the pre-grouped view of running, finished, cancelled and failed jobs. The REST API to cancel a job was changed. The REST API to cancel a job with savepoint was changed. Please check the REST API documentation for details.
Kafka Producer Flushes on Checkpoint by Default # The Flink Kafka Producer now flushes on checkpoints by default. Prior to version 1.5, the behaviour was disabled by default and users had to explicitly call setFlushOnCheckpoints(true) on the producer to enable it.
Updated Kinesis Dependency # The Kinesis dependencies of Flink’s Kinesis connector have been updated to the following versions.
\u0026lt;aws.sdk.version\u0026gt;1.11.319\u0026lt;/aws.sdk.version\u0026gt; \u0026lt;aws.kinesis-kcl.version\u0026gt;1.9.0\u0026lt;/aws.kinesis-kcl.version\u0026gt; \u0026lt;aws.kinesis-kpl.version\u0026gt;0.12.9\u0026lt;/aws.kinesis-kcl.version\u0026gt; Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
Back to top
`}),e.add({id:318,href:"/flink/flink-docs-master/zh/release-notes/flink-1.6/",title:"Release Notes - Flink 1.6",section:"Release-notes",content:` Release Notes - Flink 1.6 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.5 and Flink 1.6. Please read these notes carefully if you are planning to upgrade your Flink version to 1.6.
Changed Configuration Default Values # The default value of the slot idle timeout slot.idle.timeout is set to the default value of the heartbeat timeout (50 s).
Changed ElasticSearch 5.x Sink API # Previous APIs in the Flink ElasticSearch 5.x Sink\u0026rsquo;s RequestIndexer interface have been deprecated in favor of new signatures. When adding requests to the RequestIndexer, the requests now must be of type IndexRequest, DeleteRequest, or UpdateRequest, instead of the base ActionRequest.
Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
Back to top
`}),e.add({id:319,href:"/flink/flink-docs-master/zh/release-notes/flink-1.7/",title:"Release Notes - Flink 1.7",section:"Release-notes",content:` Release Notes - Flink 1.7 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.6 and Flink 1.7. Please read these notes carefully if you are planning to upgrade your Flink version to 1.7.
Scala 2.12 support # When using Scala 2.12 you might have to add explicit type annotations in places where they were not required when using Scala 2.11. This is an excerpt from the TransitiveClosureNaive.scala example in the Flink code base that shows the changes that could be required.
Previous code:
val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) { (prev, next, out: Collector[(Long, Long)]) =\u0026gt; { val prevPaths = prev.toSet for (n \u0026lt;- next) if (!prevPaths.contains(n)) out.collect(n) } } With Scala 2.12 you have to change it to:
val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) { (prev: Iterator[(Long, Long)], next: Iterator[(Long, Long)], out: Collector[(Long, Long)]) =\u0026gt; { val prevPaths = prev.toSet for (n \u0026lt;- next) if (!prevPaths.contains(n)) out.collect(n) } } The reason for this is that Scala 2.12 changes how lambdas are implemented. They now use the lambda support using SAM interfaces introduced in Java 8. This makes some method calls ambiguous because now both Scala-style lambdas and SAMs are candidates for methods were it was previously clear which method would be invoked.
State evolution # Before Flink 1.7, serializer snapshots were implemented as a TypeSerializerConfigSnapshot (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new TypeSerializerSnapshot interface introduced in 1.7). Moreover, the responsibility of serializer schema compatibility checks lived within the TypeSerializer, implemented in the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method.
To be future-proof and to have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. Details and migration guides can be found here.
Removal of the legacy mode # Flink no longer supports the legacy mode. If you depend on this, then please use Flink 1.6.x.
Savepoints being used for recovery # Savepoints are now used while recovering. Previously when using exactly-once sink one could get into problems with duplicate output data when a failure occurred after a savepoint was taken but before the next checkpoint occurred. This results in the fact that savepoints are no longer exclusively under the control of the user. Savepoint should not be moved nor deleted if there was no newer checkpoint or savepoint taken.
MetricQueryService runs in separate thread pool # The metric query service runs now in its own ActorSystem. It needs consequently to open a new port for the query services to communicate with each other. The query service port can be configured in flink-conf.yaml.
Granularity of latency metrics # The default granularity for latency metrics has been modified. To restore the previous behavior users have to explicitly set the granularity to subtask.
Latency marker activation # Latency metrics are now disabled by default, which will affect all jobs that do not explicitly set the latencyTrackingInterval via ExecutionConfig#setLatencyTrackingInterval. To restore the previous default behavior users have to configure the latency interval in flink-conf.yaml.
Relocation of Hadoop\u0026rsquo;s Netty dependency # We now also relocate Hadoop\u0026rsquo;s Netty dependency from io.netty to org.apache.flink.hadoop.shaded.io.netty. You can now bundle your own version of Netty into your job but may no longer assume that io.netty is present in the flink-shaded-hadoop2-uber-*.jar file.
Local recovery fixed # With the improvements to Flink\u0026rsquo;s scheduling, it can no longer happen that recoveries require more slots than before if local recovery is enabled. Consequently, we encourage our users to enable local recovery in flink-conf.yaml.
Support for multi slot TaskManagers # Flink now properly supports TaskManagers with multiple slots. Consequently, TaskManagers can now be started with an arbitrary number of slots and it is no longer recommended to start them with a single slot.
StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID # The StandaloneJobClusterEntrypoint, which is launched by the script standalone-job.sh and used for the job-mode container images, now starts all jobs with a fixed JobID. Thus, in order to run a cluster in HA mode, one needs to set a different cluster id for each job/cluster.
Scala shell does not work with Scala 2.12 # Flink\u0026rsquo;s Scala shell does not work with Scala 2.12. Therefore, the module flink-scala-shell is not being released for Scala 2.12.
See FLINK-10911 for more details.
Limitations of failover strategies # Flink\u0026rsquo;s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to \u0026quot;full\u0026quot;.
In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.
SQL over window preceding clause # The over window preceding clause is now optional. It defaults to UNBOUNDED if not specified.
OperatorSnapshotUtil writes v2 snapshots # Snapshots created with OperatorSnapshotUtil are now written in the savepoint format v2.
SBT projects and the MiniClusterResource # If you have a sbt project which uses the MiniClusterResource, you now have to add the flink-runtime test-jar dependency explicitly via:
libraryDependencies += \u0026quot;org.apache.flink\u0026quot; %% \u0026quot;flink-runtime\u0026quot; % flinkVersion % Test classifier \u0026quot;tests\u0026quot;
The reason for this is that the MiniClusterResource has been moved from flink-test-utils to flink-runtime. The module flink-test-utils has correctly a test-jar dependency on flink-runtime. However, sbt does not properly pull in transitive test-jar dependencies as described in this sbt issue. Consequently, it is necessary to specify the test-jar dependency explicitly.
Back to top
`}),e.add({id:320,href:"/flink/flink-docs-master/zh/release-notes/flink-1.8/",title:"Release Notes - Flink 1.8",section:"Release-notes",content:` Release Notes - Flink 1.8 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.7 and Flink 1.8. Please read these notes carefully if you are planning to upgrade your Flink version to 1.8.
State # Continuous incremental cleanup of old Keyed State with TTL # We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (FLINK-9510). This feature allowed to clean up and make inaccessible keyed state entries when accessing them. In addition state would now also being cleaned up when writing a savepoint/checkpoint.
Flink 1.8 introduces continuous cleanup of old entries for both the RocksDB state backend (FLINK-10471) and the heap state backend (FLINK-10473). This means that old entries (according to the ttl setting) are continuously being cleanup up.
New Support for Schema Migration when restoring Savepoints # With Flink 1.7.0 we added support for changing the schema of state when using the AvroSerializer (FLINK-10605). With Flink 1.8.0 we made great progress migrating all built-in TypeSerializers to a new serializer snapshot abstraction that theoretically allows schema migration. Of the serializers that come with Flink, we now support schema migration for the PojoSerializer (FLINK-11485), and Java EnumSerializer (FLINK-11334), As well as for Kryo in limited cases (FLINK-11323).
Savepoint compatibility # Savepoints from Flink 1.2 that contain a Scala TraversableSerializer are not compatible with Flink 1.8 anymore because of an update in this serializer (FLINK-11539). You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8.
RocksDB version bump and switch to FRocksDB (FLINK-10471) # We needed to switch to a custom build of RocksDB called FRocksDB because we need certain changes in RocksDB for supporting continuous state cleanup with TTL. The used build of FRocksDB is based on the upgraded version 5.17.2 of RocksDB. For Mac OS X, RocksDB version 5.17.2 is supported only for OS X version \u0026gt;= 10.13. See also: https://github.com/facebook/rocksdb/issues/4862.
Maven Dependencies # Changes to bundling of Hadoop libraries with Flink (FLINK-11266) # Convenience binaries that include hadoop are no longer released.
If a deployment relies on flink-shaded-hadoop2 being included in flink-dist, then you must manually download a pre-packaged Hadoop jar from the optional components section of the download page and copy it into the /lib directory. Alternatively, a Flink distribution that includes hadoop can be built by packaging flink-dist and activating the include-hadoop maven profile.
As hadoop is no longer included in flink-dist by default, specifying -DwithoutHadoop when packaging flink-dist no longer impacts the build.
Configuration # TaskManager configuration (FLINK-11716) # TaskManagers now bind to the host IP address instead of the hostname by default . This behaviour can be controlled by the configuration option taskmanager.network.bind-policy. If your Flink cluster should experience inexplicable connection problems after upgrading, try to set taskmanager.network.bind-policy: name in your flink-conf.yaml to return to the pre-1.8 behaviour.
Table API # Deprecation of direct Table constructor usage (FLINK-11447) # Flink 1.8 deprecates direct usage of the constructor of the Table class in the Table API. This constructor would previously be used to perform a join with a lateral table. You should now use table.joinLateral() or table.leftOuterJoinLateral() instead.
This change is necessary for converting the Table class into an interface, which will make the API more maintainable and cleaner in the future.
Introduction of new CSV format descriptor (FLINK-9964) # This release introduces a new format descriptor for CSV files that is compliant with RFC 4180. The new descriptor is available as org.apache.flink.table.descriptors.Csv. For now, this can only be used together with the Kafka connector. The old descriptor is available as org.apache.flink.table.descriptors.OldCsv for use with file system connectors.
Deprecation of static builder methods on TableEnvironment (FLINK-11445) # In order to separate API from actual implementation, the static methods TableEnvironment.getTableEnvironment() are deprecated. You should now use Batch/StreamTableEnvironment.create() instead.
Change in the Maven modules of Table API (FLINK-11064) # Users that had a flink-table dependency before, need to update their dependencies to flink-table-planner and the correct dependency of flink-table-api-*, depending on whether Java or Scala is used: one of flink-table-api-java-bridge or flink-table-api-scala-bridge.
Change to External Catalog Table Builders (FLINK-11522) # ExternalCatalogTable.builder() is deprecated in favour of ExternalCatalogTableBuilder().
Change to naming of Table API connector jars (FLINK-11026) # The naming scheme for kafka/elasticsearch6 sql-jars has been changed.
In maven terms, they no longer have the sql-jar qualifier and the artifactId is now prefixed with flink-sql instead of flink, e.g., flink-sql-connector-kafka....
Change to how Null Literals are specified (FLINK-11785) # Null literals in the Table API need to be defined with nullOf(type) instead of Null(type) from now on. The old approach is deprecated.
Connectors # Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (FLINK-8354) # For the Flink KafkaConsumers, we introduced a new KafkaDeserializationSchema that gives direct access to the Kafka ConsumerRecord. This subsumes the KeyedSerializationSchema functionality, which is deprecated but still available for now.
FlinkKafkaConsumer will now filter restored partitions based on topic specification (FLINK-10342) # Starting from Flink 1.8.0, the FlinkKafkaConsumer now always filters out restored partitions that are no longer associated with a specified topic to subscribe to in the restored execution. This behaviour did not exist in previous versions of the FlinkKafkaConsumer. If you wish to retain the previous behaviour, please use the disableFilterRestoredPartitionsWithSubscribedTopics() configuration method on the FlinkKafkaConsumer.
Consider this example: if you had a Kafka Consumer that was consuming from topic A, you did a savepoint, then changed your Kafka consumer to instead consume from topic B, and then restarted your job from the savepoint. Before this change, your consumer would now consume from both topic A and B because it was stored in state that the consumer was consuming from topic A. With the change, your consumer would only consume from topic B after restore because we filter the topics that are stored in state using the configured topics.
Miscellaneous Interface changes # The canEqual() method was dropped from the TypeSerializer interface (FLINK-9803) # The canEqual() methods are usually used to make proper equality checks across hierarchies of types. The TypeSerializer actually doesn\u0026rsquo;t require this property, so the method is now removed.
Removal of the CompositeSerializerSnapshot utility class (FLINK-11073) # The CompositeSerializerSnapshot utility class has been removed. You should now use CompositeTypeSerializerSnapshot instead, for snapshots of composite serializers that delegate serialization to multiple nested serializers. Please see here for instructions on using CompositeTypeSerializerSnapshot.
Memory management # In Flink 1.8.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.
Back to top
`}),e.add({id:321,href:"/flink/flink-docs-master/zh/release-notes/flink-1.9/",title:"Release Notes - Flink 1.9",section:"Release-notes",content:` Release Notes - Flink 1.9 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.8 and Flink 1.9. It also provides an overview on known shortcoming or limitations with new experimental features introduced in 1.9.
Please read these notes carefully if you are planning to upgrade your Flink version to 1.9.
Known shortcomings or limitations for new features # New Table / SQL Blink planner # Flink 1.9.0 provides support for two planners for the Table API, namely Flink\u0026rsquo;s original planner and the new Blink planner. The original planner maintains same behaviour as previous releases, while the new Blink planner is still considered experimental and has the following limitations:
The Blink planner can not be used with BatchTableEnvironment, and therefore Table programs ran with the planner can not be transformed to DataSet programs. This is by design and will also not be supported in the future. Therefore, if you want to run a batch job with the Blink planner, please use the new TableEnvironment. For streaming jobs, both StreamTableEnvironment and TableEnvironment works. Implementations of StreamTableSink should implement the consumeDataStream method instead of emitDataStream if it is used with the Blink planner. Both methods work with the original planner. This is by design to make the returned DataStreamSink accessible for the planner. Due to a bug with how transformations are not being cleared on execution, TableEnvironment instances should not be reused across multiple SQL statements when using the Blink planner. Table.flatAggregate is not supported Session and count windows are not supported when running batch jobs. The Blink planner only supports the new Catalog API, and does not support ExternalCatalog which is now deprecated. Related issues:
FLINK-13708: Transformations should be cleared because a table environment could execute multiple job FLINK-13473: Add GroupWindowed FlatAggregate support to stream Table API (Blink planner), i.e, align with Flink planner FLINK-13735: Support session window with Blink planner in batch mode FLINK-13736: Support count window with Blink planner in batch mode SQL DDL # In Flink 1.9.0, the community also added a preview feature about SQL DDL, but only for batch style DDLs. Therefore, all streaming related concepts are not supported yet, for example watermarks.
Related issues:
FLINK-13661: Add a stream specific CREATE TABLE SQL DDL FLINK-13568: DDL create table doesn\u0026rsquo;t allow STRING data type Java 9 support # Since Flink 1.9.0, Flink can now be compiled and run on Java 9. Note that certain components interacting with external systems (connectors, filesystems, metric reporters, etc.) may not work since the respective projects may have skipped Java 9 support.
Related issues:
FLINK-8033: JDK 9 support Memory management # In Flink 1.9.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.
Related issues:
FLINK-14123: Lower the default value of taskmanager.memory.fraction Deprecations and breaking changes # Scala expression DSL for Table API moved to flink-table-api-scala # Since 1.9.0, the implicit conversions for the Scala expression DSL for the Table API has been moved to flink-table-api-scala. This requires users to update the imports in their Table programs.
Users of pure Table programs should define their imports like:
import org.apache.flink.table.api._ TableEnvironment.create(...) Users of the DataStream API should define their imports like:
import org.apache.flink.table.api._ import org.apache.flink.table.api.scala._ StreamTableEnvironment.create(...) Related issues:
FLINK-13045: Move Scala expression DSL to flink-table-api-scala Failover strategies # As a result of completing fine-grained recovery (FLIP-1), Flink will now attempt to only restart tasks that are connected to failed tasks through a pipelined connection. By default, the region failover strategy is used.
Users who were not using a restart strategy or have already configured a failover strategy should not be affected. Moreover, users who already enabled the region failover strategy, along with a restart strategy that enforces a certain number of restarts or introduces a restart delay, will see changes in behavior. The region failover strategy now correctly respects constraints that are defined by the restart strategy.
Streaming users who were not using a failover strategy may be affected if their jobs are embarrassingly parallel or contain multiple independent jobs. In this case, only the failed parallel pipeline or affected jobs will be restarted.
Batch users may be affected if their job contains blocking exchanges (usually happens for shuffles) or the ExecutionMode was set to BATCH or BATCH_FORCED via the ExecutionConfig.
Overall, users should see an improvement in performance.
Related issues:
FLINK-13223: Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml FLINK-13060: FailoverStrategies should respect restart constraints Job termination via CLI # With the support of graceful job termination with savepoints for semantic correctness (FLIP-34), a few changes related to job termination has been made to the CLI.
From now on, the stop command with no further arguments stops the job with a savepoint targeted at the default savepoint location (as configured via the state.savepoints.dir property in the job configuration), or a location explicitly specified using the -p \u0026lt;savepoint-path\u0026gt; option. Please make sure to configure the savepoint path using either one of these options.
Since job terminations are now always accompanied with a savepoint, stopping jobs is expected to take longer now.
Related issues:
FLINK-13123: Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation FLINK-11458: Add TERMINATE/SUSPEND Job with Savepoint Network stack # A few changes in the network stack related to changes in the threading model of StreamTask to a mailbox-based approach requires close attention to some related configuration:
Due to changes in the lifecycle management of result partitions, partition requests as well as re-triggers will now happen sooner. Therefore, it is possible that some jobs with long deployment times and large state might start failing more frequently with PartitionNotFound exceptions compared to previous versions. If that\u0026rsquo;s the case, users should increase the value of taskmanager.network.request-backoff.max in order to have the same effective partition request timeout as it was prior to 1.9.0.
To avoid a potential deadlock, a timeout has been added for how long a task will wait for assignment of exclusive memory segments. The default timeout is 30 seconds, and is configurable via taskmanager.network.memory.exclusive-buffers-request-timeout-ms. It is possible that for some previously working deployments this default timeout value is too low and might have to be increased.
Please also notice that several network I/O metrics have had their scope changed. See the 1.9 metrics documentation for which metrics are affected. In 1.9.0, these metrics will still be available under their previous scopes, but this may no longer be the case in future versions.
Related issues:
FLINK-13013: Make sure that SingleInputGate can always request partitions FLINK-12852: Deadlock occurs when requiring exclusive buffer for RemoteInputChannel FLINK-12555: Introduce an encapsulated metric group layout for shuffle API and deprecate old one AsyncIO # Due to a bug in the AsyncWaitOperator, in 1.9.0 the default chaining behaviour of the operator is now changed so that it is never chained after another operator. This should not be problematic for migrating from older version snapshots as long as an uid was assigned to the operator. If an uid was not assigned to the operator, please see the instructions here for a possible workaround.
Related issues:
FLINK-13063: AsyncWaitOperator shouldn\u0026rsquo;t be releasing checkpointingLock Connectors and Libraries # Introduced KafkaSerializationSchema to fully replace KeyedSerializationSchema # The universal FlinkKafkaProducer (in flink-connector-kafka) supports a new KafkaSerializationSchema that will fully replace KeyedSerializationSchema in the long run. This new schema allows directly generating Kafka ProducerRecords for sending to Kafka, therefore enabling the user to use all available Kafka features (in the context of Kafka records).
Dropped connectors and libraries # The Elasticsearch 1 connector has been dropped and will no longer receive patches. Users may continue to use the connector from a previous series (like 1.8) with newer versions of Flink. It is being dropped due to being used significantly less than more recent versions (Elasticsearch versions 2.x and 5.x are downloaded 4 to 5 times more), and hasn\u0026rsquo;t seen any development for over a year.
The older Python APIs for batch and streaming have been removed and will no longer receive new patches. A new API is being developed based on the Table API as part of FLINK-12308: Support python language in Flink Table API. Existing users may continue to use these older APIs with future versions of Flink by copying both the flink-streaming-python and flink-python jars into the /lib directory of the distribution and the corresponding start scripts pyflink-stream.sh and pyflink.sh into the /bin directory of the distribution.
The older machine learning libraries have been removed and will no longer receive new patches. This is due to efforts towards a new Table-based machine learning library (FLIP-39). Users can still use the 1.8 version of the legacy library if their projects still rely on it.
Related issues:
FLINK-11693: Add KafkaSerializationSchema that directly uses ProducerRecord FLINK-12151: Drop Elasticsearch 1 connector FLINK-12903: Remove legacy flink-python APIs FLINK-12308: Support python language in Flink Table API FLINK-12597: Remove the legacy flink-libraries/flink-ml MapR dependency removed # Dependency on MapR vendor-specific artifacts has been removed, by changing the MapR filesystem connector to work purely based on reflection. This does not introduce any regression in the support for the MapR filesystem. The decision to remove hard dependencies on the MapR artifacts was made due to very flaky access to the secure https endpoint of the MapR artifact repository, and affected build stability of Flink.
Related issues:
FLINK-12578: Use secure URLs for Maven repositories FLINK-13499: Remove dependency on MapR artifact repository StateDescriptor interface change # Access to the state serializer in StateDescriptor is now modified from protected to private access. Subclasses should use the StateDescriptor#getSerializer() method as the only means to obtain the wrapped state serializer.
Related issues:
FLINK-12688: Make serializer lazy initialization thread safe in StateDescriptor Web UI dashboard # The web frontend of Flink has been updated to use the latest Angular version (7.x). The old frontend remains available in Flink 1.9.x, but will be removed in a later Flink release once the new frontend is considered stable.
Related issues:
FLINK-10705: Rework Flink Web Dashoard `}),e.add({id:322,href:"/flink/flink-docs-master/zh/versions/",title:"Versions",section:"Apache Flink Documentation",content:` Versions # An appendix of hosted documentation for all versions of Apache Flink.
v1.15 v1.14 v1.13 v1.12 v1.11 v1.10 v1.9 v1.8 v1.7 v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 `})})(),!function(e,t){"use strict";"object"==typeof module&&"object"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error("jQuery requires a window with a document");return t(e)}:t(e)}("undefined"!=typeof window?window:this,function(e,t){"use strict";var n,l,h,b,S,T,z,N,R,$,K,G,J,g=[],yt=Object.getPrototypeOf,j=g.slice,bt=g.flat?function(e){return g.flat.call(e)}:function(e){return g.concat.apply([],e)},_e=g.push,U=g.indexOf,Y={},vt=Y.toString,X=Y.hasOwnProperty,gt=X.toString,Gt=gt.call(Object),a={},o=function(e){return"function"==typeof e&&"number"!=typeof e.nodeType&&"function"!=typeof e.item},O=function(e){return e!=null&&e===e.window},i=e.document,Yt={type:!0,src:!0,nonce:!0,noModule:!0},he,se,it,st,nt,tt,Ge,ce,Fe,ye,ot,_t,at,Ot,ft,pt,jt,Oe,le,Ue,qe,xe,ut,mt,Be;function Ce(e,t,n){var s,a,o=(n=n||i).createElement("script");if(o.text=e,t)for(s in Yt)(a=t[s]||t.getAttribute&&t.getAttribute(s))&&o.setAttribute(s,a);n.head.appendChild(o).parentNode.removeChild(o)}function M(e){return e==null?e+"":"object"==typeof e||"function"==typeof e?Y[vt.call(e)]||"object":typeof e}he="3.6.1",n=function(e,t){return new n.fn.init(e,t)};function de(e){var t=!!e&&"length"in e&&e.length,n=M(e);return!o(e)&&!O(e)&&("array"===n||0===t||"number"==typeof t&&0<t&&t-1 in e)}n.fn=n.prototype={jquery:he,constructor:n,length:0,toArray:function(){return j.call(this)},get:function(e){return e==null?j.call(this):e<0?this[e+this.length]:this[e]},pushStack:function(e){var t=n.merge(this.constructor(),e);return t.prevObject=this,t},each:function(e){return n.each(this,e)},map:function(e){return this.pushStack(n.map(this,function(t,n){return e.call(t,n,t)}))},slice:function(){return this.pushStack(j.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},even:function(){return this.pushStack(n.grep(this,function(e,t){return(t+1)%2}))},odd:function(){return this.pushStack(n.grep(this,function(e,t){return t%2}))},eq:function(e){var n=this.length,t=+e+(e<0?n:0);return this.pushStack(0<=t&&t<n?[this[t]]:[])},end:function(){return this.prevObject||this.constructor()},push:_e,sort:g.sort,splice:g.splice},n.extend=n.fn.extend=function(){var t,i,a,r,c,d,e=arguments[0]||{},s=1,u=arguments.length,l=!1;for("boolean"==typeof e&&(l=e,e=arguments[s]||{},s++),"object"==typeof e||o(e)||(e={}),s===u&&(e=this,s--);s<u;s++)if(null!=(c=arguments[s]))for(i in c)t=c[i],"__proto__"!==i&&e!==t&&(l&&t&&(n.isPlainObject(t)||(r=Array.isArray(t)))?(a=e[i],d=r&&!Array.isArray(a)?[]:r||n.isPlainObject(a)?a:{},r=!1,e[i]=n.extend(l,d,t)):void 0!==t&&(e[i]=t));return e},n.extend({expando:"jQuery"+(he+Math.random()).replace(/\D/g,""),isReady:!0,error:function(e){throw new Error(e)},noop:function(){},isPlainObject:function(e){var t,n;return!!e&&"[object Object]"===vt.call(e)&&(!(t=yt(e))||"function"==typeof(n=X.call(t,"constructor")&&t.constructor)&&gt.call(n)===Gt)},isEmptyObject:function(e){var t;for(t in e)return!1;return!0},globalEval:function(e,t,n){Ce(e,{nonce:t&&t.nonce},n)},each:function(e,t){var s,n=0;if(de(e)){for(s=e.length;n<s;n++)if(!1===t.call(e[n],n,e[n]))break}else for(n in e)if(!1===t.call(e[n],n,e[n]))break;return e},makeArray:function(e,t){var s=t||[];return e!=null&&(de(Object(e))?n.merge(s,"string"==typeof e?[e]:e):_e.call(s,e)),s},inArray:function(e,t,n){return t==null?-1:U.call(t,e,n)},merge:function(e,t){for(var o=+t.length,n=0,s=e.length;n<o;n++)e[s++]=t[n];return e.length=s,e},grep:function(e,t,n){for(var o=[],s=0,i=e.length,a=!n;s<i;s++)!t(e[s],s)!==a&&o.push(e[s]);return o},map:function(e,t,n){var o,a,s=0,i=[];if(de(e))for(a=e.length;s<a;s++)null!=(o=t(e[s],s,n))&&i.push(o);else for(s in e)null!=(o=t(e[s],s,n))&&i.push(o);return bt(i)},guid:1,support:a}),"function"==typeof Symbol&&(n.fn[Symbol.iterator]=g[Symbol.iterator]),n.each("Boolean Number String Function Array Date RegExp Object Error Symbol".split(" "),function(e,t){Y["[object "+t+"]"]=t.toLowerCase()}),b=function(e){var t,n,i,r,c,l,g,b,j,x,C,k,A,F,N,R,I,ae,te,a="sizzle"+1*new Date,d=e.document,p=0,Oe=0,ne=P(),J=P(),Z=P(),z=P(),Y=function(e,t){return e===t&&(x=!0),0},we={}.hasOwnProperty,y=[],xe=y.pop,_e=y.push,v=y.push,ee=y.slice,w=function(e,t){for(var n=0,s=e.length;n<s;n++)if(e[n]===t)return n;return-1},B="checked|selected|async|autofocus|autoplay|controls|defer|disabled|hidden|ismap|loop|multiple|open|readonly|required|scoped",s=`[\\x20\\t\\r\\n\\f]`,O="(?:\\\\[\\da-fA-F]{1,6}"+s+`?|\\\\[^\\r\\n\\f]|[\\w-]|[^\0-\\x7f])+`,re="\\["+s+"*("+O+")(?:"+s+"*([*^$|!~]?=)"+s+`*(?:'((?:\\\\.|[^\\\\'])*)'|"((?:\\\\.|[^\\\\"])*)"|(`+O+"))|)"+s+"*\\]",U=":("+O+`)(?:\\((('((?:\\\\.|[^\\\\'])*)'|"((?:\\\\.|[^\\\\"])*)")|((?:\\\\.|[^\\\\()[\\]]|`+re+")*)|.*)\\)|)",ye=new RegExp(s+"+","g"),D=new RegExp("^"+s+"+|((?:^|[^\\\\])(?:\\\\.)*)"+s+"+$","g"),je=new RegExp("^"+s+"*,"+s+"*"),Q=new RegExp("^"+s+"*([>+~]|"+s+")"+s+"*"),be=new RegExp(s+"|>"),ve=new RegExp(U),ge=new RegExp("^"+O+"$"),S={ID:new RegExp("^#("+O+")"),CLASS:new RegExp("^\\.("+O+")"),TAG:new RegExp("^("+O+"|[*])"),ATTR:new RegExp("^"+re),PSEUDO:new RegExp("^"+U),CHILD:new RegExp("^:(only|first|last|nth|nth-last)-(child|of-type)(?:\\("+s+"*(even|odd|(([+-]|)(\\d*)n|)"+s+"*(?:([+-]|)"+s+"*(\\d+)|))"+s+"*\\)|)","i"),bool:new RegExp("^(?:"+B+")$","i"),needsContext:new RegExp("^"+s+"*[>+~]|:(even|odd|eq|gt|lt|nth|first|last)(?:\\("+s+"*((?:-\\d)?\\d*)"+s+"*\\)|)(?=[^-]|$)","i")},pe=/HTML$/i,he=/^(?:input|select|textarea|button)$/i,ue=/^h\d$/i,E=/^[^{]+\{\s*\[native \w/,de=/^(?:#([\w-]+)|(\w+)|\.([\w-]+))$/,q=/[+~]/,f=new RegExp("\\\\[\\da-fA-F]{1,6}"+s+`?|\\\\([^\\r\\n\\f])`,"g"),m=function(e,t){var n="0x"+e.slice(1)-65536;return t||(n<0?String.fromCharCode(n+65536):String.fromCharCode(n>>10|55296,1023&n|56320))},se=/([\0-\x1f\x7f]|^-?\d)|^-$|[^\0-\x1f\x7f-\uFFFF\w-]/g,oe=function(e,t){return t?"\0"===e?"\ufffd":e.slice(0,-1)+"\\"+e.charCodeAt(e.length-1).toString(16)+" ":"\\"+e},ie=function(){g()},le=L(function(e){return!0===e.disabled&&"fieldset"===e.nodeName.toLowerCase()},{dir:"parentNode",next:"legend"});try{v.apply(y=ee.call(d.childNodes),d.childNodes),y[d.childNodes.length].nodeType}catch{v={apply:y.length?function(e,t){_e.apply(e,ee.call(t))}:function(e,t){for(var n=e.length,s=0;e[n++]=t[s++];);e.length=n-1}}}function o(e,t,s,o){var c,d,u,f,p,b,j,m=t&&t.ownerDocument,h=t?t.nodeType:9;if(s=s||[],"string"!=typeof e||!e||1!==h&&9!==h&&11!==h)return s;if(!o&&(g(t),t=t||n,l)){if(11!==h&&(f=de.exec(e)))if(c=f[1]){if(9===h){{if(!(d=t.getElementById(c)))return s;if(d.id===c)return s.push(d),s}}else if(m&&(d=m.getElementById(c))&&A(t,d)&&d.id===c)return s.push(d),s}else{if(f[2])return v.apply(s,t.getElementsByTagName(e)),s;if((c=f[3])&&i.getElementsByClassName&&t.getElementsByClassName)return v.apply(s,t.getElementsByClassName(c)),s}if(i.qsa&&!z[e+" "]&&(!r||!r.test(e))&&(1!==h||"object"!==t.nodeName.toLowerCase())){if(j=e,m=t,1===h&&(be.test(e)||Q.test(e))){for((m=q.test(e)&&K(t.parentNode)||t)===t&&i.scope||((u=t.getAttribute("id"))?u=u.replace(se,oe):t.setAttribute("id",u=a)),b=(p=k(e)).length;b--;)p[b]=(u?"#"+u:":scope")+" "+T(p[b]);j=p.join(",")}try{return v.apply(s,m.querySelectorAll(j)),s}catch{z(e,!0)}finally{u===a&&t.removeAttribute("id")}}}return te(e.replace(D,"$1"),t,s,o)}function P(){var e=[];return function n(s,o){return e.push(s+" ")>t.cacheLength&&delete n[e.shift()],n[s+" "]=o}}function u(e){return e[a]=!0,e}function h(e){var t=n.createElement("fieldset");try{return!!e(t)}catch{return!1}finally{t.parentNode&&t.parentNode.removeChild(t),t=null}}function W(e,n){for(var s=e.split("|"),o=s.length;o--;)t.attrHandle[s[o]]=n}function G(e,t){var n=t&&e,s=n&&1===e.nodeType&&1===t.nodeType&&e.sourceIndex-t.sourceIndex;if(s)return s;if(n)for(;n=n.nextSibling;)if(n===t)return-1;return e?1:-1}function me(e){return function(t){return"input"===t.nodeName.toLowerCase()&&t.type===e}}function fe(e){return function(t){var n=t.nodeName.toLowerCase();return("input"===n||"button"===n)&&t.type===e}}function ce(e){return function(t){return"form"in t?t.parentNode&&!1===t.disabled?"label"in t?"label"in t.parentNode?t.parentNode.disabled===e:t.disabled===e:t.isDisabled===e||t.isDisabled!==!e&&le(t)===e:t.disabled===e:"label"in t&&t.disabled===e}}function _(e){return u(function(t){return t=+t,u(function(n,s){for(var o,i=e([],n.length,t),a=i.length;a--;)n[o=i[a]]&&(n[o]=!(s[o]=n[o]))})})}function K(e){return e&&"undefined"!=typeof e.getElementsByTagName&&e}for(C in i=o.support={},ae=o.isXML=function(e){var n=e&&e.namespaceURI,t=e&&(e.ownerDocument||e).documentElement;return!pe.test(n||t&&t.nodeName||"HTML")},g=o.setDocument=function(e){var o,p,u=e?e.ownerDocument||e:d;return u!=n&&9===u.nodeType&&u.documentElement&&(c=(n=u).documentElement,l=!ae(n),d!=n&&(o=n.defaultView)&&o.top!==o&&(o.addEventListener?o.addEventListener("unload",ie,!1):o.attachEvent&&o.attachEvent("onunload",ie)),i.scope=h(function(e){return c.appendChild(e).appendChild(n.createElement("div")),"undefined"!=typeof e.querySelectorAll&&!e.querySelectorAll(":scope fieldset div").length}),i.attributes=h(function(e){return e.className="i",!e.getAttribute("className")}),i.getElementsByTagName=h(function(e){return e.appendChild(n.createComment("")),!e.getElementsByTagName("*").length}),i.getElementsByClassName=E.test(n.getElementsByClassName),i.getById=h(function(e){return c.appendChild(e).id=a,!n.getElementsByName||!n.getElementsByName(a).length}),i.getById?(t.filter.ID=function(e){var t=e.replace(f,m);return function(e){return e.getAttribute("id")===t}},t.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&l){var n=t.getElementById(e);return n?[n]:[]}}):(t.filter.ID=function(e){var t=e.replace(f,m);return function(e){var n="undefined"!=typeof e.getAttributeNode&&e.getAttributeNode("id");return n&&n.value===t}},t.find.ID=function(e,t){if("undefined"!=typeof t.getElementById&&l){var s,o,i,n=t.getElementById(e);if(n){if((s=n.getAttributeNode("id"))&&s.value===e)return[n];for(i=t.getElementsByName(e),o=0;n=i[o++];)if((s=n.getAttributeNode("id"))&&s.value===e)return[n]}return[]}}),t.find.TAG=i.getElementsByTagName?function(e,t){return"undefined"!=typeof t.getElementsByTagName?t.getElementsByTagName(e):i.qsa?t.querySelectorAll(e):void 0}:function(e,t){var n,s=[],i=0,o=t.getElementsByTagName(e);if("*"===e){for(;n=o[i++];)1===n.nodeType&&s.push(n);return s}return o},t.find.CLASS=i.getElementsByClassName&&function(e,t){if("undefined"!=typeof t.getElementsByClassName&&l)return t.getElementsByClassName(e)},j=[],r=[],(i.qsa=E.test(n.querySelectorAll))&&(h(function(e){var t;c.appendChild(e).innerHTML="<a id='"+a+"'></a><select id='"+a+`-\\' msallowcapture=''><option selected=''></option></select>`,e.querySelectorAll("[msallowcapture^='']").length&&r.push("[*^$]="+s+`*(?:''|"")`),e.querySelectorAll("[selected]").length||r.push("\\["+s+"*(?:value|"+B+")"),e.querySelectorAll("[id~="+a+"-]").length||r.push("~="),(t=n.createElement("input")).setAttribute("name",""),e.appendChild(t),e.querySelectorAll("[name='']").length||r.push("\\["+s+"*name"+s+"*="+s+`*(?:''|"")`),e.querySelectorAll(":checked").length||r.push(":checked"),e.querySelectorAll("a#"+a+"+*").length||r.push(".#.+[+~]"),e.querySelectorAll("\\"),r.push(`[\\r\\n\\f]`)}),h(function(e){e.innerHTML="<a href='' disabled='disabled'></a><select disabled='disabled'><option/></select>";var t=n.createElement("input");t.setAttribute("type","hidden"),e.appendChild(t).setAttribute("name","D"),e.querySelectorAll("[name=d]").length&&r.push("name"+s+"*[*^$|!~]?="),2!==e.querySelectorAll(":enabled").length&&r.push(":enabled",":disabled"),c.appendChild(e).disabled=!0,2!==e.querySelectorAll(":disabled").length&&r.push(":enabled",":disabled"),e.querySelectorAll("*,:x"),r.push(",.*:")})),(i.matchesSelector=E.test(F=c.matches||c.webkitMatchesSelector||c.mozMatchesSelector||c.oMatchesSelector||c.msMatchesSelector))&&h(function(e){i.disconnectedMatch=F.call(e,"*"),F.call(e,"[s!='']:x"),j.push("!=",U)}),r=r.length&&new RegExp(r.join("|")),j=j.length&&new RegExp(j.join("|")),p=E.test(c.compareDocumentPosition),A=p||E.test(c.contains)?function(e,t){var s=9===e.nodeType?e.documentElement:e,n=t&&t.parentNode;return e===n||!(!n||1!==n.nodeType||!(s.contains?s.contains(n):e.compareDocumentPosition&&16&e.compareDocumentPosition(n)))}:function(e,t){if(t)for(;t=t.parentNode;)if(t===e)return!0;return!1},Y=p?function(e,t){if(e===t)return x=!0,0;var s=!e.compareDocumentPosition-!t.compareDocumentPosition;return s||(1&(s=(e.ownerDocument||e)==(t.ownerDocument||t)?e.compareDocumentPosition(t):1)||!i.sortDetached&&t.compareDocumentPosition(e)===s?e==n||e.ownerDocument==d&&A(d,e)?-1:t==n||t.ownerDocument==d&&A(d,t)?1:b?w(b,e)-w(b,t):0:4&s?-1:1)}:function(e,t){if(e===t)return x=!0,0;var s,o=0,r=e.parentNode,c=t.parentNode,i=[e],a=[t];if(!r||!c)return e==n?-1:t==n?1:r?-1:c?1:b?w(b,e)-w(b,t):0;if(r===c)return G(e,t);for(s=e;s=s.parentNode;)i.unshift(s);for(s=t;s=s.parentNode;)a.unshift(s);for(;i[o]===a[o];)o++;return o?G(i[o],a[o]):i[o]==d?-1:a[o]==d?1:0}),n},o.matches=function(e,t){return o(e,null,null,t)},o.matchesSelector=function(e,t){if(g(e),i.matchesSelector&&l&&!z[t+" "]&&(!j||!j.test(t))&&(!r||!r.test(t)))try{var s=F.call(e,t);if(s||i.disconnectedMatch||e.document&&11!==e.document.nodeType)return s}catch{z(t,!0)}return 0<o(t,n,null,[e]).length},o.contains=function(e,t){return(e.ownerDocument||e)!=n&&g(e),A(e,t)},o.attr=function(e,s){(e.ownerDocument||e)!=n&&g(e);var a=t.attrHandle[s.toLowerCase()],o=a&&we.call(t.attrHandle,s.toLowerCase())?a(e,s,!l):void 0;return void 0!==o?o:i.attributes||!l?e.getAttribute(s):(o=e.getAttributeNode(s))&&o.specified?o.value:null},o.escape=function(e){return(e+"").replace(se,oe)},o.error=function(e){throw new Error("Syntax error, unrecognized expression: "+e)},o.uniqueSort=function(e){var s,o=[],t=0,n=0;if(x=!i.detectDuplicates,b=!i.sortStable&&e.slice(0),e.sort(Y),x){for(;s=e[n++];)s===e[n]&&(t=o.push(n));for(;t--;)e.splice(o[t],1)}return b=null,e},N=o.getText=function(e){var s,n="",o=0,t=e.nodeType;if(t){if(1===t||9===t||11===t){if("string"==typeof e.textContent)return e.textContent;for(e=e.firstChild;e;e=e.nextSibling)n+=N(e)}else if(3===t||4===t)return e.nodeValue}else for(;s=e[o++];)n+=N(s);return n},(t=o.selectors={cacheLength:50,createPseudo:u,match:S,attrHandle:{},find:{},relative:{">":{dir:"parentNode",first:!0}," ":{dir:"parentNode"},"+":{dir:"previousSibling",first:!0},"~":{dir:"previousSibling"}},preFilter:{ATTR:function(e){return e[1]=e[1].replace(f,m),e[3]=(e[3]||e[4]||e[5]||"").replace(f,m),"~="===e[2]&&(e[3]=" "+e[3]+" "),e.slice(0,4)},CHILD:function(e){return e[1]=e[1].toLowerCase(),"nth"===e[1].slice(0,3)?(e[3]||o.error(e[0]),e[4]=+(e[4]?e[5]+(e[6]||1):2*("even"===e[3]||"odd"===e[3])),e[5]=+(e[7]+e[8]||"odd"===e[3])):e[3]&&o.error(e[0]),e},PSEUDO:function(e){var n,t=!e[6]&&e[2];return S.CHILD.test(e[0])?null:(e[3]?e[2]=e[4]||e[5]||"":t&&ve.test(t)&&(n=k(t,!0))&&(n=t.indexOf(")",t.length-n)-t.length)&&(e[0]=e[0].slice(0,n),e[2]=t.slice(0,n)),e.slice(0,3))}},filter:{TAG:function(e){var t=e.replace(f,m).toLowerCase();return"*"===e?function(){return!0}:function(e){return e.nodeName&&e.nodeName.toLowerCase()===t}},CLASS:function(e){var t=ne[e+" "];return t||(t=new RegExp("(^|"+s+")"+e+"("+s+"|$)"))&&ne(e,function(e){return t.test("string"==typeof e.className&&e.className||"undefined"!=typeof e.getAttribute&&e.getAttribute("class")||"")})},ATTR:function(e,t,n){return function(s){var i=o.attr(s,e);return i==null?"!="===t:!t||(i+="","="===t?i===n:"!="===t?i!==n:"^="===t?n&&0===i.indexOf(n):"*="===t?n&&-1<i.indexOf(n):"$="===t?n&&i.slice(-n.length)===n:"~="===t?-1<(" "+i.replace(ye," ")+" ").indexOf(n):"|="===t&&(i===n||i.slice(0,n.length+1)===n+"-"))}},CHILD:function(e,t,n,s,o){var c="nth"!==e.slice(0,3),r="last"!==e.slice(-4),i="of-type"===t;return 1===s&&0===o?function(e){return!!e.parentNode}:function(t,n,l){var d,h,m,f,v,j,b=c!==r?"nextSibling":"previousSibling",g=t.parentNode,_=i&&t.nodeName.toLowerCase(),y=!l&&!i,u=!1;if(g){if(c){for(;b;){for(d=t;d=d[b];)if(i?d.nodeName.toLowerCase()===_:1===d.nodeType)return!1;v=b="only"===e&&!v&&"nextSibling"}return!0}if(v=[r?g.firstChild:g.lastChild],r&&y){for(u=(h=(f=(j=(m=(d=g)[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]||[])[0]===p&&f[1])&&f[2],d=h&&g.childNodes[h];d=++h&&d&&d[b]||(u=h=0)||v.pop();)if(1===d.nodeType&&++u&&d===t){j[e]=[p,h,u];break}}else if(y&&(u=h=(f=(j=(m=(d=t)[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]||[])[0]===p&&f[1]),!1===u)for(;d=++h&&d&&d[b]||(u=h=0)||v.pop();)if((i?d.nodeName.toLowerCase()===_:1===d.nodeType)&&++u&&(y&&((j=(m=d[a]||(d[a]={}))[d.uniqueID]||(m[d.uniqueID]={}))[e]=[p,u]),d===t))break;return(u-=o)===s||u%s==0&&0<=u/s}}},PSEUDO:function(e,n){var i,s=t.pseudos[e]||t.setFilters[e.toLowerCase()]||o.error("unsupported pseudo: "+e);return s[a]?s(n):1<s.length?(i=[e,e,"",n],t.setFilters.hasOwnProperty(e.toLowerCase())?u(function(e,t){for(var a,o=s(e,n),i=o.length;i--;)e[a=w(e,o[i])]=!(t[a]=o[i])}):function(e){return s(e,0,i)}):s}},pseudos:{not:u(function(e){var t=[],s=[],n=I(e.replace(D,"$1"));return n[a]?u(function(e,t,s,o){for(var a,r=n(e,null,o,[]),i=e.length;i--;)(a=r[i])&&(e[i]=!(t[i]=a))}):function(e,o,i){return t[0]=e,n(t,null,i,s),t[0]=null,!s.pop()}}),has:u(function(e){return function(t){return 0<o(e,t).length}}),contains:u(function(e){return e=e.replace(f,m),function(t){return-1<(t.textContent||N(t)).indexOf(e)}}),lang:u(function(e){return ge.test(e||"")||o.error("unsupported lang: "+e),e=e.replace(f,m).toLowerCase(),function(t){var n;do if(n=l?t.lang:t.getAttribute("xml:lang")||t.getAttribute("lang"))return(n=n.toLowerCase())===e||0===n.indexOf(e+"-");while((t=t.parentNode)&&1===t.nodeType)return!1}}),target:function(t){var n=e.location&&e.location.hash;return n&&n.slice(1)===t.id},root:function(e){return e===c},focus:function(e){return e===n.activeElement&&(!n.hasFocus||n.hasFocus())&&!!(e.type||e.href||~e.tabIndex)},enabled:ce(!1),disabled:ce(!0),checked:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&!!e.checked||"option"===t&&!!e.selected},selected:function(e){return e.parentNode&&e.parentNode.selectedIndex,!0===e.selected},empty:function(e){for(e=e.firstChild;e;e=e.nextSibling)if(e.nodeType<6)return!1;return!0},parent:function(e){return!t.pseudos.empty(e)},header:function(e){return ue.test(e.nodeName)},input:function(e){return he.test(e.nodeName)},button:function(e){var t=e.nodeName.toLowerCase();return"input"===t&&"button"===e.type||"button"===t},text:function(e){var t;return"input"===e.nodeName.toLowerCase()&&"text"===e.type&&(null==(t=e.getAttribute("type"))||"text"===t.toLowerCase())},first:_(function(){return[0]}),last:_(function(e,t){return[t-1]}),eq:_(function(e,t,n){return[n<0?n+t:n]}),even:_(function(e,t){for(var n=0;n<t;n+=2)e.push(n);return e}),odd:_(function(e,t){for(var n=1;n<t;n+=2)e.push(n);return e}),lt:_(function(e,t,n){for(var s=n<0?n+t:t<n?t:n;0<=--s;)e.push(s);return e}),gt:_(function(e,t,n){for(var s=n<0?n+t:n;++s<t;)e.push(s);return e})}}).pseudos.nth=t.pseudos.eq,{radio:!0,checkbox:!0,file:!0,password:!0,image:!0})t.pseudos[C]=me(C);for(C in{submit:!0,reset:!0})t.pseudos[C]=fe(C);function X(){}function T(e){for(var t=0,s=e.length,n="";t<s;t++)n+=e[t].value;return n}function L(e,t,n){var s=t.dir,o=t.next,i=o||s,r=n&&"parentNode"===i,c=Oe++;return t.first?function(t,n,o){for(;t=t[s];)if(1===t.nodeType||r)return e(t,n,o);return!1}:function(t,n,l){var d,u,h,m=[p,c];if(l){for(;t=t[s];)if((1===t.nodeType||r)&&e(t,n,l))return!0}else for(;t=t[s];)if(1===t.nodeType||r)if(u=(h=t[a]||(t[a]={}))[t.uniqueID]||(h[t.uniqueID]={}),o&&o===t.nodeName.toLowerCase())t=t[s]||t;else{if((d=u[i])&&d[0]===p&&d[1]===c)return m[2]=d[2];if((u[i]=m)[2]=e(t,n,l))return!0}return!1}}function $(e){return 1<e.length?function(t,n,s){for(var o=e.length;o--;)if(!e[o](t,n,s))return!1;return!0}:e[0]}function M(e,t,n,s,o){for(var a,r=[],i=0,c=e.length,l=t!=null;i<c;i++)(a=e[i])&&(n&&!n(a,s,o)||(r.push(a),l&&t.push(i)));return r}function V(e,t,n,s,i,r){return s&&!s[a]&&(s=V(s)),i&&!i[a]&&(i=V(i,r)),u(function(a,r,c,l){var u,h,m,g=[],p=[],b=r.length,j=a||function(e,t,n){for(var s=0,i=t.length;s<i;s++)o(e,t[s],n);return n}(t||"*",c.nodeType?[c]:c,[]),f=!e||!a&&t?j:M(j,g,e,c,l),d=n?i||(a?e:b||s)?[]:r:f;if(n&&n(f,d,c,l),s)for(h=M(d,p),s(h,[],c,l),u=h.length;u--;)(m=h[u])&&(d[p[u]]=!(f[p[u]]=m));if(a){if(i||e){if(i){for(h=[],u=d.length;u--;)(m=d[u])&&h.push(f[u]=m);i(null,d=[],h,l)}for(u=d.length;u--;)(m=d[u])&&-1<(h=i?w(a,m):g[u])&&(a[h]=!(r[h]=m))}}else d=M(d===r?d.splice(b,d.length):d),i?i(null,r,d,l):v.apply(r,d)})}function H(e){for(var s,o,r,c=e.length,l=t.relative[e[0].type],d=l||t.relative[" "],n=l?1:0,u=L(function(e){return e===r},d,!0),h=L(function(e){return-1<w(r,e)},d,!0),i=[function(e,t,n){var s=!l&&(n||t!==R)||((r=t).nodeType?u(e,t,n):h(e,t,n));return r=null,s}];n<c;n++)if(o=t.relative[e[n].type])i=[L($(i),o)];else{if((o=t.filter[e[n].type].apply(null,e[n].matches))[a]){for(s=++n;s<c;s++)if(t.relative[e[s].type])break;return V(1<n&&$(i),1<n&&T(e.slice(0,n-1).concat({value:" "===e[n-2].type?"*":""})).replace(D,"$1"),o,n<s&&H(e.slice(n,s)),s<c&&H(e=e.slice(s)),s<c&&T(e))}i.push(o)}return $(i)}return X.prototype=t.filters=t.pseudos,t.setFilters=new X,k=o.tokenize=function(e,n){var s,i,a,r,c,l,d,u=J[e+" "];if(u)return n?0:u.slice(0);for(s=e,l=[],d=t.preFilter;s;){for(r in a&&!(i=je.exec(s))||(i&&(s=s.slice(i[0].length)||s),l.push(c=[])),a=!1,(i=Q.exec(s))&&(a=i.shift(),c.push({value:a,type:i[0].replace(D," ")}),s=s.slice(a.length)),t.filter)!(i=S[r].exec(s))||d[r]&&!(i=d[r](i))||(a=i.shift(),c.push({value:a,type:r,matches:i}),s=s.slice(a.length));if(!a)break}return n?s.length:s?o.error(e):J(e,l).slice(0)},I=o.compile=function(e,s){var r,c,d,h,m,f,b=[],j=[],i=Z[e+" "];if(!i){for(s||(s=k(e)),c=s.length;c--;)(i=H(s[c]))[a]?b.push(i):j.push(i);(i=Z(e,(d=j,r=0<(f=b).length,h=0<d.length,m=function(e,s,i,a,c){var u,j,_,y=0,m="0",w=e&&[],b=[],O=R,x=e||h&&t.find.TAG("*",c),C=p+=O==null?1:Math.random()||.1,E=x.length;for(c&&(R=s==n||s||c);m!==E&&null!=(u=x[m]);m++){if(h&&u){for(_=0,s||u.ownerDocument==n||(g(u),i=!l);j=d[_++];)if(j(u,s||n,i)){a.push(u);break}c&&(p=C)}r&&((u=!j&&u)&&y--,e&&w.push(u))}if(y+=m,r&&m!==y){for(_=0;j=f[_++];)j(w,b,s,i);if(e){if(0<y)for(;m--;)w[m]||b[m]||(b[m]=xe.call(a));b=M(b)}v.apply(a,b),c&&!e&&0<b.length&&1<y+f.length&&o.uniqueSort(a)}return c&&(p=C,R=O),w},r?u(m):m))).selector=e}return i},te=o.select=function(e,n,s,o){var i,a,r,u,h,d="function"==typeof e&&e,c=!o&&k(e=d.selector||e);if(s=s||[],1===c.length){if(2<(i=c[0]=c[0].slice(0)).length&&"ID"===(a=i[0]).type&&9===n.nodeType&&l&&t.relative[i[1].type]){if(!(n=(t.find.ID(a.matches[0].replace(f,m),n)||[])[0]))return s;d&&(n=n.parentNode),e=e.slice(i.shift().value.length)}for(r=S.needsContext.test(e)?0:i.length;r--;){if(a=i[r],t.relative[u=a.type])break;if((h=t.find[u])&&(o=h(a.matches[0].replace(f,m),q.test(i[0].type)&&K(n.parentNode)||n))){if(i.splice(r,1),!(e=o.length&&T(i)))return v.apply(s,o),s;break}}}return(d||I(e,c))(o,n,!l,s,!n||q.test(e)&&K(n.parentNode)||n),s},i.sortStable=a.split("").sort(Y).join("")===a,i.detectDuplicates=!!x,g(),i.sortDetached=h(function(e){return 1&e.compareDocumentPosition(n.createElement("fieldset"))}),h(function(e){return e.innerHTML="<a href='#'></a>","#"===e.firstChild.getAttribute("href")})||W("type|href|height|width",function(e,t,n){if(!n)return e.getAttribute(t,"type"===t.toLowerCase()?1:2)}),i.attributes&&h(function(e){return e.innerHTML="<input/>",e.firstChild.setAttribute("value",""),""===e.firstChild.getAttribute("value")})||W("value",function(e,t,n){if(!n&&"input"===e.nodeName.toLowerCase())return e.defaultValue}),h(function(e){return null==e.getAttribute("disabled")})||W(B,function(e,t,n){var s;if(!n)return!0===e[t]?t.toLowerCase():(s=e.getAttributeNode(t))&&s.specified?s.value:null}),o}(e),n.find=b,n.expr=b.selectors,n.expr[":"]=n.expr.pseudos,n.uniqueSort=n.unique=b.uniqueSort,n.text=b.getText,n.isXMLDoc=b.isXML,n.contains=b.contains,n.escapeSelector=b.escape;var x=function(e,t,s){for(var o=[],i=void 0!==s;(e=e[t])&&9!==e.nodeType;)if(1===e.nodeType){if(i&&n(e).is(s))break;o.push(e)}return o},lt=function(e,t){for(var n=[];e;e=e.nextSibling)1===e.nodeType&&e!==t&&n.push(e);return n},rt=n.expr.match.needsContext;function d(e,t){return e.nodeName&&e.nodeName.toLowerCase()===t.toLowerCase()}se=/^<([a-z][^/\0>:\x20\t\r\n\f]*)[\x20\t\r\n\f]*\/?>(?:<\/\1>|)$/i;function re(e,t,s){return o(t)?n.grep(e,function(e,n){return!!t.call(e,n,e)!==s}):t.nodeType?n.grep(e,function(e){return e===t!==s}):"string"!=typeof t?n.grep(e,function(e){return-1<U.call(t,e)!==s}):n.filter(t,e,s)}n.filter=function(e,t,s){var o=t[0];return s&&(e=":not("+e+")"),1===t.length&&1===o.nodeType?n.find.matchesSelector(o,e)?[o]:[]:n.find.matches(e,n.grep(t,function(e){return 1===e.nodeType}))},n.fn.extend({find:function(e){var t,s,o=this.length,i=this;if("string"!=typeof e)return this.pushStack(n(e).filter(function(){for(t=0;t<o;t++)if(n.contains(i[t],this))return!0}));for(s=this.pushStack([]),t=0;t<o;t++)n.find(e,i[t],s);return 1<o?n.uniqueSort(s):s},filter:function(e){return this.pushStack(re(this,e||[],!1))},not:function(e){return this.pushStack(re(this,e||[],!0))},is:function(e){return!!re(this,"string"==typeof e&&rt.test(e)?n(e):e||[],!1).length}}),st=/^(?:\s*(<[\w\W]+>)[^>]*|#([\w-]+))$/,(n.fn.init=function(e,t,s){var a,r;if(!e)return this;if(s=s||it,"string"==typeof e){if(!(a="<"===e[0]&&">"===e[e.length-1]&&3<=e.length?[null,e,null]:st.exec(e))||!a[1]&&t)return!t||t.jquery?(t||s).find(e):this.constructor(t).find(e);if(a[1]){if(t=t instanceof n?t[0]:t,n.merge(this,n.parseHTML(a[1],t&&t.nodeType?t.ownerDocument||t:i,!0)),se.test(a[1])&&n.isPlainObject(t))for(a in t)o(this[a])?this[a](t[a]):this.attr(a,t[a]);return this}return(r=i.getElementById(a[2]))&&(this[0]=r,this.length=1),this}return e.nodeType?(this[0]=e,this.length=1,this):o(e)?void 0!==s.ready?s.ready(e):e(n):n.makeArray(e,this)}).prototype=n.fn,it=n(i),nt=/^(?:parents|prev(?:Until|All))/,tt={children:!0,contents:!0,next:!0,prev:!0};function et(e,t){for(;(e=e[t])&&1!==e.nodeType;);return e}n.fn.extend({has:function(e){var t=n(e,this),s=t.length;return this.filter(function(){for(var e=0;e<s;e++)if(n.contains(this,t[e]))return!0})},closest:function(e,t){var s,i=0,r=this.length,o=[],a="string"!=typeof e&&n(e);if(!rt.test(e))for(;i<r;i++)for(s=this[i];s&&s!==t;s=s.parentNode)if(s.nodeType<11&&(a?-1<a.index(s):1===s.nodeType&&n.find.matchesSelector(s,e))){o.push(s);break}return this.pushStack(1<o.length?n.uniqueSort(o):o)},index:function(e){return e?"string"==typeof e?U.call(n(e),this[0]):U.call(this,e.jquery?e[0]:e):this[0]&&this[0].parentNode?this.first().prevAll().length:-1},add:function(e,t){return this.pushStack(n.uniqueSort(n.merge(this.get(),n(e,t))))},addBack:function(e){return this.add(e==null?this.prevObject:this.prevObject.filter(e))}}),n.each({parent:function(e){var t=e.parentNode;return t&&11!==t.nodeType?t:null},parents:function(e){return x(e,"parentNode")},parentsUntil:function(e,t,n){return x(e,"parentNode",n)},next:function(e){return et(e,"nextSibling")},prev:function(e){return et(e,"previousSibling")},nextAll:function(e){return x(e,"nextSibling")},prevAll:function(e){return x(e,"previousSibling")},nextUntil:function(e,t,n){return x(e,"nextSibling",n)},prevUntil:function(e,t,n){return x(e,"previousSibling",n)},siblings:function(e){return lt((e.parentNode||{}).firstChild,e)},children:function(e){return lt(e.firstChild)},contents:function(e){return null!=e.contentDocument&&yt(e.contentDocument)?e.contentDocument:(d(e,"template")&&(e=e.content||e),n.merge([],e.childNodes))}},function(e,t){n.fn[e]=function(s,o){var i=n.map(this,t,s);return"Until"!==e.slice(-5)&&(o=s),o&&"string"==typeof o&&(i=n.filter(o,i)),1<this.length&&(tt[e]||n.uniqueSort(i),nt.test(e)&&i.reverse()),this.pushStack(i)}}),h=/[^\x20\t\r\n\f]+/g;function L(e){return e}function Q(e){throw e}function Ze(e,t,n,s){var i;try{e&&o(i=e.promise)?i.call(e).done(t).fail(n):e&&o(i=e.then)?i.call(e,t,n):t.apply(void 0,[e].slice(s))}catch(e){n.apply(void 0,[e])}}n.Callbacks=function(e){e="string"==typeof e?(u=e,d={},n.each(u.match(h)||[],function(e,t){d[t]=!0}),d):n.extend({},e);var s,a,c,d,u,f,t=[],r=[],i=-1,m=function(){for(a=a||e.once,f=c=!0;r.length;i=-1)for(s=r.shift();++i<t.length;)!1===t[i].apply(s[0],s[1])&&e.stopOnFalse&&(i=t.length,s=!1);e.memory||(s=!1),c=!1,a&&(t=s?[]:"")},l={add:function(){return t&&(s&&!c&&(i=t.length-1,r.push(s)),function s(i){n.each(i,function(n,i){o(i)?e.unique&&l.has(i)||t.push(i):i&&i.length&&"string"!==M(i)&&s(i)})}(arguments),s&&!c&&m()),this},remove:function(){return n.each(arguments,function(e,s){for(var o;-1<(o=n.inArray(s,t,o));)t.splice(o,1),o<=i&&i--}),this},has:function(e){return e?-1<n.inArray(e,t):0<t.length},empty:function(){return t&&(t=[]),this},disable:function(){return a=r=[],t=s="",this},disabled:function(){return!t},lock:function(){return a=r=[],s||c||(t=s=""),this},locked:function(){return!!a},fireWith:function(e,t){return a||(t=[e,(t=t||[]).slice?t.slice():t],r.push(t),c||m()),this},fire:function(){return l.fireWith(this,arguments),this},fired:function(){return!!f}};return l},n.extend({Deferred:function(t){var i=[["notify","progress",n.Callbacks("memory"),n.Callbacks("memory"),2],["resolve","done",n.Callbacks("once memory"),n.Callbacks("once memory"),0,"resolved"],["reject","fail",n.Callbacks("once memory"),n.Callbacks("once memory"),1,"rejected"]],r="pending",a={state:function(){return r},always:function(){return s.done(arguments).fail(arguments),this},catch:function(e){return a.then(null,e)},pipe:function(){var e=arguments;return n.Deferred(function(t){n.each(i,function(n,i){var a=o(e[i[4]])&&e[i[4]];s[i[1]](function(){var e=a&&a.apply(this,arguments);e&&o(e.promise)?e.promise().progress(t.notify).done(t.resolve).fail(t.reject):t[i[0]+"With"](this,a?[e]:arguments)})}),e=null}).promise()},then:function(t,s,a){var r=0;function c(t,s,i,a){return function(){var l=this,d=arguments,h=function(){var e,n;if(!(t<r)){if((e=i.apply(l,d))===s.promise())throw new TypeError("Thenable self-resolution");n=e&&("object"==typeof e||"function"==typeof e)&&e.then,o(n)?a?n.call(e,c(r,s,L,a),c(r,s,Q,a)):(r++,n.call(e,c(r,s,L,a),c(r,s,Q,a),c(r,s,L,s.notifyWith))):(i!==L&&(l=void 0,d=[e]),(a||s.resolveWith)(l,d))}},u=a?h:function(){try{h()}catch(e){n.Deferred.exceptionHook&&n.Deferred.exceptionHook(e,u.stackTrace),r<=t+1&&(i!==Q&&(l=void 0,d=[e]),s.rejectWith(l,d))}};t?u():(n.Deferred.getStackHook&&(u.stackTrace=n.Deferred.getStackHook()),e.setTimeout(u))}}return n.Deferred(function(e){i[0][3].add(c(0,e,o(a)?a:L,e.notifyWith)),i[1][3].add(c(0,e,o(t)?t:L)),i[2][3].add(c(0,e,o(s)?s:Q))}).promise()},promise:function(e){return e!=null?n.extend(e,a):a}},s={};return n.each(i,function(e,t){var n=t[2],o=t[5];a[t[1]]=n.add,o&&n.add(function(){r=o},i[3-e][2].disable,i[3-e][3].disable,i[0][2].lock,i[0][3].lock),n.add(t[3].fire),s[t[0]]=function(){return s[t[0]+"With"](this===s?void 0:this,arguments),this},s[t[0]+"With"]=n.fireWith}),a.promise(s),t&&t.call(s,s),s},when:function(e){var a=arguments.length,t=a,r=Array(t),i=j.call(arguments),s=n.Deferred(),c=function(e){return function(t){r[e]=this,i[e]=1<arguments.length?j.call(arguments):t,--a||s.resolveWith(r,i)}};if(a<=1&&(Ze(e,s.done(c(t)).resolve,s.reject,!a),"pending"===s.state()||o(i[t]&&i[t].then)))return s.then();for(;t--;)Ze(i[t],c(t),s.reject);return s.promise()}}),Ge=/^(Eval|Internal|Range|Reference|Syntax|Type|URI)Error$/,n.Deferred.exceptionHook=function(t,n){e.console&&e.console.warn&&t&&Ge.test(t.name)&&e.console.warn("jQuery.Deferred exception: "+t.message,t.stack,n)},n.readyException=function(t){e.setTimeout(function(){throw t})},G=n.Deferred();function q(){i.removeEventListener("DOMContentLoaded",q),e.removeEventListener("load",q),n.ready()}n.fn.ready=function(e){return G.then(e).catch(function(e){n.readyException(e)}),this},n.extend({isReady:!1,readyWait:1,ready:function(e){(!0===e?--n.readyWait:n.isReady)||(n.isReady=!0)!==e&&0<--n.readyWait||G.resolveWith(i,[n])}}),n.ready.then=G.then,"complete"===i.readyState||"loading"!==i.readyState&&!i.documentElement.doScroll?e.setTimeout(n.ready):(i.addEventListener("DOMContentLoaded",q),e.addEventListener("load",q));var p=function(e,t,s,i,a,r,c){var l=0,u=e.length,d=s==null;if("object"===M(s))for(l in a=!0,s)p(e,t,l,s[l],!0,r,c);else if(void 0!==i&&(a=!0,o(i)||(c=!0),d&&(c?(t.call(e,i),t=null):(d=t,t=function(e,t,s){return d.call(n(e),s)})),t))for(;l<u;l++)t(e[l],s,c?i:i.call(e[l],l,t(e[l],s)));return a?e:d?t.call(e):u?t(e[0],s):r},qt=/^-ms-/,Kt=/-([a-z])/g;function Ut(e,t){return t.toUpperCase()}function f(e){return e.replace(qt,"ms-").replace(Kt,Ut)}T=function(e){return 1===e.nodeType||9===e.nodeType||!+e.nodeType};function H(){this.expando=n.expando+H.uid++}H.uid=1,H.prototype={cache:function(e){var t=e[this.expando];return t||(t={},T(e)&&(e.nodeType?e[this.expando]=t:Object.defineProperty(e,this.expando,{value:t,configurable:!0}))),t},set:function(e,t,n){var s,o=this.cache(e);if("string"==typeof t)o[f(t)]=n;else for(s in t)o[f(s)]=t[s];return o},get:function(e,t){return void 0===t?this.cache(e):e[this.expando]&&e[this.expando][f(t)]},access:function(e,t,n){return void 0===t||t&&"string"==typeof t&&void 0===n?this.get(e,t):(this.set(e,t,n),void 0!==n?n:t)},remove:function(e,t){var o,s=e[this.expando];if(void 0!==s){if(void 0!==t)for(o=(t=Array.isArray(t)?t.map(f):(t=f(t))in s?[t]:t.match(h)||[]).length;o--;)delete s[t[o]];(void 0===t||n.isEmptyObject(s))&&(e.nodeType?e[this.expando]=void 0:delete e[this.expando])}},hasData:function(e){var t=e[this.expando];return void 0!==t&&!n.isEmptyObject(t)}};var s=new H,r=new H,Wt=/^(?:\{[\w\W]*\}|\[[\w\W]*\])$/,$t=/[A-Z]/g;function Ve(e,t,n){var s,o;if(void 0===n&&1===e.nodeType)if(o="data-"+t.replace($t,"-$&").toLowerCase(),"string"==typeof(n=e.getAttribute(o))){try{n="true"===(s=n)||"false"!==s&&("null"===s?null:s===+s+""?+s:Wt.test(s)?JSON.parse(s):s)}catch{}r.set(e,t,n)}else n=void 0;return n}n.extend({hasData:function(e){return r.hasData(e)||s.hasData(e)},data:function(e,t,n){return r.access(e,t,n)},removeData:function(e,t){r.remove(e,t)},_data:function(e,t,n){return s.access(e,t,n)},_removeData:function(e,t){s.remove(e,t)}}),n.fn.extend({data:function(e,t){var o,i,a,n=this[0],c=n&&n.attributes;if(void 0===e){if(this.length&&(a=r.get(n),1===n.nodeType&&!s.get(n,"hasDataAttrs"))){for(i=c.length;i--;)c[i]&&0===(o=c[i].name).indexOf("data-")&&(o=f(o.slice(5)),Ve(n,o,a[o]));s.set(n,"hasDataAttrs",!0)}return a}return"object"==typeof e?this.each(function(){r.set(this,e)}):p(this,function(t){var s;if(n&&void 0===t)return void 0!==(s=r.get(n,e))?s:void 0!==(s=Ve(n,e))?s:void 0;this.each(function(){r.set(this,e,t)})},null,t,1<arguments.length,null,!0)},removeData:function(e){return this.each(function(){r.remove(this,e)})}}),n.extend({queue:function(e,t,o){var i;if(e)return t=(t||"fx")+"queue",i=s.get(e,t),o&&(!i||Array.isArray(o)?i=s.access(e,t,n.makeArray(o)):i.push(o)),i||[]},dequeue:function(e,t){t=t||"fx";var s=n.queue(e,t),a=s.length,o=s.shift(),i=n._queueHooks(e,t);"inprogress"===o&&(o=s.shift(),a--),o&&("fx"===t&&s.unshift("inprogress"),delete i.stop,o.call(e,function(){n.dequeue(e,t)},i)),!a&&i&&i.empty.fire()},_queueHooks:function(e,t){var o=t+"queueHooks";return s.get(e,o)||s.access(e,o,{empty:n.Callbacks("once memory").add(function(){s.remove(e,[t+"queue",o])})})}}),n.fn.extend({queue:function(e,t){var s=2;return"string"!=typeof e&&(t=e,e="fx",s--),arguments.length<s?n.queue(this[0],e):void 0===t?this:this.each(function(){var s=n.queue(this,e,t);n._queueHooks(this,e),"fx"===e&&"inprogress"!==s[0]&&n.dequeue(this,e)})},dequeue:function(e){return this.each(function(){n.dequeue(this,e)})},clearQueue:function(e){return this.queue(e||"fx",[])},promise:function(e,t){var o,a=1,r=n.Deferred(),i=this,c=this.length,l=function(){--a||r.resolveWith(i,[i])};for("string"!=typeof e&&(t=e,e=void 0),e=e||"fx";c--;)(o=s.get(i[c],e+"queueHooks"))&&o.empty&&(a++,o.empty.add(l));return l(),r.promise(t)}});var Re=/[+-]?(?:\d*\.|)\d+(?:[eE][+-]?\d+|)/.source,P=new RegExp("^(?:([+-])=|)("+Re+")([a-z%]*)$","i"),v=["Top","Right","Bottom","Left"],_=i.documentElement,E=function(e){return n.contains(e.ownerDocument,e)},Vt={composed:!0};_.getRootNode&&(E=function(e){return n.contains(e.ownerDocument,e)||e.getRootNode(Vt)===e.ownerDocument}),$=function(e,t){return"none"===(e=t||e).style.display||""===e.style.display&&E(e)&&"none"===n.css(e,"display")};function Le(e,t,s,o){var c,l,d=20,u=o?function(){return o.cur()}:function(){return n.css(e,t,"")},r=u(),a=s&&s[3]||(n.cssNumber[t]?"":"px"),i=e.nodeType&&(n.cssNumber[t]||"px"!==a&&+r)&&P.exec(n.css(e,t));if(i&&i[3]!==a){for(r/=2,a=a||i[3],i=+r||1;d--;)n.style(e,t,i+a),(1-c)*(1-(c=u()/r||.5))<=0&&(d=0),i/=c;i*=2,n.style(e,t,i+a),s=s||[]}return s&&(i=+i||+r||0,l=s[1]?i+(s[1]+1)*s[2]:+s[2],o&&(o.unit=a,o.start=i,o.end=l)),l}ce={};function C(e,t){for(var i,a,c,l,d,u,h,r=[],o=0,m=e.length;o<m;o++)(i=e[o]).style&&(u=i.style.display,t?("none"===u&&(r[o]=s.get(i,"display")||null,r[o]||(i.style.display="")),""===i.style.display&&$(i)&&(r[o]=(a=l=c=void 0,l=(h=i).ownerDocument,d=h.nodeName,(a=ce[d])||(c=l.body.appendChild(l.createElement(d)),a=n.css(c,"display"),c.parentNode.removeChild(c),"none"===a&&(a="block"),ce[d]=a)))):"none"!==u&&(r[o]="none",s.set(i,"display",u)));for(o=0;o<m;o++)null!=r[o]&&(e[o].style.display=r[o]);return e}n.fn.extend({show:function(){return C(this,!0)},hide:function(){return C(this)},toggle:function(e){return"boolean"==typeof e?e?this.show():this.hide():this.each(function(){$(this)?n(this).show():n(this).hide()})}});var Z,I=/^(?:checkbox|radio)$/i,Ne=/<([a-z][^/\0>\x20\t\r\n\f]*)/i,ze=/^$|^module$|\/(?:java|ecma)script/i,k=i.createDocumentFragment().appendChild(i.createElement("div"));(Z=i.createElement("input")).setAttribute("type","radio"),Z.setAttribute("checked","checked"),Z.setAttribute("name","t"),k.appendChild(Z),a.checkClone=k.cloneNode(!0).cloneNode(!0).lastChild.checked,k.innerHTML="<textarea>x</textarea>",a.noCloneChecked=!!k.cloneNode(!0).lastChild.defaultValue,k.innerHTML="<option></option>",a.option=!!k.lastChild,l={thead:[1,"<table>","</table>"],col:[2,"<table><colgroup>","</colgroup></table>"],tr:[2,"<table><tbody>","</tbody></table>"],td:[3,"<table><tbody><tr>","</tr></tbody></table>"],_default:[0,"",""]};function c(e,t){var s;return s="undefined"!=typeof e.getElementsByTagName?e.getElementsByTagName(t||"*"):"undefined"!=typeof e.querySelectorAll?e.querySelectorAll(t||"*"):[],void 0===t||t&&d(e,t)?n.merge([e],s):s}function ve(e,t){for(var n=0,o=e.length;n<o;n++)s.set(e[n],"globalEval",!t||s.get(t[n],"globalEval"))}l.tbody=l.tfoot=l.colgroup=l.caption=l.thead,l.th=l.td,a.option||(l.optgroup=l.option=[1,"<select multiple='multiple'>","</select>"]),Fe=/<|&#?\w+;/;function Ae(e,t,s,o,i){for(var a,r,h,m,p,g,u=t.createDocumentFragment(),f=[],d=0,v=e.length;d<v;d++)if((a=e[d])||0===a)if("object"===M(a))n.merge(f,a.nodeType?[a]:a);else if(Fe.test(a)){for(r=r||u.appendChild(t.createElement("div")),g=(Ne.exec(a)||["",""])[1].toLowerCase(),h=l[g]||l._default,r.innerHTML=h[1]+n.htmlPrefilter(a)+h[2],m=h[0];m--;)r=r.lastChild;n.merge(f,r.childNodes),(r=u.firstChild).textContent=""}else f.push(t.createTextNode(a));for(u.textContent="",d=0;a=f[d++];)if(o&&-1<n.inArray(a,o))i&&i.push(a);else if(p=E(a),r=c(u.appendChild(a),"script"),p&&ve(r),s)for(m=0;a=r[m++];)ze.test(a.type||"")&&s.push(a);return u}ye=/^([^.]*)(?:\.(.+)|)/;function F(){return!0}function D(){return!1}function Bt(e,t){return e===function(){try{return i.activeElement}catch{}}()==("focus"===t)}function ae(e,t,s,o,i,a){var r,c;if("object"==typeof t){for(c in"string"!=typeof s&&(o=o||s,s=void 0),t)ae(e,c,s,o,t[c],a);return e}if(o==null&&i==null?(i=s,o=s=void 0):i==null&&("string"==typeof s?(i=o,o=void 0):(i=o,o=s,s=void 0)),!1===i)i=D;else if(!i)return e;return 1===a&&(r=i,(i=function(e){return n().off(e),r.apply(this,arguments)}).guid=r.guid||(r.guid=n.guid++)),e.each(function(){n.event.add(this,t,i,o,s)})}function W(e,t,o){o?(s.set(e,t,!1),n.event.add(e,t,{namespace:!1,handler:function(e){var a,r,i=s.get(this,t);if(1&e.isTrigger&&this[t]){if(i.length)(n.event.special[t]||{}).delegateType&&e.stopPropagation();else if(i=j.call(arguments),s.set(this,t,i),r=o(this,t),this[t](),i!==(a=s.get(this,t))||r?s.set(this,t,!1):a={},i!==a)return e.stopImmediatePropagation(),e.preventDefault(),a&&a.value}else i.length&&(s.set(this,t,{value:n.event.trigger(n.extend(i[0],n.Event.prototype),i.slice(1),this)}),e.stopImmediatePropagation())}})):void 0===s.get(e,t)&&n.event.add(e,t,F)}n.event={global:{},add:function(e,t,o,i,a){var r,c,l,d,u,m,p,g,v,b,j,f=s.get(e);if(T(e))for(o.handler&&(o=(g=o).handler,a=g.selector),a&&n.find.matchesSelector(_,a),o.guid||(o.guid=n.guid++),(u=f.events)||(u=f.events=Object.create(null)),(m=f.handle)||(m=f.handle=function(t){return"undefined"!=typeof n&&n.event.triggered!==t.type?n.event.dispatch.apply(e,arguments):void 0}),p=(t=(t||"").match(h)||[""]).length;p--;)r=j=(b=ye.exec(t[p])||[])[1],v=(b[2]||"").split(".").sort(),r&&(c=n.event.special[r]||{},r=(a?c.delegateType:c.bindType)||r,c=n.event.special[r]||{},l=n.extend({type:r,origType:j,data:i,handler:o,guid:o.guid,selector:a,needsContext:a&&n.expr.match.needsContext.test(a),namespace:v.join(".")},g),(d=u[r])||((d=u[r]=[]).delegateCount=0,c.setup&&!1!==c.setup.call(e,i,v,m)||e.addEventListener&&e.addEventListener(r,m)),c.add&&(c.add.call(e,l),l.handler.guid||(l.handler.guid=o.guid)),a?d.splice(d.delegateCount++,0,l):d.push(l),n.event.global[r]=!0)},remove:function(e,t,o,i,a){var r,c,l,d,u,m,f,p,v,b,j,g=s.hasData(e)&&s.get(e);if(g&&(m=g.events)){for(f=(t=(t||"").match(h)||[""]).length;f--;)if(r=j=(u=ye.exec(t[f])||[])[1],v=(u[2]||"").split(".").sort(),r){for(l=n.event.special[r]||{},d=m[r=(i?l.delegateType:l.bindType)||r]||[],u=u[2]&&new RegExp("(^|\\.)"+v.join("\\.(?:.*\\.|)")+"(\\.|$)"),b=p=d.length;p--;)c=d[p],!a&&j!==c.origType||o&&o.guid!==c.guid||u&&!u.test(c.namespace)||i&&i!==c.selector&&("**"!==i||!c.selector)||(d.splice(p,1),c.selector&&d.delegateCount--,l.remove&&l.remove.call(e,c));b&&!d.length&&(l.teardown&&!1!==l.teardown.call(e,v,g.handle)||n.removeEvent(e,r,g.handle),delete m[r])}else for(r in m)n.event.remove(e,r+t[f],o,i,!0);n.isEmptyObject(m)&&s.remove(e,"handle events")}},dispatch:function(e){var o,i,a,l,d,u,c=new Array(arguments.length),t=n.event.fix(e),h=(s.get(this,"events")||Object.create(null))[t.type]||[],r=n.event.special[t.type]||{};for(c[0]=t,o=1;o<arguments.length;o++)c[o]=arguments[o];if(t.delegateTarget=this,!r.preDispatch||!1!==r.preDispatch.call(this,t)){for(d=n.event.handlers.call(this,t,h),o=0;(a=d[o++])&&!t.isPropagationStopped();)for(t.currentTarget=a.elem,u=0;(i=a.handlers[u++])&&!t.isImmediatePropagationStopped();)t.rnamespace&&!1!==i.namespace&&!t.rnamespace.test(i.namespace)||(t.handleObj=i,t.data=i.data,void 0!==(l=((n.event.special[i.origType]||{}).handle||i.handler).apply(a.elem,c))&&!1===(t.result=l)&&(t.preventDefault(),t.stopPropagation()));return r.postDispatch&&r.postDispatch.call(this,t),t.result}},handlers:function(e,t){var o,i,a,r,d,l=[],c=t.delegateCount,s=e.target;if(c&&s.nodeType&&!("click"===e.type&&1<=e.button))for(;s!==this;s=s.parentNode||this)if(1===s.nodeType&&("click"!==e.type||!0!==s.disabled)){for(a=[],r={},i=0;i<c;i++)void 0===r[o=(d=t[i]).selector+" "]&&(r[o]=d.needsContext?-1<n(o,this).index(s):n.find(o,this,null,[s]).length),r[o]&&a.push(d);a.length&&l.push({elem:s,handlers:a})}return s=this,c<t.length&&l.push({elem:s,handlers:t.slice(c)}),l},addProp:function(e,t){Object.defineProperty(n.Event.prototype,e,{enumerable:!0,configurable:!0,get:o(t)?function(){if(this.originalEvent)return t(this.originalEvent)}:function(){if(this.originalEvent)return this.originalEvent[e]},set:function(t){Object.defineProperty(this,e,{enumerable:!0,configurable:!0,writable:!0,value:t})}})},fix:function(e){return e[n.expando]?e:new n.Event(e)},special:{load:{noBubble:!0},click:{setup:function(e){var t=this||e;return I.test(t.type)&&t.click&&d(t,"input")&&W(t,"click",F),!1},trigger:function(e){var t=this||e;return I.test(t.type)&&t.click&&d(t,"input")&&W(t,"click"),!0},_default:function(e){var t=e.target;return I.test(t.type)&&t.click&&d(t,"input")&&s.get(t,"click")||d(t,"a")}},beforeunload:{postDispatch:function(e){void 0!==e.result&&e.originalEvent&&(e.originalEvent.returnValue=e.result)}}}},n.removeEvent=function(e,t,n){e.removeEventListener&&e.removeEventListener(t,n)},n.Event=function(e,t){if(!(this instanceof n.Event))return new n.Event(e,t);e&&e.type?(this.originalEvent=e,this.type=e.type,this.isDefaultPrevented=e.defaultPrevented||void 0===e.defaultPrevented&&!1===e.returnValue?F:D,this.target=e.target&&3===e.target.nodeType?e.target.parentNode:e.target,this.currentTarget=e.currentTarget,this.relatedTarget=e.relatedTarget):this.type=e,t&&n.extend(this,t),this.timeStamp=e&&e.timeStamp||Date.now(),this[n.expando]=!0},n.Event.prototype={constructor:n.Event,isDefaultPrevented:D,isPropagationStopped:D,isImmediatePropagationStopped:D,isSimulated:!1,preventDefault:function(){var e=this.originalEvent;this.isDefaultPrevented=F,e&&!this.isSimulated&&e.preventDefault()},stopPropagation:function(){var e=this.originalEvent;this.isPropagationStopped=F,e&&!this.isSimulated&&e.stopPropagation()},stopImmediatePropagation:function(){var e=this.originalEvent;this.isImmediatePropagationStopped=F,e&&!this.isSimulated&&e.stopImmediatePropagation(),this.stopPropagation()}},n.each({altKey:!0,bubbles:!0,cancelable:!0,changedTouches:!0,ctrlKey:!0,detail:!0,eventPhase:!0,metaKey:!0,pageX:!0,pageY:!0,shiftKey:!0,view:!0,char:!0,code:!0,charCode:!0,key:!0,keyCode:!0,button:!0,buttons:!0,clientX:!0,clientY:!0,offsetX:!0,offsetY:!0,pointerId:!0,pointerType:!0,screenX:!0,screenY:!0,targetTouches:!0,toElement:!0,touches:!0,which:!0},n.event.addProp),n.each({focus:"focusin",blur:"focusout"},function(e,t){n.event.special[e]={setup:function(){return W(this,e,Bt),!1},trigger:function(){return W(this,e),!0},_default:function(t){return s.get(t.target,e)},delegateType:t}}),n.each({mouseenter:"mouseover",mouseleave:"mouseout",pointerenter:"pointerover",pointerleave:"pointerout"},function(e,t){n.event.special[e]={delegateType:t,bindType:t,handle:function(e){var o,s=e.relatedTarget,i=e.handleObj;return s&&(s===this||n.contains(this,s))||(e.type=i.origType,o=i.handler.apply(this,arguments),e.type=t),o}}}),n.fn.extend({on:function(e,t,n,s){return ae(this,e,t,n,s)},one:function(e,t,n,s){return ae(this,e,t,n,s,1)},off:function(e,t,s){var o,i;if(e&&e.preventDefault&&e.handleObj)return o=e.handleObj,n(e.delegateTarget).off(o.namespace?o.origType+"."+o.namespace:o.origType,o.selector,o.handler),this;if("object"==typeof e){for(i in e)this.off(i,t,e[i]);return this}return!1!==t&&"function"!=typeof t||(s=t,t=void 0),!1===s&&(s=D),this.each(function(){n.event.remove(this,e,s,t)})}});var St=/<script|<style|<link/i,It=/checked\s*(?:[^=]|=\s*.checked.)/i,Ht=/^\s*<!\[CDATA\[|\]\]>\s*$/g;function Se(e,t){return d(e,"table")&&d(11!==t.nodeType?t:t.firstChild,"tr")&&n(e).children("tbody")[0]||e}function Pt(e){return e.type=(null!==e.getAttribute("type"))+"/"+e.type,e}function Rt(e){return"true/"===(e.type||"").slice(0,5)?e.type=e.type.slice(5):e.removeAttribute("type"),e}function Te(e,t){var o,i,a,c,l,d;if(1===t.nodeType){if(s.hasData(e)&&(a=s.get(e).events))for(i in s.remove(t,"handle events"),a)for(o=0,c=a[i].length;o<c;o++)n.event.add(t,i,a[i][o]);r.hasData(e)&&(l=r.access(e),d=n.extend({},l),r.set(t,d))}}function A(e,t,i,r){t=bt(t);var l,u,h,f,p,v,d=0,m=e.length,j=m-1,g=t[0],b=o(g);if(b||1<m&&"string"==typeof g&&!a.checkClone&&It.test(g))return e.each(function(n){var s=e.eq(n);b&&(t[0]=g.call(this,n,s.html())),A(s,t,i,r)});if(m&&(v=(h=Ae(t,e[0].ownerDocument,!1,e,r)).firstChild,1===h.childNodes.length&&(h=v),v||r)){for(f=(u=n.map(c(h,"script"),Pt)).length;d<m;d++)l=h,d!==j&&(l=n.clone(l,!0,!0),f&&n.merge(u,c(l,"script"))),i.call(e[d],l,d);if(f)for(p=u[u.length-1].ownerDocument,n.map(u,Rt),d=0;d<f;d++)l=u[d],ze.test(l.type||"")&&!s.access(l,"globalEval")&&n.contains(p,l)&&(l.src&&"module"!==(l.type||"").toLowerCase()?n._evalUrl&&!l.noModule&&n._evalUrl(l.src,{nonce:l.nonce||l.getAttribute("nonce")},p):Ce(l.textContent.replace(Ht,""),l,p))}return e}function De(e,t,s){for(var o,a=t?n.filter(t,e):e,i=0;null!=(o=a[i]);i++)s||1!==o.nodeType||n.cleanData(c(o)),o.parentNode&&(s&&E(o)&&ve(c(o,"script")),o.parentNode.removeChild(o));return e}n.extend({htmlPrefilter:function(e){return e},clone:function(e,t,s){var o,i,r,d,u,h,m,l=e.cloneNode(!0),f=E(e);if(!(a.noCloneChecked||1!==e.nodeType&&11!==e.nodeType||n.isXMLDoc(e)))for(i=c(l),o=0,h=(r=c(e)).length;o<h;o++)d=r[o],u=i[o],void 0,"input"===(m=u.nodeName.toLowerCase())&&I.test(d.type)?u.checked=d.checked:"input"!==m&&"textarea"!==m||(u.defaultValue=d.defaultValue);if(t)if(s)for(r=r||c(e),i=i||c(l),o=0,h=r.length;o<h;o++)Te(r[o],i[o]);else Te(e,l);return 0<(i=c(l,"script")).length&&ve(i,!f&&c(e,"script")),l},cleanData:function(e){for(var t,o,i,c=n.event.special,a=0;void 0!==(t=e[a]);a++)if(T(t)){if(o=t[s.expando]){if(o.events)for(i in o.events)c[i]?n.event.remove(t,i):n.removeEvent(t,i,o.handle);t[s.expando]=void 0}t[r.expando]&&(t[r.expando]=void 0)}}}),n.fn.extend({detach:function(e){return De(this,e,!0)},remove:function(e){return De(this,e)},text:function(e){return p(this,function(e){return void 0===e?n.text(this):this.empty().each(function(){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||(this.textContent=e)})},null,e,arguments.length)},append:function(){return A(this,arguments,function(e){1!==this.nodeType&&11!==this.nodeType&&9!==this.nodeType||Se(this,e).appendChild(e)})},prepend:function(){return A(this,arguments,function(e){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var t=Se(this,e);t.insertBefore(e,t.firstChild)}})},before:function(){return A(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this)})},after:function(){return A(this,arguments,function(e){this.parentNode&&this.parentNode.insertBefore(e,this.nextSibling)})},empty:function(){for(var e,t=0;null!=(e=this[t]);t++)1===e.nodeType&&(n.cleanData(c(e,!1)),e.textContent="");return this},clone:function(e,t){return e=e!=null&&e,t=t??e,this.map(function(){return n.clone(this,e,t)})},html:function(e){return p(this,function(e){var t=this[0]||{},s=0,o=this.length;if(void 0===e&&1===t.nodeType)return t.innerHTML;if("string"==typeof e&&!St.test(e)&&!l[(Ne.exec(e)||["",""])[1].toLowerCase()]){e=n.htmlPrefilter(e);try{for(;s<o;s++)1===(t=this[s]||{}).nodeType&&(n.cleanData(c(t,!1)),t.innerHTML=e);t=0}catch{}}t&&this.empty().append(e)},null,e,arguments.length)},replaceWith:function(){var e=[];return A(this,arguments,function(t){var s=this.parentNode;n.inArray(this,e)<0&&(n.cleanData(c(this)),s&&s.replaceChild(t,this))},e)}}),n.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(e,t){n.fn[e]=function(e){for(var o,i=[],a=n(e),r=a.length-1,s=0;s<=r;s++)o=s===r?this:this.clone(!0),n(a[s])[t](o),_e.apply(i,o.get());return this.pushStack(i)}});var me=new RegExp("^("+Re+")(?!px)[a-z%]+$","i"),ne=/^--/,ee=function(t){var n=t.ownerDocument.defaultView;return n&&n.opener||(n=e),n.getComputedStyle(t)},Pe=function(e,t,n){var s,o,i={};for(s in t)i[s]=e.style[s],e.style[s]=t[s];for(s in o=n.call(e),t)e.style[s]=i[s];return o},Lt=new RegExp(v.join("|"),"i"),Ie=`[\\x20\\t\\r\\n\\f]`,Ft=new RegExp("^"+Ie+"+|((?:^|[^\\\\])(?:\\\\.)*)"+Ie+"+$","g");function V(e,t,s){var o,r,c,l,d=ne.test(t),i=e.style;return(s=s||ee(e))&&(o=s.getPropertyValue(t)||s[t],d&&(o=o.replace(Ft,"$1")),""!==o||E(e)||(o=n.style(e,t)),!a.pixelBoxStyles()&&me.test(o)&&Lt.test(t)&&(r=i.width,c=i.minWidth,l=i.maxWidth,i.minWidth=i.maxWidth=i.width=o,o=s.width,i.width=r,i.minWidth=c,i.maxWidth=l)),void 0!==o?o+"":o}function $e(e,t){return{get:function(){if(!e())return(this.get=t).apply(this,arguments);delete this.get}}}!function(){function s(){if(t){r.style.cssText="position:absolute;left:-11111px;width:60px;margin-top:1px;padding:0;border:0",t.style.cssText="position:relative;display:block;box-sizing:border-box;overflow:scroll;margin:auto;border:1px;padding:1px;width:60%;top:1%",_.appendChild(r).appendChild(t);var n=e.getComputedStyle(t),h="1%"!==n.top,u=12===o(n.marginLeft);t.style.right="60%",d=36===o(n.right),m=36===o(n.width),t.style.position="absolute",l=12===o(t.offsetWidth/3),_.removeChild(r),t=null}}function o(e){return Math.round(parseFloat(e))}var c,l,d,u,h,m,r=i.createElement("div"),t=i.createElement("div");t.style&&(t.style.backgroundClip="content-box",t.cloneNode(!0).style.backgroundClip="",a.clearCloneStyle="content-box"===t.style.backgroundClip,n.extend(a,{boxSizingReliable:function(){return s(),m},pixelBoxStyles:function(){return s(),d},pixelPosition:function(){return s(),h},reliableMarginLeft:function(){return s(),u},scrollboxSize:function(){return s(),l},reliableTrDimensions:function(){var t,n,s,o;return c==null&&(n=i.createElement("table"),t=i.createElement("tr"),s=i.createElement("div"),n.style.cssText="position:absolute;left:-11111px;border-collapse:separate",t.style.cssText="border:1px solid",t.style.height="1px",s.style.height="9px",s.style.display="block",_.appendChild(n).appendChild(t).appendChild(s),o=e.getComputedStyle(t),c=parseInt(o.height,10)+parseInt(o.borderTopWidth,10)+parseInt(o.borderBottomWidth,10)===t.offsetHeight,_.removeChild(n)),c}}))}();var We=["Webkit","Moz","ms"],wt=i.createElement("div").style,Ke={};function ge(e){var t=n.cssProps[e]||Ke[e];return t||(e in wt?e:Ke[e]=function(e){for(var n=e[0].toUpperCase()+e.slice(1),t=We.length;t--;)if((e=We[t]+n)in wt)return e}(e)||e)}var Et=/^(none|table(?!-c[ea]).+)/,xt={position:"absolute",visibility:"hidden",display:"block"},Xe={letterSpacing:"0",fontWeight:"400"};function Qe(e,t,n){var s=P.exec(t);return s?Math.max(0,s[2]-(n||0))+(s[3]||"px"):t}function fe(e,t,s,o,i,a){var r="width"===t?1:0,l=0,c=0;if(s===(o?"border":"content"))return 0;for(;r<4;r+=2)"margin"===s&&(c+=n.css(e,s+v[r],!0,i)),o?("content"===s&&(c-=n.css(e,"padding"+v[r],!0,i)),"margin"!==s&&(c-=n.css(e,"border"+v[r]+"Width",!0,i))):(c+=n.css(e,"padding"+v[r],!0,i),"padding"!==s?c+=n.css(e,"border"+v[r]+"Width",!0,i):l+=n.css(e,"border"+v[r]+"Width",!0,i));return!o&&0<=a&&(c+=Math.max(0,Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-a-c-l-.5))||0),c}function Je(e,t,s){var i=ee(e),r=(!a.boxSizingReliable()||s)&&"border-box"===n.css(e,"boxSizing",!1,i),c=r,o=V(e,t,i),l="offset"+t[0].toUpperCase()+t.slice(1);if(me.test(o)){if(!s)return o;o="auto"}return(!a.boxSizingReliable()&&r||!a.reliableTrDimensions()&&d(e,"tr")||"auto"===o||!parseFloat(o)&&"inline"===n.css(e,"display",!1,i))&&e.getClientRects().length&&(r="border-box"===n.css(e,"boxSizing",!1,i),(c=l in e)&&(o=e[l])),(o=parseFloat(o)||0)+fe(e,t,s||(r?"border":"content"),c,i,o)+"px"}function m(e,t,n,s,o){return new m.prototype.init(e,t,n,s,o)}n.extend({cssHooks:{opacity:{get:function(e,t){if(t){var n=V(e,"opacity");return""===n?"1":n}}}},cssNumber:{animationIterationCount:!0,columnCount:!0,fillOpacity:!0,flexGrow:!0,flexShrink:!0,fontWeight:!0,gridArea:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnStart:!0,gridRow:!0,gridRowEnd:!0,gridRowStart:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,widows:!0,zIndex:!0,zoom:!0},cssProps:{},style:function(e,t,s,o){if(e&&3!==e.nodeType&&8!==e.nodeType&&e.style){var i,r,l,d=f(t),u=ne.test(t),c=e.style;if(u||(t=ge(d)),r=n.cssHooks[t]||n.cssHooks[d],void 0===s)return r&&"get"in r&&void 0!==(i=r.get(e,!1,o))?i:c[t];"string"===(l=typeof s)&&(i=P.exec(s))&&i[1]&&(s=Le(e,t,i),l="number"),s!=null&&s==s&&("number"!==l||u||(s+=i&&i[3]||(n.cssNumber[d]?"":"px")),a.clearCloneStyle||""!==s||0!==t.indexOf("background")||(c[t]="inherit"),r&&"set"in r&&void 0===(s=r.set(e,s,o))||(u?c.setProperty(t,s):c[t]=s))}},css:function(e,t,s,o){var i,a,r,c=f(t);return ne.test(t)||(t=ge(c)),(r=n.cssHooks[t]||n.cssHooks[c])&&"get"in r&&(i=r.get(e,!0,s)),void 0===i&&(i=V(e,t,o)),"normal"===i&&t in Xe&&(i=Xe[t]),""===s||s?(a=parseFloat(i),!0===s||isFinite(a)?a||0:i):i}}),n.each(["height","width"],function(e,t){n.cssHooks[t]={get:function(e,s,o){if(s)return!Et.test(n.css(e,"display"))||e.getClientRects().length&&e.getBoundingClientRect().width?Je(e,t,o):Pe(e,xt,function(){return Je(e,t,o)})},set:function(e,s,o){var c,i=ee(e),l=!a.scrollboxSize()&&"absolute"===i.position,d=(l||o)&&"border-box"===n.css(e,"boxSizing",!1,i),r=o?fe(e,t,o,d,i):0;return d&&l&&(r-=Math.ceil(e["offset"+t[0].toUpperCase()+t.slice(1)]-parseFloat(i[t])-fe(e,t,"border",!1,i)-.5)),r&&(c=P.exec(s))&&"px"!==(c[3]||"px")&&(e.style[t]=s,s=n.css(e,t)),Qe(0,s,r)}}}),n.cssHooks.marginLeft=$e(a.reliableMarginLeft,function(e,t){if(t)return(parseFloat(V(e,"marginLeft"))||e.getBoundingClientRect().left-Pe(e,{marginLeft:0},function(){return e.getBoundingClientRect().left}))+"px"}),n.each({margin:"",padding:"",border:"Width"},function(e,t){n.cssHooks[e+t]={expand:function(n){for(var s=0,i={},o="string"==typeof n?n.split(" "):[n];s<4;s++)i[e+v[s]+t]=o[s]||o[s-2]||o[0];return i}},"margin"!==e&&(n.cssHooks[e+t].set=Qe)}),n.fn.extend({css:function(e,t){return p(this,function(e,t,s){var i,a,r={},o=0;if(Array.isArray(t)){for(i=ee(e),a=t.length;o<a;o++)r[t[o]]=n.css(e,t[o],!1,i);return r}return void 0!==s?n.style(e,t,s):n.css(e,t)},e,t,1<arguments.length)}}),((n.Tween=m).prototype={constructor:m,init:function(e,t,s,o,i,a){this.elem=e,this.prop=s,this.easing=i||n.easing._default,this.options=t,this.start=this.now=this.cur(),this.end=o,this.unit=a||(n.cssNumber[s]?"":"px")},cur:function(){var e=m.propHooks[this.prop];return e&&e.get?e.get(this):m.propHooks._default.get(this)},run:function(e){var t,s=m.propHooks[this.prop];return this.options.duration?this.pos=t=n.easing[this.easing](e,this.options.duration*e,0,1,this.options.duration):this.pos=t=e,this.now=(this.end-this.start)*t+this.start,this.options.step&&this.options.step.call(this.elem,this.now,this),s&&s.set?s.set(this):m.propHooks._default.set(this),this}}).init.prototype=m.prototype,(m.propHooks={_default:{get:function(e){var t;return 1!==e.elem.nodeType||null!=e.elem[e.prop]&&null==e.elem.style[e.prop]?e.elem[e.prop]:(t=n.css(e.elem,e.prop,""))&&"auto"!==t?t:0},set:function(e){n.fx.step[e.prop]?n.fx.step[e.prop](e):1!==e.elem.nodeType||!n.cssHooks[e.prop]&&null==e.elem.style[ge(e.prop)]?e.elem[e.prop]=e.now:n.style(e.elem,e.prop,e.now+e.unit)}}}).scrollTop=m.propHooks.scrollLeft={set:function(e){e.elem.nodeType&&e.elem.parentNode&&(e.elem[e.prop]=e.now)}},n.easing={linear:function(e){return e},swing:function(e){return.5-Math.cos(e*Math.PI)/2},_default:"swing"},n.fx=m.prototype.init,n.fx.step={},_t=/^(?:toggle|show|hide)$/,at=/queueHooks$/;function oe(){J&&(!1===i.hidden&&e.requestAnimationFrame?e.requestAnimationFrame(oe):e.setTimeout(oe,n.fx.interval),n.fx.tick())}function ct(){return e.setTimeout(function(){z=void 0}),z=Date.now()}function te(e,t){var o,s=0,n={height:e};for(t=t?1:0;s<4;s+=2-t)n["margin"+(o=v[s])]=n["padding"+o]=e;return t&&(n.opacity=n.width=e),n}function dt(e,t,n){for(var o,i=(u.tweeners[t]||[]).concat(u.tweeners["*"]),s=0,a=i.length;s<a;s++)if(o=i[s].call(n,t,e))return o}function u(e,t,s){var r,c,l=0,m=u.prefilters.length,a=n.Deferred().always(function(){delete h.elem}),h=function(){if(c)return!1;for(var r=z||ct(),t=Math.max(0,i.startTime+i.duration-r),n=1-(t/i.duration||0),s=0,o=i.tweens.length;s<o;s++)i.tweens[s].run(n);return a.notifyWith(e,[i,n,t]),n<1&&o?t:(o||a.notifyWith(e,[i,1,0]),a.resolveWith(e,[i]),!1)},i=a.promise({elem:e,props:n.extend({},t),opts:n.extend(!0,{specialEasing:{},easing:n.easing._default},s),originalProperties:t,originalOptions:s,startTime:z||ct(),duration:s.duration,tweens:[],createTween:function(t,s){var o=n.Tween(e,i.opts,t,s,i.opts.specialEasing[t]||i.opts.easing);return i.tweens.push(o),o},stop:function(t){var n=0,s=t?i.tweens.length:0;if(c)return this;for(c=!0;n<s;n++)i.tweens[n].run(1);return t?(a.notifyWith(e,[i,1,0]),a.resolveWith(e,[i,t])):a.rejectWith(e,[i,t]),this}}),d=i.props;for(!function(e,t){var s,o,i,a,r;for(s in e)if(a=t[i=f(s)],o=e[s],Array.isArray(o)&&(a=o[1],o=e[s]=o[0]),s!==i&&(e[i]=o,delete e[s]),(r=n.cssHooks[i])&&"expand"in r)for(s in o=r.expand(o),delete e[i],o)s in e||(e[s]=o[s],t[s]=a);else t[i]=a}(d,i.opts.specialEasing);l<m;l++)if(r=u.prefilters[l].call(i,e,d,i.opts))return o(r.stop)&&(n._queueHooks(i.elem,i.opts.queue).stop=r.stop.bind(r)),r;return n.map(d,dt,i),o(i.opts.start)&&i.opts.start.call(e,i),i.progress(i.opts.progress).done(i.opts.done,i.opts.complete).fail(i.opts.fail).always(i.opts.always),n.fx.timer(n.extend(h,{elem:e,anim:i,queue:i.opts.queue})),i}n.Animation=n.extend(u,{tweeners:{"*":[function(e,t){var n=this.createTween(e,t);return Le(n.elem,e,P.exec(t),n),n}]},tweener:function(e,t){o(e)?(t=e,e=["*"]):e=e.match(h);for(var n,s=0,i=e.length;s<i;s++)n=e[s],u.tweeners[n]=u.tweeners[n]||[],u.tweeners[n].unshift(t)},prefilters:[function(e,t,o){var i,r,l,d,u,p,g,v,b="width"in t||"height"in t,m=this,f={},c=e.style,h=e.nodeType&&$(e),a=s.get(e,"fxshow");for(i in o.queue||(null==(d=n._queueHooks(e,"fx")).unqueued&&(d.unqueued=0,v=d.empty.fire,d.empty.fire=function(){d.unqueued||v()}),d.unqueued++,m.always(function(){m.always(function(){d.unqueued--,n.queue(e,"fx").length||d.empty.fire()})})),t)if(p=t[i],_t.test(p)){if(delete t[i],g=g||"toggle"===p,p===(h?"hide":"show")){if("show"!==p||!a||void 0===a[i])continue;h=!0}f[i]=a&&a[i]||n.style(e,i)}if((l=!n.isEmptyObject(t))||!n.isEmptyObject(f))for(i in b&&1===e.nodeType&&(o.overflow=[c.overflow,c.overflowX,c.overflowY],null==(r=a&&a.display)&&(r=s.get(e,"display")),"none"===(u=n.css(e,"display"))&&(r?u=r:(C([e],!0),r=e.style.display||r,u=n.css(e,"display"),C([e]))),("inline"===u||"inline-block"===u&&r!=null)&&"none"===n.css(e,"float")&&(l||(m.done(function(){c.display=r}),r==null&&(u=c.display,r="none"===u?"":u)),c.display="inline-block")),o.overflow&&(c.overflow="hidden",m.always(function(){c.overflow=o.overflow[0],c.overflowX=o.overflow[1],c.overflowY=o.overflow[2]})),l=!1,f)l||(a?"hidden"in a&&(h=a.hidden):a=s.access(e,"fxshow",{display:r}),g&&(a.hidden=!h),h&&C([e],!0),m.done(function(){for(i in h||C([e]),s.remove(e,"fxshow"),f)n.style(e,i,f[i])})),l=dt(h?a[i]:0,i,m),i in a||(a[i]=l.start,h&&(l.end=l.start,l.start=0))}],prefilter:function(e,t){t?u.prefilters.unshift(e):u.prefilters.push(e)}}),n.speed=function(e,t,s){var i=e&&"object"==typeof e?n.extend({},e):{complete:s||!s&&t||o(e)&&e,duration:e,easing:s&&t||t&&!o(t)&&t};return n.fx.off?i.duration=0:"number"!=typeof i.duration&&(i.duration in n.fx.speeds?i.duration=n.fx.speeds[i.duration]:i.duration=n.fx.speeds._default),null!=i.queue&&!0!==i.queue||(i.queue="fx"),i.old=i.complete,i.complete=function(){o(i.old)&&i.old.call(this),i.queue&&n.dequeue(this,i.queue)},i},n.fn.extend({fadeTo:function(e,t,n,s){return this.filter($).css("opacity",0).show().end().animate({opacity:t},e,n,s)},animate:function(e,t,o,i){var c=n.isEmptyObject(e),r=n.speed(t,o,i),a=function(){var t=u(this,n.extend({},e),r);(c||s.get(this,"finish"))&&t.stop(!0)};return a.finish=a,c||!1===r.queue?this.each(a):this.queue(r.queue,a)},stop:function(e,t,o){var i=function(e){var t=e.stop;delete e.stop,t(o)};return"string"!=typeof e&&(o=t,t=e,e=void 0),t&&this.queue(e||"fx",[]),this.each(function(){var c=!0,t=e!=null&&e+"queueHooks",r=n.timers,a=s.get(this);if(t)a[t]&&a[t].stop&&i(a[t]);else for(t in a)a[t]&&a[t].stop&&at.test(t)&&i(a[t]);for(t=r.length;t--;)r[t].elem!==this||e!=null&&r[t].queue!==e||(r[t].anim.stop(o),c=!1,r.splice(t,1));!c&&o||n.dequeue(this,e)})},finish:function(e){return!1!==e&&(e=e||"fx"),this.each(function(){var t,a=s.get(this),o=a[e+"queue"],r=a[e+"queueHooks"],i=n.timers,c=o?o.length:0;for(a.finish=!0,n.queue(this,e,[]),r&&r.stop&&r.stop.call(this,!0),t=i.length;t--;)i[t].elem===this&&i[t].queue===e&&(i[t].anim.stop(!0),i.splice(t,1));for(t=0;t<c;t++)o[t]&&o[t].finish&&o[t].finish.call(this);delete a.finish})}}),n.each(["toggle","show","hide"],function(e,t){var s=n.fn[t];n.fn[t]=function(e,n,o){return e==null||"boolean"==typeof e?s.apply(this,arguments):this.animate(te(t,!0),e,n,o)}}),n.each({slideDown:te("show"),slideUp:te("hide"),slideToggle:te("toggle"),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"},fadeToggle:{opacity:"toggle"}},function(e,t){n.fn[e]=function(e,n,s){return this.animate(t,e,n,s)}}),n.timers=[],n.fx.tick=function(){var s,e=0,t=n.timers;for(z=Date.now();e<t.length;e++)(s=t[e])()||t[e]!==s||t.splice(e--,1);t.length||n.fx.stop(),z=void 0},n.fx.timer=function(e){n.timers.push(e),n.fx.start()},n.fx.interval=13,n.fx.start=function(){J||(J=!0,oe())},n.fx.stop=function(){J=null},n.fx.speeds={slow:600,fast:200,_default:400},n.fn.delay=function(t,s){return t=n.fx&&n.fx.speeds[t]||t,s=s||"fx",this.queue(s,function(n,s){var o=e.setTimeout(n,t);s.stop=function(){e.clearTimeout(o)}})},S=i.createElement("input"),ot=i.createElement("select").appendChild(i.createElement("option")),S.type="checkbox",a.checkOn=""!==S.value,a.optSelected=ot.selected,(S=i.createElement("input")).value="t",S.type="radio",a.radioValue="t"===S.value,N=n.expr.attrHandle,n.fn.extend({attr:function(e,t){return p(this,n.attr,e,t,1<arguments.length)},removeAttr:function(e){return this.each(function(){n.removeAttr(this,e)})}}),n.extend({attr:function(e,t,s){var o,i,a=e.nodeType;if(3!==a&&8!==a&&2!==a)return"undefined"==typeof e.getAttribute?n.prop(e,t,s):(1===a&&n.isXMLDoc(e)||(o=n.attrHooks[t.toLowerCase()]||(n.expr.match.bool.test(t)?Ot:void 0)),void 0!==s?null===s?void n.removeAttr(e,t):o&&"set"in o&&void 0!==(i=o.set(e,s,t))?i:(e.setAttribute(t,s+""),s):o&&"get"in o&&null!==(i=o.get(e,t))?i:null==(i=n.find.attr(e,t))?void 0:i)},attrHooks:{type:{set:function(e,t){if(!a.radioValue&&"radio"===t&&d(e,"input")){var n=e.value;return e.setAttribute("type",t),n&&(e.value=n),t}}}},removeAttr:function(e,t){var n,o=0,s=t&&t.match(h);if(s&&1===e.nodeType)for(;n=s[o++];)e.removeAttribute(n)}}),Ot={set:function(e,t,s){return!1===t?n.removeAttr(e,s):e.setAttribute(s,s),s}},n.each(n.expr.match.bool.source.match(/\w+/g),function(e,t){var s=N[t]||n.find.attr;N[t]=function(e,t,n){var i,a,o=t.toLowerCase();return n||(a=N[o],N[o]=i,i=null!=s(e,t,n)?o:null,N[o]=a),i}}),ft=/^(?:input|select|textarea|button)$/i,pt=/^(?:a|area)$/i;function y(e){return(e.match(h)||[]).join(" ")}function w(e){return e.getAttribute&&e.getAttribute("class")||""}function we(e){return Array.isArray(e)?e:"string"==typeof e&&e.match(h)||[]}n.fn.extend({prop:function(e,t){return p(this,n.prop,e,t,1<arguments.length)},removeProp:function(e){return this.each(function(){delete this[n.propFix[e]||e]})}}),n.extend({prop:function(e,t,s){var o,i,a=e.nodeType;if(3!==a&&8!==a&&2!==a)return 1===a&&n.isXMLDoc(e)||(t=n.propFix[t]||t,o=n.propHooks[t]),void 0!==s?o&&"set"in o&&void 0!==(i=o.set(e,s,t))?i:e[t]=s:o&&"get"in o&&null!==(i=o.get(e,t))?i:e[t]},propHooks:{tabIndex:{get:function(e){var t=n.find.attr(e,"tabindex");return t?parseInt(t,10):ft.test(e.nodeName)||pt.test(e.nodeName)&&e.href?0:-1}}},propFix:{for:"htmlFor",class:"className"}}),a.optSelected||(n.propHooks.selected={get:function(e){var t=e.parentNode;return t&&t.parentNode&&t.parentNode.selectedIndex,null},set:function(e){var t=e.parentNode;t&&(t.selectedIndex,t.parentNode&&t.parentNode.selectedIndex)}}),n.each(["tabIndex","readOnly","maxLength","cellSpacing","cellPadding","rowSpan","colSpan","useMap","frameBorder","contentEditable"],function(){n.propFix[this.toLowerCase()]=this}),n.fn.extend({addClass:function(e){var t,s,i,a,r,c;return o(e)?this.each(function(t){n(this).addClass(e.call(this,t,w(this)))}):(i=we(e)).length?this.each(function(){if(a=w(this),t=1===this.nodeType&&" "+y(a)+" "){for(s=0;s<i.length;s++)r=i[s],t.indexOf(" "+r+" ")<0&&(t+=r+" ");c=y(t),a!==c&&this.setAttribute("class",c)}}):this},removeClass:function(e){var t,s,i,a,r,c;return o(e)?this.each(function(t){n(this).removeClass(e.call(this,t,w(this)))}):arguments.length?(i=we(e)).length?this.each(function(){if(a=w(this),t=1===this.nodeType&&" "+y(a)+" "){for(s=0;s<i.length;s++)for(r=i[s];-1<t.indexOf(" "+r+" ");)t=t.replace(" "+r+" "," ");c=y(t),a!==c&&this.setAttribute("class",c)}}):this:this.attr("class","")},toggleClass:function(e,t){var i,a,r,c,l=typeof e,d="string"===l||Array.isArray(e);return o(e)?this.each(function(s){n(this).toggleClass(e.call(this,s,w(this),t),t)}):"boolean"==typeof t&&d?t?this.addClass(e):this.removeClass(e):(c=we(e),this.each(function(){if(d)for(r=n(this),a=0;a<c.length;a++)i=c[a],r.hasClass(i)?r.removeClass(i):r.addClass(i);else void 0!==e&&"boolean"!==l||((i=w(this))&&s.set(this,"__className__",i),this.setAttribute&&this.setAttribute("class",i||!1===e?"":s.get(this,"__className__")||""))}))},hasClass:function(e){for(var t,s=0,n=" "+e+" ";t=this[s++];)if(1===t.nodeType&&-1<(" "+y(w(t))+" ").indexOf(n))return!0;return!1}}),jt=/\r/g,n.fn.extend({val:function(e){var t,s,a,i=this[0];return arguments.length?(a=o(e),this.each(function(s){var o;1===this.nodeType&&(null==(o=a?e.call(this,s,n(this).val()):e)?o="":"number"==typeof o?o+="":Array.isArray(o)&&(o=n.map(o,function(e){return e==null?"":e+""})),(t=n.valHooks[this.type]||n.valHooks[this.nodeName.toLowerCase()])&&"set"in t&&void 0!==t.set(this,o,"value")||(this.value=o))})):i?(t=n.valHooks[i.type]||n.valHooks[i.nodeName.toLowerCase()])&&"get"in t&&void 0!==(s=t.get(i,"value"))?s:"string"==typeof(s=i.value)?s.replace(jt,""):s??"":void 0}}),n.extend({valHooks:{option:{get:function(e){var t=n.find.attr(e,"value");return t??y(n.text(e))}},select:{get:function(e){var t,s,a,r=e.options,o=e.selectedIndex,i="select-one"===e.type,c=i?null:[],l=i?o+1:r.length;for(s=o<0?l:i?o:0;s<l;s++)if(((t=r[s]).selected||s===o)&&!t.disabled&&(!t.parentNode.disabled||!d(t.parentNode,"optgroup"))){if(a=n(t).val(),i)return a;c.push(a)}return c},set:function(e,t){for(var s,o,i=e.options,a=n.makeArray(t),r=i.length;r--;)((o=i[r]).selected=-1<n.inArray(n.valHooks.option.get(o),a))&&(s=!0);return s||(e.selectedIndex=-1),a}}}}),n.each(["radio","checkbox"],function(){n.valHooks[this]={set:function(e,t){if(Array.isArray(t))return e.checked=-1<n.inArray(n(e).val(),t)}},a.checkOn||(n.valHooks[this].get=function(e){return null===e.getAttribute("value")?"on":e.value})}),a.focusin="onfocusin"in e,Oe=/^(?:focusinfocus|focusoutblur)$/,le=function(e){e.stopPropagation()},n.extend(n.event,{trigger:function(t,a,r,c){var d,u,h,m,f,p,b,j,g=[r||i],l=X.call(t,"type")?t.type:t,v=X.call(t,"namespace")?t.namespace.split("."):[];if(d=p=h=r=r||i,3!==r.nodeType&&8!==r.nodeType&&!Oe.test(l+n.event.triggered)&&(-1<l.indexOf(".")&&(l=(v=l.split(".")).shift(),v.sort()),m=l.indexOf(":")<0&&"on"+l,(t=t[n.expando]?t:new n.Event(l,"object"==typeof t&&t)).isTrigger=c?2:3,t.namespace=v.join("."),t.rnamespace=t.namespace?new RegExp("(^|\\.)"+v.join("\\.(?:.*\\.|)")+"(\\.|$)"):null,t.result=void 0,t.target||(t.target=r),a=a==null?[t]:n.makeArray(a,[t]),u=n.event.special[l]||{},c||!u.trigger||!1!==u.trigger.apply(r,a))){if(!c&&!u.noBubble&&!O(r)){for(j=u.delegateType||l,Oe.test(j+l)||(d=d.parentNode);d;d=d.parentNode)g.push(d),h=d;h===(r.ownerDocument||i)&&g.push(h.defaultView||h.parentWindow||e)}for(b=0;(d=g[b++])&&!t.isPropagationStopped();)p=d,t.type=1<b?j:u.bindType||l,(f=(s.get(d,"events")||Object.create(null))[t.type]&&s.get(d,"handle"))&&f.apply(d,a),(f=m&&d[m])&&f.apply&&T(d)&&(t.result=f.apply(d,a),!1===t.result&&t.preventDefault());return t.type=l,c||t.isDefaultPrevented()||u._default&&!1!==u._default.apply(g.pop(),a)||!T(r)||m&&o(r[l])&&!O(r)&&((h=r[m])&&(r[m]=null),n.event.triggered=l,t.isPropagationStopped()&&p.addEventListener(l,le),r[l](),t.isPropagationStopped()&&p.removeEventListener(l,le),n.event.triggered=void 0,h&&(r[m]=h)),t.result}},simulate:function(e,t,s){var o=n.extend(new n.Event,s,{type:e,isSimulated:!0});n.event.trigger(o,null,t)}}),n.fn.extend({trigger:function(e,t){return this.each(function(){n.event.trigger(e,t,this)})},triggerHandler:function(e,t){var s=this[0];if(s)return n.event.trigger(e,t,s,!0)}}),a.focusin||n.each({focus:"focusin",blur:"focusout"},function(e,t){var o=function(e){n.event.simulate(t,e.target,n.event.fix(e))};n.event.special[t]={setup:function(){var n=this.ownerDocument||this.document||this,i=s.access(n,t);i||n.addEventListener(e,o,!0),s.access(n,t,(i||0)+1)},teardown:function(){var n=this.ownerDocument||this.document||this,i=s.access(n,t)-1;i?s.access(n,t,i):(n.removeEventListener(e,o,!0),s.remove(n,t))}}});var B=e.location,ht={guid:Date.now()},pe=/\?/;n.parseXML=function(t){var s,o;if(!t||"string"!=typeof t)return null;try{s=(new e.DOMParser).parseFromString(t,"text/xml")}catch{}return o=s&&s.getElementsByTagName("parsererror")[0],s&&!o||n.error("Invalid XML: "+(o?n.map(o.childNodes,function(e){return e.textContent}).join(`
`):t)),s};var Ct=/\[\]$/,Ye=/\r?\n/g,kt=/^(?:submit|button|image|reset|file)$/i,At=/^(?:input|select|textarea|keygen)/i;function ue(e,t,s,o){var i;if(Array.isArray(t))n.each(t,function(t,n){s||Ct.test(e)?o(e,n):ue(e+"["+("object"==typeof n&&n!=null?t:"")+"]",n,s,o)});else if(s||"object"!==M(t))o(e,t);else for(i in t)ue(e+"["+i+"]",t[i],s,o)}n.param=function(e,t){var s,i=[],a=function(e,t){var n=o(t)?t():t;i[i.length]=encodeURIComponent(e)+"="+encodeURIComponent(n??"")};if(e==null)return"";if(Array.isArray(e)||e.jquery&&!n.isPlainObject(e))n.each(e,function(){a(this.name,this.value)});else for(s in e)ue(s,e[s],t,a);return i.join("&")},n.fn.extend({serialize:function(){return n.param(this.serializeArray())},serializeArray:function(){return this.map(function(){var e=n.prop(this,"elements");return e?n.makeArray(e):this}).filter(function(){var e=this.type;return this.name&&!n(this).is(":disabled")&&At.test(this.nodeName)&&!kt.test(e)&&(this.checked||!I.test(e))}).map(function(e,t){var s=n(this).val();return s==null?null:Array.isArray(s)?n.map(s,function(e){return{name:t.name,value:e.replace(Ye,`
`)}}):{name:t.name,value:s.replace(Ye,`
`)}}).get()}});var Mt=/%20/g,Xt=/#.*$/,Tt=/([?&])_=[^&]*/,zt=/^(.*?):[ \t]*([^\r\n]*)$/gm,Dt=/^(?:GET|HEAD)$/,Nt=/^\/\//,He={},be={},Me="*/".concat("*"),je=i.createElement("a");function ke(e){return function(t,n){"string"!=typeof t&&(n=t,t="*");var s,i=0,a=t.toLowerCase().match(h)||[];if(o(n))for(;s=a[i++];)"+"===s[0]?(s=s.slice(1)||"*",(e[s]=e[s]||[]).unshift(n)):(e[s]=e[s]||[]).push(n)}}function Ee(e,t,s,o){var i={},r=e===be;function a(c){var l;return i[c]=!0,n.each(e[c]||[],function(e,n){var c=n(t,s,o);return"string"!=typeof c||r||i[c]?r?!(l=c):void 0:(t.dataTypes.unshift(c),a(c),!1)}),l}return a(t.dataTypes[0])||!i["*"]&&a("*")}function ie(e,t){var s,o,i=n.ajaxSettings.flatOptions||{};for(s in t)void 0!==t[s]&&((i[s]?e:o||(o={}))[s]=t[s]);return o&&n.extend(!0,e,o),e}return je.href=B.href,n.extend({active:0,lastModified:{},etag:{},ajaxSettings:{url:B.href,type:"GET",isLocal:/^(?:about|app|app-storage|.+-extension|file|res|widget):$/.test(B.protocol),global:!0,processData:!0,async:!0,contentType:"application/x-www-form-urlencoded; charset=UTF-8",accepts:{"*":Me,text:"text/plain",html:"text/html",xml:"application/xml, text/xml",json:"application/json, text/javascript"},contents:{xml:/\bxml\b/,html:/\bhtml/,json:/\bjson\b/},responseFields:{xml:"responseXML",text:"responseText",json:"responseJSON"},converters:{"* text":String,"text html":!0,"text json":JSON.parse,"text xml":n.parseXML},flatOptions:{url:!0,context:!0}},ajaxSetup:function(e,t){return t?ie(ie(e,n.ajaxSettings),t):ie(n.ajaxSettings,e)},ajaxPrefilter:ke(He),ajaxTransport:ke(be),ajax:function(t,s){"object"==typeof t&&(s=t,t=void 0),s=s||{};var r,c,d,u,m,p,g,b,_,w,o=n.ajaxSetup({},s),l=o.context||o,y=o.context&&(l.nodeType||l.jquery)?n(l):n.event,j=n.Deferred(),E=n.Callbacks("once memory"),v=o.statusCode||{},C={},O={},x="canceled",a={readyState:0,getResponseHeader:function(e){if(c){if(!u){u={};for(var t;t=zt.exec(w);)u[t[1].toLowerCase()+" "]=(u[t[1].toLowerCase()+" "]||[]).concat(t[2])}t=u[e.toLowerCase()+" "]}return t==null?null:t.join(", ")},getAllResponseHeaders:function(){return c?w:null},setRequestHeader:function(e,t){return c==null&&(e=O[e.toLowerCase()]=O[e.toLowerCase()]||e,C[e]=t),this},overrideMimeType:function(e){return c==null&&(o.mimeType=e),this},statusCode:function(e){var t;if(e)if(c)a.always(e[a.status]);else for(t in e)v[t]=[v[t],e[t]];return this},abort:function(e){var t=e||x;return m&&m.abort(t),f(0,t),this}};if(j.promise(a),o.url=((t||o.url||B.href)+"").replace(Nt,B.protocol+"//"),o.type=s.method||s.type||o.method||o.type,o.dataTypes=(o.dataType||"*").toLowerCase().match(h)||[""],null==o.crossDomain){d=i.createElement("a");try{d.href=o.url,d.href=d.href,o.crossDomain=je.protocol+"//"+je.host!=d.protocol+"//"+d.host}catch{o.crossDomain=!0}}if(o.data&&o.processData&&"string"!=typeof o.data&&(o.data=n.param(o.data,o.traditional)),Ee(He,o,s,a),c)return a;for(b in(g=n.event&&o.global)&&0==n.active++&&n.event.trigger("ajaxStart"),o.type=o.type.toUpperCase(),o.hasContent=!Dt.test(o.type),r=o.url.replace(Xt,""),o.hasContent?o.data&&o.processData&&0===(o.contentType||"").indexOf("application/x-www-form-urlencoded")&&(o.data=o.data.replace(Mt,"+")):(p=o.url.slice(r.length),o.data&&(o.processData||"string"==typeof o.data)&&(r+=(pe.test(r)?"&":"?")+o.data,delete o.data),!1===o.cache&&(r=r.replace(Tt,"$1"),p=(pe.test(r)?"&":"?")+"_="+ht.guid+++p),o.url=r+p),o.ifModified&&(n.lastModified[r]&&a.setRequestHeader("If-Modified-Since",n.lastModified[r]),n.etag[r]&&a.setRequestHeader("If-None-Match",n.etag[r])),(o.data&&o.hasContent&&!1!==o.contentType||s.contentType)&&a.setRequestHeader("Content-Type",o.contentType),a.setRequestHeader("Accept",o.dataTypes[0]&&o.accepts[o.dataTypes[0]]?o.accepts[o.dataTypes[0]]+("*"!==o.dataTypes[0]?", "+Me+"; q=0.01":""):o.accepts["*"]),o.headers)a.setRequestHeader(b,o.headers[b]);if(o.beforeSend&&(!1===o.beforeSend.call(l,a,o)||c))return a.abort();if(x="abort",E.add(o.complete),a.done(o.success),a.fail(o.error),m=Ee(be,o,s,a)){if(a.readyState=1,g&&y.trigger("ajaxSend",[a,o]),c)return a;o.async&&0<o.timeout&&(_=e.setTimeout(function(){a.abort("timeout")},o.timeout));try{c=!1,m.send(C,f)}catch(e){if(c)throw e;f(-1,e)}}else f(-1,"No Transport");function f(t,s,i,d){var h,f,p,b,O,u=s;c||(c=!0,_&&e.clearTimeout(_),m=void 0,w=d||"",a.readyState=0<t?4:0,h=200<=t&&t<300||304===t,i&&(f=function(e,t,n){for(var o,i,a,r,c=e.contents,s=e.dataTypes;"*"===s[0];)s.shift(),void 0===a&&(a=e.mimeType||t.getResponseHeader("Content-Type"));if(a)for(o in c)if(c[o]&&c[o].test(a)){s.unshift(o);break}if(s[0]in n)i=s[0];else{for(o in n){if(!s[0]||e.converters[o+" "+s[0]]){i=o;break}r||(r=o)}i=i||r}if(i)return i!==s[0]&&s.unshift(i),n[i]}(o,a,i)),!h&&-1<n.inArray("script",o.dataTypes)&&n.inArray("json",o.dataTypes)<0&&(o.converters["text script"]=function(){}),f=function(e,t,n,s){var o,i,a,c,l,r={},d=e.dataTypes.slice();if(d[1])for(i in e.converters)r[i.toLowerCase()]=e.converters[i];for(o=d.shift();o;)if(e.responseFields[o]&&(n[e.responseFields[o]]=t),!a&&s&&e.dataFilter&&(t=e.dataFilter(t,e.dataType)),a=o,o=d.shift())if("*"===o)o=a;else if("*"!==a&&a!==o){if(!(i=r[a+" "+o]||r["* "+o]))for(l in r)if((c=l.split(" "))[1]===o&&(i=r[a+" "+c[0]]||r["* "+c[0]])){!0===i?i=r[l]:!0!==r[l]&&(o=c[0],d.unshift(c[1]));break}if(!0!==i)if(i&&e.throws)t=i(t);else try{t=i(t)}catch(e){return{state:"parsererror",error:i?e:"No conversion from "+a+" to "+o}}}return{state:"success",data:t}}(o,f,a,h),h?(o.ifModified&&((b=a.getResponseHeader("Last-Modified"))&&(n.lastModified[r]=b),(b=a.getResponseHeader("etag"))&&(n.etag[r]=b)),204===t||"HEAD"===o.type?u="nocontent":304===t?u="notmodified":(u=f.state,O=f.data,h=!(p=f.error))):(p=u,!t&&u||(u="error",t<0&&(t=0))),a.status=t,a.statusText=(s||u)+"",h?j.resolveWith(l,[O,u,a]):j.rejectWith(l,[a,u,p]),a.statusCode(v),v=void 0,g&&y.trigger(h?"ajaxSuccess":"ajaxError",[a,o,h?O:p]),E.fireWith(l,[a,u]),g&&(y.trigger("ajaxComplete",[a,o]),--n.active||n.event.trigger("ajaxStop")))}return a},getJSON:function(e,t,s){return n.get(e,t,s,"json")},getScript:function(e,t){return n.get(e,void 0,t,"script")}}),n.each(["get","post"],function(e,t){n[t]=function(e,s,i,a){return o(s)&&(a=a||i,i=s,s=void 0),n.ajax(n.extend({url:e,type:t,dataType:a,data:s,success:i},n.isPlainObject(e)&&e))}}),n.ajaxPrefilter(function(e){var t;for(t in e.headers)"content-type"===t.toLowerCase()&&(e.contentType=e.headers[t]||"")}),n._evalUrl=function(e,t,s){return n.ajax({url:e,type:"GET",dataType:"script",cache:!0,async:!1,global:!1,converters:{"text script":function(){}},dataFilter:function(e){n.globalEval(e,t,s)}})},n.fn.extend({wrapAll:function(e){var t;return this[0]&&(o(e)&&(e=e.call(this[0])),t=n(e,this[0].ownerDocument).eq(0).clone(!0),this[0].parentNode&&t.insertBefore(this[0]),t.map(function(){for(var e=this;e.firstElementChild;)e=e.firstElementChild;return e}).append(this)),this},wrapInner:function(e){return o(e)?this.each(function(t){n(this).wrapInner(e.call(this,t))}):this.each(function(){var t=n(this),s=t.contents();s.length?s.wrapAll(e):t.append(e)})},wrap:function(e){var t=o(e);return this.each(function(s){n(this).wrapAll(t?e.call(this,s):e)})},unwrap:function(e){return this.parent(e).not("body").each(function(){n(this).replaceWith(this.childNodes)}),this}}),n.expr.pseudos.hidden=function(e){return!n.expr.pseudos.visible(e)},n.expr.pseudos.visible=function(e){return!!(e.offsetWidth||e.offsetHeight||e.getClientRects().length)},n.ajaxSettings.xhr=function(){try{return new e.XMLHttpRequest}catch{}},Ue={0:200,1223:204},R=n.ajaxSettings.xhr(),a.cors=!!R&&"withCredentials"in R,a.ajax=R=!!R,n.ajaxTransport(function(t){var n,s;if(a.cors||R&&!t.crossDomain)return{send:function(o,i){var r,a=t.xhr();if(a.open(t.type,t.url,t.async,t.username,t.password),t.xhrFields)for(r in t.xhrFields)a[r]=t.xhrFields[r];for(r in t.mimeType&&a.overrideMimeType&&a.overrideMimeType(t.mimeType),t.crossDomain||o["X-Requested-With"]||(o["X-Requested-With"]="XMLHttpRequest"),o)a.setRequestHeader(r,o[r]);n=function(e){return function(){n&&(n=s=a.onload=a.onerror=a.onabort=a.ontimeout=a.onreadystatechange=null,"abort"===e?a.abort():"error"===e?"number"!=typeof a.status?i(0,"error"):i(a.status,a.statusText):i(Ue[a.status]||a.status,a.statusText,"text"!==(a.responseType||"text")||"string"!=typeof a.responseText?{binary:a.response}:{text:a.responseText},a.getAllResponseHeaders()))}},a.onload=n(),s=a.onerror=a.ontimeout=n("error"),void 0!==a.onabort?a.onabort=s:a.onreadystatechange=function(){4===a.readyState&&e.setTimeout(function(){n&&s()})},n=n("abort");try{a.send(t.hasContent&&t.data||null)}catch(e){if(n)throw e}},abort:function(){n&&n()}}}),n.ajaxPrefilter(function(e){e.crossDomain&&(e.contents.script=!1)}),n.ajaxSetup({accepts:{script:"text/javascript, application/javascript, application/ecmascript, application/x-ecmascript"},contents:{script:/\b(?:java|ecma)script\b/},converters:{"text script":function(e){return n.globalEval(e),e}}}),n.ajaxPrefilter("script",function(e){void 0===e.cache&&(e.cache=!1),e.crossDomain&&(e.type="GET")}),n.ajaxTransport("script",function(e){var t,s;if(e.crossDomain||e.scriptAttrs)return{send:function(o,a){s=n("<script>").attr(e.scriptAttrs||{}).prop({charset:e.scriptCharset,src:e.url}).on("load error",t=function(e){s.remove(),t=null,e&&a("error"===e.type?404:200,e.type)}),i.head.appendChild(s[0])},abort:function(){t&&t()}}}),xe=[],K=/(=)\?(?=&|$)|\?\?/,n.ajaxSetup({jsonp:"callback",jsonpCallback:function(){var e=xe.pop()||n.expando+"_"+ht.guid++;return this[e]=!0,e}}),n.ajaxPrefilter("json jsonp",function(t,s,i){var a,r,c,l=!1!==t.jsonp&&(K.test(t.url)?"url":"string"==typeof t.data&&0===(t.contentType||"").indexOf("application/x-www-form-urlencoded")&&K.test(t.data)&&"data");if(l||"jsonp"===t.dataTypes[0])return a=t.jsonpCallback=o(t.jsonpCallback)?t.jsonpCallback():t.jsonpCallback,l?t[l]=t[l].replace(K,"$1"+a):!1!==t.jsonp&&(t.url+=(pe.test(t.url)?"&":"?")+t.jsonp+"="+a),t.converters["script json"]=function(){return c||n.error(a+" was not called"),c[0]},t.dataTypes[0]="json",r=e[a],e[a]=function(){c=arguments},i.always(function(){void 0===r?n(e).removeProp(a):e[a]=r,t[a]&&(t.jsonpCallback=s.jsonpCallback,xe.push(a)),c&&o(r)&&r(c[0]),c=r=void 0}),"script"}),a.createHTMLDocument=((qe=i.implementation.createHTMLDocument("").body).innerHTML="<form></form><form></form>",2===qe.childNodes.length),n.parseHTML=function(e,t,s){return"string"!=typeof e?[]:("boolean"==typeof t&&(s=t,t=!1),t||(a.createHTMLDocument?((c=(t=i.implementation.createHTMLDocument("")).createElement("base")).href=i.location.href,t.head.appendChild(c)):t=i),o=!s&&[],(r=se.exec(e))?[t.createElement(r[1])]:(r=Ae([e],t,o),o&&o.length&&n(o).remove(),n.merge([],r.childNodes)));var o,r,c},n.fn.load=function(e,t,s){var i,c,l,a=this,r=e.indexOf(" ");return-1<r&&(i=y(e.slice(r)),e=e.slice(0,r)),o(t)?(s=t,t=void 0):t&&"object"==typeof t&&(c="POST"),0<a.length&&n.ajax({url:e,type:c||"GET",dataType:"html",data:t}).done(function(e){l=arguments,a.html(i?n("<div>").append(n.parseHTML(e)).find(i):e)}).always(s&&function(e,t){a.each(function(){s.apply(this,l||[e.responseText,t,e])})}),this},n.expr.pseudos.animated=function(e){return n.grep(n.timers,function(t){return e===t.elem}).length},n.offset={setOffset:function(e,t,s){var a,c,l,d,h,m,r=n.css(e,"position"),u=n(e),i={};"static"===r&&(e.style.position="relative"),a=u.offset(),h=n.css(e,"top"),d=n.css(e,"left"),("absolute"===r||"fixed"===r)&&-1<(h+d).indexOf("auto")?(l=(m=u.position()).top,c=m.left):(l=parseFloat(h)||0,c=parseFloat(d)||0),o(t)&&(t=t.call(e,s,n.extend({},a))),null!=t.top&&(i.top=t.top-a.top+l),null!=t.left&&(i.left=t.left-a.left+c),"using"in t?t.using.call(e,i):u.css(i)}},n.fn.extend({offset:function(e){if(arguments.length)return void 0===e?this:this.each(function(t){n.offset.setOffset(this,e,t)});var s,o,t=this[0];return t?t.getClientRects().length?(s=t.getBoundingClientRect(),o=t.ownerDocument.defaultView,{top:s.top+o.pageYOffset,left:s.left+o.pageXOffset}):{top:0,left:0}:void 0},position:function(){if(this[0]){var e,s,o,t=this[0],i={top:0,left:0};if("fixed"===n.css(t,"position"))s=t.getBoundingClientRect();else{for(s=this.offset(),o=t.ownerDocument,e=t.offsetParent||o.documentElement;e&&(e===o.body||e===o.documentElement)&&"static"===n.css(e,"position");)e=e.parentNode;e&&e!==t&&1===e.nodeType&&((i=n(e).offset()).top+=n.css(e,"borderTopWidth",!0),i.left+=n.css(e,"borderLeftWidth",!0))}return{top:s.top-i.top-n.css(t,"marginTop",!0),left:s.left-i.left-n.css(t,"marginLeft",!0)}}},offsetParent:function(){return this.map(function(){for(var e=this.offsetParent;e&&"static"===n.css(e,"position");)e=e.offsetParent;return e||_})}}),n.each({scrollLeft:"pageXOffset",scrollTop:"pageYOffset"},function(e,t){var s="pageYOffset"===t;n.fn[e]=function(n){return p(this,function(e,n,o){var i;if(O(e)?i=e:9===e.nodeType&&(i=e.defaultView),void 0===o)return i?i[t]:e[n];i?i.scrollTo(s?i.pageXOffset:o,s?o:i.pageYOffset):e[n]=o},e,n,arguments.length)}}),n.each(["top","left"],function(e,t){n.cssHooks[t]=$e(a.pixelPosition,function(e,s){if(s)return s=V(e,t),me.test(s)?n(e).position()[t]+"px":s})}),n.each({Height:"height",Width:"width"},function(e,t){n.each({padding:"inner"+e,content:t,"":"outer"+e},function(s,o){n.fn[o]=function(i,a){var r=arguments.length&&(s||"boolean"!=typeof i),c=s||(!0===i||!0===a?"margin":"border");return p(this,function(t,s,i){var a;return O(t)?0===o.indexOf("outer")?t["inner"+e]:t.document.documentElement["client"+e]:9===t.nodeType?(a=t.documentElement,Math.max(t.body["scroll"+e],a["scroll"+e],t.body["offset"+e],a["offset"+e],a["client"+e])):void 0===i?n.css(t,s,c):n.style(t,s,i,c)},t,r?i:void 0,r)}})}),n.each(["ajaxStart","ajaxStop","ajaxComplete","ajaxError","ajaxSuccess","ajaxSend"],function(e,t){n.fn[t]=function(e){return this.on(t,e)}}),n.fn.extend({bind:function(e,t,n){return this.on(e,null,t,n)},unbind:function(e,t){return this.off(e,null,t)},delegate:function(e,t,n,s){return this.on(t,e,n,s)},undelegate:function(e,t,n){return 1===arguments.length?this.off(e,"**"):this.off(t,e||"**",n)},hover:function(e,t){return this.mouseenter(e).mouseleave(t||e)}}),n.each("blur focus focusin focusout resize scroll click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup contextmenu".split(" "),function(e,t){n.fn[t]=function(e,n){return 0<arguments.length?this.on(t,null,e,n):this.trigger(t)}}),ut=/^[\s\uFEFF\xA0]+|([^\s\uFEFF\xA0])[\s\uFEFF\xA0]+$/g,n.proxy=function(e,t){var s,i,a;if("string"==typeof t&&(s=e[t],t=e,e=s),o(e))return i=j.call(arguments,2),(a=function(){return e.apply(t||this,i.concat(j.call(arguments)))}).guid=e.guid=e.guid||n.guid++,a},n.holdReady=function(e){e?n.readyWait++:n.ready(!0)},n.isArray=Array.isArray,n.parseJSON=JSON.parse,n.nodeName=d,n.isFunction=o,n.isWindow=O,n.camelCase=f,n.type=M,n.now=Date.now,n.isNumeric=function(e){var t=n.type(e);return("number"===t||"string"===t)&&!isNaN(e-parseFloat(e))},n.trim=function(e){return e==null?"":(e+"").replace(ut,"$1")},"function"==typeof define&&define.amd&&define("jquery",[],function(){return n}),mt=e.jQuery,Be=e.$,n.noConflict=function(t){return e.$===n&&(e.$=Be),t&&e.jQuery===n&&(e.jQuery=mt),n},"undefined"==typeof t&&(e.jQuery=e.$=n),n})