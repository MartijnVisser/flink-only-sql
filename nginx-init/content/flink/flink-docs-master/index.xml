<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Flink Documentation on Apache Flink</title>
    <link>//localhost/flink/flink-docs-master/</link>
    <description>Recent content in Apache Flink Documentation on Apache Flink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="//localhost/flink/flink-docs-master/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Event Processing (CEP)</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/cep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/cep/</guid>
      <description>FlinkCEP - Complex event processing for Flink # FlinkCEP is the Complex Event Processing (CEP) library implemented on top of Flink. It allows you to detect event patterns in an endless stream of events, giving you the opportunity to get hold of what&amp;rsquo;s important in your data.
This page describes the API calls available in Flink CEP. We start by presenting the Pattern API, which allows you to specify the patterns that you want to detect in your stream, before presenting how you can detect and act upon matching event sequences.</description>
    </item>
    
    <item>
      <title>Execution Configuration</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/</guid>
      <description>Execution Configuration # The StreamExecutionEnvironment contains the ExecutionConfig which allows to set job specific configuration values for the runtime. To change the defaults that affect all jobs, see Configuration.
Java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ExecutionConfig executionConfig = env.getConfig(); Scala val env = StreamExecutionEnvironment.getExecutionEnvironment var executionConfig = env.getConfig Python env = StreamExecutionEnvironment.get_execution_environment() execution_config = env.get_config() The following configuration options are available: (the default is bold)
setClosureCleanerLevel(). The closure cleaner level is set to ClosureCleanerLevel.</description>
    </item>
    
    <item>
      <title>First steps</title>
      <link>//localhost/flink/flink-docs-master/docs/try-flink/local_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/try-flink/local_installation/</guid>
      <description>First steps # Welcome to Flink! :)
Flink is designed to process continuous streams of data at a lightning fast pace. This short guide will show you how to download the latest stable version of Flink, install, and run it. You will also run an example Flink job and view it in the web UI.
Downloading Flink # Note: Flink is also available as a Docker image. Flink runs on all UNIX-like environments, i.</description>
    </item>
    
    <item>
      <title>Formats</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/overview/</guid>
      <description> Formats # Flink provides a set of table formats that can be used with table connectors. A table format is a storage format defines how to map binary data onto table columns.
Flink supports the following formats:
Formats Supported Connectors CSV Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem JSON Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem, Elasticsearch Apache Avro Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem Confluent Avro Apache Kafka, Upsert Kafka Debezium CDC Apache Kafka, Filesystem Canal CDC Apache Kafka, Filesystem Maxwell CDC Apache Kafka, Filesystem OGG CDC Apache Kafka, Filesystem Apache Parquet Filesystem Apache ORC Filesystem Raw Apache Kafka, Upsert Kafka, Amazon Kinesis Data Streams, Filesystem </description>
    </item>
    
    <item>
      <title>Intro to the Python DataStream API</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/intro_to_datastream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/intro_to_datastream_api/</guid>
      <description>Intro to the Python DataStream API # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal).
Python DataStream API is a Python version of DataStream API which allows Python users could write Python DatStream API jobs.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/concepts/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/concepts/overview/</guid>
      <description>Concepts # The Hands-on Training explains the basic concepts of stateful and timely stream processing that underlie Flink&amp;rsquo;s APIs, and provides examples of how these mechanisms are used in applications. Stateful stream processing is introduced in the context of Data Pipelines &amp;amp; ETL and is further developed in the section on Fault Tolerance. Timely stream processing is introduced in the section on Streaming Analytics.
This Concepts in Depth section provides a deeper understanding of how Flink&amp;rsquo;s architecture and runtime implement these concepts.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/overview/</guid>
      <description>DataStream Formats # Available Formats # Formats define how information is encoded for storage. Currently these formats are supported:
Avro Azure Table Hadoop Parquet Text files Back to top</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/overview/</guid>
      <description>DataStream Connectors # Predefined Sources and Sinks # A few basic data sources and sinks are built into Flink and are always available. The predefined data sources include reading from files, directories, and sockets, and ingesting data from collections and iterators. The predefined data sinks support writing to files, to stdout and stderr, and to sockets.
Bundled Connectors # Connectors provide code for interfacing with various third-party systems. Currently these systems are supported:</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hive/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hive/overview/</guid>
      <description>Apache Hive # Apache Hive has established itself as a focal point of the data warehousing ecosystem. It serves as not only a SQL engine for big data analytics and ETL, but also a data management platform, where data is discovered, defined, and evolved.
Flink offers a two-fold integration with Hive.
The first is to leverage Hive&amp;rsquo;s Metastore as a persistent catalog with Flink&amp;rsquo;s HiveCatalog for storing Flink specific metadata across sessions.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/overview/</guid>
      <description>Table &amp;amp; SQL Connectors # Flink&amp;rsquo;s Table API &amp;amp; SQL programs can be connected to other external systems for reading and writing both batch and streaming tables. A table source provides access to data which is stored in external systems (such as a database, key-value store, message queue, or file system). A table sink emits a table to an external storage system. Depending on the type of source and sink, they support different formats such as CSV, Avro, Parquet, or ORC.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/overview/</guid>
      <description>File Systems # Apache Flink uses file systems to consume and persistently store data, both for the results of applications and for fault tolerance and recovery. These are some of most of the popular file systems, including local, hadoop-compatible, Amazon S3, Aliyun OSS and Azure Blob Storage.
The file system used for a particular file is determined by its URI scheme. For example, file:///home/user/text.txt refers to a file in the local file system, while hdfs://namenode:50010/data/user/text.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/ha/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/ha/overview/</guid>
      <description>High Availability # JobManager High Availability (HA) hardens a Flink cluster against JobManager failures. This feature ensures that a Flink cluster will always continue executing your submitted jobs.
JobManager High Availability # The JobManager coordinates every Flink deployment. It is responsible for both scheduling and resource management.
By default, there is a single JobManager instance per Flink cluster. This creates a single point of failure (SPOF): if the JobManager crashes, no new programs can be submitted and running programs fail.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/overview/</guid>
      <description>Deployment # Flink is a versatile framework, supporting many different deployment scenarios in a mix and match fashion.
Below, we briefly explain the building blocks of a Flink cluster, their purpose and available implementations. If you just want to start Flink locally, we recommend setting up a Standalone Cluster.
Overview and Reference Architecture # The figure below shows the building blocks of every Flink cluster. There is always somewhere a client running.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/overview/</guid>
      <description>Project Configuration # The guides in this section will show you how to configure your projects via popular build tools (Maven, Gradle), add the necessary dependencies (i.e. connectors and formats, testing), and cover some advanced configuration topics.
Every Flink application depends on a set of Flink libraries. At a minimum, the application depends on the Flink APIs and, in addition, on certain connector libraries (i.e. Kafka, Cassandra) and 3rd party dependencies required to the user to develop custom functions to process the data.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/overview/</guid>
      <description>DataSet API # DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping). The data sets are initially created from certain sources (e.g., by reading files, or from local collections). Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/types_serialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/types_serialization/</guid>
      <description>Data Types &amp;amp; Serialization # Apache Flink handles data types and serialization in a unique way, containing its own type descriptors, generic type extraction, and type serialization framework. This document describes the concepts and the rationale behind them.
Supported Data Types # Flink places some restrictions on the type of elements that can be in a DataStream. The reason for this is that the system analyzes the types to determine efficient execution strategies.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/overview/</guid>
      <description>Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
This section gives a description of the basic transformations, the effective physical partitioning after applying those as well as insights into Flink&amp;rsquo;s operator chaining.
DataStream Transformations # Map # DataStream → DataStream # Takes one element and produces one element. A map function that doubles the values of the input stream:</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/overview/</guid>
      <description>Flink DataStream API Programming Guide # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). The data streams are initially created from various sources (e.g., message queues, socket streams, files). Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). Flink programs run in a variety of contexts, standalone, or embedded in other programs.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/overview/</guid>
      <description>Operators # Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated dataflow topologies.
DataStream Transformations # DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., mapping, filtering, reducing). Please see operators for an overview of the available transformations in Python DataStream API.
Functions # Transformations accept user-defined functions as input to define the functionality of the transformations.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/overview/</guid>
      <description>Python API # PyFlink is a Python API for Apache Flink that allows you to build scalable batch and streaming workloads, such as real-time data processing pipelines, large-scale exploratory data analysis, Machine Learning (ML) pipelines and ETL processes. If you&amp;rsquo;re already familiar with Python and libraries such as Pandas, then PyFlink makes it simpler to leverage the full capabilities of the Flink ecosystem. Depending on the level of abstraction you need, there are two different APIs that can be used in PyFlink:</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/overview/</guid>
      <description>User-defined Functions # PyFlink Table API empowers users to do data transformations with Python user-defined functions.
Currently, it supports two kinds of Python user-defined functions: the general Python user-defined functions which process data one row at a time and vectorized Python user-defined functions which process data one batch at a time.
Bundling UDFs # To run Python UDFs (as well as Pandas UDFs) in any non-local mode, it is strongly recommended bundling your Python UDF definitions using the config option python-files, if your Python UDFs live outside the file where the main() function is defined.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/concepts/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/concepts/overview/</guid>
      <description>Streaming Concepts # Flink&amp;rsquo;s Table API and SQL support are unified APIs for batch and stream processing. This means that Table API and SQL queries have the same semantics regardless whether their input is bounded batch input or unbounded stream input.
The following pages explain concepts, practical limitations, and stream-specific configuration parameters of Flink&amp;rsquo;s relational APIs on streaming data.
State Management # Table programs that run in streaming mode leverage all capabilities of Flink as a stateful stream processor.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/functions/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/functions/overview/</guid>
      <description>Functions # Flink Table API &amp;amp; SQL empowers users to do data transformations with functions.
Types of Functions # There are two dimensions to classify functions in Flink.
One dimension is system (or built-in) functions v.s. catalog functions. System functions have no namespace and can be referenced with just their names. Catalog functions belong to a catalog and database therefore they have catalog and database namespaces, they can be referenced by either fully/partially qualified name (catalog.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/overview/</guid>
      <description>Table API &amp;amp; SQL # Apache Flink features two relational APIs - the Table API and SQL - for unified stream and batch processing. The Table API is a language-integrated query API for Java, Scala, and Python that allows the composition of queries from relational operators such as selection, filter, and join in a very intuitive way. Flink&amp;rsquo;s SQL support is based on Apache Calcite which implements the SQL standard.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/overview/</guid>
      <description>Queries # SELECT statements and VALUES statements are specified with the sqlQuery() method of the TableEnvironment. The method returns the result of the SELECT statement (or the VALUES statements) as a Table. A Table can be used in subsequent SQL and Table API queries, be converted into a DataStream, or written to a TableSink. SQL and Table API queries can be seamlessly mixed and are holistically optimized and translated into a single program.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/overview/</guid>
      <description>Learn Flink: Hands-On Training # Goals and Scope of this Training # This training presents an introduction to Apache Flink that includes just enough to get you started writing scalable streaming ETL, analytics, and event-driven applications, while leaving out a lot of (ultimately important) details. The focus is on providing straightforward introductions to Flink’s APIs for managing state and time, with the expectation that having mastered these fundamentals, you’ll be much better equipped to pick up the rest of what you need to know from the more detailed reference documentation.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/overview/</guid>
      <description>Gelly: Flink Graph API # Gelly is a Graph API for Flink. It contains a set of methods and utilities which aim to simplify the development of graph analysis applications in Flink. In Gelly, graphs can be transformed and modified using high-level functions similar to the ones provided by the batch processing API. Gelly provides methods to create, transform and modify graphs, as well as a library of graph algorithms.</description>
    </item>
    
    <item>
      <title>Set up Flink&#39;s Process Memory</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup/</guid>
      <description>Set up Flink&amp;rsquo;s Process Memory # Apache Flink provides efficient workloads on top of the JVM by tightly controlling the memory usage of its various components. While the community strives to offer sensible defaults to all configurations, the full breadth of applications that users deploy on Flink means this isn&amp;rsquo;t always possible. To provide the most production value to our users, Flink allows both high-level and fine-grained tuning of memory allocation within clusters.</description>
    </item>
    
    <item>
      <title>SQL</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/overview/</guid>
      <description>SQL # This page describes the SQL language supported in Flink, including Data Definition Language (DDL), Data Manipulation Language (DML) and Query Language. Flink’s SQL support is based on Apache Calcite which implements the SQL standard.
This page lists all the supported statements supported in Flink SQL for now:
SELECT (Queries) CREATE TABLE, CATALOG, DATABASE, VIEW, FUNCTION DROP TABLE, DATABASE, VIEW, FUNCTION ALTER TABLE, DATABASE, FUNCTION ANALYZE TABLE INSERT DESCRIBE EXPLAIN USE SHOW LOAD UNLOAD Data Types # Please see the dedicated page about data types.</description>
    </item>
    
    <item>
      <title>Common Configurations</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/common/</guid>
      <description>Common Configurations # Apache Flink provides several standard configuration settings that work across all file system implementations.
Default File System # A default scheme (and authority) is used if paths to files do not explicitly specify a file system scheme (and authority).
fs.default-scheme: &amp;lt;default-fs&amp;gt; For example, if the default file system configured as fs.default-scheme: hdfs://localhost:9000/, then a file path of /user/hugo/in.txt is interpreted as hdfs://localhost:9000/user/hugo/in.txt.
Connection limiting # You can limit the total number of connections that a file system can concurrently open which is useful when the file system cannot handle a large number of concurrent reads/writes or open connections at the same time.</description>
    </item>
    
    <item>
      <title>Concepts &amp; Common API</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/common/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/common/</guid>
      <description>Concepts &amp;amp; Common API # The Table API and SQL are integrated in a joint API. The central concept of this API is a Table which serves as input and output of queries. This document shows the common structure of programs with Table API and SQL queries, how to register a Table, how to query a Table, and how to emit a Table.
Structure of Table API and SQL Programs # The following code example shows the common structure of Table API and SQL programs.</description>
    </item>
    
    <item>
      <title>CSV</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/csv/</guid>
      <description>CSV Format # Format: Serialization Schema Format: Deserialization Schema
The CSV format allows to read and write CSV data based on an CSV schema. Currently, the CSV schema is derived from table schema.
Dependencies # In order to use the CSV format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.</description>
    </item>
    
    <item>
      <title>Debugging Windows &amp; Event Time</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/debugging/debugging_event_time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/debugging/debugging_event_time/</guid>
      <description>Debugging Windows &amp;amp; Event Time # Monitoring Current Event Time # Flink&amp;rsquo;s event time and watermark support are powerful features for handling out-of-order events. However, it&amp;rsquo;s harder to understand what exactly is going on because the progress of time is tracked within the system.
Low watermarks of each task can be accessed through Flink web interface or metrics system.
Each Task in Flink exposes a metric called currentInputWatermark that represents the lowest watermark received by this task.</description>
    </item>
    
    <item>
      <title>Dynamic Tables</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/</guid>
      <description>Dynamic Tables # SQL - and the Table API - offer flexible and powerful capabilities for real-time data processing. This page describes how relational concepts elegantly translate to streaming, allowing Flink to achieve the same semantics on unbounded streams.
Relational Queries on Data Streams # The following table compares traditional relational algebra and stream processing for input data, execution, and output results.
Relational Algebra / SQL Stream Processing Relations (or tables) are bounded (multi-)sets of tuples.</description>
    </item>
    
    <item>
      <title>Execution Mode (Batch/Streaming)</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/execution_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/execution_mode/</guid>
      <description>Execution Mode (Batch/Streaming) # The DataStream API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job.
There is the &amp;ldquo;classic&amp;rdquo; execution behavior of the DataStream API, which we call STREAMING execution mode. This should be used for unbounded jobs that require continuous incremental processing and are expected to stay online indefinitely.
Additionally, there is a batch-style execution mode that we call BATCH execution mode.</description>
    </item>
    
    <item>
      <title>External Resources</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/advanced/external_resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/advanced/external_resources/</guid>
      <description>External Resource Framework # In addition to CPU and memory, many workloads also need some other resources, e.g. GPUs for deep learning. To support external resources, Flink provides an external resource framework. The framework supports requesting various types of resources from the underlying resource management systems (e.g., Kubernetes), and supplies information needed for using these resources to the operators. Different resource types can be supported. You can either leverage built-in plugins provided by Flink (currently only for GPU support), or implement your own plugins for custom resource types.</description>
    </item>
    
    <item>
      <title>Fault Tolerance Guarantees</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/guarantees/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/guarantees/</guid>
      <description>Fault Tolerance Guarantees of Data Sources and Sinks # Flink&amp;rsquo;s fault tolerance mechanism recovers programs in the presence of failures and continues to execute them. Such failures include machine hardware failures, network failures, transient program failures, etc.
Flink can guarantee exactly-once state updates to user-defined state only when the source participates in the snapshotting mechanism. The following table lists the state update guarantees of Flink coupled with the bundled connectors.</description>
    </item>
    
    <item>
      <title>Fraud Detection with the DataStream API</title>
      <link>//localhost/flink/flink-docs-master/docs/try-flink/datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/try-flink/datastream/</guid>
      <description>Fraud Detection with the DataStream API # Apache Flink offers a DataStream API for building robust, stateful streaming applications. It provides fine-grained control over state and time, which allows for the implementation of advanced event-driven systems. In this step-by-step guide you&amp;rsquo;ll learn how to build a stateful streaming application with Flink&amp;rsquo;s DataStream API.
What Are You Building? # Credit card fraud is a growing concern in the digital age. Criminals steal credit card numbers by running scams or hacking into insecure systems.</description>
    </item>
    
    <item>
      <title>Generating Watermarks</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/</guid>
      <description>Generating Watermarks # In this section you will learn about the APIs that Flink provides for working with event time timestamps and watermarks. For an introduction to event time, processing time, and ingestion time, please refer to the introduction to event time.
Introduction to Watermark Strategies # In order to work with event time, Flink needs to know the events timestamps, meaning each element in the stream needs to have its event timestamp assigned.</description>
    </item>
    
    <item>
      <title>Getting Started</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/gettingstarted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/gettingstarted/</guid>
      <description>Getting Started # Flink SQL makes it simple to develop streaming applications using standard SQL. It is easy to learn Flink if you have ever worked with a database or SQL like system by remaining ANSI-SQL 2011 compliant. This tutorial will help you get started quickly with a Flink SQL development environment.
Prerequisites # You only need to have basic knowledge of SQL to follow along. No other programming experience is assumed.</description>
    </item>
    
    <item>
      <title>Graph API</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_api/</guid>
      <description>Graph API # Graph Representation # In Gelly, a Graph is represented by a DataSet of vertices and a DataSet of edges.
The Graph nodes are represented by the Vertex type. A Vertex is defined by a unique ID and a value. Vertex IDs should implement the Comparable interface. Vertices without value can be represented by setting the value type to NullValue.
Java // create a new vertex with a Long ID and a String value Vertex&amp;lt;Long, String&amp;gt; v = new Vertex&amp;lt;Long, String&amp;gt;(1L, &amp;#34;foo&amp;#34;); // create a new vertex with a Long ID and no value Vertex&amp;lt;Long, NullValue&amp;gt; v = new Vertex&amp;lt;Long, NullValue&amp;gt;(1L, NullValue.</description>
    </item>
    
    <item>
      <title>Hints</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/hints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/hints/</guid>
      <description>SQL Hints # Batch Streaming
SQL hints can be used with SQL statements to alter execution plans. This chapter explains how to use hints to force various approaches.
Generally a hint can be used to:
Enforce planner: there&amp;rsquo;s no perfect planner, so it makes sense to implement hints to allow user better control the execution; Append meta data(or statistics): some statistics like “table index for scan” and “skew info of some shuffle keys” are somewhat dynamic for the query, it would be very convenient to config them with hints because our planning metadata from the planner is very often not that accurate; Operator resource constraints: for many cases, we would give a default resource configuration for the execution operators, i.</description>
    </item>
    
    <item>
      <title>Hive Catalog</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_catalog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_catalog/</guid>
      <description>Hive Catalog # Hive Metastore has evolved into the de facto metadata hub over the years in Hadoop ecosystem. Many companies have a single Hive Metastore service instance in their production to manage all of their metadata, either Hive metadata or non-Hive metadata, as the source of truth.
For users who have both Hive and Flink deployments, HiveCatalog enables them to use Hive Metastore to manage Flink&amp;rsquo;s metadata.
For users who have just Flink deployment, HiveCatalog is the only persistent catalog provided out-of-box by Flink.</description>
    </item>
    
    <item>
      <title>Installation</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/installation/</guid>
      <description>Installation # Environment Requirements # Python version (3.6, 3.7, 3.8 or 3.9) is required for PyFlink. Please run the following command to make sure that it meets the requirements: $ python --version # the version printed here must be 3.6, 3.7, 3.8 or 3.9 Environment Setup # Your system may include multiple Python versions, and thus also include multiple Python binary executables. You can run the following ls command to find out what Python binary executables are available in your system:</description>
    </item>
    
    <item>
      <title>Monitoring Checkpointing</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/monitoring/checkpoint_monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/monitoring/checkpoint_monitoring/</guid>
      <description>Monitoring Checkpointing # Overview # Flink&amp;rsquo;s web interface provides a tab to monitor the checkpoints of jobs. These stats are also available after the job has terminated. There are four different tabs to display information about your checkpoints: Overview, History, Summary, and Configuration. The following sections will cover all of these in turn.
Monitoring # Overview Tab # The overview tabs lists the following statistics. Note that these statistics don&amp;rsquo;t survive a JobManager loss and are reset to if your JobManager fails over.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/</guid>
      <description>Standalone # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate processes) of a Flink cluster. This can easily be expanded to set up a distributed standalone cluster, which we describe in the reference section.
Introduction # The standalone mode is the most barebone way of deploying Flink: The Flink services described in the deployment overview are just launched as processes on the operating system.</description>
    </item>
    
    <item>
      <title>SSL Setup</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/security/security-ssl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/security/security-ssl/</guid>
      <description>SSL Setup # This page provides instructions on how to enable TLS/SSL authentication and encryption for network communication with and between Flink processes. NOTE: TLS/SSL authentication is not enabled by default.
Internal and External Connectivity # When securing network connections between machines processes through authentication and encryption, Apache Flink differentiates between internal and external connectivity. Internal Connectivity refers to all connections made between Flink processes. These connections run Flink custom protocols.</description>
    </item>
    
    <item>
      <title>State Processor API</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/state_processor_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/state_processor_api/</guid>
      <description>State Processor API # Apache Flink&amp;rsquo;s State Processor API provides powerful functionality to reading, writing, and modifying savepoints and checkpoints using Flink’s DataStream API under BATCH execution. Due to the interoperability of DataStream and Table API, you can even use relational Table API or SQL queries to analyze and process state data.
For example, you can take a savepoint of a running stream processing application and analyze it with a DataStream batch program to verify that the application behaves correctly.</description>
    </item>
    
    <item>
      <title>State Schema Evolution</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/</guid>
      <description>State Schema Evolution # Apache Flink streaming applications are typically designed to run indefinitely or for long periods of time. As with all long-running services, the applications need to be updated to adapt to changing requirements. This goes the same for data schemas that the applications work against; they evolve along with the application.
This page provides an overview of how you can evolve your state type&amp;rsquo;s data schema. The current restrictions varies across different types and state structures (ValueState, ListState, etc.</description>
    </item>
    
    <item>
      <title>Transformations</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/transformations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/transformations/</guid>
      <description>DataSet Transformations # This document gives a deep-dive into the available transformations on DataSets. For a general introduction to the Flink Java API, please refer to the Programming Guide.
For zipping elements in a data set with a dense index, please refer to the Zip Elements Guide.
Map # The Map transformation applies a user-defined map function on each element of a DataSet. It implements a one-to-one mapping, that is, exactly one element must be returned by the function.</description>
    </item>
    
    <item>
      <title>Using Maven</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/maven/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/maven/</guid>
      <description>How to use Maven to configure your project # This guide will show you how to configure a Flink job project with Maven, an open-source build automation tool developed by the Apache Software Foundation that enables you to build, publish, and deploy projects. You can use it to manage the entire lifecycle of your software project.
Requirements # Maven 3.0.4 (or higher) Java 11 Importing the project into your IDE # Once the project folder and files have been created, we recommend that you import this project into your IDE for developing and testing.</description>
    </item>
    
    <item>
      <title>Windows</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/windows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/windows/</guid>
      <description>Windows # Windows are at the heart of processing infinite streams. Windows split the stream into &amp;ldquo;buckets&amp;rdquo; of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink and how the programmer can benefit to the maximum from its offered functionality.
The general structure of a windowed Flink program is presented below. The first snippet refers to keyed streams, while the second to non-keyed ones.</description>
    </item>
    
    <item>
      <title>Windows</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Working with State</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/</guid>
      <description>Working with State # In this section you will learn about the APIs that Flink provides for writing stateful programs. Please take a look at Stateful Stream Processing to learn about the concepts behind stateful stream processing.
Keyed DataStream # If you want to use keyed state, you first need to specify a key on a DataStream that should be used to partition the state (and also the records in the stream themselves).</description>
    </item>
    
    <item>
      <title>ZooKeeper HA Services</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/ha/zookeeper_ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/ha/zookeeper_ha/</guid>
      <description>ZooKeeper HA Services # Flink&amp;rsquo;s ZooKeeper HA services use ZooKeeper for high availability services.
Flink leverages ZooKeeper for distributed coordination between all running JobManager instances. ZooKeeper is a separate service from Flink, which provides highly reliable distributed coordination via leader election and light-weight consistent state storage. Check out ZooKeeper&amp;rsquo;s Getting Started Guide for more information about ZooKeeper. Flink includes scripts to bootstrap a simple ZooKeeper installation.
Configuration # In order to start an HA-cluster you have to configure the following configuration keys:</description>
    </item>
    
    <item>
      <title>Amazon S3</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/s3/</guid>
      <description>Amazon S3 # Amazon Simple Storage Service (Amazon S3) provides cloud object storage for a variety of use cases. You can use S3 with Flink for reading and writing data as well in conjunction with the streaming state backends.
You can use S3 objects like regular files by specifying paths in the following format:
s3://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt; The endpoint can either be a single file or a directory, for example:
// Read from S3 bucket env.</description>
    </item>
    
    <item>
      <title>Builtin Watermark Generators</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/event-time/built_in/</guid>
      <description>Builtin Watermark Generators # As described in Generating Watermarks, Flink provides abstractions that allow the programmer to assign their own timestamps and emit their own watermarks. More specifically, one can do so by implementing the WatermarkGenerator interface.
In order to further ease the programming effort for such tasks, Flink comes with some pre-implemented timestamp assigners. This section provides a list of them. Apart from their out-of-the-box functionality, their implementation can serve as an example for custom implementations.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/config/</guid>
      <description>Configuration # All configuration is done in conf/flink-conf.yaml, which is expected to be a flat collection of YAML key value pairs with format key: value.
The configuration is parsed and evaluated when the Flink processes are started. Changes to the configuration file require restarting the relevant processes.
The out of the box configuration will use your default Java installation. You can manually set the environment variable JAVA_HOME or the configuration key env.</description>
    </item>
    
    <item>
      <title>Custom State Serialization</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/</guid>
      <description>Custom Serialization for Managed State # This page is targeted as a guideline for users who require the use of custom serialization for their state, covering how to provide a custom state serializer as well as guidelines and best practices for implementing serializers that allow state schema evolution.
If you&amp;rsquo;re simply using Flink&amp;rsquo;s own serializers, this page is irrelevant and can be ignored.
Using custom state serializers # When registering a managed operator or keyed state, a StateDescriptor is required to specify the state&amp;rsquo;s name, as well as information about the type of the state.</description>
    </item>
    
    <item>
      <title>DataStream API Integration</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/data_stream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/data_stream_api/</guid>
      <description>DataStream API Integration # Both Table API and DataStream API are equally important when it comes to defining a data processing pipeline.
The DataStream API offers the primitives of stream processing (namely time, state, and dataflow management) in a relatively low-level imperative programming API. The Table API abstracts away many internals and provides a structured and declarative API.
Both APIs can work with bounded and unbounded streams.
Bounded streams need to be managed when processing historical data.</description>
    </item>
    
    <item>
      <title>Debugging Classloading</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/debugging/debugging_classloading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/debugging/debugging_classloading/</guid>
      <description>Debugging Classloading # Overview of Classloading in Flink # When running Flink applications, the JVM will load various classes over time. These classes can be divided into three groups based on their origin:
The Java Classpath: This is Java&amp;rsquo;s common classpath, and it includes the JDK libraries, and all code in Flink&amp;rsquo;s /lib folder (the classes of Apache Flink and some dependencies). They are loaded by AppClassLoader.
The Flink Plugin Components: The plugins code in folders under Flink&amp;rsquo;s /plugins folder.</description>
    </item>
    
    <item>
      <title>Flame Graphs</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/debugging/flame_graphs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/debugging/flame_graphs/</guid>
      <description>Flame Graphs # Flame Graphs are a visualization that effectively surfaces answers to questions like:
Which methods are currently consuming CPU resources? How does consumption by one method compare to the others? Which series of calls on the stack led to executing a particular method? Flame Graph Flame Graphs are constructed by sampling stack traces a number of times. Each method call is presented by a bar, where the length of the bar is proportional to the number of times it is present in the samples.</description>
    </item>
    
    <item>
      <title>Google Cloud Storage</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/gcs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/gcs/</guid>
      <description>Google Cloud Storage # Google Cloud Storage (GCS) provides cloud storage for a variety of use cases. You can use it for reading and writing data, and for checkpoint storage when using FileSystemCheckpointStorage) with the streaming state backends.
You can use GCS objects like regular files by specifying paths in the following format:
gs://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt; The endpoint can either be a single file or a directory, for example:
// Read from GCS bucket env.</description>
    </item>
    
    <item>
      <title>History Server</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/advanced/historyserver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/advanced/historyserver/</guid>
      <description>History Server # Flink has a history server that can be used to query the statistics of completed jobs after the corresponding Flink cluster has been shut down.
Furthermore, it exposes a REST API that accepts HTTP requests and responds with JSON data.
Overview # The HistoryServer allows you to query the status and statistics of completed jobs that have been archived by a JobManager.
After you have configured the HistoryServer and JobManager, you start and stop the HistoryServer via its corresponding startup script:</description>
    </item>
    
    <item>
      <title>Hive Dialect</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_dialect/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_dialect/</guid>
      <description>Hive Dialect # Flink allows users to write SQL statements in Hive syntax when Hive dialect is used. By providing compatibility with Hive syntax, we aim to improve the interoperability with Hive and reduce the scenarios when users need to switch between Flink and Hive in order to execute different statements.
Use Hive Dialect # Flink currently supports two SQL dialects: default and hive. You need to switch to Hive dialect before you can write in Hive syntax.</description>
    </item>
    
    <item>
      <title>Intro to the DataStream API</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/datastream_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/datastream_api/</guid>
      <description>Intro to the DataStream API # The focus of this training is to broadly cover the DataStream API well enough that you will be able to get started writing streaming applications.
What can be Streamed? # Flink&amp;rsquo;s DataStream APIs for Java and Scala will let you stream anything they can serialize. Flink&amp;rsquo;s own serializer is used for
basic types, i.e., String, Long, Integer, Boolean, Array composite types: Tuples, POJOs, and Scala case classes and Flink falls back to Kryo for other types.</description>
    </item>
    
    <item>
      <title>Iterations</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/iterations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/iterations/</guid>
      <description>Iterations # Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. Such algorithms are crucial in order to realize the promise of Big Data to extract meaningful information out of your data. With increasing interest to run these kinds of algorithms on very large data sets, there is a need to execute iterations in a massively parallel fashion.
Flink programs implement iterative algorithms by defining a step function and embedding it into a special iteration operator.</description>
    </item>
    
    <item>
      <title>Iterative Graph Processing</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/iterative_graph_processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/iterative_graph_processing/</guid>
      <description>Iterative Graph Processing # Gelly exploits Flink&amp;rsquo;s efficient iteration operators to support large-scale iterative graph processing. Currently, we provide implementations of the vertex-centric, scatter-gather, and gather-sum-apply models. In the following sections, we describe these abstractions and show how you can use them in Gelly.
Vertex-Centric Iterations # The vertex-centric model, also known as &amp;ldquo;think like a vertex&amp;rdquo; or &amp;ldquo;Pregel&amp;rdquo;, expresses computation from the perspective of a vertex in the graph.</description>
    </item>
    
    <item>
      <title>Joining</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/joining/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/joining/</guid>
      <description>Joining # Window Join # A window join joins the elements of two streams that share a common key and lie in the same window. These windows can be defined by using a window assigner and are evaluated on elements from both of the streams.
The elements from both sides are then passed to a user-defined JoinFunction or FlatJoinFunction where the user can emit results that meet the join criteria.</description>
    </item>
    
    <item>
      <title>JSON</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/json/</guid>
      <description>JSON Format # Format: Serialization Schema Format: Deserialization Schema
The JSON format allows to read and write JSON data based on an JSON schema. Currently, the JSON schema is derived from table schema.
The JSON format supports append-only streams, unless you&amp;rsquo;re using a connector that explicitly support retract streams and/or upsert streams like the Upsert Kafka connector. If you need to write retract streams and/or upsert streams, we suggest you to look at CDC JSON formats like Debezium JSON and Canal JSON.</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/kafka/</guid>
      <description>Apache Kafka Connector # Flink provides an Apache Kafka connector for reading data from and writing data to Kafka topics with exactly-once guarantees.
Dependency # Apache Flink ships with a universal Kafka connector which attempts to track the latest version of the Kafka client. The version of the client it uses may change between Flink releases. Modern Kafka clients are backwards compatible with broker versions 0.10.0 or later. For details on Kafka compatibility, please refer to the official Kafka documentation.</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/kafka/</guid>
      <description>Apache Kafka SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode
The Kafka connector allows for reading data from and writing data into Kafka topics.
Dependencies # In order to use the Kafka connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Kafka version Maven dependency SQL Client JAR universal &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.</description>
    </item>
    
    <item>
      <title>Kerberos</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/security/security-kerberos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/security/security-kerberos/</guid>
      <description>Kerberos Authentication Setup and Configuration # This document briefly describes how Flink security works in the context of various deployment mechanisms (Standalone, native Kubernetes, YARN), filesystems, connectors, and state backends.
Objective # The primary goals of the Flink Kerberos security infrastructure are:
to enable secure data access for jobs within a cluster via connectors (e.g. Kafka) to authenticate to ZooKeeper (if configured to use SASL) to authenticate to Hadoop components (e.</description>
    </item>
    
    <item>
      <title>Kubernetes HA Services</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/ha/kubernetes_ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/ha/kubernetes_ha/</guid>
      <description>Kubernetes HA Services # Flink&amp;rsquo;s Kubernetes HA services use Kubernetes for high availability services.
Kubernetes high availability services can only be used when deploying to Kubernetes. Consequently, they can be configured when using standalone Flink on Kubernetes or the native Kubernetes integration
Prerequisites # In order to use Flink&amp;rsquo;s Kubernetes HA services you must fulfill the following prerequisites:
Kubernetes &amp;gt;= 1.9. Service account with permissions to create, edit, delete ConfigMaps.</description>
    </item>
    
    <item>
      <title>Monitoring Back Pressure</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/monitoring/back_pressure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/monitoring/back_pressure/</guid>
      <description>Monitoring Back Pressure # Flink&amp;rsquo;s web interface provides a tab to monitor the back pressure behaviour of running jobs.
Back Pressure # If you see a back pressure warning (e.g. High) for a task, this means that it is producing data faster than the downstream operators can consume. Records in your job flow downstream (e.g. from sources to sinks) and back pressure is propagated in the opposite direction, up the stream.</description>
    </item>
    
    <item>
      <title>Native Kubernetes</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/</guid>
      <description>Native Kubernetes # This page describes how to deploy Flink natively on Kubernetes.
Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on Kubernetes.
Introduction # Kubernetes is a popular container-orchestration system for automating computer application deployment, scaling, and management. Flink&amp;rsquo;s native Kubernetes integration allows you to directly deploy Flink on a running Kubernetes cluster. Moreover, Flink is able to dynamically allocate and de-allocate TaskManagers depending on the required resources because it can directly talk to Kubernetes.</description>
    </item>
    
    <item>
      <title>Set up TaskManager Memory</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/</guid>
      <description>Set up TaskManager Memory # The TaskManager runs user code in Flink. Configuring memory usage for your needs can greatly reduce Flink&amp;rsquo;s resource footprint and improve Job stability.
The further described memory configuration is applicable starting with the release version 1.10. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.10 release.
This memory setup guide is relevant only for TaskManagers!</description>
    </item>
    
    <item>
      <title>Stateful Stream Processing</title>
      <link>//localhost/flink/flink-docs-master/docs/concepts/stateful-stream-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/concepts/stateful-stream-processing/</guid>
      <description>Stateful Stream Processing # What is State? # While many operations in a dataflow simply look at one individual event at a time (for example an event parser), some operations remember information across multiple events (for example window operators). These operations are called stateful.
Some examples of stateful operations:
When an application searches for certain event patterns, the state will store the sequence of events encountered so far. When aggregating events per minute/hour/day, the state holds the pending aggregates.</description>
    </item>
    
    <item>
      <title>The Broadcast State Pattern</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/</guid>
      <description>The Broadcast State Pattern # In this section you will learn about how to use broadcast state in practise. Please refer to Stateful Stream Processing to learn about the concepts behind stateful stream processing.
Provided APIs # To show the provided APIs, we will start with an example before presenting their full functionality. As our running example, we will use the case where we have a stream of objects of different colors and shapes and we want to find pairs of objects of the same color that follow a certain pattern, e.</description>
    </item>
    
    <item>
      <title>Time Attributes</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/concepts/time_attributes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/concepts/time_attributes/</guid>
      <description>Time Attributes # Flink can process data based on different notions of time.
Processing time refers to the machine&amp;rsquo;s system time (also known as epoch time, e.g. Java&amp;rsquo;s System.currentTimeMillis()) that is executing the respective operation. Event time refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event happened. For more information about time handling in Flink, see the introduction about event time and watermarks.</description>
    </item>
    
    <item>
      <title>Timely Stream Processing</title>
      <link>//localhost/flink/flink-docs-master/docs/concepts/time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/concepts/time/</guid>
      <description>Timely Stream Processing # Introduction # Timely stream processing is an extension of stateful stream processing in which time plays some role in the computation. Among other things, this is the case when you do time series analysis, when doing aggregations based on certain time periods (typically called windows), or when you do event processing where the time when an event occurred is important.
In the following sections we will highlight some of the topics that you should consider when working with timely Flink Applications.</description>
    </item>
    
    <item>
      <title>Using Gradle</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/gradle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/gradle/</guid>
      <description>How to use Gradle to configure your project # You will likely need a build tool to configure your Flink project. This guide will show you how to do so with Gradle, an open-source general-purpose build tool that can be used to automate tasks in the development process.
Requirements # Gradle 7.x Java 11 Importing the project into your IDE # Once the project folder and files have been created, we recommend that you import this project into your IDE for developing and testing.</description>
    </item>
    
    <item>
      <title>WITH clause</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/with/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/with/</guid>
      <description>WITH clause # Batch Streaming
WITH provides a way to write auxiliary statements for use in a larger query. These statements, which are often referred to as Common Table Expression (CTE), can be thought of as defining temporary views that exist just for one query.
The syntax of WITH statement is:
WITH &amp;lt;with_item_definition&amp;gt; [ , ... ] SELECT ... FROM ...; &amp;lt;with_item_defintion&amp;gt;: with_item_name (column_name[, ...n]) AS ( &amp;lt;select_query&amp;gt; ) The following example defines a common table expression orders_with_total and use it in a GROUP BY query.</description>
    </item>
    
    <item>
      <title>Working Directory</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/working_directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/working_directory/</guid>
      <description>Working Directory # Flink supports to configure a working directory (FLIP-198) for Flink processes (JobManager and TaskManager). The working directory is used by the processes to store information that can be recovered upon a process restart. The requirement for this to work is that the process is started with the same identity and has access to the volume on which the working directory is stored.
Configuring the Working Directory # The working directories for the Flink processes are:</description>
    </item>
    
    <item>
      <title>Zipping Elements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/zip_elements_guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/zip_elements_guide/</guid>
      <description>Zipping Elements in a DataSet # In certain algorithms, one may need to assign unique identifiers to data set elements. This document shows how DataSetUtils can be used for that purpose.
Zip with a Dense Index # zipWithIndex assigns consecutive labels to the elements, receiving a data set as input and returning a new data set of (unique id, initial value) 2-tuples. This process requires two passes, first counting then labeling elements, and cannot be pipelined due to the synchronization of counts.</description>
    </item>
    
    <item>
      <title>3rd Party Serializers</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/third_party_serializers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/third_party_serializers/</guid>
      <description>3rd Party Serializers # If you use a custom type in your Flink program which cannot be serialized by the Flink type serializer, Flink falls back to using the generic Kryo serializer. You may register your own serializer or a serialization system like Google Protobuf or Apache Thrift with Kryo. To do that, simply register the type class and the serializer in the ExecutionConfig of your Flink program.
final ExecutionEnvironment env = ExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Aliyun OSS</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/oss/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/oss/</guid>
      <description>Aliyun Object Storage Service (OSS) # OSS: Object Storage Service # Aliyun Object Storage Service (Aliyun OSS) is widely used, particularly popular among China’s cloud users, and it provides cloud object storage for a variety of use cases. You can use OSS with Flink for reading and writing data as well in conjunction with the streaming state backends
You can use OSS objects like regular files by specifying paths in the following format:</description>
    </item>
    
    <item>
      <title>Application Profiling &amp; Debugging</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/debugging/application_profiling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/debugging/application_profiling/</guid>
      <description>Application Profiling &amp;amp; Debugging # Overview of Custom Logging with Apache Flink # Each standalone JobManager, TaskManager, HistoryServer, and ZooKeeper daemon redirects stdout and stderr to a file with a .out filename suffix and writes internal logging to a file with a .log suffix. Java options configured by the user in env.java.opts, env.java.opts.jobmanager, env.java.opts.taskmanager, env.java.opts.historyserver and env.java.opts.client can likewise define log files with use of the script variable FLINK_LOG_PREFIX and by enclosing the options in double quotes for late evaluation.</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/avro/</guid>
      <description>Avro format # Flink has built-in support for Apache Avro. This allows to easily read and write Avro data based on an Avro schema with Flink. The serialization framework of Flink is able to handle classes generated from Avro schemas. In order to use the Avro format the following dependencies are required for projects using a build automation tool (such as Maven or SBT).
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-avro&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; In order to read data from an Avro file, you have to specify an AvroInputFormat.</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/avro/</guid>
      <description>Avro format # Flink has built-in support for Apache Avro. This allows to easily read and write Avro data based on an Avro schema with Flink. The serialization framework of Flink is able to handle classes generated from Avro schemas. In order to use the Avro format the following dependencies are required for projects using a build automation tool (such as Maven or SBT).
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-avro&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; In order to use the Avro format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Avro</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/avro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/avro/</guid>
      <description>Avro Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Avro format allows to read and write Avro data based on an Avro schema. Currently, the Avro schema is derived from table schema.
Dependencies # In order to use the Avro format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.</description>
    </item>
    
    <item>
      <title>Azure Blob Storage</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/azure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/azure/</guid>
      <description>Azure Blob Storage # Azure Blob Storage is a Microsoft-managed service providing cloud storage for a variety of use cases. You can use Azure Blob Storage with Flink for reading and writing data as well in conjunction with the streaming state backends
Flink supports accessing Azure Blob Storage using both wasb:// or abfs://.
Azure recommends using abfs:// for accessing ADLS Gen2 storage accounts even though wasb:// works through backward compatibility.</description>
    </item>
    
    <item>
      <title>Azure Table storage</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/azure_table_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/azure_table_storage/</guid>
      <description>Azure Table Storage # This example is using the HadoopInputFormat wrapper to use an existing Hadoop input format implementation for accessing Azure&amp;rsquo;s Table Storage.
Download and compile the azure-tables-hadoop project. The input format developed by the project is not yet available in Maven Central, therefore, we have to build the project ourselves. Execute the following commands: git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install Setup a new Flink project using the quickstarts: curl https://flink.</description>
    </item>
    
    <item>
      <title>Batch Shuffle</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/batch/batch_shuffle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/batch/batch_shuffle/</guid>
      <description>Batch Shuffle # Overview # Flink supports a batch execution mode in both DataStream API and Table / SQL for jobs executing across bounded input. In batch execution mode, Flink offers two modes for network exchanges: Blocking Shuffle and Hybrid Shuffle.
Blocking Shuffle is the default data exchange mode for batch executions. It persists all intermediate data, and can be consumed only after fully produced. Hybrid Shuffle is the next generation data exchange mode for batch executions.</description>
    </item>
    
    <item>
      <title>Cassandra</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/cassandra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/cassandra/</guid>
      <description>Apache Cassandra Connector # This connector provides sinks that writes data into a Apache Cassandra database.
To use this connector, add the following dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-cassandra_2.12&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here.
Installing Apache Cassandra # There are multiple ways to bring up a Cassandra instance on local machine:</description>
    </item>
    
    <item>
      <title>Checkpointing</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/</guid>
      <description>Checkpointing # Every function and operator in Flink can be stateful (see working with state for details). Stateful functions store data across the processing of individual elements/events, making state a critical building block for any type of more elaborate operation.
In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution.</description>
    </item>
    
    <item>
      <title>Confluent Avro</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/avro-confluent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/avro-confluent/</guid>
      <description>Confluent Avro Format # Format: Serialization Schema Format: Deserialization Schema
The Avro Schema Registry (avro-confluent) format allows you to read records that were serialized by the io.confluent.kafka.serializers.KafkaAvroSerializer and to write records that can in turn be read by the io.confluent.kafka.serializers.KafkaAvroDeserializer.
When reading (deserializing) a record with this format the Avro writer schema is fetched from the configured Confluent Schema Registry based on the schema version id encoded in the record while the reader schema is inferred from table schema.</description>
    </item>
    
    <item>
      <title>CREATE Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/create/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/create/</guid>
      <description>CREATE Statements # CREATE statements are used to register a table/view/function into current or specified Catalog. A registered table/view/function can be used in SQL queries.
Flink SQL supports the following CREATE statements for now:
CREATE TABLE CREATE CATALOG CREATE DATABASE CREATE VIEW CREATE FUNCTION Run a CREATE statement # Java CREATE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful CREATE operation, otherwise will throw an exception.</description>
    </item>
    
    <item>
      <title>CSV</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/csv/</guid>
      <description>CSV format # To use the CSV format you need to add the Flink CSV dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-csv&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading CSV files using CsvReaderFormat. The reader utilizes Jackson library and allows passing the corresponding configuration for the CSV schema and parsing options.
CsvReaderFormat can be initialized and used like this:
CsvReaderFormat&amp;lt;SomePojo&amp;gt; csvFormat = CsvReaderFormat.</description>
    </item>
    
    <item>
      <title>Data Pipelines &amp; ETL</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/etl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/etl/</guid>
      <description>Data Pipelines &amp;amp; ETL # One very common use case for Apache Flink is to implement ETL (extract, transform, load) pipelines that take data from one or more sources, perform some transformations and/or enrichments, and then store the results somewhere. In this section we are going to look at how to use Flink&amp;rsquo;s DataStream API to implement this kind of application.
Note that Flink&amp;rsquo;s Table and SQL APIs are well suited for many ETL use cases.</description>
    </item>
    
    <item>
      <title>Docker</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/</guid>
      <description>Docker Setup # Getting Started # This Getting Started section guides you through the local setup (on one machine, but in separate containers) of a Flink cluster using Docker containers.
Introduction # Docker is a popular container runtime. There are official Docker images for Apache Flink available on Docker Hub. You can use the Docker images to deploy a Session or Application cluster on Docker. This page focuses on the setup of Flink on Docker and Docker Compose.</description>
    </item>
    
    <item>
      <title>Flink Architecture</title>
      <link>//localhost/flink/flink-docs-master/docs/concepts/flink-architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/concepts/flink-architecture/</guid>
      <description>Flink Architecture # Flink is a distributed system and requires effective allocation and management of compute resources in order to execute streaming applications. It integrates with all common cluster resource managers such as Hadoop YARN and Kubernetes, but can also be set up to run as a standalone cluster or even as a library.
This section contains an overview of Flink’s architecture and describes how its main components interact to execute applications and recover from failures.</description>
    </item>
    
    <item>
      <title>Hadoop</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/hadoop/</guid>
      <description>Hadoop formats # Project Configuration # Support for Hadoop is contained in the flink-hadoop-compatibility Maven module.
Add the following dependency to your pom.xml to use hadoop
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-hadoop-compatibility_2.12&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.8.5&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Using Hadoop InputFormats # To use Hadoop InputFormats with Flink the format must first be wrapped using either readHadoopFile or createHadoopInput of the HadoopInputs utility class.</description>
    </item>
    
    <item>
      <title>Hadoop</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/hadoop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/hadoop/</guid>
      <description>Hadoop formats # Project Configuration # Support for Hadoop is contained in the flink-hadoop-compatibility Maven module.
Add the following dependency to your pom.xml to use hadoop
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-hadoop-compatibility_2.12&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; If you want to run your Flink application locally (e.g. from your IDE), you also need to add a hadoop-client dependency such as:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.8.5&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Using Hadoop InputFormats # To use Hadoop InputFormats with Flink the format must first be wrapped using either readHadoopFile or createHadoopInput of the HadoopInputs utility class.</description>
    </item>
    
    <item>
      <title>Hive Read &amp; Write</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_read_write/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_read_write/</guid>
      <description>Hive Read &amp;amp; Write # Using the HiveCatalog, Apache Flink can be used for unified BATCH and STREAM processing of Apache Hive Tables. This means Flink can be used as a more performant alternative to Hive’s batch engine, or to continuously read and write data into and out of Hive tables to power real-time data warehousing applications.
Reading # Flink supports reading data from Hive in both BATCH and STREAMING modes.</description>
    </item>
    
    <item>
      <title>Importing Flink into an IDE</title>
      <link>//localhost/flink/flink-docs-master/docs/flinkdev/ide_setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/flinkdev/ide_setup/</guid>
      <description>Importing Flink into an IDE # The sections below describe how to import the Flink project into an IDE for the development of Flink itself. For writing Flink programs, please refer to the Java API and the Scala API quickstart guides.
Whenever something is not working in your IDE, try with the Maven command line first (mvn clean package -DskipTests) as it might be your IDE that has a bug or is not properly set up.</description>
    </item>
    
    <item>
      <title>JSON</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/json/</guid>
      <description>Json format # To use the JSON format you need to add the Flink JSON dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-json&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; For PyFlink users, you could use it directly in your jobs.
Flink supports reading/writing JSON records via the JsonSerializationSchema/JsonDeserializationSchema. These utilize the Jackson library, and support any type that is supported by Jackson, including, but not limited to, POJOs and ObjectNode.
The JsonDeserializationSchema can be used with any connector that supports the DeserializationSchema.</description>
    </item>
    
    <item>
      <title>Library Methods</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/library_methods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/library_methods/</guid>
      <description>Library Methods # Gelly has a growing collection of graph algorithms for easily analyzing large-scale Graphs.
Gelly&amp;rsquo;s library methods can be used by simply calling the run() method on the input graph:
Java ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); Graph&amp;lt;Long, Long, NullValue&amp;gt; graph = ...; // run Label Propagation for 30 iterations to detect communities on the input graph DataSet&amp;lt;Vertex&amp;lt;Long, Long&amp;gt;&amp;gt; verticesWithCommunity = graph.run(new LabelPropagation&amp;lt;Long&amp;gt;(30)); // print the result verticesWithCommunity.print(); Scala val env = ExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/advanced/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/advanced/logging/</guid>
      <description>How to use logging # All Flink processes create a log text file that contains messages for various events happening in that process. These logs provide deep insights into the inner workings of Flink, and can be used to detect problems (in the form of WARN/ERROR messages) and can help in debugging them.
The log files can be accessed via the Job-/TaskManager pages of the WebUI. The used Resource Provider (e.</description>
    </item>
    
    <item>
      <title>Microsoft Azure table</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/azure_table_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/dataset/formats/azure_table_storage/</guid>
      <description>Microsoft Azure Table Storage format # This example is using the HadoopInputFormat wrapper to use an existing Hadoop input format implementation for accessing Azure&amp;rsquo;s Table Storage.
Download and compile the azure-tables-hadoop project. The input format developed by the project is not yet available in Maven Central, therefore, we have to build the project ourselves. Execute the following commands: git clone https://github.com/mooso/azure-tables-hadoop.git cd azure-tables-hadoop mvn clean install Setup a new Flink project using the quickstarts: curl https://flink.</description>
    </item>
    
    <item>
      <title>Parquet</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/parquet/</guid>
      <description>Parquet format # Flink supports reading Parquet files, producing Flink RowData and producing Avro records. To use the format you need to add the flink-parquet dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-parquet&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; To read Avro records, you will need to add the parquet-avro dependency:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.parquet&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;parquet-avro&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.12.2&amp;lt;/version&amp;gt; &amp;lt;optional&amp;gt;true&amp;lt;/optional&amp;gt; &amp;lt;exclusions&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;groupId&amp;gt;it.unimi.dsi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;fastutil&amp;lt;/artifactId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;/exclusions&amp;gt; &amp;lt;/dependency&amp;gt; In order to use the Parquet format in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Process Function</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/process_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/process_function/</guid>
      <description>Process Function # The ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).</description>
    </item>
    
    <item>
      <title>Process Function</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/process_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/operators/process_function/</guid>
      <description>Process Function # ProcessFunction # The ProcessFunction is a low-level stream processing operation, giving access to the basic building blocks of all (acyclic) streaming applications:
events (stream elements) state (fault-tolerant, consistent, only on keyed stream) timers (event time and processing time, only on keyed stream) The ProcessFunction can be thought of as a FlatMapFunction with access to keyed state and timers. It handles events by being invoked for each event received in the input stream(s).</description>
    </item>
    
    <item>
      <title>Protobuf</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/protobuf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/protobuf/</guid>
      <description>Protobuf Format # Format: Serialization Schema Format: Deserialization Schema
The Protocol Buffers Protobuf format allows you to read and write Protobuf data, based on Protobuf generated classes.
Dependencies # In order to use the Protobuf format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-protobuf&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>Real Time Reporting with the Table API</title>
      <link>//localhost/flink/flink-docs-master/docs/try-flink/table_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/try-flink/table_api/</guid>
      <description>Real Time Reporting with the Table API # Apache Flink offers a Table API as a unified, relational API for batch and stream processing, i.e., queries are executed with the same semantics on unbounded, real-time streams or bounded, batch data sets and produce the same results. The Table API in Flink is commonly used to ease the definition of data analytics, data pipelining, and ETL applications.
What Will You Be Building?</description>
    </item>
    
    <item>
      <title>SELECT &amp; WHERE</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/select/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/select/</guid>
      <description>SELECT &amp;amp; WHERE clause # Batch Streaming
The general syntax of the SELECT statement is:
SELECT select_list FROM table_expression [ WHERE boolean_expression ] The table_expression refers to any source of data. It could be an existing table, view, or VALUES clause, the joined results of multiple existing tables, or a subquery. Assuming that the table is available in the catalog, the following would read all rows from Orders.
SELECT * FROM Orders The select_list specification * means the query will resolve all columns.</description>
    </item>
    
    <item>
      <title>Set up JobManager Memory</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/</guid>
      <description>Set up JobManager Memory # The JobManager is the controlling element of the Flink Cluster. It consists of three distinct components: Resource Manager, Dispatcher and one JobMaster per running Flink Job. This guide walks you through high level and fine-grained memory configurations for the JobManager.
The further described memory configuration is applicable starting with the release version 1.11. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.</description>
    </item>
    
    <item>
      <title>Text files</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/text_files/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/formats/text_files/</guid>
      <description>Text files format # Flink supports reading from text lines from a file using TextLineInputFormat. This format uses Java&amp;rsquo;s built-in InputStreamReader to decode the byte stream using various supported charset encodings. To use the format you need to add the Flink Connector Files dependency to your project:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-files&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; For PyFlink users, you could use it directly in your jobs.
This format is compatible with the new Source that can be used in both batch and streaming modes.</description>
    </item>
    
    <item>
      <title>Upsert Kafka</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/upsert-kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/upsert-kafka/</guid>
      <description>Upsert Kafka SQL Connector # Scan Source: Unbounded Sink: Streaming Upsert Mode
The Upsert Kafka connector allows for reading data from and writing data into Kafka topics in the upsert fashion.
As a source, the upsert-kafka connector produces a changelog stream, where each data record represents an update or delete event. More precisely, the value in a data record is interpreted as an UPDATE of the last value for the same key, if any (if a corresponding key doesn’t exist yet, the update will be considered an INSERT).</description>
    </item>
    
    <item>
      <title>Versioned Tables</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/concepts/versioned_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/concepts/versioned_tables/</guid>
      <description>Versioned Tables # Flink SQL operates over dynamic tables that evolve, which may either be append-only or updating. Versioned tables represent a special type of updating table that remembers the past values for each key.
Concept # Dynamic tables define relations over time. Often, particularly when working with metadata, a key&amp;rsquo;s old value does not become irrelevant when it changes.
Flink SQL can define versioned tables over any dynamic table with a PRIMARY KEY constraint and time attribute.</description>
    </item>
    
    <item>
      <title>Async I/O</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/</guid>
      <description>Asynchronous I/O for External Data Access # This page explains the use of Flink&amp;rsquo;s API for asynchronous I/O with external data stores. For users not familiar with asynchronous or event-driven programming, an article about Futures and event-driven programming may be useful preparation.
Note: Details about the design and implementation of the asynchronous I/O utility can be found in the proposal and design document FLIP-12: Asynchronous I/O Design and Implementation. Details about the new retry support can be found in document FLIP-232: Add Retry Support For Async I/O In DataStream API.</description>
    </item>
    
    <item>
      <title>Command-Line Interface</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/cli/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/cli/</guid>
      <description>Command-Line Interface # Flink provides a Command-Line Interface (CLI) bin/flink to run programs that are packaged as JAR files and to control their execution. The CLI is part of any Flink setup, available in local single node setups and in distributed setups. It connects to the running JobManager specified in conf/flink-conf.yaml.
Job Lifecycle Management # A prerequisite for the commands listed in this section to work is to have a running Flink deployment like Kubernetes, YARN or any other option available.</description>
    </item>
    
    <item>
      <title>Connectors and Formats</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/connector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/connector/</guid>
      <description>Connectors and Formats # Flink applications can read from and write to various external systems via connectors. It supports multiple formats in order to encode and decode data to match Flink&amp;rsquo;s data structures.
An overview of available connectors and formats is available for both DataStream and Table API/SQL.
Available artifacts # In order to use connectors and formats, you need to make sure Flink has access to the artifacts implementing them.</description>
    </item>
    
    <item>
      <title>Debezium</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/debezium/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/debezium/</guid>
      <description>Debezium Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Debezium is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL, PostgreSQL, Oracle, Microsoft SQL Server and many other databases into Kafka. Debezium provides a unified format schema for changelog and supports to serialize messages using JSON and Apache Avro.
Flink supports to interpret Debezium JSON and Avro messages as INSERT/UPDATE/DELETE messages into Flink SQL system.</description>
    </item>
    
    <item>
      <title>DROP Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/drop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/drop/</guid>
      <description>DROP Statements # DROP statements are used to remove a catalog with the given catalog name or to remove a registered table/view/function from the current or specified Catalog.
Flink SQL supports the following DROP statements for now:
DROP CATALOG DROP TABLE DROP DATABASE DROP VIEW DROP FUNCTION Run a DROP statement # Java DROP statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful DROP operation, otherwise will throw an exception.</description>
    </item>
    
    <item>
      <title>Elastic Scaling</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/elastic_scaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/elastic_scaling/</guid>
      <description>Elastic Scaling # Apache Flink allows you to rescale your jobs. You can do this manually by stopping the job and restarting from the savepoint created during shutdown with a different parallelism.
This page describes options where Flink automatically adjusts the parallelism instead.
Reactive Mode # Reactive mode is an MVP (&amp;ldquo;minimum viable product&amp;rdquo;) feature. The Flink community is actively looking for feedback by users through our mailing lists. Please check the limitations listed on this page.</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/</guid>
      <description>Elasticsearch Connector # This connector provides sinks that can request document actions to an Elasticsearch Index. To use this connector, add one of the following dependencies to your project, depending on the version of the Elasticsearch installation:
Elasticsearch version Maven Dependency 6.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch6&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! 7.x &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-elasticsearch7&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! In order to use the Elasticsearch connector in PyFlink jobs, the following dependencies are required: Elasticsearch version PyFlink JAR 6.</description>
    </item>
    
    <item>
      <title>Fine-Grained Resource Management</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/finegrained_resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/finegrained_resource/</guid>
      <description>Fine-Grained Resource Management # Apache Flink works hard to auto-derive sensible default resource requirements for all applications out of the box. For users who wish to fine-tune their resource consumption, based on knowledge of their specific scenarios, Flink offers fine-grained resource management.
This page describes the fine-grained resource management’s usage, applicable scenarios, and how it works.
Note: This feature is currently an MVP (“minimum viable product”) feature and only available to DataStream API.</description>
    </item>
    
    <item>
      <title>Firehose</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/firehose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/firehose/</guid>
      <description>Amazon Kinesis Data Firehose Sink # The Firehose sink writes to Amazon Kinesis Data Firehose.
Follow the instructions from the Amazon Kinesis Data Firehose Developer Guide to setup a Kinesis Data Firehose delivery stream.
To use the connector, add the following Maven dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-firehose&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! In order to use the AWS Kinesis Firehose connector in PyFlink jobs, the following dependencies are required: PyFlink JAR Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Firehose</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/firehose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/firehose/</guid>
      <description>Amazon Kinesis Data Firehose SQL Connector # Sink: Streaming Append Mode The Kinesis Data Firehose connector allows for writing data into Amazon Kinesis Data Firehose (KDF).
Dependencies # In order to use the AWS Kinesis Firehose connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kinesis-firehose&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.</description>
    </item>
    
    <item>
      <title>General User-defined Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/python_udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/python_udfs/</guid>
      <description>General User-defined Functions # User-defined functions are important features, because they significantly extend the expressiveness of Python Table API programs.
NOTE: Python UDF execution requires Python version (3.6, 3.7, 3.8 or 3.9) with PyFlink installed. It&amp;rsquo;s required on both the client side and the cluster side.
Scalar Functions # It supports to use Python scalar functions in Python Table API programs. In order to define a Python scalar function, one can extend the base class ScalarFunction in pyflink.</description>
    </item>
    
    <item>
      <title>Glossary</title>
      <link>//localhost/flink/flink-docs-master/docs/concepts/glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/concepts/glossary/</guid>
      <description>Glossary # Checkpoint Storage # The location where the State Backend will store its snapshot during a checkpoint (Java Heap of JobManager or Filesystem).
Flink Application Cluster # A Flink Application Cluster is a dedicated Flink Cluster that only executes Flink Jobs from one Flink Application. The lifetime of the Flink Cluster is bound to the lifetime of the Flink Application.
Flink Job Cluster # A Flink Job Cluster is a dedicated Flink Cluster that only executes a single Flink Job.</description>
    </item>
    
    <item>
      <title>Graph Algorithms</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_algorithms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_algorithms/</guid>
      <description>Graph Algorithms # The logic blocks with which the Graph API and top-level algorithms are assembled are accessible in Gelly as graph algorithms in the org.apache.flink.graph.asm package. These algorithms provide optimization and tuning through configuration parameters and may provide implicit runtime reuse when processing the same input with a similar configuration.
VertexInDegree # Annotate vertices of a directed graph with the in-degree.
DataSet&amp;lt;Vertex&amp;lt;K, LongValue&amp;gt;&amp;gt; inDegree = graph .run(new VertexInDegree().setIncludeZeroDegreeVertices(true)); Optional Configuration:</description>
    </item>
    
    <item>
      <title>Hive Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hive/hive_functions/</guid>
      <description>Hive Functions # Use Hive Built-in Functions via HiveModule # The HiveModule provides Hive built-in functions as Flink system (built-in) functions to Flink SQL and Table API users.
For detailed information, please refer to HiveModule.
Java String name = &amp;#34;myhive&amp;#34;; String version = &amp;#34;2.3.4&amp;#34;; tableEnv.loadModue(name, new HiveModule(version)); Scala val name = &amp;#34;myhive&amp;#34; val version = &amp;#34;2.3.4&amp;#34; tableEnv.loadModue(name, new HiveModule(version)); Python from pyflink.table.module import HiveModule name = &amp;#34;myhive&amp;#34; version = &amp;#34;2.</description>
    </item>
    
    <item>
      <title>Jobs and Scheduling</title>
      <link>//localhost/flink/flink-docs-master/docs/internals/job_scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/internals/job_scheduling/</guid>
      <description>Jobs and Scheduling # This document briefly describes how Flink schedules jobs and how it represents and tracks job status on the JobManager.
Scheduling # Execution resources in Flink are defined through Task Slots. Each TaskManager will have one or more task slots, each of which can run one pipeline of parallel tasks. A pipeline consists of multiple successive tasks, such as the n-th parallel instance of a MapFunction together with the n-th parallel instance of a ReduceFunction.</description>
    </item>
    
    <item>
      <title>Kinesis</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/kinesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/kinesis/</guid>
      <description>Amazon Kinesis Data Streams Connector # The Kinesis connector provides access to Amazon Kinesis Data Streams.
To use this connector, add one or more of the following dependencies to your project, depending on whether you are reading from and/or writing to Kinesis Data Streams:
KDS Connectivity Maven Dependency Source &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-kinesis&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Sink &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-aws-kinesis-streams&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Due to the licensing issue, the flink-connector-kinesis artifact is not deployed to Maven central for the prior versions.</description>
    </item>
    
    <item>
      <title>Kinesis</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/kinesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/kinesis/</guid>
      <description>Amazon Kinesis Data Streams SQL Connector # Scan Source: Unbounded Sink: Streaming Append Mode
The Kinesis connector allows for reading data from and writing data into Amazon Kinesis Data Streams (KDS).
Dependencies # In order to use the Kinesis connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/</guid>
      <description>Kubernetes Setup # Getting Started # This Getting Started guide describes how to deploy a Session cluster on Kubernetes.
Introduction # This page describes deploying a standalone Flink cluster on top of Kubernetes, using Flink&amp;rsquo;s standalone deployment. We generally recommend new users to deploy Flink on Kubernetes using native Kubernetes deployments.
Preparation # This guide expects a Kubernetes environment to be present. You can ensure that your Kubernetes setup is working by running a command like kubectl get nodes, which lists all connected Kubelets.</description>
    </item>
    
    <item>
      <title>Memory Tuning Guide</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_tuning/</guid>
      <description>Memory tuning guide # In addition to the main memory setup guide, this section explains how to set up memory depending on the use case and which options are important for each case.
Configure memory for standalone deployment # It is recommended to configure total Flink memory (taskmanager.memory.flink.size or jobmanager.memory.flink.size) or its components for standalone deployment where you want to declare how much memory is given to Flink itself. Additionally, you can adjust JVM metaspace if it causes problems.</description>
    </item>
    
    <item>
      <title>Plugins</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/filesystems/plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/filesystems/plugins/</guid>
      <description>Plugins # Plugins facilitate a strict separation of code through restricted classloaders. Plugins cannot access classes from other plugins or from Flink that have not been specifically whitelisted. This strict isolation allows plugins to contain conflicting versions of the same library without the need to relocate classes or to converge to common versions. Currently, file systems and metric reporters are pluggable but in the future, connectors, formats, and even user code should also be pluggable.</description>
    </item>
    
    <item>
      <title>Queryable State</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/queryable_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/queryable_state/</guid>
      <description>Queryable State # The client APIs for queryable state are currently in an evolving state and there are no guarantees made about stability of the provided interfaces. It is likely that there will be breaking API changes on the client side in the upcoming Flink versions. In a nutshell, this feature exposes Flink&amp;rsquo;s managed keyed (partitioned) state (see Working with State) to the outside world and allows the user to query a job&amp;rsquo;s state from outside Flink.</description>
    </item>
    
    <item>
      <title>SELECT DISTINCT</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/select-distinct/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/select-distinct/</guid>
      <description>SELECT DISTINCT # Batch Streaming
If SELECT DISTINCT is specified, all duplicate rows are removed from the result set (one row is kept from each group of duplicates).
SELECT DISTINCT id FROM Orders For streaming queries, the required state for computing the query result might grow infinitely. State size depends on number of distinct rows. You can provide a query configuration with an appropriate state time-to-live (TTL) to prevent excessive state size.</description>
    </item>
    
    <item>
      <title>Speculative Execution</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/speculative_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/speculative_execution/</guid>
      <description>Speculative Execution # This page describes the background of speculative execution, how to use it, and how to check the effectiveness of it.
Background # Speculative execution is a mechanism to mitigate job slowness which is caused by problematic nodes. A problematic node may have hardware problems, accident I/O busy, or high CPU load. These problems may make the hosted tasks run much slower than tasks on other nodes, and affect the overall execution time of a batch job.</description>
    </item>
    
    <item>
      <title>Streaming Analytics</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/streaming_analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/streaming_analytics/</guid>
      <description>Streaming Analytics # Event Time and Watermarks # Introduction # Flink explicitly supports three different notions of time:
event time: the time when an event occurred, as recorded by the device producing (or storing) the event
ingestion time: a timestamp recorded by Flink at the moment it ingests the event
processing time: the time when a specific operator in your pipeline is processing the event
For reproducible results, e.g., when computing the maximum price a stock reached during the first hour of trading on a given day, you should use event time.</description>
    </item>
    
    <item>
      <title>User-Defined Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/user_defined_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/user_defined_functions/</guid>
      <description>User-Defined Functions # Most operations require a user-defined function. This section lists different ways of how they can be specified. We also cover Accumulators, which can be used to gain insights into your Flink application.
Java Implementing an interface # The most basic way is to implement one of the provided interfaces:
class MyMapFunction implements MapFunction&amp;lt;String, Integer&amp;gt; { public Integer map(String value) { return Integer.parseInt(value); } } data.map(new MyMapFunction()); Anonymous classes # You can pass a function as an anonymous class:</description>
    </item>
    
    <item>
      <title>YARN</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/yarn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/resource-providers/yarn/</guid>
      <description>Apache Hadoop YARN # Getting Started # This Getting Started section guides you through setting up a fully functional Flink Cluster on YARN.
Introduction # Apache Hadoop YARN is a resource provider popular with many data processing frameworks. Flink services are submitted to YARN&amp;rsquo;s ResourceManager, which spawns containers on machines managed by YARN NodeManagers. Flink deploys its JobManager and TaskManager instances into such containers.
Flink can dynamically allocate and de-allocate TaskManager resources depending on the number of processing slots required by the job(s) running on the JobManager.</description>
    </item>
    
    <item>
      <title>ALTER Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/alter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/alter/</guid>
      <description>ALTER Statements # ALTER statements are used to modified a registered table/view/function definition in the Catalog.
Flink SQL supports the following ALTER statements for now:
ALTER TABLE ALTER VIEW ALTER DATABASE ALTER FUNCTION Run an ALTER statement # Java ALTER statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful ALTER operation, otherwise will throw an exception.
The following examples show how to run an ALTER statement in TableEnvironment.</description>
    </item>
    
    <item>
      <title>Canal</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/canal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/canal/</guid>
      <description>Canal Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Canal is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into other systems. Canal provides a unified format schema for changelog and supports to serialize messages using JSON and protobuf (protobuf is the default format for Canal).
Flink supports to interpret Canal JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as</description>
    </item>
    
    <item>
      <title>Event-driven Applications</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/event_driven/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/event_driven/</guid>
      <description>Event-driven Applications # Process Functions # Introduction # A ProcessFunction combines event processing with timers and state, making it a powerful building block for stream processing applications. This is the basis for creating event-driven applications with Flink. It is very similar to a RichFlatMapFunction, but with the addition of timers.
Example # If you&amp;rsquo;ve done the hands-on exercise in the Streaming Analytics training, you will recall that it uses a TumblingEventTimeWindow to compute the sum of the tips for each driver during each hour, like this:</description>
    </item>
    
    <item>
      <title>FileSystem</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/filesystem/</guid>
      <description>FileSystem # This connector provides a unified Source and Sink for BATCH and STREAMING that reads or writes (partitioned) files to file systems supported by the Flink FileSystem abstraction. This filesystem connector provides the same guarantees for both BATCH and STREAMING and is designed to provide exactly-once semantics for STREAMING execution.
The connector supports reading and writing a set of files from any (distributed) file system (e.g. POSIX, S3, HDFS) with a format (e.</description>
    </item>
    
    <item>
      <title>Flink Operations Playground</title>
      <link>//localhost/flink/flink-docs-master/docs/try-flink/flink-operations-playground/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/try-flink/flink-operations-playground/</guid>
      <description>Flink Operations Playground # There are many ways to deploy and operate Apache Flink in various environments. Regardless of this variety, the fundamental building blocks of a Flink Cluster remain the same, and similar operational principles apply.
In this playground, you will learn how to manage and run Flink Jobs. You will see how to deploy and monitor an application, experience how Flink recovers from Job failure, and perform everyday operational tasks like upgrades and rescaling.</description>
    </item>
    
    <item>
      <title>Graph Generators</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_generators/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/graph_generators/</guid>
      <description>Graph Generators # Gelly provides a collection of scalable graph generators. Each generator is
parallelizable, in order to create large datasets scale-free, generating the same graph regardless of parallelism thrifty, using as few operators as possible Graph generators are configured using the builder pattern. The parallelism of generator operators can be set explicitly by calling setParallelism(parallelism). Lowering the parallelism will reduce the allocation of memory and network buffers.
Graph-specific configuration must be called first, then configuration common to all generators, and lastly the call to generate().</description>
    </item>
    
    <item>
      <title>JDBC</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/jdbc/</guid>
      <description>JDBC SQL Connector # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append &amp;amp; Upsert Mode
The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases.
The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn&amp;rsquo;t support to consume UPDATE/DELETE messages.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/metrics/</guid>
      <description>Metrics # Flink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from any user function that extends RichFunction by calling getRuntimeContext().getMetricGroup(). This method returns a MetricGroup object on which you can create and register new metrics.
Metric types # Flink supports Counters, Gauges, Histograms and Meters.
Counter # A Counter is used to count something. The current value can be in- or decremented using inc()/inc(long n) or dec()/dec(long n).</description>
    </item>
    
    <item>
      <title>State Backends</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state_backends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state_backends/</guid>
      <description>State Backends # Flink provides different state backends that specify how and where state is stored.
State can be located on Java’s heap or off-heap. Depending on your state backend, Flink can also manage the state for the application, meaning Flink deals with the memory management (possibly spilling to disk if necessary) to allow applications to hold very large state. By default, the configuration file flink-conf.yaml determines the state backend for all Flink jobs.</description>
    </item>
    
    <item>
      <title>Task Lifecycle</title>
      <link>//localhost/flink/flink-docs-master/docs/internals/task_lifecycle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/internals/task_lifecycle/</guid>
      <description>Task Lifecycle # A task in Flink is the basic unit of execution. It is the place where each parallel instance of an operator is executed. As an example, an operator with a parallelism of 5 will have each of its instances executed by a separate task.
The StreamTask is the base for all different task sub-types in Flink&amp;rsquo;s streaming engine. This document goes through the different phases in the lifecycle of the StreamTask and describes the main methods representing each of these phases.</description>
    </item>
    
    <item>
      <title>Test Dependencies</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/testing/</guid>
      <description>Dependencies for Testing # Flink provides utilities for testing your job that you can add as dependencies.
DataStream API Testing # You need to add the following dependencies if you want to develop tests for a job built with the DataStream API:
Maven Open the pom.xml file in your project directory and add the following in the dependencies block. &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-test-utils&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;ltscope&amp;gttest&amp;lt/scope&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Check out Project configuration for more details.</description>
    </item>
    
    <item>
      <title>Troubleshooting</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_trouble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_trouble/</guid>
      <description>Troubleshooting # IllegalConfigurationException # If you see an IllegalConfigurationException thrown from TaskExecutorProcessUtils or JobManagerProcessUtils, it usually indicates that there is either an invalid configuration value (e.g. negative memory size, fraction that is greater than 1, etc.) or configuration conflicts. Check the documentation chapters or configuration options related to the memory components mentioned in the exception message.
OutOfMemoryError: Java heap space # The exception usually indicates that the JVM Heap is too small.</description>
    </item>
    
    <item>
      <title>Windowing TVF</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-tvf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-tvf/</guid>
      <description>Windowing table-valued functions (Windowing TVFs) # Batch Streaming
Windows are at the heart of processing infinite streams. Windows split the stream into “buckets” of finite size, over which we can apply computations. This document focuses on how windowing is performed in Flink SQL and how the programmer can benefit to the maximum from its offered functionality.
Apache Flink provides several window table-valued functions (TVF) to divide the elements of your table into windows, including:</description>
    </item>
    
    <item>
      <title>Bipartite Graph</title>
      <link>//localhost/flink/flink-docs-master/docs/libs/gelly/bipartite_graph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/libs/gelly/bipartite_graph/</guid>
      <description>Bipartite Graph # Bipartite Graph currently only supported in Gelly Java API. Bipartite Graph # A bipartite graph (also called a two-mode graph) is a type of graph where vertices are separated into two disjoint sets. These sets are usually called top and bottom vertices. An edge in this graph can only connect vertices from opposite sets (i.e. bottom vertex to top vertex) and cannot connect two vertices in the same set.</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/elasticsearch/</guid>
      <description>Elasticsearch SQL Connector # Sink: Batch Sink: Streaming Append &amp;amp; Upsert Mode
The Elasticsearch connector allows for writing into an index of the Elasticsearch engine. This document describes how to setup the Elasticsearch Connector to run SQL queries against Elasticsearch.
The connector can operate in upsert mode for exchanging UPDATE/DELETE messages with the external system using the primary key defined on the DDL.
If no primary key is defined on the DDL, the connector can only operate in append mode for exchanging INSERT only messages with external system.</description>
    </item>
    
    <item>
      <title>Fault Tolerance</title>
      <link>//localhost/flink/flink-docs-master/docs/learn-flink/fault_tolerance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/learn-flink/fault_tolerance/</guid>
      <description>Fault Tolerance via State Snapshots # State Backends # The keyed state managed by Flink is a sort of sharded, key/value store, and the working copy of each item of keyed state is kept somewhere local to the taskmanager responsible for that key. Operator state is also local to the machine(s) that need(s) it.
This state that Flink manages is stored in a state backend. Two implementations of state backends are available &amp;ndash; one based on RocksDB, an embedded key/value store that keeps its working state on disk, and another heap-based state backend that keeps its working state in memory, on the Java heap.</description>
    </item>
    
    <item>
      <title>INSERT Statement</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/insert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/insert/</guid>
      <description>INSERT Statement # INSERT statements are used to add rows to a table.
Run an INSERT statement # Java Single INSERT statement can be executed through the executeSql() method of the TableEnvironment. The executeSql() method for INSERT statement will submit a Flink job immediately, and return a TableResult instance which associates the submitted job. Multiple INSERT statements can be executed through the addInsertSql() method of the StatementSet which can be created by the TableEnvironment.</description>
    </item>
    
    <item>
      <title>Maxwell</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/maxwell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/maxwell/</guid>
      <description>Maxwell Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Maxwell is a CDC (Changelog Data Capture) tool that can stream changes in real-time from MySQL into Kafka, Kinesis and other streaming connectors. Maxwell provides a unified format schema for changelog and supports to serialize messages using JSON.
Flink supports to interpret Maxwell JSON messages as INSERT/UPDATE/DELETE messages into Flink SQL system. This is useful in many cases to leverage this feature, such as</description>
    </item>
    
    <item>
      <title>Metric Reporters</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/metric_reporters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/metric_reporters/</guid>
      <description>Metric Reporters # Flink allows reporting metrics to external systems. For more information about Flink&amp;rsquo;s metric system go to the metric system documentation.
Metrics can be exposed to an external system by configuring one or several reporters in conf/flink-conf.yaml. These reporters will be instantiated on each job and task manager when they are started.
Below is a list of parameters that are generally applicable to all reporters. All properties are configured by setting metrics.</description>
    </item>
    
    <item>
      <title>Migration Guide</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/mem_migration/</guid>
      <description>Migration Guide # The memory setup has changed a lot with the 1.10 release for TaskManagers and with the 1.11 release for JobManagers. Many configuration options were removed or their semantics changed. This guide will help you to migrate the TaskManager memory configuration from Flink &amp;lt;= 1.9 to &amp;gt;= 1.10 and the JobManager memory configuration from Flink &amp;lt;= 1.10 to &amp;gt;= 1.11.
It is important to review this guide because the legacy and new memory configuration can result in different sizes of memory components.</description>
    </item>
    
    <item>
      <title>Python REPL</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/repls/python_shell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/repls/python_shell/</guid>
      <description>Python REPL # Flink comes with an integrated interactive Python Shell. It can be used in a local setup as well as in a cluster setup. See the standalone resource provider page for more information about how to setup a local Flink. You can also build a local setup from source.
Note The Python Shell will run the command “python”. Please refer to the Python Table API installation guide on how to set up the Python execution environments.</description>
    </item>
    
    <item>
      <title>RabbitMQ</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/rabbitmq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/rabbitmq/</guid>
      <description>RabbitMQ Connector # License of the RabbitMQ Connector # Flink&amp;rsquo;s RabbitMQ connector defines a Maven dependency on the &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo;, is triple-licensed under the Mozilla Public License 1.1 (&amp;ldquo;MPL&amp;rdquo;), the GNU General Public License version 2 (&amp;ldquo;GPL&amp;rdquo;) and the Apache License version 2 (&amp;ldquo;ASL&amp;rdquo;).
Flink itself neither reuses source code from the &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo; nor packages binaries from the &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo;.
Users that create and publish derivative work based on Flink&amp;rsquo;s RabbitMQ connector (thereby re-distributing the &amp;ldquo;RabbitMQ AMQP Java Client&amp;rdquo;) must be aware that this may be subject to conditions declared in the Mozilla Public License 1.</description>
    </item>
    
    <item>
      <title>REST API</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/rest_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/rest_api/</guid>
      <description>REST API # Flink has a monitoring API that can be used to query status and statistics of running jobs, as well as recent completed jobs. This monitoring API is used by Flink&amp;rsquo;s own dashboard, but is designed to be used also by custom monitoring tools.
The monitoring API is a REST-ful API that accepts HTTP requests and responds with JSON data.
Overview # The monitoring API is backed by a web server that runs as part of the JobManager.</description>
    </item>
    
    <item>
      <title>Window Aggregation</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/</guid>
      <description>Window Aggregation # Window TVF Aggregation # Batch Streaming
Window aggregations are defined in the GROUP BY clause contains &amp;ldquo;window_start&amp;rdquo; and &amp;ldquo;window_end&amp;rdquo; columns of the relation applied Windowing TVF. Just like queries with regular GROUP BY clauses, queries with a group by window aggregation will compute a single result row per group.
SELECT ... FROM &amp;lt;windowed_table&amp;gt; -- relation applied windowing TVF GROUP BY window_start, window_end, ... Unlike other aggregations on continuous tables, window aggregation do not emit intermediate results but only a final result, the total aggregation at the end of the window.</description>
    </item>
    
    <item>
      <title>ANALYZE Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/analyze/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/analyze/</guid>
      <description>ANALYZE Statements # ANALYZE statements are used to collect statistics for existing tables and store the result to catalog. Only ANALYZE TABLE statements are supported now, and need to be triggered manually instead of automatically.
Attention Currently, ANALYZE TABLE only supports in batch mode. Only existing table is supported, and an exception will be thrown if the table is a view or table not exists.
Run an ANALYZE TABLE statement # Java ANALYZE TABLE statements can be executed with the executeSql() method of the TableEnvironment.</description>
    </item>
    
    <item>
      <title>Checkpoints</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/checkpoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/checkpoints/</guid>
      <description>Checkpoints # Overview # Checkpoints make state in Flink fault tolerant by allowing state and the corresponding stream positions to be recovered, thereby giving the application the same semantics as a failure-free execution.
See Checkpointing for how to enable and configure checkpoints for your program.
To understand the differences between checkpoints and savepoints see checkpoints vs. savepoints.
Checkpoint Storage # When checkpointing is enabled, managed state is persisted to ensure consistent recovery in case of failures.</description>
    </item>
    
    <item>
      <title>DESCRIBE Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/describe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/describe/</guid>
      <description>DESCRIBE Statements # DESCRIBE statements are used to describe the schema of a table or a view.
Run a DESCRIBE statement # Java DESCRIBE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns the schema of given table for a successful DESCRIBE operation, otherwise will throw an exception.
The following examples show how to run a DESCRIBE statement in TableEnvironment.
Scala DESCRIBE statements can be executed with the executeSql() method of the TableEnvironment.</description>
    </item>
    
    <item>
      <title>FileSystem</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/filesystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/filesystem/</guid>
      <description>FileSystem SQL Connector # This connector provides access to partitioned files in filesystems supported by the Flink FileSystem abstraction.
The file system connector itself is included in Flink and does not require an additional dependency. The corresponding jar can be found in the Flink distribution inside the /lib directory. A corresponding format needs to be specified for reading and writing rows from and to a file system.
The file system connector allows for reading and writing from a local or distributed filesystem.</description>
    </item>
    
    <item>
      <title>Google Cloud PubSub</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/pubsub/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/pubsub/</guid>
      <description>Google Cloud PubSub # This connector provides a Source and Sink that can read from and write to Google Cloud PubSub. To use this connector, add the following dependency to your project:
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-gcp-pubsub&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Note: This connector has been added to Flink recently. It has not received widespread testing yet. Note that the streaming connectors are currently not part of the binary distribution. See here for information about how to package the program with the libraries for cluster execution.</description>
    </item>
    
    <item>
      <title>Group Aggregation</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/group-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/group-agg/</guid>
      <description>Group Aggregation # Batch Streaming
Like most data systems, Apache Flink supports aggregate functions; both built-in and user-defined. User-defined functions must be registered in a catalog before use.
An aggregate function computes a single result from multiple input rows. For example, there are aggregates to compute the COUNT, SUM, AVG (average), MAX (maximum) and MIN (minimum) over a set of rows.
SELECT COUNT(*) FROM Orders For streaming queries, it is important to understand that Flink runs continuous queries that never terminate.</description>
    </item>
    
    <item>
      <title>Hadoop MapReduce compatibility with Flink</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/hadoop_map_reduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/hadoop_map_reduce/</guid>
      <description>Flink and Map Reduce compatibility # Flink is compatible with Apache Hadoop MapReduce interfaces and therefore allows reusing code that was implemented for Hadoop MapReduce.
You can:
use Hadoop&amp;rsquo;s Writable data types in Flink programs. use any Hadoop InputFormat as a DataSource. use any Hadoop OutputFormat as a DataSink. use a Hadoop Mapper as FlatMapFunction. use a Hadoop Reducer as GroupReduceFunction. This document shows how to use existing Hadoop MapReduce code with Flink.</description>
    </item>
    
    <item>
      <title>Hybrid Source</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/hybridsource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/hybridsource/</guid>
      <description>Hybrid Source # HybridSource is a source that contains a list of concrete sources. It solves the problem of sequentially reading input from heterogeneous sources to produce a single input stream.
For example, a bootstrap use case may need to read several days worth of bounded input from S3 before continuing with the latest unbounded input from Kafka. HybridSource switches from FileSource to KafkaSource when the bounded file input finishes without interrupting the application.</description>
    </item>
    
    <item>
      <title>Ogg</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/ogg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/ogg/</guid>
      <description>Ogg Format # Changelog-Data-Capture Format Format: Serialization Schema Format: Deserialization Schema
Oracle GoldenGate (a.k.a ogg) is a managed service providing a real-time data mesh platform, which uses replication to keep data highly available, and enabling real-time analysis. Customers can design, execute, and monitor their data replication and stream data processing solutions without the need to allocate or manage compute environments. Ogg provides a format schema for changelog and supports to serialize messages using JSON.</description>
    </item>
    
    <item>
      <title>Checkpointing under backpressure</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/checkpointing_under_backpressure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/checkpointing_under_backpressure/</guid>
      <description>Checkpointing under backpressure # Normally aligned checkpointing time is dominated by the synchronous and asynchronous parts of the checkpointing process. However, when a Flink job is running under heavy backpressure, the dominant factor in the end-to-end time of a checkpoint can be the time to propagate checkpoint barriers to all operators/subtasks. This is explained in the overview of the checkpointing process). and can be observed by high alignment time and start delay metrics.</description>
    </item>
    
    <item>
      <title>EXPLAIN Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/explain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/explain/</guid>
      <description>EXPLAIN Statements # EXPLAIN statements are used to explain the logical and optimized query plans of a query or an INSERT statement.
Run an EXPLAIN statement # Java EXPLAIN statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns explain result for a successful EXPLAIN operation, otherwise will throw an exception.
The following examples show how to run an EXPLAIN statement in TableEnvironment.
Scala EXPLAIN statements can be executed with the executeSql() method of the TableEnvironment.</description>
    </item>
    
    <item>
      <title>HBase</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/hbase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/hbase/</guid>
      <description>HBase SQL Connector # Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Upsert Mode
The HBase connector allows for reading from and writing to an HBase cluster. This document describes how to setup the HBase Connector to run SQL queries against HBase.
HBase always works in upsert mode for exchange changelog messages with the external system using a primary key defined on the DDL. The primary key must be defined on the HBase rowkey field (rowkey field must be declared).</description>
    </item>
    
    <item>
      <title>Local Execution</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/local_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/local_execution/</guid>
      <description>Local Execution # Flink can run on a single machine, even in a single Java Virtual Machine. This allows users to test and debug Flink programs locally. This section gives an overview of the local execution mechanisms.
The local environments and executors allow you to run Flink programs in a local Java Virtual Machine, or with within any JVM as part of existing programs. Most examples can be launched locally by simply hitting the &amp;ldquo;Run&amp;rdquo; button of your IDE.</description>
    </item>
    
    <item>
      <title>Over Aggregation</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/over-agg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/over-agg/</guid>
      <description>Over Aggregation # Batch Streaming
OVER aggregates compute an aggregated value for every input row over a range of ordered rows. In contrast to GROUP BY aggregates, OVER aggregates do not reduce the number of result rows to a single row for every group. Instead OVER aggregates produce an aggregated value for every input row.
The following query computes for every order the sum of amounts of all orders for the same product that were received within one hour before the current order.</description>
    </item>
    
    <item>
      <title>Parquet</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/parquet/</guid>
      <description>Parquet Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Parquet format allows to read and write Parquet data.
Dependencies # In order to use the Parquet format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-parquet&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Pulsar</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/pulsar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/pulsar/</guid>
      <description>Apache Pulsar Connector # Flink provides an Apache Pulsar connector for reading and writing data from and to Pulsar topics with exactly-once guarantees.
Dependency # You can use the connector with the Pulsar 2.8.1 or higher. Because the Pulsar connector supports Pulsar transactions, it is recommended to use the Pulsar 2.9.2 or higher. Details on Pulsar compatibility can be found in PIP-72.
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-pulsar&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard!</description>
    </item>
    
    <item>
      <title>Savepoints</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/savepoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/savepoints/</guid>
      <description>Savepoints # What is a Savepoint? # A Savepoint is a consistent image of the execution state of a streaming job, created via Flink&amp;rsquo;s checkpointing mechanism. You can use Savepoints to stop-and-resume, fork, or update your Flink jobs. Savepoints consist of two parts: a directory with (typically large) binary files on stable storage (e.g. HDFS, S3, &amp;hellip;) and a (relatively small) meta data file. The files on stable storage represent the net data of the job&amp;rsquo;s execution state image.</description>
    </item>
    
    <item>
      <title>Advanced Configuration</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/configuration/advanced/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/configuration/advanced/</guid>
      <description>Advanced Configuration Topics # Anatomy of the Flink distribution # Flink itself consists of a set of classes and dependencies that form the core of Flink&amp;rsquo;s runtime and must be present when a Flink application is started. The classes and dependencies needed to run the system handle areas such as coordination, networking, checkpointing, failover, APIs, operators (such as windowing), resource management, etc.
These core classes and dependencies are packaged in the flink-dist.</description>
    </item>
    
    <item>
      <title>Checkpoints vs. Savepoints</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/</guid>
      <description>Checkpoints vs. Savepoints # Overview # Conceptually, Flink&amp;rsquo;s savepoints are different from checkpoints in a way that&amp;rsquo;s analogous to how backups are different from recovery logs in traditional database systems.
The primary purpose of checkpoints is to provide a recovery mechanism in case of unexpected job failures. A checkpoint&amp;rsquo;s lifecycle is managed by Flink, i.e. a checkpoint is created, owned, and released by Flink - without user interaction. Because checkpoints are being triggered often, and are relied upon for failure recovery, the two main design goals for the checkpoint implementation are i) being as lightweight to create and ii) being as fast to restore from as possible.</description>
    </item>
    
    <item>
      <title>JDBC</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/datastream/jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/datastream/jdbc/</guid>
      <description>JDBC Connector # This connector provides a sink that writes data to a JDBC database.
To use it, add the following dependency to your project (along with your JDBC driver):
&amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-connector-jdbc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Note that the streaming connectors are currently NOT part of the binary distribution. See how to link with them for cluster execution here. A driver dependency is also required to connect to a specified database.</description>
    </item>
    
    <item>
      <title>Joins</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/joins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/joins/</guid>
      <description>Joins # Batch Streaming
Flink SQL supports complex and flexible join operations over dynamic tables. There are several different types of joins to account for the wide variety of semantics queries may require.
By default, the order of joins is not optimized. Tables are joined in the order in which they are specified in the FROM clause. You can tweak the performance of your join queries, by listing the tables with the lowest update frequency first and the tables with the highest update frequency last.</description>
    </item>
    
    <item>
      <title>Orc</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/orc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/orc/</guid>
      <description>Orc Format # Format: Serialization Schema Format: Deserialization Schema
The Apache Orc format allows to read and write Orc data.
Dependencies # In order to use the ORC format the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles.
Maven dependency SQL Client &amp;ltdependency&amp;gt &amp;ltgroupId&amp;gtorg.apache.flink&amp;lt/groupId&amp;gt &amp;ltartifactId&amp;gtflink-orc&amp;lt/artifactId&amp;gt &amp;ltversion&amp;gt1.16-SNAPSHOT&amp;lt/version&amp;gt &amp;lt/dependency&amp;gt Copied to clipboard! Only available for stable releases.</description>
    </item>
    
    <item>
      <title>Upgrading Applications and Flink Versions</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/upgrading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/upgrading/</guid>
      <description>Upgrading Applications and Flink Versions # Flink DataStream programs are typically designed to run for long periods of time such as weeks, months, or even years. As with all long-running services, Flink streaming applications need to be maintained, which includes fixing bugs, implementing improvements, or migrating an application to a Flink cluster of a later version.
This document describes how to update a Flink streaming application and how to migrate a running streaming application to a different Flink cluster.</description>
    </item>
    
    <item>
      <title>USE Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/use/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/use/</guid>
      <description>USE Statements # USE statements are used to set the current database or catalog, or change the resolution order and enabled status of module.
Run a USE statement # Java USE statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful USE operation, otherwise will throw an exception.
The following examples show how to run a USE statement in TableEnvironment.</description>
    </item>
    
    <item>
      <title>Vectorized User-defined Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/vectorized_python_udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/udfs/vectorized_python_udfs/</guid>
      <description>Vectorized User-defined Functions # Vectorized Python user-defined functions are functions which are executed by transferring a batch of elements between JVM and Python VM in Arrow columnar format. The performance of vectorized Python user-defined functions are usually much higher than non-vectorized Python user-defined functions as the serialization/deserialization overhead and invocation overhead are much reduced. Besides, users could leverage the popular Python libraries such as Pandas, Numpy, etc for the vectorized Python user-defined functions implementation.</description>
    </item>
    
    <item>
      <title>Window JOIN</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-join/</guid>
      <description>Window Join # Batch Streaming
A window join adds the dimension of time into the join criteria themselves. In doing so, the window join joins the elements of two streams that share a common key and are in the same window. The semantic of window join is same to the DataStream window join
For streaming queries, unlike other joins on continuous tables, window join does not emit intermediate results but only emits final results at the end of the window.</description>
    </item>
    
    <item>
      <title>Data Sources</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/sources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/sources/</guid>
      <description>Data Sources # This page describes Flink&amp;rsquo;s Data Source API and the concepts and architecture behind it. Read this, if you are interested in how data sources in Flink work, or if you want to implement a new Data Source.
If you are looking for pre-defined source connectors, please check the Connector Docs.
Data Source Concepts # Core Components
A Data Source has three core components: Splits, the SplitEnumerator, and the SourceReader.</description>
    </item>
    
    <item>
      <title>File Systems</title>
      <link>//localhost/flink/flink-docs-master/docs/internals/filesystems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/internals/filesystems/</guid>
      <description>File Systems # Flink has its own file system abstraction via the org.apache.flink.core.fs.FileSystem class. This abstraction provides a common set of operations and minimal guarantees across various types of file system implementations.
The FileSystem&amp;rsquo;s set of available operations is quite limited, in order to support a wide range of file systems. For example, appending to or mutating existing files is not supported.
File systems are identified by a file system scheme, such as file://, hdfs://, etc.</description>
    </item>
    
    <item>
      <title>Production Readiness Checklist</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/production_ready/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/production_ready/</guid>
      <description>Production Readiness Checklist # The production readiness checklist provides an overview of configuration options that should be carefully considered before bringing an Apache Flink job into production. While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs.
Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale.</description>
    </item>
    
    <item>
      <title>Raw</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/formats/raw/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/formats/raw/</guid>
      <description>Raw Format # Format: Serialization Schema Format: Deserialization Schema
The Raw format allows to read and write raw (byte based) values as a single column.
Note: this format encodes null values as null of byte[] type. This may have limitation when used in upsert-kafka, because upsert-kafka treats null values as a tombstone message (DELETE on the key). Therefore, we recommend avoiding using upsert-kafka connector and the raw format as a value.</description>
    </item>
    
    <item>
      <title>Set Operations</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/set-ops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/set-ops/</guid>
      <description>Set Operations # Batch Streaming
UNION # UNION and UNION ALL return the rows that are found in either table. UNION takes only distinct rows while UNION ALL does not remove duplicates from the result rows.
Flink SQL&amp;gt; create view t1(s) as values (&amp;#39;c&amp;#39;), (&amp;#39;a&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;c&amp;#39;); Flink SQL&amp;gt; create view t2(s) as values (&amp;#39;d&amp;#39;), (&amp;#39;e&amp;#39;), (&amp;#39;a&amp;#39;), (&amp;#39;b&amp;#39;), (&amp;#39;b&amp;#39;); Flink SQL&amp;gt; (SELECT s FROM t1) UNION (SELECT s FROM t2); +---+ | s| +---+ | c| | a| | b| | d| | e| +---+ Flink SQL&amp;gt; (SELECT s FROM t1) UNION ALL (SELECT s FROM t2); +---+ | c| +---+ | c| | a| | b| | b| | c| | d| | e| | a| | b| | b| +---+ INTERSECT # INTERSECT and INTERSECT ALL return the rows that are found in both tables.</description>
    </item>
    
    <item>
      <title>SHOW Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/show/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/show/</guid>
      <description>SHOW Statements # SHOW statements are used to list objects within their corresponding parent, such as catalogs, databases, tables and views, columns, functions, and modules. See the individual commands for more details and additional options.
SHOW CREATE statements are used to print a DDL statement with which a given object can be created. The currently &amp;lsquo;SHOW CREATE&amp;rsquo; statement is only available in printing DDL statement of the given table and view.</description>
    </item>
    
    <item>
      <title>LOAD Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/load/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/load/</guid>
      <description>LOAD Statements # LOAD statements are used to load a built-in or user-defined module.
Run a LOAD statement # Java LOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful LOAD operation; otherwise, it will throw an exception.
The following examples show how to run a LOAD statement in TableEnvironment.
Scala LOAD statements can be executed with the executeSql() method of the TableEnvironment.</description>
    </item>
    
    <item>
      <title>ORDER BY clause</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/orderby/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/orderby/</guid>
      <description>ORDER BY clause # Batch Streaming
The ORDER BY clause causes the result rows to be sorted according to the specified expression(s). If two rows are equal according to the leftmost expression, they are compared according to the next expression and so on. If they are equal according to all specified expressions, they are returned in an implementation-dependent order.
When running in streaming mode, the primary sort order of a table must be ascending on a time attribute.</description>
    </item>
    
    <item>
      <title>State Backends</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/state_backends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/state_backends/</guid>
      <description>State Backends # Programs written in the Data Stream API often hold state in various forms:
Windows gather elements or aggregates until they are triggered Transformation functions may use the key/value state interface to store values Transformation functions may implement the CheckpointedFunction interface to make their local variables fault tolerant See also state section in the streaming API guide.
When checkpointing is activated, such state is persisted upon checkpoints to guard against data loss and recover consistently.</description>
    </item>
    
    <item>
      <title>Cluster Execution</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/cluster_execution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/cluster_execution/</guid>
      <description>Cluster Execution # Flink programs can run distributed on clusters of many machines. There are two ways to send a program to a cluster for execution:
Command Line Interface # The command line interface lets you submit packaged programs (JARs) to a cluster (or single machine setup).
Please refer to the Command Line Interface documentation for details.
Remote Environment # The remote environment lets you execute Flink Java programs on a cluster directly.</description>
    </item>
    
    <item>
      <title>DataGen</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/datagen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/datagen/</guid>
      <description>DataGen SQL Connector # Scan Source: Bounded Scan Source: UnBounded
The DataGen connector allows for creating tables based on in-memory data generation. This is useful when developing queries locally without access to external systems such as Kafka. Tables can include Computed Column syntax which allows for flexible record generation.
The DataGen connector is built-in, no additional dependencies are required.
Usage # By default, a DataGen table will create an unbounded number of rows with a random value for each column.</description>
    </item>
    
    <item>
      <title>LIMIT clause</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/limit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/limit/</guid>
      <description>LIMIT clause # Batch LIMIT clause constrains the number of rows returned by the SELECT statement. In general, this clause is used in conjunction with ORDER BY to ensure that the results are deterministic.
The following example selects the first 3 rows in Orders table.
SELECT * FROM Orders ORDER BY orderTime LIMIT 3 Back to top</description>
    </item>
    
    <item>
      <title>Tuning Checkpoints and Large State</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/large_state_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/large_state_tuning/</guid>
      <description>Tuning Checkpoints and Large State # This page gives a guide how to configure and tune applications that use large state.
Overview # For Flink applications to run reliably at large scale, two conditions must be fulfilled:
The application needs to be able to take checkpoints reliably
The resources need to be sufficient catch up with the input data streams after a failure
The first sections discuss how to get well performing checkpoints at scale.</description>
    </item>
    
    <item>
      <title>UNLOAD Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/unload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/unload/</guid>
      <description>UNLOAD Statements # UNLOAD statements are used to unload a built-in or user-defined module.
Run a UNLOAD statement # Java UNLOAD statements can be executed with the executeSql() method of the TableEnvironment. The executeSql() method returns &amp;lsquo;OK&amp;rsquo; for a successful LOAD operation; otherwise it will throw an exception.
The following examples show how to run a UNLOAD statement in TableEnvironment.
Scala UNLOAD statements can be executed with the executeSql() method of the TableEnvironment.</description>
    </item>
    
    <item>
      <title>Print</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/print/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/print/</guid>
      <description>Print SQL Connector # Sink The Print connector allows for writing every row to the standard output or standard error stream.
It is designed for:
Easy test for streaming job. Very useful in production debugging. Four possible format options:
Print Condition1 Condition2 PRINT_IDENTIFIER:taskId&gt; output PRINT_IDENTIFIER provided parallelism &gt; 1 PRINT_IDENTIFIER&gt; output PRINT_IDENTIFIER provided parallelism == 1 taskId&gt; output no PRINT_IDENTIFIER provided parallelism &gt; 1 output no PRINT_IDENTIFIER provided parallelism == 1 The output string format is &amp;ldquo;$row_kind(f0,f1,f2&amp;hellip;)&amp;rdquo;, row_kind is the short string of RowKind, example is: &amp;ldquo;+I(1,1)&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>SET Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/set/</guid>
      <description>SET Statements # SET statements are used to modify the configuration or list the configuration.
Run a SET statement # SQL CLI SET statements can be executed in SQL CLI.
The following examples show how to run a SET statement in SQL CLI.
SQL CLI Flink SQL&amp;gt; SET &amp;#39;table.local-time-zone&amp;#39; = &amp;#39;Europe/Berlin&amp;#39;; [INFO] Session property has been set. Flink SQL&amp;gt; SET; &amp;#39;table.local-time-zone&amp;#39; = &amp;#39;Europe/Berlin&amp;#39; Syntax # SET (&amp;#39;key&amp;#39; = &amp;#39;value&amp;#39;)? If no key and value are specified, it just prints all the properties.</description>
    </item>
    
    <item>
      <title>Top-N</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/topn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/topn/</guid>
      <description>Top-N # Batch Streaming
Top-N queries ask for the N smallest or largest values ordered by columns. Both smallest and largest values sets are considered Top-N queries. Top-N queries are useful in cases where the need is to display only the N bottom-most or the N top- most records from batch/streaming table on a condition. This result set can be used for further analysis.
Flink uses the combination of a OVER window clause and a filter condition to express a Top-N query.</description>
    </item>
    
    <item>
      <title>BlackHole</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/blackhole/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/blackhole/</guid>
      <description>BlackHole SQL Connector # Sink Sink
The BlackHole connector allows for swallowing all input records. It is designed for:
high performance testing. UDF to output, not substantive sink. Just like /dev/null device on Unix-like operating systems.
The BlackHole connector is built-in.
How to create a BlackHole table # CREATE TABLE blackhole_table ( f0 INT, f1 INT, f2 STRING, f3 DOUBLE ) WITH ( &amp;#39;connector&amp;#39; = &amp;#39;blackhole&amp;#39; ); Alternatively, it may be based on an existing schema using the LIKE Clause.</description>
    </item>
    
    <item>
      <title>RESET Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/reset/</guid>
      <description>RESET Statements # RESET statements are used to reset the configuration to the default.
Run a RESET statement # SQL CLI RESET statements can be executed in SQL CLI.
The following examples show how to run a RESET statement in SQL CLI.
SQL CLI Flink SQL&amp;gt; RESET &amp;#39;table.planner&amp;#39;; [INFO] Session property has been reset. Flink SQL&amp;gt; RESET; [INFO] All session properties have been set to their default values. Syntax # RESET (&amp;#39;key&amp;#39;)?</description>
    </item>
    
    <item>
      <title>Window Top-N</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-topn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-topn/</guid>
      <description>Window Top-N # Batch Streaming
Window Top-N is a special Top-N which returns the N smallest or largest values for each window and other partitioned keys.
For streaming queries, unlike regular Top-N on continuous tables, window Top-N does not emit intermediate results but only a final result, the total top N records at the end of the window. Moreover, window Top-N purges all intermediate state when no longer needed. Therefore, window Top-N queries have better performance if users don&amp;rsquo;t need results updated per record.</description>
    </item>
    
    <item>
      <title>Deduplication</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/deduplication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/deduplication/</guid>
      <description>Deduplication # Batch Streaming
Deduplication removes rows that duplicate over a set of columns, keeping only the first one or the last one. In some cases, the upstream ETL jobs are not end-to-end exactly-once; this may result in duplicate records in the sink in case of failover. However, the duplicate records will affect the correctness of downstream analytical jobs - e.g. SUM, COUNT - so deduplication is needed before further analysis.</description>
    </item>
    
    <item>
      <title>JAR Statements</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/jar/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/jar/</guid>
      <description>JAR Statements # JAR statements are used to add user jars into the classpath or remove user jars from the classpath or show added jars in the classpath in the runtime.
Flink SQL supports the following JAR statements for now:
ADD JAR REMOVE JAR SHOW JARS Attention JAR statements only work in the SQL CLI.
Run a JAR statement # SQL CLI The following examples show how to run JAR statements in SQL CLI.</description>
    </item>
    
    <item>
      <title>Window Deduplication</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-deduplication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/window-deduplication/</guid>
      <description>Window Deduplication # Streaming Window Deduplication is a special Deduplication which removes rows that duplicate over a set of columns, keeping the first one or the last one for each window and partitioned keys.
For streaming queries, unlike regular Deduplicate on continuous tables, Window Deduplication does not emit intermediate results but only a final result at the end of the window. Moreover, window Deduplication purges all intermediate state when no longer needed.</description>
    </item>
    
    <item>
      <title>Pattern Recognition</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/match_recognize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sql/queries/match_recognize/</guid>
      <description>Pattern Recognition # Streaming It is a common use case to search for a set of event patterns, especially in case of data streams. Flink comes with a complex event processing (CEP) library which allows for pattern detection in event streams. Furthermore, Flink&amp;rsquo;s SQL API provides a relational way of expressing queries with a large set of built-in functions and rule-based optimizations that can be used out of the box.</description>
    </item>
    
    <item>
      <title>Batch Examples</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/dataset/examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/dataset/examples/</guid>
      <description>Batch Examples # The following example programs showcase different applications of Flink from simple word counting to graph algorithms. The code samples illustrate the use of Flink&amp;rsquo;s DataSet API.
The full source code of the following and more examples can be found in the flink-examples-batch module of the Flink source repository.
Running an example # In order to run a Flink example, we assume you have a running Flink instance available.</description>
    </item>
    
    <item>
      <title>Building Flink from Source</title>
      <link>//localhost/flink/flink-docs-master/docs/flinkdev/building/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/flinkdev/building/</guid>
      <description>Building Flink from Source # This page covers how to build Flink 1.16-SNAPSHOT from sources.
Build Flink # In order to build Flink you need the source code. Either download the source of a release or clone the git repository.
In addition you need Maven 3 and a JDK (Java Development Kit). Flink requires at least Java 11 to build.
NOTE: Maven 3.3.x can build Flink, but will not properly shade away certain dependencies.</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/types/</guid>
      <description>Data Types # Flink SQL has a rich set of native data types available to users.
Data Type # A data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of operations.
Flink&amp;rsquo;s data types are similar to the SQL standard&amp;rsquo;s data type terminology but also contain information about the nullability of a value for efficient handling of scalar expressions.</description>
    </item>
    
    <item>
      <title>Program Packaging</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/packaging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/packaging/</guid>
      <description>Program Packaging and Distributed Execution # As described earlier, Flink programs can be executed on clusters by using a remote environment. Alternatively, programs can be packaged into JAR Files (Java Archives) for execution. Packaging the program is a prerequisite to executing them through the command line interface.
Packaging Programs # To support execution from a packaged JAR file via the command line or web interface, a program must use the environment obtained by StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>Table API Tutorial</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table_api_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table_api_tutorial/</guid>
      <description>Table API Tutorial # Apache Flink offers a Table API as a unified, relational API for batch and stream processing, i.e., queries are executed with the same semantics on unbounded, real-time streams or bounded, batch data sets and produce the same results. The Table API in Flink is commonly used to ease the definition of data analytics, data pipelining, and ETL applications.
What Will You Be Building? # In this tutorial, you will learn how to build a pure Python Flink Table API pipeline.</description>
    </item>
    
    <item>
      <title>DataStream API Tutorial</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream_tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream_tutorial/</guid>
      <description>DataStream API Tutorial # Apache Flink offers a DataStream API for building robust, stateful streaming applications. It provides fine-grained control over state and time, which allows for the implementation of advanced event-driven systems. In this step-by-step guide, you’ll learn how to build a simple streaming application with PyFlink and the DataStream API.
What Will You Be Building? # In this tutorial, you will learn how to write a simple Python DataStream pipeline.</description>
    </item>
    
    <item>
      <title>Time Zone</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/timezone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/timezone/</guid>
      <description>Time Zone # Flink provides rich data types for Date and Time, including DATE, TIME, TIMESTAMP, TIMESTAMP_LTZ, INTERVAL YEAR TO MONTH, INTERVAL DAY TO SECOND (please see Date and Time for detailed information). Flink supports setting time zone in session level (please see table.local-time-zone for detailed information). These timestamp data types and time zone support of Flink make it easy to process business data across time zones.
TIMESTAMP vs TIMESTAMP_LTZ # TIMESTAMP type # TIMESTAMP(p) is an abbreviation for TIMESTAMP(p) WITHOUT TIME ZONE, the precision p supports range is from 0 to 9, 6 by default.</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/data_types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/data_types/</guid>
      <description>Data Types # In Apache Flink&amp;rsquo;s Python DataStream API, a data type describes the type of a value in the DataStream ecosystem. It can be used to declare input and output types of operations and informs the system how to serailize elements.
Pickle Serialization # If the type has not been declared, data would be serialized or deserialized using Pickle. For example, the program below specifies no data types.</description>
    </item>
    
    <item>
      <title>Intro to the Python Table API</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/intro_to_table_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/intro_to_table_api/</guid>
      <description>Intro to the Python Table API # This document is a short introduction to the PyFlink Table API, which is used to help novice users quickly understand the basic usage of PyFlink Table API. For advanced usage, please refer to other documents in this user guide.
Common Structure of Python Table API Program # All Table API and SQL programs, both batch and streaming, follow the same pattern. The following code example shows the common structure of Table API and SQL programs.</description>
    </item>
    
    <item>
      <title>TableEnvironment</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/table_environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/table_environment/</guid>
      <description>TableEnvironment # This document is an introduction of PyFlink TableEnvironment. It includes detailed descriptions of every public interface of the TableEnvironment class.
Create a TableEnvironment # The recommended way to create a TableEnvironment is to create from an EnvironmentSettings object:
from pyflink.common import Configuration from pyflink.table import EnvironmentSettings, TableEnvironment # create a streaming TableEnvironment config = Configuration() config.set_string(&amp;#39;execution.buffer-timeout&amp;#39;, &amp;#39;1 min&amp;#39;) env_settings = EnvironmentSettings \ .new_instance() \ .in_streaming_mode() \ .with_configuration(config) \ .</description>
    </item>
    
    <item>
      <title>Dependency Management</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/dependency_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/dependency_management/</guid>
      <description>Dependency Management # There are requirements to use dependencies inside the Python API programs. For example, users may need to use third-party Python libraries in Python user-defined functions. In addition, in scenarios such as machine learning prediction, users may want to load a machine learning model inside the Python user-defined functions.
When the PyFlink job is executed locally, users could install the third-party Python libraries into the local Python environment, download the machine learning model to local, etc.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/operations/operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/operations/operations/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Parallel Execution</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/parallel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/execution/parallel/</guid>
      <description>Parallel Execution # This section describes how the parallel execution of programs can be configured in Flink. A Flink program consists of multiple tasks (transformations/operators, data sources, and sinks). A task is split into several parallel instances for execution and each parallel instance processes a subset of the task&amp;rsquo;s input data. The number of parallel instances of a task is called its parallelism.
If you want to use savepoints you should also consider setting a maximum parallelism (or max parallelism).</description>
    </item>
    
    <item>
      <title>Row-based Operations</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/operations/row_based_operations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/operations/row_based_operations/</guid>
      <description>Row-based Operations # This page describes how to use row-based operations in PyFlink Table API.
Map # Performs a map operation with a python general scalar function or vectorized scalar function. The output will be flattened if the output type is a composite type.
from pyflink.common import Row from pyflink.table import EnvironmentSettings, TableEnvironment from pyflink.table.expressions import col from pyflink.table.types import DataTypes from pyflink.table.udf import udf env_settings = EnvironmentSettings.in_batch_mode() table_env = TableEnvironment.</description>
    </item>
    
    <item>
      <title>State</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/datastream/state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/datastream/state/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Table API</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/tableapi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/tableapi/</guid>
      <description>Table API # The Table API is a unified, relational API for stream and batch processing. Table API queries can be run on batch or streaming input without modifications. The Table API is a super set of the SQL language and is specially designed for working with Apache Flink. The Table API is a language-integrated API for Scala, Java and Python. Instead of specifying queries as String values as common with SQL, Table API queries are defined in a language-embedded style in Java, Scala or Python with IDE support like autocompletion and syntax validation.</description>
    </item>
    
    <item>
      <title>Data Types</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/python_types/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/python_types/</guid>
      <description>Data Types # This page describes the data types supported in PyFlink Table API.
Data Type # A data type describes the logical type of a value in the table ecosystem. It can be used to declare input and/or output types of Python user-defined functions. Users of the Python Table API work with instances of pyflink.table.types.DataType within the Python Table API or when defining user-defined functions.
A DataType instance declares the logical type which does not imply a concrete physical representation for transmission or storage.</description>
    </item>
    
    <item>
      <title>System (Built-in) Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/system_functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/system_functions/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>System (Built-in) Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/</guid>
      <description>System (Built-in) Functions # Flink Table API &amp;amp; SQL provides users with a set of built-in functions for data transformations. This page gives a brief overview of them. If a function that you need is not supported yet, you can implement a user-defined function. If you think that the function is general enough, please open a Jira issue for it with a detailed description.
Scalar Functions # The scalar functions take zero, one or more values as the input and return a single value as the result.</description>
    </item>
    
    <item>
      <title>Side Outputs</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/side_output/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/side_output/</guid>
      <description>Side Outputs # In addition to the main stream that results from DataStream operations, you can also produce any number of additional side output result streams. The type of data in the result streams does not have to match the type of data in the main stream and the types of the different side outputs can also differ. This operation can be useful when you want to split a stream of data where you would normally have to replicate the stream and then filter out from each stream the data that you don&amp;rsquo;t want to have.</description>
    </item>
    
    <item>
      <title>Execution Mode</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/python_execution_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/python_execution_mode/</guid>
      <description>Execution Mode # The Python API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job. The Python runtime execution mode defines how the Python user-defined functions will be executed.
Prior to release-1.15, there is the only execution mode called PROCESS execution mode. The PROCESS mode means that the Python user-defined functions will be executed in separate Python processes.</description>
    </item>
    
    <item>
      <title>Conversions between PyFlink Table and Pandas DataFrame</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/conversion_of_pandas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/conversion_of_pandas/</guid>
      <description>Conversions between PyFlink Table and Pandas DataFrame # PyFlink Table API supports conversion between PyFlink Table and Pandas DataFrame.
Convert Pandas DataFrame to PyFlink Table # Pandas DataFrames can be converted into a PyFlink Table. Internally, PyFlink will serialize the Pandas DataFrame using Arrow columnar format on the client. The serialized data will be processed and deserialized in Arrow source during execution. The Arrow source can also be used in streaming jobs, and is integrated with checkpointing to provide exactly-once guarantees.</description>
    </item>
    
    <item>
      <title>Conversions between Table and DataStream</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/conversion_of_data_stream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/conversion_of_data_stream/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>SQL</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/sql/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Handling Application Parameters</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/application_parameters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/application_parameters/</guid>
      <description>Handling Application Parameters # Handling Application Parameters # Almost all Flink applications, both batch and streaming, rely on external configuration parameters. They are used to specify input and output sources (like paths or addresses), system parameters (parallelism, runtime configuration), and application specific parameters (typically used within user functions).
Flink provides a simple utility called ParameterTool to provide some basic tooling for solving these problems. Please note that you don&amp;rsquo;t have to use the ParameterTool described here.</description>
    </item>
    
    <item>
      <title>Task Failure Recovery</title>
      <link>//localhost/flink/flink-docs-master/docs/ops/state/task_failure_recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/ops/state/task_failure_recovery/</guid>
      <description>Task Failure Recovery # When a task failure happens, Flink needs to restart the failed task and other affected tasks to recover the job to a normal state.
Restart strategies and failover strategies are used to control the task restarting. Restart strategies decide whether and when the failed/affected tasks can be restarted. Failover strategies decide which tasks should be restarted to recover the job.
Restart Strategies # The cluster can be started with a default restart strategy which is always used when no job specific restart strategy has been defined.</description>
    </item>
    
    <item>
      <title>User-defined Functions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/functions/udfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/functions/udfs/</guid>
      <description>User-defined Functions # User-defined functions (UDFs) are extension points to call frequently used logic or custom logic that cannot be expressed otherwise in queries.
User-defined functions can be implemented in a JVM language (such as Java or Scala) or Python. An implementer can use arbitrary third party libraries within a UDF. This page will focus on JVM-based languages, please refer to the PyFlink documentation for details on writing general and vectorized UDFs in Python.</description>
    </item>
    
    <item>
      <title>Catalogs</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/catalogs/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Modules</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/modules/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/modules/</guid>
      <description>Modules # Modules allow users to extend Flink&amp;rsquo;s built-in objects, such as defining functions that behave like Flink built-in functions. They are pluggable, and while Flink provides a few pre-built modules, users can write their own.
For example, users can define their own geo functions and plug them into Flink as built-in functions to be used in Flink SQL and Table APIs. Another example is users can load an out-of-shelf Hive module to use Hive built-in functions as Flink built-in functions.</description>
    </item>
    
    <item>
      <title>Catalogs</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/catalogs/</guid>
      <description>Catalogs # Catalogs provide metadata, such as databases, tables, partitions, views, and functions and information needed to access data stored in a database or other external systems.
One of the most crucial aspects of data processing is managing metadata. It may be transient metadata like temporary tables, or UDFs registered against the table environment. Or permanent metadata, like that in a Hive Metastore. Catalogs provide a unified API for managing metadata and making it accessible from the Table API and SQL Queries.</description>
    </item>
    
    <item>
      <title>SQL Client</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sqlclient/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sqlclient/</guid>
      <description>SQL Client # Flink’s Table &amp;amp; SQL API makes it possible to work with queries written in the SQL language, but these queries need to be embedded within a table program that is written in either Java or Scala. Moreover, these programs need to be packaged with a build tool before being submitted to a cluster. This more or less limits the usage of Flink to Java/Scala programmers.
The SQL Client aims to provide an easy way of writing, debugging, and submitting table programs to a Flink cluster without a single line of Java or Scala code.</description>
    </item>
    
    <item>
      <title>Download</title>
      <link>//localhost/flink/flink-docs-master/docs/connectors/table/downloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/connectors/table/downloads/</guid>
      <description>SQL Connectors download page # Download links are available only for stable releases. The page contains links to optional SQL Client connectors and formats that are not part of the binary distribution.
Optional SQL formats # Name Download link Avro Only available for stable versions. Avro Schema Registry Only available for stable versions. Debezium Only available for stable versions. ORC Only available for stable versions. Parquet Only available for stable versions.</description>
    </item>
    
    <item>
      <title>Network Buffer Tuning</title>
      <link>//localhost/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/</guid>
      <description>Network memory tuning guide # Overview # Each record in Flink is sent to the next subtask compounded with other records in a network buffer, the smallest unit for communication between subtasks. In order to maintain consistent high throughput, Flink uses network buffer queues (also known as in-flight data) on the input and output side of the transmission process.
Each subtask has an input queue waiting to consume data and an output queue waiting to send data to the next subtask.</description>
    </item>
    
    <item>
      <title>Testing</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/testing/</guid>
      <description>Testing # Testing is an integral part of every software development process as such Apache Flink comes with tooling to test your application code on multiple levels of the testing pyramid.
Testing User-Defined Functions # Usually, one can assume that Flink produces correct results outside of a user-defined function. Therefore, it is recommended to test those classes that contain the main business logic with unit tests as much as possible.</description>
    </item>
    
    <item>
      <title>Experimental Features</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/experimental/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/experimental/</guid>
      <description>Experimental Features # This section describes experimental features in the DataStream API. Experimental features are still evolving and can be either unstable, incomplete, or subject to heavy change in future versions.
Reinterpreting a pre-partitioned data stream as keyed stream # We can re-interpret a pre-partitioned data stream as a keyed stream to avoid shuffling.
WARNING: The re-interpreted data stream MUST already be pre-partitioned in EXACTLY the same way Flink&amp;rsquo;s keyBy would partition the data in a shuffle w.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/config/</guid>
      <description>Configuration # By default, the Table &amp;amp; SQL API is preconfigured for producing accurate results with acceptable performance.
Depending on the requirements of a table program, it might be necessary to adjust certain parameters for optimization. For example, unbounded streaming programs may need to ensure that the required state size is capped (see streaming concepts).
Overview # When instantiating a TableEnvironment, EnvironmentSettings can be used to pass the desired configuration for the current session, by passing a Configuration object to the EnvironmentSettings.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/metrics/</guid>
      <description>Metrics # PyFlink exposes a metric system that allows gathering and exposing metrics to external systems.
Registering metrics # You can access the metric system from a Python user-defined function by calling function_context.get_metric_group() in the open method. The get_metric_group() method returns a MetricGroup object on which you can create and register new metrics.
Metric types # PyFlink supports Counters, Gauges, Distribution and Meters.
Counter # A Counter is used to count something.</description>
    </item>
    
    <item>
      <title>Performance Tuning</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/tuning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/tuning/</guid>
      <description>Performance Tuning # SQL is the most widely used language for data analytics. Flink&amp;rsquo;s Table API and SQL enables users to define efficient stream analytics applications in less time and effort. Moreover, Flink Table API and SQL is effectively optimized, it integrates a lot of query optimizations and tuned operator implementations. But not all of the optimizations are enabled by default, so for some workloads, it is possible to improve performance by turning on some options.</description>
    </item>
    
    <item>
      <title>Configuration</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/python_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/python_config/</guid>
      <description>Configuration # Depending on the requirements of a Python API program, it might be necessary to adjust certain parameters for optimization.
For Python DataStream API program, the config options could be set as following:
from pyflink.common import Configuration from pyflink.datastream import StreamExecutionEnvironment config = Configuration() config.set_integer(&amp;#34;python.fn-execution.bundle.size&amp;#34;, 1000) env = StreamExecutionEnvironment.get_execution_environment(config) For Python Table API program, all the config options available for Java/Scala Table API program could also be used in the Python Table API program.</description>
    </item>
    
    <item>
      <title>Debugging</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/debugging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/debugging/</guid>
      <description>Debugging # This page describes how to debug in PyFlink.
Logging Infos # Client Side Logging # You can log contextual and debug information via print or standard Python logging modules in PyFlink jobs in places outside Python UDFs. The logging messages will be printed in the log files of the client during job submission.
from pyflink.table import EnvironmentSettings, TableEnvironment # create a TableEnvironment env_settings = EnvironmentSettings.in_streaming_mode() table_env = TableEnvironment.</description>
    </item>
    
    <item>
      <title>Connectors</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/table/python_table_api_connectors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/table/python_table_api_connectors/</guid>
      <description>Connectors # This page describes how to use connectors in PyFlink and highlights the details to be aware of when using Flink connectors in Python programs.
Note For general connector information and common configuration, please refer to the corresponding Java/Scala documentation.
Download connector and format jars # Since Flink is a Java/Scala-based project, for both connectors and formats, implementations are available as jars that need to be specified as job dependencies.</description>
    </item>
    
    <item>
      <title>Environment Variables</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/environment_variables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/environment_variables/</guid>
      <description>Environment Variables # These environment variables will affect the behavior of PyFlink:
Environment Variable Description FLINK_HOME PyFlink job will be compiled before submitting and it requires Flink&#39;s distribution to compile the job. PyFlink&#39;s installation package already contains Flink&#39;s distribution and it&#39;s used by default. This environment allows you to specify a custom Flink&#39;s distribution. PYFLINK_CLIENT_EXECUTABLE The path of the Python interpreter used to launch the Python process when submitting the Python jobs via &#34;</description>
    </item>
    
    <item>
      <title>User-defined Sources &amp; Sinks</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/sourcessinks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/sourcessinks/</guid>
      <description>User-defined Sources &amp;amp; Sinks # Dynamic tables are the core concept of Flink&amp;rsquo;s Table &amp;amp; SQL API for processing both bounded and unbounded data in a unified fashion.
Because dynamic tables are only a logical concept, Flink does not own the data itself. Instead, the content of a dynamic table is stored in external systems (such as databases, key-value stores, message queues) or files.
Dynamic sources and dynamic sinks can be used to read and write data from and to an external system.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/python/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/python/faq/</guid>
      <description>FAQ # This page describes the solutions to some common questions for PyFlink users.
Preparing Python Virtual Environment # You can download a [convenience script]({% link downloads/setup-pyflink-virtual-env.sh %}) to prepare a Python virtual env zip which can be used on Mac OS and most Linux distributions. You can specify the PyFlink version to generate a Python virtual environment required for the corresponding PyFlink version, otherwise the most recent version will be installed.</description>
    </item>
    
    <item>
      <title>Scala API Extensions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/scala_api_extensions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/scala_api_extensions/</guid>
      <description>Scala API Extensions # In order to keep a fair amount of consistency between the Scala and Java APIs, some of the features that allow a high-level of expressiveness in Scala have been left out from the standard APIs for both batch and streaming.
If you want to enjoy the full Scala experience you can choose to opt-in to extensions that enhance the Scala API via implicit conversions.
To use all the available extensions, you can just add a simple import for the DataStream API</description>
    </item>
    
    <item>
      <title>Java Lambda Expressions</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/datastream/java_lambdas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/datastream/java_lambdas/</guid>
      <description>Java Lambda Expressions # Java 8 introduced several new language features designed for faster and clearer coding. With the most important feature, the so-called &amp;ldquo;Lambda Expressions&amp;rdquo;, it opened the door to functional programming. Lambda expressions allow for implementing and passing functions in a straightforward way without having to declare additional (anonymous) classes.
Flink supports the usage of lambda expressions for all operators of the Java API, however, whenever a lambda expression uses Java generics you need to declare type information explicitly.</description>
    </item>
    
    <item>
      <title>Temporal Table Function</title>
      <link>//localhost/flink/flink-docs-master/docs/dev/table/concepts/temporal_table_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/docs/dev/table/concepts/temporal_table_function/</guid>
      <description>Temporal Table Function # A Temporal table function provides access to the version of a temporal table at a specific point in time. In order to access the data in a temporal table, one must pass a time attribute that determines the version of the table that will be returned. Flink uses the SQL syntax of table functions to provide a way to express it.
Unlike a versioned table, temporal table functions can only be defined on top of append-only streams — it does not support changelog inputs.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.10</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.10/</guid>
      <description>Release Notes - Flink 1.10 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.9 and Flink 1.10. Please read these notes carefully if you are planning to upgrade your Flink version to 1.10.
Clusters &amp;amp; Deployment # FileSystems should be loaded via Plugin Architecture # FLINK-11956 # s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through plugins but now seamlessly integrate with all credential providers.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.11</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.11/</guid>
      <description>Release Notes - Flink 1.11 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.10 and Flink 1.11. Please read these notes carefully if you are planning to upgrade your Flink version to 1.11.
Clusters &amp;amp; Deployment # Support for Application Mode # FLIP-85 # The user can now submit applications and choose to execute their main() method on the cluster rather than the client.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.12</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.12/</guid>
      <description>Release Notes - Flink 1.12 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.11 and Flink 1.12. Please read these notes carefully if you are planning to upgrade your Flink version to 1.12.
Known Issues # Unaligned checkpoint recovery may lead to corrupted data stream # FLINK-20654 # Using unaligned checkpoints in Flink 1.12.0 combined with two/multiple inputs tasks or with union inputs for single input tasks can result in corrupted state.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.13</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.13/</guid>
      <description>Release Notes - Flink 1.13 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.12 and Flink 1.13. Please read these notes carefully if you are planning to upgrade your Flink version to 1.13.
Failover # Remove state.backend.async option. # FLINK-21935 # The state.backend.async option is deprecated. Snapshots are always asynchronous now (as they were by default before) and there is no option to configure a synchronous snapshot any more.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.14</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.14/</guid>
      <description>Release notes - Flink 1.14 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.13 and Flink 1.14. Please read these notes carefully if you are planning to upgrade your Flink version to 1.14.
Known issues # State migration issues # Some of our internal serializers i.e. RowSerializer, TwoPhaseCommitSinkFunction&amp;rsquo;s serializer, LinkedListSerializer might prevent a successful job starts if state migration is necessary.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.15</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.15/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.15/</guid>
      <description>Release notes - Flink 1.15 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.14 and Flink 1.15. Please read these notes carefully if you are planning to upgrade your Flink version to 1.15.
Summary of changed dependency names # There are Several changes in Flink 1.15 that require updating dependency names when upgrading from earlier versions, mainly including the effort to opting-out Scala dependencies from non-scala modules and reorganize table modules.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.5</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.5/</guid>
      <description>Release Notes - Flink 1.5 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.
Update Configuration for Reworked Job Deployment # Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.6</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.6/</guid>
      <description>Release Notes - Flink 1.6 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.5 and Flink 1.6. Please read these notes carefully if you are planning to upgrade your Flink version to 1.6.
Changed Configuration Default Values # The default value of the slot idle timeout slot.idle.timeout is set to the default value of the heartbeat timeout (50 s).
Changed ElasticSearch 5.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.7</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.7/</guid>
      <description>Release Notes - Flink 1.7 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.6 and Flink 1.7. Please read these notes carefully if you are planning to upgrade your Flink version to 1.7.
Scala 2.12 support # When using Scala 2.12 you might have to add explicit type annotations in places where they were not required when using Scala 2.11. This is an excerpt from the TransitiveClosureNaive.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.8</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.8/</guid>
      <description>Release Notes - Flink 1.8 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.7 and Flink 1.8. Please read these notes carefully if you are planning to upgrade your Flink version to 1.8.
State # Continuous incremental cleanup of old Keyed State with TTL # We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (FLINK-9510). This feature allowed to clean up and make inaccessible keyed state entries when accessing them.</description>
    </item>
    
    <item>
      <title>Release Notes - Flink 1.9</title>
      <link>//localhost/flink/flink-docs-master/release-notes/flink-1.9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/release-notes/flink-1.9/</guid>
      <description>Release Notes - Flink 1.9 # These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.8 and Flink 1.9. It also provides an overview on known shortcoming or limitations with new experimental features introduced in 1.9.
Please read these notes carefully if you are planning to upgrade your Flink version to 1.9.
Known shortcomings or limitations for new features # New Table / SQL Blink planner # Flink 1.</description>
    </item>
    
    <item>
      <title>Versions</title>
      <link>//localhost/flink/flink-docs-master/versions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost/flink/flink-docs-master/versions/</guid>
      <description> Versions # An appendix of hosted documentation for all versions of Apache Flink.
v1.15 v1.14 v1.13 v1.12 v1.11 v1.10 v1.9 v1.8 v1.7 v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 </description>
    </item>
    
  </channel>
</rss>
